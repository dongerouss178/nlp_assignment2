{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nlp_stackoverflow_qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "link",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accepted_answers",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "other_answers",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bb822797-0a94-4bc2-a9c0-3aff9b559bc8",
       "rows": [
        [
         "0",
         "307291",
         "How does the Google &quot;Did you mean?&quot; Algorithm work?",
         "https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work",
         "Here's the explanation directly from the source ( almost ) Search 101! at min 22:03 Worth watching! Basically and according to Douglas Merrill former CTO of Google it is like this: 1) You write a ( misspelled ) word in google 2) You don't find what you wanted ( don't click on any results ) 3) You realize you misspelled the word so you rewrite the word in the search box. 4) You find what you want ( you click in the first links ) This pattern multiplied millions of times, shows what are the most common misspells and what are the most \"common\" corrections. This way Google can almost instantaneously, offer spell correction in every language. Also this means if overnight everyone start to spell night as \"nigth\" google would suggest that word instead. EDIT @ThomasRutter: Douglas describe it as \"statistical machine learning\". They know who correct the query, because they know which query comes from which user ( using cookies ) If the users perform a query, and only 10% of the users click on a result and 90% goes back and type another query ( with the corrected word ) and this time that 90% clicks on a result, then they know they have found a correction. They can also know if those are \"related\" queries of two different, because they have information of all the links they show. Furthermore, they are now including the context into the spell check, so they can even suggest different word depending on the context. See this demo of google wave ( @ 44m 06s ) that shows how the context is taken into account to automatically correct the spelling. Here it is explained how that natural language processing works. And finally here is an awesome demo of what can be done adding automatic machine translation ( @ 1h 12m 47s ) to the mix. I've added anchors of minute and seconds to the videos to skip directly to the content, if they don't work, try reloading the page or scrolling by hand to the mark.",
         "[User 28582]: I found this article some time ago: How to Write a Spelling Corrector , written by Peter Norvig (Director of Research at Google Inc.). It's an interesting read about the \"spelling correction\" topic. The examples are in Python but it's clear and simple to understand, and I think that the algorithm can be easily translated to other languages. Below follows a short description of the algorithm. The algorithm consists of two steps, preparation and word checking. Step 1: Preparation - setting up the word database Best is if you can use actual search words and their occurence. If you don't have that a large set of text can be used instead. Count the occurrence (popularity) of each word. Step 2. Word checking - finding words that are similar to the one checked Similar means that the edit distance is low (typically 0-1 or 0-2). The edit distance is the minimum number of inserts/deletes/changes/swaps needed to transform one word to another. Choose the most popular word from the previous step and suggest it as a correction (if other than the word itself)."
        ],
        [
         "1",
         "8897593",
         "How to compute the similarity between two text documents?",
         "https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents",
         "The common way of doing this is to transform the documents into TF-IDF vectors and then compute the cosine similarity between them. Any textbook on information retrieval (IR) covers this. See esp. Introduction to Information Retrieval , which is free and available online. Computing Pairwise Similarities TF-IDF (and similar text transformations) are implemented in the Python packages Gensim and scikit-learn . In the latter package, computing cosine similarities is as easy as from sklearn.feature_extraction.text import TfidfVectorizer documents = [open(f).read() for f in text_files] tfidf = TfidfVectorizer().fit_transform(documents) # no need to normalize, since Vectorizer will return normalized tf-idf pairwise_similarity = tfidf * tfidf.T or, if the documents are plain strings, >>> corpus = [\"I'd like an apple\", ... \"An apple a day keeps the doctor away\", ... \"Never compare an apple to an orange\", ... \"I prefer scikit-learn to Orange\", ... \"The scikit-learn docs are Orange and Blue\"] >>> vect = TfidfVectorizer(min_df=1, stop_words=\"english\") >>> tfidf = vect.fit_transform(corpus) >>> pairwise_similarity = tfidf * tfidf.T though Gensim may have more options for this kind of task. See also this question . [Disclaimer: I was involved in the scikit-learn TF-IDF implementation.] Interpreting the Results From above, pairwise_similarity is a Scipy sparse matrix that is square in shape, with the number of rows and columns equal to the number of documents in the corpus. >>> pairwise_similarity <5x5 sparse matrix of type ' ' with 17 stored elements in Compressed Sparse Row format> You can convert the sparse array to a NumPy array via .toarray() or .A : >>> pairwise_similarity.toarray() array([[1. , 0.17668795, 0.27056873, 0. , 0. ], [0.17668795, 1. , 0.15439436, 0. , 0. ], [0.27056873, 0.15439436, 1. , 0.19635649, 0.16815247], [0. , 0. , 0.19635649, 1. , 0.54499756], [0. , 0. , 0.16815247, 0.54499756, 1. ]]) Let's say we want to find the document most similar to the final document, \"The scikit-learn docs are Orange and Blue\". This document has index 4 in corpus . You can find the index of the most similar document by taking the argmax of that row, but first you'll need to mask the 1's, which represent the similarity of each document to itself . You can do the latter through np.fill_diagonal() , and the former through np.nanargmax() : >>> import numpy as np >>> arr = pairwise_similarity.toarray() >>> np.fill_diagonal(arr, np.nan) >>> input_doc = \"The scikit-learn docs are Orange and Blue\" >>> input_idx = corpus.index(input_doc) >>> input_idx 4 >>> result_idx = np.nanargmax(arr[input_idx]) >>> corpus[result_idx] 'I prefer scikit-learn to Orange' Note: the purpose of using a sparse matrix is to save (a substantial amount of space) for a large corpus & vocabulary. Instead of converting to a NumPy array, you could do: >>> n, _ = pairwise_similarity.shape >>> pairwise_similarity[np.arange(n), np.arange(n)] = -1.0 >>> pairwise_similarity[input_idx].argmax() 3",
         "[User 125617]: Identical to @larsman, but with some preprocessing import nltk, string from sklearn.feature_extraction.text import TfidfVectorizer nltk.download('punkt') # if necessary... stemmer = nltk.stem.porter.PorterStemmer() remove_punctuation_map = dict((ord(char), None) for char in string.punctuation) def stem_tokens(tokens): return [stemmer.stem(item) for item in tokens] '''remove punctuation, lowercase, stem''' def normalize(text): return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map))) vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english') def cosine_sim(text1, text2): tfidf = vectorizer.fit_transform([text1, text2]) return ((tfidf * tfidf.T).A)[0,1] print cosine_sim('a little bird', 'a little bird') print cosine_sim('a little bird', 'a little bird chirps') print cosine_sim('a little bird', 'a big dog barks')"
        ],
        [
         "2",
         "52455774",
         "googletrans stopped working with error &#39;NoneType&#39; object has no attribute &#39;group&#39;",
         "https://stackoverflow.com/questions/52455774/googletrans-stopped-working-with-error-nonetype-object-has-no-attribute-group",
         null,
         "[User 10667419]: Update 06.12.20: A new 'official' alpha version of googletrans with a fix was released Install the alpha version like this: pip install googletrans==3.1.0a0 Translation example: translator = Translator() translation = translator.translate(\"Der Himmel ist blau und ich mag Bananen\", dest='en') print(translation.text) #output: 'The sky is blue and I like bananas' In case it does not work, try to specify the service url like this: from googletrans import Translator translator = Translator(service_urls=['translate.googleapis.com']) translator.translate(\"Der Himmel ist blau und ich mag Bananen\", dest='en') See the discussion here for details and updates: https://github.com/ssut/py-googletrans/pull/237 Update 10.12.20: Another fix was released As pointed out by @DesiKeki and @Ahmed Breem, there is another fix which seems to work for several people: pip install googletrans==4.0.0-rc1 Github discussion here: https://github.com/ssut/py-googletrans/issues/234#issuecomment-742460612 In case the fixes above don't work for you If the above doesn't work for you, google_trans_new seems to be a good alternative that works for some people. It's unclear why the fix above works for some and doesn't for others. See details on installation and usage here: https://github.com/lushan88a/google_trans_new #pip install google_trans_new from google_trans_new import google_translator translator = google_translator() translate_text = translator.translate('สวัสดีจีน',lang_tgt='en') print(translate_text) #output: Hello china"
        ],
        [
         "3",
         "1787110",
         "What is the difference between lemmatization vs stemming?",
         "https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming",
         "Short and dense: http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . From the NLTK docs: Lemmatization and stemming are special cases of normalization. They identify a canonical representative for a set of related word forms.",
         "[User 327862]: Lemmatisation is closely related to stemming . The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications. For instance: The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up. The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation. The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context. Source : https://en.wikipedia.org/wiki/Lemmatisation"
        ],
        [
         "4",
         "39142778",
         "How to determine the language of a piece of text?",
         "https://stackoverflow.com/questions/39142778/how-to-determine-the-language-of-a-piece-of-text",
         null,
         "[User 3266110]: 1. TextBlob . (Deprecated - Use official Google Translate API instead) Requires NLTK package, uses Google. from textblob import TextBlob b = TextBlob(\"bonjour\") b.detect_language() pip install textblob Note: This solution requires internet access and Textblob is using Google Translate's language detector by calling the API . 2. Polyglot . Requires numpy and some arcane libraries, unlikely to get it work for Windows . (For Windows, get an appropriate versions of PyICU , Morfessor and PyCLD2 from here , then just pip install downloaded_wheel.whl .) Able to detect texts with mixed languages. from polyglot.detect import Detector mixed_text = u\"\"\" China (simplified Chinese: 中国; traditional Chinese: 中國), officially the People's Republic of China (PRC), is a sovereign state located in East Asia. \"\"\" for language in Detector(mixed_text).languages: print(language) # name: English code: en confidence: 87.0 read bytes: 1154 # name: Chinese code: zh_Hant confidence: 5.0 read bytes: 1755 # name: un code: un confidence: 0.0 read bytes: 0 pip install polyglot To install the dependencies, run: sudo apt-get install python-numpy libicu-dev Note: Polyglot is using pycld2 , see https://github.com/aboSamoor/polyglot/blob/master/polyglot/detect/base.py#L72 for details. 3. chardet Chardet has also a feature of detecting languages if there are character bytes in range (127-255]: >>> chardet.detect(\"Я люблю вкусные пампушки\".encode('cp1251')) {'encoding': 'windows-1251', 'confidence': 0.9637267119204621, 'language': 'Russian'} pip install chardet 4. langdetect Requires large portions of text. It uses non-deterministic approach under the hood. That means you get different results for the same text sample. Docs say you have to use following code to make it determined: from langdetect import detect, DetectorFactory DetectorFactory.seed = 0 detect('今一はお前さん') pip install langdetect 5. guess_language Can detect very short samples by using this spell checker with dictionaries. pip install guess_language-spirit 6. langid langid.py provides both a module import langid langid.classify(\"This is a test\") # ('en', -54.41310358047485) and a command-line tool: $ langid < README.md pip install langid 7. FastText FastText is a text classifier, can be used to recognize 176 languages with a proper models for language classification . Download this model , then: import fasttext model = fasttext.load_model('lid.176.ftz') print(model.predict('الشمس تشرق', k=2)) # top 2 matching languages (('__label__ar', '__label__fa'), array([0.98124713, 0.01265871])) pip install fasttext 8. pyCLD3 pycld3 is a neural network model for language identification. This package contains the inference code and a trained model. import cld3 cld3.get_language(\"影響包含對氣候的變化以及自然資源的枯竭程度\") LanguagePrediction(language='zh', probability=0.999969482421875, is_reliable=True, proportion=1.0) pip install pycld3"
        ],
        [
         "5",
         "1833252",
         "Java Stanford NLP: Part of Speech labels?",
         "https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-labels",
         "The Penn Treebank Project . Look at the Part-of-speech tagging ps. JJ is adjective. NNS is noun, plural. VBP is verb present tense. RB is adverb. That's for english. For chinese, it's the Penn Chinese Treebank. And for german it's the NEGRA corpus. CC Coordinating conjunction CD Cardinal number DT Determiner EX Existential there FW Foreign word IN Preposition or subordinating conjunction JJ Adjective JJR Adjective, comparative JJS Adjective, superlative LS List item marker MD Modal NN Noun, singular or mass NNS Noun, plural NNP Proper noun, singular NNPS Proper noun, plural PDT Predeterminer POS Possessive ending PRP Personal pronoun PRP$ Possessive pronoun RB Adverb RBR Adverb, comparative RBS Adverb, superlative RP Particle SYM Symbol TO to UH Interjection VB Verb, base form VBD Verb, past tense VBG Verb, gerund or present participle VBN Verb, past participle VBP Verb, non­3rd person singular present VBZ Verb, 3rd person singular present WDT Wh­determiner WP Wh­pronoun WP$ Possessive wh­pronoun WRB Wh­adverb",
         "[User 553223]: Explanation of each tag from the documentation: CC: conjunction, coordinating & 'n and both but either et for less minus neither nor or plus so therefore times v. versus vs. whether yet CD: numeral, cardinal mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty- seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025 fifteen 271,124 dozen quintillion DM2,000 ... DT: determiner all an another any both del each either every half la many much nary neither no some such that the them these this those EX: existential there there FW: foreign word gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte terram fiche oui corporis ... IN: preposition or conjunction, subordinating astride among uppon whether out inside pro despite on by throughout below within for towards near behind atop around if like until below next into if beside ... JJ: adjective or numeral, ordinal third ill-mannered pre-war regrettable oiled calamitous first separable ectoplasmic battery-powered participatory fourth still-to-be-named multilingual multi-disciplinary ... JJR: adjective, comparative bleaker braver breezier briefer brighter brisker broader bumper busier calmer cheaper choosier cleaner clearer closer colder commoner costlier cozier creamier crunchier cuter ... JJS: adjective, superlative calmest cheapest choicest classiest cleanest clearest closest commonest corniest costliest crassest creepiest crudest cutest darkest deadliest dearest deepest densest dinkiest ... LS: list item marker A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005 SP-44007 Second Third Three Two * a b c d first five four one six three two MD: modal auxiliary can cannot could couldn't dare may might must need ought shall should shouldn't will would NN: noun, common, singular or mass common-carrier cabbage knuckle-duster Casino afghan shed thermostat investment slide humour falloff slick wind hyena override subhumanity machinist ... NNS: noun, common, plural undergraduates scotches bric-a-brac products bodyguards facets coasts divestitures storehouses designs clubs fragrances averages subjectivists apprehensions muses factory-jobs ... NNP: noun, proper, singular Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA Shannon A.K.C. Meltex Liverpool ... NNPS: noun, proper, plural Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques Apache Apaches Apocrypha ... PDT: pre-determiner all both half many quite such sure this POS: genitive marker ' 's PRP: pronoun, personal hers herself him himself hisself it itself me myself one oneself ours ourselves ownself self she thee theirs them themselves they thou thy us PRP$: pronoun, possessive her his mine my our ours their thy your RB: adverb occasionally unabatingly maddeningly adventurously professedly stirringly prominently technologically magisterially predominately swiftly fiscally pitilessly ... RBR: adverb, comparative further gloomier grander graver greater grimmer harder harsher healthier heavier higher however larger later leaner lengthier less- perfectly lesser lonelier longer louder lower more ... RBS: adverb, superlative best biggest bluntest earliest farthest first furthest hardest heartiest highest largest least less most nearest second tightest worst RP: particle aboard about across along apart around aside at away back before behind by crop down ever fast for forth from go high i.e. in into just later low more off on open out over per pie raising start teeth that through under unto up up-pp upon whole with you SYM: symbol % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** *** TO: \"to\" as preposition or infinitive marker to UH: interjection Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly man baby diddle hush sonuvabitch ... VB: verb, base form ask assemble assess assign assume atone attention avoid bake balkanize bank begin behold believe bend benefit bevel beware bless boil bomb boost brace break bring broil brush build ... VBD: verb, past tense dipped pleaded swiped regummed soaked tidied convened halted registered cushioned exacted snubbed strode aimed adopted belied figgered speculated wore appreciated contemplated ... VBG: verb, present participle or gerund telegraphing stirring focusing angering judging stalling lactating hankerin' alleging veering capping approaching traveling besieging encrypting interrupting erasing wincing ... VBN: verb, past participle multihulled dilapidated aerosolized chaired languished panelized used experimented flourished imitated reunifed factored condensed sheared unsettled primed dubbed desired ... VBP: verb, present tense, not 3rd person singular predominate wrap resort sue twist spill cure lengthen brush terminate appear tend stray glisten obtain comprise detest tease attract emphasize mold postpone sever return wag ... VBZ: verb, present tense, 3rd person singular bases reconstructs marks mixes displeases seals carps weaves snatches slumps stretches authorizes smolders pictures emerges stockpiles seduces fizzes uses bolsters slaps speaks pleads ... WDT: WH-determiner that what whatever which whichever WP: WH-pronoun that what whatever whatsoever which who whom whosoever WP$: WH-pronoun, possessive whose WRB: Wh-adverb how however whence whenever where whereby whereever wherein whereof why"
        ],
        [
         "6",
         "54334304",
         "spaCy: Can&#39;t find model &#39;en_core_web_sm&#39; on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)",
         "https://stackoverflow.com/questions/54334304/spacy-cant-find-model-en-core-web-sm-on-windows-10-and-python-3-5-3-anaco",
         null,
         "[User 4153895]: Initially I downloaded two en packages using following statements in anaconda prompt. python -m spacy download en_core_web_lg python -m spacy download en_core_web_sm But, I kept on getting linkage error and finally running below command helped me to establish link and solved error. python -m spacy download en Also make sure you to restart your runtime if working with Jupyter. -PS : If you get linkage error try giving admin previlages."
        ],
        [
         "7",
         "34870614",
         "What does tf.nn.embedding_lookup function do?",
         "https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do",
         "embedding_lookup function retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy. E.g. matrix = np.random.random([1024, 64]) # 64-dimensional embeddings ids = np.array([0, 5, 17, 33]) print matrix[ids] # prints a matrix of shape [4, 64] params argument can be also a list of tensors in which case the ids will be distributed among the tensors. For example, given a list of 3 tensors [2, 64] , the default behavior is that they will represent ids : [0, 3] , [1, 4] , [2, 5] . partition_strategy controls the way how the ids are distributed among the list. The partitioning is useful for larger scale problems when the matrix might be too large to keep in one piece.",
         "[User 4949974]: Yes, this function is hard to understand, until you get the point. In its simplest form, it is similar to tf.gather . It returns the elements of params according to the indexes specified by ids . For example (assuming you are inside tf.InteractiveSession() ) params = tf.constant([10,20,30,40]) ids = tf.constant([0,1,2,3]) print tf.nn.embedding_lookup(params,ids).eval() would return [10 20 30 40] , because the first element (index 0) of params is 10 , the second element of params (index 1) is 20 , etc. Similarly, params = tf.constant([10,20,30,40]) ids = tf.constant([1,1,3]) print tf.nn.embedding_lookup(params,ids).eval() would return [20 20 40] . But embedding_lookup is more than that. The params argument can be a list of tensors, rather than a single tensor. params1 = tf.constant([1,2]) params2 = tf.constant([10,20]) ids = tf.constant([2,0,2,1,2,3]) result = tf.nn.embedding_lookup([params1, params2], ids) In such a case, the indexes, specified in ids , correspond to elements of tensors according to a partition strategy , where the default partition strategy is 'mod'. In the 'mod' strategy, index 0 corresponds to the first element of the first tensor in the list. Index 1 corresponds to the first element of the second tensor. Index 2 corresponds to the first element of the third tensor, and so on. Simply index i corresponds to the first element of the (i+1)th tensor , for all the indexes 0..(n-1) , assuming params is a list of n tensors. Now, index n cannot correspond to tensor n+1, because the list params contains only n tensors. So index n corresponds to the second element of the first tensor. Similarly, index n+1 corresponds to the second element of the second tensor, etc. So, in the code params1 = tf.constant([1,2]) params2 = tf.constant([10,20]) ids = tf.constant([2,0,2,1,2,3]) result = tf.nn.embedding_lookup([params1, params2], ids) index 0 corresponds to the first element of the first tensor: 1 index 1 corresponds to the first element of the second tensor: 10 index 2 corresponds to the second element of the first tensor: 2 index 3 corresponds to the second element of the second tensor: 20 Thus, the result would be: [ 2 1 2 10 2 20]"
        ],
        [
         "8",
         "15547409",
         "How to get rid of punctuation using NLTK tokenizer?",
         "https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer",
         null,
         "[User 653815]: Take a look at the other tokenizing options that nltk provides here . For example, you can define a tokenizer that picks out sequences of alphanumeric characters as tokens and drops everything else: from nltk.tokenize import RegexpTokenizer tokenizer = RegexpTokenizer(r'\\w+') tokenizer.tokenize('Eighty-seven miles to go, yet. Onward!') Output: ['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']"
        ],
        [
         "9",
         "405161",
         "Detecting syllables in a word",
         "https://stackoverflow.com/questions/405161/detecting-syllables-in-a-word",
         "Read about the TeX approach to this problem for the purposes of hyphenation. Especially see Frank Liang's thesis dissertation Word Hy-phen-a-tion by Com-put-er . His algorithm is very accurate, and then includes a small exceptions dictionary for cases where the algorithm does not work.",
         null
        ],
        [
         "10",
         "31421413",
         "How to compute precision, recall, accuracy and f1-score for the multiclass case with scikit learn?",
         "https://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case",
         "I think there is a lot of confusion about which weights are used for what. I am not sure I know precisely what bothers you so I am going to cover different topics, bear with me ;). Class weights The weights from the class_weight parameter are used to train the classifier . They are not used in the calculation of any of the metrics you are using : with different class weights, the numbers will be different simply because the classifier is different. Basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights. How they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it. The metrics Once you have a classifier, you want to know how well it is performing. Here you can use the metrics you mentioned: accuracy , recall_score , f1_score ... Usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class. I will not detail all these metrics but note that, with the exception of accuracy , they are naturally applied at the class level: as you can see in this print of a classification report they are defined for each class. They rely on concepts such as true positives or false negative that require defining which class is the positive one. precision recall f1-score support 0 0.65 1.00 0.79 17 1 0.57 0.75 0.65 16 2 0.33 0.06 0.10 17 avg / total 0.52 0.60 0.51 50 The warning F1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\". You get this warning because you are using the f1-score, recall and precision without defining how they should be computed! The question could be rephrased: from the above classification report, how do you output one global number for the f1-score? You could: Take the average of the f1-score for each class: that's the avg / total result above. It's also called macro averaging. Compute the f1-score using the global count of true positives / false negatives, etc. (you sum the number of true positives / false negatives for each class). Aka micro averaging. Compute a weighted average of the f1-score. Using 'weighted' in scikit-learn will weigh the f1-score by the support of the class: the more elements a class has, the more important the f1-score for this class in the computation. These are 3 of the options in scikit-learn, the warning is there to say you have to pick one . So you have to specify an average argument for the score method. Which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5. The whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it. Computing scores Last thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier has never seen . This is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant. Here's a way to do it using StratifiedShuffleSplit , which gives you a random splits of your data (after shuffling) that preserve the label distribution. from sklearn.datasets import make_classification from sklearn.cross_validation import StratifiedShuffleSplit from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix # We use a utility to generate artificial classification data. X, y = make_classification(n_samples=100, n_informative=10, n_classes=3) sss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=0) for train_idx, test_idx in sss: X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx] svc.fit(X_train, y_train) y_pred = svc.predict(X_test) print(f1_score(y_test, y_pred, average=\"macro\")) print(precision_score(y_test, y_pred, average=\"macro\")) print(recall_score(y_test, y_pred, average=\"macro\"))",
         "[User 1597866]: Lot of very detailed answers here but I don't think you are answering the right questions. As I understand the question, there are two concerns: How to I score a multiclass problem? How do I deal with unbalanced data? 1. You can use most of the scoring functions in scikit-learn with both multiclass problem as with single class problems. Ex.: from sklearn.metrics import precision_recall_fscore_support as score predicted = [1,2,3,4,5,1,2,1,1,4,5] y_test = [1,2,3,4,5,1,2,1,1,4,1] precision, recall, fscore, support = score(y_test, predicted) print('precision: {}'.format(precision)) print('recall: {}'.format(recall)) print('fscore: {}'.format(fscore)) print('support: {}'.format(support)) This way you end up with tangible and interpretable numbers for each of the classes. | Label | Precision | Recall | FScore | Support | |-------|-----------|--------|--------|---------| | 1 | 94% | 83% | 0.88 | 204 | | 2 | 71% | 50% | 0.54 | 127 | | ... | ... | ... | ... | ... | | 4 | 80% | 98% | 0.89 | 838 | | 5 | 93% | 81% | 0.91 | 1190 | Then... 2. ... you can tell if the unbalanced data is even a problem. If the scoring for the less represented classes (class 1 and 2) are lower than for the classes with more training samples (class 4 and 5) then you know that the unbalanced data is in fact a problem, and you can act accordingly, as described in some of the other answers in this thread. However, if the same class distribution is present in the data you want to predict on, your unbalanced training data is a good representative of the data, and hence, the unbalance is a good thing."
        ],
        [
         "11",
         "27697766",
         "Understanding min_df and max_df in scikit CountVectorizer",
         "https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer",
         "max_df is used for removing terms that appear too frequently , also known as \"corpus-specific stop words\". For example: max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents \". max_df = 25 means \"ignore terms that appear in more than 25 documents \". The default max_df is 1.0 , which means \"ignore terms that appear in more than 100% of the documents \". Thus, the default setting does not ignore any terms. min_df is used for removing terms that appear too infrequently . For example: min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents \". min_df = 5 means \"ignore terms that appear in less than 5 documents \". The default min_df is 1 , which means \"ignore terms that appear in less than 1 document \". Thus, the default setting does not ignore any terms.",
         null
        ],
        [
         "12",
         "9294926",
         "How does Apple find dates, times and addresses in emails?",
         "https://stackoverflow.com/questions/9294926/how-does-apple-find-dates-times-and-addresses-in-emails",
         "They likely use Information Extraction techniques for this. Here is a demo of Stanford's SUTime tool: http://nlp.stanford.edu:8080/sutime/process You would extract attributes about n-grams (consecutive words) in a document: numberOfLetters numberOfSymbols length previousWord nextWord nextWordNumberOfSymbols ... And then use a classification algorithm, and feed it positive and negative examples: Observation nLetters nSymbols length prevWord nextWord isPartOfDate \"Feb.\" 3 1 4 \"Wed\" \"29th\" TRUE \"DEC\" 3 0 3 \"company\" \"went\" FALSE ... You might get away with 50 examples of each, but the more the merrier. Then, the algorithm learns based on those examples, and can apply to future examples that it hasn't seen before. It might learn rules such as if previous word is only characters and maybe periods... and current word is in \"february\", \"mar.\", \"the\" ... and next word is in \"twelfth\", any_number ... then is date Here is a decent video by a Google engineer on the subject",
         "[User 464635]: That's a technology Apple actually developed a very long time ago called Apple Data Detectors . You can read more about it here: http://www.miramontes.com/writing/add-cacm/ Essentially it parses the text and detects patterns that represent specific pieces of data, then applies OS-contextual actions to it. It's neat."
        ],
        [
         "13",
         "10401076",
         "Difference between constituency parser and dependency parser",
         "https://stackoverflow.com/questions/10401076/difference-between-constituency-parser-and-dependency-parser",
         "A constituency parse tree breaks a text into sub-phrases. Non-terminals in the tree are types of phrases, the terminals are the words in the sentence, and the edges are unlabeled. For a simple sentence \"John sees Bill\", a constituency parse would be: Sentence | +-------------+------------+ | | Noun Phrase Verb Phrase | | John +-------+--------+ | | Verb Noun Phrase | | sees Bill A dependency parse connects words according to their relationships. Each vertex in the tree represents a word, child nodes are words that are dependent on the parent, and edges are labeled by the relationship. A dependency parse of \"John sees Bill\", would be: sees | +--------------+ subject | | object | | John Bill You should use the parser type that gets you closest to your goal. If you are interested in sub-phrases within the sentence, you probably want the constituency parse. If you are interested in the dependency relationships between words, then you probably want the dependency parse. The Stanford parser can give you either ( online demo ). In fact, the way it really works is to always parse the sentence with the constituency parser, and then, if needed, it performs a deterministic (rule-based) transformation on the constituency parse tree to convert it into a dependency tree. More can be found here: http://en.wikipedia.org/wiki/Phrase_structure_grammar http://en.wikipedia.org/wiki/Dependency_grammar",
         null
        ],
        [
         "14",
         "27860652",
         "What is the concept of negative-sampling in word2vec?",
         "https://stackoverflow.com/questions/27860652/what-is-the-concept-of-negative-sampling-in-word2vec",
         "The idea of word2vec is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. In equation (3) of the paper you link to, ignore the exponentiation for a moment. You have v_c . v_w ------------------- sum_i(v_ci . v_w) The numerator is basically the similarity between words c (the context) and w (the target) word. The denominator computes the similarity of all other contexts ci and the target word w . Maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not. However, computing this can be very slow, because there are many contexts ci . Negative sampling is one of the ways of addressing this problem- just select a couple of contexts ci at random. The end result is that if cat appears in the context of food , then the vector of food is more similar to the vector of cat (as measures by their dot product) than the vectors of several other randomly chosen words (e.g. democracy , greed , Freddy ), instead of all other words in language . This makes word2vec much much faster to train.",
         null
        ],
        [
         "15",
         "51956000",
         "What does Keras Tokenizer method exactly do?",
         "https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do",
         "From the source code : fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot). texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved. Why don't combine them? Because you almost always fit once and convert to sequences many times . You will fit on your training corpus once and use that exact same word_index dictionary at train / eval / testing / prediction time to convert actual text into sequences to feed them to the network. So it makes sense to keep those methods separate.",
         null
        ],
        [
         "16",
         "22904025",
         "Java or Python for Natural Language Processing",
         "https://stackoverflow.com/questions/22904025/java-or-python-for-natural-language-processing",
         null,
         "[User 610569]: Java vs Python for NLP is very much a preference or necessity. Depending on the company/projects you'll need to use one or the other and often there isn't much of a choice unless you're heading a project. Other than NLTK (www.nltk.org), there are actually other libraries for text processing in python : TextBlob : http://textblob.readthedocs.org/en/dev/ Gensim : http://radimrehurek.com/gensim/ Pattern : http://www.clips.ua.ac.be/pattern Spacy :: http://spacy.io Orange : http://orange.biolab.si/features/ Pineapple : https://github.com/proycon/pynlpl (for more, see https://pypi.python.org/pypi?%3Aaction=search&term=natural+language+processing&submit=search ) For Java , there're tonnes of others but here's another list: Freeling : http://nlp.lsi.upc.edu/freeling/ OpenNLP : http://opennlp.apache.org/ LingPipe : http://alias-i.com/lingpipe/ Stanford CoreNLP : http://stanfordnlp.github.io/CoreNLP/ (comes with wrappers for other languages, python included) CogComp NLP : https://github.com/CogComp/cogcomp-nlp This is a nice comparison for basic string processing, see http://nltk.googlecode.com/svn/trunk/doc/howto/nlp-python.html A useful comparison of GATE vs UIMA vs OpenNLP, see https://www.assembla.com/spaces/extraction-of-cost-data/wiki/Gate-vs-UIMA-vs-OpenNLP?version=4 If you're uncertain, which is the language to go for NLP, personally i say, \"any language that will give you the desired analysis/output\", see Which language or tools to learn for natural language processing? Here's a pretty recent (2017) of NLP tools: https://github.com/alvations/awesome-community-curated-nlp An older list of NLP tools (2013): http://web.archive.org/web/20130703190201/http://yauhenklimovich.wordpress.com/2013/05/20/tools-nlp Other than language processing tools, you would very much need machine learning tools to incorporate into NLP pipelines. There's a whole range in Python and Java , and once again it's up to preference and whether the libraries are user-friendly enough: Machine Learning libraries in python: Sklearn (Scikit-learn): http://scikit-learn.org/stable/ Milk : http://luispedro.org/software/milk Scipy : http://www.scipy.org/ Theano : http://deeplearning.net/software/theano/ PyML : http://pyml.sourceforge.net/ pyBrain : http://pybrain.org/ Graphlab Create (Commerical tool but free academic license for 1 year): https://dato.com/products/create/ (for more, see https://pypi.python.org/pypi?%3Aaction=search&term=machine+learning&submit=search ) Weka : http://www.cs.waikato.ac.nz/ml/weka/index.html Mallet : http://mallet.cs.umass.edu/ Mahout : https://mahout.apache.org/ With the recent (2015) deep learning tsunami in NLP , possibly you could consider: https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software I'll avoid listing deep learning tools out of non-favoritism / neutrality. Other Stackoverflow questions that also asked for NLP/ML tools: Machine Learning and Natural Language Processing What are good starting points for someone interested in natural language processing? Natural language processing Natural Language Processing in Java (NLP) Is there a good natural language processing library Simple Natural Language Processing Startup for Java What libraries offer basic or advanced NLP methods? Latest good languages and books for Natural Language Processing, the basics (For NER) Entity Extraction/Recognition with free tools while feeding Lucene Index (With PHP) NLP programming tools using PHP? (With Ruby) https://stackoverflow.com/questions/3776361/ruby-nlp-libraries"
        ],
        [
         "17",
         "41424",
         "How do you implement a &quot;Did you mean&quot;?",
         "https://stackoverflow.com/questions/41424/how-do-you-implement-a-did-you-mean",
         null,
         null
        ],
        [
         "18",
         "771918",
         "How do I do word Stemming or Lemmatization?",
         "https://stackoverflow.com/questions/771918/how-do-i-do-word-stemming-or-lemmatization",
         null,
         "[User 75694]: If you know Python, The Natural Language Toolkit (NLTK) has a very powerful lemmatizer that makes use of WordNet . Note that if you are using this lemmatizer for the first time, you must download the corpus prior to using it. This can be done by: >>> import nltk >>> nltk.download('wordnet') You only have to do this once. Assuming that you have now downloaded the corpus, it works like this: >>> from nltk.stem.wordnet import WordNetLemmatizer >>> lmtzr = WordNetLemmatizer() >>> lmtzr.lemmatize('cars') 'car' >>> lmtzr.lemmatize('feet') 'foot' >>> lmtzr.lemmatize('people') 'people' >>> lmtzr.lemmatize('fantasized','v') 'fantasize' There are other lemmatizers in the nltk.stem module , but I haven't tried them myself."
        ],
        [
         "19",
         "3522372",
         "How to config nltk data directory from code?",
         "https://stackoverflow.com/questions/3522372/how-to-config-nltk-data-directory-from-code",
         null,
         null
        ],
        [
         "20",
         "58636587",
         "How can I use BERT for long text classification?",
         "https://stackoverflow.com/questions/58636587/how-can-i-use-bert-for-long-text-classification",
         null,
         null
        ],
        [
         "21",
         "1288291",
         "How can I correctly prefix a word with &quot;a&quot; and &quot;an&quot;?",
         "https://stackoverflow.com/questions/1288291/how-can-i-correctly-prefix-a-word-with-a-and-an",
         "Download Wikipedia Unzip it and write a quick filter program that spits out only article text (the download is generally in XML format, along with non-article metadata too). Find all instances of a(n).... and make an index on the following word and all of its prefixes (you can use a simple suffixtrie for this). This should be case sensitive, and you'll need a maximum word-length - 15 letters? (optional) Discard all those prefixes which occur less than 5 times or where \"a\" vs. \"an\" achieves less than 2/3 majority (or some other threshholds - tweak here). Preferably keep the empty prefix to avoid corner-cases. You can optimize your prefix database by discarding all those prefixes whose parent shares the same \"a\" or \"an\" annotation. When determining whether to use \"A\" or \"AN\" find the longest matching prefix, and follow its lead. If you didn't discard the empty prefix in step 4, then there will always be a matching prefix (namely the empty prefix), otherwise you may need a special case for a completely-non matching string (such input should be very rare). You probably can't get much better than this - and it'll certainly beat most rule-based systems. Edit: I've implemented this in JS/C# . You can try it in your browser , or download the small, reusable javascript implementation it uses. The .NET implementation is package AvsAn on nuget . The implementations are trivial, so it should be easy to port to any other language if necessary. Turns out the \"rules\" are quite a bit more complex than I thought: it's an unanticipated result but it's a unanimous vote it's an honest decision but a honeysuckle shrub Symbols: It's an 0800 number, or an ∞ of oregano. Acronyms: It's a NASA scientist, but an NSA analyst; a FIAT car but an FAA policy. ...which just goes to underline that a rule based system would be tricky to build!",
         null
        ],
        [
         "22",
         "13883277",
         "How to use Stanford Parser in NLTK using Python",
         "https://stackoverflow.com/questions/13883277/how-to-use-stanford-parser-in-nltk-using-python",
         null,
         null
        ],
        [
         "23",
         "9647202",
         "Ordinal numbers replacement",
         "https://stackoverflow.com/questions/9647202/ordinal-numbers-replacement",
         "The package number-parser can parse ordinal words (\"first\", \"second\", etc) to integers. from number_parser import parse_ordinal n = parse_ordinal(\"first\") To convert an integer to \"1st\", \"2nd\", etc, you can use the following: def ordinal(n: int): if 11 <= (n % 100) <= 13: suffix = 'th' else: suffix = ['th', 'st', 'nd', 'rd', 'th'][min(n % 10, 4)] return str(n) + suffix Here is a more terse but less readable version (taken from Gareth on codegolf ): ordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n//10%10!=1)*(n%10<4)*n%10::4]) This works on any number: print([ordinal(n) for n in range(1,32)]) ['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th', '11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th', '20th', '21st', '22nd', '23rd', '24th', '25th', '26th', '27th', '28th', '29th', '30th', '31st']",
         null
        ],
        [
         "24",
         "10383044",
         "Fuzzy String Comparison",
         "https://stackoverflow.com/questions/10383044/fuzzy-string-comparison",
         null,
         "[User 2038264]: There is a package called thefuzz . Install via pip: pip install thefuzz Simple usage: >>> from thefuzz import fuzz >>> fuzz.ratio(\"this is a test\", \"this is a test!\") 97 The package is built on top of difflib . Why not just use that, you ask? Apart from being a bit simpler, it has a number of different matching methods (like token order insensitivity, partial string matching) which make it more powerful in practice. The process.extract functions are especially useful: find the best matching strings and ratios from a set. From their readme: Partial Ratio >>> fuzz.partial_ratio(\"this is a test\", \"this is a test!\") 100 Token Sort Ratio >>> fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\") 90 >>> fuzz.token_sort_ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\") 100 Token Set Ratio >>> fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\") 84 >>> fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\") 100 Process >>> choices = [\"Atlanta Falcons\", \"New York Jets\", \"New York Giants\", \"Dallas Cowboys\"] >>> process.extract(\"new york jets\", choices, limit=2) [('New York Jets', 100), ('New York Giants', 78)] >>> process.extractOne(\"cowboys\", choices) (\"Dallas Cowboys\", 90) ||| [User 146792]: There is a module in the standard library (called difflib ) that can compare strings and return a score based on their similarity. The SequenceMatcher class should do what you want. Small example from Python prompt: >>> from difflib import SequenceMatcher as SM >>> s1 = ' It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.' >>> s2 = ' It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines.' >>> SM(None, s1, s2).ratio() 0.9112903225806451"
        ],
        [
         "25",
         "10850997",
         "How to train the Stanford Parser with Genia Corpus?",
         "https://stackoverflow.com/questions/10850997/how-to-train-the-stanford-parser-with-genia-corpus",
         null,
         null
        ],
        [
         "26",
         "870460",
         "Is there a good natural language processing library",
         "https://stackoverflow.com/questions/870460/is-there-a-good-natural-language-processing-library",
         null,
         null
        ],
        [
         "27",
         "4951751",
         "Creating a new corpus with NLTK",
         "https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk",
         null,
         null
        ],
        [
         "28",
         "15173225",
         "Calculate cosine similarity given 2 sentence strings",
         "https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings",
         "A simple pure-Python implementation would be: import math import re from collections import Counter WORD = re.compile(r\"\\w+\") def get_cosine(vec1, vec2): intersection = set(vec1.keys()) & set(vec2.keys()) numerator = sum([vec1[x] * vec2[x] for x in intersection]) sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())]) sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())]) denominator = math.sqrt(sum1) * math.sqrt(sum2) if not denominator: return 0.0 else: return float(numerator) / denominator def text_to_vector(text): words = WORD.findall(text) return Counter(words) text1 = \"This is a foo bar sentence .\" text2 = \"This sentence is similar to a foo bar sentence .\" vector1 = text_to_vector(text1) vector2 = text_to_vector(text2) cosine = get_cosine(vector1, vector2) print(\"Cosine:\", cosine) Prints: Cosine: 0.861640436855 The cosine formula used here is described here . This does not include weighting of the words by tf-idf, but in order to use tf-idf, you need to have a reasonably large corpus from which to estimate tfidf weights. You can also develop it further, by using a more sophisticated way to extract words from a piece of text, stem or lemmatise it, etc.",
         null
        ],
        [
         "29",
         "573768",
         "Sentiment analysis for Twitter in Python",
         "https://stackoverflow.com/questions/573768/sentiment-analysis-for-twitter-in-python",
         null,
         null
        ],
        [
         "30",
         "327513",
         "Fuzzy string search library in Java",
         "https://stackoverflow.com/questions/327513/fuzzy-string-search-library-in-java",
         null,
         null
        ],
        [
         "31",
         "17317418",
         "Stemmers vs Lemmatizers",
         "https://stackoverflow.com/questions/17317418/stemmers-vs-lemmatizers",
         "Q1: \"[..] are English stemmers any useful at all today? Since we have a plethora of lemmatization tools for English\" Yes. Stemmers are much simpler, smaller, and usually faster than lemmatizers, and for many applications, their results are good enough . Using a lemmatizer for that is a waste of resources. Consider, for example, dimensionality reduction in Information Retrieval. You replace all drive/driving with driv in both the searched documents and the query. You do not care if it is drive or driv or x17a$ as long as it clusters inflectionally related words together. Q2: \"[..]how should we move on to build robust lemmatizers that can take on nounify, verbify, adjectify, and adverbify preprocesses? What is your definition of a lemma, does it include derivation ( drive - driver ) or only inflection ( drive - drives - drove )? Does it take into account semantics? If you want to include derivation (which most people would say includes verbing nouns etc.) then keep in mind that derivation is far more irregular than inflection. There are many idiosyncracies, gaps, etc. Do you really want for to change ( change trains ) and change (as coins) to have the same lemma? If not, where do you draw the boundary? How about nerve - unnerve , earth -- unearth - earthling , ... It really depends on the application. If you take into account semantics ( bank would be labeled as bank-money or bank-river depending on context), how deep do you go (do you distinguish bank-institution from bank-building )? Some apps may not care about this at all, some might want to distinguish basic semantics, and some might want it fined-grained. Q3: \"How could the lemmatization task be easily scaled to other languages that have similar morphological structures as English?\" What do you mean by \"similar morphological structures as English\"? English has very little inflectional morphology. There are good lemmatizers for languages of other morphological types (truly inflectional, agglutinative, template, ...). With a possible exception of agglutinative languages, I would argue that a lookup table (say a compressed trie) is the best solution. (Possibly with some backup rules for unknown words such as proper names). The lookup is followed by some kind of disambiguation (ranging from trivial - take the first one, or take the first one consistent with the words POS tag, to much more sophisticated). The more sophisticated disambiguations are usually supervised stochastical algorithms (e.g. TreeTagger or Faster ), although a combination of machine learning and manually created rules has been done too (see e.g. this ). Obviously, for most languages, you do not want to create the lookup table by hand, but instead, generate it from a description of the morphology of that language. For inflectional languages, you can go the engineering way of Hajic for Czech or Mikheev for Russian, or, if you are daring, you use two-level morphology. Or you can do something in between, such as Hana (myself) (Note that these are all full morphological analyzers that include lemmatization as one of their features). Or you can learn the lemmatizer in an unsupervised manner a la Yarowsky and Wicentowski , possibly with manual post-processing, correcting the most frequent words. There are way too many options and it really all depends on what you want to do with the results.",
         null
        ],
        [
         "32",
         "19130512",
         "Stopword removal with NLTK",
         "https://stackoverflow.com/questions/19130512/stopword-removal-with-nltk",
         "I suggest you create your own list of operator words that you take out of the stopword list. Sets can be conveniently subtracted, so: operators = set(('and', 'or', 'not')) stop = set(stopwords...) - operators Then you can simply test if a word is in or not in the set without relying on whether your operators are part of the stopword list. You can then later switch to another stopword list or add an operator. if word.lower() not in stop: # use word",
         "[User 610569]: There is an in-built stopword list in NLTK made up of 2,400 stopwords for 11 languages (Porter et al), see http://nltk.org/book/ch02.html >>> from nltk import word_tokenize >>> from nltk.corpus import stopwords >>> stop = set(stopwords.words('english')) >>> sentence = \"this is a foo bar sentence\" >>> print([i for i in sentence.lower().split() if i not in stop]) ['foo', 'bar', 'sentence'] >>> [i for i in word_tokenize(sentence.lower()) if i not in stop] ['foo', 'bar', 'sentence'] I recommend looking at using tf-idf to remove stopwords, see Effects of Stemming on the term frequency?"
        ],
        [
         "33",
         "10554052",
         "What are the major differences and benefits of Porter and Lancaster Stemming algorithms?",
         "https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg",
         "At the very basics of it, the major difference between the porter and lancaster stemming algorithms is that the lancaster stemmer is significantly more aggressive than the porter stemmer. The three major stemming algorithms in use today are Porter, Snowball(Porter2), and Lancaster (Paice-Husk), with the aggressiveness continuum basically following along those same lines. Porter is the least aggressive algorithm, with the specifics of each algorithm actually being fairly lengthy and technical. Here is a break down for you though: Porter: Most commonly used stemmer without a doubt, also one of the most gentle stemmers. One of the few stemmers that actually has Java support which is a plus, though it is also the most computationally intensive of the algorithms(Granted not by a very significant margin). It is also the oldest stemming algorithm by a large margin. Porter2: Nearly universally regarded as an improvement over porter, and for good reason. Porter himself in fact admits that it is better than his original algorithm. Slightly faster computation time than porter, with a fairly large community around it. Lancaster: Very aggressive stemming algorithm, sometimes to a fault. With porter and snowball, the stemmed representations are usually fairly intuitive to a reader, not so with Lancaster, as many shorter words will become totally obfuscated. The fastest algorithm here, and will reduce your working set of words hugely, but if you want more distinction, not the tool you would want. Honestly, I feel that Snowball is usually the way to go. There are certain circumstances in which Lancaster will hugely trim down your working set, which can be very useful, however the marginal speed increase over snowball in my opinion is not worth the lack of precision. Porter has the most implementations though and so is usually the default go-to algorithm, but if you can, use snowball. Snowball - Additional info Snowball is a small string processing language designed for creating stemming algorithms for use in Information Retrieval. The Snowball compiler translates a Snowball script into another language - currently ISO C, C#, Go, Java, Javascript, Object Pascal, Python and Rust are supported. History of the name Since it effectively provides a ‘suffix STRIPPER GRAMmar’, I had toyed with the idea of calling it ‘strippergram’, but good sense has prevailed, and so it is ‘Snowball’ named as a tribute to SNOBOL, the excellent string handling language of Messrs Farber, Griswold, Poage and Polonsky from the 1960s. ---Martin Porter Stemmers implemented in the Snowball language are sometimes simply referred to as Snowball stemmers. For example, see the Natural Language Toolkit: nltk.stem.snowball .",
         null
        ],
        [
         "34",
         "40288323",
         "What do spaCy&#39;s part-of-speech and dependency tags mean?",
         "https://stackoverflow.com/questions/40288323/what-do-spacys-part-of-speech-and-dependency-tags-mean",
         null,
         "[User 1709587]: tl;dr answer Just expand the lists at: https://spacy.io/api/annotation#pos-tagging (POS tags) and https://spacy.io/api/annotation#dependency-parsing (dependency tags) Longer answer The docs have greatly improved since I first asked this question, and spaCy now documents this much better. Part-of-speech tags The pos and tag attributes are tabulated at https://spacy.io/api/annotation#pos-tagging , and the origin of those lists of values is described. At the time of this (January 2020) edit, the docs say of the pos attribute that: spaCy maps all language-specific part-of-speech tags to a small, fixed set of word type tags following the Universal Dependencies scheme . The universal tags don’t code for any morphological features and only cover the word type. They’re available as the Token.pos and Token.pos_ attributes. As for the tag attribute, the docs say: The English part-of-speech tagger uses the OntoNotes 5 version of the Penn Treebank tag set. We also map the tags to the simpler Universal Dependencies v2 POS tag set. and The German part-of-speech tagger uses the TIGER Treebank annotation scheme. We also map the tags to the simpler Universal Dependencies v2 POS tag set. You thus have a choice between using a coarse-grained tag set that is consistent across languages ( .pos ), or a fine-grained tag set ( .tag ) that is specific to a particular treebank, and hence a particular language. .pos_ tag list The docs list the following coarse-grained tags used for the pos and pos_ attributes: ADJ : adjective, e.g. big, old, green, incomprehensible, first ADP : adposition, e.g. in, to, during ADV : adverb, e.g. very, tomorrow, down, where, there AUX : auxiliary, e.g. is, has (done), will (do), should (do) CONJ : conjunction, e.g. and, or, but CCONJ : coordinating conjunction, e.g. and, or, but DET : determiner, e.g. a, an, the INTJ : interjection, e.g. psst, ouch, bravo, hello NOUN : noun, e.g. girl, cat, tree, air, beauty NUM : numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV PART : particle, e.g. ’s, not, PRON : pronoun, e.g I, you, he, she, myself, themselves, somebody PROPN : proper noun, e.g. Mary, John, London, NATO, HBO PUNCT : punctuation, e.g. ., (, ), ? SCONJ : subordinating conjunction, e.g. if, while, that SYM : symbol, e.g. $, %, §, ©, +, −, ×, ÷, =, :), 😝 VERB : verb, e.g. run, runs, running, eat, ate, eating X : other, e.g. sfpksdpsxmsa SPACE : space, e.g. Note that the docs are lying slightly when they say that this list follows the Universal Dependencies Scheme; there are two tags listed above that aren't part of that scheme. One of those is CONJ , which used to exist in the Universal POS Tags scheme but has been split into CCONJ and SCONJ since spaCy was first written. Based on the mappings of tag->pos in the docs, it would seem that spaCy's current models don't actually use CONJ , but it still exists in spaCy's code and docs for some reason - perhaps backwards compatibility with old models. The second is SPACE , which isn't part of the Universal POS Tags scheme (and never has been, as far as I know) and is used by spaCy for any spacing besides single normal ASCII spaces (which don't get their own token): >>> document = en_nlp(\"This\\nsentence\\thas some weird spaces in\\n\\n\\n\\n\\t\\t it.\") >>> for token in document: ... print('%r (%s)' % (str(token), token.pos_)) ... 'This' (DET) '\\n' (SPACE) 'sentence' (NOUN) '\\t' (SPACE) 'has' (VERB) ' ' (SPACE) 'some' (DET) 'weird' (ADJ) 'spaces' (NOUN) 'in' (ADP) '\\n\\n\\n\\n\\t\\t ' (SPACE) 'it' (PRON) '.' (PUNCT) I'll omit the full list of .tag_ tags (the finer-grained ones) from this answer, since they're numerous, well-documented now, different for English and German, and probably more likely to change between releases. Instead, look at the list in the docs (e.g. https://spacy.io/api/annotation#pos-en for English) which lists every possible tag, the .pos_ value it maps to, and a description of what it means. Dependency tokens There are now three different schemes that spaCy uses for dependency tagging: one for English , one for German , and one for everything else . Once again, the list of values is huge and I won't reproduce it in full here. Every dependency has a brief definition next to it, but unfortunately, many of them - like \"appositional modifier\" or \"clausal complement\" - are terms of art that are rather alien to an everyday programmer like me. If you're not a linguist, you'll simply have to research the meanings of those terms of art to make sense of them. I can at least provide a starting point for that research for people working with English text, though. If you'd like to see some examples of the CLEAR dependencies (used by the English model) in real sentences, check out the 2012 work of Jinho D. Choi: either his Optimization of Natural Language Processing Components for Robustness and Scalability or his Guidelines for the CLEAR Style Constituent to Dependency Conversion (which seems to just be a subsection of the former paper). Both list all the CLEAR dependency labels that existed in 2012 along with definitions and example sentences. (Unfortunately, the set of CLEAR dependency labels has changed a little since 2012, so some of the modern labels are not listed or exemplified in Choi's work - but it remains a useful resource despite being slightly outdated.) ||| [User 1061236]: Just a quick tip about getting the detail meaning of the short forms. You can use explain method like following: spacy.explain('pobj') which will give you output like: 'object of preposition'"
        ],
        [
         "35",
         "36952763",
         "How to return history of validation loss in Keras",
         "https://stackoverflow.com/questions/36952763/how-to-return-history-of-validation-loss-in-keras",
         null,
         "[User 5154673]: Just an example started from history = model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=0) You can use print(history.history.keys()) to list all data in history. Then, you can print the history of validation loss like this: print(history.history['val_loss'])"
        ],
        [
         "36",
         "6115677",
         "English grammar for parsing in NLTK",
         "https://stackoverflow.com/questions/6115677/english-grammar-for-parsing-in-nltk",
         null,
         null
        ],
        [
         "37",
         "526469",
         "Practical examples of NLTK use",
         "https://stackoverflow.com/questions/526469/practical-examples-of-nltk-use",
         null,
         null
        ],
        [
         "38",
         "26899235",
         "Python NLTK: SyntaxError: Non-ASCII character &#39;\\xc3&#39; in file (Sentiment Analysis -NLP)",
         "https://stackoverflow.com/questions/26899235/python-nltk-syntaxerror-non-ascii-character-xc3-in-file-sentiment-analysis",
         "Add the following to the top of your file # coding=utf-8 If you go to the link in the error you can seen the reason why: Defining the Encoding Python will default to ASCII as standard encoding if no other encoding hints are given. To define a source code encoding, a magic comment must be placed into the source files either as first or second line in the file, such as: # coding=",
         null
        ],
        [
         "39",
         "9706769",
         "Any tutorials for developing chatbots?",
         "https://stackoverflow.com/questions/9706769/any-tutorials-for-developing-chatbots",
         "You can read a nice introduction to various techniques used to design chatbots here: http://www.gamasutra.com/view/feature/6305/beyond_fa%C3%A7ade_pattern_matching_.php Also, here are a few useful links: http://web.archive.org/web/20120320060043/ http://ai-programming.com/bot_tutorial.htm http://www.alicebot.org/be.html http://en.wikipedia.org/wiki/List_of_chatterbots http://www.codeproject.com/Articles/36106/Chatbot-Tutorial http://www.slideshare.net/amyiris/ai-and-python-developing-a-conversational-interface-using-python The Natural Language Toolkit (python) implements a few chatbots: http://nltk.github.com/api/nltk.chat.html Simple pipeline architecture for a spoken dialogue system from the book Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit By Steven Bird, Ewan Klein, Edward Loper:",
         null
        ],
        [
         "40",
         "2452982",
         "How to extract common / significant phrases from a series of text entries",
         "https://stackoverflow.com/questions/2452982/how-to-extract-common-significant-phrases-from-a-series-of-text-entries",
         null,
         "[User 86542]: I suspect you don't just want the most common phrases, but rather you want the most interesting collocations . Otherwise, you could end up with an overrepresentation of phrases made up of common words and fewer interesting and informative phrases. To do this, you'll essentially want to extract n-grams from your data and then find the ones that have the highest point wise mutual information (PMI). That is, you want to find the words that co-occur together much more than you would expect them to by chance. The NLTK collocations how-to covers how to do this in a about 7 lines of code, e.g.: import nltk from nltk.collocations import * bigram_measures = nltk.collocations.BigramAssocMeasures() trigram_measures = nltk.collocations.TrigramAssocMeasures() # change this to read in your data finder = BigramCollocationFinder.from_words( nltk.corpus.genesis.words('english-web.txt')) # only bigrams that appear 3+ times finder.apply_freq_filter(3) # return the 10 n-grams with the highest PMI finder.nbest(bigram_measures.pmi, 10)"
        ],
        [
         "41",
         "37889914",
         "What is a projection layer in the context of neural networks?",
         "https://stackoverflow.com/questions/37889914/what-is-a-projection-layer-in-the-context-of-neural-networks",
         "I find the previous answers here a bit overcomplicated - a projection layer is just a simple matrix multiplication, or in the context of NN, a regular/dense/linear layer, without the non-linear activation in the end (sigmoid/tanh/relu/etc.) The idea is to project the (e.g.) 100K-dimensions discrete vector into a 600-dimensions continuous vector (I chose the numbers here randomly, \"your mileage may vary\"). The exact matrix parameters are learned through the training process. What happens before/after already depends on the model and context, and is not what OP asks. (In practice you wouldn't even bother with the matrix multiplication (as you are multiplying a 1-hot vector which has 1 for the word index and 0's everywhere else), and would treat the trained matrix as a lookout table (i.e. the 6257th word in the corpus = the 6257th row/column (depends how you define it) in the projection matrix).)",
         null
        ],
        [
         "42",
         "3763640",
         "Where can I learn more about the Google search &quot;did you mean&quot; algorithm?",
         "https://stackoverflow.com/questions/3763640/where-can-i-learn-more-about-the-google-search-did-you-mean-algorithm",
         null,
         "[User 329769]: You should check out Peter Norvigs article about implementing the spell checker in a few lines of python: How to Write a Spelling Corrector It also has links for implementations in other languages (i.e. C#)"
        ],
        [
         "43",
         "41170726",
         "Add/remove custom stop words with spacy",
         "https://stackoverflow.com/questions/41170726/add-remove-custom-stop-words-with-spacy",
         "You can edit them before processing your text like this (see this post ): >>> import spacy >>> nlp = spacy.load(\"en\") >>> nlp.vocab[\"the\"].is_stop = False >>> nlp.vocab[\"definitelynotastopword\"].is_stop = True >>> sentence = nlp(\"the word is definitelynotastopword\") >>> sentence[0].is_stop False >>> sentence[3].is_stop True Note: This seems to work <=v1.8. For newer versions, see other answers.",
         "[User 4384857]: Using Spacy 2.0.11, you can update its stopwords set using one of the following: To add a single stopword: import spacy nlp = spacy.load(\"en\") nlp.Defaults.stop_words.add(\"my_new_stopword\") To add several stopwords at once: import spacy nlp = spacy.load(\"en\") nlp.Defaults.stop_words |= {\"my_new_stopword1\",\"my_new_stopword2\",} To remove a single stopword: import spacy nlp = spacy.load(\"en\") nlp.Defaults.stop_words.remove(\"whatever\") To remove several stopwords at once: import spacy nlp = spacy.load(\"en\") nlp.Defaults.stop_words -= {\"whatever\", \"whenever\"} Note: To see the current set of stopwords, use: print(nlp.Defaults.stop_words) Update : It was noted in the comments that this fix only affects the current execution. To update the model, you can use the methods nlp.to_disk(\"/path\") and nlp.from_disk(\"/path\") (further described at https://spacy.io/usage/saving-loading )."
        ],
        [
         "44",
         "10252448",
         "How to check whether a sentence is correct (simple grammar check in Python)?",
         "https://stackoverflow.com/questions/10252448/how-to-check-whether-a-sentence-is-correct-simple-grammar-check-in-python",
         null,
         "[User 1047788]: There are various Web Services providing automated proofreading and grammar checking. Some have a Python library to simplify querying. As far as I can tell, most of those tools (certainly After the Deadline and LanguageTool) are rule based. The checked text is compared with a large set of rules describing common errors. If a rule matches, the software calls it an error. If a rule does not match, the software does nothing (it cannot detect errors it does not have rules for). After the Deadline import ATD ATD.setDefaultKey(\"your API key\") errors = ATD.checkDocument(\"Looking too the water. Fixing your writing typoss.\") for error in errors: print \"%s error for: %s **%s**\" % (error.type, error.precontext, error.string) print \"some suggestions: %s\" % (\", \".join(error.suggestions),) Expected output: grammar error for: Looking **too the** some suggestions: to the spelling error for: writing **typoss** some suggestions: typos It is possible to run the server application on your own machine, 4 GB RAM is recommended. LanguageTool https://pypi.python.org/pypi/language-check >>> import language_check >>> tool = language_check.LanguageTool('en-US') >>> text = 'A sentence with a error in the Hitchhiker’s Guide tot he Galaxy' >>> matches = tool.check(text) >>> matches[0].fromy, matches[0].fromx (0, 16) >>> matches[0].ruleId, matches[0].replacements ('EN_A_VS_AN', ['an']) >>> matches[1].fromy, matches[1].fromx (0, 50) >>> matches[1].ruleId, matches[1].replacements ('TOT_HE', ['to the']) >>> print(matches[1]) Line 1, column 51, Rule ID: TOT_HE[1] Message: Did you mean 'to the'? Suggestion: to the ... >>> language_check.correct(text, matches) 'A sentence with an error in the Hitchhiker’s Guide to the Galaxy' It is also possible to run the server side privately. Ginger Additionally, this is a hacky (screen scraping) library for Ginger, arguably one of the most polished free-to-use grammar checking options out there. Microsoft Word It should be possible to script Microsoft Word and use its grammar checking functionality. More There is a curated list of grammar checkers on Open Office website . Noted in comments by Patrick."
        ],
        [
         "45",
         "8772692",
         "Semantic search with NLP and elasticsearch",
         "https://stackoverflow.com/questions/8772692/semantic-search-with-nlp-and-elasticsearch",
         "There may be several approaches with different implementation complexity. The easiest one is to create list of topics (like plumbing), attach bag of words (like \"pipe\"), identify search request by majority of keywords and search only in specified topic (you can add field topic to your elastic search documents and set it as mandatory with + during search). Of course, if you have lots of documents, manual creation of topic list and bag of words is very time expensive. You can use machine learning to automate some of tasks. Basically, it is enough to have distance measure between words and/or documents to automatically discover topics (e.g. by data clustering ) and classify query to one of these topics. Mix of these techniques may also be a good choice (for example, you can manually create topics and assign initial documents to them, but use classification for query assignment). Take a look at Wikipedia's article on latent semantic analysis to better understand the idea. Also pay attention to the 2 linked articles on data clustering and document classification . And yes, Maui Indexer may become good helper tool this way. Finally, you can try to build an engine that \"understands\" meaning of the phrase (not just uses terms frequency) and searches appropriate topics. Most probably, this will involve natural language processing and ontology-based knowledgebases . But in fact, this field is still in active research and without previous experience it will be very hard for you to implement something like this.",
         null
        ],
        [
         "46",
         "15057945",
         "How do I tokenize a string sentence in NLTK?",
         "https://stackoverflow.com/questions/15057945/how-do-i-tokenize-a-string-sentence-in-nltk",
         "This is actually on the main page of nltk.org : >>> import nltk >>> sentence = \"\"\"At eight o'clock on Thursday morning ... Arthur didn't feel very good.\"\"\" >>> tokens = nltk.word_tokenize(sentence) >>> tokens ['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']",
         null
        ],
        [
         "47",
         "293000",
         "Algorithm to determine how positive or negative a statement/text is",
         "https://stackoverflow.com/questions/293000/algorithm-to-determine-how-positive-or-negative-a-statement-text-is",
         "There is a sub-field of natural language processing called sentiment analysis that deals specifically with this problem domain. There is a fair amount of commercial work done in the area because consumer products are so heavily reviewed in online user forums (ugc or user-generated-content). There is also a prototype platform for text analytics called GATE from the university of sheffield, and a python project called nltk . Both are considered flexible, but not very high performance. One or the other might be good for working out your own ideas.",
         null
        ],
        [
         "48",
         "27416164",
         "What is CoNLL data format?",
         "https://stackoverflow.com/questions/27416164/what-is-conll-data-format",
         "There are many different CoNLL formats since CoNLL is a different shared task each year. The format for CoNLL 2009 is described here . Each line represents a single word with a series of tab-separated fields. _ s indicate empty values. Mate-Parser's manual says that it uses the first 12 columns of CoNLL 2009: ID FORM LEMMA PLEMMA POS PPOS FEAT PFEAT HEAD PHEAD DEPREL PDEPREL The definition of some of these columns come from earlier shared tasks (the CoNLL-X format used in 2006 and 2007): ID (index in sentence, starting at 1) FORM (word form itself) LEMMA (word's lemma or stem) POS (part of speech) FEAT (list of morphological features separated by |) HEAD (index of syntactic parent, 0 for ROOT ) DEPREL (syntactic relationship between HEAD and this word) There are variants of those columns (e.g., PPOS but not POS ) that start with P indicate that the value was automatically predicted rather a gold standard value. Update: There is now a CoNLL-U data format as well which extends the CoNLL-X format.",
         null
        ],
        [
         "49",
         "999410",
         "Natural Language Processing in Ruby",
         "https://stackoverflow.com/questions/999410/natural-language-processing-in-ruby",
         null,
         "[User 1062679]: Three excellent and mature NLP packages are Stanford Core NLP , Open NLP and LingPipe . There are Ruby bindings to the Stanford Core NLP tools (GPL license) as well as the OpenNLP tools (Apache License). On the more experimental side of things, I maintain a Text Retrieval, Extraction and Annotation Toolkit (Treat), released under the GPL, that provides a common API for almost every NLP-related gem that exists for Ruby. The following list of Treat's features can also serve as a good reference in terms of stable natural language processing gems compatible with Ruby 1.9. Text segmenters and tokenizers ( punkt-segmenter , tactful_tokenizer , srx-english , scalpel ) Natural language parsers for English, French and German and named entity extraction for English ( stanford-core-nlp ). Word inflection and conjugation ( linguistics ), stemming ( ruby-stemmer , uea-stemmer , lingua , etc.) WordNet interface ( rwordnet ), POS taggers ( rbtagger , engtagger , etc.) Language ( whatlanguage ), date/time ( chronic , kronic , nickel ), keyword ( lda-ruby ) extraction. Text retrieval with indexation and full-text search ( ferret ). Named entity extraction ( stanford-core-nlp ). Basic machine learning with decision trees ( decisiontree ), MLPs ( ruby-fann ), SVMs ( rb-libsvm ) and linear classification ( tomz-liblinear-ruby-swig ). Text similarity metrics ( levenshtein-ffi , fuzzy-string-match , tf-idf-similarity ). Not included in Treat, but relevant to NLP: hotwater (string distance algorithms), yomu (binders to Apache Tiki for reading .doc, .docx, .pages, .odt, .rtf, .pdf), graph-rank (an implementation of GraphRank)."
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 98
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>accepted_answers</th>\n",
       "      <th>other_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307291</td>\n",
       "      <td>How does the Google &amp;quot;Did you mean?&amp;quot; ...</td>\n",
       "      <td>https://stackoverflow.com/questions/307291/how...</td>\n",
       "      <td>Here's the explanation directly from the sourc...</td>\n",
       "      <td>[User 28582]: I found this article some time a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8897593</td>\n",
       "      <td>How to compute the similarity between two text...</td>\n",
       "      <td>https://stackoverflow.com/questions/8897593/ho...</td>\n",
       "      <td>The common way of doing this is to transform t...</td>\n",
       "      <td>[User 125617]: Identical to @larsman, but with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52455774</td>\n",
       "      <td>googletrans stopped working with error &amp;#39;No...</td>\n",
       "      <td>https://stackoverflow.com/questions/52455774/g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[User 10667419]: Update 06.12.20: A new 'offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1787110</td>\n",
       "      <td>What is the difference between lemmatization v...</td>\n",
       "      <td>https://stackoverflow.com/questions/1787110/wh...</td>\n",
       "      <td>Short and dense: http://nlp.stanford.edu/IR-bo...</td>\n",
       "      <td>[User 327862]: Lemmatisation is closely relate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39142778</td>\n",
       "      <td>How to determine the language of a piece of text?</td>\n",
       "      <td>https://stackoverflow.com/questions/39142778/h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[User 3266110]: 1. TextBlob . (Deprecated - Us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>35716121</td>\n",
       "      <td>How to extract phrases from corpus using gensim</td>\n",
       "      <td>https://stackoverflow.com/questions/35716121/h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[User 5702489]: I got the solution for the pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>35275001</td>\n",
       "      <td>Use of PunktSentenceTokenizer in NLTK</td>\n",
       "      <td>https://stackoverflow.com/questions/35275001/u...</td>\n",
       "      <td>PunktSentenceTokenizer is the abstract class f...</td>\n",
       "      <td>[User 3450064]: PunktSentenceTokenizer is an s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>212219</td>\n",
       "      <td>What are good starting points for someone inte...</td>\n",
       "      <td>https://stackoverflow.com/questions/212219/wha...</td>\n",
       "      <td>Tough call, NLP is a much wider field than mos...</td>\n",
       "      <td>[User unknown]: Chomsky is totally the wrong s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>42821330</td>\n",
       "      <td>Restore original text from Keras’s imdb dataset</td>\n",
       "      <td>https://stackoverflow.com/questions/42821330/r...</td>\n",
       "      <td>Your example is coming out as gibberish, it's ...</td>\n",
       "      <td>[User 2864966]: You can get the original datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>25534214</td>\n",
       "      <td>NLTK WordNet Lemmatizer: Shouldn&amp;#39;t it lemm...</td>\n",
       "      <td>https://stackoverflow.com/questions/25534214/n...</td>\n",
       "      <td>The WordNet lemmatizer does take the POS tag i...</td>\n",
       "      <td>[User 1607029]: The best way to troubleshoot t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id                                              title  \\\n",
       "0        307291  How does the Google &quot;Did you mean?&quot; ...   \n",
       "1       8897593  How to compute the similarity between two text...   \n",
       "2      52455774  googletrans stopped working with error &#39;No...   \n",
       "3       1787110  What is the difference between lemmatization v...   \n",
       "4      39142778  How to determine the language of a piece of text?   \n",
       "..          ...                                                ...   \n",
       "93     35716121    How to extract phrases from corpus using gensim   \n",
       "94     35275001              Use of PunktSentenceTokenizer in NLTK   \n",
       "95       212219  What are good starting points for someone inte...   \n",
       "96     42821330    Restore original text from Keras’s imdb dataset   \n",
       "97     25534214  NLTK WordNet Lemmatizer: Shouldn&#39;t it lemm...   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://stackoverflow.com/questions/307291/how...   \n",
       "1   https://stackoverflow.com/questions/8897593/ho...   \n",
       "2   https://stackoverflow.com/questions/52455774/g...   \n",
       "3   https://stackoverflow.com/questions/1787110/wh...   \n",
       "4   https://stackoverflow.com/questions/39142778/h...   \n",
       "..                                                ...   \n",
       "93  https://stackoverflow.com/questions/35716121/h...   \n",
       "94  https://stackoverflow.com/questions/35275001/u...   \n",
       "95  https://stackoverflow.com/questions/212219/wha...   \n",
       "96  https://stackoverflow.com/questions/42821330/r...   \n",
       "97  https://stackoverflow.com/questions/25534214/n...   \n",
       "\n",
       "                                     accepted_answers  \\\n",
       "0   Here's the explanation directly from the sourc...   \n",
       "1   The common way of doing this is to transform t...   \n",
       "2                                                 NaN   \n",
       "3   Short and dense: http://nlp.stanford.edu/IR-bo...   \n",
       "4                                                 NaN   \n",
       "..                                                ...   \n",
       "93                                                NaN   \n",
       "94  PunktSentenceTokenizer is the abstract class f...   \n",
       "95  Tough call, NLP is a much wider field than mos...   \n",
       "96  Your example is coming out as gibberish, it's ...   \n",
       "97  The WordNet lemmatizer does take the POS tag i...   \n",
       "\n",
       "                                        other_answers  \n",
       "0   [User 28582]: I found this article some time a...  \n",
       "1   [User 125617]: Identical to @larsman, but with...  \n",
       "2   [User 10667419]: Update 06.12.20: A new 'offic...  \n",
       "3   [User 327862]: Lemmatisation is closely relate...  \n",
       "4   [User 3266110]: 1. TextBlob . (Deprecated - Us...  \n",
       "..                                                ...  \n",
       "93  [User 5702489]: I got the solution for the pro...  \n",
       "94  [User 3450064]: PunktSentenceTokenizer is an s...  \n",
       "95  [User unknown]: Chomsky is totally the wrong s...  \n",
       "96  [User 2864966]: You can get the original datas...  \n",
       "97  [User 1607029]: The best way to troubleshoot t...  \n",
       "\n",
       "[98 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
