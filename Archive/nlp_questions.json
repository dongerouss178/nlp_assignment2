[
  {
    "question_id": 307291,
    "title": "How does the Google &quot;Did you mean?&quot; Algorithm work?",
    "body": "<p>I've been developing an internal website for a portfolio management tool.  There is a lot of text data, company names etc.  I've been really impressed with some search engines ability to very quickly respond to queries with \"Did you mean: xxxx\".</p>\n\n<p>I need to be able to intelligently take a user query and respond with not only raw search results but also with a \"Did you mean?\" response when there is a highly likely alternative answer etc</p>\n\n<p>[I'm developing in <a href=\"http://en.wikipedia.org/wiki/ASP.NET\" rel=\"noreferrer\">ASP.NET</a> (VB - don't hold it against me! )]</p>\n\n<p>UPDATE:\nOK, how can I mimic this without the millions of 'unpaid users'?</p>\n\n<ul>\n<li>Generate typos for each 'known' or 'correct' term and perform lookups?</li>\n<li>Some other more elegant method?</li>\n</ul>\n",
    "score": 469,
    "creation_date": 1227224091,
    "view_count": 104046,
    "answer_count": 18,
    "tags": "algorithm;machine-learning;nlp;spell-checking;text-search"
  },
  {
    "question_id": 8897593,
    "title": "How to compute the similarity between two text documents?",
    "body": "<p>I want to take two documents and determine how similar they are. Any programming language if fine but I prefer Python.</p>\n",
    "score": 287,
    "creation_date": 1326815469,
    "view_count": 315888,
    "answer_count": 14,
    "tags": "python;nlp"
  },
  {
    "question_id": 52455774,
    "title": "googletrans stopped working with error &#39;NoneType&#39; object has no attribute &#39;group&#39;",
    "body": "<p>I was trying <code>googletrans</code> and it was working quite well. Since this morning I started getting below error. I went through multiple posts from stackoverflow and other sites and found probably my ip is banned to use the service for sometime. I tried using multiple service provider internet that has different ip and stil facing the same issue ? I also tried to use <code>googletrans</code> on different laptops , still same issue ..Is <code>googletrans</code> package broken or something google did at their end ?</p>\n\n<pre><code>&gt;&gt;&gt; from googletrans import Translator\n&gt;&gt;&gt; translator = Translator()\n&gt;&gt;&gt; translator.translate('안녕하세요.')\n\nTraceback (most recent call last):\n  File \"&lt;pyshell#2&gt;\", line 1, in &lt;module&gt;\n    translator.translate('안녕하세요.')\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/client.py\", line 172, in translate\n    data = self._translate(text, dest, src)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/client.py\", line 75, in _translate\n    token = self.token_acquirer.do(text)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/gtoken.py\", line 180, in do\n    self._update()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/gtoken.py\", line 59, in _update\n    code = unicode(self.RE_TKK.search(r.text).group(1)).replace('var ', '')\nAttributeError: 'NoneType' object has no attribute 'group'\n</code></pre>\n",
    "score": 224,
    "creation_date": 1537612148,
    "view_count": 233749,
    "answer_count": 18,
    "tags": "python;nlp;google-translate;googletrans"
  },
  {
    "question_id": 1787110,
    "title": "What is the difference between lemmatization vs stemming?",
    "body": "<p>When do I use each ?</p>\n\n<p>Also...is the NLTK lemmatization dependent upon Parts of Speech?\nWouldn't it be more accurate if it was?</p>\n",
    "score": 207,
    "creation_date": 1259023691,
    "view_count": 168981,
    "answer_count": 15,
    "tags": "nlp;nltk;lemmatization"
  },
  {
    "question_id": 39142778,
    "title": "How to determine the language of a piece of text?",
    "body": "<p>I want to get this:</p>\n<pre class=\"lang-none prettyprint-override\"><code>Input text: &quot;ру́сский язы́к&quot;\nOutput text: &quot;Russian&quot; \n\nInput text: &quot;中文&quot;\nOutput text: &quot;Chinese&quot; \n\nInput text: &quot;にほんご&quot;\nOutput text: &quot;Japanese&quot; \n\nInput text: &quot;العَرَبِيَّة&quot;\nOutput text: &quot;Arabic&quot;\n</code></pre>\n<p>How can I do it in python?</p>\n",
    "score": 204,
    "creation_date": 1472120760,
    "view_count": 195526,
    "answer_count": 17,
    "tags": "python;nlp"
  },
  {
    "question_id": 1833252,
    "title": "Java Stanford NLP: Part of Speech labels?",
    "body": "<p>The Stanford NLP, demo'd <a href=\"http://nlp.stanford.edu:8080/parser/\" rel=\"noreferrer\">here</a>, gives an output like this:</p>\n\n<pre><code>Colorless/JJ green/JJ ideas/NNS sleep/VBP furiously/RB ./.\n</code></pre>\n\n<p>What do the Part of Speech tags mean? I am unable to find an official list. Is it Stanford's own system, or are they using universal tags? (What is <code>JJ</code>, for instance?)</p>\n\n<p>Also, when I am iterating through the sentences, looking for nouns, for instance, I end up doing something like checking to see if the tag <code>.contains('N')</code>. This feels pretty weak. Is there a better way to programmatically search for a certain part of speech?</p>\n",
    "score": 186,
    "creation_date": 1259764250,
    "view_count": 105406,
    "answer_count": 9,
    "tags": "java;nlp;stanford-nlp;part-of-speech"
  },
  {
    "question_id": 54334304,
    "title": "spaCy: Can&#39;t find model &#39;en_core_web_sm&#39; on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)",
    "body": "<p>What is the difference between <code>spacy.load('en_core_web_sm')</code> and <code>spacy.load('en')</code>? <a href=\"https://stackoverflow.com/questions/50487495/what-is-difference-between-en-core-web-sm-en-core-web-mdand-en-core-web-lg-mod\">This link</a> explains different model sizes. But I am still not clear how <code>spacy.load('en_core_web_sm')</code> and <code>spacy.load('en')</code> differ</p>\n<p><code>spacy.load('en')</code> runs fine for me. But the <code>spacy.load('en_core_web_sm')</code> throws error</p>\n<p>I have installed <code>spacy</code>as below. when I go to Jupyter notebook and run command <code>nlp = spacy.load('en_core_web_sm')</code> I get the below error</p>\n<pre><code>---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\n&lt;ipython-input-4-b472bef03043&gt; in &lt;module&gt;()\n      1 # Import spaCy and load the language library\n      2 import spacy\n----&gt; 3 nlp = spacy.load('en_core_web_sm')\n      4 \n      5 # Create a Doc object\n\nC:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\spacy\\__init__.py in load(name, **overrides)\n     13     if depr_path not in (True, False, None):\n     14         deprecation_warning(Warnings.W001.format(path=depr_path))\n---&gt; 15     return util.load_model(name, **overrides)\n     16 \n     17 \n\nC:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\spacy\\util.py in load_model(name, **overrides)\n    117     elif hasattr(name, 'exists'):  # Path or Path-like to model data\n    118         return load_model_from_path(name, **overrides)\n--&gt; 119     raise IOError(Errors.E050.format(name=name))\n    120 \n    121 \n\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n</code></pre>\n<p>how I installed Spacy ---</p>\n<pre><code>(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz&gt;conda install -c conda-forge spacy\nFetching package metadata .............\nSolving package specifications: .\n\nPackage plan for installation in environment C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder:\n\nThe following NEW packages will be INSTALLED:\n\n    blas:           1.0-mkl\n    cymem:          1.31.2-py35h6538335_0    conda-forge\n    dill:           0.2.8.2-py35_0           conda-forge\n    msgpack-numpy:  0.4.4.2-py_0             conda-forge\n    murmurhash:     0.28.0-py35h6538335_1000 conda-forge\n    plac:           0.9.6-py_1               conda-forge\n    preshed:        1.0.0-py35h6538335_0     conda-forge\n    pyreadline:     2.1-py35_1000            conda-forge\n    regex:          2017.11.09-py35_0        conda-forge\n    spacy:          2.0.12-py35h830ac7b_0    conda-forge\n    termcolor:      1.1.0-py_2               conda-forge\n    thinc:          6.10.3-py35h830ac7b_2    conda-forge\n    tqdm:           4.29.1-py_0              conda-forge\n    ujson:          1.35-py35hfa6e2cd_1001   conda-forge\n\nThe following packages will be UPDATED:\n\n    msgpack-python: 0.4.8-py35_0                         --&gt; 0.5.6-py35he980bc4_3 conda-forge\n\nThe following packages will be DOWNGRADED:\n\n    freetype:       2.7-vc14_2               conda-forge --&gt; 2.5.5-vc14_2\n\nProceed ([y]/n)? y\n\nblas-1.0-mkl.t 100% |###############################| Time: 0:00:00   0.00  B/s\ncymem-1.31.2-p 100% |###############################| Time: 0:00:00   1.65 MB/s\nmsgpack-python 100% |###############################| Time: 0:00:00   5.37 MB/s\nmurmurhash-0.2 100% |###############################| Time: 0:00:00   1.49 MB/s\nplac-0.9.6-py_ 100% |###############################| Time: 0:00:00   0.00  B/s\npyreadline-2.1 100% |###############################| Time: 0:00:00   4.62 MB/s\nregex-2017.11. 100% |###############################| Time: 0:00:00   3.31 MB/s\ntermcolor-1.1. 100% |###############################| Time: 0:00:00 187.81 kB/s\ntqdm-4.29.1-py 100% |###############################| Time: 0:00:00   2.51 MB/s\nujson-1.35-py3 100% |###############################| Time: 0:00:00   1.66 MB/s\ndill-0.2.8.2-p 100% |###############################| Time: 0:00:00   4.34 MB/s\nmsgpack-numpy- 100% |###############################| Time: 0:00:00   0.00  B/s\npreshed-1.0.0- 100% |###############################| Time: 0:00:00   0.00  B/s\nthinc-6.10.3-p 100% |###############################| Time: 0:00:00   5.49 MB/s\nspacy-2.0.12-p 100% |###############################| Time: 0:00:10   7.42 MB/s\n\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz&gt;python -V\nPython 3.5.3 :: Anaconda custom (64-bit)\n\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz&gt;python -m spacy download en\nCollecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n    100% |################################| 37.4MB ...\nInstalling collected packages: en-core-web-sm\n  Running setup.py install for en-core-web-sm ... done\nSuccessfully installed en-core-web-sm-2.0.0\n\n    Linking successful\n    C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\en_core_web_sm\n    --&gt;\n    C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\spacy\\data\\en\n\n    You can now load the model via spacy.load('en')\n\n\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz&gt;\n</code></pre>\n",
    "score": 177,
    "creation_date": 1548271461,
    "view_count": 470156,
    "answer_count": 34,
    "tags": "python;python-3.x;nlp;spacy"
  },
  {
    "question_id": 34870614,
    "title": "What does tf.nn.embedding_lookup function do?",
    "body": "<pre><code>tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None)\n</code></pre>\n\n<p>I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)?</p>\n\n<p>For instance, in the <code>skip-gram</code> model if we use <code>tf.nn.embedding_lookup(embeddings, train_inputs)</code>, then for each <code>train_input</code> it finds the correspond embedding?</p>\n",
    "score": 174,
    "creation_date": 1453187680,
    "view_count": 79174,
    "answer_count": 9,
    "tags": "python;tensorflow;deep-learning;word-embedding;nlp"
  },
  {
    "question_id": 15547409,
    "title": "How to get rid of punctuation using NLTK tokenizer?",
    "body": "<p>I'm just starting to use NLTK and I don't quite understand how to get a list of words from text. If I use <code>nltk.word_tokenize()</code>, I get a list of words and punctuation. I need only the words instead. How can I get rid of punctuation? Also <code>word_tokenize</code> doesn't work with multiple sentences: dots are added to the last word.</p>\n",
    "score": 168,
    "creation_date": 1363868527,
    "view_count": 309516,
    "answer_count": 13,
    "tags": "python;nlp;tokenize;nltk"
  },
  {
    "question_id": 405161,
    "title": "Detecting syllables in a word",
    "body": "<p>I need to find a fairly efficient  way to detect syllables in a word. E.g.,</p>\n\n<p>Invisible -> in-vi-sib-le</p>\n\n<p>There are some syllabification rules that could be used:</p>\n\n<p>V\nCV\nVC\nCVC\nCCV\nCCCV\nCVCC</p>\n\n<p>*where V is a vowel and C is a consonant.\nE.g., </p>\n\n<p>Pronunciation (5 Pro-nun-ci-a-tion; CV-CVC-CV-V-CVC)</p>\n\n<p>I've tried few methods, among which were using regex (which helps only if you want to count syllables) or hard coded rule definition (a brute force approach which proves to be very inefficient) and finally using a finite state automata (which did not result with anything useful).</p>\n\n<p>The purpose of my application is to create a dictionary of all syllables in a given language. This dictionary will later be used for spell checking applications (using Bayesian classifiers) and text to speech synthesis. </p>\n\n<p>I would appreciate if one could give me tips on an alternate way to solve this problem besides my previous approaches. </p>\n\n<p>I work in Java, but any tip in C/C++, C#, Python, Perl... would work for me.</p>\n",
    "score": 156,
    "creation_date": 1230829721,
    "view_count": 84724,
    "answer_count": 17,
    "tags": "nlp;spell-checking;hyphenation"
  },
  {
    "question_id": 31421413,
    "title": "How to compute precision, recall, accuracy and f1-score for the multiclass case with scikit learn?",
    "body": "<p>I'm working in a sentiment analysis problem the data looks like this:</p>\n<pre><code>label instances\n    5    1190\n    4     838\n    3     239\n    1     204\n    2     127\n</code></pre>\n<p>So my data is unbalanced since 1190 <code>instances</code> are labeled with <code>5</code>. For the classification Im using scikit's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\" rel=\"nofollow noreferrer\">SVC</a>. The problem is I do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. So I tried the following approaches:</p>\n<p>First:</p>\n<pre><code>wclf = SVC(kernel='linear', C= 1, class_weight={1: 10})\nwclf.fit(X, y)\nweighted_prediction = wclf.predict(X_test)\n\nprint 'Accuracy:', accuracy_score(y_test, weighted_prediction)\nprint 'F1 score:', f1_score(y_test, weighted_prediction,average='weighted')\nprint 'Recall:', recall_score(y_test, weighted_prediction,\n                              average='weighted')\nprint 'Precision:', precision_score(y_test, weighted_prediction,\n                                    average='weighted')\nprint '\\n clasification report:\\n', classification_report(y_test, weighted_prediction)\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, weighted_prediction)\n</code></pre>\n<p>Second:</p>\n<pre><code>auto_wclf = SVC(kernel='linear', C= 1, class_weight='auto')\nauto_wclf.fit(X, y)\nauto_weighted_prediction = auto_wclf.predict(X_test)\n\nprint 'Accuracy:', accuracy_score(y_test, auto_weighted_prediction)\n\nprint 'F1 score:', f1_score(y_test, auto_weighted_prediction,\n                            average='weighted')\n\nprint 'Recall:', recall_score(y_test, auto_weighted_prediction,\n                              average='weighted')\n\nprint 'Precision:', precision_score(y_test, auto_weighted_prediction,\n                                    average='weighted')\n\nprint '\\n clasification report:\\n', classification_report(y_test,auto_weighted_prediction)\n\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, auto_weighted_prediction)\n</code></pre>\n<p>Third:</p>\n<pre><code>clf = SVC(kernel='linear', C= 1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n\n\nfrom sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\n\nprint 'Accuracy:', accuracy_score(y_test, prediction)\nprint 'F1 score:', f1_score(y_test, prediction)\nprint 'Recall:', recall_score(y_test, prediction)\nprint 'Precision:', precision_score(y_test, prediction)\nprint '\\n clasification report:\\n', classification_report(y_test,prediction)\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, prediction)\n\n\nF1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.\n  sample_weight=sample_weight)\n/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.\n  sample_weight=sample_weight)\n/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1082: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.\n  sample_weight=sample_weight)\n 0.930416613529\n</code></pre>\n<p>However, Im getting warnings like this:</p>\n<pre><code>/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172:\nDeprecationWarning: The default `weighted` averaging is deprecated,\nand from version 0.18, use of precision, recall or F-score with \nmulticlass or multilabel data or pos_label=None will result in an \nexception. Please set an explicit value for `average`, one of (None, \n'micro', 'macro', 'weighted', 'samples'). In cross validation use, for \ninstance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;\n</code></pre>\n<p>How can I deal correctly with my unbalanced data in order to compute in the right way classifier's metrics?</p>\n",
    "score": 147,
    "creation_date": 1436933856,
    "view_count": 333145,
    "answer_count": 4,
    "tags": "python;machine-learning;scikit-learn;nlp"
  },
  {
    "question_id": 27697766,
    "title": "Understanding min_df and max_df in scikit CountVectorizer",
    "body": "<p>I have five text files that I input to a CountVectorizer. When specifying <code>min_df</code> and <code>max_df</code> to the CountVectorizer instance what does the min/max document frequency exactly mean? Is it the frequency of a word in its particular text file or is it the frequency of the word in the entire overall corpus (five text files)?</p>\n<p>What are the differences when <code>min_df</code> and <code>max_df</code> are provided as integers or as floats?</p>\n<p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn-feature-extraction-text-countvectorizer\" rel=\"noreferrer\">The documentation</a> doesn't seem to provide a thorough explanation nor does it supply an example to demonstrate the use of these two parameters. Could someone provide an explanation or example demonstrating <code>min_df</code> and <code>max_df</code>?</p>\n",
    "score": 133,
    "creation_date": 1419897433,
    "view_count": 113227,
    "answer_count": 6,
    "tags": "python;machine-learning;scikit-learn;nlp"
  },
  {
    "question_id": 9294926,
    "title": "How does Apple find dates, times and addresses in emails?",
    "body": "<p>In the iOS email client, when an email contains a date, time or location, the text becomes a hyperlink and it is possible to create an appointment or look at a map simply by tapping the link. It not only works for emails in English, but in other languages also. I love this feature and would like to understand how they do it. </p>\n\n<p>The naive way to do this would be to have many regular expressions and run them all. However I  this is not going to scale very well and will work for only a specific language or date format, etc. I think that Apple must be using some concept of machine learning to extract entities (8:00PM, 8PM, 8:00, 0800, 20:00, 20h, 20h00, 2000 etc.).</p>\n\n<p>Any idea how Apple is able to extract entities so quickly in its email client? What machine learning algorithm would you to apply accomplish such task? </p>\n",
    "score": 133,
    "creation_date": 1329315126,
    "view_count": 25920,
    "answer_count": 6,
    "tags": "machine-learning;nlp;information-extraction;named-entity-recognition"
  },
  {
    "question_id": 10401076,
    "title": "Difference between constituency parser and dependency parser",
    "body": "<p>What is the difference between a <em>constituency parser</em> and a <em>dependency parser</em>? What are the different usages of the two?</p>\n",
    "score": 129,
    "creation_date": 1335890701,
    "view_count": 40130,
    "answer_count": 1,
    "tags": "parsing;nlp;terminology"
  },
  {
    "question_id": 27860652,
    "title": "What is the concept of negative-sampling in word2vec?",
    "body": "<p>I'm reading the 2014 paper <em><a href=\"https://arxiv.org/pdf/1402.3722v1.pdf\" rel=\"nofollow noreferrer\">word2vec Explained: Deriving Mikolov et al.’s\nNegative-Sampling Word-Embedding Method</a></em> (note: direct download link) and it references the concept of &quot;negative-sampling&quot;:</p>\n<blockquote>\n<p>Mikolov et al. present the negative-sampling approach as a more efficient\nway of deriving word embeddings. While negative-sampling is based on the\nskip-gram model, it is in fact optimizing a different objective.</p>\n</blockquote>\n<p>I have some issue understanding the concept of negative-sampling.</p>\n<p><a href=\"https://arxiv.org/pdf/1402.3722v1.pdf\" rel=\"nofollow noreferrer\">https://arxiv.org/pdf/1402.3722v1.pdf</a></p>\n<p>Can anyone explain in layman's terms what negative-sampling is?</p>\n",
    "score": 124,
    "creation_date": 1420806685,
    "view_count": 63554,
    "answer_count": 3,
    "tags": "machine-learning;nlp;word2vec"
  },
  {
    "question_id": 51956000,
    "title": "What does Keras Tokenizer method exactly do?",
    "body": "<p>On occasion, circumstances require us to do the following:</p>\n\n<pre><code>from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=my_max)\n</code></pre>\n\n<p>Then, invariably, we chant this mantra:</p>\n\n<pre><code>tokenizer.fit_on_texts(text) \nsequences = tokenizer.texts_to_sequences(text)\n</code></pre>\n\n<p>While I (more or less) understand what the total effect is, I can't figure out what each one does separately, regardless of how much research I do (including, obviously, the documentation). I don't think I've ever seen one without the other. </p>\n\n<p>So what does each do? Are there any circumstances where you would use either one without the other? If not, why aren't they simply combined into something like:</p>\n\n<pre><code>sequences = tokenizer.fit_on_texts_to_sequences(text)\n</code></pre>\n\n<p>Apologies if I'm missing something obvious, but I'm pretty new at this.</p>\n",
    "score": 119,
    "creation_date": 1534882128,
    "view_count": 118348,
    "answer_count": 4,
    "tags": "python;keras;nlp"
  },
  {
    "question_id": 22904025,
    "title": "Java or Python for Natural Language Processing",
    "body": "<p>I would like to know which programming language is better for natural language processing. <em>Java</em> or <em>Python</em>? I have found lots of questions and answers regarding about it. But I am still lost in choosing which one to use.</p>\n\n<p>And I want to know which NLP library to use for Java since there are lots of libraries (LingPipe, GATE, OpenNLP, StandfordNLP). For Python, most programmers recommend NLTK.</p>\n\n<p>But if I am to do some text processing or information extraction from <strong>unstructured data</strong> (just free formed plain English text) to get some useful information, what is the best option? Java or Python? Suitable library?</p>\n\n<p><strong>Updated</strong></p>\n\n<p>What I want to do is to extract useful product information from unstructured data (E.g. users make different forms of advertisement about mobiles or laptops with not very standard English language)</p>\n",
    "score": 119,
    "creation_date": 1396847327,
    "view_count": 74408,
    "answer_count": 2,
    "tags": "java;python;nlp"
  },
  {
    "question_id": 41424,
    "title": "How do you implement a &quot;Did you mean&quot;?",
    "body": "<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work\">How does the Google “Did you mean?” Algorithm work?</a>  </p>\n</blockquote>\n\n\n\n<p>Suppose you have a search system already in your website. How can you implement the \"Did you mean:<code>&lt;spell_checked_word&gt;</code>\" like Google does in some <a href=\"http://www.google.com/search?hl=en&amp;q=spellling&amp;btnG=Search\" rel=\"noreferrer\">search queries</a>?</p>\n",
    "score": 118,
    "creation_date": 1220438173,
    "view_count": 33164,
    "answer_count": 17,
    "tags": "nlp"
  },
  {
    "question_id": 771918,
    "title": "How do I do word Stemming or Lemmatization?",
    "body": "<p>I've tried PorterStemmer and Snowball but both don't work on all words, missing some very common ones. </p>\n\n<p>My test words are: \"<strong>cats running ran cactus cactuses cacti community communities</strong>\", and both get less than half right.</p>\n\n<p><strong>See also:</strong></p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/190775\">Stemming algorithm that produces real words</a></li>\n<li><a href=\"https://stackoverflow.com/questions/595110\">Stemming - code examples or open source projects?</a></li>\n</ul>\n",
    "score": 114,
    "creation_date": 1240308423,
    "view_count": 145208,
    "answer_count": 22,
    "tags": "nlp;stemming;lemmatization"
  },
  {
    "question_id": 3522372,
    "title": "How to config nltk data directory from code?",
    "body": "<p>How to config nltk data directory from code?</p>\n",
    "score": 106,
    "creation_date": 1282225378,
    "view_count": 87116,
    "answer_count": 7,
    "tags": "python;path;directory;nlp;nltk"
  },
  {
    "question_id": 58636587,
    "title": "How can I use BERT for long text classification?",
    "body": "<p>We know that BERT has a maximum length limit of tokens = 512. So if an article has a length of much bigger than 512, such as 10000 tokens in text, how can BERT be used?</p>\n",
    "score": 105,
    "creation_date": 1572492851,
    "view_count": 113004,
    "answer_count": 11,
    "tags": "nlp;text-classification;bert-language-model"
  },
  {
    "question_id": 1288291,
    "title": "How can I correctly prefix a word with &quot;a&quot; and &quot;an&quot;?",
    "body": "<p>I have a .NET application where, given a noun, I want it to correctly prefix that word with \"a\" or \"an\". How would I do that?</p>\n\n<p>Before you think the answer is to simply check if the first letter is a vowel, consider phrases like:</p>\n\n<ul>\n<li>an honest mistake</li>\n<li>a used car</li>\n</ul>\n",
    "score": 103,
    "creation_date": 1250519646,
    "view_count": 19753,
    "answer_count": 25,
    "tags": "c#;nlp;linguistics"
  },
  {
    "question_id": 13883277,
    "title": "How to use Stanford Parser in NLTK using Python",
    "body": "<p>Is it possible to use Stanford Parser in NLTK? (I am not talking about Stanford POS.)</p>\n",
    "score": 100,
    "creation_date": 1355505140,
    "view_count": 114751,
    "answer_count": 18,
    "tags": "python;parsing;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 9647202,
    "title": "Ordinal numbers replacement",
    "body": "<p>I am currently looking for the way to replace words like first, second, third,...with appropriate ordinal number representation (1st, 2nd, 3rd).\nI have been googling for the last week and I didn't find any useful standard tool or any function from NLTK.</p>\n\n<p>So is there any or should I write some regular expressions manually?</p>\n\n<p>Thanks for any advice</p>\n",
    "score": 98,
    "creation_date": 1331389669,
    "view_count": 101394,
    "answer_count": 16,
    "tags": "python;nlp;nltk;ordinals"
  },
  {
    "question_id": 10383044,
    "title": "Fuzzy String Comparison",
    "body": "<p>What I am striving to complete is a program which reads in a file and will compare each sentence according to the original sentence. The sentence which is a perfect match to the original will receive a score of 1 and a sentence which is the total opposite will receive a 0. All other fuzzy sentences will receive a grade in between 1 and 0. </p>\n\n<p>I am unsure which operation to use to allow me to complete this in Python 3. </p>\n\n<p>I have included the sample text in which the Text 1 is the original and the other preceding strings are the comparisons.  </p>\n\n<h2>Text: Sample</h2>\n\n<p>Text 1: It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.</p>\n\n<p>Text 20: It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines\n// Should score high point but not 1</p>\n\n<p>Text 21: It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines\n// Should score lower than text 20</p>\n\n<p>Text 22: I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.\n// Should score lower than text 21 but NOT 0</p>\n\n<p>Text 24: It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.\n// Should score a 0!</p>\n",
    "score": 97,
    "creation_date": 1335785840,
    "view_count": 147579,
    "answer_count": 4,
    "tags": "python;nlp;fuzzy-comparison"
  },
  {
    "question_id": 10850997,
    "title": "How to train the Stanford Parser with Genia Corpus?",
    "body": "<p>I have some problems to create a new model for Stanford Parser.</p>\n\n<p>I have also downloaded the last version from Stanford: \n<a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"noreferrer\">http://nlp.stanford.edu/software/lex-parser.shtml</a></p>\n\n<p>And here, Genia Corpus in 2 formats, xml and ptb (Penn Treebank).</p>\n\n<p>Standford Parser can train with ptd files ; then I downloaded Genia Corpus, because I want to work with biomedical text:</p>\n\n<p><s><a href=\"http://categorizer.tmit.bme.hu/~illes/genia_ptb/\" rel=\"noreferrer\">http://categorizer.tmit.bme.hu/~illes/genia_ptb/</a></s> <sup>(link no longer available)</sup>  (genia_ptb.tar.gz)</p>\n\n<p>Then, I have a short Main class to get dependency representation of one biomedical sentence:</p>\n\n<pre><code>    String treebankPath = \"/stanford-parser-2012-05-22/genia_ptb/GENIA_treebank_v1/ptb\";\n\n    Treebank tr = op.tlpParams.diskTreebank();\n    tr.loadPath(treebankPath);  \n    LexicalizedParser lpc=LexicalizedParser.trainFromTreebank(tr,op);\n</code></pre>\n\n<p>I have tried different ways, but always get the same result.</p>\n\n<p>I have an error in the last line. This is my output:</p>\n\n<pre><code>Currently Fri Jun 01 15:02:57 CEST 2012\nOptions parameters:\nuseUnknownWordSignatures 2\nsmoothInUnknownsThreshold 100\nsmartMutation false\nuseUnicodeType false\nunknownSuffixSize 1\nunknownPrefixSize 1\nflexiTag true\nuseSignatureForKnownSmoothing false\nparserParams edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams\nforceCNF false\ndoPCFG true\ndoDep false\nfreeDependencies false\ndirectional true\ngenStop true\ndistance true\ncoarseDistance false\ndcTags false\nnPrune false\nTrain parameters: smooth=false PA=true GPA=false selSplit=true (400.0; deleting [VP^SQ, VP^VP, VP^SINV, VP^NP]) mUnary=1 mUnaryTags=false sPPT=false tagPA=true tagSelSplit=false (0.0) rightRec=true leftRec=false collinsPunc=false markov=true mOrd=2 hSelSplit=true (10) compactGrammar=3 postPA=false postGPA=false selPSplit=false (0.0) tagSelPSplit=false (0.0) postSplitWithBase=false fractionBeforeUnseenCounting=0.5 openClassTypesThreshold=50 preTransformer=null taggedFiles=null\nUsing EnglishTreebankParserParams splitIN=4 sPercent=true sNNP=0 sQuotes=false sSFP=false rbGPA=false j#=false jJJ=false jNounTags=false sPPJJ=false sTRJJ=false sJJCOMP=false sMoreLess=false unaryDT=true unaryRB=true unaryPRP=false reflPRP=false unaryIN=false sCC=1 sNT=false sRB=false sAux=2 vpSubCat=false mDTV=2 sVP=3 sVPNPAgr=false sSTag=0 mVP=false sNP%=0 sNPPRP=false dominatesV=1 dominatesI=false dominatesC=false mCC=0 sSGapped=4 numNP=false sPoss=1 baseNP=1 sNPNNP=0 sTMP=1 sNPADV=1 cTags=true rightPhrasal=false gpaRootVP=false splitSbar=0 mPPTOiIN=0\nBinarizing trees...done. Time elapsed: 141 ms\nExtracting PCFG...done. Time elapsed: 56 ms\nCompiling grammar...done Time elapsed: 1 ms\nExtracting Lexicon...Exception in thread \"main\" edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: edu.stanford.nlp.util.MetaClass$ClassCreationException: java.lang.ClassNotFoundException: edu.stanford.nlp.parser.lexparser.EnglishUnknownWordModelTrainer\n    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:39)\n    at edu.stanford.nlp.parser.lexparser.BaseLexicon.initializeTraining(BaseLexicon.java:335)\n    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTreebank(LexicalizedParser.java:800)\n    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.trainFromTreebank(LexicalizedParser.java:226)\n    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.trainFromTreebank(LexicalizedParser.java:237)\n    at ABravoDemo.main(ABravoDemo.java:35)\nCaused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: java.lang.ClassNotFoundException: edu.stanford.nlp.parser.lexparser.EnglishUnknownWordModelTrainer\n    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:353)\n    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:370)\n    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:37)\n    ... 5 more\nCaused by: java.lang.ClassNotFoundException: edu.stanford.nlp.parser.lexparser.EnglishUnknownWordModelTrainer\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:200)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:303)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:248)\n    at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:316)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:169)\n    at edu.stanford.nlp.util.MetaClass$ClassFactory.construct(MetaClass.java:119)\n    at edu.stanford.nlp.util.MetaClass$ClassFactory.&lt;init&gt;(MetaClass.java:192)\n    at edu.stanford.nlp.util.MetaClass$ClassFactory.&lt;init&gt;(MetaClass.java:53)\n    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:349)\n    ... 7 more\n</code></pre>\n\n<p>How could I create a new model with this corpus ?</p>\n",
    "score": 93,
    "creation_date": 1338556430,
    "view_count": 4452,
    "answer_count": 3,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 870460,
    "title": "Is there a good natural language processing library",
    "body": "<p>I need to implement some NLP in my current module. I am looking for some good library that can help me here. I came across 'LingPipe' but could not completely follow on how to use it.<br>\nBasically, we need to implement a feature where the application can decipher customer instructions (delivery instructions) typed in plain english. Eg:</p>\n\n<ul>\n<li>Will pick up at 12:00 noon tomorrow</li>\n<li>Request delivery after 10th June</li>\n<li>Please do not send before Wednesday</li>\n<li>Add 10 more units of XYZ to the order</li>\n</ul>\n",
    "score": 92,
    "creation_date": 1242416361,
    "view_count": 59995,
    "answer_count": 3,
    "tags": "java;nlp"
  },
  {
    "question_id": 4951751,
    "title": "Creating a new corpus with NLTK",
    "body": "<p>I reckoned that often the answer to my title is to go and read the documentations, but I ran through the <a href=\"http://www.nltk.org/book\" rel=\"noreferrer\">NLTK book</a> but it doesn't give the answer. I'm kind of new to Python.</p>\n\n<p>I have a bunch of <code>.txt</code> files and I want to be able to use the corpus functions that NLTK provides for the corpus <code>nltk_data</code>. </p>\n\n<p>I've tried <code>PlaintextCorpusReader</code> but I couldn't get further than:</p>\n\n<pre><code>&gt;&gt;&gt;import nltk\n&gt;&gt;&gt;from nltk.corpus import PlaintextCorpusReader\n&gt;&gt;&gt;corpus_root = './'\n&gt;&gt;&gt;newcorpus = PlaintextCorpusReader(corpus_root, '.*')\n&gt;&gt;&gt;newcorpus.words()\n</code></pre>\n\n<p>How do I segment the <code>newcorpus</code> sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read <code>PlaintextCorpusReader</code> class?</p>\n\n<p>Can you also lead me to how I can write the segmented data into text files?</p>\n",
    "score": 91,
    "creation_date": 1297293588,
    "view_count": 90802,
    "answer_count": 4,
    "tags": "python;nlp;nltk;corpus"
  },
  {
    "question_id": 15173225,
    "title": "Calculate cosine similarity given 2 sentence strings",
    "body": "<p>From <a href=\"https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity\">Python: tf-idf-cosine: to find document similarity</a> , it is possible to calculate document similarity using tf-idf cosine. Without importing external libraries, are that any ways to calculate cosine similarity between 2 strings?</p>\n\n<pre><code>s1 = \"This is a foo bar sentence .\"\ns2 = \"This sentence is similar to a foo bar sentence .\"\ns3 = \"What is this string ? Totally not related to the other two lines .\"\n\ncosine_sim(s1, s2) # Should give high cosine similarity\ncosine_sim(s1, s3) # Shouldn't give high cosine similarity value\ncosine_sim(s2, s3) # Shouldn't give high cosine similarity value\n</code></pre>\n",
    "score": 90,
    "creation_date": 1362218789,
    "view_count": 156510,
    "answer_count": 8,
    "tags": "python;string;nlp;similarity;cosine-similarity"
  },
  {
    "question_id": 573768,
    "title": "Sentiment analysis for Twitter in Python",
    "body": "<p>I'm looking for an open source implementation, preferably in python, of <strong>Textual Sentiment Analysis</strong> (<a href=\"http://en.wikipedia.org/wiki/Sentiment_analysis\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Sentiment_analysis</a>). Is anyone familiar with such open source implementation I can use?</p>\n\n<p>I'm writing an application that searches twitter for some search term, say \"youtube\", and counts \"happy\" tweets vs. \"sad\" tweets. \nI'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.\nI haven't been able to find such sentiment analyzer so far, specifically not in python. \nAre you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.</p>\n\n<p>Note, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.</p>\n\n<p>BTW, twitter does support the \":)\" and \":(\" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.</p>\n\n<p>Thanks!</p>\n\n<p>BTW, an early demo is <a href=\"http://twitgraph.appspot.com/?show_inputs=1&amp;duration=30&amp;q=youtube+annotations\" rel=\"noreferrer\">here</a> and the code I have so far is <a href=\"http://code.google.com/p/twitgraph/\" rel=\"noreferrer\">here</a> and I'd love to opensource it with any interested developer.</p>\n",
    "score": 88,
    "creation_date": 1235251224,
    "view_count": 52226,
    "answer_count": 12,
    "tags": "python;machine-learning;nlp;open-source;sentiment-analysis"
  },
  {
    "question_id": 327513,
    "title": "Fuzzy string search library in Java",
    "body": "<p>I'm looking for a high performance Java library for fuzzy string search.</p>\n\n<p>There are numerous algorithms to find similar strings, Levenshtein distance, Daitch-Mokotoff Soundex, n-grams etc.</p>\n\n<p>What Java implementations exists? Pros and cons for them? I'm aware of Lucene, any other solution or Lucene is best?</p>\n\n<p>I found these, does anyone have experience with them?  </p>\n\n<ul>\n<li><a href=\"http://www.dcs.shef.ac.uk/~sam/simmetrics.html\" rel=\"noreferrer\">SimMetrics</a>  </li>\n<li><a href=\"http://ngramj.sourceforge.net/\" rel=\"noreferrer\">NGramJ</a>  </li>\n</ul>\n",
    "score": 87,
    "creation_date": 1227964631,
    "view_count": 85337,
    "answer_count": 8,
    "tags": "java;nlp;fuzzy-search"
  },
  {
    "question_id": 17317418,
    "title": "Stemmers vs Lemmatizers",
    "body": "<p>Natural Language Processing (NLP), especially for English, has evolved into the stage where stemming would become an archaic technology if \"perfect\" lemmatizers exist. It's because stemmers change the surface form of a word/token into some meaningless stems. </p>\n\n<p>Then again the definition of the \"perfect\" lemmatizer is questionable because different NLP task would have required different level of lemmatization. E.g. <a href=\"https://stackoverflow.com/questions/14489309/convert-words-between-verb-noun-adjective-forms\">Convert words between verb/noun/adjective forms</a>. </p>\n\n<p><strong>Stemmers</strong> </p>\n\n<pre><code>[in]: having\n[out]: hav\n</code></pre>\n\n<p><strong>Lemmatizers</strong></p>\n\n<pre><code>[in]: having\n[out]: have\n</code></pre>\n\n<ul>\n<li><p>So the question is, are English stemmers any useful at all today? Since we have a plethora of lemmatization tools for English</p></li>\n<li><p>If not, then how should we move on to build robust lemmatizers that\ncan take on <code>nounify</code>, <code>verbify</code>, <code>adjectify</code> and <code>adverbify</code>\npreprocesses?</p></li>\n<li><p>How could the lemmatization task be easily scaled to other languages\nthat have similar morphological structures as English?</p></li>\n</ul>\n",
    "score": 81,
    "creation_date": 1372241941,
    "view_count": 26062,
    "answer_count": 4,
    "tags": "nlp;wordnet;stemming;text-analysis;lemmatization"
  },
  {
    "question_id": 19130512,
    "title": "Stopword removal with NLTK",
    "body": "<p>I am trying to process a user entered text by removing stopwords using nltk toolkit, but with stopword-removal the words like 'and', 'or', 'not' gets removed. I want these words to be present after stopword removal process as they are operators which are required for later processing text as query. I don't know which are the words which can be operators in text query, and I also want to remove unnecessary words from my text.</p>\n",
    "score": 80,
    "creation_date": 1380691772,
    "view_count": 166488,
    "answer_count": 6,
    "tags": "python;nlp;nltk;stop-words"
  },
  {
    "question_id": 10554052,
    "title": "What are the major differences and benefits of Porter and Lancaster Stemming algorithms?",
    "body": "<p>I'm Working on document classification tasks in java.</p>\n\n<p>Both algorithms came highly recommended, what are the benefits and disadvantages of each and which is more commonly used in the literature for Natural Language Processing tasks? </p>\n",
    "score": 79,
    "creation_date": 1336749015,
    "view_count": 41180,
    "answer_count": 1,
    "tags": "java;machine-learning;nlp"
  },
  {
    "question_id": 40288323,
    "title": "What do spaCy&#39;s part-of-speech and dependency tags mean?",
    "body": "<p>spaCy tags up each of the <code>Token</code>s in a <code>Document</code> with a part of speech (in two different formats, one stored in the <code>pos</code> and <code>pos_</code> properties of the <code>Token</code> and the other stored in the <code>tag</code> and <code>tag_</code> properties) and a syntactic dependency to its <code>.head</code> token (stored in the <code>dep</code> and <code>dep_</code> properties).</p>\n\n<p>Some of these tags are self-explanatory, even to somebody like me without a linguistics background:</p>\n\n<pre><code>&gt;&gt;&gt; import spacy\n&gt;&gt;&gt; en_nlp = spacy.load('en')\n&gt;&gt;&gt; document = en_nlp(\"I shot a man in Reno just to watch him die.\")\n&gt;&gt;&gt; document[1]\nshot\n&gt;&gt;&gt; document[1].pos_\n'VERB'\n</code></pre>\n\n<p>Others... are not:</p>\n\n<pre><code>&gt;&gt;&gt; document[1].tag_\n'VBD'\n&gt;&gt;&gt; document[2].pos_\n'DET'\n&gt;&gt;&gt; document[3].dep_\n'dobj'\n</code></pre>\n\n<p>Worse, the <a href=\"https://spacy.io/docs/\" rel=\"noreferrer\">official docs</a> don't contain even a list of the possible tags for most of these properties, nor the meanings of any of them. They sometimes mention what tokenization standard they use, but these claims aren't currently entirely accurate and on top of that the standards are tricky to track down.</p>\n\n<p>What are the possible values of the <code>tag_</code>, <code>pos_</code>, and <code>dep_</code> properties, and what do they mean?</p>\n",
    "score": 78,
    "creation_date": 1477581272,
    "view_count": 73670,
    "answer_count": 9,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 36952763,
    "title": "How to return history of validation loss in Keras",
    "body": "<p>Using Anaconda Python 2.7 Windows 10.</p>\n\n<p>I am training a language model using the Keras exmaple:</p>\n\n<pre><code>print('Build model...')\nmodel = Sequential()\nmodel.add(GRU(512, return_sequences=True, input_shape=(maxlen, len(chars))))\nmodel.add(Dropout(0.2))\nmodel.add(GRU(512, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\ndef sample(a, temperature=1.0):\n    # helper function to sample an index from a probability array\n    a = np.log(a) / temperature\n    a = np.exp(a) / np.sum(np.exp(a))\n    return np.argmax(np.random.multinomial(1, a, 1))\n\n\n# train the model, output generated text after each iteration\nfor iteration in range(1, 3):\n    print()\n    print('-' * 50)\n    print('Iteration', iteration)\n    model.fit(X, y, batch_size=128, nb_epoch=1)\n    start_index = random.randint(0, len(text) - maxlen - 1)\n\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        print()\n        print('----- diversity:', diversity)\n\n        generated = ''\n        sentence = text[start_index: start_index + maxlen]\n        generated += sentence\n        print('----- Generating with seed: \"' + sentence + '\"')\n        sys.stdout.write(generated)\n\n        for i in range(400):\n            x = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(sentence):\n                x[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            generated += next_char\n            sentence = sentence[1:] + next_char\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\n</code></pre>\n\n<p>According to Keras documentation, the <code>model.fit</code> method returns a History callback, which has a history attribute containing the lists of successive losses and other metrics.</p>\n\n<pre><code>hist = model.fit(X, y, validation_split=0.2)\nprint(hist.history)\n</code></pre>\n\n<p>After training my model, if I run <code>print(model.history)</code> I get the error:</p>\n\n<pre><code> AttributeError: 'Sequential' object has no attribute 'history'\n</code></pre>\n\n<p>How do I return my model history after training my model with the above code?</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>The issue was that:</p>\n\n<p>The following had to first be defined:</p>\n\n<pre><code>from keras.callbacks import History \nhistory = History()\n</code></pre>\n\n<p>The callbacks option had to be called</p>\n\n<pre><code>model.fit(X_train, Y_train, nb_epoch=5, batch_size=16, callbacks=[history])\n</code></pre>\n\n<p>But now if I print</p>\n\n<pre><code>print(history.History)\n</code></pre>\n\n<p>it returns</p>\n\n<pre><code>{}\n</code></pre>\n\n<p>even though I ran an iteration. </p>\n",
    "score": 78,
    "creation_date": 1462005950,
    "view_count": 211323,
    "answer_count": 13,
    "tags": "python;neural-network;nlp;deep-learning;keras"
  },
  {
    "question_id": 6115677,
    "title": "English grammar for parsing in NLTK",
    "body": "<p>Is there a ready-to-use English grammar that I can just load it and use in NLTK? I've searched around examples of parsing with NLTK, but it seems like that I have to manually specify grammar before parsing a sentence. </p>\n\n<p>Thanks a lot!</p>\n",
    "score": 77,
    "creation_date": 1306264660,
    "view_count": 47961,
    "answer_count": 9,
    "tags": "python;nlp;grammar;nltk"
  },
  {
    "question_id": 526469,
    "title": "Practical examples of NLTK use",
    "body": "<p>I'm playing around with the <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">Natural Language Toolkit</a> (NLTK).</p>\n\n<p>Its documentation (<a href=\"http://www.nltk.org/book\" rel=\"noreferrer\">Book</a> and <a href=\"http://nltk.googlecode.com/svn/trunk/doc/howto/index.html\" rel=\"noreferrer\">HOWTO</a>) are quite bulky and the examples are sometimes slightly advanced. </p>\n\n<p>Are there any good but basic examples of uses/applications of NLTK? I'm thinking of things like the <a href=\"http://streamhacker.wordpress.com/tag/nltk/\" rel=\"noreferrer\">NTLK articles</a> on the <em>Stream Hacker</em> blog. </p>\n",
    "score": 77,
    "creation_date": 1234129273,
    "view_count": 52464,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 26899235,
    "title": "Python NLTK: SyntaxError: Non-ASCII character &#39;\\xc3&#39; in file (Sentiment Analysis -NLP)",
    "body": "<p>I am playing around with NLTK to do an assignment on sentiment analysis. I am using Python 2.7. NLTK 3.0 and NumPy1.9.1 version. </p>\n\n<p>This is the code :</p>\n\n<pre><code>__author__ = 'karan'\nimport nltk\nimport re\nimport sys\n\n\n\ndef main():\n    print(\"Start\");\n    # getting the stop words\n    stopWords = open(\"english.txt\",\"r\");\n    stop_word = stopWords.read().split();\n    AllStopWrd = []\n    for wd in stop_word:\n        AllStopWrd.append(wd);\n    print(\"stop words-&gt; \",AllStopWrd);\n\n    # sample and also cleaning it\n    tweet1= 'Love, my new toyí ½í¸í ½í¸#iPhone6. Its good https://twitter.com/Sandra_Ortega/status/513807261769424897/photo/1'\n    print(\"old tweet-&gt; \",tweet1)\n    tweet1 = tweet1.lower()\n    tweet1 = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet1).split())\n    print(tweet1);\n    tw = tweet1.split()\n    print(tw)\n\n\n    #tokenize\n    sentences = nltk.word_tokenize(tweet1)\n    print(\"tokenized -&gt;\", sentences)\n\n\n    #remove stop words\n    Otweet =[]\n    for w in tw:\n        if w not in AllStopWrd:\n            Otweet.append(w);\n    print(\"sans stop word-&gt; \",Otweet)\n\n\n    # get taggers for neg/pos/inc/dec/inv words\n    taggers ={}\n    negWords = open(\"neg.txt\",\"r\");\n    neg_word = negWords.read().split();\n    print(\"ned words-&gt; \",neg_word)\n    posWords = open(\"pos.txt\",\"r\");\n    pos_word = posWords.read().split();\n    print(\"pos words-&gt; \",pos_word)\n    incrWords = open(\"incr.txt\",\"r\");\n    inc_word = incrWords.read().split();\n    print(\"incr words-&gt; \",inc_word)\n    decrWords = open(\"decr.txt\",\"r\");\n    dec_word = decrWords.read().split();\n    print(\"dec wrds-&gt; \",dec_word)\n    invWords = open(\"inverse.txt\",\"r\");\n    inv_word = invWords.read().split();\n    print(\"inverse words-&gt; \",inv_word)\n    for nw in neg_word:\n        taggers.update({nw:'negative'});\n    for pw in pos_word:\n        taggers.update({pw:'positive'});\n    for iw in inc_word:\n        taggers.update({iw:'inc'});\n    for dw in dec_word:\n        taggers.update({dw:'dec'});\n    for ivw in inv_word:\n        taggers.update({ivw:'inv'});\n    print(\"tagger-&gt; \",taggers)\n    print(taggers.get('little'))\n\n    # get parts of speech\n    posTagger = [nltk.pos_tag(tw)]\n    print(\"posTagger-&gt; \",posTagger)\n\nmain();\n</code></pre>\n\n<p>This is the error that I am getting when running my code:</p>\n\n<pre><code>SyntaxError: Non-ASCII character '\\xc3' in file C:/Users/karan/PycharmProjects/mainProject/sentiment.py on line 19, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details\n</code></pre>\n\n<p>How do I fix this error?</p>\n\n<p>I also tried the code using Python 3.4.2 and with NLTK 3.0 and NumPy 1.9.1 but then I get the error:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 80, in &lt;module&gt;\n    main();\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 72, in main\n    posTagger = [nltk.pos_tag(tw)]\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\tag\\__init__.py\", line 100, in pos_tag\n    tagger = load(_POS_TAGGER)\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\data.py\", line 779, in load\n    resource_val = pickle.load(opened_resource)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xcb in position 0: ordinal not in range(128)\n</code></pre>\n",
    "score": 76,
    "creation_date": 1415838155,
    "view_count": 148929,
    "answer_count": 1,
    "tags": "python;unicode;nlp;nltk"
  },
  {
    "question_id": 9706769,
    "title": "Any tutorials for developing chatbots?",
    "body": "<p>As a engineering student, I would like to make a chat bot using python. So, I searched a lot but couldn't really find stuff that would teach me or give me some concrete information to build a intelligent chat bot.</p>\n\n<p>I would like to make a chatbot that gives human-like responses (Simply like a friend chatting with you). I am currently expecting it to be as just a software on my laptop (would like to implement in IM, IRC or websites later).</p>\n\n<p>So, I am looking for a tutorial/ any other information which would certainly help me to get my project done.</p>\n",
    "score": 73,
    "creation_date": 1331745279,
    "view_count": 74622,
    "answer_count": 2,
    "tags": "python;artificial-intelligence;nlp;chatbot"
  },
  {
    "question_id": 2452982,
    "title": "How to extract common / significant phrases from a series of text entries",
    "body": "<p>I have a series of text items- raw HTML from a MySQL database. I want to find the most common phrases in these entries (not the single most common phrase, and ideally, not enforcing word-for-word matching). </p>\n\n<p>My example is any review on Yelp.com, that shows 3 snippets from hundreds of reviews of a given restaurant, in the format: </p>\n\n<p>\"Try the hamburger\" (in 44 reviews)</p>\n\n<p>e.g., the \"Review Highlights\" section of this page: </p>\n\n<p><a href=\"http://www.yelp.com/biz/sushi-gen-los-angeles\" rel=\"noreferrer\"><a href=\"http://www.yelp.com/biz/sushi-gen-los-angeles/\" rel=\"noreferrer\">http://www.yelp.com/biz/sushi-gen-los-angeles/</a></a></p>\n\n<p>I have NLTK installed and I've played around with it a bit, but am honestly overwhelmed by the options. This seems like a rather common problem and I haven't been able to find a straightforward solution by searching here.</p>\n",
    "score": 73,
    "creation_date": 1268728958,
    "view_count": 72540,
    "answer_count": 4,
    "tags": "nlp;text-extraction;nltk;text-analysis"
  },
  {
    "question_id": 37889914,
    "title": "What is a projection layer in the context of neural networks?",
    "body": "<p>I am currently trying to understand the architecture behind the <em>word2vec</em> neural net learning algorithm, for representing words as vectors based on their context.</p>\n\n<p>After reading <a href=\"http://arxiv.org/pdf/1301.3781v3.pdf\" rel=\"noreferrer\">Tomas Mikolov paper</a> I came across what he defines as a <strong>projection layer</strong>. Even though this term is widely used when referred to <em>word2vec</em>, I couldn't find a precise definition of what it actually is in the neural net context.</p>\n\n<p><a href=\"https://i.sstatic.net/W46yb.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/W46yb.png\" alt=\"Word2Vec Neural Net architecture\"></a></p>\n\n<p>My question is, in the neural net context, what is a projection layer? Is it the name given to a hidden layer whose links to previous nodes share the same weights? Do its units actually have an activation function of some kind?</p>\n\n<p>￼Another resource that also refers more broadly to the problem can be found in <a href=\"http://www.coling-2014.org/COLING%202014%20Tutorial-fix%20-%20Tomas%20Mikolov.pdf\" rel=\"noreferrer\">this tutorial</a>, which also refers to a <em>projection layer</em> around page 67.</p>\n",
    "score": 70,
    "creation_date": 1466195407,
    "view_count": 30409,
    "answer_count": 3,
    "tags": "machine-learning;nlp;neural-network;word2vec"
  },
  {
    "question_id": 3763640,
    "title": "Where can I learn more about the Google search &quot;did you mean&quot; algorithm?",
    "body": "<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"https://stackoverflow.com/questions/41424/how-do-you-implement-a-did-you-mean\">How do you implement a &ldquo;Did you mean&rdquo;?</a>  </p>\n</blockquote>\n\n\n\n<p>I am writing an application where I require functionality similar to Google's \"did you mean?\" feature used by their search engine:</p>\n\n<p><img src=\"https://i.sstatic.net/cZCpI.jpg\" alt=\"alt text\"></p>\n\n<p>Is there source code available for such a thing or where can I find articles that would help me to build my own?</p>\n",
    "score": 69,
    "creation_date": 1285096519,
    "view_count": 18884,
    "answer_count": 11,
    "tags": "algorithm;nlp;spell-checking"
  },
  {
    "question_id": 41170726,
    "title": "Add/remove custom stop words with spacy",
    "body": "<p>What is the best way to add/remove stop words with spacy? I am using <a href=\"https://spacy.io/docs/api/token\" rel=\"noreferrer\"><code>token.is_stop</code></a> function and would like to make some custom changes to the set. I was looking at the documentation but could not find anything regarding of stop words. Thanks!</p>\n",
    "score": 69,
    "creation_date": 1481825509,
    "view_count": 80697,
    "answer_count": 8,
    "tags": "python;nlp;stop-words;spacy"
  },
  {
    "question_id": 10252448,
    "title": "How to check whether a sentence is correct (simple grammar check in Python)?",
    "body": "<p>How to check whether a sentence is valid in Python?</p>\n\n<p>Examples:</p>\n\n<pre><code>I love Stackoverflow - Correct\nI Stackoverflow love - Incorrect\n</code></pre>\n",
    "score": 69,
    "creation_date": 1334950393,
    "view_count": 106793,
    "answer_count": 6,
    "tags": "python;nlp;grammar"
  },
  {
    "question_id": 8772692,
    "title": "Semantic search with NLP and elasticsearch",
    "body": "<p>I am experimenting with elasticsearch as a search server and my task is to build a \"semantic\" search functionality. From a short text phrase like \"I have a burst pipe\" the system should infer that the user is searching for a plumber and return all plumbers indexed in elasticsearch. </p>\n\n<p>Can that be done directly in a search server like elasticsearch or do I have to use a natural language processing (NLP) tool like e.g. Maui Indexer. What is the exact terminology for my task at hand, text classification? Though the given text is very short as it is a search phrase.</p>\n",
    "score": 68,
    "creation_date": 1325966933,
    "view_count": 37625,
    "answer_count": 8,
    "tags": "search;nlp"
  },
  {
    "question_id": 15057945,
    "title": "How do I tokenize a string sentence in NLTK?",
    "body": "<p>I am using nltk, so I want to create my own custom texts just like the default ones on nltk.books. However, I've just got up to the method like</p>\n\n<pre><code>my_text = ['This', 'is', 'my', 'text']\n</code></pre>\n\n<p>I'd like to discover any way to input my \"text\" as:</p>\n\n<pre><code>my_text = \"This is my text, this is a nice way to input text.\"\n</code></pre>\n\n<p>Which method, python's or from nltk allows me to do this. And more important, how can I dismiss punctuation symbols?</p>\n",
    "score": 67,
    "creation_date": 1361748372,
    "view_count": 158993,
    "answer_count": 2,
    "tags": "python;nlp;tokenize;nltk"
  },
  {
    "question_id": 293000,
    "title": "Algorithm to determine how positive or negative a statement/text is",
    "body": "<p>I need an algorithm to determine if a sentence, paragraph or article is negative or positive in tone... or better yet, how negative or positive.</p>\n\n<p>For instance:</p>\n\n<blockquote>\n  <blockquote>\n    <p>Jason is the worst SO user I have ever witnessed (-10)</p>\n    \n    <p>Jason is an SO user (0)</p>\n    \n    <p>Jason is the best SO user I have ever seen (+10)</p>\n    \n    <p>Jason is the best at sucking with SO (-10)</p>\n    \n    <p>While, okay at SO, Jason is the worst at doing bad (+10)</p>\n  </blockquote>\n</blockquote>\n\n<p>Not easy, huh? :)</p>\n\n<p>I don't expect somebody to explain this algorithm to me, but I assume there is already much work on something like this in academia somewhere. If you can point me to some articles or research, I would love it.</p>\n\n<p>Thanks.</p>\n",
    "score": 67,
    "creation_date": 1226780065,
    "view_count": 59561,
    "answer_count": 14,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 27416164,
    "title": "What is CoNLL data format?",
    "body": "<p>I am using a open source jar (Mate Parser) which outputs in the CoNLL 2009 format after dependency parsing. I want to use the dependency parsing results for Information Extraction, however, I only understand part of the output in the CoNLL data format.</p>\n<p>Can someone explain the CoNLL data format?</p>\n",
    "score": 67,
    "creation_date": 1418276751,
    "view_count": 55363,
    "answer_count": 2,
    "tags": "nlp;text-parsing;text-mining;information-extraction"
  },
  {
    "question_id": 999410,
    "title": "Natural Language Processing in Ruby",
    "body": "<p>I'm looking to do some sentence analysis (mostly for twitter apps) and infer some general characteristics. Are there any good natural language processing libraries for this sort of thing in Ruby?</p>\n\n<p>Similar to <a href=\"https://stackoverflow.com/questions/870460/java-is-there-a-good-natural-language-processing-library\">Is there a good natural language processing library</a> but for Ruby. I'd prefer something very general, but any leads are appreciated!</p>\n",
    "score": 66,
    "creation_date": 1245120797,
    "view_count": 23260,
    "answer_count": 11,
    "tags": "ruby;artificial-intelligence;nlp"
  },
  {
    "question_id": 45735070,
    "title": "Keras Text Preprocessing - Saving Tokenizer object to file for scoring",
    "body": "<p>I've trained a sentiment classifier model using Keras library by following the below steps(broadly).</p>\n\n<ol>\n<li>Convert Text corpus into sequences using Tokenizer object/class</li>\n<li>Build a model using the model.fit() method </li>\n<li>Evaluate this model</li>\n</ol>\n\n<p>Now for scoring using this model, I was able to save the model to a file and load from a file. However I've not found a way to save the Tokenizer object to file. Without this I'll have to process the corpus every time I need to score even a single sentence. Is there a way around this?</p>\n",
    "score": 65,
    "creation_date": 1502972732,
    "view_count": 53904,
    "answer_count": 6,
    "tags": "machine-learning;neural-network;nlp;deep-learning;keras"
  },
  {
    "question_id": 62328,
    "title": "Is there an algorithm that tells the semantic similarity of two phrases",
    "body": "<p>input: phrase 1, phrase 2</p>\n\n<p>output: semantic similarity value (between 0 and 1), or the probability these two phrases are talking about the same thing</p>\n",
    "score": 65,
    "creation_date": 1221481602,
    "view_count": 49878,
    "answer_count": 11,
    "tags": "algorithm;nlp;semantics"
  },
  {
    "question_id": 49964028,
    "title": "SpaCy OSError: Can&#39;t find model &#39;en&#39;",
    "body": "<p>even though I downloaded the model it cannot load it</p>\n\n<pre><code>[jalal@goku entity-sentiment-analysis]$ which python\n/scratch/sjn/anaconda/bin/python\n[jalal@goku entity-sentiment-analysis]$ sudo python -m spacy download en\n[sudo] password for jalal: \nCollecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n    100% |████████████████████████████████| 37.4MB 9.4MB/s \nInstalling collected packages: en-core-web-sm\n  Running setup.py install for en-core-web-sm ... done\nSuccessfully installed en-core-web-sm-2.0.0\n\n    Linking successful\n    /usr/lib/python2.7/site-packages/en_core_web_sm --&gt;\n    /usr/lib64/python2.7/site-packages/spacy/data/en\n\n    You can now load the model via spacy.load('en')\n\nimport spacy \n\nnlp = spacy.load('en')\n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\n&lt;ipython-input-2-0fcabaab8c3d&gt; in &lt;module&gt;()\n      1 import spacy\n      2 \n----&gt; 3 nlp = spacy.load('en')\n\n/scratch/sjn/anaconda/lib/python3.6/site-packages/spacy/__init__.py in load(name, **overrides)\n     17             \"to load. For example:\\nnlp = spacy.load('{}')\".format(depr_path),\n     18             'error')\n---&gt; 19     return util.load_model(name, **overrides)\n     20 \n     21 \n\n/scratch/sjn/anaconda/lib/python3.6/site-packages/spacy/util.py in load_model(name, **overrides)\n    118     elif hasattr(name, 'exists'):  # Path or Path-like to model data\n    119         return load_model_from_path(name, **overrides)\n--&gt; 120     raise IOError(\"Can't find model '%s'\" % name)\n    121 \n    122 \n\nOSError: Can't find model 'en'\n</code></pre>\n\n<p>How should I fix this?</p>\n\n<p>If I don't use sudo for downloading the en model, I get:</p>\n\n<pre><code>Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n    100% |████████████████████████████████| 37.4MB 9.6MB/s ta 0:00:011   62% |████████████████████            | 23.3MB 8.6MB/s eta 0:00:02\nRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /scratch/sjn/anaconda/lib/python3.6/site-packages\nYou are using pip version 10.0.0, however version 10.0.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n\n    Error: Couldn't link model to 'en'\n    Creating a symlink in spacy/data failed. Make sure you have the required\n    permissions and try re-running the command as admin, or use a\n    virtualenv. You can still import the model as a module and call its\n    load() method, or create the symlink manually.\n\n    /scratch/sjn/anaconda/lib/python3.6/site-packages/en_core_web_sm --&gt;\n    /scratch/sjn/anaconda/lib/python3.6/site-packages/spacy/data/en\n\n\n    Download successful but linking failed\n    Creating a shortcut link for 'en' didn't work (maybe you don't have\n    admin permissions?), but you can still load the model via its full\n    package name:\n\n    nlp = spacy.load('en_core_web_sm')\n</code></pre>\n",
    "score": 64,
    "creation_date": 1524385982,
    "view_count": 132271,
    "answer_count": 14,
    "tags": "nlp;spacy"
  },
  {
    "question_id": 19790188,
    "title": "Expanding English language contractions in Python",
    "body": "<p>The English language has <a href=\"http://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\">a couple of contractions</a>. For instance:</p>\n\n<pre><code>you've -&gt; you have\nhe's -&gt; he is\n</code></pre>\n\n<p>These can sometimes cause headache when you are doing natural language processing. Is there a Python library, which can expand these contractions?</p>\n",
    "score": 64,
    "creation_date": 1383658342,
    "view_count": 63714,
    "answer_count": 8,
    "tags": "python;nlp;text-processing"
  },
  {
    "question_id": 19994396,
    "title": "Best way to identify and extract dates from text Python?",
    "body": "<p>As part of a larger personal project I'm working on, I'm attempting to separate out inline dates from a variety of text sources.</p>\n\n<p>For example, I have a large list of strings (that usually take the form of English sentences or statements) that take a variety of forms:</p>\n\n<blockquote>\n  <p>Central design committee session Tuesday 10/22 6:30 pm</p>\n  \n  <p>Th 9/19 LAB: Serial encoding (Section 2.2)</p>\n  \n  <p>There will be another one on December 15th for those who are unable to make it today.</p>\n  \n  <p>Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm</p>\n  \n  <p>He will be flying in Sept. 15th.</p>\n</blockquote>\n\n<p>While these dates are in-line with natural text, none of them are in specifically natural language forms themselves (e.g., there's no \"The meeting will be two weeks from tomorrow\"—it's all explicit).  </p>\n\n<p>As someone who doesn't have too much experience with this kind of processing, what would be the best place to begin? I've looked into things like the <code>dateutil.parser</code> module and <a href=\"https://github.com/bear/parsedatetime\">parsedatetime</a>, but those seem to be for <em>after</em> you've isolated the date.</p>\n\n<p>Because of this, is there any good way to extract the date and the extraneous text </p>\n\n<pre><code>input:  Th 9/19 LAB: Serial encoding (Section 2.2)\noutput: ['Th 9/19', 'LAB: Serial encoding (Section 2.2)']\n</code></pre>\n\n<p>or something similar? It seems like this sort of processing is done by applications like Gmail and Apple Mail, but is it possible to implement in Python?</p>\n",
    "score": 62,
    "creation_date": 1384494622,
    "view_count": 86259,
    "answer_count": 8,
    "tags": "python;parsing;date;nlp"
  },
  {
    "question_id": 38045290,
    "title": "Text Summarization Evaluation - BLEU vs ROUGE",
    "body": "<p>With the results of two different summary systems (sys1 and sys2) and the same reference summaries, I evaluated them with both BLEU and ROUGE. The problem is: All ROUGE scores of sys1 was higher than sys2 (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, ROUGE-L, ROUGE-SU4, ...) but the BLEU score of sys1 was less than the BLEU score of sys2 (quite much).</p>\n<p>So my question is: Both ROUGE and BLEU are based on n-gram to measure the similar between the summaries of systems and the summaries of human. So why there are differences in results of evaluation like that? And what's the main different of ROUGE vs BLEU to explain this issue?</p>\n",
    "score": 62,
    "creation_date": 1466996563,
    "view_count": 48292,
    "answer_count": 3,
    "tags": "nlp;text-processing;rouge;bleu"
  },
  {
    "question_id": 491971,
    "title": "What programming language is most like natural language?",
    "body": "<p>I got the idea for this question from numerous situations where I don't understand what the person is talking about and when others don't understand me.</p>\n\n<p>So, a \"smart\" solution would be to speak a computer language. :)</p>\n\n<p>I am interested how far a programming language can go to get near to (English) natural language. When I say near, I mean not just to use words and sentences, but to be able to \"do\" things a natural language can \"do\" and by \"do\" I mean that it can be used (in a very limited way) as a replacement for natural language.</p>\n\n<p>I know that this is impossible (is it?) but I think that this can be interesting.</p>\n",
    "score": 61,
    "creation_date": 1233241959,
    "view_count": 28098,
    "answer_count": 33,
    "tags": "programming-languages;nlp"
  },
  {
    "question_id": 17116446,
    "title": "What do the BILOU tags mean in Named Entity Recognition?",
    "body": "<p>Title pretty much sums up the question.  I've noticed that in some papers people have referred to a BILOU encoding scheme for NER as opposed to the typical BIO tagging scheme (Such as this paper by Ratinov and Roth in 2009 <a href=\"http://cogcomp.cs.illinois.edu/page/publication_view/199\" rel=\"noreferrer\">http://cogcomp.cs.illinois.edu/page/publication_view/199</a>)</p>\n\n<p>From working with the 2003 CoNLL data I know that</p>\n\n<pre><code>B stands for 'beginning' (signifies beginning of an NE)\nI stands for 'inside' (signifies that the word is inside an NE)\nO stands for 'outside' (signifies that the word is just a regular word outside of an NE)\n</code></pre>\n\n<p>While I've been told that the words in BILOU stand for</p>\n\n<pre><code>B - 'beginning'\nI - 'inside'\nL - 'last'\nO - 'outside'\nU - 'unit'\n</code></pre>\n\n<p>I've also seen people reference another tag </p>\n\n<pre><code>E - 'end', use it concurrently with the 'last' tag\nS - 'singleton', use it concurrently with the 'unit' tag\n</code></pre>\n\n<p>I'm pretty new to the NER literature, but I've been unable to find something clearly explaining these tags.  My questions in particular relates to what the difference between 'last' and 'end' tags are, and what 'unit' tag stands for.</p>\n",
    "score": 60,
    "creation_date": 1371240335,
    "view_count": 29109,
    "answer_count": 6,
    "tags": "nlp;named-entity-recognition"
  },
  {
    "question_id": 4588541,
    "title": "Hamming Distance vs. Levenshtein Distance",
    "body": "<p>For the problem I'm working on, finding distances between two sequences to determine their similarity, sequence order is very important. However, the sequences that I have are not all the same length, so I pad any deficient strings with empty points such that both sequences are the same length in order to satisfy the Hamming distance requirement. Is there any major problem with me doing this, since all I care about are the number of transpositions (not insertions or deletions like Levenshtein does)?</p>\n\n<p>I've found that Hamming distance is much, much faster than Levenshtein as a distance metric for sequences of longer length. When should one use Levenshtein distance (or derivatives of Levenshtein distance) instead of the much cheaper Hamming distance? Hamming distance can be considered the upper bound for possible Levenshtein distances between two sequences, so if I am comparing the two sequences for a order-biased similarity metric rather than the absolute minimal number of moves to match the sequences, there isn't an apparent reason for me to choose Levenshtein over Hamming as a metric, is there?</p>\n",
    "score": 60,
    "creation_date": 1294090154,
    "view_count": 26118,
    "answer_count": 2,
    "tags": "algorithm;diff;nlp;levenshtein-distance;hamming-distance"
  },
  {
    "question_id": 55382596,
    "title": "How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP?",
    "body": "<p>I have seen that NLP models such as <a href=\"https://github.com/google-research/bert\" rel=\"noreferrer\">BERT</a> utilize WordPiece for tokenization. In WordPiece, we split the tokens like <strong><code>playing</code></strong> to <strong><code>play</code></strong> and <strong><code>##ing</code></strong>. It is mentioned that it covers a wider spectrum of Out-Of-Vocabulary (OOV) words. Can someone please help me explain how WordPiece tokenization is actually done, and how it handles effectively helps to rare/OOV words? </p>\n",
    "score": 59,
    "creation_date": 1553705554,
    "view_count": 23114,
    "answer_count": 2,
    "tags": "nlp;word-embedding"
  },
  {
    "question_id": 195010,
    "title": "How can I split multiple joined words?",
    "body": "<p>I have an array of 1000 or so entries, with examples below:</p>\n\n<pre><code>wickedweather\nliquidweather\ndriveourtrucks\ngocompact\nslimprojector\n</code></pre>\n\n<p>I would like to be able to split these into their respective words, as:</p>\n\n<pre><code>wicked weather\nliquid weather\ndrive our trucks\ngo compact\nslim projector\n</code></pre>\n\n<p>I was hoping a regular expression my do the trick.  But, since there is no boundary to stop on, nor is there any sort of capitalization that I could possibly key on, I am thinking, that some sort of reference to a dictionary might be necessary?  </p>\n\n<p>I suppose it could be done by hand, but why - when it can be done with code! =)  But this has stumped me.  Any ideas?  </p>\n",
    "score": 58,
    "creation_date": 1223779039,
    "view_count": 37841,
    "answer_count": 16,
    "tags": "string;nlp"
  },
  {
    "question_id": 7551262,
    "title": "Training data for sentiment analysis",
    "body": "<p>Where can I get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? I want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media.</p>\n\n<p>I find corpora that have reviews of products and movies. Is there a corpus for the business domain including reviews of companies, that match the language of business?</p>\n",
    "score": 57,
    "creation_date": 1317017934,
    "view_count": 42907,
    "answer_count": 6,
    "tags": "nlp;machine-learning;text-analysis;sentiment-analysis;training-data"
  },
  {
    "question_id": 4188706,
    "title": "Sentiment Analysis Dictionaries",
    "body": "<p>I was wondering if anybody knew where I could obtain dictionaries of positive and negative words.  I'm looking into sentiment analysis and this is a crucial part of it.</p>\n",
    "score": 56,
    "creation_date": 1289854365,
    "view_count": 71326,
    "answer_count": 9,
    "tags": "dictionary;nlp;sentiment-analysis"
  },
  {
    "question_id": 933212,
    "title": "Is it possible to guess a user&#39;s mood based on the structure of text?",
    "body": "<p>I assume a natural language processor would need to be used to parse the text itself, but what suggestions do you have for an algorithm to detect a user's mood based on text that they have written? I doubt it would be very accurate, but I'm still interested nonetheless.</p>\n\n<p>EDIT: I am by no means an expert on linguistics or natural language processing, so I apologize if this question is too general or stupid.</p>\n",
    "score": 56,
    "creation_date": 1243817010,
    "view_count": 14867,
    "answer_count": 10,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 20290870,
    "title": "Improving the extraction of human names with nltk",
    "body": "<p>I am trying to extract human names from text. </p>\n\n<p>Does anyone have a method that they would recommend?</p>\n\n<p>This is what I tried (code is below):\nI am using <code>nltk</code> to find everything marked as a person and then generating a list of all the NNP parts of that person. I am skipping persons where there is only one NNP which avoids grabbing a lone surname.</p>\n\n<p>I am getting decent results but was wondering if there are better ways to go about solving this problem.</p>\n\n<p>Code:</p>\n\n<pre><code>import nltk\nfrom nameparser.parser import HumanName\n\ndef get_human_names(text):\n    tokens = nltk.tokenize.word_tokenize(text)\n    pos = nltk.pos_tag(tokens)\n    sentt = nltk.ne_chunk(pos, binary = False)\n    person_list = []\n    person = []\n    name = \"\"\n    for subtree in sentt.subtrees(filter=lambda t: t.node == 'PERSON'):\n        for leaf in subtree.leaves():\n            person.append(leaf[0])\n        if len(person) &gt; 1: #avoid grabbing lone surnames\n            for part in person:\n                name += part + ' '\n            if name[:-1] not in person_list:\n                person_list.append(name[:-1])\n            name = ''\n        person = []\n\n    return (person_list)\n\ntext = \"\"\"\nSome economists have responded positively to Bitcoin, including \nFrancois R. Velde, senior economist of the Federal Reserve in Chicago \nwho described it as \"an elegant solution to the problem of creating a \ndigital currency.\" In November 2013 Richard Branson announced that \nVirgin Galactic would accept Bitcoin as payment, saying that he had invested \nin Bitcoin and found it \"fascinating how a whole new global currency \nhas been created\", encouraging others to also invest in Bitcoin.\nOther economists commenting on Bitcoin have been critical. \nEconomist Paul Krugman has suggested that the structure of the currency \nincentivizes hoarding and that its value derives from the expectation that \nothers will accept it as payment. Economist Larry Summers has expressed \na \"wait and see\" attitude when it comes to Bitcoin. Nick Colas, a market \nstrategist for ConvergEx Group, has remarked on the effect of increasing \nuse of Bitcoin and its restricted supply, noting, \"When incremental \nadoption meets relatively fixed supply, it should be no surprise that \nprices go up. And that’s exactly what is happening to BTC prices.\"\n\"\"\"\n\nnames = get_human_names(text)\nprint \"LAST, FIRST\"\nfor name in names: \n    last_first = HumanName(name).last + ', ' + HumanName(name).first\n        print last_first\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>LAST, FIRST\nVelde, Francois\nBranson, Richard\nGalactic, Virgin\nKrugman, Paul\nSummers, Larry\nColas, Nick\n</code></pre>\n\n<p>Apart from Virgin Galactic, this is all valid output. Of course, knowing that Virgin Galactic isn't a human name in the context of this article is the hard (maybe impossible) part.</p>\n",
    "score": 56,
    "creation_date": 1385746387,
    "view_count": 109063,
    "answer_count": 7,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 202750,
    "title": "Is there a human readable programming language?",
    "body": "<p>I mean, is there a coded language with human style coding?\nFor example:</p>\n\n<pre><code>Create an object called MyVar and initialize it to 10;\nTake MyVar and call MyMethod() with parameters. . .\n</code></pre>\n\n<p>I know it's not so useful, but it can be interesting to create such a grammar.</p>\n",
    "score": 55,
    "creation_date": 1224017473,
    "view_count": 17810,
    "answer_count": 51,
    "tags": "nlp;grammar"
  },
  {
    "question_id": 35596031,
    "title": "gensim word2vec: Find number of words in vocabulary",
    "body": "<p>After training a word2vec model using python <a href=\"http://radimrehurek.com/gensim/models/word2vec.html\" rel=\"noreferrer\">gensim</a>, how do you find the number of words in the model's vocabulary?</p>\n",
    "score": 55,
    "creation_date": 1456299588,
    "view_count": 97266,
    "answer_count": 5,
    "tags": "python;neural-network;nlp;gensim;word2vec"
  },
  {
    "question_id": 74947992,
    "title": "How to remove the error &quot;SystemError: initialization of _internal failed without raising an exception&quot;",
    "body": "<p>I am trying to import Top2Vec package for nlp topic modelling. But even after upgrading pip, numpy this error is coming.</p>\n<p>I tried</p>\n<pre><code>pip install --upgrade pip\n</code></pre>\n<pre><code>pip install --upgrade numpy\n</code></pre>\n<p>I was expecting to run</p>\n<pre><code>from top2vec import Top2Vec\n\nmodel = Top2Vec(FAQs, speed='learn', workers=8)\n</code></pre>\n<p>but it is giving the mentioned error</p>\n",
    "score": 55,
    "creation_date": 1672295869,
    "view_count": 93489,
    "answer_count": 5,
    "tags": "python;import;nlp;google-colaboratory"
  },
  {
    "question_id": 30795944,
    "title": "How can a sentence or a document be converted to a vector?",
    "body": "<p>We have models for converting words to vectors (for example the word2vec model). Do similar models exist which convert sentences/documents into vectors, using perhaps the vectors learnt for the individual words?</p>\n",
    "score": 55,
    "creation_date": 1434087416,
    "view_count": 36515,
    "answer_count": 5,
    "tags": "vector;nlp;word2vec"
  },
  {
    "question_id": 559510,
    "title": "Looking for Java spell checker library",
    "body": "<p>I am looking for an open source Java spell checking library which has dictionaries for at least the following languages: French, German, Spanish, and Czech. Any suggestion?</p>\n",
    "score": 55,
    "creation_date": 1234920872,
    "view_count": 57326,
    "answer_count": 8,
    "tags": "java;nlp;spell-checking;languagetool"
  },
  {
    "question_id": 1164186,
    "title": "how to check if a string looks randomized, or human generated and pronouncable?",
    "body": "<p>For the purpose of identifying [possible] bot-generated usernames.</p>\n\n<p>Suppose you have a username like \"bilbomoothof\" .. it may be nonsense, but it still contains pronouncable sounds and so appears human-generated.</p>\n\n<p>I accept that it could have been randomly generated from a dictionary of syllables, or word parts, but let's assume for a moment that the bot in question is a bit rubbish.</p>\n\n<ol>\n<li>Suppose you have a username like\n\"sdfgbhm342r3f\", to a human this is\nclearly a random string. But can\nthis be identified programatically?</li>\n<li>Are there any algorithms available\n(similar to Soundex, etc..) that can\nidentify pronounceable sounds within\na string like this?</li>\n</ol>\n\n<p>Solutions applicable in PHP/MySQL most appreciated.</p>\n",
    "score": 55,
    "creation_date": 1248256094,
    "view_count": 5609,
    "answer_count": 11,
    "tags": "mysql;algorithm;nlp;spam;phonetics"
  },
  {
    "question_id": 39843584,
    "title": "gensim Doc2Vec vs tensorflow Doc2Vec",
    "body": "<p>I'm trying to compare my implementation of Doc2Vec (via tf) and gensims implementation. It seems atleast visually that the gensim ones are performing better.</p>\n\n<p>I ran the following code to train the gensim model and the one below that for tensorflow model. My questions are as follows:</p>\n\n<ol>\n<li>Is my tf implementation of Doc2Vec correct. Basically is it supposed to be concatenating the word vectors and the document vector to predict the middle word in a certain context?</li>\n<li>Does the <code>window=5</code> parameter in gensim mean that I am using two words on either side to predict the middle one? Or is it 5 on either side. Thing is there are quite a few documents that are smaller than length 10.</li>\n<li>Any insights as to why Gensim is performing better? Is my model any different to how they implement it?</li>\n<li>Considering that this is effectively a matrix factorisation problem, why is the TF model even getting an answer? There are infinite solutions to this since its a rank deficient problem. &lt;- This last question is simply a bonus.</li>\n</ol>\n\n<h3>Gensim</h3>\n\n<pre><code>model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=10, hs=0, min_count=2, workers=cores)\nmodel.build_vocab(corpus)\nepochs = 100\nfor i in range(epochs):\n    model.train(corpus)\n</code></pre>\n\n<h3>TF</h3>\n\n<pre><code>batch_size = 512\nembedding_size = 100 # Dimension of the embedding vector.\nnum_sampled = 10 # Number of negative examples to sample.\n\n\ngraph = tf.Graph()\n\nwith graph.as_default(), tf.device('/cpu:0'):\n    # Input data.\n    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size/context_window])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size/context_window, 1])\n\n    # The variables   \n    word_embeddings =  tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, (context_window+1)*embedding_size],\n                             stddev=1.0 / np.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    ###########################\n    # Model.\n    ###########################\n    # Look up embeddings for inputs and stack words side by side\n    embed_words = tf.reshape(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),\n                            shape=[int(batch_size/context_window),-1])\n    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)\n    embed = tf.concat(1,[embed_words, embed_docs])\n    # Compute the softmax loss, using a sample of the negative labels each time.\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n                                   train_labels, num_sampled, vocabulary_size))\n\n    # Optimizer.\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n</code></pre>\n\n<h2>Update:</h2>\n\n<p>Check out the jupyter notebook <a href=\"https://github.com/sachinruk/doc2vec_tf\" rel=\"noreferrer\">here</a> (I have both models working and tested in here). It still feels like the gensim model is performing better in this initial analysis.</p>\n",
    "score": 55,
    "creation_date": 1475550807,
    "view_count": 17335,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;gensim;doc2vec"
  },
  {
    "question_id": 38287772,
    "title": "CBOW v.s. skip-gram: why invert context and target words?",
    "body": "<p>In <a href=\"https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html#vector-representations-of-words\" rel=\"noreferrer\">this</a> page, it is said that: </p>\n\n<blockquote>\n  <p>[...] skip-gram inverts contexts and targets, and tries to predict each context word from its target word [...]</p>\n</blockquote>\n\n<p>However, looking at the training dataset it produces, the content of the X and Y pair seems to be interexchangeable, as those two pairs of (X, Y): </p>\n\n<blockquote>\n  <p><code>(quick, brown), (brown, quick)</code></p>\n</blockquote>\n\n<p>So, why distinguish that much between context and targets if it is the same thing in the end? </p>\n\n<p>Also, doing <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb\" rel=\"noreferrer\">Udacity's Deep Learning course exercise on word2vec</a>, I wonder why they seem to do the difference between those two approaches that much in this problem: </p>\n\n<blockquote>\n  <p>An alternative to skip-gram is another Word2Vec model called CBOW (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.</p>\n</blockquote>\n\n<p>Would not this yields the same results?</p>\n",
    "score": 54,
    "creation_date": 1468113694,
    "view_count": 41078,
    "answer_count": 3,
    "tags": "nlp;tensorflow;deep-learning;word2vec;word-embedding"
  },
  {
    "question_id": 34232190,
    "title": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score",
    "body": "<p>I am working on keyword extraction problem. Consider the very general case</p>\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n\nt = &quot;&quot;&quot;Two Travellers, walking in the noonday sun, sought the shade of a widespreading tree to rest. As they lay looking up among the pleasant leaves, they saw that it was a Plane Tree.\n\n&quot;How useless is the Plane!&quot; said one of them. &quot;It bears no fruit whatever, and only serves to litter the ground with leaves.&quot;\n\n&quot;Ungrateful creatures!&quot; said a voice from the Plane Tree. &quot;You lie here in my cooling shade, and yet you say I am useless! Thus ungratefully, O Jupiter, do men receive their blessings!&quot;\n\nOur best blessings are often the least appreciated.&quot;&quot;&quot;\n\ntfs = tfidf.fit_transform(t.split(&quot; &quot;))\nstr = 'tree cat travellers fruit jupiter'\nresponse = tfidf.transform([str])\nfeature_names = tfidf.get_feature_names()\n\nfor col in response.nonzero()[1]:\n    print(feature_names[col], ' - ', response[0, col])\n</code></pre>\n<p>and this gives me</p>\n<pre><code>  (0, 28)   0.443509712811\n  (0, 27)   0.517461475101\n  (0, 8)    0.517461475101\n  (0, 6)    0.517461475101\ntree  -  0.443509712811\ntravellers  -  0.517461475101\njupiter  -  0.517461475101\nfruit  -  0.517461475101\n</code></pre>\n<p>which is good. For any new document that comes in, is there a way to get the top n terms with the highest tfidf score?</p>\n",
    "score": 54,
    "creation_date": 1449866375,
    "view_count": 70725,
    "answer_count": 3,
    "tags": "python;scikit-learn;nlp;nltk;tf-idf"
  },
  {
    "question_id": 13603882,
    "title": "Feature Selection and Reduction for Text Classification",
    "body": "<p>I am currently working on a project, a <strong>simple sentiment analyzer</strong> such that there will be <strong>2 and 3 classes</strong> in <strong>separate cases</strong>. I am using a <strong>corpus</strong> that is pretty <strong>rich</strong> in the means of <strong>unique words</strong> (around 200.000). I used <strong>bag-of-words</strong> method for <strong>feature selection</strong> and to reduce the number of <strong>unique features</strong>, an elimination is done due to a <strong>threshold value</strong> of <strong>frequency of occurrence</strong>. The <strong>final set of features</strong> includes around 20.000 features, which is actually a <strong>90% decrease</strong>, but <strong>not enough</strong> for intended <strong>accuracy</strong> of test-prediction. I am using <strong>LibSVM</strong> and <strong>SVM-light</strong> in turn for training and prediction (both <strong>linear</strong> and <strong>RBF kernel</strong>) and also <strong>Python</strong> and <strong>Bash</strong> in general.</p>\n\n<p>The <strong>highest accuracy</strong> observed so far <strong>is around 75%</strong> and I <strong>need at least 90%</strong>. This is the case for <strong>binary classification</strong>. For <strong>multi-class training</strong>, the accuracy falls to <strong>~60%</strong>. I <strong>need at least 90%</strong> at both cases and can not figure how to increase it: via <strong>optimizing training parameters</strong> or <strong>via optimizing feature selection</strong>?</p>\n\n<p>I have read articles about <strong>feature selection</strong> in text classification and what I found is that three different methods are used, which have actually a clear correlation among each other. These methods are as follows:</p>\n\n<ul>\n<li>Frequency approach of <strong>bag-of-words</strong> (BOW)</li>\n<li><strong>Information Gain</strong> (IG)</li>\n<li><strong>X^2 Statistic</strong> (CHI)</li>\n</ul>\n\n<p>The first method is already the one I use, but I use it very simply and need guidance for a better use of it in order to obtain high enough accuracy. I am also lacking knowledge about practical implementations of <strong>IG</strong> and <strong>CHI</strong> and looking for any help to guide me in that way.</p>\n\n<p>Thanks a lot, and if you need any additional info for help, just let me know.</p>\n\n<hr>\n\n<ul>\n<li><p>@larsmans: <strong>Frequency Threshold</strong>: I am looking for the occurrences of unique words in examples, such that if a word is occurring in different examples frequently enough, it is included in the feature set as a unique feature.   </p></li>\n<li><p>@TheManWithNoName: First of all thanks for your effort in explaining the general concerns of document classification. I examined and experimented all the methods you bring forward and others. I found <strong>Proportional Difference</strong> (PD) method the best for feature selection, where features are uni-grams and <strong>Term Presence</strong> (TP) for the weighting (I didn't understand why you tagged <strong>Term-Frequency-Inverse-Document-Frequency</strong> (TF-IDF) as an indexing method, I rather consider it as a <strong>feature weighting</strong> approach).  <strong>Pre-processing</strong> is also an important aspect for this task as you mentioned. I used certain types of string elimination for refining the data as well as <strong>morphological parsing</strong> and <strong>stemming</strong>. Also note that I am working on <strong>Turkish</strong>, which has <strong>different characteristics</strong> compared to English. Finally, I managed to reach <strong>~88% accuracy</strong> (f-measure) for <strong>binary</strong> classification and <strong>~84%</strong> for <strong>multi-class</strong>. These values are solid proofs of the success of the model I used. This is what I have done so far. Now working on clustering and reduction models, have tried <strong>LDA</strong> and <strong>LSI</strong> and moving on to <strong>moVMF</strong> and maybe <strong>spherical models</strong> (LDA + moVMF), which seems to work better on corpus those have objective nature, like news corpus. If you have any information and guidance on these issues, I will appreciate. I need info especially to setup an interface (python oriented, open-source) between <strong>feature space dimension reduction</strong> methods (LDA, LSI, moVMF etc.) and <strong>clustering methods</strong> (k-means, hierarchical etc.).</p></li>\n</ul>\n",
    "score": 53,
    "creation_date": 1354101719,
    "view_count": 33022,
    "answer_count": 5,
    "tags": "python;nlp;svm;sentiment-analysis;feature-extraction"
  },
  {
    "question_id": 8998979,
    "title": "What Is the Difference Between POS Tagging and Shallow Parsing?",
    "body": "<p>I'm currently taking a Natural Language Processing course at my University and still confused with some basic concept. I get the definition of POS Tagging from the <a href=\"http://www-nlp.stanford.edu/fsnlp/\" rel=\"noreferrer\">Foundations of Statistical Natural Language Processing</a> book:</p>\n\n<blockquote>\n  <p>Tagging is the task of labeling (or tagging) each word in a sentence\n  with its appropriate part of speech. We decide whether each word is a\n  noun, verb, adjective, or whatever.</p>\n</blockquote>\n\n<p>But I can't find a definition of Shallow Parsing in the book since it also describe shallow parsing as one of the utilities of POS Tagging. So I began to search the web and found no direct explanation of shallow parsing, but in <a href=\"http://en.wikipedia.org/wiki/Shallow_parsing\" rel=\"noreferrer\">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>Shallow parsing (also chunking, \"light parsing\") is an analysis of a sentence which identifies the constituents (noun groups, verbs, verb groups, etc.), but does not specify their internal structure, nor their role in the main sentence.</p>\n</blockquote>\n\n<p>I frankly don't see the difference, but it may be because of my English or just me not understanding simple basic concept. Can anyone please explain the difference between shallow parsing and POS Tagging? Is shallow parsing often also called Shallow Semantic Parsing?</p>\n\n<p>Thanks before.</p>\n",
    "score": 52,
    "creation_date": 1327475231,
    "view_count": 25247,
    "answer_count": 5,
    "tags": "nlp;pos-tagger"
  },
  {
    "question_id": 61473330,
    "title": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling cublasCreate(handle)",
    "body": "<p>I got the following error when I ran my PyTorch deep learning model in Google Colab</p>\n<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in linear(input, weight, bias)\n   1370         ret = torch.addmm(bias, input, weight.t())\n   1371     else:\n-&gt; 1372         output = input.matmul(weight.t())\n   1373         if bias is not None:\n   1374             output += bias\n\nRuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\n</code></pre>\n<p>I even reduced batch size from 128 to 64 i.e., reduced to half,  but still, I got this error. Earlier, I ran the same code with a batch size of 128 but didn't get any error like this.</p>\n",
    "score": 52,
    "creation_date": 1588052382,
    "view_count": 140237,
    "answer_count": 11,
    "tags": "python;pytorch;nlp;cuda;bert-language-model"
  },
  {
    "question_id": 70161,
    "title": "How to read values from numbers written as words?",
    "body": "<p>As we all know numbers can be written either in numerics, or called by their names. While there are a lot of examples to be found that convert 123 into one hundred twenty three, I could not find good examples of how to convert it the other way around.</p>\n\n<p>Some of the caveats:</p>\n\n<ol>\n<li>cardinal/nominal or ordinal: \"one\" and \"first\"</li>\n<li>common spelling mistakes: \"forty\"/\"fourty\"</li>\n<li>hundreds/thousands: 2100 -> \"twenty one hundred\" and also \"two thousand and one hundred\"</li>\n<li>separators: \"eleven hundred fifty two\", but also \"elevenhundred fiftytwo\" or \"eleven-hundred fifty-two\" and whatnot</li>\n<li>colloquialisms: \"thirty-something\"</li>\n<li>fractions: 'one third', 'two fifths'</li>\n<li>common names: 'a dozen', 'half'</li>\n</ol>\n\n<p>And there are probably more caveats possible that are not yet listed.\nSuppose the algorithm needs to be very robust, and even understand spelling mistakes.</p>\n\n<p>What fields/papers/studies/algorithms should I read to learn how to write all this?\nWhere is the information?</p>\n\n<blockquote>\n  <p>PS: My final parser should actually understand 3 different languages, English, Russian and Hebrew. And maybe at a later stage more languages will be added. Hebrew also has male/female numbers, like \"one man\" and \"one woman\" have a different \"one\" — \"ehad\" and \"ahat\". Russian also has some of its own complexities.</p>\n</blockquote>\n\n<p>Google does a great job at this. For example:</p>\n\n<p><a href=\"http://www.google.com/search?q=two+thousand+and+one+hundred+plus+five+dozen+and+four+fifths+in+decimal\" rel=\"noreferrer\">http://www.google.com/search?q=two+thousand+and+one+hundred+plus+five+dozen+and+four+fifths+in+decimal</a></p>\n\n<p>(the reverse is also possible <a href=\"http://www.google.com/search?q=999999999999+in+english\" rel=\"noreferrer\">http://www.google.com/search?q=999999999999+in+english</a>)</p>\n",
    "score": 51,
    "creation_date": 1221551256,
    "view_count": 16251,
    "answer_count": 12,
    "tags": "algorithm;language-agnostic;parsing;numbers;nlp"
  },
  {
    "question_id": 2661778,
    "title": "tag generation from a text content",
    "body": "<p>I am curious if there is an algorithm/method exists to generate keywords/tags from a given text, by using some weight calculations, occurrence ratio or other tools.</p>\n\n<p>Additionally, I will be grateful if you point any Python based solution / library for this.</p>\n\n<p>Thanks</p>\n",
    "score": 51,
    "creation_date": 1271583563,
    "view_count": 34609,
    "answer_count": 5,
    "tags": "python;tags;machine-learning;nlp;nltk"
  },
  {
    "question_id": 37793118,
    "title": "Load Pretrained glove vectors in python",
    "body": "<p>I have downloaded pretrained glove vector file from the internet. It is a .txt file. I am unable to load and access it. It is easy to load and access a word vector binary file using gensim but I don't know how to do it when it is a text file format.</p>\n",
    "score": 50,
    "creation_date": 1465830078,
    "view_count": 90749,
    "answer_count": 14,
    "tags": "python-2.7;vector;nlp"
  },
  {
    "question_id": 3753021,
    "title": "Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?",
    "body": "<p>Using NLTK and <a href=\"https://en.wikipedia.org/wiki/WordNet\" rel=\"noreferrer\">WordNet</a>, how do I convert simple tense verb into its present, past or past participle form?</p>\n\n<p><strong>For example:</strong></p>\n\n<p>I want to write a function which would give me verb in expected form as follows.</p>\n\n<pre><code>v = 'go'\npresent = present_tense(v)\nprint present # prints \"going\"\n\npast = past_tense(v)\nprint past # prints \"went\"\n</code></pre>\n",
    "score": 50,
    "creation_date": 1284996990,
    "view_count": 39632,
    "answer_count": 4,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 53248838,
    "title": "Definition of downstream tasks in NLP",
    "body": "<p>What does downstream tasks terminology mean in NLP? I saw this terminology used in several articles but I can't understand the idea behind it.</p>\n",
    "score": 50,
    "creation_date": 1541939917,
    "view_count": 23696,
    "answer_count": 2,
    "tags": "nlp"
  },
  {
    "question_id": 11333903,
    "title": "NLTK Named Entity Recognition with Custom Data",
    "body": "<p>I'm trying to extract named entities from my text using NLTK. I find that NLTK NER is not very accurate for my purpose and I want to add some more tags of my own as well. I've been trying to find a way to train my own NER, but I don't seem to be able to find the right resources. \nI have a couple of questions regarding NLTK-</p>\n\n<ol>\n<li>Can I use my own data to train an Named Entity Recognizer in NLTK?</li>\n<li>If I can train using my own data, is the named_entity.py the file to be modified?</li>\n<li>Does the input file format have to be in IOB eg. Eric NNP B-PERSON ?</li>\n<li>Are there any resources - apart from the nltk cookbook and nlp with python that I can use?</li>\n</ol>\n\n<p>I would really appreciate help in this regard</p>\n",
    "score": 50,
    "creation_date": 1341426248,
    "view_count": 29921,
    "answer_count": 6,
    "tags": "python;nlp;nltk;named-entity-recognition"
  },
  {
    "question_id": 65703260,
    "title": "Computational Complexity of Self-Attention in the Transformer Model",
    "body": "<p>I recently went through the <a href=\"https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\" rel=\"noreferrer\">Transformer</a> paper from Google Research describing how self-attention layers could completely replace traditional RNN-based sequence encoding layers for machine translation. In Table 1 of the paper, the authors compare the computational complexities of different sequence encoding layers, and state (later on) that self-attention layers are faster than RNN layers when the sequence length <code>n</code> is smaller than the dimension of the vector representations <code>d</code>.</p>\n<p>However, the self-attention layer seems to have an inferior complexity than claimed if my understanding of the computations is correct. Let <code>X</code> be the input to a self-attention layer. Then, <code>X</code> will have shape <code>(n, d)</code> since there are <code>n</code> word-vectors (corresponding to rows) each of dimension <code>d</code>. Computing the output of self-attention requires the following steps (consider single-headed self-attention for simplicity):</p>\n<ol>\n<li>Linearly transforming the rows of <code>X</code> to compute the query <code>Q</code>, key <code>K</code>, and value <code>V</code> matrices, each of which has shape <code>(n, d)</code>. This is accomplished by post-multiplying <code>X</code> with 3 learned matrices of shape <code>(d, d)</code>, amounting to a computational complexity of <code>O(n d^2)</code>.</li>\n<li>Computing the layer output, specified in Equation 1 of the paper as <code>SoftMax(Q Kt / sqrt(d)) V</code>, where the softmax is computed over each row. Computing <code>Q Kt</code> has complexity <code>O(n^2 d)</code>, and post-multiplying the resultant with <code>V</code> has complexity <code>O(n^2 d)</code> as well.</li>\n</ol>\n<p>Therefore, the total complexity of the layer is <code>O(n^2 d + n d^2)</code>, which is worse than that of a traditional RNN layer. I obtained the same result for multi-headed attention too, on considering the appropriate intermediate representation dimensions (<code>dk</code>, <code>dv</code>) and finally multiplying by the number of heads <code>h</code>.</p>\n<p>Why have the authors ignored the cost of computing the Query, Key, and Value matrices while reporting total computational complexity?</p>\n<p>I understand that the proposed layer is fully parallelizable across the <code>n</code> positions, but I believe that Table 1 does not take this into account anyway.</p>\n",
    "score": 50,
    "creation_date": 1610545630,
    "view_count": 52569,
    "answer_count": 5,
    "tags": "machine-learning;deep-learning;neural-network;nlp;artificial-intelligence"
  },
  {
    "question_id": 25145552,
    "title": "TFIDF for Large Dataset",
    "body": "<p>I have a corpus which has around 8 million news articles, I need to get the TFIDF representation of them as a sparse matrix. I have been able to do that using scikit-learn for relatively lower number of samples, but I believe it can't be used for such a huge dataset as it loads the input matrix into memory first and that's an expensive process.</p>\n\n<p>Does anyone know, what would be the best way to extract out the TFIDF vectors for large datasets?</p>\n",
    "score": 49,
    "creation_date": 1407262149,
    "view_count": 36140,
    "answer_count": 4,
    "tags": "python;lucene;nlp;scikit-learn;tf-idf"
  },
  {
    "question_id": 3920759,
    "title": "Unsupervised Sentiment Analysis",
    "body": "<p>I've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work.</p>\n\n<p>My question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic?</p>\n",
    "score": 48,
    "creation_date": 1286943942,
    "view_count": 33343,
    "answer_count": 7,
    "tags": "machine-learning;nlp;sentiment-analysis"
  },
  {
    "question_id": 3227524,
    "title": "How to detect language of user entered text?",
    "body": "<p>I am dealing with an application that is accepting user input in different languages (currently 3 languages fixed). The requirement is that users can enter text and dont bother to select the language via a provided checkbox in the UI.</p>\n\n<p>Is there an <strong>existing Java library</strong> to detect the language of a text?</p>\n\n<p>I want something like this:</p>\n\n<pre><code>text = \"To be or not to be thats the question.\"\n\n// returns ISO 639 Alpha-2 code\nlanguage = detect(text);\n\nprint(language);\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>EN\n</code></pre>\n\n<p><strong>I dont want to know how to create a language detector by myself</strong> (i have seen plenty of blogs trying to do that). The library should provide a simple APi and also work completely offline. Open-source or commercial closed doesn't matter.</p>\n\n<p>i also found this questions on SO (and a few more):</p>\n\n<p><a href=\"https://stackoverflow.com/questions/3173005\">How to detect language</a><br>\n<a href=\"https://stackoverflow.com/questions/2752691\">How to detect language of text?</a></p>\n",
    "score": 46,
    "creation_date": 1278929248,
    "view_count": 64828,
    "answer_count": 7,
    "tags": "java;nlp;language-detection"
  },
  {
    "question_id": 27470670,
    "title": "How to use Gensim doc2vec with pre-trained word vectors?",
    "body": "<p>I recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec?</p>\n\n<p>Or is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training?</p>\n\n<p>Thanks.</p>\n",
    "score": 46,
    "creation_date": 1418570023,
    "view_count": 42376,
    "answer_count": 4,
    "tags": "python;nlp;gensim;word2vec;doc2vec"
  },
  {
    "question_id": 7455188,
    "title": "Entity Extraction/Recognition with free tools while feeding Lucene Index",
    "body": "<p>I'm currently investigating the options to extract person names, locations, tech words and categories from text (a lot articles from the web) which will then feeded into a Lucene/ElasticSearch index. The additional information is then added as metadata and should increase precision of the search. </p>\n\n<p>E.g. when someone queries 'wicket' he should be able to decide whether he means the cricket sport or the Apache project. I tried to implement this on my own with minor success so far. Now I found a lot tools, but I'm not sure if they are suited for this task and which of them integrates good with Lucene or if precision of entity extraction is high enough.</p>\n\n<ul>\n<li><a href=\"http://dbpedia.org/spotlight\" rel=\"noreferrer\">Dbpedia Spotlight</a>, the <a href=\"http://spotlight.dbpedia.org/demo/index.xhtml\" rel=\"noreferrer\">demo</a> looks very promising</li>\n<li><a href=\"http://incubator.apache.org/opennlp/index.html\" rel=\"noreferrer\">OpenNLP</a> requires <a href=\"https://stackoverflow.com/questions/6952512/how-i-train-an-named-entity-reconigzer-identifier-in-opennlp\">training</a>. Which training data to use?</li>\n<li><a href=\"http://opennlp.sourceforge.net/projects.html\" rel=\"noreferrer\">OpenNLP tools</a></li>\n<li><a href=\"http://incubator.apache.org/stanbol/\" rel=\"noreferrer\">Stanbol</a></li>\n<li><a href=\"http://www.nltk.org/download\" rel=\"noreferrer\">NLTK</a></li>\n<li><a href=\"http://balie.sourceforge.net/\" rel=\"noreferrer\">balie</a></li>\n<li><a href=\"http://uima.apache.org/\" rel=\"noreferrer\">UIMA</a></li>\n<li><a href=\"http://gate.ac.uk/\" rel=\"noreferrer\">GATE</a> -> <a href=\"http://gate.ac.uk/wiki/code-repository/\" rel=\"noreferrer\">example code</a></li>\n<li><a href=\"http://mahout.apache.org/\" rel=\"noreferrer\">Apache Mahout</a></li>\n<li><a href=\"http://nlp.stanford.edu/software/CRF-NER.shtml\" rel=\"noreferrer\">Stanford CRF-NER</a></li>\n<li><a href=\"http://code.google.com/p/maui-indexer\" rel=\"noreferrer\">maui-indexer</a></li>\n<li><a href=\"http://mallet.cs.umass.edu/\" rel=\"noreferrer\">Mallet</a></li>\n<li><a href=\"http://cogcomp.cs.illinois.edu/page/software_view/4\" rel=\"noreferrer\">Illinois Named Entity Tagger</a> Not open source but free</li>\n<li><a href=\"http://code.google.com/p/wikipedianerdata/source/browse/trunk/src/main/entityExtractor/EntityExtractionManager.java?r=2\" rel=\"noreferrer\">wikipedianer data</a></li>\n</ul>\n\n<p><strong>My questions:</strong></p>\n\n<ul>\n<li>Does anyone have experience with some of the listed tools above and its precision/recall? Or if there is training data required + available.</li>\n<li>Are there articles or tutorials where I can get started with entity extraction(NER) for each and every tool?</li>\n<li>How can they be integrated with Lucene?</li>\n</ul>\n\n<p>Here are some questions related to that subject:</p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/5544475/does-an-algorithm-exist-to-help-detect-the-primary-topic-of-an-english-sentence\">Does an algorithm exist to help detect the &quot;primary topic&quot; of an English sentence?</a></li>\n<li><a href=\"https://stackoverflow.com/questions/188176/named-entity-recognition-libraries-for-java\">Named Entity Recognition Libraries for Java</a></li>\n<li><a href=\"https://stackoverflow.com/questions/5571519/named-entity-recognition-with-java\">Named entity recognition with Java</a></li>\n</ul>\n",
    "score": 46,
    "creation_date": 1316266973,
    "view_count": 18124,
    "answer_count": 4,
    "tags": "lucene;nlp;semantic-web;mahout;opennlp"
  },
  {
    "question_id": 2233435,
    "title": "Machine Learning and Natural Language Processing",
    "body": "<p>Assume you know a student who wants to study Machine Learning and Natural Language Processing.</p>\n\n<p>What specific computer science subjects should they focus on and which programming languages are specifically designed to solve these types of problems?</p>\n\n<p>I am not looking for your favorite subjects and tools, but rather industry standards.</p>\n\n<p><b>Example</b>: I'm guessing that knowing Prolog and Matlab might help them.  They also might want to study Discrete Structures*, Calculus, and Statistics.</p>\n\n<p>*Graphs and trees. Functions: properties, recursive definitions, solving recurrences. Relations: properties, equivalence, partial order. Proof techniques, inductive proof. Counting techniques and discrete probability.  Logic: propositional calculus, first-order predicate calculus. Formal reasoning: natural deduction, resolution. Applications to program correctness and automatic reasoning. Introduction to algebraic structures in computing.</p>\n",
    "score": 45,
    "creation_date": 1265759645,
    "view_count": 16021,
    "answer_count": 9,
    "tags": "math;machine-learning;nlp"
  },
  {
    "question_id": 1643616,
    "title": "Algorithms to detect phrases and keywords from text",
    "body": "<p>I have around 100 megabytes of text, without any markup, divided to approximately 10,000 entries. I would like to automatically generate a 'tag' list. The problem is that there are word groups (i.e. phrases) that only make sense when they are grouped together.</p>\n\n<p>If I just count the words, I get a large number of really common words (is, the, for, in, am, etc.). I have counted the words and the number of other words that are before and after it, but now I really cannot figure out what to do next The information relating to the 2 and 3 word phrases is present, but how do I extract this data?</p>\n",
    "score": 45,
    "creation_date": 1256821867,
    "view_count": 35119,
    "answer_count": 5,
    "tags": "algorithm;nlp;text-processing"
  },
  {
    "question_id": 42781292,
    "title": "Doc2Vec Get most similar documents",
    "body": "<p>I am trying to build a document retrieval model that returns most documents ordered by their relevancy with respect to a query or a search string. For this I trained a doc2vec model using the <code>Doc2Vec</code> model in gensim. My dataset is in the form of a pandas dataset which has each document stored as a string on each line. This is the code I have so far</p>\n\n<pre><code>import gensim, re\nimport pandas as pd\n\n# TOKENIZER\ndef tokenizer(input_string):\n    return re.findall(r\"[\\w']+\", input_string)\n\n# IMPORT DATA\ndata = pd.read_csv('mp_1002_prepd.txt')\ndata.columns = ['merged']\ndata.loc[:, 'tokens'] = data.merged.apply(tokenizer)\nsentences= []\nfor item_no, line in enumerate(data['tokens'].values.tolist()):\n    sentences.append(LabeledSentence(line,[item_no]))\n\n# MODEL PARAMETERS\ndm = 1 # 1 for distributed memory(default); 0 for dbow \ncores = multiprocessing.cpu_count()\nsize = 300\ncontext_window = 50\nseed = 42\nmin_count = 1\nalpha = 0.5\nmax_iter = 200\n\n# BUILD MODEL\nmodel = gensim.models.doc2vec.Doc2Vec(documents = sentences,\ndm = dm,\nalpha = alpha, # initial learning rate\nseed = seed,\nmin_count = min_count, # ignore words with freq less than min_count\nmax_vocab_size = None, # \nwindow = context_window, # the number of words before and after to be used as context\nsize = size, # is the dimensionality of the feature vector\nsample = 1e-4, # ?\nnegative = 5, # ?\nworkers = cores, # number of cores\niter = max_iter # number of iterations (epochs) over the corpus)\n\n# QUERY BASED DOC RANKING ??\n</code></pre>\n\n<p>The part where I am struggling is in finding documents that are most similar/relevant to the query. I used the <code>infer_vector</code> but then realised that it considers the query as a document, updates the model and returns the results. I tried using the <code>most_similar</code> and <code>most_similar_cosmul</code> methods but I get words along with a similarity score(I guess) in return. What I want to do is when I enter a search string(a query), I should get the documents (ids) that are most relevant along with a similarity score(cosine etc). How do I get this part done?</p>\n",
    "score": 44,
    "creation_date": 1489481006,
    "view_count": 40305,
    "answer_count": 1,
    "tags": "python;nlp;gensim;doc2vec"
  },
  {
    "question_id": 3182268,
    "title": "NLTK and language detection",
    "body": "<p>How do I detect what language a text is written in using NLTK?</p>\n\n<p>The examples I've seen use <code>nltk.detect</code>, but when I've installed it on my mac, I cannot find this package.</p>\n",
    "score": 43,
    "creation_date": 1278365432,
    "view_count": 60908,
    "answer_count": 5,
    "tags": "python;nlp;nltk;detection"
  },
  {
    "question_id": 35716121,
    "title": "How to extract phrases from corpus using gensim",
    "body": "<p>For preprocessing the corpus I was planing to extarct common phrases from the corpus, for this I tried using <strong>Phrases</strong> model in gensim, I tried below code but it's not giving me desired output.</p>\n\n<p><strong>My code</strong></p>\n\n<pre><code>from gensim.models import Phrases\ndocuments = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\"]\n\nsentence_stream = [doc.split(\" \") for doc in documents]\nbigram = Phrases(sentence_stream)\nsent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']\nprint(bigram[sent])\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>[u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']\n</code></pre>\n\n<p><strong>But it should come as</strong> </p>\n\n<pre><code>[u'the', u'mayor', u'of', u'new_york', u'was', u'there']\n</code></pre>\n\n<p>But when I tried to print vocab of train data, I can see bigram, but its not working with test data, where I am going wrong?</p>\n\n<pre><code>print bigram.vocab\n\ndefaultdict(&lt;type 'int'&gt;, {'useful': 1, 'was_there': 1, 'learning_can': 1, 'learning': 1, 'of_new': 1, 'can_be': 1, 'mayor': 1, 'there': 1, 'machine': 1, 'new': 1, 'was': 1, 'useful_sometimes': 1, 'be': 1, 'mayor_of': 1, 'york_was': 1, 'york': 1, 'machine_learning': 1, 'the_mayor': 1, 'new_york': 1, 'of': 1, 'sometimes': 1, 'can': 1, 'be_useful': 1, 'the': 1}) \n</code></pre>\n",
    "score": 43,
    "creation_date": 1456813821,
    "view_count": 37778,
    "answer_count": 1,
    "tags": "python;nlp;gensim"
  },
  {
    "question_id": 35275001,
    "title": "Use of PunktSentenceTokenizer in NLTK",
    "body": "<p>I am learning Natural Language Processing using NLTK.\nI came across the code using <code>PunktSentenceTokenizer</code> whose actual use I cannot understand in the given code. The code is given :</p>\n\n<pre><code>import nltk\nfrom nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\n\ntrain_text = state_union.raw(\"2005-GWBush.txt\")\nsample_text = state_union.raw(\"2006-GWBush.txt\")\n\ncustom_sent_tokenizer = PunktSentenceTokenizer(train_text) #A\n\ntokenized = custom_sent_tokenizer.tokenize(sample_text)   #B\n\ndef process_content():\ntry:\n    for i in tokenized[:5]:\n        words = nltk.word_tokenize(i)\n        tagged = nltk.pos_tag(words)\n        print(tagged)\n\nexcept Exception as e:\n    print(str(e))\n\n\nprocess_content()\n</code></pre>\n\n<p>So, why do we use PunktSentenceTokenizer.  And what is going on in the line marked A and B. I mean there is a training text and the other a sample text, but what is the need for two data sets to get the Part of Speech tagging.</p>\n\n<p>Line marked as <code>A</code> and <code>B</code> is which I am not able to understand.</p>\n\n<p>PS : I did try to look in the NLTK book but could not understand what is the real use of PunktSentenceTokenizer</p>\n",
    "score": 43,
    "creation_date": 1454950547,
    "view_count": 57566,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 212219,
    "title": "What are good starting points for someone interested in natural language processing?",
    "body": "<h1>Question</h1>\n\n<p>So I've recently came up with some new possible projects that would have to deal with deriving 'meaning' from text submitted and generated by users.</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Natural_language_processing\" rel=\"nofollow noreferrer\">Natural language processing</a> is the field that deals with these kinds of issues, and after some initial research I found the <a href=\"http://opennlp.sourceforge.net/\" rel=\"nofollow noreferrer\">OpenNLP Hub</a> and university collaborations like the <a href=\"http://attempto.ifi.uzh.ch/site/\" rel=\"nofollow noreferrer\">attempto project</a>. And stackoverflow has <a href=\"https://stackoverflow.com/questions/88984/your-favorite-natural-language-parser\">this</a>.</p>\n\n<p>If anyone could link me to some good resources, from reseach papers and introductionary texts to apis, I'd be happier than a 6 year-old kid opening his christmas presents!</p>\n\n<h1>Update</h1>\n\n<p>Through one of your recommendations I've found <a href=\"http://www.opencyc.org/\" rel=\"nofollow noreferrer\">opencyc</a> (<em>'the world's largest and most complete general knowledge base and commonsense reasoning engine'</em>). Even more amazing still, there's a project that is a distilled version of opencyc called <a href=\"http://umbel.org/\" rel=\"nofollow noreferrer\">UMBEL</a>. It features semantic data in rdf/owl/skos n3 syntax.</p>\n\n<p>I've also stumbled upon <a href=\"http://antlr.org/\" rel=\"nofollow noreferrer\">antlr</a>, a parser generator for <em>'constructing recognizers, interpreters, compilers, and translators from grammatical descriptions'</em>.</p>\n\n<p>And there's a question on here by me, that lists tons of <a href=\"https://stackoverflow.com/questions/202092/where-can-i-find-free-and-open-data\">free and open data</a>.</p>\n\n<p>Thanks stackoverflow community!</p>\n",
    "score": 43,
    "creation_date": 1224251571,
    "view_count": 8733,
    "answer_count": 10,
    "tags": "nlp;dcg"
  },
  {
    "question_id": 42821330,
    "title": "Restore original text from Keras’s imdb dataset",
    "body": "<p>Restore original text from Keras’s imdb dataset</p>\n\n<p>I want to restore imdb’s original text from Keras’s imdb dataset.</p>\n\n<p>First, when I load Keras’s imdb dataset, it returned sequence of word index.</p>\n\n<p>\n\n<pre><code>&gt;&gt;&gt; (X_train, y_train), (X_test, y_test) = imdb.load_data()\n&gt;&gt;&gt; X_train[0]\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n</code></pre>\n\n<p>I found imdb.get_word_index method(), it returns word index dictionary like {‘create’: 984, ‘make’: 94,…}. For converting, I create index word dictionary.\n\n\n<pre><code>&gt;&gt;&gt; word_index = imdb.get_word_index()\n&gt;&gt;&gt; index_word = {v:k for k,v in word_index.items()}\n</code></pre>\n\n<p>Then, I tried to restore original text like following.</p>\n\n<p>\n\n<pre><code>&gt;&gt;&gt; ' '.join(index_word.get(w) for w in X_train[5])\n\"the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i\"\n</code></pre>\n\n<p>I’m not good at English, but I know this sentence is something strange.</p>\n\n<p>Why is this happened? How can I restore original text?</p>\n",
    "score": 42,
    "creation_date": 1489614559,
    "view_count": 17201,
    "answer_count": 9,
    "tags": "python;machine-learning;neural-network;nlp;keras"
  },
  {
    "question_id": 25534214,
    "title": "NLTK WordNet Lemmatizer: Shouldn&#39;t it lemmatize all inflections of a word?",
    "body": "<p>I'm using the NLTK WordNet Lemmatizer for a Part-of-Speech tagging project by first modifying each word in the training corpus to its stem (in place modification), and then training only on the new corpus. However, I found that the lemmatizer is not functioning as I expected it to.</p>\n\n<p>For example, the word <code>loves</code> is lemmatized to <code>love</code> which is correct, but the word <code>loving</code> remains <code>loving</code> even after lemmatization. Here <code>loving</code> is as in the sentence \"I'm loving it\".</p>\n\n<p>Isn't <code>love</code> the stem of the inflected word <code>loving</code>? Similarly, many other 'ing' forms remain as they are after lemmatization. Is this the correct behavior?</p>\n\n<p>What are some other lemmatizers that are accurate? (need not be in NLTK) Are there morphology analyzers or lemmatizers that also take into account a word's Part Of Speech tag, in deciding the word stem? For example, the word <code>killing</code> should have <code>kill</code> as the stem if <code>killing</code> is used as a verb, but it should have <code>killing</code> as the stem if it is used as a noun (as in <code>the killing was done by xyz</code>).</p>\n",
    "score": 41,
    "creation_date": 1409163012,
    "view_count": 62845,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 30431688,
    "title": "How to connect Cortana commands to custom scripts?",
    "body": "<p>This may be a little early to ask this, but I'm running Windows 10 Technical Preview Build 10122. I'd like to set up Cortana to have custom commands. Here's how she works:</p>\n\n<pre><code>Hey Cortana, &lt;she'll listen and process this command&gt;\n</code></pre>\n\n<p>Microsoft will process the command and if there isn't anything for it, she'll just search the input on bing. However, I'd like to be able to say something like, just for example </p>\n\n<pre><code>Hey Cortana, I'm going to bed now\n</code></pre>\n\n<p>And have the input <code>I'm going to bed now</code> trigger run a batch script, a VBScript, a command, or any some sort some of custom response that basically does the following.</p>\n\n<pre><code>C:\\&gt; shutdown -s\n</code></pre>\n\n<p>Is there a way to set up a predefined custom commands for Cortana?</p>\n\n<p><strong>Update:</strong></p>\n\n<p>I created this <a href=\"https://youtu.be/GICF03UAOcQ\" rel=\"noreferrer\"><strong>basic YouTube tutorial</strong></a> and <a href=\"https://youtu.be/GICF03UAOcQ\" rel=\"noreferrer\"><strong>this more advanced one</strong></a> with a corresponding <a href=\"https://github.com/crclayton/custom-cortana-commands-template\" rel=\"noreferrer\"><strong>GitHub repo</strong></a> based on talkitbr's excellent and very helpful answer <a href=\"https://stackoverflow.com/a/31387719/2374028\">below</a>. </p>\n\n<p>At first his answer was beyond my understanding so I decided to break it down in a bit more detail for future users like myself.</p>\n",
    "score": 41,
    "creation_date": 1432531317,
    "view_count": 35384,
    "answer_count": 2,
    "tags": "scripting;nlp;windows-10;cortana"
  },
  {
    "question_id": 14095971,
    "title": "How to tweak the NLTK sentence tokenizer",
    "body": "<p>I'm using NLTK to analyze a few classic texts and I'm running in to trouble tokenizing the text by sentence. For example, here's what I get for a snippet from <em><a href=\"http://www.gutenberg.org/cache/epub/2701/pg2701.txt\">Moby Dick</a></em>:</p>\n\n<pre><code>import nltk\nsent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n\n'''\n(Chapter 16)\nA clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but\nthat's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?\"\n'''\nsample = 'A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but that\\'s a rather cold and clammy reception in the winter time, ain\\'t it, Mrs. Hussey?\"'\n\nprint \"\\n-----\\n\".join(sent_tokenize.tokenize(sample))\n'''\nOUTPUT\n\"A clam for supper?\n-----\na cold clam; is THAT what you mean, Mrs.\n-----\nHussey?\n-----\n\" says I, \"but that\\'s a rather cold and clammy reception in the winter time, ain\\'t it, Mrs.\n-----\nHussey?\n-----\n\"\n'''\n</code></pre>\n\n<p>I don't expect perfection here, considering that Melville's syntax is a bit dated, but NLTK ought to be able to handle terminal double quotes and titles like \"Mrs.\" Since the tokenizer is the result of an unsupervised training algo, however, I can't figure out how to tinker with it.</p>\n\n<p>Anyone have recommendations for a better sentence tokenizer? I'd prefer a simple heuristic that I can hack rather than having to train my own parser. </p>\n",
    "score": 40,
    "creation_date": 1356911940,
    "view_count": 24927,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 1003326,
    "title": "Is there a natural language parser for date/times in javascript?",
    "body": "<p>Is there a natural language parser for date/times in javascript?</p>\n",
    "score": 40,
    "creation_date": 1245178480,
    "view_count": 19797,
    "answer_count": 7,
    "tags": "javascript;datetime;nlp"
  },
  {
    "question_id": 7443330,
    "title": "How do I do dependency parsing in NLTK?",
    "body": "<p>Going through the NLTK book, it's not clear how to generate a dependency tree from a given sentence.</p>\n\n<p>The relevant section of the book: <a href=\"https://www.nltk.org/book/ch08.html#dependencies-and-dependency-grammar\" rel=\"noreferrer\">sub-chapter on dependency grammar</a> gives an <a href=\"https://www.nltk.org/book/ch08.html#fig-depgraph0\" rel=\"noreferrer\">example figure</a> but it doesn't show how to parse a sentence to come up with those relationships - or maybe I'm missing something fundamental in NLP?</p>\n\n<p><strong>EDIT:</strong>\nI want something similar to what the <a href=\"http://nlp.stanford.edu:8080/parser/\" rel=\"noreferrer\">stanford parser</a> does:\nGiven a sentence \"I shot an elephant in my sleep\", it should return something like:</p>\n\n<pre><code>nsubj(shot-2, I-1)\ndet(elephant-4, an-3)\ndobj(shot-2, elephant-4)\nprep(shot-2, in-5)\nposs(sleep-7, my-6)\npobj(in-5, sleep-7)\n</code></pre>\n",
    "score": 39,
    "creation_date": 1316168809,
    "view_count": 53706,
    "answer_count": 7,
    "tags": "python;nlp;grammar;nltk"
  },
  {
    "question_id": 309884,
    "title": "Code Golf: Number to Words",
    "body": "<p>The code golf series seem to be fairly popular.  I ran across some code that converts a number to its word representation.  Some examples would be (powers of 2 for programming fun):</p>\n\n<ul>\n<li>2 -> Two</li>\n<li>1024 -> One Thousand Twenty Four</li>\n<li>1048576 -> One Million Forty Eight Thousand Five Hundred Seventy Six</li>\n</ul>\n\n<p>The algorithm my co-worker came up was almost two hundred lines long.  Seems like there would be a more concise way to do it.</p>\n\n<p>Current guidelines:</p>\n\n<ul>\n<li>Submissions in any <strong>programming</strong> language welcome (I apologize to\nPhiLho for the initial lack of clarity on this one)</li>\n<li>Max input of 2^64 (see following link for words, thanks mmeyers)</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Long_and_short_scales\" rel=\"nofollow noreferrer\">Short scale</a> with English output preferred, but any algorithm is welcome.  Just comment along with the programming language as to the method used.</li>\n</ul>\n",
    "score": 38,
    "creation_date": 1227295539,
    "view_count": 13180,
    "answer_count": 22,
    "tags": "language-agnostic;nlp;code-golf;rosetta-stone"
  },
  {
    "question_id": 35857519,
    "title": "Efficiently count word frequencies in python",
    "body": "<p>I'd like to count frequencies of all words in a text file.</p>\n\n<pre><code>&gt;&gt;&gt; countInFile('test.txt')\n</code></pre>\n\n<p>should return <code>{'aaa':1, 'bbb': 2, 'ccc':1}</code> if the target text file is like:</p>\n\n<pre><code># test.txt\naaa bbb ccc\nbbb\n</code></pre>\n\n<p>I've implemented it with pure python following <a href=\"https://stackoverflow.com/questions/12117576/how-to-count-word-frequencies-within-a-file-in-python\">some posts</a>. However, I've found out pure-python ways are insufficient due to huge file size (> 1GB).</p>\n\n<p>I think borrowing sklearn's power is a candidate.</p>\n\n<p>If you let CountVectorizer count frequencies for each line, I guess you will get word frequencies by summing up each column. But, it sounds a bit indirect way.</p>\n\n<p>What is the most efficient and straightforward way to count words in a file with python?</p>\n\n<h3>Update</h3>\n\n<p>My (very slow) code is here:</p>\n\n<pre><code>from collections import Counter\n\ndef get_term_frequency_in_file(source_file_path):\n    wordcount = {}\n    with open(source_file_path) as f:\n        for line in f:\n            line = line.lower().translate(None, string.punctuation)\n            this_wordcount = Counter(line.split())\n            wordcount = add_merge_two_dict(wordcount, this_wordcount)\n    return wordcount\n\ndef add_merge_two_dict(x, y):\n    return { k: x.get(k, 0) + y.get(k, 0) for k in set(x) | set(y) }\n</code></pre>\n",
    "score": 38,
    "creation_date": 1457401941,
    "view_count": 39547,
    "answer_count": 8,
    "tags": "python;nlp;scikit-learn;word-count;frequency-distribution"
  },
  {
    "question_id": 44238154,
    "title": "What is the difference between Luong attention and Bahdanau attention?",
    "body": "<p>These two attentions are used in <strong>seq2seq</strong> modules. The two different attentions are introduced as multiplicative and additive attentions in <a href=\"https://www.tensorflow.org/versions/master/api_guides/python/contrib.seq2seq\" rel=\"noreferrer\">this</a> TensorFlow documentation. What is the difference?</p>\n",
    "score": 38,
    "creation_date": 1496047417,
    "view_count": 38473,
    "answer_count": 5,
    "tags": "tensorflow;deep-learning;nlp;attention-model"
  },
  {
    "question_id": 1783653,
    "title": "Computing precision and recall in Named Entity Recognition",
    "body": "<p>Now I am about to report the results from Named Entity Recognition. One thing that I find a bit confusing is that my understanding of precision and recall was that one simply sums up true positives, true negatives, false positives and false negatives over all classes.</p>\n\n<p>But this seems implausible now that I think of it as each misclassification would give simultaneously rise to one false positive and one false negative (e.g. a token that should have been labelled as \"A\" but was labelled as \"B\" is a false negative for \"A\" and false positive for \"B\"). Thus the number of the false positives and the false negatives over all classes would be the same which means that precision is (always!) equal to recall. This simply can't be true so there is an error in my reasoning and I wonder where it is. It is certainly something quite obvious and straight-forward but it escapes me right now.</p>\n",
    "score": 38,
    "creation_date": 1258988444,
    "view_count": 21585,
    "answer_count": 7,
    "tags": "nlp;precision-recall"
  },
  {
    "question_id": 65431837,
    "title": "Transformers v4.x: Convert slow tokenizer to fast tokenizer",
    "body": "<p>I'm following the transformer's pretrained model <a href=\"https://huggingface.co/joeddav/xlm-roberta-large-xnli?text=%0A&amp;candidate_labels=&amp;multi_class=true\" rel=\"noreferrer\">xlm-roberta-large-xnli</a> example</p>\n<pre><code>from transformers import pipeline\nclassifier = pipeline(&quot;zero-shot-classification&quot;,\n                      model=&quot;joeddav/xlm-roberta-large-xnli&quot;)\n</code></pre>\n<p>and I get the following error</p>\n<pre><code>ValueError: Couldn't instantiate the backend tokenizer from one of: (1) a `tokenizers` library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\n</code></pre>\n<p>I'm using Transformers version <code>'4.1.1'</code></p>\n",
    "score": 37,
    "creation_date": 1608763470,
    "view_count": 44604,
    "answer_count": 5,
    "tags": "python;nlp;huggingface-transformers;huggingface-tokenizers"
  },
  {
    "question_id": 76313592,
    "title": "import langchain =&gt; Error : TypeError: issubclass() arg 1 must be a class",
    "body": "<p>I want to use langchain for my project.</p>\n<p>so I installed it using following command : <code>pip install langchain</code></p>\n<p>but While importing &quot;langchain&quot; I am facing following Error:</p>\n<pre><code>File /usr/lib/python3.8/typing.py:774, in _GenericAlias.__subclasscheck__(self, cls)\n    772 if self._special:\n    773     if not isinstance(cls, _GenericAlias):\n--&gt; 774         return issubclass(cls, self.__origin__)\n    775     if cls._special:\n    776         return issubclass(cls.__origin__, self.__origin__)\n\nTypeError: issubclass() arg 1 must be a class\n</code></pre>\n<p>Any one who can solve this error ?</p>\n",
    "score": 37,
    "creation_date": 1684836587,
    "view_count": 32545,
    "answer_count": 7,
    "tags": "python;nlp;data-science;chatbot;langchain"
  },
  {
    "question_id": 23704510,
    "title": "How do I test whether an nltk resource is already installed on the machine running my code?",
    "body": "<p>I just started my first NLTK project and am confused about the proper setup. I need several resources like the Punkt Tokenizer and the maxent pos tagger. I myself downloaded them using the GUI <code>nltk.download()</code>. For my collaborators I of course want that this things get downloaded automatically. I haven't found any idiomatic code for that in the docu. </p>\n\n<p>Am I supposed to just put <code>nltk.data.load('tokenizers/punkt/english.pickle')</code> and their like into the code? Is this going to download the resources every time the script is run? Am I to provide feedback to the  user (i.e. my co-developers) of what is being downloaded and why this is taking so long? There MUST be gear out there that does the job, right? :)</p>\n\n<p>//Edit To explify my question: <br>\n<strong>How do I test whether an nltk resource (like the Punkt Tokenizer) is already installed on the machine running my code, and install it if it is not?</strong> </p>\n",
    "score": 37,
    "creation_date": 1400274223,
    "view_count": 21184,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 45126071,
    "title": "How to extract numbers (along with comparison adjectives or ranges)",
    "body": "<p>I am working on two NLP projects in Python, and both have a similar task to <strong>extract numerical values and comparison operators</strong> from sentences, like the following:</p>\n\n<pre><code>\"... greater than $10 ... \",\n\"... weight not more than 200lbs ...\",\n\"... height in 5-7 feets ...\",\n\"... faster than 30 seconds ... \"\n</code></pre>\n\n<p>I found two different approaches to solve this problem:</p>\n\n<ul>\n<li>using very complex regular expressions.</li>\n<li>using <a href=\"https://en.wikipedia.org/wiki/Named-entity_recognition\" rel=\"nofollow noreferrer\">Named Entity Recognition</a> (and some regexes, too).</li>\n</ul>\n\n<p>How can I parse numerical values out of such sentences? I assume this is a common task in NLP.</p>\n\n<hr>\n\n<p>The desired output would be something like:</p>\n\n<p><strong>Input:</strong></p>\n\n<blockquote>\n  <p>\"greater than $10\"</p>\n</blockquote>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>{'value': 10, 'unit': 'dollar', 'relation': 'gt', 'position': 3}\n</code></pre>\n",
    "score": 37,
    "creation_date": 1500189567,
    "view_count": 6690,
    "answer_count": 2,
    "tags": "python;regex;nlp;nltk;spacy"
  },
  {
    "question_id": 14097388,
    "title": "Can an algorithm detect sarcasm",
    "body": "<p>I was asked to write an algorithm to detect sarcasm but I came across a flaw (or what seems like one) in the logic.</p>\n<p>For example if a person says</p>\n<blockquote>\n<p>A: I love Justin Beiber. Do you like him to?</p>\n<p>B: Yeah. Sure. <i>I absolutely love him.</i></p>\n</blockquote>\n<p>Now this may be considered sarcasm or not and the only way to know seems to be to know if B is serious or not.</p>\n<p>(I wasn't supposed to be in depth. We were given a bunch of phrases and just were told that if these were in the sentence then it was sarcastic but I got interested?)</p>\n<p>Is there any way to work around this? Or are computers absolutely stuck when it comes to sarcasm?</p>\n<p>(I suppose it depends on the tone of the speaker but my input is text)</p>\n",
    "score": 37,
    "creation_date": 1356927672,
    "view_count": 10156,
    "answer_count": 4,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 13423919,
    "title": "Computing N Grams using Python",
    "body": "<p>I needed to compute the Unigrams,  BiGrams and Trigrams for a text file containing text like: </p>\n\n<p>\"Cystic fibrosis affects 30,000 children and young adults in the US alone\nInhaling the mists of salt water can reduce the pus and infection that fills the airways of cystic fibrosis sufferers, although side effects include a nasty coughing fit and a harsh taste. \nThat's the conclusion of two studies published in this week's issue of The New England Journal of Medicine.\"</p>\n\n<p>I started in Python and used the following code:</p>\n\n<pre><code>#!/usr/bin/env python\n# File: n-gram.py\ndef N_Gram(N,text):\nNList = []                      # start with an empty list\nif N&gt; 1:\n    space = \" \" * (N-1)         # add N - 1 spaces\n    text = space + text + space # add both in front and back\n# append the slices [i:i+N] to NList\nfor i in range( len(text) - (N - 1) ):\n    NList.append(text[i:i+N])\nreturn NList                    # return the list\n# test code\nfor i in range(5):\nprint N_Gram(i+1,\"text\")\n# more test code\nnList = N_Gram(7,\"Here is a lot of text to print\")\nfor ngram in iter(nList):\nprint '\"' + ngram + '\"'\n</code></pre>\n\n<p><a href=\"http://www.daniweb.com/software-development/python/threads/39109/generating-n-grams-from-a-word\" rel=\"noreferrer\">http://www.daniweb.com/software-development/python/threads/39109/generating-n-grams-from-a-word</a></p>\n\n<p>But it works for all the n-grams within a word, when I want it from between words as in CYSTIC and FIBROSIS or CYSTIC FIBROSIS. Can someone help me out as to how I can get this done? </p>\n",
    "score": 36,
    "creation_date": 1353097595,
    "view_count": 107773,
    "answer_count": 8,
    "tags": "python;nlp;nltk;n-gram"
  },
  {
    "question_id": 40325980,
    "title": "How is the Vader &#39;compound&#39; polarity score calculated in Python NLTK?",
    "body": "<p>I'm using the Vader SentimentAnalyzer to obtain the polarity scores. I used the probability scores for positive/negative/neutral before, but I just realized the \"compound\" score, ranging from -1 (most neg) to 1 (most pos) would provide a single measure of polarity. I wonder how the \"compound\" score computed. Is that calculated from the [pos, neu, neg] vector? </p>\n",
    "score": 36,
    "creation_date": 1477800952,
    "view_count": 61092,
    "answer_count": 2,
    "tags": "python;nlp;nltk;sentiment-analysis;vader"
  },
  {
    "question_id": 190775,
    "title": "Stemming algorithm that produces real words",
    "body": "<p>I need to take a paragraph of text and extract from it a list of \"tags\".  Most of this is quite straight forward. However I need some help now stemming the resulting word list to avoid duplicates. Example: Community / Communities</p>\n\n<p>I've used an implementation of Porter Stemmer algorithm (I'm writing in PHP by the way):</p>\n\n<p><a href=\"http://tartarus.org/~martin/PorterStemmer/php.txt\" rel=\"noreferrer\">http://tartarus.org/~martin/PorterStemmer/php.txt</a></p>\n\n<p>This works, up to a point, but doesn't return \"real\" words.  The example above is stemmed to \"commun\".</p>\n\n<p>I've tried \"Snowball\" (suggested within another Stack Overflow thread).</p>\n\n<p><a href=\"http://snowball.tartarus.org/demo.php\" rel=\"noreferrer\">http://snowball.tartarus.org/demo.php</a></p>\n\n<p>For my example (community / communities), Snowball stems to \"communiti\".</p>\n\n<p><strong>Question</strong></p>\n\n<p>Are there any other stemming algorithms that will do this? Has anyone else solved this problem?</p>\n\n<p><em>My current thinking is that I could use a stemming algorithm to avoid duplicates and then pick the shortest word I encounter to be the actual word to display.</em></p>\n",
    "score": 36,
    "creation_date": 1223635409,
    "view_count": 37295,
    "answer_count": 3,
    "tags": "php;nlp;stemming;snowball;porter-stemmer"
  },
  {
    "question_id": 30821188,
    "title": "Python NLTK pos_tag not returning the correct part-of-speech tag",
    "body": "<p>Having this:</p>\n\n<pre><code>text = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n</code></pre>\n\n<p>And running:</p>\n\n<pre><code>nltk.pos_tag(text)\n</code></pre>\n\n<p>I get:</p>\n\n<pre><code>[('The', 'DT'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n</code></pre>\n\n<p>This is incorrect. The tags for <code>quick brown lazy</code> in the sentence should be:</p>\n\n<pre><code>('quick', 'JJ'), ('brown', 'JJ') , ('lazy', 'JJ')\n</code></pre>\n\n<p>Testing this through their <a href=\"http://nlp.stanford.edu:8080/corenlp/process\">online tool</a> gives the same result; <code>quick</code>, <code>brown</code> and <code>fox</code> should be adjectives not nouns.</p>\n",
    "score": 36,
    "creation_date": 1434214348,
    "view_count": 21398,
    "answer_count": 3,
    "tags": "python;machine-learning;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 30746460,
    "title": "How to interpret scikit&#39;s learn confusion matrix and classification report?",
    "body": "<p>I have a sentiment analysis task, for this Im using this <a href=\"http://pastebin.com/ikbKQcsc\" rel=\"noreferrer\">corpus</a> the opinions have 5 classes (<code>very neg</code>, <code>neg</code>, <code>neu</code>, <code>pos</code>, <code>very pos</code>), from 1 to 5. So I do the classification as follows:</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ntfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True,\n                            sublinear_tf=False, ngram_range=(2,2))\nfrom sklearn.cross_validation import train_test_split, cross_val_score\n\nimport pandas as pd\n\ndf = pd.read_csv('/corpus.csv',\n                     header=0, sep=',', names=['id', 'content', 'label'])\n\nX = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n                                                    y, test_size=0.33)\n\n\nfrom sklearn.svm import SVC\nsvm_1 = SVC(kernel='linear')\nsvm_1.fit(X, y)\nsvm_1_prediction = svm_1.predict(X_test)\n</code></pre>\n\n<p>Then with the metrics I obtained the following confusion matrix and classification report, as follows:</p>\n\n<pre><code>print '\\nClasification report:\\n', classification_report(y_test, svm_1_prediction)\nprint '\\nConfussion matrix:\\n',confusion_matrix(y_test, svm_1_prediction)\n</code></pre>\n\n<p>Then, this is the result:</p>\n\n<pre><code>Clasification report:\n             precision    recall  f1-score   support\n\n          1       1.00      0.76      0.86        71\n          2       1.00      0.84      0.91        43\n          3       1.00      0.74      0.85        89\n          4       0.98      0.95      0.96       288\n          5       0.87      1.00      0.93       367\n\navg / total       0.94      0.93      0.93       858\n\n\nConfussion matrix:\n[[ 54   0   0   0  17]\n [  0  36   0   1   6]\n [  0   0  66   5  18]\n [  0   0   0 273  15]\n [  0   0   0   0 367]]\n</code></pre>\n\n<p>How can I interpret the above confusion matrix and classification report. I tried reading the <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\" rel=\"noreferrer\">documentation</a> and this <a href=\"https://stats.stackexchange.com/questions/95209/how-can-i-interpret-sklearn-confusion-matrix\">question</a>. But still can interpretate what happened here particularly with this data?. Wny this matrix is somehow \"diagonal\"?. By the other hand what means the recall, precision, f1score and support for this data?. What can I say about this data?. Thanks in advance guys</p>\n",
    "score": 36,
    "creation_date": 1433905922,
    "view_count": 50970,
    "answer_count": 3,
    "tags": "machine-learning;nlp;scikit-learn;svm;confusion-matrix"
  },
  {
    "question_id": 5479333,
    "title": "What are the available tools to summarize or simplify text?",
    "body": "<p>Is there any library, preferably in python but at least open source, that can summarize and or simplify natural-language text?</p>\n",
    "score": 36,
    "creation_date": 1301435189,
    "view_count": 31555,
    "answer_count": 7,
    "tags": "python;nlp;text-processing"
  },
  {
    "question_id": 14489309,
    "title": "Convert words between verb/noun/adjective forms",
    "body": "<p>i would like a python library function that translates/converts across different parts of speech. sometimes it should output multiple words (e.g. \"coder\" and \"code\" are both nouns from the verb \"to code\", one's the subject the other's the object)</p>\n\n<pre><code># :: String =&gt; List of String\nprint verbify('writer') # =&gt; ['write']\nprint nounize('written') # =&gt; ['writer']\nprint adjectivate('write') # =&gt; ['written']\n</code></pre>\n\n<p>i mostly care about verbs &lt;=> nouns, for a note taking program i want to write. i.e. i can write \"caffeine antagonizes A1\" or \"caffeine is an A1 antagonist\" and with some NLP it can figure out they mean the same thing. (i know that's not easy, and that it will take NLP that parses and doesn't just tag, but i want to hack up a prototype).</p>\n\n<p>similar questions ...\n<a href=\"https://stackoverflow.com/questions/7548479/converting-adjectives-and-adverbs-to-their-noun-forms\">Converting adjectives and adverbs to their noun forms</a>\n(this answer only stems down to the root POS. i want to go between POS.)</p>\n\n<p>ps called Conversion in linguistics <a href=\"http://en.wikipedia.org/wiki/Conversion_%28linguistics%29\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Conversion_%28linguistics%29</a></p>\n",
    "score": 36,
    "creation_date": 1358974907,
    "view_count": 29434,
    "answer_count": 5,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 4634787,
    "title": "FreqDist with NLTK",
    "body": "<p>The Python package <code>nltk</code> has the <a href=\"https://www.nltk.org/api/nltk.probability.html#nltk.probability.FreqDist\" rel=\"nofollow noreferrer\">FreqDist</a> function which gives you the frequency of words within a text. I am trying to pass my text as an argument but the result is of the form:</p>\n<pre class=\"lang-none prettyprint-override\"><code>[' ', 'e', 'a', 'o', 'n', 'i', 't', 'r', 's', 'l', 'd', 'h', 'c', 'y', 'b', 'u', 'g', '\\n', 'm', 'p', 'w', 'f', ',', 'v', '.', &quot;'&quot;, 'k', 'B', '&quot;', 'M', 'H', '9', 'C', '-', 'N', 'S', '1', 'A', 'G', 'P', 'T', 'W', '[', ']', '(', ')', '0', '7', 'E', 'J', 'O', 'R', 'j', 'x']\n</code></pre>\n<p>whereas in the example on the <code>nltk</code> website, the result was whole words not characters. Here is how I am currently using the function:</p>\n<pre class=\"lang-py prettyprint-override\"><code>file_y = open(fileurl)\np = file_y.read()\nfdist = FreqDist(p)\nvocab = fdist.keys()\nvocab[:100]\n</code></pre>\n<p>What I am doing wrong?</p>\n",
    "score": 35,
    "creation_date": 1294503166,
    "view_count": 115288,
    "answer_count": 6,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 56470403,
    "title": "Spacy nlp = spacy.load(&quot;en_core_web_lg&quot;)",
    "body": "<p>I already have spaCy downloaded, but everytime I try the <code>nlp = spacy.load(\"en_core_web_lg\")</code>, command, I get this error: </p>\n\n<p><code>OSError: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.</code></p>\n\n<p>I already tried </p>\n\n<pre><code>&gt;&gt;&gt; import spacy\n&gt;&gt;&gt; nlp = spacy.load(\"en_core_web_sm\")\n</code></pre>\n\n<p>and this does not work like it would on my personal computer. </p>\n\n<p>My question is how do I work around this?  What directory specifically do I need to drop the spacy en model into on my computer so that it is found?</p>\n",
    "score": 35,
    "creation_date": 1559790169,
    "view_count": 78063,
    "answer_count": 9,
    "tags": "python;python-3.x;nlp;spacy"
  },
  {
    "question_id": 55619176,
    "title": "How to cluster similar sentences using BERT",
    "body": "<p>For ElMo, FastText and Word2Vec, I'm averaging the word embeddings within a sentence and using HDBSCAN/KMeans clustering to group similar sentences.</p>\n\n<p>A good example of the implementation can be seen in this short article: <a href=\"http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/\" rel=\"noreferrer\">http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/</a></p>\n\n<p>I would like to do the same thing using BERT (using the BERT python package from hugging face), however I am rather unfamiliar with how to extract the raw word/sentence vectors in order to input them into a clustering algorithm. I know that BERT can output sentence representations - so how would I actually extract the raw vectors from a sentence?</p>\n\n<p>Any information would be helpful.</p>\n",
    "score": 35,
    "creation_date": 1554921080,
    "view_count": 47549,
    "answer_count": 6,
    "tags": "python;nlp;artificial-intelligence;word-embedding;bert-language-model"
  },
  {
    "question_id": 44395656,
    "title": "Applying Spacy Parser to Pandas DataFrame w/ Multiprocessing",
    "body": "<p>Say I have a dataset, like</p>\n\n<pre><code>iris = pd.DataFrame(sns.load_dataset('iris'))\n</code></pre>\n\n<p>I can use <code>Spacy</code> and <code>.apply</code> to parse a string column into tokens (my real dataset has >1 word/token per entry of course)</p>\n\n<pre><code>import spacy # (I have version 1.8.2)\nnlp = spacy.load('en')\niris['species_parsed'] = iris['species'].apply(nlp)\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>   sepal_length   ... species    species_parsed\n0           1.4   ... setosa          (setosa)\n1           1.4   ... setosa          (setosa)\n2           1.3   ... setosa          (setosa)\n</code></pre>\n\n<p>I can also use this convenient multiprocessing function (<a href=\"http://www.racketracer.com/2016/07/06/pandas-in-parallel/\" rel=\"noreferrer\">thanks to this blogpost</a>) to do most arbitrary apply functions on a dataframe in parallel:</p>\n\n<pre><code>from multiprocessing import Pool, cpu_count\ndef parallelize_dataframe(df, func, num_partitions):\n\n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_partitions)\n    df = pd.concat(pool.map(func, df_split))\n\n    pool.close()\n    pool.join()\n    return df\n</code></pre>\n\n<p>for example:</p>\n\n<pre><code>def my_func(df):\n    df['length_of_word'] = df['species'].apply(lambda x: len(x))\n    return df\n\nnum_cores = cpu_count()\niris = parallelize_dataframe(iris, my_func, num_cores)\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>   sepal_length species  length_of_word\n0           5.1  setosa               6\n1           4.9  setosa               6\n2           4.7  setosa               6\n</code></pre>\n\n<p>...But for some reason, I can't apply the Spacy parser to a dataframe using multiprocessing this way. </p>\n\n<pre><code>def add_parsed(df):\n    df['species_parsed'] = df['species'].apply(nlp)\n    return df\n\niris = parallelize_dataframe(iris, add_parsed, num_cores)\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>   sepal_length species  length_of_word species_parsed\n0           5.1  setosa               6             ()\n1           4.9  setosa               6             ()\n2           4.7  setosa               6             ()\n</code></pre>\n\n<p>Is there some other way to do this? I'm loving Spacy for NLP but I have a lot of text data and so I'd like to parallelize some processing functions, but ran into this issue.</p>\n",
    "score": 35,
    "creation_date": 1496767842,
    "view_count": 25439,
    "answer_count": 1,
    "tags": "python;nlp;multiprocessing;spacy"
  },
  {
    "question_id": 3531746,
    "title": "What’s a good Python profanity filter library?",
    "body": "<p>Like <a href=\"https://stackoverflow.com/questions/1521646/best-profanity-filter\">https://stackoverflow.com/questions/1521646/best-profanity-filter</a>, but for Python — and I’m looking for libraries I can run and control myself locally, as opposed to web services.</p>\n\n<p>(And whilst it’s always great to hear your fundamental objections of principle to profanity filtering, I’m not specifically looking for them here. I know profanity filtering can’t pick up every hurtful thing being said. I know swearing, in the grand scheme of things, isn’t a particularly big issue. I know you need some human input to deal with issues of content. I’d just like to find a good library, and see what use I can make of it.)</p>\n",
    "score": 34,
    "creation_date": 1282314037,
    "view_count": 29039,
    "answer_count": 6,
    "tags": "python;nlp;profanity"
  },
  {
    "question_id": 47350942,
    "title": "How to verify installed spaCy version?",
    "body": "<p>I have installed <strong>spaCy</strong> with python for my NLP project.</p>\n\n<p>I have installed that using <code>pip</code>.  How can I verify installed spaCy version?</p>\n\n<p>using </p>\n\n<pre><code>pip install -U spacy\n</code></pre>\n\n<p>What is command to verify installed spaCy version?</p>\n",
    "score": 34,
    "creation_date": 1510922636,
    "view_count": 58111,
    "answer_count": 8,
    "tags": "python;nlp;pip;version;spacy"
  },
  {
    "question_id": 22433884,
    "title": "Python Gensim: how to calculate document similarity using the LDA model?",
    "body": "<p>I've got a trained LDA model and I want to calculate the similarity score between two documents from the corpus I trained my model on.\nAfter studying all the Gensim tutorials and functions, I still can't get my head around it. Can somebody give me a hint? Thanks!</p>\n",
    "score": 34,
    "creation_date": 1394952685,
    "view_count": 29678,
    "answer_count": 3,
    "tags": "python;nlp;lda;gensim"
  },
  {
    "question_id": 5907296,
    "title": "Java API for plural forms of English words",
    "body": "<p>Are there any Java API(s) which will provide plural form of English words (e.g. <code>cacti</code> for <code>cactus</code>)?</p>\n",
    "score": 34,
    "creation_date": 1304660989,
    "view_count": 22828,
    "answer_count": 7,
    "tags": "java;dictionary;nlp;lexical;pluralize"
  },
  {
    "question_id": 43510778,
    "title": "Python - How to intuit word from abbreviated text using NLP?",
    "body": "<p>I was recently working on a data set that used abbreviations for various words. For example,</p>\n\n<pre><code>wtrbtl = water bottle\nbwlingbl = bowling ball\nbsktball = basketball\n</code></pre>\n\n<p>There did not seem to be any consistency in terms of the convention used, i.e. sometimes they used vowels sometimes not. I am trying to build a mapping object like the one above for abbreviations and their corresponding words without a complete corpus or comprehensive list of terms (i.e. abbreviations could be introduced that are not explicitly known). For simplicity sake say it is restricted to stuff you would find in a gym but it could be anything.</p>\n\n<p>Basically, if you only look at the left hand side of the examples, what kind of model could do the same processing as our brain in terms of relating each abbreviation to the corresponding full text label. </p>\n\n<p>My ideas have stopped at taking the first and last letter and finding those in a dictionary. Then assign a priori probabilities based on context. But since there are a large number of morphemes without a marker that indicates end of word I don't see how its possible to split them. </p>\n\n<p>UPDATED: </p>\n\n<p>I also had the idea to combine a couple string metric algorithms like a Match Rating Algorithm to determine a set of related terms and then calculate the Levenshtein Distance between each word in the set to the target abbreviation. However, I am still in the dark when it comes to abbreviations for words not in a master dictionary. Basically, inferring word construction - may a Naive Bayes model could help but I am concerned that any error in precision caused by using the algorithms above will invalid any model training process. </p>\n\n<p>Any help is appreciated, as I am really stuck on this one.</p>\n",
    "score": 34,
    "creation_date": 1492665459,
    "view_count": 19164,
    "answer_count": 4,
    "tags": "python;machine-learning;nlp;abbreviation"
  },
  {
    "question_id": 29332851,
    "title": "What does NN VBD IN DT NNS RB means in NLTK?",
    "body": "<p>when I chunk text, I get lots of codes in the output like\n<code>NN, VBD, IN, DT, NNS, RB</code>.\nIs there a list documented somewhere which tells me the meaning of these? \nI have tried googling <code>nltk chunk code</code> <code>nltk chunk grammar</code> <code>nltk chunk tokens</code>.</p>\n\n<p>But I am not able to find any documentation which explains what these codes mean.</p>\n",
    "score": 34,
    "creation_date": 1427652525,
    "view_count": 33258,
    "answer_count": 4,
    "tags": "python;nlp;nltk;text-parsing;pos-tagger"
  },
  {
    "question_id": 40011896,
    "title": "NLTK vs Stanford NLP",
    "body": "<p>I have recently started to use NLTK toolkit for creating few solutions using Python.</p>\n<p>I hear a lot of community activity regarding using Stanford NLP.\nCan anyone tell me the difference between NLTK and Stanford NLP? Are they two different libraries? I know that NLTK has an interface to Stanford NLP but can anyone throw some light on few basic differences or even more in detail.</p>\n<p>Can Stanford NLP be used using Python?</p>\n",
    "score": 34,
    "creation_date": 1476329791,
    "view_count": 18152,
    "answer_count": 7,
    "tags": "python;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 17447045,
    "title": "Java library for keywords extraction from input text",
    "body": "<p>I'm looking for a Java library to extract keywords from a block of text.</p>\n\n<p>The process should be as follows:</p>\n\n<p>stop word cleaning -> stemming -> searching for keywords based on English linguistics statistical information - meaning if a word appears more times in the text than in the English language in terms of probability than it's a keyword candidate.</p>\n\n<p>Is there a library that performs this task?</p>\n",
    "score": 34,
    "creation_date": 1372851786,
    "view_count": 25822,
    "answer_count": 3,
    "tags": "java;nlp;extract;keyword;stemming"
  },
  {
    "question_id": 3113428,
    "title": "Classifying Documents into Categories",
    "body": "<p>I've got about 300k documents stored in a Postgres database that are tagged with topic categories (there are about 150 categories in total).  I have another 150k documents that don't yet have categories.  I'm trying to find the best way to programmaticly categorize them.</p>\n\n<p>I've been exploring <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">NLTK</a> and its Naive Bayes Classifier.  Seems like a good starting point (if you can suggest a better classification algorithm for this task, I'm all ears).</p>\n\n<p>My problem is that I don't have enough RAM to train the NaiveBayesClassifier on all 150 categoies/300k documents at once (training on 5 categories used 8GB).  Furthermore, accuracy of the classifier seems to drop as I train on more categories (90% accuracy with 2 categories, 81% with 5, 61% with 10).</p>\n\n<p>Should I just train a classifier on 5 categories at a time, and run all 150k documents through the classifier to see if there are matches?  It seems like this would work, except that there would be a lot of false positives where documents that don't really match any of the categories get shoe-horned into on by the classifier just because it's the best match available...  Is there a way to have a \"none of the above\" option for the classifier just in case the document doesn't fit into any of the categories?</p>\n\n<p>Here is my test class <a href=\"http://gist.github.com/451880\" rel=\"noreferrer\">http://gist.github.com/451880</a></p>\n",
    "score": 34,
    "creation_date": 1277409402,
    "view_count": 15373,
    "answer_count": 3,
    "tags": "python;machine-learning;nlp;nltk;naivebayes"
  },
  {
    "question_id": 59956670,
    "title": "Parsing city of origin / destination city from a string",
    "body": "<p>I have a pandas dataframe where one column is a bunch of strings with certain travel details. My goal is to parse each string to extract the city of origin and destination city (I would like to ultimately have two new columns titled 'origin' and 'destination').</p>\n\n<p>The data:</p>\n\n<pre><code>df_col = [\n    'new york to venice, italy for usd271',\n    'return flights from brussels to bangkok with etihad from â‚¬407',\n    'from los angeles to guadalajara, mexico for usd191',\n    'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags'\n]\n</code></pre>\n\n<p>This should result in:</p>\n\n<pre><code>Origin: New York, USA; Destination: Venice, Italy\nOrigin: Brussels, BEL; Destination: Bangkok, Thailand\nOrigin: Los Angeles, USA; Destination: Guadalajara, Mexico\nOrigin: Paris, France; Destination: Australia / New Zealand (this is a complicated case given two countries)\n</code></pre>\n\n<p>Thus far I have tried:\nA variety of NLTK methods, but what has gotten me closest is using the <code>nltk.pos_tag</code> method to tag each word in the string. The result is a list of tuples with each word and associated tag. Here's an example...</p>\n\n<pre><code>[('Fly', 'NNP'), ('to', 'TO'), ('Australia', 'NNP'), ('&amp;', 'CC'), ('New', 'NNP'), ('Zealand', 'NNP'), ('from', 'IN'), ('Paris', 'NNP'), ('from', 'IN'), ('â‚¬422', 'NNP'), ('return', 'NN'), ('including', 'VBG'), ('2', 'CD'), ('checked', 'VBD'), ('bags', 'NNS'), ('!', '.')]\n</code></pre>\n\n<p>I am stuck at this stage and am unsure how to best implement this. Can anyone point me in the right direction, please? Thanks.</p>\n",
    "score": 33,
    "creation_date": 1580243964,
    "view_count": 9977,
    "answer_count": 2,
    "tags": "python;regex;pandas;nlp;nltk"
  },
  {
    "question_id": 25735644,
    "title": "Python - RegEx for splitting text into sentences (sentence-tokenizing)",
    "body": "<p>I want to make a list of sentences from a string and then print them out. I don't want to use NLTK to do this.  So it needs to split on a period at the end of the sentence and not at decimals or abbreviations or title of a name or if the sentence has a .com   This is attempt at regex that doesn't work.</p>\n\n<pre><code>import re\n\ntext = \"\"\"\\\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\n\"\"\"\nsentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n\nfor stuff in sentences:\n        print(stuff)    \n</code></pre>\n\n<p>Example output of what it should look like</p>\n\n<pre><code>Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. \nDid he mind?\nAdam Jones Jr. thinks he didn't.\nIn any case, this isn't true...\nWell, with a probability of .9 it isn't.\n</code></pre>\n",
    "score": 33,
    "creation_date": 1410227757,
    "view_count": 83971,
    "answer_count": 10,
    "tags": "python;regex;nlp;tokenize"
  },
  {
    "question_id": 90580,
    "title": "Word frequency algorithm for natural language processing",
    "body": "<p>Without getting a degree in information retrieval, I'd like to know if there exists any algorithms for counting the frequency that words occur in a given body of text.  The goal is to get a \"general feel\" of what people are saying over a set of textual comments.  Along the lines of <a href=\"http://wordle.net/\" rel=\"noreferrer\">Wordle</a>.</p>\n\n<p>What I'd like:</p>\n\n<ul>\n<li>ignore articles, pronouns, etc ('a', 'an', 'the', 'him', 'them' etc)</li>\n<li>preserve proper nouns</li>\n<li>ignore hyphenation, except for soft kind</li>\n</ul>\n\n<p>Reaching for the stars, these would be peachy:</p>\n\n<ul>\n<li>handling stemming &amp; plurals (e.g. like, likes, liked, liking match the same result)</li>\n<li>grouping of adjectives (adverbs, etc) with their subjects (\"great service\" as opposed to \"great\", \"service\")</li>\n</ul>\n\n<p>I've attempted some basic stuff using Wordnet but I'm just tweaking things blindly and hoping it works for my specific data.  Something more generic would be great.</p>\n",
    "score": 33,
    "creation_date": 1221720566,
    "view_count": 22641,
    "answer_count": 8,
    "tags": "algorithm;nlp;word-frequency"
  },
  {
    "question_id": 3656762,
    "title": "N-gram generation from a sentence",
    "body": "<p>How to generate an n-gram of a string like:</p>\n\n<pre><code>String Input=\"This is my car.\"\n</code></pre>\n\n<p>I want to generate n-gram with this input:</p>\n\n<pre><code>Input Ngram size = 3\n</code></pre>\n\n<p>Output should be:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>This\nis\nmy\ncar\n\nThis is\nis my\nmy car\n\nThis is my\nis my car\n</code></pre>\n\n<p>Give some idea in Java, how to implement that or if any library is available for it.</p>\n\n<p>I am trying to use <a href=\"http://lucene.apache.org/java/3_0_2/api/all/org/apache/lucene/analysis/ngram/NGramTokenizer.html\" rel=\"noreferrer\">this NGramTokenizer</a> but its giving n-gram's of character sequence and I want n-grams of word sequence.</p>\n",
    "score": 33,
    "creation_date": 1283846004,
    "view_count": 59785,
    "answer_count": 7,
    "tags": "java;lucene;nlp;n-gram"
  },
  {
    "question_id": 76768226,
    "title": "Target modules for applying PEFT / LoRA on different models",
    "body": "<p>I am looking at a few <a href=\"https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o#scrollTo=NuAx3zBeUL1q\" rel=\"noreferrer\">different</a> <a href=\"https://www.philschmid.de/fine-tune-flan-t5-peft\" rel=\"noreferrer\">examples</a> of using PEFT on different models. The <code>LoraConfig</code> object contains a <code>target_modules</code> array. In some examples, the target modules are <code>[&quot;query_key_value&quot;]</code>, sometimes it is <code>[&quot;q&quot;, &quot;v&quot;]</code>, sometimes something else.</p>\n<p>I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?</p>\n<p>One example (for the model Falcon 7B):</p>\n<pre><code>peft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;,\n    target_modules=[\n        &quot;query_key_value&quot;,\n        &quot;dense&quot;,\n        &quot;dense_h_to_4h&quot;,\n        &quot;dense_4h_to_h&quot;,\n    ]\n</code></pre>\n<p>Another example (for the model Opt-6.7B):</p>\n<pre><code>config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n    lora_dropout=0.05,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;\n)\n</code></pre>\n<p>Yet another (for the model Flan-T5-xxl):</p>\n<pre><code>lora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n target_modules=[&quot;q&quot;, &quot;v&quot;],\n lora_dropout=0.05,\n bias=&quot;none&quot;,\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n</code></pre>\n",
    "score": 33,
    "creation_date": 1690348998,
    "view_count": 36589,
    "answer_count": 3,
    "tags": "nlp;huggingface-transformers;huggingface;fine-tuning;peft"
  },
  {
    "question_id": 5544475,
    "title": "Does an algorithm exist to help detect the &quot;primary topic&quot; of an English sentence?",
    "body": "<p>I'm trying to find out if there is a known algorithm that can detect the \"key concept\" of a sentence.</p>\n\n<p>The use case is as follows:</p>\n\n<ol>\n<li>User enters a sentence as a query (Does chicken taste like turkey?)</li>\n<li>Our system identifies the concepts of the sentence (chicken, turkey)</li>\n<li>And it runs a search of our corpus content</li>\n</ol>\n\n<p>The area that we're lacking in is identifying what the core \"topic\" of the sentence is really about.  The sentence \"Does chicken taste like turkey\" has a primary topic of \"chicken\", because the user is asking about the taste of chicken.  While \"turkey\" is a helper topic of less importance.</p>\n\n<p>So... I'm trying to find out if there is an algorithm that will help me identify the primary topic of a sentence... Let me know if you are aware of any!!! </p>\n",
    "score": 33,
    "creation_date": 1301951955,
    "view_count": 17915,
    "answer_count": 12,
    "tags": "algorithm;nlp;semantics;lexical-analysis"
  },
  {
    "question_id": 36800654,
    "title": "How is the TFIDFVectorizer in scikit-learn supposed to work?",
    "body": "<p>I'm trying to get words that are distinctive of certain documents using the TfIDFVectorizer class in scikit-learn. It creates a tfidf matrix with all the words and their scores in all the documents, but then it seems to count common words, as well. This is some of the code I'm running: </p>\n\n<pre><code>vectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(contents)\nfeature_names = vectorizer.get_feature_names()\ndense = tfidf_matrix.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns=feature_names, index=characters)\ns = pd.Series(df.loc['Adam'])\ns[s &gt; 0].sort_values(ascending=False)[:10]\n</code></pre>\n\n<p>I expected this to return a list of distinctive words for the document 'Adam', but what it does it return a list of common words: </p>\n\n<pre><code>and     0.497077\nto      0.387147\nthe     0.316648\nof      0.298724\nin      0.186404\nwith    0.144583\nhis     0.140998\n</code></pre>\n\n<p>I might not understand it perfectly, but as I understand it, tf-idf is supposed to find words that are distinctive of one document in a corpus, finding words that appear frequently in one document, but not in other documents. Here, <code>and</code> appears frequently in other documents, so I don't know why it's returning a high value here. </p>\n\n<p>The complete code I'm using to generate this is <a href=\"https://github.com/JonathanReeve/milton-analysis/blob/v0.1/tfidf-scikit.ipynb\" rel=\"noreferrer\">in this Jupyter notebook</a>. </p>\n\n<p>When I compute tf/idfs semi-manually, using the NLTK and computing scores for each word, I get the appropriate results. For the 'Adam' document: </p>\n\n<pre><code>fresh        0.000813\nprime        0.000813\nbone         0.000677\nrelate       0.000677\nblame        0.000677\nenough       0.000677\n</code></pre>\n\n<p>That looks about right, since these are words that appear in the 'Adam' document, but not as much in other documents in the corpus. The complete code used to generate this is in <a href=\"https://github.com/JonathanReeve/milton-analysis/blob/v0.1/tfidf-nltk.ipynb\" rel=\"noreferrer\">this Jupyter notebook</a>. </p>\n\n<p>Am I doing something wrong with the scikit code? Is there another way to initialize this class where it returns the right results? Of course, I can ignore stopwords by passing <code>stop_words = 'english'</code>, but that doesn't really solve the problem, since common words of any sort shouldn't have high scores here. </p>\n",
    "score": 33,
    "creation_date": 1461348582,
    "view_count": 68904,
    "answer_count": 5,
    "tags": "python;nlp;scikit-learn"
  },
  {
    "question_id": 56927602,
    "title": "Unable to load the spacy model &#39;en_core_web_lg&#39; on Google colab",
    "body": "<p>I am using spacy in google colab to build an NER model for which I have downloaded the spaCy 'en_core_web_lg' model using</p>\n<pre><code>import spacy.cli\nspacy.cli.download(&quot;en_core_web_lg&quot;)\n</code></pre>\n<p>and I get a message saying</p>\n<pre><code>✔ Download and installation successful\nYou can now load the model via spacy.load('en_core_web_lg')\n</code></pre>\n<p>However then when i try to load the model</p>\n<pre><code>nlp = spacy.load('en_core_web_lg')\n</code></pre>\n<p>the following error is printed:</p>\n<pre><code>OSError: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n</code></pre>\n<p>Could anyone help me with this problem?</p>\n",
    "score": 32,
    "creation_date": 1562551716,
    "view_count": 37135,
    "answer_count": 4,
    "tags": "python;nlp;google-colaboratory;spacy"
  },
  {
    "question_id": 8590370,
    "title": "What is NLTK POS tagger asking me to download?",
    "body": "<p>I just started using a part-of-speech tagger, and I am facing many problems. </p>\n\n<p>I started POS tagging with the following:</p>\n\n<pre><code>import nltk\ntext=nltk.word_tokenize(\"We are going out.Just you and me.\")\n</code></pre>\n\n<p>When I want to print <code>'text'</code>, the following happens:</p>\n\n<pre><code>print nltk.pos_tag(text)\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nFile \"F:\\Python26\\lib\\site-packages\\nltk\\tag\\__init__.py\", line 63, in pos_tag\ntagger = nltk.data.load(_POS_TAGGER)\nFile \"F:\\Python26\\lib\\site-packages\\nltk\\data.py\", line 594, in load\nresource_val = pickle.load(_open(resource_url))\nFile \"F:\\Python26\\lib\\site-packages\\nltk\\data.py\", line 673, in _open\n return find(path).open()\n File \"F:\\Python26\\lib\\site-packages\\nltk\\data.py\", line 455, in find\n   raise LookupError(resource_not_found)`  \nLookupError:\n Resource 'taggers/maxent_treebank_pos_tagger/english.pickle' not\n found.  Please use the NLTK Downloader to obtain the resource:\n\n&gt;&gt;&gt; nltk.download().\n\n Searched in:\n    - 'C:\\\\Documents and Settings\\\\Administrator/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'F:\\\\Python26\\\\nltk_data'\n    - 'F:\\\\Python26\\\\lib\\\\nltk_data'\n    - 'C:\\\\Documents and Settings\\\\Administrator\\\\Application Data\\\\nltk_data'\n</code></pre>\n\n<p>I used <code>nltk.download()</code> but it did not work.</p>\n",
    "score": 32,
    "creation_date": 1324473242,
    "view_count": 49897,
    "answer_count": 7,
    "tags": "python;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 6815270,
    "title": "Generating questions from text (NLP)",
    "body": "<p>What approaches are there to generating question from a sentence? Let's say I have a sentence \"Jim's dog was very hairy and smelled like wet newspaper\" - which  toolkit is capable of generating a question like \"What did Jim's dog smelled like?\" or \"How hairy was Jim's dog?\"</p>\n\n<p>Thanks!</p>\n",
    "score": 32,
    "creation_date": 1311592920,
    "view_count": 24147,
    "answer_count": 4,
    "tags": "text;nlp;generator;toolkit"
  },
  {
    "question_id": 47818669,
    "title": "Difference between Rasa core and Rasa nlu",
    "body": "<p>I tried to understand the difference between <a href=\"https://core.rasa.ai/\" rel=\"nofollow noreferrer\">Rasa core</a> and <a href=\"https://nlu.rasa.ai/installation.html\" rel=\"nofollow noreferrer\">Rasa NLU</a> from the official documentation, but I don't understand much. What I understood is that Rasa core is used to guide the flow of the conversation, while Rasa NLU is used to process the text to extract information (entities).</p>\n<p>There are examples to build chatbots in <a href=\"https://core.rasa.ai/tutorial_basics.html\" rel=\"nofollow noreferrer\">Rasa core</a> as well as <a href=\"https://nlu.rasa.ai/tutorial.html\" rel=\"nofollow noreferrer\">Rasa NLU</a>. I couldn't understand what the difference in the two approaches is and when to adopt one instead of the other approach.</p>\n<p>Could you please help me to understand this better?</p>\n",
    "score": 32,
    "creation_date": 1513271121,
    "view_count": 9375,
    "answer_count": 5,
    "tags": "nlp;artificial-intelligence;chatbot;rasa-nlu;rasa-core"
  },
  {
    "question_id": 36034454,
    "title": "What meaning does the length of a Word2vec vector have?",
    "body": "<p>I am using Word2vec through <a href=\"https://radimrehurek.com/gensim/\" rel=\"noreferrer\"><em>gensim</em></a> with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the <code>Word2Vec</code> object are not unit vectors:</p>\n\n<pre><code>&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; from gensim.models import Word2Vec\n&gt;&gt;&gt; w2v = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n&gt;&gt;&gt; king_vector = w2v['king']\n&gt;&gt;&gt; numpy.linalg.norm(king_vector)\n2.9022589\n</code></pre>\n\n<p>However, in the <a href=\"https://github.com/piskvorky/gensim/blob/0.12.4/gensim/models/word2vec.py#L1153-L1213\" rel=\"noreferrer\"><code>most_similar</code></a> method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented <code>.syn0norm</code> property, which contains only unit vectors:</p>\n\n<pre><code>&gt;&gt;&gt; w2v.init_sims()\n&gt;&gt;&gt; unit_king_vector = w2v.syn0norm[w2v.vocab['king'].index]\n&gt;&gt;&gt; numpy.linalg.norm(unit_king_vector)\n0.99999994\n</code></pre>\n\n<p>The larger vector is just a scaled up version of the unit vector:</p>\n\n<pre><code>&gt;&gt;&gt; king_vector - numpy.linalg.norm(king_vector) * unit_king_vector\narray([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,\n         0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,\n        -7.45058060e-09,   0.00000000e+00,   3.72529030e-09,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n        ... (some lines omitted) ...\n        -1.86264515e-09,  -3.72529030e-09,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)\n</code></pre>\n\n<p>Given that word similarity comparisons in Word2Vec are done by <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noreferrer\">cosine similarity</a>, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean <em>something</em>, since gensim exposes them to me rather than only exposing the unit vectors in <code>.syn0norm</code>.</p>\n\n<p>How are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?</p>\n",
    "score": 32,
    "creation_date": 1458127887,
    "view_count": 11694,
    "answer_count": 2,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 23877375,
    "title": "word2vec lemmatization of corpus before training",
    "body": "<p>Word2vec seems to be mostly trained on raw corpus data. However, lemmatization is a standard preprocessing for many semantic similarity tasks. I was wondering if anybody had experience in lemmatizing the corpus before training word2vec and if this is a useful preprocessing step to do.</p>\n",
    "score": 32,
    "creation_date": 1401136536,
    "view_count": 17205,
    "answer_count": 2,
    "tags": "nlp;word2vec;gensim;lemmatization"
  },
  {
    "question_id": 28214148,
    "title": "How to perform Lemmatization in R?",
    "body": "<p>This question is a possible duplicate of <strong><a href=\"https://stackoverflow.com/questions/22993796/lemmatizer-in-r-or-python-am-are-is-be\">Lemmatizer in R or python (am, are, is -> be?)</a></strong>, but I'm adding it again since the previous one was closed saying it was too broad and the only answer it has is not efficient (as it accesses an external website for this, which is too slow as I have very large corpus to find the lemmas for). So a part of this question will be similar to the above mentioned question.</p>\n\n<p>According to Wikipedia, lemmatization is defined as:</p>\n\n<blockquote>\n  <p>Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.</p>\n</blockquote>\n\n<p>A simple Google search for lemmatization in R will <em>only</em> point to the package <code>wordnet</code> of R. When I tried this package expecting that a character vector <code>c(\"run\", \"ran\", \"running\")</code> input to the lemmatization function would result in <code>c(\"run\", \"run\", \"run\")</code>, I saw that this package only provides functionality similar to <code>grepl</code> function through various filter names and a dictionary.</p>\n\n<p>An example code from <code>wordnet</code> package, which gives maximum of 5 words starting with \"car\", as the filter name explains itself:</p>\n\n<pre><code>filter &lt;- getTermFilter(\"StartsWithFilter\", \"car\", TRUE)\nterms &lt;- getIndexTerms(\"NOUN\", 5, filter)\nsapply(terms, getLemma)\n</code></pre>\n\n<p>The above is <strong>NOT</strong> the lemmatization that I'm looking for. What I'm looking for is, using <code>R</code> I want to find true roots of the words: (For e.g. from <code>c(\"run\", \"ran\", \"running\")</code> to <code>c(\"run\", \"run\", \"run\")</code>).</p>\n",
    "score": 31,
    "creation_date": 1422532516,
    "view_count": 42225,
    "answer_count": 6,
    "tags": "r;nlp;lemmatization"
  },
  {
    "question_id": 203684,
    "title": "How can I use NLP to parse recipe ingredients?",
    "body": "<p>I need to parse recipe ingredients into amount, measurement, item, and description as applicable to the line, such as 1 cup flour, the peel of 2 lemons and 1 cup packed brown sugar etc. What would be the best way of doing this? I am interested in using python for the project so I am assuming using the nltk is the best bet but I am open to other languages.</p>\n",
    "score": 31,
    "creation_date": 1224043039,
    "view_count": 18684,
    "answer_count": 5,
    "tags": "parsing;nlp"
  },
  {
    "question_id": 17421887,
    "title": "how to determine the number of topics for LDA?",
    "body": "<p>I am a freshman in LDA and I want to use it in my work. However, some problems appear. </p>\n\n<p>In order to get the best performance, I want to estimate the best topic number. After reading \"Finding Scientific topics\", I know that I can calculate logP(w|z) firstly and then use the harmonic mean of a series of P(w|z) to estimate P(w|T).</p>\n\n<p>My question is what does the \"a series of\" mean? </p>\n",
    "score": 31,
    "creation_date": 1372756964,
    "view_count": 44025,
    "answer_count": 5,
    "tags": "nlp;data-mining;lda"
  },
  {
    "question_id": 12173503,
    "title": "How can I do Train And Test step in Giza++?",
    "body": "<p>In artificial intelligence methods we have two stages of training.\nThese stages are data and testing.</p>\n\n<p>In the training stage we give a huge amount of data to a system and we normally test it with smaller volume of data. Then we evaluate the output.</p>\n\n<p>Now the question is can this training be done through the built in functionality embedded in GIZA++ or we should write a separate application for that?</p>\n\n<p>If we should write a separate application can anybody help me by suggesting an already written application? Or a manual? \nNote: I want to have an alignment program not a statistical machine translation</p>\n\n<p>I would prefer to train in Giza++ so I can test with unobserved data.</p>\n\n<p>Thanks in advance.</p>\n",
    "score": 31,
    "creation_date": 1346228041,
    "view_count": 1027,
    "answer_count": 1,
    "tags": "machine-learning;nlp;giza++"
  },
  {
    "question_id": 60492839,
    "title": "How to compare sentence similarities using embeddings from BERT",
    "body": "<p>I am using the HuggingFace Transformers package to access pretrained models. As my use case needs functionality for both English and Arabic, I am using the <a href=\"https://github.com/google-research/bert/blob/master/multilingual.md\" rel=\"noreferrer\">bert-base-multilingual-cased</a> pretrained model. I need to be able to compare the similarity of sentences using something such as cosine similarity. To use  this, I first need to get an embedding vector for each sentence, and can then compute the cosine similarity.</p>\n\n<p>Firstly, what is the best way to extratc the semantic embedding from the BERT model? Would taking the last hidden state of the model after being fed the sentence suffice?</p>\n\n<pre><code>import torch\nfrom transformers import BertModel, BertTokenizer\n\nmodel_class = BertModel\ntokenizer_class = BertTokenizer\npretrained_weights = 'bert-base-multilingual-cased'\n\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)\n\nsentence = 'this is a test sentence'\n\ninput_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])\nwith torch.no_grad():\n    output_tuple = model(input_ids)\n    last_hidden_states = output_tuple[0]\n\nprint(last_hidden_states.size(), last_hidden_states)\n</code></pre>\n\n<p>Secondly, if this is a sufficient way to get embeddings from my sentence, I now have another problem where the embedding vectors have different lengths depending on the length of the original sentence. The shapes output are <code>[1, n, vocab_size]</code>, where <code>n</code> can have any value. </p>\n\n<p>In order to compute two vectors' cosine similarity, they need to be the same  length. How can I do this here? Could something as naive as first summing across <code>axis=1</code> still work? What other options do I have? </p>\n",
    "score": 30,
    "creation_date": 1583166007,
    "view_count": 30070,
    "answer_count": 5,
    "tags": "python;vector;nlp;cosine-similarity;huggingface-transformers"
  },
  {
    "question_id": 47388497,
    "title": "What is the difference between Dialogflow bot framework vs Rasa nlu bot framework?",
    "body": "<p>What is the difference between Dialogflow bot framework vs Rasa nlu  bot framework ?Any other open source frameworks available in market with NLP support?</p>\n",
    "score": 30,
    "creation_date": 1511169387,
    "view_count": 17950,
    "answer_count": 4,
    "tags": "nlp;open-source;chatbot;dialogflow-es;rasa-nlu"
  },
  {
    "question_id": 891772,
    "title": "Stack Overflow Related questions algorithm",
    "body": "<p>The related questions that appear after entering the title, and those that are in the right side bar when viewing a question seem to suggest very apt questions.</p>\n\n<p>Stack Overflow only does a SQL search for it and uses no special algorithms, said Spolsky in a talk.</p>\n\n<p>What algorithms exist to give good answers in such a case.\nHow do U do database search in such a case? Make the title searchable and search on the keywords or search on tags and those questions with many votes on top?</p>\n",
    "score": 30,
    "creation_date": 1242890807,
    "view_count": 3921,
    "answer_count": 6,
    "tags": "sql;search;full-text-search;nlp"
  },
  {
    "question_id": 5771745,
    "title": "How to get all article pages under a Wikipedia Category and its sub-categories?",
    "body": "<p>I want to get all the articles names under a category and its sub-categories. </p>\n\n<p>Options I'm aware of:</p>\n\n<ol>\n<li>Using the Wikipedia API. Does it have such an option??</li>\n<li>d/l the dump. Which format would be better for my usage? </li>\n<li>There is also an option to search in Wikipedia something like <code>incategory:\"music\"</code>, but I didn't see an option to view that in XML.</li>\n</ol>\n\n<p>Please share your thoughts</p>\n",
    "score": 30,
    "creation_date": 1303662494,
    "view_count": 16212,
    "answer_count": 3,
    "tags": "sql;web-services;nlp;wikipedia;wikipedia-api"
  },
  {
    "question_id": 2293636,
    "title": "What is a good Java library for Parts-Of-Speech tagging?",
    "body": "<p>I'm looking for a good open source <a href=\"http://en.wikipedia.org/wiki/Part-of-speech_tagging\" rel=\"noreferrer\">POS Tagger</a> in Java. Here's what I have come up with so far.</p>\n\n<ul>\n<li><a href=\"http://alias-i.com/lingpipe/\" rel=\"noreferrer\">LingPipe</a></li>\n<li><a href=\"http://nlp.stanford.edu/software/tagger.shtml\" rel=\"noreferrer\">Stanford</a></li>\n<li><a href=\"http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=FLBJPOS\" rel=\"noreferrer\">LBJ</a></li>\n<li><a href=\"http://www.markwatson.com/opensource/\" rel=\"noreferrer\">FastTag</a></li>\n</ul>\n\n<p>Anybody got any recommendations?</p>\n",
    "score": 30,
    "creation_date": 1266545321,
    "view_count": 23811,
    "answer_count": 3,
    "tags": "java;nlp"
  },
  {
    "question_id": 8842817,
    "title": "Selecting the most fluent text from a set of possibilities via grammar checking (Python)",
    "body": "<h1>Some background</h1>\n\n<p>I am a literature student at New College of Florida, currently working on an overly ambitious creative project. <strong>The project is geared towards the algorithmic generation of poetry</strong>. It's written in Python. My Python knowledge and Natural Language Processing knowledge come only from teaching myself things through the internet. I've been working with this stuff for about a year, so I'm not helpless, but at various points I've had trouble moving forward in this project. Currently, I am entering the final phases of development, and have hit a little roadblock.</p>\n\n<p><strong>I need to implement some form of grammatical normalization, so that the output doesn't come out as un- conjugated/inflected caveman-speak.</strong> About a month ago some friendly folks on SO <a href=\"https://stackoverflow.com/questions/8541447/some-nlp-stuff-to-do-with-grammar-tagging-stemming-and-word-sense-disambiguat\">gave me some advice on how I might solve this issue</a> by using an <strong>ngram language modeller</strong>, basically -- but I'm looking for yet other solutions, as it seems that NLTK's NgramModeler is not fit for my needs. (The possibilities of POS tagging were also mentioned, but my text may be too fragmentary and strange for an implementation of such to come easy, given my amateur-ness.)</p>\n\n<h1>Perhaps I need something like AtD, but hopefully less complex</h1>\n\n<p><strong>I think need something that works like <a href=\"http://afterthedeadline.com/\" rel=\"nofollow noreferrer\">After the Deadline</a></strong> or <a href=\"http://queequeg.sourceforge.net/index-e.html\" rel=\"nofollow noreferrer\">Queequeg</a>, but neither of these seem exactly right. Queequeg is probably not a good fit -- it was written in 2003 for Unix and I can't get it working on Windows for the life of me (have tried everything). But I like that all it checks for is proper verb conjugation and number agreement.</p>\n\n<p>On the other hand, AtD is much more rigorous, offering more capabilities than I need. But I can't seem to get the <a href=\"http://blog.afterthedeadline.com/2009/09/15/python-bindings-for-atd/\" rel=\"nofollow noreferrer\">python bindings</a> for it working. (I get 502 errors from the AtD server, which I'm sure are easy to fix, but my application is going to be online, and I'd rather avoid depending on another server. I can't afford to run an AtD server myself, because the number of \"services\" my application is going to require of my web host is already threatening to cause problems in getting this application hosted cheaply.)</p>\n\n<h2>Things I'd like to avoid</h2>\n\n<p><strong>Building Ngram language models myself doesn't seem right for the task.</strong> my application throws a lot of unknown vocabulary, skewing all the results. (Unless I use a corpus that's so large that it runs way too slow for my application -- the application needs to be pretty snappy.)</p>\n\n<p><strong>Strictly checking grammar is neither right for the task.</strong> the grammar doesn't need to be perfect, and the sentences don't have to be any more sensible than the kind of English-like jibberish that you can generate using ngrams. Even if it's jibberish, I just need to enforce verb conjugation, number agreement, and do things like remove extra articles.</p>\n\n<p>In fact, I don't even need any kind of <em>suggestions</em> for corrections. I think all I need is for something to tally up how many errors seem to occur in each sentence in a group of possible sentences, so I can sort by their score and pick the one with the least grammatical issues.</p>\n\n<h1>A simple solution? Scoring fluency by detecting obvious errors</h1>\n\n<p>If a script exists that takes care of all this, I'd be overjoyed (I haven't found one yet). I can write code for what I can't find, of course; I'm looking for advice on how to optimize my approach.</p>\n\n<p>Let's say we have a tiny bit of text already laid out:</p>\n\n<p><code>existing_text = \"The old river\"</code></p>\n\n<p>Now let's say my script needs to figure out which inflection of the verb \"to bear\" could come next. I'm open to suggestions about this routine. <strong>But I need help mostly with step #2</strong>, rating fluency by tallying grammatical errors:</p>\n\n<ol>\n<li>Use the Verb Conjugation methods in <a href=\"http://nodebox.net/code/index.php/Linguistics\" rel=\"nofollow noreferrer\">NodeBox Linguistics</a> to come up with all conjugations of this verb; <code>['bear', 'bears', 'bearing', 'bore', 'borne']</code>.</li>\n<li>Iterate over the possibilities, (shallowly) checking the grammar of the string resulting from <code>existing_text + \" \" + possibility</code> (\"The old river bear\", \"The old river bears\", etc). Tally the error count for each construction. In this case the only construction to raise an error, seemingly, would be \"The old river bear\".</li>\n<li>Wrapping up should be easy... Of the possibilities with the lowest error count, select randomly.</li>\n</ol>\n",
    "score": 30,
    "creation_date": 1326404684,
    "view_count": 3541,
    "answer_count": 3,
    "tags": "python;nlp;grammar;nltk;linguistics"
  },
  {
    "question_id": 65199011,
    "title": "is there a way to check similarity between two full sentences in python?",
    "body": "<p>I am making a project like this one here:\n<a href=\"https://www.youtube.com/watch?v=dovB8uSUUXE&amp;feature=youtu.be\" rel=\"noreferrer\">https://www.youtube.com/watch?v=dovB8uSUUXE&amp;feature=youtu.be</a>\nbut i am facing trouble because i need to check the similarity between the sentences for example:\nif the user said: <strong>'the person wear red T-shirt'</strong> instead of <strong>'the boy wear red T-shirt'</strong>\nI want a method to check the similarity between these two sentences without having to check the similarity between each word\nis there a way to do this in python?</p>\n<p>I am trying to find a way to check the similarity between two sentences.</p>\n",
    "score": 29,
    "creation_date": 1607430829,
    "view_count": 45177,
    "answer_count": 2,
    "tags": "python;deep-learning;nlp;nltk;sentence-similarity"
  },
  {
    "question_id": 43377265,
    "title": "Determine if text is in English?",
    "body": "<p>I am using both <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">Nltk</a> and <a href=\"http://scikit-learn.org/stable/\" rel=\"noreferrer\">Scikit Learn</a> to do some text processing. However, within my list of documents I have some documents that are not in English. For example, the following could be true:</p>\n\n<pre><code>[ \"this is some text written in English\", \n  \"this is some more text written in English\", \n  \"Ce n'est pas en anglais\" ] \n</code></pre>\n\n<p>For the purposes of my analysis, I want all sentences that are not in English to be removed as part of pre-processing. However, is there a good way to do this? I have been Googling, but cannot find anything specific that will let me recognize if strings are in English or not. Is this something that is not offered as functionality in either <code>Nltk</code> or <code>Scikit learn</code>? <b>EDIT</b> I've seen questions both like <a href=\"https://stackoverflow.com/questions/29099621/how-to-find-out-wether-a-word-exists-in-english-using-nltk\">this</a> and <a href=\"https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\">this</a> but both are for individual words... Not a \"document\". Would I have to loop through every word in a sentence to check if the whole sentence is in English?</p>\n\n<p>I'm using Python, so libraries that are in Python would be preferable, but I can switch languages if needed, just thought that Python would be the best for this.</p>\n",
    "score": 29,
    "creation_date": 1492022492,
    "view_count": 68752,
    "answer_count": 8,
    "tags": "python;scikit-learn;nlp;nltk"
  },
  {
    "question_id": 6039238,
    "title": "How to auto-tag content, algorithms and suggestions needed",
    "body": "<p>I am working with some really large databases of newspaper articles, I have them in a MySQL database, and I can query them all.</p>\n\n<p>I am now searching for ways to help me tag these articles with somewhat descriptive tags. </p>\n\n<p>All these articles is accessible from a URL that looks like this:</p>\n\n<pre><code>http://web.site/CATEGORY/this-is-the-title-slug\n</code></pre>\n\n<p>So at least I can use the category to figure what type of content that we are working with. However, I also want to tag based on the article-text.</p>\n\n<p>My initial approach was doing this:</p>\n\n<ol>\n<li>Get all articles</li>\n<li>Get all words, remove all punctuation, split by space, and count them by occurrence </li>\n<li>Analyze them, and filter common non-descriptive words out like \"them\", \"I\", \"this\", \"these\", \"their\" etc.</li>\n<li>When all the common words was filtered out, the only thing left is words that is tag-worthy.</li>\n</ol>\n\n<p>But this turned out to be a rather manual task, and not a very pretty or helpful approach. </p>\n\n<p>This also suffered from the problem of words or names that are split by space, for example if 1.000 articles contains the name \"John Doe\", and 1.000 articles contains the name of \"John Hanson\", I would only get the word \"John\" out of it, not his first name, and last name.</p>\n",
    "score": 29,
    "creation_date": 1305685665,
    "view_count": 26017,
    "answer_count": 8,
    "tags": "tags;nlp;tagging"
  },
  {
    "question_id": 2009498,
    "title": "How does Amazon&#39;s Statistically Improbable Phrases work?",
    "body": "<p>How does something like Statistically Improbable Phrases work?</p>\n\n<p>According to amazon: </p>\n\n<blockquote>\n  <p>Amazon.com's Statistically Improbable\n  Phrases, or \"SIPs\", are the most\n  distinctive phrases in the text of\n  books in the Search Inside!™ program.\n  To identify SIPs, our computers scan\n  the text of all books in the Search\n  Inside! program. If they find a phrase\n  that occurs a large number of times in\n  a particular book relative to all\n  Search Inside! books, that phrase is a\n  SIP in that book.</p>\n  \n  <p>SIPs are not necessarily improbable\n  within a particular book, but they are\n  improbable relative to all books in\n  Search Inside!. For example, most SIPs\n  for a book on taxes are tax related.\n  But because we display SIPs in order\n  of their improbability score, the\n  first SIPs will be on tax topics that\n  this book mentions more often than\n  other tax books. For works of fiction,\n  SIPs tend to be distinctive word\n  combinations that often hint at\n  important plot elements.</p>\n</blockquote>\n\n<p>For instance, for Joel's first book, the SIPs are: leaky abstractions, antialiased text, own dog food, bug count, daily builds, bug database, software schedules</p>\n\n<p>One interesting complication is that these are phrases of either 2 or 3 words. This makes things a little more interesting because these phrases can overlap with or contain each other.</p>\n",
    "score": 29,
    "creation_date": 1262729629,
    "view_count": 5504,
    "answer_count": 5,
    "tags": "algorithm;nlp;platform-agnostic"
  },
  {
    "question_id": 13488817,
    "title": "Pointwise mutual information on text",
    "body": "<p>I was wondering how one would calculate the pointwise mutual information for text classification. To be more exact, I want to classify tweets in categories. I have a dataset of tweets (which are annotated), and I have a dictionary per category of words which belong to that category. Given this information, how is it possible to calculate the PMI for each category per tweet, to classify a tweet in one of these categories.</p>\n",
    "score": 29,
    "creation_date": 1353485194,
    "view_count": 31428,
    "answer_count": 1,
    "tags": "statistics;machine-learning;nlp"
  },
  {
    "question_id": 26022866,
    "title": "How can a tree be encoded as input to a neural network?",
    "body": "<p>I have a tree, specifically a parse tree with tags at the nodes and strings/words at the leaves. I want to pass this tree as input into a neural network all the while preserving its structure.</p>\n\n<p>Current approach\nAssume we have some dictionary of words w1,w2.....wn\nEncode the words that appear in the parse tree as n dimensional binary vectors with a 1 showing up in the ith spot whenever the word in the parse tree is wi</p>\n\n<p>Now how about the tree structure? There are about 2^n  possible parent tags for n words that appear at the leaves So we cant set a max length of input words and then just brute force enumerate all trees.</p>\n\n<p>Right now all i can think of is to approximate the tree by choosing the direct parent of a leaf. This can be represented by a binary vector as well with dimension equal to number of different types of tags - on the order of ~ 100 i suppose.\nMy input is then two dimensional. The first is just the vector representation of a word and the second is the vector representation of its parent tag</p>\n\n<p>Except this will lose a lot of the structure in the sentence. Is there a standard/better way of solving this problem?</p>\n",
    "score": 29,
    "creation_date": 1411579371,
    "view_count": 7019,
    "answer_count": 3,
    "tags": "machine-learning;nlp;neural-network;stanford-nlp;deep-learning"
  },
  {
    "question_id": 42986405,
    "title": "How to speed up Gensim Word2vec model load time?",
    "body": "<p>I'm building a chatbot so I need to vectorize the user's input using Word2Vec. </p>\n\n<p>I'm using a pre-trained model with 3 million words by Google (GoogleNews-vectors-negative300).</p>\n\n<p>So I load the model using Gensim:</p>\n\n<pre><code>import gensim\nmodel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n</code></pre>\n\n<p>The problem is that it takes about 2 minutes to load the model. I can't let the user wait that long.</p>\n\n<p>So what can I do to speed up the load time?</p>\n\n<p>I thought about putting each of the 3 million words and their corresponding vector into a MongoDB database. That would certainly speed things up but intuition tells me it's not a good idea.</p>\n",
    "score": 28,
    "creation_date": 1490301058,
    "view_count": 30056,
    "answer_count": 4,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 9492707,
    "title": "How can I split a text into sentences using the Stanford parser?",
    "body": "<p>How can I split a text or paragraph into sentences using <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"noreferrer\">Stanford parser</a>?</p>\n\n<p>Is there any method that can extract sentences, such as <code>getSentencesFromString()</code> as it's provided for <a href=\"http://stanfordparser.rubyforge.org/\" rel=\"noreferrer\">Ruby</a>?</p>\n",
    "score": 28,
    "creation_date": 1330481993,
    "view_count": 36838,
    "answer_count": 12,
    "tags": "java;parsing;artificial-intelligence;nlp;stanford-nlp"
  },
  {
    "question_id": 76551067,
    "title": "How to create a langchain doc from an str?",
    "body": "<p>I've searched all over langchain documentation on their official website but I didn't find how to create a langchain doc from a str variable in python so I searched in their GitHub code and I found this :</p>\n<pre><code>  doc=Document(\n                page_content=&quot;text&quot;,\n                metadata={&quot;source&quot;: &quot;local&quot;}\n            )\n\n</code></pre>\n<p>PS: I added the metadata attribute<br>\nthen I tried using that doc with my chain:<br>\nMemory and Chain:</p>\n<pre><code>memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;, input_key=&quot;human_input&quot;)\nchain = load_qa_chain(\n    llm, chain_type=&quot;stuff&quot;, memory=memory, prompt=prompt\n)\n\n</code></pre>\n<p>the call method:</p>\n<pre><code>  chain({&quot;input_documents&quot;: doc, &quot;human_input&quot;: query})\n</code></pre>\n<p>prompt template:</p>\n<pre><code>template = &quot;&quot;&quot;You are a senior financial analyst analyzing the below document and having a conversation with a human.\n{context}\n{chat_history}\nHuman: {human_input}\nsenior financial analyst:&quot;&quot;&quot;\n\nprompt = PromptTemplate(\n    input_variables=[&quot;chat_history&quot;, &quot;human_input&quot;, &quot;context&quot;], template=template\n)\n</code></pre>\n<p>but I am  getting the following error:</p>\n<pre><code>AttributeError: 'tuple' object has no attribute 'page_content'\n\n</code></pre>\n<p>when I tried to check the type and the page content of the Document object before using it with the chain I got this</p>\n<pre><code>print(type(doc))\n&lt;class 'langchain.schema.Document'&gt;\nprint(doc.page_content)\n&quot;text&quot;\n\n\n</code></pre>\n",
    "score": 28,
    "creation_date": 1687705782,
    "view_count": 50304,
    "answer_count": 5,
    "tags": "python;nlp;langchain;large-language-model"
  },
  {
    "question_id": 1639855,
    "title": "POS tagging in German",
    "body": "<p>I am using NLTK to extract nouns from a text-string starting with the following command:</p>\n\n<pre><code>tagged_text = nltk.pos_tag(nltk.Text(nltk.word_tokenize(some_string)))\n</code></pre>\n\n<p>It works fine in English. <strong>Is there an easy way to make it work for German as well?</strong> </p>\n\n<p>(I have no experience with natural language programming, but I managed to use the python nltk library which is great so far.)</p>\n",
    "score": 28,
    "creation_date": 1256761068,
    "view_count": 23817,
    "answer_count": 6,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 30703485,
    "title": "Data sets for emotion detection in text",
    "body": "<p>I'm implementing a system that could detect the human emotion in text. Are there any manually annotated data sets available for supervised learning and testing? </p>\n\n<p>Here are some interesting datasets:\n<a href=\"https://dataturks.com/projects/trending\" rel=\"noreferrer\">https://dataturks.com/projects/trending</a></p>\n",
    "score": 28,
    "creation_date": 1433748895,
    "view_count": 26107,
    "answer_count": 1,
    "tags": "database;dataset;nlp;text-mining;emotion"
  },
  {
    "question_id": 56071689,
    "title": "What&#39;s the major difference between glove and word2vec?",
    "body": "<p>What is the difference between word2vec and glove? \nAre both the ways to train a word embedding? if yes then how can we use both?</p>\n",
    "score": 28,
    "creation_date": 1557468619,
    "view_count": 18313,
    "answer_count": 2,
    "tags": "machine-learning;nlp;word2vec;word-embedding;glove"
  },
  {
    "question_id": 6603262,
    "title": "Ease of use: Stanford CoreNLP vs. OpenNLP",
    "body": "<p>I looking to use a suite of NLP tools for a personal project, and I was wondering whether Stanford's CoreNLP is easier to use or OpenNLP. Or is there another free package you would reccomend?\nI haven't really done any NLP before, so I am looking for something that I can quickly use to learn the concepts and prototype my ideas. Any help is appreciated.</p>\n",
    "score": 28,
    "creation_date": 1309987833,
    "view_count": 20263,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 16133184,
    "title": "How to detect that two sentences are similar?",
    "body": "<p>I want to compute how similar two arbitrary sentences are to each other.  For example:</p>\n\n<blockquote>\n  <ol>\n  <li>A mathematician found a solution to the problem.</li>\n  <li>The problem was solved by a young mathematician.</li>\n  </ol>\n</blockquote>\n\n<p>I can use a tagger, a stemmer, and a parser, but I don’t know how detect that these sentences are similar.</p>\n",
    "score": 28,
    "creation_date": 1366560286,
    "view_count": 18274,
    "answer_count": 3,
    "tags": "nlp;similarity;stanford-nlp;opennlp"
  },
  {
    "question_id": 48962171,
    "title": "How to Train GloVe algorithm on my own corpus",
    "body": "<p>I tried to follow <a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">this.</a><br>\nBut some how I wasted a lot of time ending up with nothing useful.<br>\nI just want to train a <code>GloVe</code> model on my own corpus (~900Mb corpus.txt file).\nI downloaded the files provided in the link above and compiled it using <code>cygwin</code> (after editing the demo.sh file and changed it to <code>VOCAB_FILE=corpus.txt</code> . should I leave <code>CORPUS=text8</code> unchanged?)\nthe output was:  </p>\n\n<ol>\n<li>cooccurrence.bin </li>\n<li>cooccurrence.shuf.bin  </li>\n<li>text8</li>\n<li>corpus.txt</li>\n<li>vectors.txt</li>\n</ol>\n\n<p>How can I used those files to load it as a <code>GloVe</code> model on python?</p>\n",
    "score": 28,
    "creation_date": 1519470657,
    "view_count": 31048,
    "answer_count": 5,
    "tags": "nlp;stanford-nlp;gensim;word2vec;glove"
  },
  {
    "question_id": 64337550,
    "title": "Neither PyTorch nor TensorFlow &gt;= 2.0 have been found.Models won&#39;t be available and only tokenizers, configuration and file/data utilities can be used",
    "body": "<p>I am trying to install transformers using pip</p>\n<pre><code>pip install transformers\n</code></pre>\n<p>after import transformers</p>\n<p>this error show</p>\n<pre><code>Neither PyTorch nor TensorFlow &gt;= 2.0 have been found.Models won't be available and only tokenizers, configuration, and file/data utilities can be used.\n</code></pre>\n<p>although I install TensorFlow-GPU= 2.3.1 and using conda</p>\n<p>system info</p>\n<pre><code>Windows 10 \npython 3.6\ncuda 10.1\ntensorflow-gpu= 2.3.1\n</code></pre>\n",
    "score": 28,
    "creation_date": 1602600570,
    "view_count": 58091,
    "answer_count": 6,
    "tags": "python;tensorflow;nlp"
  },
  {
    "question_id": 22158530,
    "title": "Is it possible to train Stanford NER system to recognize more named entities types?",
    "body": "<p>I'm using some NLP libraries now, (stanford and nltk) \nStanford I saw the demo part but just want to ask if it possible to use it to identify more entity types.</p>\n\n<p>So currently stanford NER system (as the demo shows) can recognize entities as person(name), organization or location. But the organizations recognized are limited to universities or some, big organizations. I'm wondering if I can use its API to write program for more entity types, like if my input is \"Apple\" or  \"Square\" it can recognize it as a company.</p>\n\n<p>Do I have to make my own training dataset?</p>\n\n<p>Further more, if I ever want to extract entities and their relationships between each other, I feel I should use the stanford dependency parser.\nI mean, extract first the named entities and other parts tagged as \"noun\" and find relations between them.</p>\n\n<p>Am I correct.</p>\n\n<p>Thanks.</p>\n",
    "score": 28,
    "creation_date": 1393884438,
    "view_count": 18504,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 30843011,
    "title": "Save and reuse TfidfVectorizer in scikit learn",
    "body": "<p>I am using TfidfVectorizer in scikit learn to create a matrix from text data. Now I need to save this object to reuse it later. I tried to use pickle, but it gave the following error.</p>\n<pre class=\"lang-none prettyprint-override\"><code>loc=open('vectorizer.obj','w')\npickle.dump(self.vectorizer,loc)\n*** TypeError: can't pickle instancemethod objects\n</code></pre>\n<p>I tried using <code>joblib</code> in sklearn.externals, which again gave similar error. Is there any way to save this object so that I can reuse it later?</p>\n<p>Here is my full object:</p>\n<pre class=\"lang-py prettyprint-override\"><code>class changeToMatrix(object):\n    def __init__(self,ngram_range=(1,1),tokenizer=StemTokenizer()):\n        from sklearn.feature_extraction.text import TfidfVectorizer\n        self.vectorizer = TfidfVectorizer(ngram_range=ngram_range,analyzer='word',lowercase=True,\n                                          token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',\n                                          tokenizer=tokenizer)\n\n    def load_ref_text(self,text_file):\n        textfile = open(text_file,'r')\n        lines = textfile.readlines()\n        textfile.close()\n        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n        sentences = [item.strip().strip('.') for item in sent_tokenizer.tokenize(' '.join(lines).strip())]\n        #vectorizer is transformed in this step\n        chk2 = pd.DataFrame(self.vectorizer.fit_transform(sentences1).toarray())\n        return sentences, [chk2]\n\n    def get_processed_data(self,data_loc):\n        ref_sentences,ref_dataframes=self.load_ref_text(data_loc)\n        loc = open(&quot;indexedData/vectorizer.obj&quot;,&quot;w&quot;)\n        pickle.dump(self.vectorizer,loc) #getting error here\n        loc.close()\n        return ref_sentences, ref_dataframes\n</code></pre>\n",
    "score": 28,
    "creation_date": 1434364538,
    "view_count": 43892,
    "answer_count": 2,
    "tags": "python;nlp;scikit-learn;pickle;text-mining"
  },
  {
    "question_id": 60186935,
    "title": "How to build semantic search for a given domain",
    "body": "<p>There is a problem we are trying to solve where we want to do a semantic search on our set of data,\ni.e we have a domain-specific data (example: sentences talking about automobiles)</p>\n\n<p>Our data is just a bunch of sentences and what we want is to give a phrase and get back the sentences which are:</p>\n\n<ol>\n<li>Similar to that phrase</li>\n<li>Has a part of a sentence that is similar to the phrase</li>\n<li>A sentence which is having contextually similar meanings </li>\n</ol>\n\n<p><br/></p>\n\n<p>Let me try giving you an example suppose I search for the phrase \"Buying Experience\", I should get the sentences like:</p>\n\n<ul>\n<li>I never thought car buying could take less than 30 minutes to sign\nand buy.</li>\n<li><p>I found a car that i liked and the purchase process was<br>\nstraightforward and easy</p></li>\n<li><p>I absolutely hated going car shopping, but today i’m glad i did</p></li>\n</ul>\n\n<p><br/>\nI want to lay emphasis on the fact that we are looking for <strong>contextual similarity</strong> and not just a brute force word search.</p>\n\n<p>If the sentence uses different words then also it should be able to find it.</p>\n\n<p>Things that we have already tried:</p>\n\n<ol>\n<li><p><a href=\"https://www.opensemanticsearch.org/\" rel=\"noreferrer\">Open Semantic Search</a> the problem we faced here is generating ontology from the data we have, or\nfor that sake searching for available ontology from different domains of our interest.</p></li>\n<li><p>Elastic Search(BM25 + Vectors(tf-idf)), we tried this where it gave a few sentences but precision was not that great. The accuracy was bad\nas well. We tried against a human-curated dataset, it was able to get around 10% of the sentences only.</p></li>\n<li><p>We tried different embeddings like the once mentioned in <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"noreferrer\">sentence-transformers</a> and also went through the <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.p\" rel=\"noreferrer\">example</a> and tried evaluating against our human-curated set\nand that also had very low accuracy.</p></li>\n<li><p>We tried <a href=\"https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604\" rel=\"noreferrer\">ELMO</a>. This was better but still lower accuracy than we expected and there is a\ncognitive load to decide the cosine value below which we shouldn't consider the sentences. This even applies to point 3.</p></li>\n</ol>\n\n<p>Any help will be appreciated. Thanks a lot for the help in advance</p>\n",
    "score": 28,
    "creation_date": 1581505570,
    "view_count": 7190,
    "answer_count": 3,
    "tags": "python;elasticsearch;nlp;sentence-similarity;huggingface-transformers"
  },
  {
    "question_id": 34721984,
    "title": "stopword removing when using the word2vec",
    "body": "<p>I have been trying word2vec for a while now using the gensim's word2vec library. My question is do I have to remove stopwords from my input text?  Because, based on my initial experimental results, I could see words like 'of', 'when'.. (stopwords) popping up when I do a <code>model.most_similar('someword')</code>..?</p>\n\n<p>But I didn't see anywhere referring that stop word removal is necessary with word2vec? Does the word2vec is supposed to handle stop words even if you don't remove them?</p>\n\n<p>What are the must do pre processing things (like for topic modeling, it's almost a must that you should do stopword removal)?</p>\n",
    "score": 27,
    "creation_date": 1452516550,
    "view_count": 21620,
    "answer_count": 3,
    "tags": "nlp;gensim;word2vec"
  },
  {
    "question_id": 2326063,
    "title": "How do I tell what language is a plain-text file written in?",
    "body": "<p>Suppose we have a text file with the content:\n\"Je suis un beau homme ...\"</p>\n\n<p>another with:\n\"I am a brave man\"</p>\n\n<p>the third with a text in German:\n\"Guten morgen. Wie geht's ?\"</p>\n\n<p>How do we write a function that would tell us: with such a probability the text in the first\nfile is in English, in the second we have French etc?</p>\n\n<p>Links to books / out-of-the-box solutions are welcome. I write in Java, but I can learn Python if needed.</p>\n\n<p><em>My comments</em> </p>\n\n<ol>\n<li>There's one small comment I need to add. The text may contain phrases in different languages, as part of whole or as a result of a mistake. In classic litterature we have a lot of examples, because the aristocracy members were multilingual. So the probability better describes the situation, as most parts of the text are in one language, while others may be written in another.</li>\n<li>Google API - Internet Connection. I would prefer not to use remote functions/services, as I need to do it myself or use a downloadable library. I'd like to make a research on that topic.</li>\n</ol>\n",
    "score": 27,
    "creation_date": 1267015903,
    "view_count": 4856,
    "answer_count": 10,
    "tags": "java;nlp"
  },
  {
    "question_id": 45394949,
    "title": "what is dimensionality in word embeddings?",
    "body": "<p>I want to understand what is meant by \"dimensionality\" in word embeddings.</p>\n\n<p>When I embed a word in the form of a matrix for NLP tasks, what role does dimensionality play? Is there a visual example which can help me understand this concept?</p>\n",
    "score": 27,
    "creation_date": 1501370659,
    "view_count": 34557,
    "answer_count": 6,
    "tags": "nlp;terminology;dimensionality-reduction;word-embedding"
  },
  {
    "question_id": 17684930,
    "title": "NLTK for Persian",
    "body": "<p>How to use functions of NLTK for Persian?</p>\n\n<p>For example: 'concordance'. When I use 'concordance', the answer is 'not match', however there is the parameter of concordance in my text.</p>\n\n<p>the input is very simple .it contains of \"hello سلام\".when parameter of 'concordance' is 'hello' the answer is correct ,but , if it's 'سلام' the answer is 'not matches'.the expected output for me is 'Displaying 1 of 1 matches'.</p>\n\n<pre><code>    import nltk\n    from urllib import urlopen\n    url = \"file:///home/.../1.html\"\n    raw = urlopen(url).read()\n    raw = nltk.clean_html(raw)\n    tokens = nltk.word_tokenize(raw)\n    tokens = tokens[:12]\n    text = nltk.Text(tokens)\n    print text.concordance('سلام')\n</code></pre>\n",
    "score": 27,
    "creation_date": 1374001455,
    "view_count": 10746,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 76448287,
    "title": "How can i solve ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1` when using Huggingface&#39;s TrainArguments?",
    "body": "<p>I'm using the <code>transformers</code> library in Google colab, and\nWhen i am using TrainingArguments from transformers library i'm getting Import error with this  code:</p>\n<pre><code>from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir = &quot;/content/our-model&quot;,\n    learning_rate=2e-5,\n    per_device_train_batch_size= 64,\n    per_device_eval_batch_size = 16,\n    num_train_epochs = 2,\n    weight_decay = 0.01,\n    evaluation_strategy = &quot;epoch&quot;,\n    save_strategy = &quot;epoch&quot;,\n    load_best_model_at_end = True,\n    push_to_hub = False\n)\n</code></pre>\n<p>This is the error i'm getting:</p>\n<pre><code>&lt;ipython-input-28-0518ea5ff407&gt; in &lt;cell line: 2&gt;()\n      1 from transformers import TrainingArguments\n----&gt; 2 training_args = TrainingArguments(\n      3     output_dir = &quot;/content/our-model&quot;,\n      4     learning_rate=2e-5,\n      5     per_device_train_batch_size= 64,\n\n4 frames\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)\n   1670         if not is_sagemaker_mp_enabled():\n   1671             if not is_accelerate_available(min_version=&quot;0.20.1&quot;):\n-&gt; 1672                 raise ImportError(\n   1673                     &quot;Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`&quot;\n   1674                 )\n\nImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U \n</code></pre>\n<p>I already tried pip install for 0.20.1 version of accelerate and pip install transformers[torch]\nand both didn't worked.</p>\n",
    "score": 27,
    "creation_date": 1686433863,
    "view_count": 41242,
    "answer_count": 4,
    "tags": "python;nlp;importerror;huggingface-transformers;huggingface"
  },
  {
    "question_id": 4858467,
    "title": "Combining a Tokenizer into a Grammar and Parser with NLTK",
    "body": "<p>I am making my way through the NLTK book and I can't seem to do something that would appear to be a natural first step for building a decent grammar.</p>\n\n<p>My goal is to build a grammar for a particular text corpus. </p>\n\n<p><em>(Initial question: Should I even try to start a grammar from scratch or should I start with a predefined grammar? If I should start with another grammar, which is a good one to start with for English?)</em></p>\n\n<p>Suppose I have the following simple grammar:</p>\n\n<pre><code>simple_grammar = nltk.parse_cfg(\"\"\"\nS -&gt; NP VP\nPP -&gt; P NP\nNP -&gt; Det N | Det N PP\nVP -&gt; V NP | VP PP\nDet -&gt; 'a' | 'A'\nN -&gt; 'car' | 'door'\nV -&gt; 'has'\nP -&gt; 'in' | 'for'\n \"\"\");\n</code></pre>\n\n<p>This grammar can parse a very simple sentence, such as:</p>\n\n<pre><code>parser = nltk.ChartParser(simple_grammar)\ntrees = parser.nbest_parse(\"A car has a door\")\n</code></pre>\n\n<p>Now I want to extend this grammar to handle sentences with other nouns and verbs. How do I add those nouns and verbs to my grammar without manually defining them in the grammar? </p>\n\n<p>For example, suppose I want to be able to parse the sentence \"A car has wheels\". I know that the supplied tokenizers can <em>magically</em> figure out which words are verbs/nouns, etc. How can I use the output of the tokenizer to tell the grammar that \"wheels\" is a noun?</p>\n",
    "score": 27,
    "creation_date": 1296529617,
    "view_count": 10060,
    "answer_count": 3,
    "tags": "python;nlp;grammar;nltk"
  },
  {
    "question_id": 19312573,
    "title": "NLTK for Named Entity Recognition",
    "body": "<p>I am trying to use NLTK toolkit to get extract place, date and time from text messages. I just installed the toolkit on my machine and I wrote this quick snippet to test it out:</p>\n\n<pre><code>sentence = \"Let's meet tomorrow at 9 pm\";\ntokens = nltk.word_tokenize(sentence)\npos_tags = nltk.pos_tag(tokens)\nprint nltk.ne_chunk(pos_tags, binary=True)\n</code></pre>\n\n<p>I was assuming that it will identify the date (tomorrow) and time (9 pm). But, surprisingly it failed to recognize that. I get the following result when I run my above code:</p>\n\n<pre><code>(S (GPE Let/NNP) 's/POS meet/NN tomorrow/NN at/IN 9/CD pm/NN)\n</code></pre>\n\n<p>Can someone help me understand if I am missing something or NLTK is just not mature enough to tag time and date properly. Thanks!</p>\n",
    "score": 27,
    "creation_date": 1381476543,
    "view_count": 22660,
    "answer_count": 3,
    "tags": "machine-learning;nlp;nltk;text-processing;named-entity-recognition"
  },
  {
    "question_id": 23689,
    "title": "Natural language date/time parser for .NET?",
    "body": "<p>Does anyone know of a .NET date/time parser similar to <a href=\"http://chronic.rubyforge.org/\" rel=\"noreferrer\">Chronic for Ruby</a> (handles stuff like \"tomorrow\" or \"3pm next thursday\")?</p>\n\n<p>Note: I do write Ruby (which is how I know about Chronic) but this project must use .NET.</p>\n",
    "score": 27,
    "creation_date": 1219445110,
    "view_count": 6470,
    "answer_count": 9,
    "tags": ".net;datetime;nlp"
  },
  {
    "question_id": 54573853,
    "title": "NLTK available languages for stopwords",
    "body": "<p>I'm wondering where I can find the full list of supported langs (and their keys) for the NLTK stopwords.</p>\n\n<p>I find a list in <a href=\"https://pypi.org/project/stop-words/\" rel=\"noreferrer\">https://pypi.org/project/stop-words/</a> but it does not contain the keys for each country. So, it is not clear if you can retrieve the list by simply <code>stopwords.words(\"Bulgarian\")</code>. In fact, that will throw an error. </p>\n\n<p>I checked in the NLTK site and there are 4 documents matching \"stopwords\" but none of them describes that. \n<a href=\"https://www.nltk.org/search.html?q=stopwords&amp;check_keywords=yes&amp;area=default\" rel=\"noreferrer\">https://www.nltk.org/search.html?q=stopwords&amp;check_keywords=yes&amp;area=default</a></p>\n\n<p>And nothing is sayd in their book:\n<a href=\"http://www.nltk.org/book/ch02.html#stopwords_index_term\" rel=\"noreferrer\">http://www.nltk.org/book/ch02.html#stopwords_index_term</a></p>\n\n<p>So, do you know where can I find the list of keys?</p>\n",
    "score": 27,
    "creation_date": 1549544108,
    "view_count": 57173,
    "answer_count": 3,
    "tags": "python;nlp;nltk;stop-words"
  },
  {
    "question_id": 8683588,
    "title": "Understanding NLTK collocation scoring for bigrams and trigrams",
    "body": "<p><strong>Background:</strong></p>\n\n<p>I am trying to compare pairs of words to see which pair is \"more likely to occur\" in US English than another pair.  My plan is/was to use the collocation facilities in NLTK to score word pairs, with the higher scoring pair being the most likely.</p>\n\n<p><strong>Approach:</strong></p>\n\n<p>I coded the following in Python using NLTK (several steps and imports removed for brevity):</p>\n\n<pre><code>bgm    = nltk.collocations.BigramAssocMeasures()\nfinder = BigramCollocationFinder.from_words(tokens)\nscored = finder.score_ngrams( bgm.likelihood_ratio  )\nprint scored\n</code></pre>\n\n<p><strong>Results:</strong></p>\n\n<p>I then examined the results using 2 word pairs, one of which should be highly likely to co-occur, and one pair which should not (\"roasted cashews\" and \"gasoline cashews\"). I was surprised to see these word pairing score identically:</p>\n\n<pre><code>[(('roasted', 'cashews'), 5.545177444479562)]\n[(('gasoline', 'cashews'), 5.545177444479562)]\n</code></pre>\n\n<p>I would have expected 'roasted cashews' to score higher than 'gasoline cashews' in my test.</p>\n\n<p><strong>Questions:</strong></p>\n\n<ol>\n<li>Am I misunderstanding the use of collocations?</li>\n<li>Is my code incorrect?</li>\n<li>Is my assumption that the scores should be different wrong, and if so why?</li>\n</ol>\n\n<p>Thank you very much for any information or help!</p>\n",
    "score": 27,
    "creation_date": 1325275756,
    "view_count": 27518,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 188176,
    "title": "Named Entity Recognition Libraries for Java",
    "body": "<p>I am looking for a simple but \"good enough\" Named Entity Recognition library (and dictionary) for java, I am looking to process emails and documents and extract some \"basic information\" like:\nNames, places, Address and Dates</p>\n\n<p>I've been looking around, and most seems to be on the heavy side and full NLP kind of projects. </p>\n\n<p>Any recommendations ?</p>\n",
    "score": 27,
    "creation_date": 1223571294,
    "view_count": 23772,
    "answer_count": 4,
    "tags": "java;nlp;named-entity-recognition"
  },
  {
    "question_id": 48019843,
    "title": "PCA on word2vec embeddings",
    "body": "<p>I am trying to reproduce the results of this paper: <a href=\"https://arxiv.org/pdf/1607.06520.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1607.06520.pdf</a></p>\n\n<p>Specifically this part:</p>\n\n<blockquote>\n  <p>To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest.</p>\n</blockquote>\n\n<p><a href=\"https://i.sstatic.net/EOJJK.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/EOJJK.png\" alt=\"enter image description here\"></a></p>\n\n<p>I am using the same set of word vectors as the authors (Google News Corpus, 300 dimensions), which I load into word2vec. </p>\n\n<p>The 'ten gender pair difference vectors' the authors refer to are computed from the following word pairs:</p>\n\n<p><a href=\"https://i.sstatic.net/7b6Dj.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/7b6Dj.png\" alt=\"enter image description here\"></a></p>\n\n<p>I've computed the differences between each normalized vector in the following way:</p>\n\n<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-\nnegative300.bin', binary = True)\nmodel.init_sims()\n\npairs = [('she', 'he'),\n('her', 'his'),\n('woman', 'man'),\n('Mary', 'John'),\n('herself', 'himself'),\n('daughter', 'son'),\n('mother', 'father'),\n('gal', 'guy'),\n('girl', 'boy'),\n('female', 'male')]\n\ndifference_matrix = np.array([model.word_vec(a[0], use_norm=True) - model.word_vec(a[1], use_norm=True) for a in pairs])\n</code></pre>\n\n<p>I then perform PCA on the resulting matrix, with 10 components, as per the paper:</p>\n\n<pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=10)\npca.fit(difference_matrix)\n</code></pre>\n\n<p>However I get very different results when I look at <code>pca.explained_variance_ratio_</code> :</p>\n\n<pre><code>array([  2.83391436e-01,   2.48616155e-01,   1.90642492e-01,\n         9.98411858e-02,   5.61260498e-02,   5.29706681e-02,\n         2.75670634e-02,   2.21957722e-02,   1.86491774e-02,\n         1.99108478e-32])\n</code></pre>\n\n<p>or with a chart:</p>\n\n<p><a href=\"https://i.sstatic.net/RuNEi.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/RuNEi.png\" alt=\"enter image description here\"></a></p>\n\n<p>The first component accounts for less than 30% of the variance when it should be above 60%! </p>\n\n<p>The results I get are similar to what I get when I try to do the PCA on randomly selected vectors, so I must be doing something wrong, but I can't figure out what.</p>\n\n<p>Note: I've tried without normalizing the vectors, but I get the same results.</p>\n",
    "score": 27,
    "creation_date": 1514537129,
    "view_count": 16106,
    "answer_count": 2,
    "tags": "python;scikit-learn;nlp;pca;word2vec"
  },
  {
    "question_id": 31836058,
    "title": "NLTK Named Entity recognition to a Python list",
    "body": "<p>I used NLTK's <code>ne_chunk</code> to extract named entities from a text:</p>\n\n<pre><code>my_sent = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n\n\nnltk.ne_chunk(my_sent, binary=True)\n</code></pre>\n\n<p>But I can't figure out how to save these entities to a list? E.g. –</p>\n\n<pre><code>print Entity_list\n('WASHINGTON', 'New York', 'Loretta', 'Brooklyn', 'African')\n</code></pre>\n\n<p>Thanks.</p>\n",
    "score": 26,
    "creation_date": 1438786713,
    "view_count": 66508,
    "answer_count": 7,
    "tags": "python;nlp;nltk;named-entity-recognition"
  },
  {
    "question_id": 15016025,
    "title": "How to print the LDA topics models from gensim? Python",
    "body": "<p>Using <code>gensim</code> I was able to extract topics from a set of documents in LSA but how do I access the topics generated from the LDA models?</p>\n\n<p>When printing the <code>lda.print_topics(10)</code> the code gave the following error because <code>print_topics()</code> return a <code>NoneType</code>:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"/home/alvas/workspace/XLINGTOP/xlingtop.py\", line 93, in &lt;module&gt;\n    for top in lda.print_topics(2):\nTypeError: 'NoneType' object is not iterable\n</code></pre>\n\n<p>The code:</p>\n\n<pre><code>from gensim import corpora, models, similarities\nfrom gensim.models import hdpmodel, ldamodel\nfrom itertools import izip\n\ndocuments = [\"Human machine interface for lab abc computer applications\",\n              \"A survey of user opinion of computer system response time\",\n              \"The EPS user interface management system\",\n              \"System and human system engineering testing of EPS\",\n              \"Relation of user perceived response time to error measurement\",\n              \"The generation of random binary unordered trees\",\n              \"The intersection graph of paths in trees\",\n              \"Graph minors IV Widths of trees and well quasi ordering\",\n              \"Graph minors A survey\"]\n\n# remove common words and tokenize\nstoplist = set('for a of the and to in'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist]\n         for document in documents]\n\n# remove words that appear only once\nall_tokens = sum(texts, [])\ntokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\ntexts = [[word for word in text if word not in tokens_once]\n         for text in texts]\n\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\n# I can print out the topics for LSA\nlsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\ncorpus_lsi = lsi[corpus]\n\nfor l,t in izip(corpus_lsi,corpus):\n  print l,\"#\",t\nprint\nfor top in lsi.print_topics(2):\n  print top\n\n# I can print out the documents and which is the most probable topics for each doc.\nlda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)\ncorpus_lda = lda[corpus]\n\nfor l,t in izip(corpus_lda,corpus):\n  print l,\"#\",t\nprint\n\n# But I am unable to print out the topics, how should i do it?\nfor top in lda.print_topics(10):\n  print top\n</code></pre>\n",
    "score": 26,
    "creation_date": 1361501262,
    "view_count": 49289,
    "answer_count": 10,
    "tags": "python;nlp;lda;topic-modeling;gensim"
  },
  {
    "question_id": 37253326,
    "title": "How to find the most common words using spacy?",
    "body": "<p>I'm using spacy with python and its working fine for tagging each word but I was wondering if it was possible to find the most common words in a string. Also is it possible to get the most common nouns, verbs, adverbs and so on?</p>\n\n<p>There's a count_by function included but I cant seem to get it to run in any meaningful way.</p>\n",
    "score": 26,
    "creation_date": 1463399653,
    "view_count": 34727,
    "answer_count": 3,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 2006763,
    "title": "What are the prerequisites to learning natural language processing?",
    "body": "<p>I am planning to learn natural language processing this year.</p>\n\n<p>But when I start reading introductory books on this topic, I found that I miss a lot of points relating mainly to mathematics.</p>\n\n<p>So I'm here searching for what I should learn before I can learn nlp, well, more smoothly?</p>\n\n<p>Thanks in advance.</p>\n",
    "score": 26,
    "creation_date": 1262703255,
    "view_count": 17073,
    "answer_count": 4,
    "tags": "nlp"
  },
  {
    "question_id": 66868221,
    "title": "Gensim 3.8.0 to Gensim 4.0.0",
    "body": "<p>I have trained a Word2Vec model using Gensim 3.8.0. Later I tried to use the pretrained model using Gensim 4.0.o on GCP. I used the following code:</p>\n<pre><code>model = KeyedVectors.load_word2vec_format(wv_path, binary= False)\nwords = model.wv.vocab.keys()\nself.word2vec = {word:model.wv[word]%EMBEDDING_DIM for word in words}\n</code></pre>\n<p>I was getting error that &quot;model.mv&quot; has been removed from Gensim 4.0.0.\nThen I used the following code:</p>\n<pre><code>model = KeyedVectors.load_word2vec_format(wv_path, binary= False)\nwords = model.vocab.keys()\nword2vec = {word:model[word]%EMBEDDING_DIM for word in words}\n</code></pre>\n<p>And getting the following error:</p>\n<pre><code>AttributeError: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\n</code></pre>\n<p>Can anyone please suggest that how can I use the pretrained model &amp; return a dictionary in Gensim 4.0.0?</p>\n",
    "score": 26,
    "creation_date": 1617096492,
    "view_count": 53741,
    "answer_count": 4,
    "tags": "python;nlp;gensim;word2vec;word-embedding"
  },
  {
    "question_id": 45420466,
    "title": "Gensim: KeyError: &quot;word not in vocabulary&quot;",
    "body": "<p>I have a trained Word2vec model using Python's Gensim Library. I have a tokenized list as below. The vocab size is 34 but I am just giving few out of 34:</p>\n\n<pre><code>b = ['let',\n 'know',\n 'buy',\n 'someth',\n 'featur',\n 'mashabl',\n 'might',\n 'earn',\n 'affili',\n 'commiss',\n 'fifti',\n 'year',\n 'ago',\n 'graduat',\n '21yearold',\n 'dustin',\n 'hoffman',\n 'pull',\n 'asid',\n 'given',\n 'one',\n 'piec',\n 'unsolicit',\n 'advic',\n 'percent',\n 'buy']\n</code></pre>\n\n<p><strong>Model</strong></p>\n\n<pre><code>model = gensim.models.Word2Vec(b,min_count=1,size=32)\nprint(model) \n### prints: Word2Vec(vocab=34, size=32, alpha=0.025) ####\n</code></pre>\n\n<p>if I try to get the similarity score by doing <code>model['buy']</code> of one the words in the list, I get the </p>\n\n<blockquote>\n  <p>KeyError: \"word 'buy' not in vocabulary\"</p>\n</blockquote>\n\n<p>Can you guys suggest me what I am doing wrong and what are the ways to check the model which can be further used to train PCA or t-sne in order to visualize similar words forming a topic? Thank you. </p>\n",
    "score": 26,
    "creation_date": 1501516748,
    "view_count": 45913,
    "answer_count": 2,
    "tags": "python;nlp;gensim;word2vec;topic-modeling"
  },
  {
    "question_id": 4199441,
    "title": "Best Algorithmic Approach to Sentiment Analysis",
    "body": "<p>My requirement is taking in news articles and determining if they are positive or negative about a subject.  I am taking the approach outlined below, but I keep reading NLP may be of use here.  All that I have read has pointed at NLP detecting opinion from fact, which I don't think would matter much in my case. I'm wondering two things:</p>\n\n<p>1)  Why wouldn't my algorithm work and/or how can I improve it? ( I know sarcasm would probably be a pitfall, but again I don't see that occurring much in the type of news we will be getting)</p>\n\n<p>2)  How would NLP help, why should I use it?</p>\n\n<p>My algorithmic approach (I have dictionaries of positive, negative, and negation words):</p>\n\n<p>1) Count number of positive and negative words in article</p>\n\n<p>2) If a negation word is found with 2 or 3 words of the positive or negative word, (ie: NOT the best) negate the score.</p>\n\n<p>3) Multiply the scores by weights that have been manually assigned to each word. (1.0 to start)</p>\n\n<p>4) Add up the totals for positive and negative to get the sentiment score.</p>\n",
    "score": 26,
    "creation_date": 1289944638,
    "view_count": 18112,
    "answer_count": 6,
    "tags": "nlp;sentiment-analysis"
  },
  {
    "question_id": 29786985,
    "title": "What&#39;s the disadvantage of LDA for short texts?",
    "body": "<p>I am trying to understand why Latent Dirichlet Allocation(LDA) performs poorly in short text environments like Twitter. I've read the paper 'A biterm topic model for short text', however, I still do not understand \"the sparsity of word co-occurrences\". </p>\n\n<p>From my point of view, the generation part of LDA is reasonable for any kind of texts, but what causes bad results in short texts is the sampling procedure. I am guessing LDA samples a topic for a word based on two parts: (1) topics of other words in the same doc (2) topic assignments of other occurrences of this word. Since the (1) part of a short text cannot reflect the true distribution of it, that causes a poor topic assignment for each word.</p>\n\n<p>If you have found this question, please feel free to post your idea and help me understand this.</p>\n",
    "score": 26,
    "creation_date": 1429671956,
    "view_count": 15250,
    "answer_count": 2,
    "tags": "nlp;lda;topic-modeling"
  },
  {
    "question_id": 13937720,
    "title": "How to train a classifier with only positive and neutral data?",
    "body": "<p>My question : How to train a classifier with only positive and neutral data?</p>\n\n<p>I am building a personalized article recommendation system for education purposes. The data I use is from Instapaper.</p>\n\n<p><strong>Datasets</strong></p>\n\n<p>I only have positive data:\n- Articles that I have read and \"liked\", regardless of read/unread status</p>\n\n<p>And neutral data (because I have expressed interest in it, but I may not like it later anyway):\n- Articles that are unread\n- Articles that I have read and marked as read but I did not \"like\" it</p>\n\n<p>The data I do not have is negative data:\n- Articles that I did not send to Instapaper to read it later (I am not interested, although I have browsed that page/article)\n- Articles that I might not even have clicked into, but I might have or might not have archive it.</p>\n\n<p><strong>My problem</strong></p>\n\n<p>In such a problem, negative data is basically missing. I have thought of the following solution(s) but did not resolve to them yet:</p>\n\n<p>1) Feed a number of negative data to the classifier\nPros: Immediate negative data to teach the classifier\nCons: As the number of articles I like increase, the negative data effect on the classifier dims out</p>\n\n<p>2) Turn the \"neutral\" data into negative data\nPros: Now I have all the positive and (new) negative data I need\nCons: Despite the neutral data is of mild interest to me, I'd still like to get recommendations on such article, but perhaps as a less value class.</p>\n",
    "score": 26,
    "creation_date": 1355849449,
    "view_count": 11442,
    "answer_count": 7,
    "tags": "machine-learning;nlp;recommendation-engine"
  },
  {
    "question_id": 52113939,
    "title": "Spacy, Strange similarity between two sentences",
    "body": "<p>I have downloaded <code>en_core_web_lg</code> model and trying to find similarity between two sentences:</p>\n\n<pre><code>nlp = spacy.load('en_core_web_lg')\n\nsearch_doc = nlp(\"This was very strange argument between american and british person\")\n\nmain_doc = nlp(\"He was from Japan, but a true English gentleman in my eyes, and another one of the reasons as to why I liked going to school.\")\n\nprint(main_doc.similarity(search_doc))\n</code></pre>\n\n<p>Which returns very strange value:</p>\n\n<pre><code>0.9066019751888448\n</code></pre>\n\n<p>These two sentences should not be <strong>90% similar</strong> they have very different meanings.</p>\n\n<p>Why this is happening? Do I need to add some kind of additional vocabulary in order to make similarity result more reasonable?</p>\n",
    "score": 25,
    "creation_date": 1535712919,
    "view_count": 39621,
    "answer_count": 5,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 27139908,
    "title": "Load PreComputed Vectors Gensim",
    "body": "<p>I am using the Gensim Python package to learn a neural language model, and I know that you can provide a training corpus to learn the model. However, there already exist many precomputed word vectors available in text format (e.g. <a href=\"http://www-nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">http://www-nlp.stanford.edu/projects/glove/</a>). Is there some way to initialize a Gensim Word2Vec model that just makes use of some precomputed vectors, rather than having to learn the vectors from scratch?</p>\n\n<p>Thanks! </p>\n",
    "score": 25,
    "creation_date": 1416965741,
    "view_count": 20405,
    "answer_count": 3,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 4083060,
    "title": "Determine if a sentence is an inquiry",
    "body": "<p>How can I detect if a search query is in the form of a question?</p>\n\n<p>For example, a customer might search for \"how do I track my order\" (notice no question mark).</p>\n\n<p>I'm guessing most direct questions would conform to a particular grammar.</p>\n\n<p>Very simple guessing approach:</p>\n\n<pre><code>START WORDS = [who, what, when, where, why, how, is, can, does, do]\n\nisQuestion(sentence):\n  sentence ends with '?'\n  OR sentence starts with one of START WORDS\n</code></pre>\n\n<p>START WORDS list could be longer. The scope is a website search box, so I imagine the list shouldn't need to include too many words.</p>\n\n<p>Is there a library that can do this better than my simple guessing approach? Any improvements on my approach?</p>\n",
    "score": 25,
    "creation_date": 1288741067,
    "view_count": 23168,
    "answer_count": 6,
    "tags": "nlp"
  },
  {
    "question_id": 36182502,
    "title": "add stemming support to CountVectorizer (sklearn)",
    "body": "<p>I'm trying to add stemming to my pipeline in NLP with sklearn.</p>\n\n<pre><code>from nltk.stem.snowball import FrenchStemmer\n\nstop = stopwords.words('french')\nstemmer = FrenchStemmer()\n\n\nclass StemmedCountVectorizer(CountVectorizer):\n    def __init__(self, stemmer):\n        super(StemmedCountVectorizer, self).__init__()\n        self.stemmer = stemmer\n\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc:(self.stemmer.stem(w) for w in analyzer(doc))\n\nstem_vectorizer = StemmedCountVectorizer(stemmer)\ntext_clf = Pipeline([('vect', stem_vectorizer), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='linear', C=1)) ])\n</code></pre>\n\n<p>When using this pipeline with the CountVectorizer of sklearn it works. And if I create manually the features like this it works also.<br/></p>\n\n<pre><code>vectorizer = StemmedCountVectorizer(stemmer)\nvectorizer.fit_transform(X)\ntfidf_transformer = TfidfTransformer()\nX_tfidf = tfidf_transformer.fit_transform(X_counts)\n</code></pre>\n\n<p><strong>EDIT</strong>:</p>\n\n<p>If I try this pipeline on my IPython Notebook it displays the [*] and nothing happens. When I look at my terminal, it gives this error :<br/></p>\n\n<pre><code>Process PoolWorker-12:\nTraceback (most recent call last):\n  File \"C:\\Anaconda2\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\n    self.run()\n  File \"C:\\Anaconda2\\lib\\multiprocessing\\process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Anaconda2\\lib\\multiprocessing\\pool.py\", line 102, in worker\n    task = get()\n  File \"C:\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\pool.py\", line 360, in get\n    return recv()\nAttributeError: 'module' object has no attribute 'StemmedCountVectorizer'\n</code></pre>\n\n<p><em>Example</em></p>\n\n<p>Here is the complete example</p>\n\n<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn import grid_search\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom nltk.stem.snowball import FrenchStemmer\n\nstemmer = FrenchStemmer()\nanalyzer = CountVectorizer().build_analyzer()\n\ndef stemming(doc):\n    return (stemmer.stem(w) for w in analyzer(doc))\n\nX = ['le chat est beau', 'le ciel est nuageux', 'les gens sont gentils', 'Paris est magique', 'Marseille est tragique', 'JCVD est fou']\nY = [1,0,1,1,0,0]\n\ntext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC())])\nparameters = { 'vect__analyzer': ['word', stemming]}\n\ngs_clf = grid_search.GridSearchCV(text_clf, parameters, n_jobs=-1)\ngs_clf.fit(X, Y)\n</code></pre>\n\n<p>If you remove stemming from the parameters it works otherwise it doesn't work.</p>\n\n<p><strong>UPDATE</strong>:</p>\n\n<p>The problem seems to be in the parallelization process because when removing <strong>n_jobs=-1</strong> the problem disappear.</p>\n",
    "score": 25,
    "creation_date": 1458747453,
    "view_count": 33447,
    "answer_count": 3,
    "tags": "python;nlp;scikit-learn"
  },
  {
    "question_id": 11892128,
    "title": "Tutorials For Natural Language Processing",
    "body": "<p>I recently attended a class on <a href=\"https://class.coursera.org/nlp/lecture\" rel=\"noreferrer\">coursera</a> about \"Natural Language Processing\" and I learnt a lot about parsing, IR and other interesting aspects like Q&amp;A etc. though I grasped the concepts well but I did not actually get any practical knowledge of it. Can anyone suggest me good online tutorials or books for Natural Language Processing?</p>\n\n<p>Thanks</p>\n",
    "score": 25,
    "creation_date": 1344547330,
    "view_count": 28852,
    "answer_count": 6,
    "tags": "algorithm;machine-learning;nlp;artificial-intelligence"
  },
  {
    "question_id": 6800509,
    "title": "Are there APIs for text analysis/mining in Java?",
    "body": "<p>I want to know if there is an API to do text analysis in Java. Something that can extract all words in a text, separate words, expressions, etc.  Something that can inform if a word found is a number, date, year, name, currency, etc.</p>\n\n<p>I'm starting the text analysis now, so I only need an API to kickoff. I made a web-crawler, now I need something to analyze the downloaded data. Need methods to count the number of words in a page, similar words, data type and another resources related to the text.</p>\n\n<p>Are there APIs for text analysis in Java?</p>\n\n<p>EDIT: Text-mining, I want to mining the text. An API for Java that provides this.</p>\n",
    "score": 25,
    "creation_date": 1311425794,
    "view_count": 16955,
    "answer_count": 5,
    "tags": "java;api;nlp;analysis;text-mining"
  },
  {
    "question_id": 47118678,
    "title": "Difference between Fasttext .vec and .bin file",
    "body": "<p>I recently downloaded fasttext pretrained model for english. I got two files:</p>\n\n<ol>\n<li>wiki.en.vec</li>\n<li>wiki.en.bin</li>\n</ol>\n\n<p>I am not sure what is the difference between the two files?</p>\n",
    "score": 25,
    "creation_date": 1509861798,
    "view_count": 14408,
    "answer_count": 2,
    "tags": "python;nlp;deep-learning;word2vec;fasttext"
  },
  {
    "question_id": 11624672,
    "title": "How to proceed with NLP task for recognizing intent and slots",
    "body": "<p>I wanted to write a program for asking questions about weather. What are the algorithms and techniques I should start looking at.</p>\n\n<p>ex: Will it be sunny this weekend in Chicago.\nI wanted to know the <strong>intent</strong> = weather query, <strong>date</strong> = this weekend, <strong>location</strong> = chicago.</p>\n\n<p>User can express the same query in many forms. </p>\n\n<p>I would like to solve some constrained form and looking for ideas on how to get started. The solution needs to be just good enough.</p>\n",
    "score": 25,
    "creation_date": 1343107763,
    "view_count": 17289,
    "answer_count": 3,
    "tags": "machine-learning;nlp;artificial-intelligence;text-processing"
  },
  {
    "question_id": 8949517,
    "title": "Is there any Treebank for free?",
    "body": "<p>Is any place I can download Treebank of English phrases for free or less than $100? I need training data containing bunch of syntactic parsed sentences (>1000) in English in any format. Basically all I need is just words in this sentences being recognized by part of speech.</p>\n",
    "score": 25,
    "creation_date": 1327104543,
    "view_count": 20490,
    "answer_count": 3,
    "tags": "nlp;dataset;tagging;corpus"
  },
  {
    "question_id": 57532679,
    "title": "Why &quot;GELU&quot; activation function is used instead of ReLu in BERT?",
    "body": "<p>The activation function <strong>Gaussian Error Linear Units(GELUs)</strong> is used in the popular NLP model <em><strong>BERT</strong></em>. Is there any solid reason ?</p>\n",
    "score": 25,
    "creation_date": 1566005661,
    "view_count": 23190,
    "answer_count": 2,
    "tags": "deep-learning;nlp"
  },
  {
    "question_id": 5141092,
    "title": "Determine the difficulty of an english word",
    "body": "<p>I am working a word based game. My word database contains around 10,000 english words (sorted alphabetically). I am planning to have 5 difficulty levels in the game. Level 1 shows the easiest words and Level 5 shows the most difficult words, relatively speaking.</p>\n\n<p>I need to divide the 10,000 long words list into 5 levels, starting from the easiest words to difficult ones. I am looking for a program to do this for me.</p>\n\n<p><strong>Can someone tell me if there is an algorithm or a method to quantitatively measure the difficulty of an english word?</strong> </p>\n\n<p>I have some thoughts revolving around using the \"<em>word length</em>\" and \"<em>word frequency</em>\" as factors, and come up with a formula or something that accomplishes this.</p>\n",
    "score": 25,
    "creation_date": 1298890709,
    "view_count": 17332,
    "answer_count": 13,
    "tags": "algorithm;nlp;lexical"
  },
  {
    "question_id": 42827175,
    "title": "Gensim: What is difference between word2vec and doc2vec?",
    "body": "<p>I'm kinda newbie and not native english so have some trouble understanding <code>Gensim</code>'s <code>word2vec</code> and <code>doc2vec</code>.</p>\n\n<p>I think both give me some words most similar with query word I request, by <code>most_similar()</code>(after training).</p>\n\n<p>How can tell which case I have to use <code>word2vec</code> or <code>doc2vec</code>?</p>\n\n<p>Someone could explain difference in short word, please?</p>\n\n<p>Thanks.</p>\n",
    "score": 25,
    "creation_date": 1489647267,
    "view_count": 28095,
    "answer_count": 1,
    "tags": "nlp;gensim"
  },
  {
    "question_id": 25902119,
    "title": "scikit-learn TfidfVectorizer meaning?",
    "body": "<p>I was reading about TfidfVectorizer <a href=\"http://stanford.edu/~rjweiss/public_html/IRiSS2013/text2/notebooks/tfidf.html\" rel=\"noreferrer\">implementation</a> of scikit-learn, i don´t understand what´s the output of the method, for example:</p>\n\n<pre><code>new_docs = ['He watches basketball and baseball', 'Julie likes to play basketball', 'Jane loves to play baseball']\nnew_term_freq_matrix = tfidf_vectorizer.transform(new_docs)\nprint tfidf_vectorizer.vocabulary_\nprint new_term_freq_matrix.todense()\n</code></pre>\n\n<p>output:</p>\n\n<pre><code>{u'me': 8, u'basketball': 1, u'julie': 4, u'baseball': 0, u'likes': 5, u'loves': 7, u'jane': 3, u'linda': 6, u'more': 9, u'than': 10, u'he': 2}\n[[ 0.57735027  0.57735027  0.57735027  0.          0.          0.          0.\n   0.          0.          0.          0.        ]\n [ 0.          0.68091856  0.          0.          0.51785612  0.51785612\n   0.          0.          0.          0.          0.        ]\n [ 0.62276601  0.          0.          0.62276601  0.          0.          0.\n   0.4736296   0.          0.          0.        ]]\n</code></pre>\n\n<p>What is?(e.g.: u'me': 8 ):</p>\n\n<pre><code>{u'me': 8, u'basketball': 1, u'julie': 4, u'baseball': 0, u'likes': 5, u'loves': 7, u'jane': 3, u'linda': 6, u'more': 9, u'than': 10, u'he': 2}\n</code></pre>\n\n<p>is this a matrix or just a vector?, i can´t understand what´s telling me the output:</p>\n\n<pre><code>[[ 0.57735027  0.57735027  0.57735027  0.          0.          0.          0.\n   0.          0.          0.          0.        ]\n [ 0.          0.68091856  0.          0.          0.51785612  0.51785612\n   0.          0.          0.          0.          0.        ]\n [ 0.62276601  0.          0.          0.62276601  0.          0.          0.\n   0.4736296   0.          0.          0.        ]]\n</code></pre>\n\n<p>Could anybody explain me in more detail these outputs?</p>\n\n<p>Thanks!</p>\n",
    "score": 25,
    "creation_date": 1410997834,
    "view_count": 31279,
    "answer_count": 3,
    "tags": "machine-learning;nlp;scikit-learn;feature-extraction;document-classification"
  },
  {
    "question_id": 4197751,
    "title": "Is there a formal grammar for english language?",
    "body": "<p>I'm browsing web searching for english language grammar, but i found only few simple examples like:</p>\n\n<pre><code>s -&gt; np vp\nnp -&gt; det n\nvp -&gt; v | v np\ndet -&gt; 'a' | 'the'\nn -&gt; 'woman' | 'man'\nv -&gt; 'shoots' \n</code></pre>\n\n<p>Maybe I don't realise how big this problem is because i tought that grammar has been formalised. Can somebody provide me a source for some expanded formal english grammar?</p>\n",
    "score": 25,
    "creation_date": 1289932953,
    "view_count": 7004,
    "answer_count": 6,
    "tags": "parsing;nlp"
  },
  {
    "question_id": 30016904,
    "title": "Determining tense of a sentence Python",
    "body": "<p>Following several other posts, [e.g. <a href=\"https://stackoverflow.com/questions/3434144/detect-english-verb-tenses-using-nltk\">Detect English verb tenses using NLTK</a> , <a href=\"https://stackoverflow.com/questions/19966345/identifying-verb-tenses-in-python\">Identifying verb tenses in python</a>, <a href=\"https://stackoverflow.com/questions/2539782/python-nltk-figure-out-tense\">Python NLTK figure out tense</a> ] I wrote the following code to determine tense of a sentence in Python using POS tagging: </p>\n\n<pre><code>from nltk import word_tokenize, pos_tag\n\ndef determine_tense_input(sentence):\n    text = word_tokenize(sentence)\n    tagged = pos_tag(text)\n\n    tense = {}\n    tense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n    tense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n    tense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]]) \n    return(tense)\n</code></pre>\n\n<p>This returns a value for the usage of past/present/future verbs, which I typically then take the max value of as the tense of the sentence. The accuracy is moderately decent, but I am wondering if there is a better way of doing this.</p>\n\n<p>For example, is there now by-chance a package written which is more dedicated to extracting the tense of a sentence? [note - 2 of the 3 stack-overflow posts are 4-years old, so things may have now changed]. Or alternatively, should I be using a different parser from within nltk to increase accuracy? If not, hope the above code may help someone else!</p>\n",
    "score": 25,
    "creation_date": 1430673413,
    "view_count": 18053,
    "answer_count": 5,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 7386856,
    "title": "Python Arabic NLP",
    "body": "<p>I'm in the process of assessing the capabilities of the NLTK in processing Arabic text in a research to analyze and extract sentiments.</p>\n\n<p>Question is as follows:</p>\n\n<ol>\n<li>Is the NTLK capable of handling and allows the analysis of Arabic text?</li>\n<li>Is python capable of manipulating\\tokenizing Arabic text?</li>\n<li>Will I be able to parse and store Arabic text using Python?</li>\n</ol>\n\n<p>If python and NTLK aren't the tools for this job, what tools would you recommend (if existent)?</p>\n\n<p>Thank you.</p>\n\n<hr>\n\n<h3>EDIT</h3>\n\n<p>Based on research:</p>\n\n<ol>\n<li>NTLK is only capable of stemming Arabic text: <a href=\"http://text-processing.com/demo/\" rel=\"noreferrer\">Link</a></li>\n<li>Python is capable of handling Arabic text since it supports UTF-8 unicode: <a href=\"http://www.spencegreen.com/2008/12/19/python-arabic-unicode/\" rel=\"noreferrer\">Link</a></li>\n<li>Parsing and Lemmatization of Arabic text can be done using: \nSNLPG (The Stanford Natural Language Processing Group) Statistical Parser: <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"noreferrer\">Link</a></li>\n</ol>\n",
    "score": 25,
    "creation_date": 1315825340,
    "view_count": 18626,
    "answer_count": 1,
    "tags": "python;arabic;nlp"
  },
  {
    "question_id": 92006,
    "title": "How do I determine if a random string sounds like English?",
    "body": "<p>I have an algorithm that generates strings based on a list of input words. How do I separate only the strings that sounds like English words? ie. discard <strong>RDLO</strong> while keeping <strong>LORD</strong>.</p>\n\n<p><strong>EDIT:</strong> To clarify, they do not need to be actual words in the dictionary. They just need to sound like English. For example <strong>KEAL</strong> would be accepted.</p>\n",
    "score": 24,
    "creation_date": 1221740420,
    "view_count": 5849,
    "answer_count": 13,
    "tags": "string;linguistics;nlp"
  },
  {
    "question_id": 1578062,
    "title": "Lemmatization java",
    "body": "<p>I am looking for a <a href=\"http://en.wikipedia.org/wiki/Lemmatisation\" rel=\"noreferrer\">lemmatisation</a> implementation for English in Java. I found a few already, but I need something that does not need to much memory to run (1 GB top).\nThanks. I do not need a stemmer.</p>\n",
    "score": 24,
    "creation_date": 1255700002,
    "view_count": 40637,
    "answer_count": 5,
    "tags": "java;nlp"
  },
  {
    "question_id": 1789254,
    "title": "Clustering text in Python",
    "body": "<p>I need to cluster some text documents and have been researching various options.  It looks like LingPipe can cluster plain text without prior conversion (to vector space etc), but it's the only tool I've seen that explicitly claims to work on strings.</p>\n\n<p>Are there any Python tools that can cluster text directly?  If not, what's the best way to handle this?</p>\n",
    "score": 24,
    "creation_date": 1259059419,
    "view_count": 24070,
    "answer_count": 3,
    "tags": "python;cluster-analysis;nlp"
  },
  {
    "question_id": 796412,
    "title": "How to turn plural words singular?",
    "body": "<p>I'm preparing some table names for an ORM, and I want to turn plural table names into single entity names. My only problem is finding an algorithm that does it reliably. Here's what I'm doing right now:</p>\n\n<ol>\n<li>If a word ends with <em>-ies</em>, I replace the ending with <em>-y</em></li>\n<li>If a word ends with <em>-es</em>, I remove this ending. This doesn't always work however - for example, it replaces <em>Types</em> with <em>Typ</em></li>\n<li>Otherwise, I just remove the trailing <em>-s</em></li>\n</ol>\n\n<p>Does anyone know of a better algorithm?</p>\n",
    "score": 24,
    "creation_date": 1240898717,
    "view_count": 24520,
    "answer_count": 13,
    "tags": "algorithm;nlp;lemmatization"
  },
  {
    "question_id": 26890605,
    "title": "Filter Twitter feeds only by language",
    "body": "<p>I am using Tweepy API for extracting Twitter feeds. I want to extract all Twitter feeds of a specific language only. The language filter works only if <code>track</code> filter is provided. The following code returns 406 error:</p>\n\n<pre><code>l = StdOutListener()\nauth = OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\nstream = Stream(auth, l)\nstream.filter(languages=[\"en\"])\n</code></pre>\n\n<p>How can I extract <strong>all</strong> the tweets from certain language using Tweepy? </p>\n",
    "score": 24,
    "creation_date": 1415806255,
    "view_count": 37777,
    "answer_count": 7,
    "tags": "python;twitter;nlp;tweepy;twitter-streaming-api"
  },
  {
    "question_id": 10098533,
    "title": "Implementing Bag-of-Words Naive-Bayes classifier in NLTK",
    "body": "<p>I basically have the <a href=\"https://stackoverflow.com/questions/2162718/python-nltk-code-snippet-to-train-a-classifier-naive-bayes-using-feature-frequ\">same question as this guy</a>.. The <a href=\"http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html#document-classify-all-words\" rel=\"nofollow noreferrer\">example in the NLTK book</a> for the Naive Bayes classifier considers only whether a word occurs in a document as a feature.. it doesn't consider the frequency of the words as the feature to look at (\"bag-of-words\").</p>\n\n<p><a href=\"https://stackoverflow.com/a/2226115/378622\">One of the answers</a> seems to suggest this can't be done with the built in NLTK classifiers.  Is that the case?  How can I do frequency/bag-of-words NB classification with NLTK?</p>\n",
    "score": 24,
    "creation_date": 1334106000,
    "view_count": 27432,
    "answer_count": 3,
    "tags": "python;machine-learning;nlp;nltk;naivebayes"
  },
  {
    "question_id": 9973596,
    "title": "ArrayList as key in HashMap",
    "body": "<p>Would it be possible to add an <code>ArrayList</code> as the key of <code>HashMap</code>. I would like to keep the frequency count of bigrams. The bigram is the key and the value is its frequency.</p>\n\n<p>For each of the bigrams like \"he is\", I create an <code>ArrayList</code> for it and insert it into the <code>HashMap</code>. But I am not getting the correct output.</p>\n\n<pre><code>public HashMap&lt;ArrayList&lt;String&gt;, Integer&gt; getBigramMap(String word1, String word2) {\n    HashMap&lt;ArrayList&lt;String&gt;, Integer&gt; hm = new HashMap&lt;ArrayList&lt;String&gt;, Integer&gt;();\n    ArrayList&lt;String&gt; arrList1 = new ArrayList&lt;String&gt;();\n    arrList1 = getBigram(word1, word2);\n    if (hm.get(arrList1) != null) {\n        hm.put(arrList1, hm.get(arrList1) + 1);\n    } else {\n        hm.put(arrList1, 1);\n    }\n    System.out.println(hm.get(arrList1));\n    return hm;\n}\n\n\npublic ArrayList&lt;String&gt; getBigram(String word1, String word2) {\n    ArrayList&lt;String&gt; arrList2 = new ArrayList&lt;String&gt;();\n    arrList2.add(word1);\n    arrList2.add(word2);\n    return arrList2;\n}\n</code></pre>\n",
    "score": 24,
    "creation_date": 1333357661,
    "view_count": 63717,
    "answer_count": 10,
    "tags": "java;hashmap;nlp"
  },
  {
    "question_id": 2667057,
    "title": "English dictionary as txt or xml file with support of synonyms",
    "body": "<p>Can someone point me to where I can download English dictionary as a txt or xml file. I am building a simple app for myself and looking for something what I could start using immediately without learning complex API.</p>\n\n<p>Support for synonyms would be great, that is it should be easier to retrieve all the synonyms for a particular word.</p>\n\n<p>It would be absolutely fantastic if the dictionary would be listing British and American spelling of the words where they differ.</p>\n\n<p>Even if it would be small dictionary (a few thousand words) that's OK, I only need it for a small project.</p>\n\n<p>I even would be willing to buy one if the price is reasonable, and the dictionary is easy to use - simple XML would be great.</p>\n\n<p>Any directions please.</p>\n",
    "score": 24,
    "creation_date": 1271677585,
    "view_count": 33360,
    "answer_count": 4,
    "tags": "dictionary;nlp;wordnet"
  },
  {
    "question_id": 2264806,
    "title": "How to automatically determine text quality?",
    "body": "<p>A lot of Natural Language Processing (NLP) algorithms and libraries have a hard time working with random texts from the web, usually because they are presupposing clean, articulate writing. I can understand why that would be easier than parsing YouTube comments.</p>\n\n<p>My question is: given a random piece of text, is there a process to determine whether that text is well written, and is a good candidate for use in NLP? What is the general name for these algorithm?</p>\n\n<p>I would appreciate links to articles, algorithms or code libraries, but I would settle for good search terms.</p>\n",
    "score": 24,
    "creation_date": 1266224232,
    "view_count": 12869,
    "answer_count": 6,
    "tags": "nlp"
  },
  {
    "question_id": 49239941,
    "title": "What is &quot;unk&quot; in the pretrained GloVe vector files (e.g. glove.6B.50d.txt)?",
    "body": "<p>I found \"unk\" token in the glove vector file glove.6B.50d.txt downloaded <a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">from https://nlp.stanford.edu/projects/glove/</a>. Its value is as follows:</p>\n\n<pre><code>unk -0.79149 0.86617 0.11998 0.00092287 0.2776 -0.49185 0.50195 0.00060792 -0.25845 0.17865 0.2535 0.76572 0.50664 0.4025 -0.0021388 -0.28397 -0.50324 0.30449 0.51779 0.01509 -0.35031 -1.1278 0.33253 -0.3525 0.041326 1.0863 0.03391 0.33564 0.49745 -0.070131 -1.2192 -0.48512 -0.038512 -0.13554 -0.1638 0.52321 -0.31318 -0.1655 0.11909 -0.15115 -0.15621 -0.62655 -0.62336 -0.4215 0.41873 -0.92472 1.1049 -0.29996 -0.0063003 0.3954\n</code></pre>\n\n<p>Is it a token to be used for unknown words or is it some kind of abbreviation?</p>\n",
    "score": 24,
    "creation_date": 1520871644,
    "view_count": 10767,
    "answer_count": 2,
    "tags": "neural-network;deep-learning;nlp;word-embedding;glove"
  },
  {
    "question_id": 34881790,
    "title": "Split string into sentences using regex",
    "body": "<p>I have random text stored in <code>$sentences</code>. Using regex, I want to split the text into sentences, see:</p>\n\n<pre><code>function splitSentences($text) {\n    $re = '/                # Split sentences on whitespace between them.\n        (?&lt;=                # Begin positive lookbehind.\n          [.!?]             # Either an end of sentence punct,\n        | [.!?][\\'\"]        # or end of sentence punct and quote.\n        )                   # End positive lookbehind.\n        (?&lt;!                # Begin negative lookbehind.\n          Mr\\.              # Skip either \"Mr.\"\n        | Mrs\\.             # or \"Mrs.\",\n        | T\\.V\\.A\\.         # or \"T.V.A.\",\n                            # or... (you get the idea).\n        )                   # End negative lookbehind.\n        \\s+                 # Split on whitespace between sentences.\n        /ix';\n\n    $sentences = preg_split($re, $text, -1, PREG_SPLIT_NO_EMPTY);\n    return $sentences;\n}\n\n$sentences = splitSentences($sentences);\n\nprint_r($sentences);\n</code></pre>\n\n<p>It works fine.</p>\n\n<p>However, it doesn't split into sentences if there are unicode characters: </p>\n\n<pre><code>$sentences = 'Entertainment media properties.Â Fairy Tail and Tokyo Ghoul.';\n</code></pre>\n\n<p>Or this scenario:</p>\n\n<pre><code>$sentences = \"Entertainment media properties.&amp;Acirc;&amp;nbsp; Fairy Tail and Tokyo Ghoul.\";\n</code></pre>\n\n<p>What can I do to make it work when unicode characters exist in the text?</p>\n\n<p>Here is an <a href=\"http://ideone.com/ZQhPSV\">ideone</a> for testing.</p>\n\n<h2>Bounty info</h2>\n\n<p>I am looking for a complete solution to this. Before posting an answer, please read the comment thread I had with WiktorStribiżew for more relevant info on this issue. </p>\n",
    "score": 24,
    "creation_date": 1453220436,
    "view_count": 3633,
    "answer_count": 7,
    "tags": "php;regex;unicode;nlp"
  },
  {
    "question_id": 30653642,
    "title": "Combining bag of words and other features in one model using sklearn and pandas",
    "body": "<p>I am trying to model the score that a post receives, based on both the text of the post, and other features (time of day, length of post, etc.)</p>\n\n<p>I am wondering how to best combine these different types of features into one model. Right now, I have something like the following (stolen from <a href=\"https://stackoverflow.com/questions/22687365/concatenate-custom-features-with-countvectorizer\">here</a> and <a href=\"https://stackoverflow.com/questions/27993058/pandas-apply-to-dateframe-produces-built-in-method-values-of\">here</a>). </p>\n\n<pre><code>import pandas as pd\n...\n\ndef features(p):\n    terms = vectorizer(p[0])\n    d = {'feature_1': p[1], 'feature_2': p[2]}\n    for t in terms:\n        d[t] = d.get(t, 0) + 1\n    return d\n\nposts = pd.read_csv('path/to/csv')\n\n# Create vectorizer for function to use\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2)).build_tokenizer()\ny = posts[\"score\"].values.astype(np.float32) \nvect = DictVectorizer()\n\n# This is the part I want to fix\ntemp = zip(list(posts.message), list(posts.feature_1), list(posts.feature_2))\ntokenized = map(lambda x: features(x), temp)\nX = vect.fit_transform(tokenized)\n</code></pre>\n\n<p>It seems very silly to extract all of the features I want out of the pandas dataframe, just to zip them all back together. Is there a better way of doing this step?</p>\n\n<p>The CSV looks something like the following:</p>\n\n<pre><code>ID,message,feature_1,feature_2\n1,'This is the text',4,7\n2,'This is more text',3,2\n...\n</code></pre>\n",
    "score": 24,
    "creation_date": 1433450090,
    "view_count": 11942,
    "answer_count": 1,
    "tags": "python;pandas;machine-learning;nlp;scikit-learn"
  },
  {
    "question_id": 15377290,
    "title": "Unsupervised automatic tagging algorithms?",
    "body": "<p>I want to build a web application that lets users upload <em>documents</em>, <em>videos</em>, <em>images</em>, <em>music</em>, and then give them an ability to search them. Think of it as <em>Dropbox</em> + Semantic Search.</p>\n\n<p>When user uploads a new file, e.g. <strong>Document1.docx</strong>, how could I automatically generate tags based on the content of the file? In other words no user input is needed to determine what the file is about. If suppose that <strong>Document1.docx</strong> is a research paper on data mining, then when user searches for <em>data mining</em>, or <em>research paper</em>, or <em>document1</em>, that file should be returned in search results, since <em>data mining</em> and <em>research paper</em> will most likely be potential auto-generated tags for that given document.</p>\n\n<p><em><strong>1. Which algorithms would you recommend for this problem?</em></strong></p>\n\n<p><em><strong>2. Is there an natural language library that could do this for me?</em></strong></p>\n\n<p><em><strong>3. Which machine learning techniques should I look into to improve tagging precision?</em></strong></p>\n\n<p><em><strong>4. How could I extend this to video and image automatic tagging?</em></strong></p>\n\n<p>Thanks in advance!</p>\n",
    "score": 24,
    "creation_date": 1363150100,
    "view_count": 22808,
    "answer_count": 5,
    "tags": "algorithm;machine-learning;nlp;tagging"
  },
  {
    "question_id": 6123212,
    "title": "Are There Any Good C++ Suffix Trie Libraries?",
    "body": "<p>Does anyone know of a really rock solid C++ library for suffix tries?  Other than the one in Mummer?<br>\nIdeally, I'd like:<br>\nSome concept of concurrency.<br>\nGood caching behavior.<br>\nPermissive license.<br>\nSupport for arbitrary alphabets.</p>\n",
    "score": 24,
    "creation_date": 1306320336,
    "view_count": 8034,
    "answer_count": 3,
    "tags": "c++;algorithm;tree;nlp;trie"
  },
  {
    "question_id": 42269313,
    "title": "Interpreting the sum of TF-IDF scores of words across documents",
    "body": "<p>First let's extract the TF-IDF scores per term per document:</p>\n\n<pre><code>from gensim import corpora, models, similarities\ndocuments = [\"Human machine interface for lab abc computer applications\",\n              \"A survey of user opinion of computer system response time\",\n              \"The EPS user interface management system\",\n              \"System and human system engineering testing of EPS\",\n              \"Relation of user perceived response time to error measurement\",\n              \"The generation of random binary unordered trees\",\n              \"The intersection graph of paths in trees\",\n              \"Graph minors IV Widths of trees and well quasi ordering\",\n              \"Graph minors A survey\"]\nstoplist = set('for a of the and to in'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\n</code></pre>\n\n<p>Printing it out:</p>\n\n<pre><code>for doc in corpus_tfidf:\n    print doc\n</code></pre>\n\n<p>[out]:</p>\n\n<pre><code>[(0, 0.4301019571350565), (1, 0.4301019571350565), (2, 0.4301019571350565), (3, 0.4301019571350565), (4, 0.2944198962221451), (5, 0.2944198962221451), (6, 0.2944198962221451)]\n[(4, 0.3726494271826947), (7, 0.27219160459794917), (8, 0.3726494271826947), (9, 0.27219160459794917), (10, 0.3726494271826947), (11, 0.5443832091958983), (12, 0.3726494271826947)]\n[(6, 0.438482464916089), (7, 0.32027755044706185), (9, 0.32027755044706185), (13, 0.6405551008941237), (14, 0.438482464916089)]\n[(5, 0.3449874408519962), (7, 0.5039733231394895), (14, 0.3449874408519962), (15, 0.5039733231394895), (16, 0.5039733231394895)]\n[(9, 0.21953536176370683), (10, 0.30055933182961736), (12, 0.30055933182961736), (17, 0.43907072352741366), (18, 0.43907072352741366), (19, 0.43907072352741366), (20, 0.43907072352741366)]\n[(21, 0.48507125007266594), (22, 0.48507125007266594), (23, 0.48507125007266594), (24, 0.48507125007266594), (25, 0.24253562503633297)]\n[(25, 0.31622776601683794), (26, 0.31622776601683794), (27, 0.6324555320336759), (28, 0.6324555320336759)]\n[(25, 0.20466057569885868), (26, 0.20466057569885868), (29, 0.2801947048062438), (30, 0.40932115139771735), (31, 0.40932115139771735), (32, 0.40932115139771735), (33, 0.40932115139771735), (34, 0.40932115139771735)]\n[(8, 0.6282580468670046), (26, 0.45889394536615247), (29, 0.6282580468670046)]\n</code></pre>\n\n<p>If we want to find the \"saliency\" or \"importance\" of the words within this corpus, <strong>can we simple do the sum of the tf-idf scores across all documents and divide it by the number of documents?</strong> I.e. </p>\n\n<pre><code>&gt;&gt;&gt; tfidf_saliency = Counter()\n&gt;&gt;&gt; for doc in corpus_tfidf:\n...     for word, score in doc:\n...         tfidf_saliency[word] += score / len(corpus_tfidf)\n... \n&gt;&gt;&gt; tfidf_saliency\nCounter({7: 0.12182694202050007, 8: 0.11121194156107769, 26: 0.10886469856464989, 29: 0.10093919463036093, 9: 0.09022272408985754, 14: 0.08705221175200946, 25: 0.08482488519466996, 6: 0.08143359568202602, 10: 0.07480097322359022, 12: 0.07480097322359022, 4: 0.07411881371164887, 13: 0.07117278898823597, 5: 0.07104525967490458, 27: 0.07027283689263066, 28: 0.07027283689263066, 11: 0.060487023243988705, 15: 0.055997035904387725, 16: 0.055997035904387725, 21: 0.05389680556362955, 22: 0.05389680556362955, 23: 0.05389680556362955, 24: 0.05389680556362955, 17: 0.048785635947490406, 18: 0.048785635947490406, 19: 0.048785635947490406, 20: 0.048785635947490406, 0: 0.04778910634833961, 1: 0.04778910634833961, 2: 0.04778910634833961, 3: 0.04778910634833961, 30: 0.045480127933079706, 31: 0.045480127933079706, 32: 0.045480127933079706, 33: 0.045480127933079706, 34: 0.045480127933079706})\n</code></pre>\n\n<p>Looking at the output, could we assume that the most \"prominent\" word in the corpus is:</p>\n\n<pre><code>&gt;&gt;&gt; dictionary[7]\nu'system'\n&gt;&gt;&gt; dictionary[8]\nu'survey'\n&gt;&gt;&gt; dictionary[26]\nu'graph'\n</code></pre>\n\n<p>If so, <strong>what is the mathematical interpretation of the sum of TF-IDF scores of words across documents?</strong></p>\n",
    "score": 24,
    "creation_date": 1487235974,
    "view_count": 13462,
    "answer_count": 5,
    "tags": "python;statistics;nlp;tf-idf;gensim"
  },
  {
    "question_id": 34628224,
    "title": "pronoun resolution backwards",
    "body": "<p>The usual coreference resolution works in the following way:</p>\n\n<p>Provided</p>\n\n<pre><code>The man likes math. He really does.\n</code></pre>\n\n<p>it figures out that </p>\n\n<pre><code>he \n</code></pre>\n\n<p>refers to </p>\n\n<pre><code>the man.\n</code></pre>\n\n<p>There are plenty of tools to do this.</p>\n\n<p>However, is there a way to do it backwards? </p>\n\n<p>For example,</p>\n\n<p>given</p>\n\n<pre><code>The man likes math. The man really does.\n</code></pre>\n\n<p>I want to do the pronoun resolution \"backwards,\"</p>\n\n<p>so that I get an output like</p>\n\n<pre><code>The man likes math. He really does.\n</code></pre>\n\n<p>My input text will mostly be 3~10 sentences, and I'm working with python.</p>\n",
    "score": 24,
    "creation_date": 1452067209,
    "view_count": 1399,
    "answer_count": 1,
    "tags": "python;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 20827741,
    "title": "nltk NaiveBayesClassifier training for sentiment analysis",
    "body": "<p>I am training the <code>NaiveBayesClassifier</code> in Python using sentences, and it gives me the error below. I do not understand what the error might be, and any help would be good. </p>\n\n<p>I have tried many other input formats, but the error remains. The code given below:</p>\n\n<pre><code>from text.classifiers import NaiveBayesClassifier\nfrom text.blob import TextBlob\ntrain = [('I love this sandwich.', 'pos'),\n         ('This is an amazing place!', 'pos'),\n         ('I feel very good about these beers.', 'pos'),\n         ('This is my best work.', 'pos'),\n         (\"What an awesome view\", 'pos'),\n         ('I do not like this restaurant', 'neg'),\n         ('I am tired of this stuff.', 'neg'),\n         (\"I can't deal with this\", 'neg'),\n         ('He is my sworn enemy!', 'neg'),\n         ('My boss is horrible.', 'neg') ]\n\ntest = [('The beer was good.', 'pos'),\n        ('I do not enjoy my job', 'neg'),\n        (\"I ain't feeling dandy today.\", 'neg'),\n        (\"I feel amazing!\", 'pos'),\n        ('Gary is a friend of mine.', 'pos'),\n        (\"I can't believe I'm doing this.\", 'neg') ]\nclassifier = nltk.NaiveBayesClassifier.train(train)\n</code></pre>\n\n<p>I am including the traceback below.</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:\\Users\\5460\\Desktop\\train01.py\", line 15, in &lt;module&gt;\n    all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))\n  File \"C:\\Users\\5460\\Desktop\\train01.py\", line 15, in &lt;genexpr&gt;\n    all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))\n  File \"C:\\Python27\\lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 87, in word_tokenize\n    return _word_tokenize(text)\n  File \"C:\\Python27\\lib\\site-packages\\nltk\\tokenize\\treebank.py\", line 67, in tokenize\n    text = re.sub(r'^\\\"', r'``', text)\n  File \"C:\\Python27\\lib\\re.py\", line 151, in sub\n    return _compile(pattern, flags).sub(repl, string, count)\nTypeError: expected string or buffer\n</code></pre>\n",
    "score": 23,
    "creation_date": 1388336444,
    "view_count": 41767,
    "answer_count": 3,
    "tags": "python;nlp;nltk;sentiment-analysis;textblob"
  },
  {
    "question_id": 3797746,
    "title": "How to do a Python split() on languages (like Chinese) that don&#39;t use whitespace as word separator?",
    "body": "<p>I want to split a sentence into a list of words.</p>\n\n<p>For English and European languages this is easy, just use split()</p>\n\n<pre><code>&gt;&gt;&gt; \"This is a sentence.\".split()\n['This', 'is', 'a', 'sentence.']\n</code></pre>\n\n<p>But I also need to deal with sentences in languages such as Chinese that don't use whitespace as word separator.</p>\n\n<pre><code>&gt;&gt;&gt; u\"这是一个句子\".split()\n[u'\\u8fd9\\u662f\\u4e00\\u4e2a\\u53e5\\u5b50']\n</code></pre>\n\n<p>Obviously that doesn't work.</p>\n\n<p>How do I split such a sentence into a list of words?</p>\n\n<p><strong>UPDATE:</strong></p>\n\n<p>So far the answers seem to suggest that this requires natural language processing techniques and that the word boundaries in Chinese are ambiguous. I'm not sure I understand why. The word boundaries in Chinese seem very definite to me. Each Chinese word/character has a corresponding unicode and is displayed on screen as an separate word/character.</p>\n\n<p>So where does the ambiguity come from. As you can see in my Python console output Python has no problem telling that my example sentence is made up of 5 characters:</p>\n\n<pre><code>这 - u8fd9\n是 - u662f\n一 - u4e00\n个 - u4e2a\n句 - u53e5\n子 - u5b50\n</code></pre>\n\n<p>So obviously Python has no problem telling the word/character boundaries. I just need those words/characters in a list.</p>\n",
    "score": 23,
    "creation_date": 1285503718,
    "view_count": 23225,
    "answer_count": 9,
    "tags": "python;string;unicode;nlp;cjk"
  },
  {
    "question_id": 31440803,
    "title": "How to fetch vectors for a word list with Word2Vec?",
    "body": "<p>I want to create a text file that is essentially a dictionary, with each word being paired with its vector representation through word2vec. I'm assuming the process would be to first train word2vec and then look-up each word from my list and find its representation (and then save it in a new text file)? </p>\n\n<p>I'm new to word2vec and I don't know how to go about doing this. I've read from several of the main sites, and several of the questions on Stack, and haven't found a good tutorial yet.</p>\n",
    "score": 23,
    "creation_date": 1436993418,
    "view_count": 47903,
    "answer_count": 9,
    "tags": "machine-learning;nlp;artificial-intelligence;word2vec"
  },
  {
    "question_id": 450493,
    "title": "Natural English language words",
    "body": "<p>I need the most exhaustive English word list I can find for several types of language processing operations, but I could not find anything on the internet that has good enough quality.</p>\n\n<p>There are 1,000,000 words in the English language including foreign and/or technical words. </p>\n\n<p>Can you please suggest such a source (or close to 500k words) that can be downloaded from the internet that is maybe a bit categorized? What input do you use for your language processing applications?</p>\n",
    "score": 23,
    "creation_date": 1232114756,
    "view_count": 6763,
    "answer_count": 6,
    "tags": "nlp"
  },
  {
    "question_id": 15609324,
    "title": "Training n-gram NER with Stanford NLP",
    "body": "<p>Recently I have been trying to train n-gram entities with Stanford Core NLP. I have followed the following tutorials - <a href=\"http://nlp.stanford.edu/software/crf-faq.shtml#b\" rel=\"noreferrer\">http://nlp.stanford.edu/software/crf-faq.shtml#b</a></p>\n\n<p>With this, I am able to specify only unigram tokens and the class it belongs to. Can any one guide me through so that I can extend it to n-grams. I am trying to extract known entities like movie names from chat data set.  </p>\n\n<p>Please guide me through in case I have mis-interpretted the Stanford Tutorials and the same can be used for the n-gram training. </p>\n\n<p>What I am stuck with is the following property</p>\n\n<pre><code>#structure of your training file; this tells the classifier\n#that the word is in column 0 and the correct answer is in\n#column 1\nmap = word=0,answer=1\n</code></pre>\n\n<p>Here the first column is the word (unigram) and the second column is the entity, for example </p>\n\n<pre><code>CHAPTER O\nI   O\nEmma    PERS\nWoodhouse   PERS\n</code></pre>\n\n<p>Now that I need to train known entities (say movie names) like <strong>Hulk</strong>, <strong>Titanic</strong> etc as movies, it would be easy with this approach. But in case I need to train <strong>I know what you did last summer</strong> or <strong>Baby's day out</strong>, what is the best approach ? </p>\n",
    "score": 23,
    "creation_date": 1364194762,
    "view_count": 16765,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp;opennlp;named-entity-recognition;named-entity-extraction"
  },
  {
    "question_id": 860809,
    "title": "How do you parse a paragraph of text into sentences? (perferrably in Ruby)",
    "body": "<p>How do you take paragraph or large amount of text and break it into sentences (perferably using Ruby) taking into account cases such as Mr. and Dr. and U.S.A?  (Assuming you just put the sentences into an array of arrays)</p>\n\n<p>UPDATE:\nOne possible solution I thought of involves using a parts-of-speech tagger (POST) and a classifier to determine the end of a sentence:</p>\n\n<p>Getting data from Mr. Jones felt the warm sun on his face as he stepped out onto the balcony of his summer home in Italy.  He was happy to be alive.</p>\n\n<p>CLASSIFIER\nMr./PERSON Jones/PERSON felt/O the/O warm/O sun/O on/O his/O face/O as/O he/O stepped/O out/O onto/O the/O balcony/O of/O his/O summer/O home/O in/O Italy/LOCATION ./O He/O was/O happy/O to/O be/O alive/O ./O</p>\n\n<p>POST\nMr./NNP Jones/NNP felt/VBD the/DT warm/JJ sun/NN on/IN his/PRP$ face/NN as/IN he/PRP stepped/VBD out/RP onto/IN the/DT balcony/NN of/IN his/PRP$ summer/NN home/NN in/IN Italy./NNP He/PRP was/VBD happy/JJ to/TO be/VB alive./IN </p>\n\n<p>Can we assume, since Italy is a location, the period is the valid end of the sentence? Since ending on \"Mr.\" would have no other parts-of-speech, can we assume this is not a valid end-of-sentence period? Is this the best answer to the my question? </p>\n\n<p>Thoughts?</p>\n",
    "score": 23,
    "creation_date": 1242254988,
    "view_count": 18320,
    "answer_count": 14,
    "tags": "ruby;text;parsing;split;nlp"
  },
  {
    "question_id": 42094180,
    "title": "SpaCy: how to load Google news word2vec vectors?",
    "body": "<p>I've tried several methods of loading the google news word2vec vectors (<a href=\"https://code.google.com/archive/p/word2vec/\" rel=\"noreferrer\">https://code.google.com/archive/p/word2vec/</a>):</p>\n\n<pre><code>en_nlp = spacy.load('en',vector=False)\nen_nlp.vocab.load_vectors_from_bin_loc('GoogleNews-vectors-negative300.bin')\n</code></pre>\n\n<p>The above gives:</p>\n\n<pre><code>MemoryError: Error assigning 18446744072820359357 bytes\n</code></pre>\n\n<p>I've also tried with the .gz packed vectors; or by loading and saving them with gensim to a new format:</p>\n\n<pre><code>from gensim.models.word2vec import Word2Vec\nmodel = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\nmodel.save_word2vec_format('googlenews2.txt')\n</code></pre>\n\n<p>This file then contains the words and their word vectors on each line.\nI tried to load them with:</p>\n\n<pre><code>en_nlp.vocab.load_vectors('googlenews2.txt')\n</code></pre>\n\n<p>but it returns \"0\".</p>\n\n<p>What is the correct way to do this?</p>\n\n<p><strong>Update:</strong></p>\n\n<p>I can load my own created file into spacy.\nI use a test.txt file with \"string 0.0 0.0 ....\" on each line. Then zip this txt with .bzip2 to test.txt.bz2.\nThen I create a spacy compatible binary file:</p>\n\n<pre><code>spacy.vocab.write_binary_vectors('test.txt.bz2', 'test.bin')\n</code></pre>\n\n<p>That I can load into spacy:</p>\n\n<pre><code>nlp.vocab.load_vectors_from_bin_loc('test.bin')\n</code></pre>\n\n<p>This works!\nHowever, when I do the same process for the googlenews2.txt, I get the following error:</p>\n\n<pre><code>lib/python3.6/site-packages/spacy/cfile.pyx in spacy.cfile.CFile.read_into (spacy/cfile.cpp:1279)()\n\nOSError: \n</code></pre>\n",
    "score": 23,
    "creation_date": 1486482616,
    "view_count": 20921,
    "answer_count": 4,
    "tags": "python;nlp;word2vec;spacy"
  },
  {
    "question_id": 58289342,
    "title": "TF2.0: Translation model: Error when restoring the saved model: Unresolved object in checkpoint (root).optimizer.iter: attributes",
    "body": "<p>I am trying to restore the checkpoints and predict on different sentences <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\" rel=\"noreferrer\">NMT Attention Model</a>. While restoring the checkpoints and predicting, I am getting gibberish results with warning below:</p>\n\n<pre><code>   Unresolved object in checkpoint (root).optimizer.iter: attributes {\n  name: \"VARIABLE_VALUE\"\n  full_name: \"Adam/iter\"\n  checkpoint_key: \"optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE\"\n}\n</code></pre>\n\n<p>Below is the additional warnings that I am getting and the results:</p>\n\n<pre><code>WARNING: Logging before flag parsing goes to stderr.\nW1008 09:57:52.766877 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.iter\nW1008 09:57:52.767037 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_1\nW1008 09:57:52.767082 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_2\nW1008 09:57:52.767120 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.decay\nW1008 09:57:52.767155 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.learning_rate\nW1008 09:57:52.767194 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.embedding.embeddings\nW1008 09:57:52.767228 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.state_spec\nW1008 09:57:52.767262 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.fc.kernel\nW1008 09:57:52.767296 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.fc.bias\nW1008 09:57:52.767329 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.embedding.embeddings\nW1008 09:57:52.767364 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.state_spec\nW1008 09:57:52.767396 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.cell.kernel\nW1008 09:57:52.767429 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.cell.recurrent_kernel\nW1008 09:57:52.767461 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.cell.bias\nW1008 09:57:52.767493 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W1.kernel\nW1008 09:57:52.767526 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W1.bias\nW1008 09:57:52.767558 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W2.kernel\nW1008 09:57:52.767590 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W2.bias\nW1008 09:57:52.767623 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.V.kernel\nW1008 09:57:52.767657 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.V.bias\nW1008 09:57:52.767688 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.kernel\nW1008 09:57:52.767721 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.recurrent_kernel\nW1008 09:57:52.767755 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.bias\nW1008 09:57:52.767786 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.embedding.embeddings\nW1008 09:57:52.767818 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc.kernel\nW1008 09:57:52.767851 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc.bias\nW1008 09:57:52.767884 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.embedding.embeddings\nW1008 09:57:52.767915 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.kernel\nW1008 09:57:52.767949 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.recurrent_kernel\nW1008 09:57:52.767981 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.bias\nW1008 09:57:52.768013 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.kernel\nW1008 09:57:52.768044 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.bias\nW1008 09:57:52.768077 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.kernel\nW1008 09:57:52.768109 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.bias\nW1008 09:57:52.768143 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.kernel\nW1008 09:57:52.768175 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.bias\nW1008 09:57:52.768207 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.kernel\nW1008 09:57:52.768239 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.recurrent_kernel\nW1008 09:57:52.768271 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.bias\nW1008 09:57:52.768303 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.embedding.embeddings\nW1008 09:57:52.768335 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc.kernel\nW1008 09:57:52.768367 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc.bias\nW1008 09:57:52.768399 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.embedding.embeddings\nW1008 09:57:52.768431 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.kernel\nW1008 09:57:52.768463 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.recurrent_kernel\nW1008 09:57:52.768495 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.bias\nW1008 09:57:52.768527 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.kernel\nW1008 09:57:52.768559 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.bias\nW1008 09:57:52.768591 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.kernel\nW1008 09:57:52.768623 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.bias\nW1008 09:57:52.768654 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.kernel\nW1008 09:57:52.768686 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.bias\nW1008 09:57:52.768718 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.kernel\nW1008 09:57:52.768750 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.recurrent_kernel\nW1008 09:57:52.768782 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.bias\nW1008 09:57:52.768816 4594230720 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\nInput: &lt;start&gt; hola &lt;end&gt;\nPredicted translation: ? attack now relax hello \n</code></pre>\n\n<p>The warning at the very end says:</p>\n\n<p>'A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used...'  what does it mean?</p>\n",
    "score": 23,
    "creation_date": 1570547830,
    "view_count": 23149,
    "answer_count": 3,
    "tags": "python-3.x;tensorflow;nlp;tensorflow2.0;machine-translation"
  },
  {
    "question_id": 9350437,
    "title": "Incompatible initial and maximum heap sizes specified",
    "body": "<p>I got this Error when I run a java class which has an NLP library .... </p>\n\n<pre><code>Error occurred during initialization of VM\nIncompatible initial and maximum heap sizes specified\n</code></pre>\n\n<p>any idea how i can solve this error :)</p>\n",
    "score": 23,
    "creation_date": 1329664981,
    "view_count": 81652,
    "answer_count": 2,
    "tags": "java;netbeans;jar;nlp"
  },
  {
    "question_id": 69907682,
    "title": "What are differences between AutoModelForSequenceClassification vs AutoModel",
    "body": "<p>We can create a model from AutoModel(TFAutoModel) function:</p>\n<pre><code>from transformers import AutoModel \nmodel = AutoModel.from_pretrained('distilbert-base-uncase')\n</code></pre>\n<p>In other hand, a model is created by AutoModelForSequenceClassification(TFAutoModelForSequenceClassification):</p>\n<pre><code>from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification('distilbert-base-uncase')\n</code></pre>\n<p>As I know, both models use distilbert-base-uncase library to create models.\nFrom name of methods, the second class( <strong>AutoModelForSequenceClassification</strong> ) is created for Sequence Classification.</p>\n<p>But what are really differences in 2 classes? And how to use them correctly?</p>\n<p>(I searched in huggingface but it is not clear)</p>\n",
    "score": 23,
    "creation_date": 1636515235,
    "view_count": 22710,
    "answer_count": 2,
    "tags": "nlp;text-classification;huggingface-transformers"
  },
  {
    "question_id": 1383503,
    "title": "How to determine the (natural) language of a document?",
    "body": "<p>I have a set of documents in two languages: English and German. There is no usable meta information about these documents, a program can look at the content only. Based on that, the program has to decide which of the two languages the document is written in.</p>\n\n<p>Is there any \"standard\" algorithm for this problem that can be implemented in a few hours' time? Or alternatively, a free .NET library or toolkit that can do this? I know about <a href=\"http://alias-i.com/lingpipe/\" rel=\"noreferrer\">LingPipe</a>, but it is </p>\n\n<ol>\n<li>Java</li>\n<li>Not free for \"semi-commercial\" usage</li>\n</ol>\n\n<p>This problem seems to be surprisingly hard. I checked out the <a href=\"http://www.google.com/uds/samples/language/detect.html\" rel=\"noreferrer\">Google AJAX Language API</a> (which I found by searching this site first), but it was ridiculously bad. For six web pages in German to which I pointed it only one guess was correct. The other guesses were Swedish, English, Danish and French...</p>\n\n<p>A simple approach I came up with is to use a list of stop words. My app already uses such a list for German documents in order to analyze them with Lucene.Net. If my app scans the documents for occurrences of stop words from either language the one with more occurrences would win. A very naive approach, to be sure, but it <em>might</em> be good enough. Unfortunately I don't have the time to become an expert at natural-language processing, although it is an intriguing topic.</p>\n",
    "score": 23,
    "creation_date": 1252162231,
    "view_count": 8294,
    "answer_count": 11,
    "tags": ".net;nlp;text-mining"
  },
  {
    "question_id": 3875382,
    "title": "Lucene Standard Analyzer vs Snowball",
    "body": "<p>Just getting started with Lucene.Net.  I indexed 100,000 rows using standard analyzer, ran some test queries, and noticed plural queries don't return results if the original term was singular.  I understand snowball analyzer adds stemming support, which sounds nice.  However, I'm wondering if there are any drawbacks to gong with snowball over standard?  Am I losing anything by going with it?  Are there any other analyzers out there to consider?</p>\n",
    "score": 23,
    "creation_date": 1286387110,
    "view_count": 19270,
    "answer_count": 3,
    "tags": "full-text-search;lucene;lucene.net;nlp;snowball"
  },
  {
    "question_id": 4543008,
    "title": "Efficient Context-Free Grammar parser, preferably Python-friendly",
    "body": "<p>I am in need of parsing a small subset of English for one of my project, described as a context-free grammar with (1-level) feature structures (<a href=\"http://code.google.com/p/nltk/source/browse/trunk/nltk/examples/grammars/book_grammars/feat0.fcfg?r=8260\">example</a>) and I need to do it efficiently .</p>\n\n<p>Right now I'm using <a href=\"http://www.nltk.org/\">NLTK</a>'s parser which produces the right output but is very slow. For my grammar of ~450 fairly ambiguous non-lexicon rules and half a million lexical entries, parsing simple sentences can take anywhere from 2 to 30 seconds, depending it seems on the number of resulting trees. Lexical entries have little to no effect on performance.</p>\n\n<p>Another problem is that loading the (25MB) grammar+lexicon at the beginning can take up to a minute.</p>\n\n<p>From what I can find in literature, the running time of the algorithm used to parse such a grammar (Earley or CKY) should be linear to the size of the grammar and cubic to the size of the input token list. My experience with NLTK indicates that ambiguity is what hurts the performance most, not the absolute size of the grammar.</p>\n\n<p>So now I'm looking for a CFG parser to replace NLTK. I've been considering <a href=\"http://www.dabeaz.com/ply/\">PLY</a> but I can't tell whether it supports feature structures in CFGs, which are required in my case, and the examples I've seen seem to be doing a lot of procedural parsing rather than just specifying a grammar. Can anybody show me an example of PLY both supporting feature structs and using a declarative grammar?</p>\n\n<p>I'm also fine with any other parser that can do what I need efficiently. A Python interface is preferable but not absolutely necessary.</p>\n",
    "score": 23,
    "creation_date": 1293498375,
    "view_count": 34393,
    "answer_count": 7,
    "tags": "python;parsing;nlp;grammar;nltk"
  },
  {
    "question_id": 63705803,
    "title": "Merge related words in NLP",
    "body": "<p>I'd like to define a new word which includes count values from two (or more) different words. For example:</p>\n<pre><code>Words Frequency\n0   mom 250\n1   2020    151\n2   the 124\n3   19  82\n4   mother  81\n... ... ...\n10  London  6\n11  life    6\n12  something   6\n</code></pre>\n<p>I would like to define mother as <code>mom + mother</code>:</p>\n<pre><code>Words Frequency\n0   mother  331\n1   2020    151\n2   the 124\n3   19  82\n... ... ...\n9   London  6\n10  life    6\n11  something   6\n</code></pre>\n<p>This is a way to alternative define group of words having some meaning (at least for my purpose).</p>\n<p>Any suggestion would be appreciated.</p>\n",
    "score": 23,
    "creation_date": 1599050645,
    "view_count": 10070,
    "answer_count": 6,
    "tags": "python;nlp;cluster-analysis;word2vec;wordnet"
  },
  {
    "question_id": 5265416,
    "title": "How do I approximate &quot;Did you mean?&quot; without using Google?",
    "body": "<p>I am aware of the duplicates of this question:</p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work\">How does the Google “Did you mean?” Algorithm work?</a></li>\n<li><a href=\"https://stackoverflow.com/questions/41424/how-do-you-implement-a-did-you-mean\">How do you implement a “Did you mean”?</a></li>\n<li>... and many others. </li>\n</ul>\n\n<p>These questions are interested in how the algorithm actually works. My question is more like: Let's assume Google did not exist or maybe this feature did not exist and we don't have user input. How does one go about implementing an approximate version of this algorithm? </p>\n\n<p><strong>Why is this interesting?</strong></p>\n\n<p>Ok. Try typing \"<a href=\"http://www.google.com/#sclient=psy&amp;hl=en&amp;site=&amp;source=hp&amp;q=qualfy&amp;aq=f&amp;aqi=&amp;aql=&amp;oq=&amp;pbx=1&amp;bav=on.2,or.&amp;fp=170344a196d61403\" rel=\"nofollow noreferrer\">qualfy</a>\" into Google and it tells you:</p>\n\n<blockquote>\n  <p><strong>Did you mean:</strong> <em>qualify</em></p>\n</blockquote>\n\n<p>Fair enough. It uses Statistical Machine Learning on data collected from billions of users to do this. But now try typing this: \"<a href=\"http://www.google.com/search?sourceid=chrome&amp;ie=UTF-8&amp;q=Trytoreconnectyou\" rel=\"nofollow noreferrer\">Trytoreconnectyou</a>\" into Google and it tells you:</p>\n\n<blockquote>\n  <p><strong>Did you mean:</strong> <em>Try To Reconnect You</em></p>\n</blockquote>\n\n<p>Now this is the more interesting part. How does Google determine this? Have a dictionary handy and guess the most probably words again using user input? And how does it differentiate between a misspelled word and a sentence? </p>\n\n<p>Now considering that most programmers do not have access to input from billions of users, I am looking for the best approximate way to implement this algorithm and what resources are available (datasets, libraries etc.). Any suggestions?</p>\n",
    "score": 23,
    "creation_date": 1299788219,
    "view_count": 4670,
    "answer_count": 7,
    "tags": "algorithm;language-agnostic;nlp;machine-learning"
  },
  {
    "question_id": 26569299,
    "title": "Word2Vec: Number of Dimensions",
    "body": "<p>I am using Word2Vec with a dataset of roughly 11,000,000 tokens looking to do both word similarity (as part of synonym extraction for a downstream task) but I don't have a good sense of how many dimensions I should use with Word2Vec. Does anyone have a good heuristic for the range of dimensions to consider based on the number of tokens/sentences?</p>\n",
    "score": 23,
    "creation_date": 1414291500,
    "view_count": 18885,
    "answer_count": 3,
    "tags": "machine-learning;nlp;word2vec"
  },
  {
    "question_id": 48432300,
    "title": "Using keras tokenizer for new words not in training set",
    "body": "<p>I'm currently using the Keras Tokenizer to create a word index and then matching that word index to the the imported GloVe dictionary to create an embedding matrix.  However, the problem I have is that this seems to defeat one of the advantages of using a word vector embedding since when using the trained model for predictions if it runs into a new word that's not in the tokenizer's word index it removes it from the sequence.  </p>\n\n<pre><code>#fit the tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(texts)\nword_index = tokenizer.word_index\n\n#load glove embedding into a dict\nembeddings_index = {}\ndims = 100\nglove_data = 'glove.6B.'+str(dims)+'d.txt'\nf = open(glove_data)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    value = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = value\nf.close()\n\n#create embedding matrix\nembedding_matrix = np.zeros((len(word_index) + 1, dims))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector[:dims]\n\n#Embedding layer:\nembedding_layer = Embedding(embedding_matrix.shape[0],\n                        embedding_matrix.shape[1],\n                        weights=[embedding_matrix],\n                        input_length=12)\n\n#then to make a prediction\nsequence = tokenizer.texts_to_sequences([\"Test sentence\"])\nmodel.predict(sequence)\n</code></pre>\n\n<p>So is there a way I can still use the tokenizer to transform sentences into an array and still use as much of the words GloVe dictionary as I can instead of only the ones that show up in my training text?  </p>\n\n<p>Edit: Upon further contemplation, I guess one option would be to add a text or texts to the texts that the tokenizer is fit on that includes a list of the keys in the glove dictionary. Though that might mess with some of the statistics if I want to use tf-idf. Is there either a preferable way to doing this or a different better approach?</p>\n",
    "score": 23,
    "creation_date": 1516830923,
    "view_count": 12654,
    "answer_count": 3,
    "tags": "python;machine-learning;nlp;deep-learning;keras"
  },
  {
    "question_id": 2441361,
    "title": "NLP: any easy and good methods to find semantic similarity between words?",
    "body": "<p>I don't know whether StackOverflow covers NLP, so I am gonna give this a shot.\nI am interested to find the semantic relatedness of two words from a specific domain, i.e. \"image quality\" and \"noise\".  I am doing some research to determine if reviews of cameras are positive or negative for a particular attribute of the camera.  (like image quality in each one of the reviews). </p>\n\n<p>However, not everybody uses the exact same wording \"image quality\" in the posts, so I am out to see if there is a way for me to build something like that:</p>\n\n<p>\"image quality\" which includes (\"noise\", \"color\", \"sharpness\", etc etc) \nso I can wrap all everything within one big umbrella.</p>\n\n<p>I am doing this for another language, so Wordnet is not necessarily helpful. And no, I do not work for Google or Microsoft so I do not have data from people's clicking behaviour as input data either.      </p>\n\n<p>However, I do have a lot of text, pos-tagged, segmented etc.          </p>\n",
    "score": 23,
    "creation_date": 1268546960,
    "view_count": 17466,
    "answer_count": 8,
    "tags": "nlp"
  },
  {
    "question_id": 2783033,
    "title": "Text mining with PHP",
    "body": "<p>I'm doing a project for a college class I'm taking.</p>\n\n<p>I'm using PHP to build a simple web app that classify tweets as \"positive\" (or happy) and \"negative\" (or sad) based on a set of dictionaries. The algorithm I'm thinking of right now is Naive Bayes classifier or decision tree.</p>\n\n<p>However, I can't find any PHP library that helps me do some serious language processing. Python has NLTK (<a href=\"http://www.nltk.org\" rel=\"noreferrer\">http://www.nltk.org</a>). Is there anything like that for PHP?</p>\n\n<p>I'm planning to use WEKA as the back end of the web app (by calling Weka in command line from within PHP), but it doesn't seem that efficient.</p>\n\n<p>Do you have any idea what I should use for this project? Or should I just switch to Python?</p>\n\n<p>Thanks</p>\n",
    "score": 23,
    "creation_date": 1273166273,
    "view_count": 18051,
    "answer_count": 6,
    "tags": "php;nlp;data-mining;nltk;weka"
  },
  {
    "question_id": 3573872,
    "title": "How to find out if a sentence is a question (interrogative)?",
    "body": "<p>Is there an open source Java library/algorithm for finding if a particular piece of text is a question or not?\n<br/>\nI am working on a question answering system that needs to analyze if the text input by user is a question.\n<br/> \nI think the problem can probably be solved by using opensource NLP libraries but its obviously more complicated than simple part of speech tagging. So if someone can instead tell the algorithm for it by using an existing opensource NLP library, that would be good too.\n<br/>\nAlso let me know if you know a library/toolkit that uses data mining to solve this problem. Although it will be difficult to get sufficient data for training purposes, I will be able to use stack exchange data for training.</p>\n",
    "score": 23,
    "creation_date": 1282815677,
    "view_count": 9357,
    "answer_count": 3,
    "tags": "java;algorithm;nlp;data-mining;text-processing"
  },
  {
    "question_id": 45113130,
    "title": "How to add new embeddings for unknown words in Tensorflow (training &amp; pre-set for testing)",
    "body": "<p>I am curious as to how I can add a normal-randomized 300 dimension vector (elements' type = tf.float32) whenever a word unknown to the pre-trained vocabulary is encountered. I am using pre-trained GloVe word embeddings, but in some cases, I realize I encounter unknown words, and I want to create a normal-randomized word vector for this new found unknown word. </p>\n\n<p>The problem is that with my current set up, I use <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/lookup/index_table_from_tensor\" rel=\"noreferrer\">tf.contrib.lookup.index_table_from_tensor</a> to convert from words to integers based on the known vocabulary. This function can create new tokens and hash them for some predefined number of out of vocabulary words, but my <code>embed</code> will not contain an embedding for this new unknown hash value. I am uncertain if I can simply append a randomized embedding to the end of the <code>embed</code> list.</p>\n\n<p>I also would like to do this in an efficient way, so pre-built tensorflow function or method involving tensorflow functions would probably be the most efficient. I define pre-known special tokens such as an end of sentence token and a default unknown as the empty string (\"at index 0), but this is limited in its power to learn for various different unknown words. I currently use <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup\" rel=\"noreferrer\">tf.nn.embedding_lookup()</a> as the final embedding step.</p>\n\n<p>I would like to be able to add new random 300d vectors for each unknown word in the training data, and I would also like to add pre-made random word vectors for any unknown tokens not seen in training that is possibly encountered during testing. What is the most efficient way of doing this?</p>\n\n<pre><code>def embed_tensor(string_tensor, trainable=True):\n    \"\"\"    \n    Convert List of strings into list of indicies then into 300d vectors\n    \"\"\"\n    # ordered lists of vocab and corresponding (by index) 300d vector\n    vocab, embed = load_pretrained_glove()\n\n    # Set up tensorflow look up from string word to unique integer\n    vocab_lookup = tf.contrib.lookup.index_table_from_tensor(\n        mapping=tf.constant(vocab),\n        default_value = 0)\n    string_tensor = vocab_lookup.lookup(string_tensor)\n\n    # define the word embedding \n    embedding_init = tf.Variable(tf.constant(np.asarray(embed),\n                                 dtype=tf.float32),\n                                 trainable=trainable,\n                                 name=\"embed_init\")\n\n    # return the word embedded version of the sentence (300d vectors/word)\n    return tf.nn.embedding_lookup(embedding_init, string_tensor)\n</code></pre>\n",
    "score": 23,
    "creation_date": 1500076998,
    "view_count": 13264,
    "answer_count": 3,
    "tags": "python;tensorflow;nlp"
  },
  {
    "question_id": 10856896,
    "title": "Choose or generate canonical variant from multiple sentences",
    "body": "<p>I'm working with an API that maps my GTIN/EAN queries to product data.</p>\n\n<p>Since the data returned originates from merchant product feeds, the following is almost universally the case:</p>\n\n<ul>\n<li>Multiple results per GTIN</li>\n<li>Products' titles are pretty much unstructured</li>\n<li>Products' titles are \"polluted\" with\n<ul>\n<li>SEO-related stuff,</li>\n<li>information about the quantity contained,</li>\n<li>\"buy two, get one free\" offers,</li>\n<li>etc.</li>\n</ul></li>\n</ul>\n\n<p><strong>I'm looking for a programmatic way to either</strong></p>\n\n<ul>\n<li><em><strong>choose</em> the \"cleanest\"/most canonical version available</strong></li>\n<li><strong>or <em>generate</em> a new one that represents the \"lowest common denominator\".</strong></li>\n</ul>\n\n<p>Consider the following example results for a single EAN query:</p>\n\n<ul>\n<li>Nivea Deo Roll-On Dry Impact for Men</li>\n<li>NIVEA DEO Roll on Dry/blau</li>\n<li>Nivea Deo Roll-On Dry Impact for Men, 50 ml, 3er Pack (3 x 50 ml)</li>\n<li>Nivea Deo Roll on Dry/blau 50 ml</li>\n<li>Nivea Deoroller 50ml dry for Men blau   Mindestabnahme: 6 Stück (1 VE)</li>\n<li>NIVEA Deoroller, Dry Impact for Men</li>\n<li>NIVEA DEO Roll on Dry/blau_50 ml</li>\n</ul>\n\n<p><strong>My homebrew approach looks like this:</strong></p>\n\n<ul>\n<li>Basic cleanup:\n<ul>\n<li>Lowercase the titles,</li>\n<li>strip excessive whitespace,</li>\n<li>throw out apparent stopwords such as \"buy\" and \"click\"</li>\n</ul></li>\n<li>Build an array for <code>word =&gt; global occurence</code>\n<ul>\n<li><code>\"Nivea\" =&gt; 7</code></li>\n<li><code>\"Deo\" =&gt; 5</code></li>\n<li><code>\"Deoroller\" =&gt; 2</code></li>\n<li><code>…</code></li>\n<li><code>\"VE\" =&gt; 1</code></li>\n</ul></li>\n<li>Calculate the \"cumulative word value\" for each of the titles\n<ul>\n<li><code>\"Nivea Deo\" =&gt; 12</code></li>\n<li><code>\"Nivea Deoroller VE\" =&gt; 10</code></li>\n</ul></li>\n<li>Divide the cumulative value by the length of the title, resulting in a score\n<ul>\n<li><code>\"Nivea Deo\" =&gt; 6</code></li>\n<li><code>\"Nivea Deoroller VE\" =&gt; 3.34</code></li>\n</ul></li>\n</ul>\n\n<p>Obviously, my approach is pretty basic, error-prone and biased towards short sentences with frequently used words – yielding more or less satisfactory results.</p>\n\n<ul>\n<li><strong>Would you choose a different approach?</strong></li>\n<li><strong>Is there some NLP magic way to take care of the problem that I don't know of?</strong></li>\n</ul>\n",
    "score": 23,
    "creation_date": 1338582302,
    "view_count": 866,
    "answer_count": 4,
    "tags": "php;text-mining;information-extraction;nlp"
  },
  {
    "question_id": 1928997,
    "title": "How to find common phrases in a large body of text",
    "body": "<p>I'm working on a project at the moment where I need to pick out the most common phrases in a huge body of text. For example say we have three sentences like the following:</p>\n\n<ul>\n<li><strong>The dog jumped</strong> over the woman.</li>\n<li><strong>The dog jumped</strong> into the car.</li>\n<li><strong>The dog jumped</strong> up the stairs.</li>\n</ul>\n\n<p>From the above example I would want to extract \"<em>the dog jumped</em>\" as it is the most common phrase in the text. At first I thought, \"oh lets use a directed graph [with repeated nodes]\":</p>\n\n<p><a href=\"http://img.skitch.com/20091218-81ii2femnfgfipd9jtdg32m74f.png\">directed graph http://img.skitch.com/20091218-81ii2femnfgfipd9jtdg32m74f.png</a></p>\n\n<p><strong>EDIT</strong>: Apologies, I made a mistake while making this diagram \"over\", \"into\" and \"up\" should all link back to \"the\".</p>\n\n<p>I was going to maintain a count of how many times a word occurred in each node object (\"the\" would be 6; \"dog\" and \"jumped\", 3; etc.) but despite many other problems the main one came up when we add a few more examples like (please ignore the bad grammar :-)): </p>\n\n<ul>\n<li>Dog jumped up and down.</li>\n<li>Dog jumped like no dog had ever jumped before.</li>\n<li>Dog jumped happily.</li>\n</ul>\n\n<p>We now have a problem since \"<em>dog</em>\" would start a new root node (at the same level as \"the\") and we would not identify \"<em>dog jumped</em>\" as now being the most common phrase. So now I am thinking maybe I could use an undirected graph to map the relationships between all the words and eventually pick out the common phrases but I'm not sure how this is going to work either, as you lose the important relationship of order between the words. </p>\n\n<p>So does anyone have any general ideas on how to identify common phrases in a large body of text and what data structure I would use.</p>\n\n<p>Thanks,\nBen</p>\n",
    "score": 23,
    "creation_date": 1261151545,
    "view_count": 16487,
    "answer_count": 1,
    "tags": "data-structures;graph;data-mining;text-analysis"
  },
  {
    "question_id": 9343929,
    "title": "How to stem words in python list?",
    "body": "<p>I have python list like below </p>\n\n<pre><code>documents = [\"Human machine interface for lab abc computer applications\",\n             \"A survey of user opinion of computer system response time\",\n             \"The EPS user interface management system\",\n             \"System and human system engineering testing of EPS\",\n             \"Relation of user perceived response time to error measurement\",\n             \"The generation of random binary unordered trees\",\n             \"The intersection graph of paths in trees\",\n             \"Graph minors IV Widths of trees and well quasi ordering\",\n             \"Graph minors A survey\"]\n</code></pre>\n\n<p>Now i need to stem it (each word) and get another list. How do i do that ? </p>\n",
    "score": 22,
    "creation_date": 1329589752,
    "view_count": 44495,
    "answer_count": 7,
    "tags": "python;nlp"
  },
  {
    "question_id": 1077600,
    "title": "Converting words to numbers in PHP",
    "body": "<p>I am trying to convert numerical values written as words into integers. For example,</p>\n<pre><code>iPhone has two hundred and thirty thousand seven hundred and eighty three apps\n</code></pre>\n<p>would become</p>\n<pre><code>iPhone has 230783 apps\n</code></pre>\n<p>Is there any library or function that does this?</p>\n",
    "score": 22,
    "creation_date": 1246589724,
    "view_count": 25169,
    "answer_count": 6,
    "tags": "php;nlp;numbers"
  },
  {
    "question_id": 28618400,
    "title": "How to identify the subject of a sentence?",
    "body": "<p>Can Python + NLTK be used to identify the subject of a sentence? From what I have learned till now is that a sentence can be broken into a head and its dependents. For e.g. \"I shot an elephant\". In this sentence, I and elephant are dependents to shot. But How do I discern that the subject in this sentence is I. </p>\n",
    "score": 22,
    "creation_date": 1424385488,
    "view_count": 38562,
    "answer_count": 7,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 1218335,
    "title": "&quot;Stop words&quot; list for English?",
    "body": "<p>I'm generating some statistics for some English-language text and I would like to skip uninteresting words such as \"a\" and \"the\".</p>\n\n<ul>\n<li>Where can I find some lists of these uninteresting words?</li>\n<li>Is a list of these words the same as a list of the most frequently used words in English?</li>\n</ul>\n\n<p>update: these are apparently called \"stop words\" and not \"skip words\".</p>\n",
    "score": 22,
    "creation_date": 1249197556,
    "view_count": 21500,
    "answer_count": 6,
    "tags": "language-agnostic;indexing;filtering;stop-words;nlp"
  },
  {
    "question_id": 27032517,
    "title": "what does the vector of a word in word2vec represents?",
    "body": "<p><a href=\"https://code.google.com/p/word2vec/\" rel=\"noreferrer\">word2vec</a> is a open source tool by Google: </p>\n\n<ul>\n<li><p>For each word it provides a vector of float values, what exactly do they represent?</p></li>\n<li><p>There is also a paper on <a href=\"http://cs.stanford.edu/~quocle/paragraph_vector.pdf\" rel=\"noreferrer\">paragraph vector</a> can anyone explain how they are using word2vec in order to obtain fixed length vector for a paragraph.</p></li>\n</ul>\n",
    "score": 22,
    "creation_date": 1416462050,
    "view_count": 12133,
    "answer_count": 2,
    "tags": "machine-learning;nlp;neural-network;gensim"
  },
  {
    "question_id": 75586733,
    "title": "ChatGPT Token Limit",
    "body": "<p>I want ChatGPT to remember past conversations and have a consistent (stateful) conversation.</p>\n<p>I have seen several code of ChatGPT prompt engineering.</p>\n<p>There were two ways to design the prompt shown below (pseudo code):</p>\n<ol>\n<li><p><strong>Use a single input</strong> (Cheap) &lt;- Better if possible</p>\n</li>\n<li><p><strong>Stack all of previous history</strong> (Expensive, Token Limitation)</p>\n</li>\n</ol>\n<pre class=\"lang-py prettyprint-override\"><code>def openai_chat(prompt):\n    completions = openai.Completion.create(\n        engine = &quot;text-davinci-003&quot;,\n        prompt = prompt,\n        max_tokens = 1024,\n        n = 1,\n        temperature = 0.8,\n    )\n    response = completions.choices[0].text.strip()\n    return response\n\n# 1. Use a single input\nwhile True:\n    prompt = input(&quot;User: &quot;)\n    completion = openai_chat(prompt)\n\n# 2. Stack all of previous history (prompt + completion)\nprompt = &quot;&quot;\nwhile True:\n    cur_prompt = input(&quot;User: &quot;)\n    prompt += cur_prompt  # pseudo code\n    completion = openai_chat(prompt)\n    prompt += completion  # pseudo code\n</code></pre>\n<p>Is it possible to choose the first way (the cheap one) to have a consistent conversation?</p>\n<p>In other words, does ChatGPT remember past history even if the prompt only has the current input?</p>\n",
    "score": 22,
    "creation_date": 1677542809,
    "view_count": 30412,
    "answer_count": 4,
    "tags": "text;nlp;prompt;openai-api;gpt-3"
  },
  {
    "question_id": 3053580,
    "title": "Parsing a string for dates in PHP",
    "body": "<p>Given an arbitrary string, for example (<code>\"I'm going to play croquet next Friday\"</code> or <code>\"Gadzooks, is it 17th June already?\"</code>), how would you go about extracting the dates from there?</p>\n\n<p>If this is looking like a good candidate for the too-hard basket, perhaps you could suggest an alternative. I want to be able to parse Twitter messages for dates. The tweets I'd be looking at would be ones which users are directing at this service, so they could be coached into using an easier format, however I'd like it to be as transparent as possible. Is there a good middle ground you could think of?</p>\n",
    "score": 22,
    "creation_date": 1276694500,
    "view_count": 2174,
    "answer_count": 9,
    "tags": "php;datetime;parsing;nlp"
  },
  {
    "question_id": 56241856,
    "title": "AttributeError: module &#39;torch&#39; has no attribute &#39;_six&#39;. Bert model in Pytorch",
    "body": "<p>I tried to load pre-trained model by using BertModel class in pytorch.</p>\n\n<p>I have _six.py under torch, but it still shows module 'torch' has no attribute '_six'</p>\n\n<pre><code>import torch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n# Load pre-trained model (weights)\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval()\n</code></pre>\n\n<pre><code>~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __setattr__(self, name, value)\n    551                                     .format(torch.typename(value), name))\n    552                 modules[name] = value\n--&gt; 553             else:\n    554                 buffers = self.__dict__.get('_buffers')\n    555                 if buffers is not None and name in buffers:\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in register_parameter(self, name, param)\n    140             raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n    141         elif hasattr(self, name) and name not in self._parameters:\n--&gt; 142             raise KeyError(\"attribute '{}' already exists\".format(name))\n    143 \n    144         if param is None:\n\nAttributeError: module 'torch' has no attribute '_six'\n</code></pre>\n",
    "score": 22,
    "creation_date": 1558453281,
    "view_count": 31398,
    "answer_count": 6,
    "tags": "python;deep-learning;nlp;pytorch;bert-language-model"
  },
  {
    "question_id": 44492430,
    "title": "How does spacy use word embeddings for Named Entity Recognition (NER)?",
    "body": "<p>I'm trying to train an NER model using <code>spaCy</code> to identify locations, (person) names, and organisations. I'm trying to understand how <code>spaCy</code> recognises entities in text and I've not been able to find an answer. From <a href=\"https://github.com/explosion/spaCy/issues/491\" rel=\"noreferrer\">this issue</a> on Github and <a href=\"https://github.com/explosion/spaCy/blob/master/examples/training/train_ner_standalone.py\" rel=\"noreferrer\">this example</a>, it appears that spaCy uses a number of features present in the text such as POS tags, prefixes, suffixes, and other character and word-based features in the text to train an Averaged Perceptron.</p>\n\n<p>However, nowhere in the code does it appear that <code>spaCy</code> uses the GLoVe embeddings (although each word in the sentence/document appears to have them, if present in the GLoVe corpus).</p>\n\n<p>My questions are - </p>\n\n<ol>\n<li>Are these used in the NER system now? </li>\n<li>If I were to switch out the word vectors to a different set, should I expect performance to change in a meaningful way?</li>\n<li>Where in the code can I find out how (if it all) <code>spaCy</code> is using the word vectors?</li>\n</ol>\n\n<p>I've tried looking through the Cython code, but I'm not able to understand whether the labelling system uses word embeddings.</p>\n",
    "score": 22,
    "creation_date": 1497247718,
    "view_count": 9318,
    "answer_count": 1,
    "tags": "python;nlp;named-entity-recognition;spacy"
  },
  {
    "question_id": 1269146,
    "title": "How to strip headers/footers from Project Gutenberg texts?",
    "body": "<p>I've tried various methods to strip the license from Project Gutenberg texts, for use as a corpus for a language learning project, but I can't seem to come up with an unsupervised, reliable approach.  The best heuristic I've come up with so far is stripping the first twenty eight lines and the last 398, which worked for a large number of the texts.  Any suggestions as to ways I can automatically strip the text (which is very similar for lots of the texts, but with slight differences in each case, and a few different templates, as well), as well as suggestions for how to verify that the text has been stripped accurately, would be very useful.</p>\n",
    "score": 22,
    "creation_date": 1250117290,
    "view_count": 4164,
    "answer_count": 4,
    "tags": "nlp;text-processing;heuristics;corpus;stripping"
  },
  {
    "question_id": 21395011,
    "title": "Python module with access to english dictionaries including definitions of words",
    "body": "<p>I am looking for a python module that helps me get the definition(s) from an english dictionary for a word.</p>\n\n<p>There is of course <code>enchant</code>, which helps me check if the word exists in the English language, but it does not provide definitions of them (at least I don't see anything like that in the docs)</p>\n\n<p>There is also WordNet, which is accessible with NLTK. It has definitions and even sample sentences, but WordNet does not contain all English words. Common words like \"how\", \"I\", \"You\", \"should\", \"could\"... are not part of WordNet.</p>\n\n<p>Is there any python module that gives access to a full english dictionary including definitions of words?</p>\n",
    "score": 21,
    "creation_date": 1390871242,
    "view_count": 38251,
    "answer_count": 5,
    "tags": "python;dictionary;module;nlp;nltk"
  },
  {
    "question_id": 66675261,
    "title": "How can i work with Example for nlp.update problem with spacy3.0",
    "body": "<p>i am trying to train my data with spacy v3.0 and appareantly the nlp.update do not accept any tuples. Here is the piece of code:</p>\n<pre><code>import spacy\nimport random\nimport json\nnlp = spacy.blank(&quot;en&quot;)\nner = nlp.create_pipe(&quot;ner&quot;)\nnlp.add_pipe('ner')\nner.add_label(&quot;label&quot;)\n# Start the training\nnlp.begin_training()\n# Loop for 40 iterations\nfor itn in range(40):\n    # Shuffle the training data\n    random.shuffle(TRAINING_DATA)\n    losses = {}\n# Batch the examples and iterate over them\n    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n        texts = [text for text, entities in batch]\n        annotations = [entities for text, entities in batch]\n# Update the model\n        nlp.update(texts, annotations, losses=losses, drop=0.3)\n    print(losses)\n</code></pre>\n<p>and i am receiving error</p>\n<pre><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-79-27d69961629b&gt; in &lt;module&gt;\n     18         annotations = [entities for text, entities in batch]\n     19 # Update the model\n---&gt; 20         nlp.update(texts, annotations, losses=losses, drop=0.3)\n     21     print(losses)\n\n~\\Anaconda3\\lib\\site-packages\\spacy\\language.py in update(self, examples, _, drop, sgd, losses, component_cfg, exclude)\n   1086         &quot;&quot;&quot;\n   1087         if _ is not None:\n-&gt; 1088             raise ValueError(Errors.E989)\n   1089         if losses is None:\n   1090             losses = {}\n\nValueError: [E989] `nlp.update()` was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of `(text, annotation)` tuples. \n</code></pre>\n<p>I set my train data format:</p>\n<pre><code>TRAINING_DATA = []\nfor entry in labeled_data:\n    entities = []\n    for e in entry['labels']:\n        entities.append((e[0], e[1],e[2]))\n    spacy_entry = (entry['text'], {&quot;entities&quot;: entities})\n    TRAINING_DATA.append(spacy_entry)\n</code></pre>\n<p>My train data looks like this:</p>\n<pre><code>[('Part List', {'entities': []}), ('pending', {'entities': []}), ('3D Printing', {'entities': [(0, 11, 'Process')]}), ('Recommended to use a FDM 3D printer with PLA material.', {'entities': [(25, 36, 'Process'), (41, 44, 'Material')]}), ('ï»¿', {'entities': []}), ('No need supports or rafts.', {'entities': []}), ('Resolution: 0.20mm', {'entities': []}), ('Fill density 20%', {'entities': []}), ('As follows from the analysis, part of the project is devoted to 3D', {'entities': [(64, 66, 'Process')]}), ('printing, as all static components were created using 3D modelling and', {'entities': [(54, 66, 'Process')]}), ('subsequent printing.', {'entities': []}), ('ï»¿', {'entities': []}), ('In our project, we created several versions of the', {'entities': []}), ('model during modelling, which we will describe and document in the', {'entities': []}), ('following subchapters. As a tool for 3D modelling, we used the Sketchup', {'entities': [(37, 49, 'Process')]}), ('Make tool, version from 2017. The main reason was the high degree of', {'entities': []}), ('intuitiveness and simplicity of the tool, as we had not encountered 3D', {'entities': [(68, 70, 'Process')]}), ('modelling before and needed a relatively flexible and efficient tool to', {'entities': []}), ('guarantee the desired result. with zero previous experience.', {'entities': []}), ('In this version, which is shown in the figures Figure 13 - Version no. 2 side view and Figure 24 - Version no. 2 - front view, for the first time, the specific dimensions of the infuser were clarified and', {'entities': []}), ('modelled. The details of the lower servo attachment, the cable hole in', {'entities': []}), ('the main mast, the winding cylinder mounting, the protrusion on the', {'entities': [(36, 44, 'Process')]}), ('winding cylinder for holding the tea bag, the preparation for fitting', {'entities': []}), ('the wooden and aluminium plate and the shape of the cylinder end that', {'entities': [(15, 25, 'Material')]}), ('exactly fit the servo were also reworked.', {'entities': []}), ('After the creation of this', {'entities': []}), ('version of the model, this model was subsequently officially consulted', {'entities': []}), ('and commented on for the first time.', {'entities': []}), ('In this version, which is shown in the figures Figure 13 - Version no. 2 side view and Figure 24 - Version no. 2 - front view, for the first time, the specific dimensions of the infuser were clarified and', {'entities': []}), ('modelled. The details of the lower servo attachment, the cable hole in', {'entities': []}), ('the main mast, the winding cylinder mounting, the protrusion on the', {'entities': [(36, 44, 'Process')]})]\n</code></pre>\n<p>I would appreciate your help as a new contributor. Thanks a lot!</p>\n",
    "score": 21,
    "creation_date": 1615991819,
    "view_count": 20715,
    "answer_count": 4,
    "tags": "nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 8077641,
    "title": "How to get the WordNet synset given an offset ID?",
    "body": "<p>I have a WordNet synset offset (for example <code>id=\"n#05576222\"</code>). Given this offset, how can I get the synset using Python?</p>\n",
    "score": 21,
    "creation_date": 1320918650,
    "view_count": 14370,
    "answer_count": 4,
    "tags": "python;python-2.7;nlp;nltk;wordnet"
  },
  {
    "question_id": 3466972,
    "title": "How to split a string into words. Ex: &quot;stringintowords&quot; -&gt; &quot;String Into Words&quot;?",
    "body": "<p>What is the right way to split a string into words ?\n(string doesn't contain any spaces or punctuation marks)</p>\n\n<p>For example: \"stringintowords\" -> \"String Into Words\"</p>\n\n<p>Could you please advise what algorithm should be used here ?</p>\n\n<p>! Update: For those who think this question is just for curiosity. This algorithm could be used to camеlcase domain names (\"sportandfishing .com\" -> \"SportAndFishing .com\") and this algo is currently used by aboutus dot org to do this conversion dynamically.</p>\n",
    "score": 21,
    "creation_date": 1281611407,
    "view_count": 21087,
    "answer_count": 10,
    "tags": "algorithm;nlp;dynamic-programming;split;text-segmentation"
  },
  {
    "question_id": 62948332,
    "title": "How to add attention layer to a Bi-LSTM",
    "body": "<p>I am developing a Bi-LSTM model and want to add a attention layer to it. But I am not getting how to add it.</p>\n<p>My current code for the model is</p>\n<pre><code>model = Sequential()\nmodel.add(Embedding(max_words, 1152, input_length=max_len, weights=[embeddings]))\nmodel.add(BatchNormalization())\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n</code></pre>\n<p>And the model summary is</p>\n<pre><code>Model: &quot;sequential_1&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 1152, 1152)        278396928 \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 1152, 1152)        4608      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 1152, 1152)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1152, 1152)        0         \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 64)                303360    \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 64)                256       \n_________________________________________________________________\nactivation_2 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 278,705,217\nTrainable params: 278,702,785\nNon-trainable params: 2,432\n</code></pre>\n",
    "score": 21,
    "creation_date": 1594967311,
    "view_count": 12303,
    "answer_count": 2,
    "tags": "python-3.x;tensorflow;machine-learning;keras;nlp"
  },
  {
    "question_id": 40542523,
    "title": "NLTK: corpus-level bleu vs sentence-level BLEU score",
    "body": "<p>I have imported nltk in python to calculate BLEU Score on Ubuntu. I understand how sentence-level BLEU score works, but I don't understand how corpus-level BLEU score work.</p>\n\n<p>Below is my code for corpus-level BLEU score:</p>\n\n<pre><code>import nltk\n\nhypothesis = ['This', 'is', 'cat'] \nreference = ['This', 'is', 'a', 'cat']\nBLEUscore = nltk.translate.bleu_score.corpus_bleu([reference], [hypothesis], weights = [1])\nprint(BLEUscore)\n</code></pre>\n\n<p>For some reason, the bleu score is 0 for the above code. I was expecting a corpus-level BLEU score of at least 0.5.</p>\n\n<p>Here is my code for sentence-level BLEU score</p>\n\n<pre><code>import nltk\n\nhypothesis = ['This', 'is', 'cat'] \nreference = ['This', 'is', 'a', 'cat']\nBLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [1])\nprint(BLEUscore)\n</code></pre>\n\n<p>Here the sentence-level BLEU score is 0.71 which I expect, taking into account the brevity-penalty and the missing word \"a\". However, I don't understand how corpus-level BLEU score work.</p>\n\n<p>Any help would be appreciated.</p>\n",
    "score": 21,
    "creation_date": 1478846687,
    "view_count": 38576,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;nltk;bleu"
  },
  {
    "question_id": 24073030,
    "title": "What are co-occurence matrixes and how are they used in NLP?",
    "body": "<p>The <a href=\"https://pypi.python.org/pypi/google-ngram-downloader/\" rel=\"noreferrer\">pypi docs for a google ngram downloader</a> say that &quot;sometimes you need an aggregate data over the dataset. For example to build a co-occurrence matrix.&quot;</p>\n<p>The wikipedia for co-occurence matrix has to do with image processing and googling the term seems to bring up some sort of SEO trick.</p>\n<p>So what are co-occurrence matrixes (in computational linguistics/NLP)? How are they used in NLP?</p>\n",
    "score": 21,
    "creation_date": 1402020077,
    "view_count": 33996,
    "answer_count": 2,
    "tags": "nlp"
  },
  {
    "question_id": 36966019,
    "title": "How areTF-IDF calculated by the scikit-learn TfidfVectorizer",
    "body": "<p>I run the following code to convert the text matrix to TF-IDF matrix.</p>\n<pre><code>text = ['This is a string','This is another string','TFIDF computation calculation','TfIDF is the product of TF and IDF']\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)\n                    \nX = vectorizer.fit_transform(text)\nX_vocab = vectorizer.get_feature_names_out()\nX_mat = X.todense()\nX_idf = vectorizer.idf_\n</code></pre>\n<p>I get the following output</p>\n<p>X_vocab =</p>\n<pre><code>[u'calculation',\n u'computation',\n u'idf',\n u'product',\n u'string',\n u'tf',\n u'tfidf']\n</code></pre>\n<p>and X_mat =</p>\n<pre><code>  ([[ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,\n      0.        ,  0.        ],\n    [ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,\n      0.        ,  0.        ],\n    [ 1.91629073,  1.91629073,  0.        ,  0.        ,  0.        ,\n      0.        ,  1.51082562],\n    [ 0.        ,  0.        ,  1.91629073,  1.91629073,  0.        ,\n      1.91629073,  1.51082562]])\n</code></pre>\n<p>Now I dont understand how these scores are computed. My idea is that for the text[0], score for only 'string' is computed and there is a score in the 5th coloumn. But as TF_IDF is the product of term frequency which is 2 and IDF which is log(4/2) is 1.39 and not 1.51 as shown in the matrix. How is the TF-IDF score calculated in scikit-learn.</p>\n",
    "score": 21,
    "creation_date": 1462101384,
    "view_count": 14161,
    "answer_count": 3,
    "tags": "nlp;scikit-learn;tf-idf"
  },
  {
    "question_id": 57984502,
    "title": "How to access/use Google&#39;s pre-trained Word2Vec model without manually downloading the model?",
    "body": "<p>I want to analyse some text on a Google Compute server on Google Cloud Platform (GCP) using the Word2Vec model.</p>\n\n<p>However, the un-compressed word2vec model from <a href=\"https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\" rel=\"noreferrer\">https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/</a> is over 3.5GB and it will take time to download it manually and upload it to a cloud instance. </p>\n\n<p>Is there any way to access this (or any other) pre-trained Word2Vec model on a Google Compute server without uploading it myself?</p>\n",
    "score": 21,
    "creation_date": 1568775958,
    "view_count": 18830,
    "answer_count": 3,
    "tags": "python;google-cloud-platform;nlp;google-compute-engine;word2vec"
  },
  {
    "question_id": 21160310,
    "title": "training data format for NLTK punkt",
    "body": "<p>I would like to run <code>nltk</code> <code>Punkt</code> to split sentences. There is no training model so I train model separately, but I am not sure if the training data format I am using is correct.</p>\n\n<p>My training data is one sentence per line. I wasn't able to find any documentation about this, only this thread (<a href=\"https://groups.google.com/forum/#!topic/nltk-users/bxIEnmgeCSM\" rel=\"nofollow noreferrer\">https://groups.google.com/forum/#!topic/nltk-users/bxIEnmgeCSM</a>) sheds some light about training data format.</p>\n\n<p>What is the correct training data format for <code>NLTK</code> <code>Punkt</code> sentence tokenizer?</p>\n",
    "score": 21,
    "creation_date": 1389871141,
    "view_count": 9791,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 191248,
    "title": "Latent Dirichlet Allocation, pitfalls, tips and programs",
    "body": "<p>I'm experimenting with <a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"noreferrer\">Latent Dirichlet Allocation</a> for topic disambiguation and assignment, and I'm looking for advice.</p>\n\n<ol>\n<li>Which program is the \"best\", where best is some combination of easiest to use, best prior estimation, fast</li>\n<li>How do I incorporate my intuitions about topicality.  Let's say I think I know that some items in the corpus are really in the same category, like all articles by the same author.  Can I add that into the analysis?</li>\n<li>Any unexpected pitfalls or tips I should know before embarking?</li>\n</ol>\n\n<p>I'd prefer is there are R or Python front ends for whatever program, but I expect (and accept) that I'll be dealing with C.  </p>\n",
    "score": 21,
    "creation_date": 1223644987,
    "view_count": 9706,
    "answer_count": 6,
    "tags": "algorithm;statistics;nlp"
  },
  {
    "question_id": 220187,
    "title": "Algorithms or libraries for textual analysis, specifically: dominant words, phrases across text, and collection of text",
    "body": "<p>I'm working on a project where I need to analyze a page of text and collections of pages of text to determine dominant words.   I'd like to know if there is a library (prefer c# or java) that will handle the heavy lifting for me.  If not, is there an algorithm or multiple that would achieve my goals below.  </p>\n\n<p>What I want to do is similar to word clouds built from a url or rss feed that you find on the web, except I don't want the visualization.  They are used all the time for analyzing the presidential candidate speeches to see what the theme or most used words are.  </p>\n\n<p>The complication, is that I need to do this on thousands of short documents, and then collections or categories of these documents.  </p>\n\n<p>My initial plan was to parse the document out, then filter common words - of, the, he, she, etc..  Then count the number of times the remaining words show up in the text (and overall collection/category).  </p>\n\n<p>The problem is that in the future, I would like to handle stemming, plural forms, etc..   I would also like to see if there is a way to identify important phrases. (Instead of a count of a word, the count of a phrase being 2-3 words together)</p>\n\n<p>Any guidance on a strategy, libraries or algorithms that would help are appreciated.  </p>\n",
    "score": 21,
    "creation_date": 1224542302,
    "view_count": 11035,
    "answer_count": 7,
    "tags": "algorithm;text;nlp;analysis;lexical-analysis"
  },
  {
    "question_id": 11351290,
    "title": "nltk tokenization and contractions",
    "body": "<p>I'm tokenizing text with nltk, just sentences fed to wordpunct_tokenizer. This splits contractions (e.g. 'don't' to 'don' +\" ' \"+'t') but I want to keep them as one word. I'm refining my methods for a more measured  and precise tokenization of text, so I need to delve deeper into the nltk tokenization module beyond simple tokenization. </p>\n\n<p>I'm guessing this is common and I'd like feedback from others who've maybe had to deal with the particular issue before.</p>\n\n<p>edit: </p>\n\n<p>Yeah this a general, splattershot question I know</p>\n\n<p>Also, as a novice to nlp, do I need to worry about contractions at all?</p>\n\n<p>EDIT: </p>\n\n<p>The SExprTokenizer or TreeBankWordTokenizer seems to do what I'm looking for for now.</p>\n",
    "score": 21,
    "creation_date": 1341516778,
    "view_count": 15895,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 42381902,
    "title": "Interpreting negative Word2Vec similarity from gensim",
    "body": "<p>E.g. we train a word2vec model using <code>gensim</code>:</p>\n\n<pre><code>from gensim import corpora, models, similarities\nfrom gensim.models.word2vec import Word2Vec\n\ndocuments = [\"Human machine interface for lab abc computer applications\",\n              \"A survey of user opinion of computer system response time\",\n              \"The EPS user interface management system\",\n              \"System and human system engineering testing of EPS\",\n              \"Relation of user perceived response time to error measurement\",\n              \"The generation of random binary unordered trees\",\n              \"The intersection graph of paths in trees\",\n              \"Graph minors IV Widths of trees and well quasi ordering\",\n              \"Graph minors A survey\"]\n\ntexts = [[word for word in document.lower().split()] for document in documents]\nw2v_model = Word2Vec(texts, size=500, window=5, min_count=1)\n</code></pre>\n\n<p>And when we query the similarity between words, we find negative similarity scores:</p>\n\n<pre><code>&gt;&gt;&gt; w2v_model.similarity('graph', 'computer')\n0.046929569156789336\n&gt;&gt;&gt; w2v_model.similarity('graph', 'system')\n0.063683518562347399\n&gt;&gt;&gt; w2v_model.similarity('survey', 'generation')\n-0.040026775040430063\n&gt;&gt;&gt; w2v_model.similarity('graph', 'trees')\n-0.0072684112978664561\n</code></pre>\n\n<p><strong>How do we interpret the negative scores?</strong> </p>\n\n<p>If it's a cosine similarity shouldn't the range be <code>[0,1]</code>?</p>\n\n<p><strong>What is the upper bound and lower bound of the <code>Word2Vec.similarity(x,y)</code> function?</strong> There isn't much written in the docs: <a href=\"https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity\" rel=\"noreferrer\">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity</a> =(</p>\n\n<p>Looking at the Python wrapper code, there isn't much too: <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165\" rel=\"noreferrer\">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165</a></p>\n\n<p>(If possible, please do point me to the <code>.pyx</code> code of where the similarity function is implemented.)</p>\n",
    "score": 21,
    "creation_date": 1487732446,
    "view_count": 11692,
    "answer_count": 3,
    "tags": "python;nlp;similarity;gensim;word2vec"
  },
  {
    "question_id": 8393424,
    "title": "How to analyze twitters messages? (improving my algorithm)",
    "body": "<p>I had a nice idea to implement. I call it </p>\n\n<blockquote>\n  <p>FixTheUnFixed</p>\n</blockquote>\n\n<p>The idea goes like this, imagine you are driving or traveling all over the world and when you see some obstacle or damage - broken light, trash which cover all the street or any other problem you would like the responsible authority will fix it.</p>\n\n<p>all you have to do is tweet something like that, and you can add a photo, and of course location, using the inherit location service of twitter or Facebook applications.</p>\n\n<p><strong>Tweet like this:</strong></p>\n\n<pre><code>@FixTheUnFixed there is a broken fire hydrant here\n@FixTheUnFixed my cellular company charged me 18,572$\n  for using my iPhone aboard.\n</code></pre>\n\n<p>I thought a lot about how to get processing the messages.\nmost of the issues that will come up are municipality concerns and I would like to get the location and re-tweet to the relevant municipality or to send them an email. </p>\n\n<p>my two ideas for getting this address are by google it (with google API).</p>\n\n<p>the pseudo algorithm is:</p>\n\n<pre><code>1. get the location the Twitter's or Facebook's status sent from.\n2. look for key words such as trash, cats, animals etc.\n3. finding the relevant authority e-mail , twitter or Facebook account.\n4. send the message to the authority account and re-tweet it to the public\n     world so they can follow if there is any change.\n</code></pre>\n\n<ul>\n<li>In 3.@algo is there any smart way to implement it?</li>\n<li>I don't want to spam the authorities and and neither publish spam of\nsneaky people.</li>\n<li>How can I improve the algorithm above?</li>\n<li>How can I search  for the communication resources of the relvent\nauthorities?</li>\n</ul>\n",
    "score": 21,
    "creation_date": 1323127930,
    "view_count": 764,
    "answer_count": 4,
    "tags": "iphone;facebook;algorithm;twitter;nlp"
  },
  {
    "question_id": 64914598,
    "title": "PyTorch: RuntimeError: Input, output and indices must be on the current device",
    "body": "<p>I am running a BERT model on torch. It's a multi-class sentiment classification task with about 30,000 rows. I have already put everything on cuda, but not sure why I'm getting the following run time error. Here is my code:</p>\n<pre><code>for epoch in tqdm(range(1, epochs+1)):\n    \n    model.train()\n    \n    loss_train_total = 0\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }       \n\n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n         \n        \n    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-67-9306225bb55a&gt; in &lt;module&gt;()\n     17                  }       \n     18 \n---&gt; 19         outputs = model(**inputs)\n     20 \n     21         loss = outputs[0]\n\n8 frames\n/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   1850         # remove once script supports set_grad_enabled\n   1851         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 1852     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   1853 \n   1854 \n\nRuntimeError: Input, output and indices must be on the current device\n</code></pre>\n<p>Any suggestions would be appreciated. Thanks!</p>\n",
    "score": 21,
    "creation_date": 1605799046,
    "view_count": 30656,
    "answer_count": 1,
    "tags": "python;nlp;pytorch;bert-language-model"
  },
  {
    "question_id": 33071503,
    "title": "How to remove stopwords efficiently from a list of ngram tokens in R",
    "body": "<p>Here's an appeal for a better way to do something that I can already do inefficiently: <strong>filter a series of n-gram tokens using \"stop words\"</strong> so that the occurrence of any stop word term in an n-gram triggers removal.</p>\n\n<p>I'd very much like to have one solution that works for both unigrams and n-grams, although it would be ok to have two versions, one with a \"fixed\" flag and one with a \"regex\" flag.  I'm putting the two aspects of the question together since someone may have a solution that tries a different approach that addresses both fixed and regular expression stopword patterns.</p>\n\n<p>Formats:  </p>\n\n<ul>\n<li><p><strong>tokens</strong> are a list of character vectors, which may be unigrams, or n-grams concatenated by a <code>_</code> (underscore) character.  </p></li>\n<li><p><strong>stopwords</strong> are a character vector.  Right now I am content to let this be a fixed string, but it would be a nice bonus to be able to implement this using regular expression formatted stopwords too.</p></li>\n</ul>\n\n<p><strong>Desired Output:</strong> A list of characters matching the input <strong>tokens</strong> but with any component token matching a stop word being removed.  (This means a unigram match, or a match to one of the terms which the n-gram comprises.)</p>\n\n<p><strong>Examples, test data, and working code and benchmarks to build on:</strong></p>\n\n<pre><code>tokens1 &lt;- list(text1 = c(\"this\", \"is\", \"a\", \"test\", \"text\", \"with\", \"a\", \"few\", \"words\"), \n                text2 = c(\"some\", \"more\", \"words\", \"in\", \"this\", \"test\", \"text\"))\ntokens2 &lt;- list(text1 = c(\"this_is\", \"is_a\", \"a_test\", \"test_text\", \"text_with\", \"with_a\", \"a_few\", \"few_words\"), \n                text2 = c(\"some_more\", \"more_words\", \"words_in\", \"in_this\", \"this_text\", \"text_text\"))\ntokens3 &lt;- list(text1 = c(\"this_is_a\", \"is_a_test\", \"a_test_text\", \"test_text_with\", \"text_with_a\", \"with_a_few\", \"a_few_words\"),\n                text2 = c(\"some_more_words\", \"more_words_in\", \"words_in_this\", \"in_this_text\", \"this_text_text\"))\nstopwords &lt;- c(\"is\", \"a\", \"in\", \"this\")\n\n# remove any single token that matches a stopword\nremoveTokensOP1 &lt;- function(w, stopwords) {\n    lapply(w, function(x) x[-which(x %in% stopwords)])\n}\n\n# remove any word pair where a single word contains a stopword\nremoveTokensOP2 &lt;- function(w, stopwords) {\n    matchPattern &lt;- paste0(\"(^|_)\", paste(stopwords, collapse = \"(_|$)|(^|_)\"), \"(_|$)\")\n    lapply(w, function(x) x[-grep(matchPattern, x)])\n}\n\nremoveTokensOP1(tokens1, stopwords)\n## $text1\n## [1] \"test\"  \"text\"  \"with\"  \"few\"   \"words\"\n## \n## $text2\n## [1] \"some\"  \"more\"  \"words\" \"test\"  \"text\" \n\nremoveTokensOP2(tokens1, stopwords)\n## $text1\n## [1] \"test\"  \"text\"  \"with\"  \"few\"   \"words\"\n## \n## $text2\n## [1] \"some\"  \"more\"  \"words\" \"test\"  \"text\" \n\nremoveTokensOP2(tokens2, stopwords)\n## $text1\n## [1] \"test_text\" \"text_with\" \"few_words\"\n## \n## $text2\n## [1] \"some_more\"  \"more_words\" \"text_text\" \n\nremoveTokensOP2(tokens3, stopwords)\n## $text1\n## [1] \"test_text_with\"\n## \n## $text2\n## [1] \"some_more_words\"\n\n# performance benchmarks for answers to build on\nrequire(microbenchmark)\nmicrobenchmark(OP1_1 = removeTokensOP1(tokens1, stopwords),\n               OP2_1 = removeTokensOP2(tokens1, stopwords),\n               OP2_2 = removeTokensOP2(tokens2, stopwords),\n               OP2_3 = removeTokensOP2(tokens3, stopwords),\n               unit = \"relative\")\n## Unit: relative\n## expr      min       lq     mean   median       uq      max neval\n## OP1_1 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000   100\n## OP2_1 5.119066 3.812845 3.438076 3.714492 3.547187 2.838351   100\n## OP2_2 5.230429 3.903135 3.509935 3.790143 3.631305 2.510629   100\n## OP2_3 5.204924 3.884746 3.578178 3.753979 3.553729 8.240244   100\n</code></pre>\n",
    "score": 21,
    "creation_date": 1444608552,
    "view_count": 6221,
    "answer_count": 3,
    "tags": "r;performance;n-gram;stop-words;text-analysis"
  },
  {
    "question_id": 43463792,
    "title": "what is the difference between bigram and unigram text features extraction",
    "body": "<p>I searched online to do bi-gram and unigram text features' extraction, but still didn't find something useful information, can someone tell me what is the difference between them?</p>\n\n<p>For example, if I have a text \"I have a lovely dog\"\nwhat will happen if I use bi-gram way to do features extraction and to do unigram extraction?</p>\n",
    "score": 20,
    "creation_date": 1492491004,
    "view_count": 28147,
    "answer_count": 2,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 10369393,
    "title": "Need a python module for stemming of text documents",
    "body": "<p>I need a good  python module for stemming text documents in the pre-processing stage.</p>\n\n<p>I found this one </p>\n\n<p><a href=\"http://pypi.python.org/pypi/PyStemmer/1.0.1\" rel=\"noreferrer\">http://pypi.python.org/pypi/PyStemmer/1.0.1</a></p>\n\n<p>but i cannot find the documentation int the link provided. </p>\n\n<p>I anyone knows where to find the documentation or any other good stemming algorithm please help.</p>\n",
    "score": 20,
    "creation_date": 1335669081,
    "view_count": 34603,
    "answer_count": 5,
    "tags": "python;module;preprocessor;nlp;stemming"
  },
  {
    "question_id": 2696392,
    "title": "I want a machine to learn to categorize short texts",
    "body": "<p>I have a ton of short stories about 500 words long and I want to categorize them into one of, let's say, 20 categories:</p>\n<ul>\n<li>Entertainment</li>\n<li>Food</li>\n<li>Music</li>\n<li>etc</li>\n</ul>\n<p>I can hand-classify a bunch of them, but I want to implement machine learning to guess the categories eventually. What's the best way to approach this? Is there a standard approach to machine learning I should be using? I don't think a decision tree would work well since it's text data...</p>\n",
    "score": 20,
    "creation_date": 1272000223,
    "view_count": 10620,
    "answer_count": 8,
    "tags": "machine-learning;nlp;classification"
  },
  {
    "question_id": 31847682,
    "title": "How to compute skipgrams in python?",
    "body": "<p>A k <a href=\"http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf\" rel=\"noreferrer\">skipgram</a> is an ngram which is a superset of all ngrams and each  (k-i )skipgram till (k-i)==0 (which includes 0 skip grams). So how to efficiently compute these skipgrams in python?</p>\n\n<p>Following is the code i tried but it is not doing as expected:</p>\n\n<pre><code>&lt;pre&gt;\n    input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n    def find_skipgrams(input_list, N,K):\n  bigram_list = []\n  nlist=[]\n\n  K=1\n  for k in range(K+1):\n      for i in range(len(input_list)-1):\n          if i+k+1&lt;len(input_list):\n              nlist=[]\n              for j in range(N+1):\n                  if i+k+j+1&lt;len(input_list):\n                    nlist.append(input_list[i+k+j+1])\n\n          bigram_list.append(nlist)\n  return bigram_list\n\n&lt;/pre&gt;\n</code></pre>\n\n<p>The above code is not rendering correctly, but print <code>find_skipgrams(['all', 'this', 'happened', 'more', 'or', 'less'],2,1)</code> gives following output</p>\n\n<blockquote>\n  <p>[['this', 'happened', 'more'], ['happened', 'more', 'or'], ['more',\n  'or', 'less'], ['or', 'less'], ['less'], ['happened', 'more', 'or'],\n  ['more', 'or', 'less'], ['or', 'less'], ['less'], ['less']]</p>\n</blockquote>\n\n<p>The code listed here also does not give correct output:\n<a href=\"https://github.com/heaven00/skipgram/blob/master/skipgram.py\" rel=\"noreferrer\">https://github.com/heaven00/skipgram/blob/master/skipgram.py</a></p>\n\n<p>print skipgram_ndarray(\"What is your name\")  gives:\n['What,is', 'is,your', 'your,name', 'name,', 'What,your', 'is,name']</p>\n\n<p>name is a unigram!</p>\n",
    "score": 20,
    "creation_date": 1438839874,
    "view_count": 13221,
    "answer_count": 5,
    "tags": "python;nlp;n-gram;language-model"
  },
  {
    "question_id": 42525072,
    "title": "Get selected feature names TFIDF Vectorizer",
    "body": "<p>I'm using python and I want to get the TFIDF representation for a large corpus of data, I'm using the following code to convert the docs into their TFIDF form.</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(\n    min_df=1,  # min count for relevant vocabulary\n    max_features=4000,  # maximum number of features\n    strip_accents='unicode',  # replace all accented unicode char \n    # by their corresponding  ASCII char\n    analyzer='word',  # features made of words\n    token_pattern=r'\\w{1,}',  # tokenize only words of 4+ chars\n    ngram_range=(1, 1),  # features made of a single tokens\n    use_idf=True,  # enable inverse-document-frequency reweighting\n    smooth_idf=True,  # prevents zero division for unseen words\n    sublinear_tf=False)\n\ntfidf_df = tfidf_vectorizer.fit_transform(df['text'])\n</code></pre>\n\n<p>Here I pass a parameter <code>max_features</code>. The vectorizer will select the best features and return a scipy sparse matrix. Problem is I dont know which features are getting selected and how do I map those feature names back to the scipy matrix I get? Basically for the <code>n</code> selected features from the <code>m</code> number of documents, I want a <code>m x n</code> matrix with the selected features as the column names instead of their integer ids. How do I accomplish this?</p>\n",
    "score": 20,
    "creation_date": 1488351101,
    "view_count": 43078,
    "answer_count": 2,
    "tags": "python;scikit-learn;nlp"
  },
  {
    "question_id": 61331991,
    "title": "BERT - Pooled output is different from first vector of sequence output",
    "body": "<p>I am using BERT in Tensorflow and there is one detail I dont quite understand. Accordin the the documentation (<a href=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\" rel=\"noreferrer\">https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1</a>), pooled output is the of the entire sequence. Based on the original paper, it seems like this is the output for the token \"CLS\" at the beginning of the setence.</p>\n\n<pre><code>pooled_output[0]\n</code></pre>\n\n<p>However, when I look at the output corresponding to the first token in the sentence</p>\n\n<pre><code>setence_output[0,0,:]\n</code></pre>\n\n<p>which I believe corresponds to the token \"CLS\" (the first token in the sentence), the 2 results are different.</p>\n",
    "score": 20,
    "creation_date": 1587416471,
    "view_count": 18225,
    "answer_count": 4,
    "tags": "tensorflow;keras;deep-learning;nlp"
  },
  {
    "question_id": 10674832,
    "title": "Count verbs, nouns, and other parts of speech with python&#39;s NLTK",
    "body": "<p>I have multiple texts and I would like to create profiles of them based on their usage of various parts of speech, like nouns and verbs. Basially, I need to count how many times each part of speech is used.</p>\n\n<p>I have tagged the text but am not sure how to go further:</p>\n\n<pre><code>tokens = nltk.word_tokenize(text.lower())\ntext = nltk.Text(tokens)\ntags = nltk.pos_tag(text)\n</code></pre>\n\n<p>How can I save the counts for each part of speech into a variable?</p>\n",
    "score": 20,
    "creation_date": 1337528474,
    "view_count": 23566,
    "answer_count": 1,
    "tags": "python;nlp;tagging;nltk;part-of-speech"
  },
  {
    "question_id": 39763091,
    "title": "How to extract subjects in a sentence and their respective dependent phrases?",
    "body": "<p>I am trying to work on subject extraction in a sentence, so that I can get the sentiments in accordance with the subject. I am using <code>nltk</code> in python2.7 for this purpose. Take the following sentence as an example:</p>\n\n<p><code>Donald Trump is the worst president of USA, but Hillary is better than him</code></p>\n\n<p>He we can see that <code>Donald Trump</code> and <code>Hillary</code> are the two subjects, and sentiments related to <code>Donald Trump</code> is negative but related to <code>Hillary</code> are positive. Till now, I am able to break this sentence into chunks of noun phrases, and I am able to get the following:</p>\n\n<pre><code>(S\n  (NP Donald/NNP Trump/NNP)\n  is/VBZ\n  (NP the/DT worst/JJS president/NN)\n  in/IN\n  (NP USA,/NNP)\n  but/CC\n  (NP Hillary/NNP)\n  is/VBZ\n  better/JJR\n  than/IN\n  (NP him/PRP))\n</code></pre>\n\n<p>Now, how do I approach in finding the subjects from these noun phrases? Then how do I group the phrases meant for both the subjects together? Once I have the <strong>phrases meant for both the subjects separately</strong>, I can perform sentiment analysis on both of them separately.</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>I looked into the library mentioned by @Krzysiek (<code>spacy</code>), and it gave me dependency trees as well in the sentences. </p>\n\n<p>Here is the code:</p>\n\n<pre><code>from spacy.en import English\nparser = English()\n\nexample = u\"Donald Trump is the worst president of USA, but Hillary is better than him\"\nparsedEx = parser(example)\n# shown as: original token, dependency tag, head word, left dependents, right dependents\nfor token in parsedEx:\n    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])\n</code></pre>\n\n<p>Here are the dependency trees:</p>\n\n<pre><code>(u'Donald', u'compound', u'Trump', [], [])\n(u'Trump', u'nsubj', u'is', [u'Donald'], [])\n(u'is', u'ROOT', u'is', [u'Trump'], [u'president', u',', u'but', u'is'])\n(u'the', u'det', u'president', [], [])\n(u'worst', u'amod', u'president', [], [])\n(u'president', u'attr', u'is', [u'the', u'worst'], [u'of'])\n(u'of', u'prep', u'president', [], [u'USA'])\n(u'USA', u'pobj', u'of', [], [])\n(u',', u'punct', u'is', [], [])\n(u'but', u'cc', u'is', [], [])\n(u'Hillary', u'nsubj', u'is', [], [])\n(u'is', u'conj', u'is', [u'Hillary'], [u'better'])\n(u'better', u'acomp', u'is', [], [u'than'])\n(u'than', u'prep', u'better', [], [u'him'])\n(u'him', u'pobj', u'than', [], [])\n</code></pre>\n\n<p>This gives in depth insights into the dependencies of the different tokens of the sentences. Here is the <a href=\"http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf\" rel=\"noreferrer\">link</a> to the paper which describes the dependencies between different pairs.  How can I use this tree to attach the contextual words for different subjects to them?</p>\n",
    "score": 20,
    "creation_date": 1475131097,
    "view_count": 27234,
    "answer_count": 2,
    "tags": "python;nlp;nltk;spacy"
  },
  {
    "question_id": 5502688,
    "title": "Using Markov chains (or something similar) to produce an IRC-bot",
    "body": "<p>I tried google and found little that I could understand.</p>\n\n<p>I understand <a href=\"http://en.wikipedia.org/wiki/Markov_chain\" rel=\"noreferrer\">Markov chains</a> to a very basic level: It's a mathematical model that only depends on previous input to change states..so sort of a FSM with weighted random chances instead of different criteria?</p>\n\n<p>I've heard that you can use them to generate semi-intelligent nonsense, given sentences of existing words to use as a dictionary of kinds. </p>\n\n<p>I can't think of search terms to find this, so can anyone link me or explain how I could produce something that gives a semi-intelligent answer? (if you asked it about pie, it would not start going on about the vietnam war it had heard about)</p>\n\n<p>I plan on:</p>\n\n<ul>\n<li>Having this bot idle in IRC channels for a bit</li>\n<li>Strip any usernames out of the string and store as sentences or whatever</li>\n<li>Over time, use this as the basis for the above.</li>\n</ul>\n",
    "score": 20,
    "creation_date": 1301586886,
    "view_count": 8349,
    "answer_count": 3,
    "tags": "artificial-intelligence;nlp;markov-chains"
  },
  {
    "question_id": 1898768,
    "title": "What is the difference between Forward-backward algorithm and Viterbi algorithm?",
    "body": "<p>What is the difference between Forward-backward algorithm on n-gram model and Viterbi algorithm on Hidden Markov model (HMM)?</p>\n\n<p>When I review the implementation of these two algorithms, only thing I found is that the transaction probability is coming from different probabilistic models.</p>\n\n<p>Is there a difference between these 2 algorithms? </p>\n",
    "score": 20,
    "creation_date": 1260761612,
    "view_count": 9902,
    "answer_count": 3,
    "tags": "algorithm;machine-learning;nlp;hidden-markov-models;viterbi"
  },
  {
    "question_id": 36942270,
    "title": "NLTK was unable to find the gs file",
    "body": "<p>I'm trying to use NLTK, the stanford natural language toolkit.\nAfter install the required files, I start to execute the demo code:\n<a href=\"http://www.nltk.org/index.html\" rel=\"noreferrer\">http://www.nltk.org/index.html</a></p>\n\n<pre><code>&gt;&gt;&gt; import nltk\n\n&gt;&gt;&gt; sentence = \"\"\"At eight o'clock on Thursday morning\n... Arthur didn't feel very good.\"\"\"\n\n&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)\n\n&gt;&gt;&gt; tokens\n\n['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n</code></pre>\n\n<p>'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']</p>\n\n<pre><code>&gt;&gt;&gt; tagged = nltk.pos_tag(tokens)\n\n&gt;&gt;&gt; tagged[0:6]\n\n[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\n</code></pre>\n\n<p>('Thursday', 'NNP'), ('morning', 'NN')]</p>\n\n<pre><code>&gt;&gt;&gt; entities = nltk.chunk.ne_chunk(tagged)\n\n&gt;&gt;&gt; entities\n</code></pre>\n\n<p>Then I get message:</p>\n\n<pre><code>LookupError: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n</code></pre>\n\n<p>I tried google, but there's no one tell what the missing gs file is.</p>\n",
    "score": 20,
    "creation_date": 1461943801,
    "view_count": 11312,
    "answer_count": 8,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 15009656,
    "title": "How to use NLTK to generate sentences from an induced grammar?",
    "body": "<p>I have a (large) list of parsed sentences (which were parsed using the Stanford parser), for example, the sentence \"Now you can be entertained\" has the following tree:</p>\n\n<pre><code>(ROOT\n  (S\n    (ADVP (RB Now))\n    (, ,)\n    (NP (PRP you))\n    (VP (MD can)\n      (VP (VB be)\n        (VP (VBN entertained))))\n    (. .)))\n</code></pre>\n\n<p>I am using the set of sentence trees to induce a grammar using nltk:</p>\n\n<pre><code>import nltk\n\n# ... for each sentence tree t, add its production to allProductions\nallProductions += t.productions()\n\n# Induce the grammar\nS = nltk.Nonterminal('S')\ngrammar = nltk.induce_pcfg(S, allProductions)\n</code></pre>\n\n<p>Now I would like to use <code>grammar</code> to generate new, random sentences. My hope is that since the grammar was learned from a specific set of input examples, then the generated sentences will be semantically similar. Can I do this in nltk?</p>\n\n<p>If I can't use nltk to do this, do any other tools exist that can take the (possibly reformatted) <code>grammar</code> and generate sentences?</p>\n",
    "score": 20,
    "creation_date": 1361471257,
    "view_count": 20237,
    "answer_count": 5,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 19145332,
    "title": "NLTK - Counting Frequency of Bigram",
    "body": "<p>This is a Python and NLTK newbie question. </p>\n\n<p>I want to find frequency of bigrams which occur more than 10 times together and have the highest PMI. </p>\n\n<p>For this, I am working with this code</p>\n\n<pre><code>def get_list_phrases(text):\n\n    tweet_phrases = []\n\n    for tweet in text:\n        tweet_words = tweet.split()\n        tweet_phrases.extend(tweet_words)\n\n\n    bigram_measures = nltk.collocations.BigramAssocMeasures()\n    finder = BigramCollocationFinder.from_words(tweet_phrases,window_size = 13)\n    finder.apply_freq_filter(10)\n    finder.nbest(bigram_measures.pmi,20)  \n\n    for k,v in finder.ngram_fd.items():\n      print(k,v)\n</code></pre>\n\n<p>However, this does not restricts the results to top 20. I see results which have frequency &lt; 10. I am new to the world of Python. </p>\n\n<p>Can someone please point out how to modify this to get only the top 20. </p>\n\n<p>Thank You</p>\n",
    "score": 20,
    "creation_date": 1380742485,
    "view_count": 21905,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 16383194,
    "title": "Stupid Backoff implementation clarification",
    "body": "<p>Hello people I'm implementing the <a href=\"http://www.aclweb.org/anthology/D07-1090.pdf\" rel=\"noreferrer\">Stupid Backoff</a> (page 2, equation 5) smoothing technique for a project I'm working on and I have a question on its implementation. This is a smoothing algorithm used in NLP, Good-Turing is I guess the most well known similar algorithm.</p>\n\n<p>A brief description of the algorithm is:\nWhen trying to find the probability of word appearing in a sentence it will first look for context for the word at the n-gram level and if there is no n-gram of that size it will recurse to the (n-1)-gram and multiply its score with 0.4. The recursion stops at unigrams.</p>\n\n<p>So if I want to find the probability of \"day\" in the context of \"a sunny day\" it would first look to see if the tri-gram \"a sunny day\" exists in the corpus, if not it would try the same with the bigram \"sunny day\" and finally it would just get the frequency for \"day\" divided by the corpus size (total number of words in the training data).</p>\n\n<p>My question is: Do I multiply the score with 0.4 every time I reduce the size of the n-gram?</p>\n\n<p>So in the above example if we are not able to find a tri-gram or bi-gram the final score would be:</p>\n\n<p>0.4 * 0.4 * frequency(day) / corpus_size?</p>\n\n<p>or do I just multiply once at the final level so regardless of how many backoffs I have to make I just multiply the final score with 0.4?</p>\n",
    "score": 20,
    "creation_date": 1367746584,
    "view_count": 15360,
    "answer_count": 2,
    "tags": "nlp;smoothing"
  },
  {
    "question_id": 5708352,
    "title": "Named Entity Recognition for NLTK in Python. Identifying the NE",
    "body": "<p>I need to classify words into their parts of speech. Like a verb, a noun, an adverb etc..\nI used the </p>\n\n<pre><code>nltk.word_tokenize() #to identify word in a sentence \nnltk.pos_tag()       #to identify the parts of speech\nnltk.ne_chunk()      #to identify Named entities. \n</code></pre>\n\n<p>The out put of  this is a tree.\n Eg </p>\n\n<pre><code>&gt;&gt;&gt; sentence = \"I am Jhon from America\"\n&gt;&gt;&gt; sent1 = nltk.word_tokenize(sentence )\n&gt;&gt;&gt; sent2 = nltk.pos_tag(sent1)\n&gt;&gt;&gt; sent3 =  nltk.ne_chunk(sent2, binary=True)\n&gt;&gt;&gt; sent3\nTree('S', [('I', 'PRP'), ('am', 'VBP'), Tree('NE', [('Jhon', 'NNP')]), ('from', 'IN'), Tree('NE', [('America', 'NNP')])])\n</code></pre>\n\n<p>When accessing the element in this tree, i did it as follows:</p>\n\n<pre><code>&gt;&gt;&gt; sent3[0]\n('I', 'PRP')\n&gt;&gt;&gt; sent3[0][0]\n'I'\n&gt;&gt;&gt; sent3[0][1]\n'PRP'\n</code></pre>\n\n<p>But when accessing a Named Entity:</p>\n\n<pre><code>&gt;&gt;&gt; sent3[2]\nTree('NE', [('Jhon', 'NNP')])\n&gt;&gt;&gt; sent3[2][0]\n('Jhon', 'NNP')\n&gt;&gt;&gt; sent3[2][1]    \nTraceback (most recent call last):\n  File \"&lt;pyshell#121&gt;\", line 1, in &lt;module&gt;\n    sent3[2][1]\n  File \"C:\\Python26\\lib\\site-packages\\nltk\\tree.py\", line 139, in __getitem__\n    return list.__getitem__(self, index)\nIndexError: list index out of range\n</code></pre>\n\n<p>I got the above error.</p>\n\n<p>What i want is to get the output  as 'NE' similar to the previous 'PRP' so i cant identify which word is a Named Entity.\nIs there any way of doing this with NLTK in python?? If so please post the command. Or is there a function in the tree library to do this? I need the node value 'NE'</p>\n",
    "score": 20,
    "creation_date": 1303157656,
    "view_count": 15273,
    "answer_count": 6,
    "tags": "python;nlp;nltk;named-entity-recognition"
  },
  {
    "question_id": 29575784,
    "title": "what is distant supervision?",
    "body": "<p>According to my understanding, Distant Supervision is the process of specifying the concept which the individual words of a passage, usually a sentence, are trying to convey. </p>\n\n<p>For example, a database maintains the structured relationship <code>concerns( NLP, this sentence).</code></p>\n\n<p>Our distant supervision system would take as input the sentence: <code>\"This is a sentence about NLP.\"</code></p>\n\n<p>Based on this sentence it would recognize the entities, since as a pre-processing step the sentence would have been passed through a named-entity recognizer, <code>NLP</code> &amp; <code>this sentence</code>. </p>\n\n<p>Since our database has it that <code>NLP</code> and <code>this sentence</code> are related by the bond of <code>concern(s)</code> it would identify the input sentence as expressing the relationship <code>Concerns(NLP, this sentence)</code>. </p>\n\n<p>My questions is two fold: </p>\n\n<p>1) What is the use of that? Is it that later our system might see a sentence in \"the wild\" such as <code>That sentence is about OPP</code> and realize that it's seen something similar to that before and thereby realize the novel relationship such that <code>concerns(OPP, that sentence).</code>, based only on the words/ individual tokens?</p>\n\n<p>2) Does it take into account the actual words of the sentence? The verb 'is' and the adverb 'about' for instance, realizing (through WordNet or some other hyponymy system) that this is somehow similar to the higher-order concept \"concerns\"?</p>\n\n<p>Does anyone have some code used to generate a distant supervision system that I could look at, i.e. a system that cross references a KB, such as Freebase, and a corpus, such as the NYTimes, and produces a distant supervision database? I think that would go a long way in clarifying my conception of distant supervision. </p>\n",
    "score": 20,
    "creation_date": 1428740980,
    "view_count": 17569,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp;supervised-learning;unsupervised-learning"
  },
  {
    "question_id": 4771293,
    "title": "Can an author&#39;s unique &quot;literary style&quot; be used to identify him/her as the author of a text?",
    "body": "<p>Let's imagine, I have two English language texts written by the same person.\nIs it possible to apply some Markov chain algorithm to analyse each: create some kind of fingerprint based on statistical data, and compare fingerprints gotten from different texts?\nLet's say, we have a library with 100 texts. Some person wrote text number 1 and some other as well, and we need to guess which one by analyzing his/her writing style.\nIs there any known algorithm doing it? Can be Markov chains applied here?</p>\n",
    "score": 20,
    "creation_date": 1295738554,
    "view_count": 5186,
    "answer_count": 2,
    "tags": "machine-learning;data-mining;markov-chains;nlp"
  },
  {
    "question_id": 29041603,
    "title": "nltk sentence tokenizer, consider new lines as sentence boundary",
    "body": "<p>I am using nltk's <code>PunkSentenceTokenizer</code> to tokenize a text to a set of sentences. However, the tokenizer doesn't seem to consider new paragraph or new lines as a new sentence.</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.tokenize.punkt import PunktSentenceTokenizer\n&gt;&gt;&gt; tokenizer = PunktSentenceTokenizer()\n&gt;&gt;&gt; tokenizer.tokenize('Sentence 1 \\n Sentence 2. Sentence 3.')\n['Sentence 1 \\n Sentence 2.', 'Sentence 3.']\n&gt;&gt;&gt; tokenizer.span_tokenize('Sentence 1 \\n Sentence 2. Sentence 3.')\n[(0, 24), (25, 36)]\n</code></pre>\n\n<p>I would like it to to consider new lines as a boundary of sentences as well. Anyway to do this (I need to save the offsets too)?</p>\n",
    "score": 20,
    "creation_date": 1426279296,
    "view_count": 10196,
    "answer_count": 1,
    "tags": "python;nlp;nltk;tokenize"
  },
  {
    "question_id": 2633353,
    "title": "Algorithm for Negating Sentences",
    "body": "<p>I was wondering if anyone was familiar with any attempts at algorithmic sentence negation.</p>\n\n<p>For example, given a sentence like \"This book is good\" provide any number of alternative sentences meaning the opposite like \"This book is not good\" or even \"This book is bad\".</p>\n\n<p>Obviously, accomplishing this with a high degree of accuracy would probably be beyond the scope of current NLP, but I'm sure there has been some work on the subject.  If anybody knows of any work, care to point me to some papers?</p>\n",
    "score": 20,
    "creation_date": 1271194027,
    "view_count": 7291,
    "answer_count": 7,
    "tags": "nlp;text-processing;linguistics;negation"
  },
  {
    "question_id": 32979254,
    "title": "Using Word2Vec for topic modeling",
    "body": "<p>I have read that the most common technique for topic modeling (extracting possible topics from text) is Latent Dirichlet allocation (LDA).</p>\n\n<p>However, I am interested whether it is a good idea to try out topic modeling with Word2Vec as it clusters words in vector space. Couldn't the clusters therefore be regarded as topics?</p>\n\n<p>Do you think it makes sense to follow this approach for the sake of some research? In the end what I am interested in is to extract keywords from text according to topics.</p>\n",
    "score": 20,
    "creation_date": 1444163719,
    "view_count": 23539,
    "answer_count": 3,
    "tags": "nlp;topic-modeling;word2vec"
  },
  {
    "question_id": 10623163,
    "title": "Pattern Recognition Algorithms in Node.js or PHP?",
    "body": "<p>I would like to begin experimenting with algorithms that recognize patterns in data. I deal with many types of sequences (image pixels, text input, user movement), and it would be fun to make use of <a href=\"http://en.wikipedia.org/wiki/Pattern_recognition\" rel=\"noreferrer\">Pattern Recognition</a> to try to pull meaningful data out of different datasets. Like the majority of the web, my data is mostly text or integer-key based.</p>\n\n<p>Are their any classes that give the basic framework for checking/creating patterns for PHP or Nodejs?</p>\n",
    "score": 20,
    "creation_date": 1337187295,
    "view_count": 7265,
    "answer_count": 3,
    "tags": "php;algorithm;node.js;nlp;pattern-recognition"
  },
  {
    "question_id": 16205020,
    "title": "Measuring semantic similarity between two phrases",
    "body": "<p>I want to measure semantic similarity between two phrases/sentences. Is there any framework that I can use directly and reliably?</p>\n\n<p>I have already checked out <a href=\"https://stackoverflow.com/questions/62328/is-there-an-algorithm-that-tells-the-semantic-similarity-of-two-phrases\">this question</a>, but its pretty old and I couldn't find real helpful answer there. There was <a href=\"http://swoogle.umbc.edu/SimService/\" rel=\"nofollow noreferrer\">one link</a>, but I found this unreliable.</p>\n\n<p>e.g.:<br>\nI have a phrase: felt crushed<br>\nI have several choices: force inwards,pulverized, destroyed emotionally, reshaping etc.<br>\nI want to find the term/phrase with highest similarity to the first one.<br>\nThe answer here is: destroyed emotionally.</p>\n\n<p>The bigger picture is: I want to identify which frame from FrameNet matches to the given verb as per its usage in a sentence.</p>\n\n<p>Update : I found <a href=\"https://code.google.com/p/ws4j/\" rel=\"nofollow noreferrer\">this library</a> very useful for measuring similarity between two words. Also the ConceptNet similarity mechanism is very good.</p>\n\n<p>and <a href=\"http://simlibrary.wordpress.com/\" rel=\"nofollow noreferrer\">this library</a> for measuring semantic similarity between sentences</p>\n\n<p>If anyone has any insights please share.</p>\n",
    "score": 20,
    "creation_date": 1366853888,
    "view_count": 17479,
    "answer_count": 2,
    "tags": "algorithm;nlp;semantics;wordnet"
  },
  {
    "question_id": 5752043,
    "title": "Is there a tutorial about giza++?",
    "body": "<p>The Urls in its 'readme' file is not valid (<a href=\"http://www.fjoch.com/mkcls.html\" rel=\"noreferrer\">http://www.fjoch.com/mkcls.html</a> and <a href=\"http://www.fjoch.com/GIZA++.html\" rel=\"noreferrer\">http://www.fjoch.com/GIZA++.html</a>). Is there a good tutorial about giza++? Or is there some alternatives that have complete documentation? </p>\n",
    "score": 19,
    "creation_date": 1303443366,
    "view_count": 11788,
    "answer_count": 5,
    "tags": "nlp;machine-translation;giza++"
  },
  {
    "question_id": 1598940,
    "title": "In Natural language processing, what is the purpose of chunking?",
    "body": "<p>In Natural language processing, what is the purpose of chunking?</p>\n",
    "score": 19,
    "creation_date": 1256104295,
    "view_count": 19227,
    "answer_count": 3,
    "tags": "computer-science;nlp"
  },
  {
    "question_id": 15067734,
    "title": "LDA model generates different topics everytime i train on the same corpus",
    "body": "<p>I am using python <code>gensim</code> to train an Latent Dirichlet Allocation (LDA) model from a small corpus of 231 sentences. However, each time i repeat the process, it generates different topics. </p>\n\n<p><strong>Why does the same LDA parameters and corpus generate different topics everytime?</strong></p>\n\n<p><strong>And how do i stabilize the topic generation?</strong></p>\n\n<p>I'm using this corpus (<a href=\"http://pastebin.com/WptkKVF0\">http://pastebin.com/WptkKVF0</a>) and this list of stopwords (<a href=\"http://pastebin.com/LL7dqLcj\">http://pastebin.com/LL7dqLcj</a>) and here's my code:</p>\n\n<pre><code>from gensim import corpora, models, similarities\nfrom gensim.models import hdpmodel, ldamodel\nfrom itertools import izip\nfrom collections import defaultdict\nimport codecs, os, glob, math\n\nstopwords = [i.strip() for i in codecs.open('stopmild','r','utf8').readlines() if i[0] != \"#\" and i != \"\"]\n\ndef generateTopics(corpus, dictionary):\n    # Build LDA model using the above corpus\n    lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)\n    corpus_lda = lda[corpus]\n\n    # Group topics with similar words together.\n    tops = set(lda.show_topics(50))\n    top_clusters = []\n    for l in tops:\n        top = []\n        for t in l.split(\" + \"):\n            top.append((t.split(\"*\")[0], t.split(\"*\")[1]))\n        top_clusters.append(top)\n\n    # Generate word only topics\n    top_wordonly = []\n    for i in top_clusters:\n        top_wordonly.append(\":\".join([j[1] for j in i]))\n\n    return lda, corpus_lda, top_clusters, top_wordonly\n\n####################################################################### \n\n# Read textfile, build dictionary and bag-of-words corpus\ndocuments = []\nfor line in codecs.open(\"./europarl-mini2/map/coach.en-es.all\",\"r\",\"utf8\"):\n    lemma = line.split(\"\\t\")[3]\n    documents.append(lemma)\ntexts = [[word for word in document.lower().split() if word not in stopwords]\n             for document in documents]\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\nlda, corpus_lda, topic_clusters, topic_wordonly = generateTopics(corpus, dictionary)\n\nfor i in topic_wordonly:\n    print i\n</code></pre>\n",
    "score": 19,
    "creation_date": 1361797708,
    "view_count": 19837,
    "answer_count": 4,
    "tags": "python;nlp;lda;topic-modeling;gensim"
  },
  {
    "question_id": 61172400,
    "title": "what does padding_idx do in nn.embeddings()",
    "body": "<p>I'm learning pytorch and\nI'm wondering what does the <code>padding_idx</code> attribute do in <code>torch.nn.Embedding(n1, d1, padding_idx=0)</code>?\nI have looked everywhere and couldn't find something I can get. \nCan you show example to illustrate this?</p>\n",
    "score": 19,
    "creation_date": 1586699061,
    "view_count": 18902,
    "answer_count": 2,
    "tags": "python;deep-learning;nlp;pytorch;recurrent-neural-network"
  },
  {
    "question_id": 54495502,
    "title": "How to get all words from spacy vocab?",
    "body": "<p>I need all the words from Spacy vocab. Suppose, I initialize my spacy model as </p>\n\n<pre><code>nlp = spacy.load('en')\n</code></pre>\n\n<p>How do I get the text of words from <code>nlp.vocab</code>?</p>\n",
    "score": 19,
    "creation_date": 1549127905,
    "view_count": 14209,
    "answer_count": 2,
    "tags": "python-3.x;nlp;spacy"
  },
  {
    "question_id": 53403306,
    "title": "How to batch convert sentence lengths to masks in PyTorch?",
    "body": "<p>For example, from</p>\n\n<pre><code>lens = [3, 5, 4]\n</code></pre>\n\n<p>we want to get</p>\n\n<pre><code>mask = [[1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 0]]\n</code></pre>\n\n<p>Both of which are <code>torch.LongTensor</code>s.</p>\n",
    "score": 19,
    "creation_date": 1542757632,
    "view_count": 5006,
    "answer_count": 3,
    "tags": "nlp;pytorch"
  },
  {
    "question_id": 76633836,
    "title": "What does langchain CharacterTextSplitter&#39;s chunk_size param even do?",
    "body": "<p>My default assumption was that the <code>chunk_size</code> parameter would set a ceiling on the size of the chunks/splits that come out of the <code>split_text</code> method, but that's clearly not right:</p>\n<pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n\nchunk_size = 6\nchunk_overlap = 2\n\nc_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n\ntext = 'abcdefghijklmnopqrstuvwxyz'\n\nc_splitter.split_text(text)\n</code></pre>\n<p>prints: <code>['abcdefghijklmnopqrstuvwxyz']</code>, i.e. one single chunk that is much larger than <code>chunk_size=6</code>.</p>\n<p>So I understand that it didn't split the text into chunks because it never encountered the separator. But so then the question is what <em>is</em> the <code>chunk_size</code> even doing?</p>\n<p>I checked the documentation page for <code>langchain.text_splitter.CharacterTextSplitter</code> <a href=\"https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter\" rel=\"noreferrer\">here</a> but did not see an answer to this question. And I asked the &quot;mendable&quot; chat-with-langchain-docs search functionality, but got the answer &quot;The chunk_size parameter of the CharacterTextSplitter determines the maximum number of characters in each chunk of text.&quot;...which is not true, as the code sample above shows.</p>\n",
    "score": 19,
    "creation_date": 1688701827,
    "view_count": 35540,
    "answer_count": 3,
    "tags": "python;machine-learning;text;nlp;langchain"
  },
  {
    "question_id": 23429117,
    "title": "Saving nltk drawn parse tree to image file",
    "body": "<p><img src=\"https://i.sstatic.net/0wi0Q.png\" alt=\"enter image description here\"></p>\n\n<p>Is there any way to save the draw image from tree.draw() to an image file programmatically? I tried looking through the documentation, but I couldn't find anything.</p>\n",
    "score": 19,
    "creation_date": 1399036615,
    "view_count": 14766,
    "answer_count": 4,
    "tags": "python;tree;nlp;nltk;text-parsing"
  },
  {
    "question_id": 54396405,
    "title": "How can I preprocess NLP text (lowercase, remove special characters, remove numbers, remove emails, etc) in one pass?",
    "body": "<p>How can I preprocess NLP text (lowercase, remove special characters, remove numbers, remove emails, etc) in one pass using Python?</p>\n\n<pre><code>Here are all the things I want to do to a Pandas dataframe in one pass in python:\n1. Lowercase text\n2. Remove whitespace\n3. Remove numbers\n4. Remove special characters\n5. Remove emails\n6. Remove stop words\n7. Remove NAN\n8. Remove weblinks\n9. Expand contractions (if possible not necessary)\n10. Tokenize\n</code></pre>\n\n<p>Here's how I am doing it all individually:</p>\n\n<pre><code>    def preprocess(self, dataframe):\n\n\n    self.log.info(\"In preprocess function.\")\n\n    dataframe1 = self.remove_nan(dataframe)\n    dataframe2 = self.lowercase(dataframe1)\n    dataframe3 = self.remove_whitespace(dataframe2)\n\n    # Remove emails and websites before removing special characters\n    dataframe4 = self.remove_emails(self, dataframe3)\n    dataframe5 = self.remove_website_links(self, dataframe4)\n\n    dataframe6 = self.remove_special_characters(dataframe5)\n    dataframe7 - self.remove_numbers(dataframe6)\n    self.remove_stop_words(dataframe8) # Doesn't return anything for now\n    dataframe7 = self.tokenize(dataframe6)\n\n    self.log.info(f\"Sample of preprocessed data: {dataframe4.head()}\")\n\n    return dataframe7\n\ndef remove_nan(self, dataframe):\n    \"\"\"Pass in a dataframe to remove NAN from those columns.\"\"\"\n    return dataframe.dropna()\n\ndef lowercase(self, dataframe):\n    logging.info(\"Converting dataframe to lowercase\")\n    lowercase_dataframe = dataframe.apply(lambda x: x.lower())\n    return lowercase_dataframe\n\n\ndef remove_special_characters(self, dataframe):\n    self.log.info(\"Removing special characters from dataframe\")\n    no_special_characters = dataframe.replace(r'[^A-Za-z0-9 ]+', '', regex=True)\n    return no_special_characters\n\ndef remove_numbers(self, dataframe):\n    self.log.info(\"Removing numbers from dataframe\")\n    removed_numbers = dataframe.str.replace(r'\\d+','')\n    return removed_numbers\n\ndef remove_whitespace(self, dataframe):\n    self.log.info(\"Removing whitespace from dataframe\")\n    # replace more than 1 space with 1 space\n    merged_spaces = dataframe.str.replace(r\"\\s\\s+\",' ')\n    # delete beginning and trailing spaces\n    trimmed_spaces = merged_spaces.apply(lambda x: x.str.strip())\n    return trimmed_spaces\n\ndef remove_stop_words(self, dataframe):\n    # TODO: An option to pass in a custom list of stopwords would be cool.\n    set(stopwords.words('english'))\n\ndef remove_website_links(self, dataframe):\n    self.log.info(\"Removing website links from dataframe\")\n    no_website_links = dataframe.str.replace(r\"http\\S+\", \"\")\n    return no_website_links\n\ndef tokenize(self, dataframe):\n    tokenized_dataframe = dataframe.apply(lambda row: word_tokenize(row))\n    return tokenized_dataframe\n\ndef remove_emails(self, dataframe):\n    no_emails = dataframe.str.replace(r\"\\S*@\\S*\\s?\")\n    return no_emails\n\ndef expand_contractions(self, dataframe):\n    # TODO: Not a priority right now. Come back to it later.\n    return dataframe\n</code></pre>\n",
    "score": 19,
    "creation_date": 1548655871,
    "view_count": 36364,
    "answer_count": 3,
    "tags": "python;pandas;nlp"
  },
  {
    "question_id": 7591258,
    "title": "Fast n-gram calculation",
    "body": "<p>I'm using NLTK to search for n-grams in a corpus but it's taking a very long time in some cases. I've noticed calculating n-grams isn't an uncommon feature in other packages (apparently Haystack has some functionality for it). Does this mean there's a potentially faster way of finding n-grams in my corpus if I abandon NLTK? If so, what can I use to speed things up?</p>\n",
    "score": 19,
    "creation_date": 1317257374,
    "view_count": 15612,
    "answer_count": 4,
    "tags": "python;nlp;nltk;n-gram"
  },
  {
    "question_id": 66954682,
    "title": "Token indices sequence length is longer than the specified maximum sequence length for this model (651 &gt; 512) with Hugging face sentiment classifier",
    "body": "<p>I'm trying to get the sentiments for comments with the help of hugging face sentiment analysis pretrained model. It's returning error like <code>Token indices sequence length is longer than the specified maximum sequence length for this model (651 &gt; 512)</code> with Hugging face sentiment classifier.</p>\n<p>Below I'm attaching the code please look at it</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nimport transformers\nimport pandas as pd\n\nmodel = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/Huggingface-Sentiment-Pipeline')\ntoken = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Huggingface-Sentiment-Pipeline')\n\nclassifier = pipeline(task='sentiment-analysis', model=model, tokenizer=token)\n\ndata = pd.read_csv('/content/drive/MyDrive/DisneylandReviews.csv', encoding='latin-1')\n\ndata.head()\n</code></pre>\n<p>Output is</p>\n<pre><code>    Review\n0   If you've ever been to Disneyland anywhere you...\n1   Its been a while since d last time we visit HK...\n2   Thanks God it wasn t too hot or too humid wh...\n3   HK Disneyland is a great compact park. Unfortu...\n4   the location is not in the city, took around 1...\n</code></pre>\n<p>Followed by</p>\n<pre><code>classifier(&quot;My name is mark&quot;)\n</code></pre>\n<p>Output is</p>\n<pre><code>[{'label': 'POSITIVE', 'score': 0.9953688383102417}]\n</code></pre>\n<p>Followed by code</p>\n<pre><code>basic_sentiment = [i['label'] for i in value if 'label' in i]\nbasic_sentiment\n</code></pre>\n<p>Output is</p>\n<pre><code>['POSITIVE']\n</code></pre>\n<p>Appending the total rows to empty list</p>\n<pre><code>text = []\n\nfor index, row in data.iterrows():\n    text.append(row['Review'])\n</code></pre>\n<p>I'm trying to get the sentiment for all the rows</p>\n<pre><code>sent = []\n\nfor i in range(len(data)):\n    sentiment = classifier(data.iloc[i,0])\n    sent.append(sentiment)\n</code></pre>\n<p>The error is :</p>\n<pre><code>Token indices sequence length is longer than the specified maximum sequence length for this model (651 &gt; 512). Running this sequence through the model will result in indexing errors\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n&lt;ipython-input-19-4bb136563e7c&gt; in &lt;module&gt;()\n      2 \n      3 for i in range(len(data)):\n----&gt; 4     sentiment = classifier(data.iloc[i,0])\n      5     sent.append(sentiment)\n\n11 frames\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   1914         # remove once script supports set_grad_enabled\n   1915         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 1916     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   1917 \n   1918 \n\nIndexError: index out of range in self\n</code></pre>\n",
    "score": 19,
    "creation_date": 1617633190,
    "view_count": 44147,
    "answer_count": 3,
    "tags": "deep-learning;nlp;sentiment-analysis;huggingface-transformers;huggingface-tokenizers"
  },
  {
    "question_id": 47666699,
    "title": "Using word2vec to classify words in categories",
    "body": "<p><strong>BACKGROUND</strong></p>\n\n<p>I have vectors with some sample data and each vector has a category name (Places,Colors,Names).</p>\n\n<pre><code>['john','jay','dan','nathan','bob']  -&gt; 'Names'\n['yellow', 'red','green'] -&gt; 'Colors'\n['tokyo','bejing','washington','mumbai'] -&gt; 'Places'\n</code></pre>\n\n<p>My objective is to train a model that take a new input string and predict which category it belongs to. For example if a new input is \"purple\" then I should be able to predict 'Colors' as the correct category. If the new input is \"Calgary\" it should predict 'Places' as the correct category.</p>\n\n<p><strong>APPROACH</strong></p>\n\n<p>I did some research and came across <a href=\"https://radimrehurek.com/gensim/models/word2vec.html\" rel=\"noreferrer\">Word2vec</a>. This library has a \"similarity\" and \"mostsimilarity\" function which i can use. So one brute force approach I thought of is the following:</p>\n\n<ol>\n<li>Take new input.</li>\n<li>Calculate it's similarity with each word in each vector and take an average.</li>\n</ol>\n\n<p>So for instance for input \"pink\" I can calculate its similarity with words in vector \"names\" take a average and then do that for the other 2 vectors also. The vector that gives me the highest similarity average would be the correct vector for the input to belong to.</p>\n\n<p><strong>ISSUE</strong></p>\n\n<p>Given my limited knowledge in NLP and machine learning I am not sure if that is the best approach and hence I am looking for help and suggestions on better approaches to solve my problem. I am open to all suggestions and also please point out any mistakes I may have made as I am new to machine learning and NLP world.</p>\n",
    "score": 19,
    "creation_date": 1512533795,
    "view_count": 14438,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;word2vec;gensim"
  },
  {
    "question_id": 13765349,
    "title": "Multi-term named entities in Stanford Named Entity Recognizer",
    "body": "<p>I'm using the Stanford Named Entity Recognizer <a href=\"http://nlp.stanford.edu/software/CRF-NER.shtml\">http://nlp.stanford.edu/software/CRF-NER.shtml</a> and it's working fine. This is</p>\n\n<pre><code>    List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(text);\n    for (List&lt;CoreLabel&gt; sentence : out) {\n        for (CoreLabel word : sentence) {\n            if (!StringUtils.equals(word.get(AnswerAnnotation.class), \"O\")) {\n                namedEntities.add(word.word().trim());           \n            }\n        }\n    }\n</code></pre>\n\n<p>However the problem I'm finding is identifying names and surnames. If the recognizer encounters \"Joe Smith\", it is returning \"Joe\" and \"Smith\" separately. I'd really like it to return \"Joe Smith\" as one term. </p>\n\n<p>Could this be achieved through the recognizer maybe through a configuration? I didn't find anything in the javadoc till now. </p>\n\n<p>Thanks!</p>\n",
    "score": 19,
    "creation_date": 1354891542,
    "view_count": 10253,
    "answer_count": 8,
    "tags": "nlp;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 77237818,
    "title": "How to load a huggingface pretrained transformer model directly to GPU?",
    "body": "<p>I want to load a huggingface pretrained transformer model directly to GPU (not enough CPU space)\ne.g. loading BERT</p>\n<pre><code>from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(&quot;bert-base-uncased&quot;)\n</code></pre>\n<p>would be loaded to CPU until executing</p>\n<pre><code>model.to('cuda')\n</code></pre>\n<p>now the model is loaded into GPU</p>\n<p>I want to load the model directly into GPU when executing <code>from_pretrained</code>. Is this possible?</p>\n",
    "score": 19,
    "creation_date": 1696514260,
    "view_count": 56536,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers"
  },
  {
    "question_id": 76707715,
    "title": "stucking at downloading shards for loading LLM model from huggingface",
    "body": "<p>I am just using huggingface example to use their LLM model, but it stuck at the:</p>\n<pre><code>downloading shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]\n</code></pre>\n<p>(I am using Jupiter notebook, <code>python 3.11</code>, and all requirements were installed)</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = &quot;tiiuae/falcon-40b-instruct&quot;\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    &quot;text-generation&quot;,\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=&quot;auto&quot;,\n)\nsequences = pipeline(\n   &quot;Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:&quot;,\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f&quot;Result: {seq['generated_text']}&quot;)\n\n</code></pre>\n<p>how can I fix it?</p>\n",
    "score": 19,
    "creation_date": 1689623840,
    "view_count": 14947,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers"
  },
  {
    "question_id": 47778403,
    "title": "Computing TF-IDF on the whole dataset or only on training data?",
    "body": "<p>In the chapter seven of this book \"TensorFlow Machine Learning Cookbook\" the author in pre-processing data uses <code>fit_transform</code> function of scikit-learn to get the <code>tfidf</code> features of text for training. The author gives all text data to the function before separating it into train and test. Is it a true action or we must separate data first and then perform <code>fit_transform</code> on train and <code>transform</code> on test?</p>\n",
    "score": 19,
    "creation_date": 1513100061,
    "view_count": 15283,
    "answer_count": 3,
    "tags": "python;machine-learning;scikit-learn;nlp;tf-idf"
  },
  {
    "question_id": 2494508,
    "title": "Recognizing language of a short text?",
    "body": "<p>I have a list of articles, and each article has its own title and description. Unfortunately, from the sources I am using, there is no way to know what language they are written in.</p>\n\n<p>Furthermore, the text is not entirely written in 1 language; almost always English words are present.</p>\n\n<p>I reckon I would need dictionary databases stored on my machine, but it feels a bit impractical. What would you suggest I do?</p>\n",
    "score": 19,
    "creation_date": 1269280170,
    "view_count": 7238,
    "answer_count": 6,
    "tags": "python;nlp"
  },
  {
    "question_id": 11832490,
    "title": "stanford core nlp java output",
    "body": "<p>I'm a newbie with Java and Stanford NLP toolkit and trying to use them for a project. Specifically, I'm trying to use Stanford Corenlp toolkit to annotate a text (with Netbeans and not command line) and I tried to use the code provided on <a href=\"http://nlp.stanford.edu/software/corenlp.shtml#Usage\" rel=\"noreferrer\">http://nlp.stanford.edu/software/corenlp.shtml#Usage</a> (Using the Stanford CoreNLP API).. question is: can anybody tell me how I can get the output in a file so that I can further process it?</p>\n\n<p>I've tried printing the graphs and the sentence to the console, just to see the content. That works. Basically what I'd need is to return the annotated document, so that I can call it from my main class and output a text file (if that's possible). I'm trying to look in the API of stanford corenlp, but I don't really know what is the best way to return such kind of information, given my lack of experience.</p>\n\n<p>Here is the code:</p>\n\n<pre><code>Properties props = new Properties();\n    props.put(\"annotators\", \"tokenize, ssplit, pos, lemma, ner, parse, dcoref\");\n    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n\n    // read some text in the text variable\n    String text = \"the quick fox jumps over the lazy dog\";\n\n    // create an empty Annotation just with the given text\n    Annotation document = new Annotation(text);\n\n    // run all Annotators on this text\n    pipeline.annotate(document);\n\n    // these are all the sentences in this document\n    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types\n    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);\n\n    for(CoreMap sentence: sentences) {\n      // traversing the words in the current sentence\n      // a CoreLabel is a CoreMap with additional token-specific methods\n      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {\n        // this is the text of the token\n        String word = token.get(TextAnnotation.class);\n        // this is the POS tag of the token\n        String pos = token.get(PartOfSpeechAnnotation.class);\n        // this is the NER label of the token\n        String ne = token.get(NamedEntityTagAnnotation.class);       \n      }\n\n      // this is the parse tree of the current sentence\n      Tree tree = sentence.get(TreeAnnotation.class);\n\n      // this is the Stanford dependency graph of the current sentence\n      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);\n    }\n\n    // This is the coreference link graph\n    // Each chain stores a set of mentions that link to each other,\n    // along with a method for getting the most representative mention\n    // Both sentence and token offsets start at 1!\n    Map&lt;Integer, CorefChain&gt; graph = \n      document.get(CorefChainAnnotation.class);\n</code></pre>\n",
    "score": 19,
    "creation_date": 1344271609,
    "view_count": 20704,
    "answer_count": 1,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 3121217,
    "title": "Cosine Similarity of Vectors of different lengths?",
    "body": "<p><a href=\"https://stackoverflow.com/questions/3113428/classifying-documents-into-categories/3114191#3114191\">I'm trying to use TF-IDF</a> to sort documents into categories.  I've calculated the tf_idf for some documents, but now when I try to calculate the Cosine Similarity between two of these documents I get a traceback saying:</p>\n\n<pre><code>#len(u)==201, len(v)==246\n\ncosine_distance(u, v)\nValueError: objects are not aligned\n\n#this works though:\ncosine_distance(u[:200], v[:200])\n&gt;&gt; 0.52230249969265641\n</code></pre>\n\n<p>Is slicing the vector so that len(u)==len(v) the right approach?  I would think that cosine similarity would work with vectors of different lengths.</p>\n\n<p>I'm using <a href=\"https://stackoverflow.com/questions/2380394/simple-implementation-of-n-gram-tf-idf-and-cosine-similarity-in-python/2754261#2754261\">this function</a>:</p>\n\n<pre><code>def cosine_distance(u, v):\n    \"\"\"\n    Returns the cosine of the angle between vectors v and u. This is equal to\n    u.v / |u||v|.\n    \"\"\"\n    return numpy.dot(u, v) / (math.sqrt(numpy.dot(u, u)) * math.sqrt(numpy.dot(v, v))) \n</code></pre>\n\n<p>Also -- is the order of the tf_idf values in the vectors important?  Should they be sorted -- or is it of no importance for this calculation?</p>\n",
    "score": 19,
    "creation_date": 1277497671,
    "view_count": 34654,
    "answer_count": 3,
    "tags": "python;nlp;similarity;nltk;tf-idf"
  },
  {
    "question_id": 610399,
    "title": "Finding related words (specifically physical objects) to a specific word",
    "body": "<p>I am trying to find words (specifically physical objects) related to a single word. For example:</p>\n\n<p><strong>Tennis</strong>: tennis racket, tennis ball, tennis shoe</p>\n\n<p><strong>Snooker</strong>: snooker cue, snooker ball, chalk </p>\n\n<p><strong>Chess</strong>: chessboard, chess piece</p>\n\n<p><strong>Bookcase</strong>: book</p>\n\n<p>I have tried to use WordNet, specifically the meronym semantic relationship; however, this method is not consistent as the results below show:</p>\n\n<p><strong>Tennis</strong>: serve, volley, foot-fault, set point, return, advantage</p>\n\n<p><strong>Snooker</strong>: <em>nothing</em></p>\n\n<p><strong>Chess</strong>: chess move, checkerboard (whose own meronym relationships shows ‘square’ &amp; 'diagonal') </p>\n\n<p><strong>Bookcase</strong>: shelve</p>\n\n<p>Weighting of terms will eventually be required, but that is not really a concern now.</p>\n\n<p>Anyone have any suggestions on how to do this?  </p>\n\n<hr>\n\n<p>Just an update: Ended up using a mixture of both Jeff's and StompChicken's answers.</p>\n\n<p>The quality of information retrieved from Wikipedia is excellent, specifically how (unsurprisingly) there is so much relevant information (in comparison to some corpora where terms such as 'blog' and 'ipod' do not exist).</p>\n\n<p>The range of results from Wikipedia is the best part.  The software is able to match terms such as (lists cut for brevity):</p>\n\n<ul>\n<li>golf: [ball, iron, tee, bag, club]</li>\n<li>photography: [camera, film, photograph, art, image] </li>\n<li>fishing: [fish, net, hook, trap, bait, lure, rod]</li>\n</ul>\n\n<p>The biggest problem is classifying certain words as physical artefacts; default WordNet is not a reliable resource as many terms (such as 'ipod', and even 'trampolining') do not exist in it.</p>\n",
    "score": 19,
    "creation_date": 1236171085,
    "view_count": 5861,
    "answer_count": 2,
    "tags": "nlp;semantics;wordnet"
  },
  {
    "question_id": 70622895,
    "title": "Transformers model from Hugging-Face throws error that specific classes couldn t be loaded",
    "body": "<p>Hi after running this code below, I get the following error.</p>\n<p><em>ValueError: Could not load model facebook/bart-large-mnli with any of the following classes: (&lt;class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'&gt;,).</em></p>\n<pre><code>import tensorflow as tf\nfrom transformers import pipeline\n\nclassifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)\n</code></pre>\n<p>Could someone please help.\nThank you!</p>\n",
    "score": 19,
    "creation_date": 1641566610,
    "view_count": 50659,
    "answer_count": 3,
    "tags": "python;tensorflow;nlp;huggingface-transformers"
  },
  {
    "question_id": 76528610,
    "title": "403 Forbidden 453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints only",
    "body": "<p>I had recently registered for the free level Twitter API, and I would like to use Tweepy to help me extract tweets from users.</p>\n<pre class=\"lang-py prettyprint-override\"><code>api_key = config['twitter']['api_key']\napi_secret = config['twitter']['api_key_secret']\n\naccess_token = config['twitter']['access_token']\naccess_token_secret = config['twitter']['access_token_secret']\n\nauth = tweepy.OAuthHandler(api_key, api_secret)\nauth.set_access_token(access_token, access_token_secret)\n\napi = tweepy.API(auth)\ntweets = api.home_timeline()\n</code></pre>\n<p>But after I ran it, the below error showed up:</p>\n<blockquote>\n<p>Forbidden: 403 Forbidden 453 - You currently have access to a subset\nof Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media\npost, oauth) only. If you need access to this endpoint, you may need a\ndifferent access level. You can learn more here:\n<a href=\"https://developer.twitter.com/en/portal/product\" rel=\"noreferrer\">https://developer.twitter.com/en/portal/product</a>.</p>\n</blockquote>\n<p>If I understand correctly, the Twitter API document did allow for free-level access. However, I am not sure why it does not work for me.</p>\n<p>Does any professional know how to solve this issue?</p>\n",
    "score": 19,
    "creation_date": 1687408245,
    "view_count": 30859,
    "answer_count": 1,
    "tags": "python;twitter;nlp;tweepy"
  },
  {
    "question_id": 43909954,
    "title": "Extracting food items from sentences",
    "body": "<p>Given a sentence: </p>\n\n<blockquote>\n  <p>I had peanut butter and jelly sandwich and a cup of coffee for\n  breakfast</p>\n</blockquote>\n\n<p>I want to be able to extract the following food items from it:</p>\n\n<p><strong>peanut butter and jelly sandwich</strong></p>\n\n<p><strong>coffee</strong></p>\n\n<p>Till now, using POS tagging, I have been able to extract the individual food items, i.e.</p>\n\n<p><strong>peanut, butter, jelly, sandwich, coffee</strong></p>\n\n<p>But like I said, what I need is <strong>peanut butter and jelly sandwich</strong> instead of the individual items.</p>\n\n<p>Is there some way of doing this without having a corpus or database of food items in the backend? </p>\n",
    "score": 19,
    "creation_date": 1494490411,
    "view_count": 3550,
    "answer_count": 5,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 42479370,
    "title": "Getting feature names from within a FeatureUnion + Pipeline",
    "body": "<p>I am using a FeatureUnion to join features found from the title and description of events:</p>\n\n<pre><code>union = FeatureUnion(\n    transformer_list=[\n    # Pipeline for pulling features from the event's title\n        ('title', Pipeline([\n            ('selector', TextSelector(key='title')),\n            ('count', CountVectorizer(stop_words='english')),\n        ])),\n\n        # Pipeline for standard bag-of-words model for description\n        ('description', Pipeline([\n            ('selector', TextSelector(key='description_snippet')),\n            ('count', TfidfVectorizer(stop_words='english')),\n        ])),\n    ],\n\n    transformer_weights ={\n            'title': 1.0,\n            'description': 0.2\n    },\n)\n</code></pre>\n\n<p>However, calling <code>union.get_feature_names()</code> gives me an error: \"Transformer title (type Pipeline) does not provide get_feature_names.\" I'd like to see some of the features that are generated by my different Vectorizers. How do I do this?</p>\n",
    "score": 19,
    "creation_date": 1488177860,
    "view_count": 15854,
    "answer_count": 2,
    "tags": "python-3.x;scikit-learn;nlp;feature-extraction"
  },
  {
    "question_id": 29381505,
    "title": "Why does word2vec use 2 representations for each word?",
    "body": "<p>I am trying to understand why word2vec's skipgram model has 2 representations for each word (the hidden representation which is the word embedding) and the output representation (also called context word embedding) . Is this just for generality where the context can be anything (not just words) or is there a more fundamental reason </p>\n",
    "score": 19,
    "creation_date": 1427852681,
    "view_count": 7690,
    "answer_count": 3,
    "tags": "machine-learning;nlp;word2vec"
  },
  {
    "question_id": 144339,
    "title": "What would the best tool to create a natural DSL in Java?",
    "body": "<p>A couple of days ago, I read a blog entry (<a href=\"http://ayende.com/Blog/archive/2008/09/08/Implementing-generic-natural-language-DSL.aspx\" rel=\"noreferrer\">http://ayende.com/Blog/archive/2008/09/08/Implementing-generic-natural-language-DSL.aspx</a>) where the author discuss the idea of a generic natural language DSL parser using .NET.</p>\n\n<p>The brilliant part of his idea, in my opinion, is that the text is parsed and matched against classes using the same name as the sentences. </p>\n\n<p>Taking as an example, the following lines:</p>\n\n<pre>\nCreate user user1 with email test@email.com and password test\nLog user1 in\nTake user1 to category t-shirts\nMake user1 add item Flower T-Shirt to cart\nTake user1 to checkout\n</pre>\n\n<p>Would get converted using a collection of \"known\" objects, that takes the result of parsing. Some example objects would be (using Java for my example):</p>\n\n<pre><code>public class CreateUser {\n    private final String user;\n    private String email;\n    private String password;\n\n    public CreateUser(String user) {\n    this.user = user;\n    }\n\n    public void withEmail(String email) {\n    this.email = email;\n    }\n\n    public String andPassword(String password) {\n        this.password = password;\n    }\n}\n</code></pre>\n\n<p>So, when processing the first sentence, CreateUser class would be a match (obviously because it's a concatenation of \"create user\") and, since it takes a parameter on the constructor, the parser would take \"user1\" as being the user parameter. </p>\n\n<p>After that, the parser would identify that the next part, \"with email\" also matches a method name, and since that method takes a parameter, it would parse \"test@email.com\" as being the email parameter. </p>\n\n<p>I think you get the idea by now, right? One quite clear application of that, at least for me, would be to allow application testers create \"testing scripts\" in natural language and then parse the sentences into classes that uses JUnit to check for app behaviors.</p>\n\n<p>I'd like to hear ideas, tips and opinions on tools or resource that could code such parser using Java. Better yet if we could avoid using complex lexers, or frameworks like ANTLR, which I think maybe would be using a hammer to kill a fly.</p>\n\n<p>More than that, if anyone is up to start an open source project for that, I would definitely be interested.</p>\n",
    "score": 18,
    "creation_date": 1222545372,
    "view_count": 14598,
    "answer_count": 6,
    "tags": "java;dsl;nlp;parsing"
  },
  {
    "question_id": 5836148,
    "title": "How to use OpenNLP with Java?",
    "body": "<p>I want to POStag an English sentence and do some processing. I would like to use openNLP. I have it installed </p>\n\n<p>When I execute the command</p>\n\n<pre><code>I:\\Workshop\\Programming\\nlp\\opennlp-tools-1.5.0-bin\\opennlp-tools-1.5.0&gt;java -jar opennlp-tools-1.5.0.jar POSTagger models\\en-pos-maxent.bin &lt; Text.txt\n</code></pre>\n\n<p>It gives output POSTagging the input in Text.txt</p>\n\n<pre><code>    Loading POS Tagger model ... done (4.009s)\nMy_PRP$ name_NN is_VBZ Shabab_NNP i_FW am_VBP 22_CD years_NNS old._.\n\n\nAverage: 66.7 sent/s\nTotal: 1 sent\nRuntime: 0.015s\n</code></pre>\n\n<p>I hope it installed properly?</p>\n\n<p>Now how do i do this POStagging from inside a java application? I have added the openNLPtools, jwnl, maxent jar to the project but how do i invoke the POStagging?</p>\n",
    "score": 18,
    "creation_date": 1304103211,
    "view_count": 33456,
    "answer_count": 3,
    "tags": "java;nlp;pos-tagger;opennlp"
  },
  {
    "question_id": 7670427,
    "title": "How does language detection work?",
    "body": "<p>I have been wondering for some time how does Google translate(or maybe a hypothetical translator) detect language from the string entered in the \"from\" field. I have been thinking about this and only thing I can think of is looking for words that are unique to a language in the input string. The other way could be to check sentence formation or other semantics in addition to keywords. But this seems to be a very difficult task considering different languages and their semantics. I did some research to find that there are ways that use n-gram sequences and use some statistical models to detect language. Would appreciate a high level answer too.</p>\n",
    "score": 18,
    "creation_date": 1317877064,
    "view_count": 11799,
    "answer_count": 5,
    "tags": "algorithm;nlp;pattern-matching"
  },
  {
    "question_id": 42711144,
    "title": "How can I install torchtext?",
    "body": "<p>I have PyTorch installed in my machine but whenever I try to do the following-</p>\n\n<pre><code>from torchtext import data\nfrom torchtext import datasets\n</code></pre>\n\n<p>I get the following error.</p>\n\n<pre><code>ImportError: No module named 'torchtext'\n</code></pre>\n\n<p>How can I install torchtext?</p>\n",
    "score": 18,
    "creation_date": 1489124847,
    "view_count": 59370,
    "answer_count": 8,
    "tags": "python;deep-learning;pytorch;nlp"
  },
  {
    "question_id": 9595983,
    "title": "Tools for text simplification (Java)",
    "body": "<p>What is the best tool that can do text simplification using Java?</p>\n\n<p>Here is an example of text simplification:</p>\n\n<pre><code>John, who was the CEO of a company, played golf.\n                       ↓\nJohn played golf. John was the CEO of a company.\n</code></pre>\n",
    "score": 18,
    "creation_date": 1331095651,
    "view_count": 11239,
    "answer_count": 4,
    "tags": "java;nlp;stanford-nlp;gate"
  },
  {
    "question_id": 35345761,
    "title": "Python re.split() vs nltk word_tokenize and sent_tokenize",
    "body": "<p>I was going through <a href=\"https://stackoverflow.com/questions/7501609/python-re-split-vs-split/7501659#7501659\">this question</a>.</p>\n\n<p>Am just wondering whether NLTK would be faster than regex in word/sentence tokenization.</p>\n",
    "score": 18,
    "creation_date": 1455210716,
    "view_count": 24109,
    "answer_count": 1,
    "tags": "python;regex;nlp;nltk;tokenize"
  },
  {
    "question_id": 62525680,
    "title": "Save only best weights with huggingface transformers",
    "body": "<p>Currently, I'm building a new transformer-based model with huggingface-transformers, where attention layer is different from the original one. I used <code>run_glue.py</code> to check performance of my model on GLUE benchmark. However, I found that Trainer class of huggingface-transformers saves all the checkpoints that I set, where I can set the maximum number of checkpoints to save. However, I want to save only the weight (or other stuff like optimizers) with <strong>best</strong> performance on validation dataset, and current Trainer class doesn't seem to provide such thing. (If we set the maximum number of checkpoints, then it removes older checkpoints, not ones with worse performances). <a href=\"https://github.com/huggingface/transformers/issues/2675\" rel=\"noreferrer\">Someone already asked about same question on Github</a>, but I can't figure out how to modify the script and do what I want. Currently, I'm thinking about making a custom Trainer class that inherits original one and change the <code>train()</code> method, and it would be great if there's an easy and simple way to do this. Thanks in advance.</p>\n",
    "score": 18,
    "creation_date": 1592873636,
    "view_count": 23350,
    "answer_count": 5,
    "tags": "deep-learning;nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 47727078,
    "title": "What does a weighted word embedding mean?",
    "body": "<p>In the <a href=\"http://www.aclweb.org/anthology/S17-2100\" rel=\"noreferrer\">paper</a> that I am trying to implement, it says,</p>\n\n<blockquote>\n  <p>In this work, tweets were modeled using three types of text\n  representation. The first one is a bag-of-words model weighted by\n  tf-idf (term frequency\n  - inverse document frequency) (Section\n  2.1.1). The second represents a sentence by averaging the word embeddings of all words (in the sentence) and the third represents a\n  sentence by averaging the weighted word embeddings of all words, the\n  weight of a word is given by tf-idf (Section\n  2.1.2).</p>\n</blockquote>\n\n<p>I am not sure about the <em>third representation</em> which is mentioned as the weighted word embeddings which is using the weight of a word is given by tf-idf. I am not even sure if they can used together. </p>\n",
    "score": 18,
    "creation_date": 1512810962,
    "view_count": 15190,
    "answer_count": 2,
    "tags": "machine-learning;nlp;word2vec;tf-idf;word-embedding"
  },
  {
    "question_id": 45605946,
    "title": "How to do text pre-processing using spaCy?",
    "body": "<p>How to do preprocessing steps like Stopword removal , punctuation removal , stemming and lemmatization in spaCy using python.</p>\n\n<p>I have text data in csv file like paragraphs and sentences. I want to do text cleaning. </p>\n\n<p>Kindly give example by loading csv in pandas dataframe </p>\n",
    "score": 18,
    "creation_date": 1502346242,
    "view_count": 38204,
    "answer_count": 5,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 40783383,
    "title": "Error using langdetect in python: &quot;No features in text&quot;",
    "body": "<p>Hey I have a csv with multilingual text. All I want is a column appended with a the language detected. So I coded as below,</p>\n\n<pre><code>from langdetect import detect \nimport csv\nwith open('C:\\\\Users\\\\dell\\\\Downloads\\\\stdlang.csv') as csvinput:\nwith open('C:\\\\Users\\\\dell\\\\Downloads\\\\stdlang.csv') as csvoutput:\nwriter = csv.writer(csvoutput, lineterminator='\\n')\nreader = csv.reader(csvinput)\n\n    all = []\n    row = next(reader)\n    row.append('Lang')\n    all.append(row)\n\n    for row in reader:\n        row.append(detect(row[0]))\n        all.append(row)\n\n    writer.writerows(all)\n</code></pre>\n\n<p>But I am getting the error as <code>LangDetectException: No features in text</code></p>\n\n<p>The traceback is as follows </p>\n\n<pre><code>runfile('C:/Users/dell/.spyder2-py3/temp.py', wdir='C:/Users/dell/.spyder2-py3')\nTraceback (most recent call last):\n\n  File \"&lt;ipython-input-25-5f98f4f8be50&gt;\", line 1, in &lt;module&gt;\n    runfile('C:/Users/dell/.spyder2-py3/temp.py', wdir='C:/Users/dell/.spyder2-py3')\n\n  File \"C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py\", line 714, in runfile\n    execfile(filename, namespace)\n\n  File \"C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py\", line 89, in execfile\n    exec(compile(f.read(), filename, 'exec'), namespace)\n\n  File \"C:/Users/dell/.spyder2-py3/temp.py\", line 21, in &lt;module&gt;\n    row.append(detect(row[0]))\n\n  File \"C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\langdetect\\detector_factory.py\", line 130, in detect\n    return detector.detect()\n\n  File \"C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\langdetect\\detector.py\", line 136, in detect\n    probabilities = self.get_probabilities()\n\n  File \"C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\langdetect\\detector.py\", line 143, in get_probabilities\n    self._detect_block()\n\n  File \"C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\langdetect\\detector.py\", line 150, in _detect_block\n    raise LangDetectException(ErrorCode.CantDetectError, 'No features in text.')\n</code></pre>\n\n<p>LangDetectException: No features in text.</p>\n\n<p>This is how my csv looks like\n1)skunkiest smokiest yummiest strain pain killer and mood lifter\n2)Relaxation, euphorique, surélevée, somnolence, concentré, picotement, une augmentation de l’appétit, soulager la douleur Giggly, physique, esprit sédation\n3)Reduzierte Angst, Ruhe, gehobener Stimmung, zerebrale Energie, Körper Sedierung\n4)Calmante, relajante muscular, Relajación Mental, disminución de náuseas\n5)重いフルーティーな幸せ非常に強力な頭石のバースト</p>\n\n<p>Please help me with this. </p>\n",
    "score": 18,
    "creation_date": 1479982018,
    "view_count": 33760,
    "answer_count": 5,
    "tags": "python;text-analysis;language-detection"
  },
  {
    "question_id": 1032288,
    "title": "N-grams: Explanation + 2 applications",
    "body": "<p>I want to implement some applications with n-grams (preferably in PHP). </p>\n\n<hr>\n\n<p>Which type of n-grams is more adequate for most purposes? A word level or a character level n-gram? How could you implement an n-gram-tokenizer in PHP?</p>\n\n<hr>\n\n<p>First, I would like to know what N-grams exactly are. Is this correct? It's how I understand n-grams:</p>\n\n<p>Sentence: \"I live in NY.\"</p>\n\n<p>word level bigrams (2 for n): \"# I', \"I live\", \"live in\", \"in NY\", 'NY #'</p>\n\n<p>character level bigrams (2 for n): \"#I\", \"I#\", \"#l\", \"li\", \"iv\", \"ve\", \"e#\", \"#i\", \"in\", \"n#\", \"#N\", \"NY\", \"Y#\"</p>\n\n<p>When you have this array of n-gram-parts, you drop the duplicate ones and add a counter for each part giving the frequency:</p>\n\n<p>word level bigrams: [1, 1, 1, 1, 1]</p>\n\n<p>character level bigrams: [2, 1, 1, ...]</p>\n\n<p>Is this correct?</p>\n\n<hr>\n\n<p>Furthermore, I would like to learn more about what you can do with n-grams:</p>\n\n<ul>\n<li>How can I identify the language of a text using n-grams?</li>\n<li>Is it possible to do machine translation using n-grams even if you don't have a bilingual corpus?</li>\n<li>How can I build a spam filter (spam, ham)? Combine n-grams with a Bayesian filter?</li>\n<li>How can I do topic spotting? For example: Is a text about basketball or dogs? My approach (do the following with a Wikipedia article for \"dogs\" and \"basketball\"): build the n-gram vectors for both documents, normalize them, calculate Manhattan/Euclidian distance, the closer the result is to 1 the higher is the similarity</li>\n</ul>\n\n<p>What do you think about my application approaches, especially the last one?</p>\n\n<hr>\n\n<p>I hope you can help me. Thanks in advance!</p>\n",
    "score": 18,
    "creation_date": 1245760625,
    "view_count": 10437,
    "answer_count": 2,
    "tags": "php;nlp;analysis;n-gram"
  },
  {
    "question_id": 53870599,
    "title": "Disabling part of the nlp pipeline",
    "body": "<p>I am running spaCy v2.x on a windows box with python3. I do not have admin privelages, so i have to call the pipeline as:</p>\n\n<p><code>nlp = en_core_web_sm.load()</code></p>\n\n<p>When I run my same script on a *nix box, I can load the pipeline as:</p>\n\n<p><code>nlp = spacy.load('en', disable = ['ner', 'tagger', 'parser', 'textcat'])</code></p>\n\n<p>All I am do is tokenizing, so I do not need the entire pipeline. On the windows box, if I load the pipeline like:</p>\n\n<p><code>nlp = en_core_web_sm.load(disable = ['ner', 'tagger', 'parser', 'textcat'])</code></p>\n\n<p>Does that actually disable the components?</p>\n\n<p><a href=\"https://spacy.io/usage/processing-pipelines\" rel=\"noreferrer\">spaCy information on the nlp pipeline</a></p>\n",
    "score": 18,
    "creation_date": 1545316118,
    "view_count": 15788,
    "answer_count": 2,
    "tags": "python-3.x;nlp;spacy"
  },
  {
    "question_id": 20314636,
    "title": "What does the tag SBAR mean in Stanford’s parse-tree representation?",
    "body": "<p>When the <a href=\"http://nlp.stanford.edu:8080/parser/\" rel=\"nofollow noreferrer\">Online Stanford Parser tool</a> is fed this original sentence: </p>\n\n<blockquote>\n  <p>After she ate the cake, Emma visited Tony in his room.</p>\n</blockquote>\n\n<p>It produces the following parse-tree representation as its output:</p>\n\n<pre><code>(ROOT\n  (S\n    (SBAR (IN After)\n      (S\n        (NP (PRP she))\n        (VP (VBD ate)\n          (NP (DT the) (NN cake)))))\n    (, ,)\n    (NP (NNP Emma))\n    (VP (VBD visited)\n      (NP\n        (NP (NNP Tony))\n        (PP (IN in)\n          (NP (PRP$ his) (NN room)))))\n    (. .)))\n</code></pre>\n\n<p>My questions are:</p>\n\n<ol>\n<li>What does the <code>SBAR</code> tag mean?</li>\n<li>Why are there two different <code>S</code> tags?</li>\n<li>What is the correct NLP parse-tree representation of this sentence?</li>\n</ol>\n",
    "score": 18,
    "creation_date": 1385916676,
    "view_count": 10428,
    "answer_count": 1,
    "tags": "parsing;nlp;stanford-nlp;parse-tree"
  },
  {
    "question_id": 2954814,
    "title": "SOLR and Natural Language Parsing - Can I use it?",
    "body": "<h3>Requirements</h3>\n\n<p><a href=\"https://stackoverflow.com/questions/90580/word-frequency-algorithm-for-natural-language-processing\">Word frequency algorithm for natural language processing</a></p>\n\n<h3>Using Solr</h3>\n\n<p>While the answer for that question is excellent, I was wondering if I could make use of all the time I spent getting to know SOLR for my NLP.</p>\n\n<p>I thought of SOLR because:</p>\n\n<ol>\n<li>It's got a bunch of tokenizers and performs a lot of NLP.</li>\n<li>It's pretty use to use out of the box.</li>\n<li>It's restful distributed app, so it's easy to hook up</li>\n<li>I've spent some time with it, so using could save me time.</li>\n</ol>\n\n<h3>Can I use Solr?</h3>\n\n<p>Although the above reasons are good, I don't know SOLR THAT well, so I need to know if it would be appropriate for my requirements.</p>\n\n<h3>Ideal Usage</h3>\n\n<p>Ideally, I'd like to configure SOLR, and then be able to send SOLR some text, and retrieve the indexed tonkenized content.</p>\n\n<h3>Context</h3>\n\n<p>I'm working on a small component of a bigger recommendation engine.</p>\n",
    "score": 18,
    "creation_date": 1275449663,
    "view_count": 20629,
    "answer_count": 4,
    "tags": "lucene;solr;nlp;recommendation-engine"
  },
  {
    "question_id": 1284782,
    "title": "How to correct the user input (Kind of google &quot;did you mean?&quot;)",
    "body": "<p>I have the following requirement: -</p>\n\n<p>I have many (say 1 million) values (names).\nThe user will type a search string.</p>\n\n<p>I don't expect the user to spell the names correctly.</p>\n\n<p>So, I want to make kind of Google \"Did you mean\". This will list all the possible values from my datastore. There is a similar but not same question <a href=\"https://stackoverflow.com/questions/135777/a-stringtoken-parser-which-gives-google-search-style-did-you-mean-suggestions\">here</a>. This did not answer my question.</p>\n\n<p>My question: -\n1) I think it is not advisable to store those data in RDBMS. Because then I won't have filter on the SQL queries. And I have to do full table scan. So, in <strong>this situation how the data should be stored?</strong></p>\n\n<p>2) The second question is the same as <a href=\"https://stackoverflow.com/questions/135777/a-stringtoken-parser-which-gives-google-search-style-did-you-mean-suggestions\">this</a>. But, just for the completeness of my question: how do I search through the large data set?\nSuppose, there is a name Franky in the dataset. \nIf a user types as Phranky, how do I match the Franky? Do I have to loop through all the names?</p>\n\n<p>I came across <a href=\"http://en.wikipedia.org/wiki/Levenshtein_distance\" rel=\"nofollow noreferrer\">Levenshtein Distance</a>, which will be a good technique to find the possible strings. But again, my question is do I have to operate on all 1 million values from my data store?</p>\n\n<p>3) I know, Google does it by watching users behavior. But I want to do it without watching user behavior, i.e. by using, I don't know yet, say distance algorithms. Because the former method will require large volume of searches to start with!</p>\n\n<p>4) As <a href=\"https://stackoverflow.com/users/146077/kirk-broadhurst\">Kirk Broadhurst</a> pointed out in an answer <a href=\"https://stackoverflow.com/questions/1284782/how-to-correct-the-user-input-kind-of-google-did-you-mean/1360275#1360275\">below</a>, there are two possible scenarios: -</p>\n\n<ul>\n<li>Users mistyping a word (an edit\ndistance algorithm)</li>\n<li>Users not knowing a word and guessing\n(a phonetic match algorithm)</li>\n</ul>\n\n<p>I am interested in both of these. They are really two separate things; e.g. Sean and Shawn sound the same but have an edit distance of 3 - too high to be considered a typo.</p>\n",
    "score": 18,
    "creation_date": 1250442208,
    "view_count": 2829,
    "answer_count": 8,
    "tags": "language-agnostic;nlp;spell-checking;information-retrieval;autosuggest"
  },
  {
    "question_id": 61134275,
    "title": "Difficulty in understanding the tokenizer used in Roberta model",
    "body": "<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModel, AutoTokenizer\n\ntokenizer1 = AutoTokenizer.from_pretrained(\"roberta-base\")\ntokenizer2 = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nsequence = \"A Titan RTX has 24GB of VRAM\"\nprint(tokenizer1.tokenize(sequence))\nprint(tokenizer2.tokenize(sequence))\n</code></pre>\n\n<p>Output:</p>\n\n<p>['A', 'ĠTitan', 'ĠRTX', 'Ġhas', 'Ġ24', 'GB', 'Ġof', 'ĠVR', 'AM']</p>\n\n<p>['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']</p>\n\n<p>Bert model uses WordPiece tokenizer. Any word that does not occur in the WordPiece vocabulary is broken down into sub-words greedily. For example, 'RTX' is broken into 'R', '##T' and '##X' where ## indicates it is a subtoken. </p>\n\n<p>Roberta uses BPE tokenizer but I'm unable to understand </p>\n\n<p>a) how BPE tokenizer works? </p>\n\n<p>b) what does G represents in each of tokens?</p>\n",
    "score": 18,
    "creation_date": 1586494706,
    "view_count": 12331,
    "answer_count": 2,
    "tags": "nlp;pytorch;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 55817040,
    "title": "removing stop words using spacy",
    "body": "<p>I am cleaning a column in my <code>data frame</code>, Sumcription, and am trying to do 3 things:</p>\n\n<ol>\n<li>Tokenize</li>\n<li>Lemmantize</li>\n<li><p>Remove stop words </p>\n\n<pre><code>import spacy        \nnlp = spacy.load('en_core_web_sm', parser=False, entity=False)        \ndf['Tokens'] = df.Sumcription.apply(lambda x: nlp(x))    \nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS        \nspacy_stopwords.add('attach')\ndf['Lema_Token']  = df.Tokens.apply(lambda x: \" \".join([token.lemma_ for token in x if token not in spacy_stopwords]))\n</code></pre></li>\n</ol>\n\n<p>However, when I print for example: </p>\n\n<pre><code>df.Lema_Token.iloc[8]\n</code></pre>\n\n<p>The output still has the word attach in it:\n<code>attach poster on the wall because it is cool</code></p>\n\n<p>Why does it not remove the stop word?</p>\n\n<p>I also tried this:</p>\n\n<pre><code>df['Lema_Token_Test']  = df.Tokens.apply(lambda x: [token.lemma_ for token in x if token not in spacy_stopwords])\n</code></pre>\n\n<p>But the str <code>attach</code> still appears.</p>\n",
    "score": 18,
    "creation_date": 1556042781,
    "view_count": 31937,
    "answer_count": 1,
    "tags": "python;nlp;spacy;python-3.7;data-cleaning"
  },
  {
    "question_id": 32011615,
    "title": "How to create a good NER training model in OpenNLP?",
    "body": "<p>I just have started with OpenNLP. I need to create a simple training model to recognize name entities. </p>\n\n<p>Reading the doc here <a href=\"https://opennlp.apache.org/docs/1.8.0/apidocs/opennlp-tools/opennlp/tools/namefind\" rel=\"nofollow noreferrer\">https://opennlp.apache.org/docs/1.8.0/apidocs/opennlp-tools/opennlp/tools/namefind</a> I see this simple text to train the model:</p>\n\n<pre><code>&lt;START:person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .\nMr . &lt;START:person&gt; Vinken &lt;END&gt; is chairman of Elsevier N.V. , the Dutch publishing group .\n&lt;START:person&gt; Rudolph Agnew &lt;END&gt; , 55 years old and former chairman of Consolidated Gold Fields PLC ,\n    was named a director of this British industrial conglomerate .\n</code></pre>\n\n<p>The questions are two:</p>\n\n<ul>\n<li><p>Why should i have to put the names of the persons in a text (phrase) context ? Why not write person's name one for each line? like:</p>\n\n<pre><code>&lt;START:person&gt; Robert &lt;END&gt;\n\n&lt;START:person&gt; Maria &lt;END&gt;\n\n&lt;START:person&gt; John &lt;END&gt;\n</code></pre></li>\n<li><p>How can I also add extra information to that name?\nFor example I would like to save the information Male/Female for each name.</p></li>\n</ul>\n\n<p>(I know there are systems that try to understand it reading the last letter, like the \"a\" for <strong>Female</strong> etc but i would like to add it myself)</p>\n\n<p>Thanks.</p>\n",
    "score": 18,
    "creation_date": 1439559826,
    "view_count": 7745,
    "answer_count": 1,
    "tags": "java;nlp;text-mining;opennlp;named-entity-recognition"
  },
  {
    "question_id": 3917134,
    "title": "What are some good natural language parsing tools for Perl?",
    "body": "<p>I've heard that Perl is used a lot for NLP, but I can't find almost any good NLP tools for Perl. What are some good Perl NLP tools/resources? Python has NLTK. Java has OpenNLP. Does Perl have anything similar?</p>\n\n<p>This is really a general question, but if someone could also specifically address chunking and POS-tagging, that would be awesome!</p>\n",
    "score": 18,
    "creation_date": 1286902665,
    "view_count": 4678,
    "answer_count": 4,
    "tags": "perl;nlp"
  },
  {
    "question_id": 11911469,
    "title": "TF*IDF for Search Queries",
    "body": "<p>Okay, so I have been following these two posts on TF*IDF but am little confused : <a href=\"http://css.dzone.com/articles/machine-learning-text-feature\">http://css.dzone.com/articles/machine-learning-text-feature</a></p>\n\n<p>Basically, I want to create a search query that contains searches through multiple documents. I would like to use the scikit-learn toolkit as well as the NLTK library for Python</p>\n\n<p>The problem is that I don't see where the two TF*IDF vectors come from. I need one search query and multiple documents to search. I figured that I calculate the TF*IDF scores of each document against each query and find the cosine similarity between them, and then rank them by sorting the scores in descending order. However, the code doesn't seem to come up with the right vectors.</p>\n\n<p>Whenever I reduce the query to only one search, it is returning a huge list of 0's which is really strange. </p>\n\n<p><strong>Here is the code:</strong></p>\n\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\n\ntrain_set = (\"The sky is blue.\", \"The sun is bright.\") #Documents\ntest_set = (\"The sun in the sky is bright.\") #Query\nstopWords = stopwords.words('english')\n\nvectorizer = CountVectorizer(stop_words = stopWords)\ntransformer = TfidfTransformer()\n\ntrainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\ntestVectorizerArray = vectorizer.transform(test_set).toarray()\nprint 'Fit Vectorizer to train set', trainVectorizerArray\nprint 'Transform Vectorizer to test set', testVectorizerArray\n\ntransformer.fit(trainVectorizerArray)\nprint transformer.transform(trainVectorizerArray).toarray()\n\ntransformer.fit(testVectorizerArray)\n\ntfidf = transformer.transform(testVectorizerArray)\nprint tfidf.todense()\n</code></pre>\n",
    "score": 18,
    "creation_date": 1344653089,
    "view_count": 9717,
    "answer_count": 1,
    "tags": "python;nlp;nltk;scikit-learn;tf-idf"
  },
  {
    "question_id": 49702372,
    "title": "Speed up Spacy Named Entity Recognition",
    "body": "<p>I'm using spacy to recognize street addresses on web pages.  </p>\n\n<p>My model is initialized basically using spacy's new entity type sample code found here:\n<a href=\"https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py\" rel=\"noreferrer\">https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py</a></p>\n\n<p>My training data consists of plain text webpages with their corresponding Street Address entities and character positions.</p>\n\n<p>I was able to quickly build a model in spacy to start making predictions, but I found its prediction speed to be very slow.</p>\n\n<p>My code works by iterating through serveral raw HTML pages and then feeding each page's plain text version into spacy as it's iterating.  For reasons I can't get into, I need to make predictions with Spacy page by page, inside of the iteration loop.</p>\n\n<p>After the model is loaded, I'm using the standard way of making predictions, which I'm referring to as the prediction/evaluation phase:</p>\n\n<pre><code>  doc = nlp(plain_text_webpage)\n\n  if len(doc.ents) &gt; 0:\n\n         print (\"found entity\")\n</code></pre>\n\n<p>Questions:</p>\n\n<ol>\n<li><p>How can I speed up the entity prediction / recognition phase?  I'm using a c4.8xlarge instance on AWS and all 36 cores are constantly maxed out when spacy is evaluating the data.  Spacy is turning processing a few million webpages from a 1 minute job to a 1 hour+ job.</p></li>\n<li><p>Will the speed of entity recognition improve as my model becomes more accurate?</p></li>\n<li><p>Is there a way to remove pipelines like tagger during this phase, can ER be decoupled like that and still be accurate?  Will removing other pipelines affect the model itself or is it just a temporary thing?</p></li>\n<li><p>I saw that you can use GPU during the ER training phase, can it also be used in this evaluating phase in my code for faster predictions?</p></li>\n</ol>\n\n<hr>\n\n<p><strong>Update:</strong></p>\n\n<p>I managed to significantly cut down the processing time by:</p>\n\n<ol>\n<li><p>Using a custom tokenizer (used the one in the docs)</p></li>\n<li><p>Disabling other pipelines that aren't for Named Entity Recognition</p></li>\n<li><p>Instead of feeding the whole body of text from each webpage into spacy, I'm only sending over a maximum of 5,000 characters</p></li>\n</ol>\n\n<p>My updated code to load the model:</p>\n\n<pre><code>nlp = spacy.load('test_model/', disable=['parser', 'tagger', 'textcat'])\nnlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\ndoc = nlp(text)\n</code></pre>\n\n<p>However, it is <strong><em>still</em></strong> too slow (20X slower than I need it)</p>\n\n<p><strong>Questions:</strong></p>\n\n<ol>\n<li><p>Are there any other improvements I can make to speed up the Named Entity Recognition?  Any fat I can cut from spacy?</p></li>\n<li><p>I'm still looking to see if a GPU based solution would help - I saw that GPU use is supported during the Named Entity Recognition training phase, can it also be used in this evaluation phase in my code for faster predictions?</p></li>\n</ol>\n",
    "score": 18,
    "creation_date": 1523058294,
    "view_count": 9752,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 16408163,
    "title": "ARPA language model documentation",
    "body": "<p>Where can I find documentation on ARPA language model format?</p>\n\n<p>I am developing simple speech recognition app with pocket-sphinx STT engine. ARPA is recommended there for performance reasons.\nI want to understand how much can I do to adjust my language model for my custom needs.</p>\n\n<p>All I found is some very brief ARPA format descriptions:</p>\n\n<ul>\n<li><a href=\"http://kered.org/blog/2008-08-12/arpa-language-model-file-format/\" rel=\"noreferrer\">http://kered.org/blog/2008-08-12/arpa-language-model-file-format/</a></li>\n<li><a href=\"http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html\" rel=\"noreferrer\">http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html</a></li>\n<li><a href=\"http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html\" rel=\"noreferrer\">http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html</a></li>\n</ul>\n\n<p>I am beginner to STT and I have trouble to wrap head around this (n-grams, etc...). I am looking for more detailed docs. Something like documentation on JSGF grammar here:</p>\n\n<p><a href=\"http://www.w3.org/TR/jsgf/\" rel=\"noreferrer\">http://www.w3.org/TR/jsgf/</a></p>\n",
    "score": 18,
    "creation_date": 1367878448,
    "view_count": 11521,
    "answer_count": 3,
    "tags": "nlp;speech-recognition;cmusphinx;sphinx4;language-model"
  },
  {
    "question_id": 4754547,
    "title": "Redefining &quot;sentence&quot; in Emacs? (single space between sentences, but ignoring abbreviations)",
    "body": "<p>I would like to be able to navigate by sentence in Emacs (M-a, M-e). Here's the problem: by default, Emacs expects that each sentence is separated by two spaces, and I'm used to just putting a single space. Of course, that setting can be turned off, to allow for sentences separated by only a single space, like so:</p>\n\n\n\n<pre class=\"lang-lisp prettyprint-override\"><code>(setq sentence-end-double-space nil)   \n</code></pre>\n\n<p>But then Emacs thinks that a sentence has ended after abbreviations with a full stop (\".\"), e.g. after something like \"...a weird command, e.g. foo...\".</p>\n\n<p>So rather than using the above code, is there a way to define the      sentence-end variable so that it counts [.!?] as marking the end of the sentence, iff what follows is one or more spaces followed by a capital letter [A-Z]?</p>\n\n<p>And...to also allow [.!?] to mark the end of a sentence, if followed by zero or more spaces followed by a \"\\\"? [The reason for this latter condition is for writing LaTeX code: where a sentence is followed by a LaTeX command like \\footnote{}, e.g. \"...and so we can see that the point is proved.\\footnote{In some alternate world, at least.}\"] </p>\n\n<p>I tried playing around with the definition of sentence-end, and came up with:</p>\n\n\n\n<pre class=\"lang-lisp prettyprint-override\"><code>(setq sentence-end \"[.!?][]'\\\")}]*\\\\(\\\\$\\\\|[ ]+[A-Z]\\\\|[ ]+[A-Z]\\\\| \\\\)[\n ;]*\")\n</code></pre>\n\n<p>But this doesn't seem to work at all.</p>\n\n<p>Any suggestions? </p>\n",
    "score": 18,
    "creation_date": 1295573261,
    "view_count": 2628,
    "answer_count": 1,
    "tags": "regex;emacs;nlp;typography;punctuation"
  },
  {
    "question_id": 38423387,
    "title": "Why does word2Vec use cosine similarity?",
    "body": "<p>I have been reading the papers on Word2Vec (e.g. <a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\" rel=\"noreferrer\">this one</a>), and I think I understand training the vectors to maximize the probability of other words found in the same contexts.</p>\n\n<p>However, I do not understand why cosine is the correct measure of word similarity.  Cosine similarity says that two vectors point in the same direction, but they could have different magnitudes.</p>\n\n<p>For example, cosine similarity makes sense comparing bag-of-words for documents.  Two documents might be of different length, but have similar distributions of words.</p>\n\n<p>Why not, say, Euclidean distance?</p>\n\n<p>Can anyone one explain why cosine similarity works for word2Vec?</p>\n",
    "score": 18,
    "creation_date": 1468772709,
    "view_count": 10147,
    "answer_count": 2,
    "tags": "nlp;deep-learning;word2vec"
  },
  {
    "question_id": 44274199,
    "title": "Character-Word Embeddings from lm_1b in Keras",
    "body": "<p>I would like to use some pre-trained word embeddings in a Keras NN model, which have been published by Google in a <a href=\"https://arxiv.org/pdf/1602.02410.pdf\" rel=\"noreferrer\">very well known article</a>.   They have provided the code to train a new model, as well as the embeddings <a href=\"https://github.com/tensorflow/models/tree/master/lm_1b\" rel=\"noreferrer\">here</a>.</p>\n\n<p>However, it is not clear from the documentation how to retrieve an embedding vector from a given string of characters (word) from a simple python function call.  Much of the documentation seems to center on dumping vectors to a <em>file</em> for an entire sentence presumably for sentimental analysis.  </p>\n\n<p>So far, I have seen that you can feed in pretrained embeddings with the following syntax:</p>\n\n<pre><code>embedding_layer = Embedding(number_of_words??,\n                            out_dim=128??,\n                            weights=[pre_trained_matrix_here],\n                            input_length=60??,\n                            trainable=False)\n</code></pre>\n\n<p>However, converting the different files and their structures to <code>pre_trained_matrix_here</code> is not quite clear to me.</p>\n\n<p>They have several softmax outputs, so I am uncertain which one would belong - and furthermore how to align the words in my input to the dictionary of words for which they have.</p>\n\n<p>Is there a simple manner to use these word/char embeddings in keras and/or to construct the character/word embedding portion of the model in keras such that further layers may be added for other NLP tasks?</p>\n",
    "score": 18,
    "creation_date": 1496193589,
    "view_count": 1737,
    "answer_count": 2,
    "tags": "machine-learning;nlp;keras;language-model;word-embedding"
  },
  {
    "question_id": 48201131,
    "title": "How to access Dialogflow V2 API from a webpage?",
    "body": "<p>I have a webpage where I want to use dialogflow chatbot. This is a custom chat window, so I don't want to use one click integration. I am able to access the chat agent <strong>V1 API</strong> using javascript/ajax (by passing <strong>client access token</strong> in the request header).</p>\n\n<p>But I don't know how to do it in <strong>V2 API</strong>. The dialogflow documentation is not clear to me(I have setup Authentication by referring <a href=\"https://dialogflow.com/docs/reference/v2-auth-setup#using_the_key\" rel=\"noreferrer\">this</a> link. I don't know how to proceed further). I'm not familiar with Google cloud either. So a working sample or a step by step how to access the API guideline will be very much appreciated.</p>\n",
    "score": 18,
    "creation_date": 1515652738,
    "view_count": 4024,
    "answer_count": 1,
    "tags": "nlp;chatbot;actions-on-google;dialogflow-es"
  },
  {
    "question_id": 37611061,
    "title": "spaCy token.tag_ full list",
    "body": "<p>The official documentation of <a href=\"https://spacy.io/docs#token-postags\" rel=\"noreferrer\"><code>token.tag_</code></a> in <code>spaCy</code> is as follows:</p>\n\n<blockquote>\n  <p>A fine-grained, more detailed tag that represents the word-class and some basic morphological information for the token. These tags are primarily designed to be good features for subsequent models, particularly the syntactic parser. They are language and treebank dependent. The tagger is trained to predict these fine-grained tags, and then a mapping table is used to reduce them to the coarse-grained  .pos tags.</p>\n</blockquote>\n\n<p>But it doesn't list the full available tags and each tag's explanation. Where can I find it?</p>\n",
    "score": 17,
    "creation_date": 1464947180,
    "view_count": 30711,
    "answer_count": 6,
    "tags": "nlp;pos-tagger;spacy"
  },
  {
    "question_id": 5364493,
    "title": "Lemmatizing POS tagged words with NLTK?",
    "body": "<p>I have POS tagged some words with nltk.pos_tag(), so they are given treebank tags. I would like to lemmatize these words using the known POS tags, but I am not sure how. I was looking at Wordnet lemmatizer, but I am not sure how to convert the treebank POS tags to tags accepted by the lemmatizer. How can I perform this conversion simply, or is there a lemmatizer that uses treebank tags?</p>\n",
    "score": 17,
    "creation_date": 1300564156,
    "view_count": 9381,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 67546911,
    "title": "Python: BERT Error - Some weights of the model checkpoint at were not used when initializing BertModel",
    "body": "<p>I am creating an entity extraction model in PyTorch using <code>bert-base-uncased</code> but when I try to run the model I get this error:</p>\n<pre><code>Some weights of the model checkpoint at D:\\Transformers\\bert-entity-extraction\\input\\bert-base-uncased_L-12_H-768_A-12 were not used when initializing BertModel:    \n['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight',   'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias',  \n 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight',  \n 'cls.predictions.bias']  \n    - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n    - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n</code></pre>\n<p>I have downloaded the bert model from <a href=\"https://github.com/google-research/bert\" rel=\"nofollow noreferrer\">here</a> and the additional files from <a href=\"https://huggingface.co/bert-base-uncased/tree/main\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>Following is the code for my model:</p>\n<pre><code>import config\nimport torch\nimport transformers\nimport torch.nn as nn\n\ndef loss_fn(output, target, mask, num_labels):\n\n    lfn = nn.CrossEntropyLoss()\n    active_loss = mask.view(-1) == 1\n    active_logits = output.view(-1, num_labels)\n    active_labels = torch.where(\n        active_loss,\n        target.view(-1),\n        torch.tensor(lfn.ignore_index).type_as(target)\n    )\n    loss = lfn(active_logits, active_labels)\n    return loss\n\nclass EntityModel(nn.Module):\n    def __init__(self, num_tag, num_pos):\n        super(EntityModel, self).__init__()\n\n        self.num_tag = num_tag\n        self.num_pos = num_pos\n        self.bert = transformers.BertModel.from_pretrained(config.BASE_MODEL_PATH)\n        self.bert_drop_1 = nn.Dropout(p = 0.3)\n        self.bert_drop_2 = nn.Dropout(p = 0.3)\n        self.out_tag = nn.Linear(768, self.num_tag)\n        self.out_pos = nn.Linear(768, self.num_pos)\n\n    def forward(self, ids, mask, token_type_ids, target_pos, target_tag):\n        o1, _ = self.bert(ids, \n                          attention_mask = mask,\n                          token_type_ids = token_type_ids)\n\n        bo_tag = self.bert_drop_1(o1)\n        bo_pos = self.bert_drop_2(o1)\n\n        tag = self.out_tag(bo_tag)\n        pos = self.out_pos(bo_pos)\n\n        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n\n        loss = (loss_tag + loss_pos) / 2\n\n        return tag, pos, loss \n\nprint(&quot;model.py run success!&quot;)\n</code></pre>\n",
    "score": 17,
    "creation_date": 1621083032,
    "view_count": 42728,
    "answer_count": 3,
    "tags": "python;nlp;pytorch;bert-language-model;huggingface-transformers"
  },
  {
    "question_id": 66590981,
    "title": "Transformer: Error importing packages. &quot;ImportError: cannot import name &#39;SAVE_STATE_WARNING&#39; from &#39;torch.optim.lr_scheduler&#39;&quot;",
    "body": "<p>I am working on a machine learning project on Google Colab, it seems recently there is an issue when trying to import packages from transformers. The error message says:</p>\n<blockquote>\n<p>ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' (/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py)</p>\n</blockquote>\n<p>The code is simple as follow:</p>\n<pre><code>!pip install transformers==3.5.1\n\nfrom transformers import BertTokenizer\n</code></pre>\n<p>So far I've tried to install different versions of the transformers, and import some other packages, but it seems importing any package with:</p>\n<pre><code>from transformers import *Package\n</code></pre>\n<p>is not working, and will result in the same error. I wonder if anyone is running into the same issue as well?\n<a href=\"https://i.sstatic.net/paBmu.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/paBmu.png\" alt=\"Screenshot of the error\" /></a></p>\n",
    "score": 17,
    "creation_date": 1615499027,
    "view_count": 88745,
    "answer_count": 5,
    "tags": "python;nlp;google-colaboratory;bert-language-model;huggingface-transformers"
  },
  {
    "question_id": 15857384,
    "title": "Accuracy: ANNIE vs Stanford NLP vs OpenNLP with UIMA",
    "body": "<p>My work is planning on using a UIMA cluster to run documents through to extract named entities and what not.  As I understand it, UIMA have very few NLP components packaged with it.  I've been testing GATE for awhile now and am fairly comfortable with it.  It does ok on normal text, but when we run it through some representative test data, the accuracy drops way down.  The text data we have internally is sometimes all caps, sometimes all lowercase, or a mix of the two in the same document.  Even using ANNIE's all caps rules, the accuracy still leaves much to be desired.  I've recently heard of Stanford NLP and OpenNLP but haven't had time to extensively train and test them.  How do those two compare in terms of accuracy with ANNIE?  Do they work with UIMA like GATE does?</p>\n\n<p>Thanks in advance.</p>\n",
    "score": 17,
    "creation_date": 1365293172,
    "view_count": 9213,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp;opennlp;gate;uima"
  },
  {
    "question_id": 61323621,
    "title": "How to understand hidden_states of the returns in BertModel?(huggingface-transformers)",
    "body": "<blockquote>\n  <p>Returns last_hidden_state (torch.FloatTensor of shape (batch_size,\n  sequence_length, hidden_size)): Sequence of hidden-states at the\n  output of the last layer of the model.</p>\n  \n  <p>pooler_output (torch.FloatTensor: of shape (batch_size, hidden_size)):\n  Last layer hidden-state of the first token of the sequence\n  (classification token) further processed by a Linear layer and a Tanh\n  activation function. The Linear layer weights are trained from the\n  next sentence prediction (classification) objective during\n  pre-training.</p>\n  \n  <p>This output is usually not a good summary of the semantic content of\n  the input, you’re often better with averaging or pooling the sequence\n  of hidden-states for the whole input sequence.</p>\n  \n  <p>hidden_states (tuple(torch.FloatTensor), optional, returned when\n  config.output_hidden_states=True): Tuple of torch.FloatTensor (one for\n  the output of the embeddings + one for the output of each layer) of\n  shape (batch_size, sequence_length, hidden_size).</p>\n  \n  <p>Hidden-states of the model at the output of each layer plus the\n  initial embedding outputs.</p>\n  \n  <p>attentions (tuple(torch.FloatTensor), optional, returned when\n  config.output_attentions=True): Tuple of torch.FloatTensor (one for\n  each layer) of shape (batch_size, num_heads, sequence_length,\n  sequence_length).</p>\n  \n  <p>Attentions weights after the attention softmax, used to compute the\n  weighted average in the self-attention heads.</p>\n</blockquote>\n\n<p>This is from <a href=\"https://huggingface.co/transformers/model_doc/bert.html#bertmodel\" rel=\"noreferrer\">https://huggingface.co/transformers/model_doc/bert.html#bertmodel</a>. Although the description in the document is clear, I still don't understand the <strong>hidden_states</strong> of returns. There is a tuple, one for the output of the embeddings, and the other for the output of each layer.\nPlease tell me how to distinguish them, or what is the meaning of them? Thanks very much!![wink~</p>\n",
    "score": 17,
    "creation_date": 1587389197,
    "view_count": 32148,
    "answer_count": 3,
    "tags": "nlp;pytorch;huggingface-transformers;bert-language-model;electrate"
  },
  {
    "question_id": 20998832,
    "title": "What does the Brown clustering algorithm output mean?",
    "body": "<p>I've ran the brown-clustering algorithm from <a href=\"https://github.com/percyliang/brown-cluster\">https://github.com/percyliang/brown-cluster</a> and also a python implementation <a href=\"https://github.com/mheilman/tan-clustering\">https://github.com/mheilman/tan-clustering</a>. And they both give some sort of binary and another integer for each unique token. For example:</p>\n\n<pre><code>0        the        6\n10        chased        3\n110        dog        2\n1110        mouse        2\n1111        cat        2\n</code></pre>\n\n<p><strong>What does the binary and the integer mean?</strong></p>\n\n<p>From the first <a href=\"https://github.com/percyliang/brown-cluster\">link</a>, the binary is known as a <code>bit-string</code>, see <a href=\"http://saffron.deri.ie/acl_acl/document/ACL_ANTHOLOGY_ACL_P11-1053/\">http://saffron.deri.ie/acl_acl/document/ACL_ANTHOLOGY_ACL_P11-1053/</a></p>\n\n<p>But how do I tell from the output that <code>dog and mouse and cat</code> is one cluster and <code>the and chased</code> is not in the same cluster?</p>\n",
    "score": 17,
    "creation_date": 1389192485,
    "view_count": 8333,
    "answer_count": 5,
    "tags": "python;algorithm;machine-learning;nlp;cluster-analysis"
  },
  {
    "question_id": 77074676,
    "title": "ImportError: cannot import name &#39;deprecated&#39; from &#39;typing_extensions&#39;",
    "body": "<p>I want to download spacy, but the version of typing-extensions is lowered in the terminal:</p>\n<pre><code>ERROR: pydantic 2.3.0 has requirement typing-extensions&gt;=4.6.1, but you'll have typing-extensions 4.4.0 which is incompatible.\nERROR: pydantic-core 2.6.3 has requirement typing-extensions!=4.7.0,&gt;=4.6.0, but you'll have typing-extensions 4.4.0 which is incompatible.\nInstalling collected packages: typing-extensions\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.7.1\n    Uninstalling typing-extensions-4.7.1:\n      Successfully uninstalled typing-extensions-4.7.1\nSuccessfully installed typing-extensions-4.4.0\n</code></pre>\n<p>Next I want to install the language pack <code>python -m spacy download en</code>, but another error occurs：</p>\n<pre><code>(base) E:\\Anaconda&gt;python -m spacy download en\nTraceback (most recent call last):\n  File &quot;E:\\Anaconda\\lib\\site-packages\\confection\\__init__.py&quot;, line 38, in &lt;module&gt;\n    from pydantic.v1 import BaseModel, Extra, ValidationError, create_model\n  File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\__init__.py&quot;, line 13, in &lt;module&gt;\n    from . import dataclasses\n  File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\dataclasses.py&quot;, line 11, in &lt;module&gt;\n    from ._internal import _config, _decorators, _typing_extra\n  File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\_internal\\_config.py&quot;, line 9, in &lt;module&gt;\n    from ..config import ConfigDict, ExtraValues, JsonEncoder, JsonSchemaExtraCallable\n  File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\config.py&quot;, line 9, in &lt;module&gt;\n    from .deprecated.config import BaseConfig\n  File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\deprecated\\config.py&quot;, line 6, in &lt;module&gt;\n    from typing_extensions import Literal, deprecated\nImportError: cannot import name 'deprecated' from 'typing_extensions' (E:\\Anaconda\\lib\\site-packages\\typing_extensions.py)\n</code></pre>\n<p>My current python version is 3.7, should I update it? Or is there any better solution? I'm a newbie in this area, thank you all！</p>\n",
    "score": 17,
    "creation_date": 1694314709,
    "view_count": 56663,
    "answer_count": 4,
    "tags": "python;pip;nlp;spacy;python-typing"
  },
  {
    "question_id": 1793516,
    "title": "Ideas for Natural Language Processing project?",
    "body": "<p>I have to do a final project for my computational linguistics class. We've been using OCaml the entire time, but I also have familiarity with Java. We've studied morphology, FSMs, collecting parse trees, CYK parsing, tries, pushdown automata, regular expressions, formal language theory, some semantics, etc.</p>\n\n<p>Here are some ideas I've come up with. Do you have anything you think would be cool?</p>\n\n<ol>\n<li><p>A script that scans Facebook threads for obnoxious* comments and silently hides them with JS (this would be run with the user's consent, obviously)</p></li>\n<li><p>An analysis of a piece of writing using semantics, syntax, punctuation usage, and other metrics, to try to \"fingerprint\" the author. It could be used to determine if two works are likely written by the same author. Or, someone could put in a bunch of writing he's done over time, and get a sense of how his style has changed.</p></li>\n<li><p>A chat bot (less interesting/original)</p></li>\n</ol>\n\n<p>I may be permitted to use pre-existing libraries to do this. Do any exist for OCaml? Without a library/toolkit, the above three ideas are probably infeasible, unless I limit it to a very specific domain.</p>\n\n<p>Lower level ideas:</p>\n\n<ol>\n<li><p>Operations on finite state machines - minimizing, composing transducers, proving that an FSM is in a minimal possible state. I am very interested in graph theory, so any overlap with FSMs could be a good venue to explore. (What else can I do with FSMs?)</p></li>\n<li><p>Something cool with regex?</p></li>\n<li><p>Something cool with CYK?</p></li>\n</ol>\n\n<p>Does anyone else have any cool ideas?</p>\n\n<p>*obnoxious defined as having following certain patterns typical of junior high schoolers. The vagueness of this term is not an issue; for the credit I could define whatever I want and target that.</p>\n",
    "score": 17,
    "creation_date": 1259103244,
    "view_count": 12372,
    "answer_count": 9,
    "tags": "parsing;nlp;ocaml"
  },
  {
    "question_id": 34805790,
    "title": "How to avoid NLTK&#39;s sentence tokenizer splitting on abbreviations?",
    "body": "<p>I'm currently using NLTK for language processing, but I have encountered a problem of sentence tokenizing.</p>\n\n<p>Here's the problem:\nAssume I have a sentence: \"Fig. 2 shows a U.S.A. map.\"\nWhen I use punkt tokenizer, my code looks like this:</p>\n\n<pre><code>from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\npunkt_param = PunktParameters()\nabbreviation = ['U.S.A', 'fig']\npunkt_param.abbrev_types = set(abbreviation)\ntokenizer = PunktSentenceTokenizer(punkt_param)\ntokenizer.tokenize('Fig. 2 shows a U.S.A. map.')\n</code></pre>\n\n<p>It returns this:</p>\n\n<pre><code>['Fig. 2 shows a U.S.A.', 'map.']\n</code></pre>\n\n<p>The tokenizer can't detect the abbreviation \"U.S.A.\", but it worked on \"fig\".\nNow when I use the default tokenizer NLTK provides:</p>\n\n<pre><code>import nltk\nnltk.tokenize.sent_tokenize('Fig. 2 shows a U.S.A. map.')\n</code></pre>\n\n<p>This time I get:</p>\n\n<pre><code>['Fig.', '2 shows a U.S.A. map.']\n</code></pre>\n\n<p>It recognizes the more common \"U.S.A.\" but fails to see \"fig\"!</p>\n\n<p>How can I combine these two methods? I want to use default abbreviation choices as well as adding my own abbreviations.</p>\n",
    "score": 17,
    "creation_date": 1452841262,
    "view_count": 7657,
    "answer_count": 1,
    "tags": "python;nlp;nltk;tokenize"
  },
  {
    "question_id": 1286301,
    "title": "Using the Python NLTK (2.0b5) on the Google App Engine",
    "body": "<p>I have been trying to make the NLTK (Natural Language Toolkit) work on the Google App Engine.  The steps I followed are:</p>\n\n<ol>\n<li>Download the installer and run it (a .dmg file, as I am using a Mac).</li>\n<li>copy the nltk folder out of the python site-packages directory and place it as a sub-folder in my project folder.</li>\n<li>Create a python module in the folder that contains the nltk sub-folder and add the line: <code>from nltk.tokenize import *</code> </li>\n</ol>\n\n<p>Unfortunately, after launching it I get this error (note that this error is raised deep within NLTK and I'm seeing it for my system installation of python as opposed to the one that is in the sub-folder of the GAE project):</p>\n\n<pre><code> &lt;type 'exceptions.ImportError'&gt;: No module named nltk\nTraceback (most recent call last):\n  File \"/base/data/home/apps/xxxx/1.335654715894946084/main.py\", line 13, in &lt;module&gt;\n    from lingua import reducer\n  File \"/base/data/home/apps/xxxx/1.335654715894946084/lingua/reducer.py\", line 11, in &lt;module&gt;\n    from nltk.tokenizer import *\n  File \"/base/data/home/apps/xxxx/1.335654715894946084/lingua/nltk/__init__.py\", line 73, in &lt;module&gt;\n    from internals import config_java\n  File \"/base/data/home/apps/xxxx/1.335654715894946084/lingua/nltk/internals.py\", line 19, in &lt;module&gt;\n    from nltk import __file__\n</code></pre>\n\n<p>Note: this is how the error looks in the logs when uploaded to GAE.  If I run it locally I get the same error (except it seems to originate inside my site-packages instance of NLTK ... so no difference there).  And \"xxxx\" signifies the project name.</p>\n\n<p>So in summary:</p>\n\n<ul>\n<li>Is what I am trying to do even possible?  Will NLTK even run on the App Engine?</li>\n<li>Is there something I missed?  That is: copying \"nltk\" to the GAE project isn't enough?</li>\n</ul>\n\n<p><strong>EDIT: fixed typo and removed unnecessary step</strong></p>\n",
    "score": 17,
    "creation_date": 1250487415,
    "view_count": 6029,
    "answer_count": 4,
    "tags": "python;google-app-engine;nlp;nltk"
  },
  {
    "question_id": 2764116,
    "title": "tag generation from a small text content (such as tweets)",
    "body": "<p>I have already asked a <a href=\"https://stackoverflow.com/questions/2661778/tag-generation-from-a-text-content\">similar question</a> earlier but I have notcied that I have big constrain: I am working on small text sets suchs as user Tweets to generate tags(keywords).</p>\n\n<p>And it seems like the accepted suggestion ( point-wise mutual information algorithm) is meant to work on bigger documents.</p>\n\n<p>With this constrain(working on small set of texts), how can I generate tags ?</p>\n\n<p>Regards</p>\n",
    "score": 17,
    "creation_date": 1272964859,
    "view_count": 4797,
    "answer_count": 2,
    "tags": "twitter;nlp;text-extraction;nltk;text-analysis"
  },
  {
    "question_id": 11798389,
    "title": "What NLP tools to use to match phrases having similar meaning or semantics",
    "body": "<p>I am working on a project which requires me to match a phrase or keyword with a set of similar keywords. I need to perform semantic analysis for the same. </p>\n\n<p>an example:</p>\n\n<p>Relevant QT<br>\ncheap health insurance<br>\naffordable health insurance<br>\nlow cost medical insurance<br>\nhealth plan for less<br>\ninexpensive health coverage</p>\n\n<p>Common Meaning</p>\n\n<p>low cost health insurance</p>\n\n<p>Here the the word under Common Meaning column should match the under Relevant QT column. I looked at a bunch of tools and techniques to do the same. S-Match seemed very promising, but I have to work in Python, not in Java. Also Latent Semantic Analysis looks good but I think its more for document classification based upon a Keyword rather than keyword matching. I am somewhat familiar with NLTK. Could someone provide some insight on what direction I should proceed and what tools I should use for the same?</p>\n",
    "score": 17,
    "creation_date": 1344006553,
    "view_count": 18792,
    "answer_count": 3,
    "tags": "python;nlp;nltk;latent-semantic-indexing"
  },
  {
    "question_id": 32458269,
    "title": "How does word2vec or skip-gram model convert words to vector?",
    "body": "<p>I have been reading a lot of papers on NLP, and came across many models. I got the SVD Model and representing it in 2-D, but I still did not get how do we make a word vector by giving a corpus to the word2vec/skip-gram model? Is it also co-occurrence matrix representation for each word? Can you explain it by taking an example corpus:</p>\n\n<pre><code>Hello, my name is John.\nJohn works in Google.\nGoogle has the best search engine. \n</code></pre>\n\n<p>Basically, how does skip gram convert <code>John</code> to a vector?</p>\n",
    "score": 17,
    "creation_date": 1441716406,
    "view_count": 7803,
    "answer_count": 3,
    "tags": "nlp;word2vec"
  },
  {
    "question_id": 30017491,
    "title": "Problems obtaining most informative features with scikit learn?",
    "body": "<p>Im triying to obtain the most informative features from a <a href=\"http://pastebin.com/3qYc9mfZ\" rel=\"noreferrer\">textual corpus</a>. From this well answered <a href=\"https://stackoverflow.com/questions/26976362/how-to-get-most-informative-features-for-scikit-learn-classifier-for-different-c\">question</a> I know that this task could be done as follows:</p>\n\n<pre><code>def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n    labelid = list(classifier.classes_).index(classlabel)\n    feature_names = vectorizer.get_feature_names()\n    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print classlabel, feat, coef\n</code></pre>\n\n<p>Then:</p>\n\n<pre><code>most_informative_feature_for_class(tfidf_vect, clf, 5)\n</code></pre>\n\n<p>For this classfier:</p>\n\n<pre><code>X = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n                                                    y, test_size=0.33)\nclf = SVC(kernel='linear', C=1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n</code></pre>\n\n<p>The problem is the output of <code>most_informative_feature_for_class</code>:</p>\n\n<pre><code>5 a_base_de_bien bastante   (0, 2451)   -0.210683496368\n  (0, 3533) -0.173621065386\n  (0, 8034) -0.135543062425\n  (0, 10346)    -0.173621065386\n  (0, 15231)    -0.154148294738\n  (0, 18261)    -0.158890483047\n  (0, 21083)    -0.297476572586\n  (0, 434)  -0.0596263855375\n  (0, 446)  -0.0753492277856\n  (0, 769)  -0.0753492277856\n  (0, 1118) -0.0753492277856\n  (0, 1439) -0.0753492277856\n  (0, 1605) -0.0753492277856\n  (0, 1755) -0.0637950312345\n  (0, 3504) -0.0753492277856\n  (0, 3511) -0.115802483001\n  (0, 4382) -0.0668983049212\n  (0, 5247) -0.315713152154\n  (0, 5396) -0.0753492277856\n  (0, 5753) -0.0716096348446\n  (0, 6507) -0.130661516772\n  (0, 7978) -0.0753492277856\n  (0, 8296) -0.144739048504\n  (0, 8740) -0.0753492277856\n  (0, 8906) -0.0753492277856\n  : :\n  (0, 23282)    0.418623443832\n  (0, 4100) 0.385906085143\n  (0, 15735)    0.207958503155\n  (0, 16620)    0.385906085143\n  (0, 19974)    0.0936828782325\n  (0, 20304)    0.385906085143\n  (0, 21721)    0.385906085143\n  (0, 22308)    0.301270427482\n  (0, 14903)    0.314164150621\n  (0, 16904)    0.0653764031957\n  (0, 20805)    0.0597723455204\n  (0, 21878)    0.403750815828\n  (0, 22582)    0.0226150073272\n  (0, 6532) 0.525138162099\n  (0, 6670) 0.525138162099\n  (0, 10341)    0.525138162099\n  (0, 13627)    0.278332617058\n  (0, 1600) 0.326774799211\n  (0, 2074) 0.310556919237\n  (0, 5262) 0.176400451433\n  (0, 6373) 0.290124806858\n  (0, 8593) 0.290124806858\n  (0, 12002)    0.282832270298\n  (0, 15008)    0.290124806858\n  (0, 19207)    0.326774799211\n</code></pre>\n\n<p>It is not returning the label nor the words. Why this is happening and how can I print the words and the labels?. Do you guys this is happening since I am using pandas to read the data?. Another thing I tried is the following, form this <a href=\"https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers\">question</a>:</p>\n\n<pre><code>def print_top10(vectorizer, clf, class_labels):\n    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n    feature_names = vectorizer.get_feature_names()\n    for i, class_label in enumerate(class_labels):\n        top10 = np.argsort(clf.coef_[i])[-10:]\n        print(\"%s: %s\" % (class_label,\n              \" \".join(feature_names[j] for j in top10)))\n\n\nprint_top10(tfidf_vect,clf,y)\n</code></pre>\n\n<p>But I get this traceback:</p>\n\n<p>Traceback (most recent call last):</p>\n\n<pre><code>  File \"/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py\", line 237, in &lt;module&gt;\n    print_top10(tfidf_vect,clf,5)\n  File \"/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py\", line 231, in print_top10\n    for i, class_label in enumerate(class_labels):\nTypeError: 'int' object is not iterable\n</code></pre>\n\n<p>Any idea of how to solve this, in order to get the features with the highest coefficient values?.</p>\n",
    "score": 17,
    "creation_date": 1430676432,
    "view_count": 3537,
    "answer_count": 1,
    "tags": "python;pandas;machine-learning;nlp;scikit-learn"
  },
  {
    "question_id": 22370144,
    "title": "Can stop-words be found automatically?",
    "body": "<p>In NLP, stop-words removal is a typical pre-processing step. And it is typically done in an empirical way based on what we think stop-words should be.</p>\n\n<p>But in my opinion, we should generalize the concept of stop-words. And the stop-words could vary for corpora from different domains. I am wondering if we can define the stop-words mathematically, such as by its statistical characteristics. And then can we automatically extract stop-words from a corpora for a specific domain.</p>\n\n<p>Is there any similar thought and progress on this? Could anyone shed some light?</p>\n",
    "score": 17,
    "creation_date": 1394689930,
    "view_count": 4956,
    "answer_count": 5,
    "tags": "machine-learning;nlp;data-mining;text-mining"
  },
  {
    "question_id": 23735576,
    "title": "Gensim train word2vec on wikipedia - preprocessing and parameters",
    "body": "<p>I am trying to train the word2vec model from <code>gensim</code> using the Italian wikipedia\n\"<a href=\"http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2\" rel=\"nofollow noreferrer\">http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2</a>\"</p>\n\n<p>However, I am not sure what is the best preprocessing for this corpus.</p>\n\n<p><code>gensim</code> model accepts a list of tokenized sentences.\nMy first try is to just use the standard <code>WikipediaCorpus</code> preprocessor from <code>gensim</code>. This extract each article, remove punctuation and split words on spaces. With this tool each sentence would correspond to an entire model, and I am not sure of the impact of this fact on the model.</p>\n\n<p>After this I train the model with default parameters. Unfortunately after training it seems that I do not manage to obtain very meaningful similarities.</p>\n\n<p>What is the most appropriate preprocessing on the Wikipedia corpus for this task? (if this questions are too broad please help me by pointing to a relevant tutorial / article )</p>\n\n<p>This the code of my first trial:</p>\n\n<pre><code>from gensim.corpora import WikiCorpus\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\ncorpus = WikiCorpus('itwiki-latest-pages-articles.xml.bz2',dictionary=False)\nmax_sentence = -1\n\ndef generate_lines():\n    for index, text in enumerate(corpus.get_texts()):\n        if index &lt; max_sentence or max_sentence==-1:\n            yield text\n        else:\n            break\n\nfrom gensim.models.word2vec import BrownCorpus, Word2Vec\nmodel = Word2Vec() \nmodel.build_vocab(generate_lines()) #This strangely builds a vocab of \"only\" 747904 words which is &lt;&lt; than those reported in the literature 10M words\nmodel.train(generate_lines(),chunksize=500)\n</code></pre>\n",
    "score": 17,
    "creation_date": 1400495841,
    "view_count": 9432,
    "answer_count": 2,
    "tags": "nlp;gensim;word2vec"
  },
  {
    "question_id": 2832394,
    "title": "Sentiment analysis with NLTK python for sentences using sample data or webservice?",
    "body": "<p>I am embarking upon a NLP project for sentiment analysis.</p>\n\n<p>I have successfully installed NLTK for python (seems like a great piece of software for this). However,I am having trouble understanding how it can be used to accomplish my task.</p>\n\n<p>Here is my task:</p>\n\n<ol>\n<li>I start with one long piece of data (lets say several hundred tweets on the subject of the UK election from their webservice)</li>\n<li>I would like to break this up into sentences (or info no longer than 100 or so chars) (I guess i can just do this in python??)</li>\n<li>Then to search through all the sentences for specific instances within that sentence e.g. \"David Cameron\"</li>\n<li>Then I would like to check for positive/negative sentiment in each sentence and count them accordingly</li>\n</ol>\n\n<p>NB: I am not really worried too much about accuracy because my data sets are large and also not worried too much about sarcasm. </p>\n\n<p>Here are the troubles I am having:</p>\n\n<ol>\n<li><p>All the data sets I can find e.g. the corpus movie review data that comes with NLTK arent in webservice format. It looks like this has had some processing done already. As far as I can see the processing (by stanford) was done with WEKA. Is it not possible for NLTK to do all this on its own? Here all the data sets have already been organised into positive/negative already e.g. polarity dataset <a href=\"http://www.cs.cornell.edu/People/pabo/movie-review-data/\" rel=\"noreferrer\">http://www.cs.cornell.edu/People/pabo/movie-review-data/</a> How is this done? (to organise the sentences by sentiment, is it definitely WEKA? or something else?)</p></li>\n<li><p>I am not sure I understand why WEKA and NLTK would be used together. Seems like they do much the same thing. If im processing the data with WEKA first to find sentiment why would I need NLTK? Is it possible to explain why this might be necessary?</p></li>\n</ol>\n\n<p>I have found a few scripts that get somewhat near this task, but all are using the same pre-processed data. Is it not possible to process this data myself to find sentiment in sentences rather than using the data samples given in the link?</p>\n\n<p>Any help is much appreciated and will save me much hair!</p>\n\n<p>Cheers Ke</p>\n",
    "score": 17,
    "creation_date": 1273820659,
    "view_count": 17076,
    "answer_count": 2,
    "tags": "nlp;nltk;weka;classification"
  },
  {
    "question_id": 44291798,
    "title": "How to preprocess text for embedding?",
    "body": "<p>In the traditional \"one-hot\" representation of words as vectors you have a vector of the same dimension as the cardinality of your vocabulary. To reduce dimensionality usually stopwords are removed, as well as applying stemming, lemmatizing, etc. to normalize the features you want to perform some NLP task on.</p>\n\n<p>I'm having trouble understanding whether/how to preprocess text to be embedded (e.g. word2vec). My goal is to use these word embeddings as features for a NN to classify texts into topic A, not topic A, and then perform event extraction on them on documents of topic A (using a second NN).</p>\n\n<p>My first instinct is to preprocess removing stopwords, lemmatizing stemming, etc. But as I learn about NN a bit more I realize that applied to natural language, the CBOW and skip-gram models would in fact require the whole set of words to be present --to be able to predict a word from context one would need to know the actual context, not a reduced form of the context after normalizing... right?). The actual sequence of POS tags seems to be key for a human-feeling prediction of words.</p>\n\n<p>I've found <a href=\"https://groups.google.com/forum/#!topic/word2vec-toolkit/TI-TQC-b53w\" rel=\"noreferrer\">some guidance online</a> but I'm still curious to know what the community here thinks:</p>\n\n<ol>\n<li>Are there any recent commonly accepted best practices regarding punctuation, stemming, lemmatizing, stopwords, numbers, lowercase etc?</li>\n<li>If so, what are they? Is it better in general to process as little as possible, or more on the heavier side to normalize the text? Is there a trade-off?</li>\n</ol>\n\n<p>My thoughts: </p>\n\n<p>It is better to remove punctuation (but e.g. in Spanish don't remove the accents because the do convey contextual information), change written numbers to numeric, do not lowercase everything (useful for entity extraction), no stemming, no lemmatizing. </p>\n\n<p>Does this sound right?</p>\n",
    "score": 17,
    "creation_date": 1496253817,
    "view_count": 8896,
    "answer_count": 3,
    "tags": "neural-network;nlp"
  },
  {
    "question_id": 53598243,
    "title": "Is there a bi gram or tri gram feature in Spacy?",
    "body": "<p>The below code breaks the sentence into individual tokens and the output is as below </p>\n\n<pre><code> \"cloud\"  \"computing\"  \"is\" \"benefiting\"  \" major\"  \"manufacturing\"  \"companies\"\n\n\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\ndoc = nlp(\"Cloud computing is benefiting major manufacturing companies\")\nfor token in doc:\n    print(token.text)\n</code></pre>\n\n<p>What I would ideally want is, to read 'cloud computing' together as it is technically one word. </p>\n\n<p>Basically I am looking for a bi gram. Is there any feature in Spacy that allows Bi gram or Tri grams ?</p>\n",
    "score": 16,
    "creation_date": 1543855856,
    "view_count": 23914,
    "answer_count": 4,
    "tags": "python-3.x;nlp;tokenize;spacy;n-gram"
  },
  {
    "question_id": 6462709,
    "title": "nltk language model (ngram) calculate the prob of a word from context",
    "body": "<p>I am using Python and NLTK to build a language model as follows:</p>\n\n\n\n<pre class=\"lang-py prettyprint-override\"><code>from nltk.corpus import brown\nfrom nltk.probability import LidstoneProbDist, WittenBellProbDist\nestimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)\nlm = NgramModel(3, brown.words(categories='news'), estimator)\n# Thanks to miku, I fixed this problem\nprint lm.prob(\"word\", [\"This is a context which generates a word\"])\n&gt;&gt; 0.00493261081006\n# But I got another program like this one...\nprint lm.prob(\"b\", [\"This is a context which generates a word\"]) \n</code></pre>\n\n<p>But it doesn't seem to work. The result is as follows:</p>\n\n<pre><code>&gt;&gt;&gt; print lm.prob(\"word\", \"This is a context which generates a word\")\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/local/lib/python2.6/dist-packages/nltk/model/ngram.py\", line 79, in prob\n    return self._alpha(context) * self._backoff.prob(word, context[1:])\n  File \"/usr/local/lib/python2.6/dist-packages/nltk/model/ngram.py\", line 79, in prob\n    return self._alpha(context) * self._backoff.prob(word, context[1:])\n  File \"/usr/local/lib/python2.6/dist-packages/nltk/model/ngram.py\", line 82, in prob\n    \"context %s\" % (word, ' '.join(context)))\nTypeError: not all arguments converted during string formatting\n</code></pre>\n\n<p>Can anyone help me out? Thanks!</p>\n",
    "score": 16,
    "creation_date": 1308882528,
    "view_count": 22743,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 43913983,
    "title": "Using Wordnet Synsets from Python for Italian Language",
    "body": "<p>I'm starting to program with NLTK in Python for Natural Italian Language processing. I've seen some simple examples of the WordNet Library that has a nice set of SynSet that permits you to navigate from a word (for example: \"dog\") to his synonyms and his antonyms, his hyponyms and hypernyms and so on...</p>\n\n<p>My question is: \nIf I start with an italian word (for example:\"cane\" - that means \"dog\") is there a way to navigate between synonyms, antonyms, hyponyms... for the italian word as you do for the english one? Or... There is an Equivalent to WordNet for the Italian Language ?</p>\n\n<p>Thanks in advance</p>\n",
    "score": 16,
    "creation_date": 1494501729,
    "view_count": 13521,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 43878332,
    "title": "Gensim saved dictionary has no id2token",
    "body": "<p>I have saved a Gensim dictionary to disk. When I load it, the <code>id2token</code> attribute dict is not populated.</p>\n\n<p>A simple piece of the code that saves the dictionary:</p>\n\n<pre><code>dictionary = corpora.Dictionary(tag_docs)\ndictionary.save(\"tag_dictionary_lda.pkl\")\n</code></pre>\n\n<p>Now when I load it (I'm loading it in an jupyter notebook), it still works fine for mapping tokens to IDs, but <code>id2token</code> does not work (I cannot map IDs to tokens) and in fact <code>id2token</code> is not populated at all.</p>\n\n<pre><code>&gt; dictionary = corpora.Dictionary.load(\"../data/tag_dictionary_lda.pkl\")\n&gt; dictionary.token2id[\"love\"]\nOut: 1613\n\n&gt; dictionary.doc2bow([\"love\"])\nOut: [(1613, 1)]\n\n&gt; dictionary.id2token[1613]\nOut: \n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input&gt; in &lt;module&gt;()\n----&gt; 1 dictionary.id2token[1613]\n\nKeyError: 1613\n\n&gt; list(dictionary.id2token.keys())\nOut: []\n</code></pre>\n\n<p>Any thoughts? </p>\n",
    "score": 16,
    "creation_date": 1494358019,
    "view_count": 9528,
    "answer_count": 1,
    "tags": "python;nlp;gensim"
  },
  {
    "question_id": 18439795,
    "title": "NLP/Machine Learning text comparison",
    "body": "<p>I'm currently in the process of developing a program with the capability of comparing a small text (say 250 characters) to a collection of similar texts (around 1000-2000 texts).</p>\n\n<p>The purpose is to evalute if text A is similar to one or more texts in the collection and if so, the text in the collection has to be retrievable by ID. Each texts will have a unique ID.</p>\n\n<p>There is two ways I'd like the output to be:</p>\n\n<p><strong>Option 1:</strong>\nText A matched Text B with 90% similarity, Text C with 70% similarity, and so on.</p>\n\n<p><strong>Option 2:</strong>\nText A matched Text D with highest similarity</p>\n\n<p>I have read some machine learning in school but I'm not sure which algorithm suits this problem the best or if I should consider using NLP (not familiar with the subject).</p>\n\n<p>Does anyone have a suggestion of what algorithm to use or where I can find the nessecary literature to solve my problem?</p>\n",
    "score": 16,
    "creation_date": 1377505727,
    "view_count": 26524,
    "answer_count": 2,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 26070245,
    "title": "Clause Extraction using Stanford parser",
    "body": "<p>I have a complex sentence and I  need to separate it into main and dependent clause.\nFor example for the sentence<br>\nABC cites the fact that chemical additives are banned in many countries and feels they may be banned in this state too.<br>\nThe split required</p>\n\n<pre><code>1)ABC cites the fact   \n2)chemical additives are banned in many countries   \n3)ABC feels they may be banned in this state too.    \n</code></pre>\n\n<p>I think I could use the Stanford Parser tree or dependencies, but I am not  sure how to proceed from here.  </p>\n\n<p>The tree </p>\n\n<pre>\n(ROOT\n  (S\n    (NP (NNP ABC))\n    (VP (VBZ cites)\n      (NP (DT the) (NN fact))\n      (SBAR (IN that)\n        (S\n          (NP (NN chemical) (NNS additives))\n          (VP\n            (VP (VBP are)\n              (VP (VBN banned)\n                (PP (IN in)\n                  (NP (JJ many) (NNS countries)))))\n            (CC and)\n            (VP (VBZ feels)\n              (SBAR\n                (S\n                  (NP (PRP they))\n                  (VP (MD may)\n                    (VP (VB be)\n                      (VP (VBN banned)\n                        (PP (IN in)\n                          (NP (DT this) (NN state)))\n                        (ADVP (RB too))))))))))))\n    (. .)))\n</pre>\n\n<p>and the collapsed dependency parse  </p>\n\n<pre>\nnsubj(cites-2, ABC-1)  \nroot(ROOT-0, cites-2)  \ndet(fact-4, the-3)   \ndobj(cites-2, fact-4)  \nmark(banned-9, that-5)  \nnn(additives-7, chemical-6)  \nnsubjpass(banned-9, additives-7)   \nnsubj(feels-14, additives-7)   \nauxpass(banned-9, are-8)   \nccomp(cites-2, banned-9)   \namod(countries-12, many-11)  \nprep_in(banned-9, countries-12)   \nccomp(cites-2, feels-14)    \nconj_and(banned-9, feels-14)    \nnsubjpass(banned-18, they-15)   \naux(banned-18, may-16)    \nauxpass(banned-18, be-17)    \nccomp(feels-14, banned-18)   \ndet(state-21, this-20)    \nprep_in(banned-18, state-21)    \nadvmod(banned-18, too-22)   \n</pre>\n",
    "score": 16,
    "creation_date": 1411779251,
    "view_count": 11346,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 4757947,
    "title": "What is a chunker in Natural Language Processing?",
    "body": "<p>Does anyone know what is a chunker in the context of text processing and what is it's usage?</p>\n",
    "score": 16,
    "creation_date": 1295607261,
    "view_count": 13838,
    "answer_count": 3,
    "tags": "nlp;chunking"
  },
  {
    "question_id": 57882417,
    "title": "Is it possible to use Google BERT to calculate similarity between two textual documents?",
    "body": "<p>Is it possible to use Google BERT for calculating similarity between two textual documents? As I understand BERT's input is supposed to be a limited size sentences. Some works use BERT for similarity calculation for sentences like:</p>\n\n<p><a href=\"https://github.com/AndriyMulyar/semantic-text-similarity\" rel=\"noreferrer\">https://github.com/AndriyMulyar/semantic-text-similarity</a></p>\n\n<p><a href=\"https://github.com/beekbin/bert-cosine-sim\" rel=\"noreferrer\">https://github.com/beekbin/bert-cosine-sim</a></p>\n\n<p>Is there an implementation of BERT done to use it for large documents instead of sentences as inputs ( Documents with thousands of words)?</p>\n",
    "score": 16,
    "creation_date": 1568178198,
    "view_count": 22651,
    "answer_count": 5,
    "tags": "python;text;scikit-learn;nlp;word-embedding"
  },
  {
    "question_id": 20075335,
    "title": "Is wordnet path similarity commutative?",
    "body": "<p>I am using the wordnet API from nltk.\nWhen I compare one synset with another I got <code>None</code> but when I compare them the other way around I get a float value.</p>\n\n<p>Shouldn't they give the same value?\nIs there an explanation or is this a bug of wordnet?</p>\n\n<p>Example:</p>\n\n<pre><code>wn.synset('car.n.01').path_similarity(wn.synset('automobile.v.01')) # None\nwn.synset('automobile.v.01').path_similarity(wn.synset('car.n.01')) # 0.06666666666666667\n</code></pre>\n",
    "score": 16,
    "creation_date": 1384874386,
    "view_count": 5076,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 71581197,
    "title": "What is the loss function used in Trainer from the Transformers library of Hugging Face?",
    "body": "<p>What is the loss function used in Trainer from the Transformers library of Hugging Face?</p>\n<p>I am trying to fine tune a BERT model using the <strong>Trainer class</strong> from the Transformers library of Hugging Face.</p>\n<p>In their <a href=\"https://huggingface.co/docs/transformers/main_classes/trainer\" rel=\"nofollow noreferrer\">documentation</a>, they mention that one can specify a customized loss function by overriding the <code>compute_loss</code> method in the class. However, if I do not do the method override and use the Trainer to fine tine a BERT model directly for sentiment classification, what is the default loss function being use? Is it the categorical crossentropy? Thanks!</p>\n",
    "score": 16,
    "creation_date": 1648002951,
    "view_count": 28028,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;artificial-intelligence;huggingface-transformers"
  },
  {
    "question_id": 6572207,
    "title": "Stanford Core NLP - understanding coreference resolution",
    "body": "<p>I'm having some trouble understanding the changes made to the coref resolver in the last version of the Stanford NLP tools.\nAs an example, below is a sentence and the corresponding CorefChainAnnotation:</p>\n\n<pre><code>The atom is a basic unit of matter, it consists of a dense central nucleus surrounded by a cloud of negatively charged electrons.\n\n{1=[1 1, 1 2], 5=[1 3], 7=[1 4], 9=[1 5]}\n</code></pre>\n\n<p>I am not sure I understand the meaning of these numbers. Looking at the source doesn't really help either.</p>\n\n<p>Thank you</p>\n",
    "score": 16,
    "creation_date": 1309786340,
    "view_count": 10648,
    "answer_count": 3,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 5032210,
    "title": "php sentence boundaries detection",
    "body": "<p>I would like to divide a text into sentences in PHP. I'm currently using a regex, which brings ~95% accuracy and would like to improve by using a better approach. I've seen NLP tools that do that in Perl, Java, and C but didn't see anything that fits PHP. Do you know of such a tool? </p>\n",
    "score": 16,
    "creation_date": 1297962899,
    "view_count": 8664,
    "answer_count": 6,
    "tags": "php;regex;nlp;text-segmentation"
  },
  {
    "question_id": 75549632,
    "title": "Difference between AutoModelForSeq2SeqLM and AutoModelForCausalLM",
    "body": "<p>As per the title, how are these two Auto Classes on Huggingface different from each other? I tried reading the documentation but did not find differentiating information</p>\n",
    "score": 16,
    "creation_date": 1677181550,
    "view_count": 24245,
    "answer_count": 1,
    "tags": "machine-learning;nlp;huggingface-transformers"
  },
  {
    "question_id": 51012476,
    "title": "Spacy custom tokenizer to include only hyphen words as tokens using Infix regex",
    "body": "<p>I want to include hyphenated words for example: <em>long-term, self-esteem,</em> etc. as a single token in Spacy. After looking at some similar posts on StackOverflow, <a href=\"https://github.com/explosion/spaCy/issues/1504\" rel=\"noreferrer\">Github</a>, its <a href=\"https://spacy.io/usage/linguistic-features#tokenization\" rel=\"noreferrer\">documentation</a> and <a href=\"http://www.longest.io/2018/01/27/spacy-custom-tokenization.html\" rel=\"noreferrer\">elsewhere</a>, I also wrote a custom tokenizer as below:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import re\nfrom spacy.tokenizer import Tokenizer\n\nprefix_re = re.compile(r'''^[\\[\\(\"']''')\nsuffix_re = re.compile(r'''[\\]\\)\"']$''')\ninfix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n\ndef custom_tokenizer(nlp):\n    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n                                suffix_search=suffix_re.search,\n                                infix_finditer=infix_re.finditer,\n                                token_match=None)\n\nnlp = spacy.load('en_core_web_lg')\nnlp.tokenizer = custom_tokenizer(nlp)\n\ndoc = nlp(u'Note: Since the fourteenth century the practice of “medicine” has become a profession; and more importantly, it\\'s a male-dominated profession.')\n[token.text for token in doc]\n</code></pre>\n\n<p>So for this sentence: \n<em>'Note: Since the fourteenth century the practice of “medicine” has become a profession; and more importantly, it\\'s a male-dominated profession.'</em></p>\n\n<p><em>Now, the tokens after incorporating the custom Spacy Tokenizer are:</em> </p>\n\n<p>'Note', ':', 'Since', 'the', 'fourteenth', 'century', 'the', 'practice', 'of',\n<strong>'“medicine',</strong>  '<strong>”</strong>', 'has', ';', 'become', 'a',\n'profession', ',', 'and', 'more', 'importantly', ',', \n<strong>\"it's\",</strong> 'a', '<strong>male-dominated</strong>', 'profession', '.'</p>\n\n<p><em>Earlier, the tokens before this change were:</em> </p>\n\n<p>'Note',  ':',  'Since',  'the',  'fourteenth',  'century',  'the',  'practice',  'of',  '<strong>“</strong>',  '<strong>medicine</strong>',  '<strong>”</strong>',  'has', 'become',  'a',  'profession',  ';',  'and',  'more',  'importantly',  ',',  '<strong>it</strong>',  \"<strong>'s</strong>\",  'a',  '<strong>male</strong>',  '<strong>-</strong>',  '<strong>dominated</strong>',  'profession',  '.'</p>\n\n<p><em>And, the expected tokens should be:</em></p>\n\n<p>'Note',  ':',  'Since',  'the',  'fourteenth',  'century',  'the',  'practice',  'of',  '<strong>“</strong>',  '<strong>medicine</strong>',  '<strong>”</strong>',  'has', 'become',  'a',  'profession',  ';',  'and',  'more',  'importantly',  ',',  '<strong>it</strong>',  \"<strong>'s</strong>\",  'a',  '<strong>male-dominated</strong>',  'profession',  '.'</p>\n\n<p><strong>Summary:</strong> As one can see...</p>\n\n<ul>\n<li><strong>the hyphen word is included and so are the other punctuation marks except for the double quotes and apostrophe...</strong></li>\n<li>...but now, the <strong>apostrophe and double quotes don't have the earlier or expected behaviour.</strong></li>\n<li>I have tried different permutations and combinations for the regex compile for the Infix but no progress to fix this issue.</li>\n</ul>\n",
    "score": 16,
    "creation_date": 1529862300,
    "view_count": 11040,
    "answer_count": 1,
    "tags": "regex;nlp;tokenize;spacy;linguistics"
  },
  {
    "question_id": 22507623,
    "title": "Existing API for NLP in C++?",
    "body": "<p>Is/are there existing C++ NLP API(s) out there? The closest thing I have found is <code>CLucene</code>, a port of <code>Lucene</code>. However, it seems a bit obsolete and the documentation is far from complete.</p>\n\n<p>Ideally, this/these API(s) would permit tokenization, stemming and PoS tagging.</p>\n",
    "score": 16,
    "creation_date": 1395236163,
    "view_count": 16039,
    "answer_count": 6,
    "tags": "c++;api;nlp;pos-tagger"
  },
  {
    "question_id": 54924582,
    "title": "Is it possible to freeze only certain embedding weights in the embedding layer in pytorch?",
    "body": "<p>When using GloVe embedding in NLP tasks, some words from the dataset might not exist in GloVe. Therefore, we instantiate random weights for these unknown words.</p>\n\n<p>Would it be possible to freeze weights gotten from GloVe, and train only the newly instantiated weights?</p>\n\n<p>I am only aware that we can set:\nmodel.embedding.weight.requires_grad = False</p>\n\n<p>But this makes the new words untrainable..</p>\n\n<p>Or are there better ways to extract semantics of words.. </p>\n",
    "score": 16,
    "creation_date": 1551353019,
    "view_count": 8927,
    "answer_count": 1,
    "tags": "python;nlp;pytorch;word-embedding;glove"
  },
  {
    "question_id": 28314337,
    "title": "TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0] while using RF classifier?",
    "body": "<p>I am learning about random forests in scikit learn and as an example I would like to use Random forest classifier for text classification, with my own dataset. So first I vectorized the text with tfidf and for classification:</p>\n\n<pre><code>from sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier(n_estimators=10) \nclassifier.fit(X_train, y_train)           \nprediction = classifier.predict(X_test)\n</code></pre>\n\n<p>When I run the classification I got this:</p>\n\n<pre><code>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n</code></pre>\n\n<p>then I used the <code>.toarray()</code> for <code>X_train</code> and I got the following:</p>\n\n<pre><code>TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\n</code></pre>\n\n<p>From a previous <a href=\"https://stackoverflow.com/questions/21689141/classifying-text-documents-with-random-forests\">question</a> as I understood I need to reduce the dimensionality of the numpy array so I do the same:</p>\n\n<pre><code>from sklearn.decomposition.truncated_svd import TruncatedSVD        \npca = TruncatedSVD(n_components=300)                                \nX_reduced_train = pca.fit_transform(X_train)               \n\nfrom sklearn.ensemble import RandomForestClassifier                 \nclassifier=RandomForestClassifier(n_estimators=10)                  \nclassifier.fit(X_reduced_train, y_train)                            \nprediction = classifier.predict(X_testing) \n</code></pre>\n\n<p>Then I got this exception:</p>\n\n<pre><code>  File \"/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py\", line 419, in predict\n    n_samples = len(X)\n  File \"/usr/local/lib/python2.7/site-packages/scipy/sparse/base.py\", line 192, in __len__\n    raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\nTypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\n</code></pre>\n\n<p>The I tried the following:</p>\n\n<pre><code>prediction = classifier.predict(X_train.getnnz()) \n</code></pre>\n\n<p>And got this:</p>\n\n<pre><code>  File \"/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py\", line 419, in predict\n    n_samples = len(X)\nTypeError: object of type 'int' has no len()\n</code></pre>\n\n<p>Two questions were raised from this: How can I use Random forests to classify correctly? and what's happening with <code>X_train</code>?. </p>\n\n<p>Then I tried the following:</p>\n\n<pre><code>df = pd.read_csv('/path/file.csv',\nheader=0, sep=',', names=['id', 'text', 'label'])\n\n\n\nX = tfidf_vect.fit_transform(df['text'].values)\ny = df['label'].values\n\n\n\nfrom sklearn.decomposition.truncated_svd import TruncatedSVD\npca = TruncatedSVD(n_components=2)\nX = pca.fit_transform(X)\n\na_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifier=RandomForestClassifier(n_estimators=10)\nclassifier.fit(a_train, b_train)\nprediction = classifier.predict(a_test)\n\nfrom sklearn.metrics.metrics import precision_score, recall_score, confusion_matrix, classification_report\nprint '\\nscore:', classifier.score(a_train, b_test)\nprint '\\nprecision:', precision_score(b_test, prediction)\nprint '\\nrecall:', recall_score(b_test, prediction)\nprint '\\n confussion matrix:\\n',confusion_matrix(b_test, prediction)\nprint '\\n clasification report:\\n', classification_report(b_test, prediction)\n</code></pre>\n",
    "score": 16,
    "creation_date": 1423028915,
    "view_count": 56903,
    "answer_count": 3,
    "tags": "python;numpy;machine-learning;nlp;scikit-learn"
  },
  {
    "question_id": 20362993,
    "title": "How to load sentences into Python gensim?",
    "body": "<p>I am trying to use the <a href=\"http://radimrehurek.com/gensim/models/word2vec.html\"><code>word2vec</code></a> module from <code>gensim</code> natural language processing library in Python.</p>\n\n<p>The docs say to initialize the model:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from gensim.models import word2vec\nmodel = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n</code></pre>\n\n<p>What format does <code>gensim</code> expect for the input sentences?  I have raw text</p>\n\n<pre><code>\"the quick brown fox jumps over the lazy dogs\"\n\"Then a cop quizzed Mick Jagger's ex-wives briefly.\"\netc.\n</code></pre>\n\n<p>What additional processing do I need to post into <code>word2fec</code>?</p>\n\n<hr>\n\n<p><strong>UPDATE:</strong> Here is what I have tried.  When it loads the sentences, I get nothing.</p>\n\n<pre><code>&gt;&gt;&gt; sentences = ['the quick brown fox jumps over the lazy dogs',\n             \"Then a cop quizzed Mick Jagger's ex-wives briefly.\"]\n&gt;&gt;&gt; x = word2vec.Word2Vec()\n&gt;&gt;&gt; x.build_vocab([s.encode('utf-8').split( ) for s in sentences])\n&gt;&gt;&gt; x.vocab\n{}\n</code></pre>\n",
    "score": 16,
    "creation_date": 1386109556,
    "view_count": 13993,
    "answer_count": 2,
    "tags": "python;nlp;gensim"
  },
  {
    "question_id": 21883108,
    "title": "Fast/Optimize N-gram implementations in python",
    "body": "<p>Which ngram implementation is fastest in python?</p>\n\n<p>I've tried to profile nltk's vs scott's zip (<a href=\"http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\">http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/</a>):</p>\n\n<pre><code>from nltk.util import ngrams as nltkngram\nimport this, time\n\ndef zipngram(text,n=2):\n  return zip(*[text.split()[i:] for i in range(n)])\n\ntext = this.s\n\nstart = time.time()\nnltkngram(text.split(), n=2)\nprint time.time() - start\n\nstart = time.time()\nzipngram(text, n=2)\nprint time.time() - start\n</code></pre>\n\n<p><strong>[out]</strong></p>\n\n<pre><code>0.000213146209717\n6.50882720947e-05\n</code></pre>\n\n<p>Is there any faster implementation for generating ngrams in python?</p>\n",
    "score": 16,
    "creation_date": 1392819361,
    "view_count": 8359,
    "answer_count": 3,
    "tags": "python;nlp;nltk;information-retrieval;n-gram"
  },
  {
    "question_id": 20727552,
    "title": "Abbreviation detection",
    "body": "<p>Under what field of study under natural language processing does abbreviation detection come? Looking for sources to learn abbreviation detection. I have considered Semantics, which basically detect synonyms. so i thought i might do multi-word semantics that would detect that \"nlp\" and \"natural language processing\" are similar. but i have found NO solution to do multi-word semantics. </p>\n\n<p><strong>Note:</strong> I know its really easy to down vote this question, but try to understand my problem. I have struggled for months now and any help is GREATLY appreciated...</p>\n\n<p>Thankyou</p>\n",
    "score": 16,
    "creation_date": 1387700415,
    "view_count": 12546,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 13826331,
    "title": "How to split a Thai sentence, which does not use spaces, into words?",
    "body": "<p>How to split word from Thai sentence? English we can split word by space.</p>\n\n<p>Example: <code>I go to school</code>, split = <code>['I', 'go', 'to' ,'school']</code>  Split by looking only space.</p>\n\n<p>But Thai language had no space, so I don't know how to do.\nExample spit ฉันจะไปโรงเรียน to from txt file to ['ฉัน' 'จะ' 'ไป' 'โรง' 'เรียน'] = output another txt file.</p>\n\n<p>Are there any programs or libraries that identify Thai word boundaries and split?</p>\n",
    "score": 16,
    "creation_date": 1355250882,
    "view_count": 8613,
    "answer_count": 5,
    "tags": "string;parsing;split;nlp"
  },
  {
    "question_id": 41162876,
    "title": "Get weight matrices from gensim word2Vec",
    "body": "<p>I am using gensim word2vec package in python.\nI would like to retrieve the <code>W</code> and <code>W'</code> weight matrices that have been learn during the skip-gram learning.</p>\n<p>It seems to me that <code>model.syn0</code> gives me the first one but I am not sure how I can get the other one. Any idea?</p>\n<p>I would actually love to find any exhaustive documentation on models accessible attributes because the official one does not seem to be precise (for instance <code>syn0</code> is not described as an attribute)</p>\n",
    "score": 16,
    "creation_date": 1481800751,
    "view_count": 9095,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;word2vec;gensim"
  },
  {
    "question_id": 7857648,
    "title": "Sentence compression using NLP",
    "body": "<p>Using Machine translation, can I obtain a very compressed version of a sentence,\neg. <strong>I would really like to have a delicious tasty cup of coffee</strong> would be translated to <strong>I want coffee</strong>\nDoes any of the NLP engines provide such a functionality?</p>\n\n<p>I got a few research papers that does <a href=\"http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli_a_00002\" rel=\"noreferrer\">paraphase generation</a> and <a href=\"http://www.cs.jhu.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf\" rel=\"noreferrer\">sentence compression</a>. But is there any library which has already implemented this?</p>\n",
    "score": 16,
    "creation_date": 1319261840,
    "view_count": 4320,
    "answer_count": 4,
    "tags": "nlp;nltk;stanford-nlp;opennlp"
  },
  {
    "question_id": 3125926,
    "title": "Does NLTK have a tool for dependency parsing?",
    "body": "<p>I'm building a NLP application and have been using the Stanford Parser for most of my parsing work, but I would like to start using Python.</p>\n\n<p>So far, NLTK seems like the best bet, but I cannot figure out how to parse grammatical dependencies.  I.e. this is an example from the Stanford Parser.  I want to be able to produce this in NTLK using Python from the original sentence \"I am switching to Python.\":</p>\n\n<pre><code>nsubj(switching-3, I-1)\naux(switching-3, am-2)\nprep_to(switching-3, Python-5)\n</code></pre>\n\n<p>Can anyone give me a shove in the right direction to parse grammatical dependencies?</p>\n",
    "score": 16,
    "creation_date": 1277597518,
    "view_count": 5257,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 26341518,
    "title": "Effective 1-5 grams extraction with python",
    "body": "<p>I have a huge files of 3,000,000 lines and each line have 20-40 words. I have to extract 1 to 5 ngrams from the corpus. My input files are tokenized plain text, e.g.:</p>\n\n<pre><code>This is a foo bar sentence .\nThere is a comma , in this sentence .\nSuch is an example text .\n</code></pre>\n\n<p>Currently, I am doing it as below but this don't seem to be a efficient way to extract the 1-5grams:</p>\n\n<pre><code>#!/usr/bin/env python -*- coding: utf-8 -*-\n\nimport io, os\nfrom collections import Counter\nimport sys; reload(sys); sys.setdefaultencoding('utf-8')\n\nwith io.open('train-1.tok.en', 'r', encoding='utf8') as srcfin, \\\nio.open('train-1.tok.jp', 'r', encoding='utf8') as trgfin:\n    # Extract words from file. \n    src_words = ['&lt;s&gt;'] + srcfin.read().replace('\\n', ' &lt;/s&gt; &lt;s&gt; ').split()\n    del src_words[-1] # Removes the final '&lt;s&gt;'\n    trg_words = ['&lt;s&gt;'] + trgfin.read().replace('\\n', ' &lt;/s&gt; &lt;s&gt; ').split()\n    del trg_words[-1] # Removes the final '&lt;s&gt;'\n\n    # Unigrams count.\n    src_unigrams = Counter(src_words) \n    trg_unigrams = Counter(trg_words) \n    # Sum of unigram counts.\n    src_sum_unigrams = sum(src_unigrams.values())\n    trg_sum_unigrams = sum(trg_unigrams.values())\n\n    # Bigrams count.\n    src_bigrams = Counter(zip(src_words,src_words[1:]))\n    trg_bigrams = Counter(zip(trg_words,trg_words[1:]))\n    # Sum of bigram counts.\n    src_sum_bigrams = sum(src_bigrams.values())\n    trg_sum_bigrams = sum(trg_bigrams.values())\n\n    # Trigrams count.\n    src_trigrams = Counter(zip(src_words,src_words[1:], src_words[2:]))\n    trg_trigrams = Counter(zip(trg_words,trg_words[1:], trg_words[2:]))\n    # Sum of trigram counts.\n    src_sum_trigrams = sum(src_bigrams.values())\n    trg_sum_trigrams = sum(trg_bigrams.values())\n</code></pre>\n\n<p><strong>Is there any other way to do this more efficiently?</strong></p>\n\n<p><strong>How to optimally extract different N ngrams simultaneously?</strong></p>\n\n<p>From <a href=\"https://stackoverflow.com/questions/21883108/fast-optimize-n-gram-implementations-in-python\">Fast/Optimize N-gram implementations in python</a>, essentially this:</p>\n\n<pre><code>zip(*[words[i:] for i in range(n)])\n</code></pre>\n\n<p>when hard-coded is this for bigrams, <code>n=2</code>:</p>\n\n<pre><code>zip(src_words,src_words[1:])\n</code></pre>\n\n<p>and is this for trigrams, <code>n=3</code>:</p>\n\n<pre><code>zip(src_words,src_words[1:],src_words[2:])\n</code></pre>\n",
    "score": 16,
    "creation_date": 1413207903,
    "view_count": 3409,
    "answer_count": 3,
    "tags": "python;nlp;nltk;information-retrieval;n-gram"
  },
  {
    "question_id": 4696180,
    "title": "Open-source OCR library for Arabic",
    "body": "<p>I was looking around for an OCR library - optimally it would be open-source - that I could use on some Arabic pdfs. Googling it didn't result in anything useful. I was wondering if anyone knows a related OCR library or even one that works on related languages (Farsi and Urdu could be relevant) that Arabic support could be added to. </p>\n\n<p>Any general suggestions on how to approach this will be appreciated. </p>\n",
    "score": 16,
    "creation_date": 1295040814,
    "view_count": 6148,
    "answer_count": 3,
    "tags": "image-processing;nlp;ocr;arabic"
  },
  {
    "question_id": 3434144,
    "title": "Detect English verb tenses using NLTK",
    "body": "<p>I am looking for a way given an English text count verb phrases in it in past, present and future tenses. For now I am using <a href=\"http://www.nltk.org\" rel=\"noreferrer\">NLTK</a>, do a POS (Part-Of-Speech) tagging, and then count say 'VBD' to get past tenses. This is not accurate enough though, so I guess I need to go further and use chunking, then analyze VP-chunks for specific tense patterns. Is there anything existing that does that? Any further reading that might be helpful? The <a href=\"http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html\" rel=\"noreferrer\">NLTK book</a> is focused mostly on NP-chunks, and I can find quite few info on VP-chunks.</p>\n",
    "score": 16,
    "creation_date": 1281267074,
    "view_count": 10596,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 44855603,
    "title": "TypeError: can&#39;t pickle _thread.lock objects in Seq2Seq",
    "body": "<p>I'm having trouble using buckets in my Tensorflow model. When I run it with <code>buckets = [(100, 100)]</code>, it works fine. When I run it with <code>buckets = [(100, 100), (200, 200)]</code> it doesn't work at all (stacktrace at bottom).</p>\n\n<p>Interestingly, running Tensorflow's Seq2Seq tutorial gives the same kind of issue with a nearly identical stacktrace. For testing purposes, the link to the repository is <a href=\"https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate\" rel=\"noreferrer\">here</a>. </p>\n\n<p><strong>I'm not sure what the issue is, but having more than one bucket always seems to trigger it.</strong></p>\n\n<p>This code won't work as a standalone, but this is the function where it is crashing - remember that changing <code>buckets</code> from <code>[(100, 100)]</code> to <code>[(100, 100), (200, 200)]</code> triggers the crash.</p>\n\n<pre><code>class MySeq2Seq(object):\n    def __init__(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, batch_size, learning_rate):\n        self.source_vocab_size = source_vocab_size\n        self.target_vocab_size = target_vocab_size\n        self.buckets = buckets\n        self.batch_size = batch_size\n\n        cell = single_cell = tf.nn.rnn_cell.GRUCell(size)\n        if num_layers &gt; 1:\n            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n\n        # The seq2seq function: we use embedding for the input and attention\n        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n            return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n                encoder_inputs, decoder_inputs, cell,\n                num_encoder_symbols=source_vocab_size,\n                num_decoder_symbols=target_vocab_size,\n                embedding_size=size,\n                feed_previous=do_decode)\n\n        # Feeds for inputs\n        self.encoder_inputs = []\n        self.decoder_inputs = []\n        self.target_weights = []\n        for i in range(buckets[-1][0]):\n            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"encoder{0}\".format(i)))\n        for i in range(buckets[-1][1] + 1):\n            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"decoder{0}\".format(i)))\n            self.target_weights.append(tf.placeholder(tf.float32, shape=[None], name=\"weight{0}\".format(i)))\n\n        # Our targets are decoder inputs shifted by one\n        targets = [self.decoder_inputs[i + 1] for i in range(len(self.decoder_inputs) - 1)]\n        self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n            self.encoder_inputs, self.decoder_inputs, targets,\n            self.target_weights, [(100, 100)],\n            lambda x, y: seq2seq_f(x, y, False))\n\n        # Gradients update operation for training the model\n        params = tf.trainable_variables()\n        self.updates = []\n        for b in range(len(buckets)):\n            self.updates.append(tf.train.AdamOptimizer(learning_rate).minimize(self.losses[b]))\n\n        self.saver = tf.train.Saver(tf.global_variables())\n</code></pre>\n\n<p>Stacktrace:</p>\n\n<pre><code>    Traceback (most recent call last):\n  File \"D:/Stuff/IdeaProjects/myproject/src/main.py\", line 38, in &lt;module&gt;\n    model = predict.make_model(input_vocab_size, output_vocab_size, buckets, cell_size, model_layers, batch_size, learning_rate)\n  File \"D:\\Stuff\\IdeaProjects\\myproject\\src\\predictor.py\", line 88, in make_model\n    size=cell_size, num_layers=model_layers, batch_size=batch_size, learning_rate=learning_rate)\n  File \"D:\\Stuff\\IdeaProjects\\myproject\\src\\predictor.py\", line 45, in __init__\n    lambda x, y: seq2seq_f(x, y, False))\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\legacy_seq2seq\\python\\ops\\seq2seq.py\", line 1206, in model_with_buckets\n    decoder_inputs[:bucket[1]])\n  File \"D:\\Stuff\\IdeaProjects\\myproject\\src\\predictor.py\", line 45, in &lt;lambda&gt;\n    lambda x, y: seq2seq_f(x, y, False))\n  File \"D:\\Stuff\\IdeaProjects\\myproject\\src\\predictor.py\", line 28, in seq2seq_f\n    feed_previous=do_decode)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\legacy_seq2seq\\python\\ops\\seq2seq.py\", line 848, in embedding_attention_seq2seq\n    encoder_cell = copy.deepcopy(cell)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 161, in deepcopy\n    y = copier(memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 476, in __deepcopy__\n    setattr(result, k, copy.deepcopy(v, memo))\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 215, in _deepcopy_list\n    append(deepcopy(a, memo))\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 180, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 280, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 240, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 180, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 280, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 240, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 180, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 280, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 240, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 180, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 280, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 240, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\copy.py\", line 169, in deepcopy\n    rv = reductor(4)\nTypeError: can't pickle _thread.lock objects\n</code></pre>\n",
    "score": 16,
    "creation_date": 1498861861,
    "view_count": 27999,
    "answer_count": 2,
    "tags": "python-3.x;tensorflow;nlp;lstm;sequence-to-sequence"
  },
  {
    "question_id": 8424806,
    "title": "Verb Conjugations Database",
    "body": "<p>Does anyone know of a good database for verb conjugations? I am building a natural language processing app. Although I've been able to make great use of WordNet it doesn't allow me to check the form of a verb in a conjugation database or conjugate other verbs.</p>\n\n<p>I've thought about scraping a site like on of these:\n    <a href=\"http://conjugator.reverso.net/conjugation-english-verb-find.html\" rel=\"noreferrer\">http://conjugator.reverso.net/conjugation-english-verb-find.html</a>\n    <a href=\"http://www.verbix.com/webverbix/English/find.html\" rel=\"noreferrer\">http://www.verbix.com/webverbix/English/find.html</a></p>\n\n<p>and building my database from that. However I'd rather just download my own usable database. I'm not looking for a program or script to do this for me (not interested in NLTK)... I'd much rather a MySQL database (or something I can easily turn into a MySQL database) so I can do my own processing and computation.</p>\n",
    "score": 16,
    "creation_date": 1323305520,
    "view_count": 7609,
    "answer_count": 3,
    "tags": "database;nlp"
  },
  {
    "question_id": 8856347,
    "title": "How to know if two words have the same base?",
    "body": "<p>I want to know, in several languages, if two words are:</p>\n\n<ul>\n<li>either the same word,</li>\n<li>or the grammatical variants of the same word.</li>\n</ul>\n\n<p>For example:</p>\n\n<ul>\n<li><code>had</code> and <code>has</code> has the same base: in both cases, it's the verb <code>have</code>,</li>\n<li><code>city</code> and <code>cities</code> has the same base.</li>\n<li><code>went</code> and <code>gone</code> has the same base.</li>\n</ul>\n\n<p>Is there a way to use the Microsoft Word API to not just spell check text, but also normalize a word to a base or, at least, determine if two words have the same base?</p>\n\n<p>If not, what are the (free or paid) libraries (not web services) which allow me to do it (again, in several languages)?</p>\n",
    "score": 16,
    "creation_date": 1326484009,
    "view_count": 1072,
    "answer_count": 2,
    "tags": "c#;grammar;nlp"
  },
  {
    "question_id": 76084214,
    "title": "What is recommended number of threads for pytorch based on available CPU cores?",
    "body": "<p>First I want to say that I don't have much experience with pytorch, ML, NLP and other related topics, so I may confuse some concepts. Sorry.</p>\n<p>I downloaded few models from Hugging Face, organized them in one Python script and started to perform benchmark to get overview of performance. During benchmark I monitored CPU usage and saw that only 50% of CPU was used. I have 8 vCPU, but only 4 of them are loaded at 100% at the same time. The load is jumping, i.e. there may be cores 1, 3, 5, 7 that are loaded at 100%, then cores 2, 4, 6, 8 that are loaded at 100%. But in total CPU load never raises above 50%, it also never goes below 50%. This 50% load is constant.</p>\n<p>After quick googling I found <a href=\"https://pytorch.org/docs/stable/torch.html#parallelism\" rel=\"noreferrer\">parallelism doc</a>. I called <code>get_num_threads()</code> and <code>get_num_interop_threads()</code> and output was <code>4</code> for both calls. Only 50% of available CPU cores which kind of explains why CPU load was at 50%.</p>\n<p>Then I called <code>set_num_threads(8)</code> and <code>set_num_interop_threads(8)</code>, and then performed benchmark. CPU usage was at constant 100%. In general performance was a bit faster, but some models started to work a bit slowly than at 50% of CPU.</p>\n<p>So I wonder why pytorch by default uses only half of CPU? It is optimal and recommended way? Should I manually call <code>set_num_threads()</code> and <code>set_num_interop_threads()</code> with all available CPU cores if I want to achieve best performance?</p>\n<p>Edit.</p>\n<p>I made an additional benchmarks:</p>\n<ul>\n<li>one pytorch process with 50% of vCPU is a bit faster than one pytorch process with 100% of vCPU. Earlier it was vice versa, so I think it depends on models that are being used.</li>\n<li>two pytorch concurrent processes with 50% of vCPU will handle more inputs than one pytorch process with 50% of vCPU, but it is not 2x increase, it is ~1.2x increase. Process time of one input is much slower than with one pytorch process.</li>\n<li>two pytroch concurrent processes with 100% of vCPU can't complete even one input. I guess CPU is constantly switching between these processes.</li>\n</ul>\n<p>So thank you to Phoenix's answer, I think it is completely reasonable to use pytorch default settings which sets number of threads according to number of physical (not virtual) cores.</p>\n<p>Edit.</p>\n<p>pytorch documentation about this - <a href=\"https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html\" rel=\"noreferrer\">https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html</a></p>\n",
    "score": 16,
    "creation_date": 1682244173,
    "view_count": 7456,
    "answer_count": 1,
    "tags": "python;pytorch;nlp;huggingface-transformers;huggingface"
  },
  {
    "question_id": 41841339,
    "title": "Running .exe on Azure",
    "body": "<p>I have a flask web app that is published on azure. In my project I have a 'senna-win32.exe' that takes in input and sends out some output. My code for calling this .exe looks like this:</p>\n\n<pre><code> senna_path = 'senna-win32.exe'\n p = subprocess.Popen(senna_path,stdout=subprocess.PIPE,stdin=subprocess.PIPE, stderr=subprocess.PIPE)\n stdout = p.communicate(input=bytes(userInput, 'utf-8'))[0]\n inList = stdout.decode()\n</code></pre>\n\n<p>It seems to work on my local pc, but on azure, it doesn't raise any issues but does nothing. </p>\n\n<p>Can I not execute .exe file on azure? It is a web app and not a cloud service, I'm really trying to avoid the web/worker roles since .exe doesn't do whole lot of processing.</p>\n",
    "score": 16,
    "creation_date": 1485303627,
    "view_count": 2529,
    "answer_count": 1,
    "tags": "python;visual-studio;azure;nlp"
  },
  {
    "question_id": 23042699,
    "title": "FreqDist in NLTK not sorting output",
    "body": "<p>I'm new to Python and I'm trying to teach myself language processing. NLTK in python has a function called FreqDist that gives the frequency of words in a text, but for some reason it's not working properly.</p>\n\n<p>This is what the tutorial has me write:</p>\n\n<pre><code>fdist1 = FreqDist(text1)\nvocabulary1 = fdist1.keys()\nvocabulary1[:50]\n</code></pre>\n\n<p>So basically it's supposed to give me a list of the 50 most frequent words in the text. When I run the code, though, the result is the 50 <em>least</em> frequent words in order of least frequent to most frequent, as opposed to the other way around. The output I am getting is as follows:</p>\n\n<pre><code>[u'succour', u'four', u'woods', u'hanging', u'woody', u'conjure', u'looking', u'eligible', u'scold', u'unsuitableness', u'meadows', u'stipulate', u'leisurely', u'bringing', u'disturb', u'internally', u'hostess', u'mohrs', u'persisted', u'Does', u'succession', u'tired', u'cordially', u'pulse', u'elegant', u'second', u'sooth', u'shrugging', u'abundantly', u'errors', u'forgetting', u'contributed', u'fingers', u'increasing', u'exclamations', u'hero', u'leaning', u'Truth', u'here', u'china', u'hers', u'natured', u'substance', u'unwillingness...]\n</code></pre>\n\n<p>I'm copying the tutorial exactly, but I must be doing something wrong.</p>\n\n<p>Here is the link to the tutorial:</p>\n\n<p><a href=\"http://www.nltk.org/book/ch01.html#sec-computing-with-language-texts-and-words\" rel=\"noreferrer\">http://www.nltk.org/book/ch01.html#sec-computing-with-language-texts-and-words</a></p>\n\n<p>The example is right under the heading \"Figure 1.3: Counting Words Appearing in a Text (a frequency distribution)\"</p>\n\n<p>Does anyone know how I might fix this?</p>\n",
    "score": 15,
    "creation_date": 1397391836,
    "view_count": 25942,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 13788229,
    "title": "Very simple text classification by machine learning?",
    "body": "<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"https://stackoverflow.com/questions/8136677/text-classification-into-categories\">Text Classification into Categories</a>  </p>\n</blockquote>\n\n\n\n<p>I am currently working on a solution to get the type of food served in a database with 10k restaurants based on their description. I'm using lists of keywords to decide which kind of food is being served.</p>\n\n<p>I read a little bit about machine learning but I have no practical experience with it at all. Can anyone explain to me if/why it would a be better solution to a simple problem like this? I find accuracy more important than performance!</p>\n\n<p>simplified example:</p>\n\n<pre><code>[\"China\", \"Chinese\", \"Rice\", \"Noodles\", \"Soybeans\"]\n[\"Belgium\", \"Belgian\", \"Fries\", \"Waffles\", \"Waterzooi\"]\n</code></pre>\n\n<p>a possible description could be:</p>\n\n<p>\"Hong's Garden Restaurant offers savory, reasonably priced <strong>Chinese</strong> to our customers. If you find that you have a sudden craving for\n<strong>rice</strong>, <strong>noodles</strong> or <strong>soybeans</strong> at 8 o’clock on a Saturday evening, don’t worry! We’re open seven days a week and offer carryout service. You can get <strong>fries</strong> here as well!\"</p>\n",
    "score": 15,
    "creation_date": 1355062840,
    "view_count": 36332,
    "answer_count": 1,
    "tags": "python;algorithm;machine-learning;text-analysis"
  },
  {
    "question_id": 43018030,
    "title": "Replace apostrophe/short words in python",
    "body": "<p>I am using python to clean a given sentence. Suppose that my sentence is:</p>\n\n<pre><code>What's the best way to ensure this?\n</code></pre>\n\n<p>I want to convert:</p>\n\n<pre><code>What's -&gt; What is\n</code></pre>\n\n<p>Similarly,</p>\n\n<pre><code> must've -&gt; must have\n</code></pre>\n\n<p>Also, verbs to original form,</p>\n\n<pre><code>told -&gt; tell\n</code></pre>\n\n<p>Singular to plural, and so on.</p>\n\n<p>I am currently exploring textblob. But not all of the above is possible using it. </p>\n",
    "score": 15,
    "creation_date": 1490454667,
    "view_count": 18491,
    "answer_count": 4,
    "tags": "python;nlp;textblob"
  },
  {
    "question_id": 42206557,
    "title": "How to find similar words with FastText?",
    "body": "<p>I am playing around with <code>FastText</code>, <a href=\"https://pypi.python.org/pypi/fasttext\" rel=\"noreferrer\">https://pypi.python.org/pypi/fasttext</a>,which is quite similar to <code>Word2Vec</code>. Since it seems to be a pretty new library with not to many built in functions yet, I was wondering how to extract morphological similar words. </p>\n\n<p>For eg: <code>model.similar_word(\"dog\")</code> -> dogs. But there is no function built-in.</p>\n\n<p>If I type \n<code>model[\"dog\"]</code> </p>\n\n<p>I only get the vector, that might be used to compare cosine similarity.\n<code>model.cosine_similarity(model[\"dog\"], model[\"dogs\"]])</code>. </p>\n\n<p>Do I have to make some sort of loop and do <code>cosine_similarity</code> on all possible pairs in a text? That would take time ...!!!</p>\n",
    "score": 15,
    "creation_date": 1486996411,
    "view_count": 27548,
    "answer_count": 6,
    "tags": "python;nlp;word2vec;fasttext"
  },
  {
    "question_id": 9671388,
    "title": "R count number of commas and string",
    "body": "<p>I have a string:</p>\n\n<pre><code>    str1 &lt;- \"This is a string, that I've written \n        to ask about a question, or at least tried to.\"\n</code></pre>\n\n<p>How would I :</p>\n\n<p>1) count the number of commas</p>\n\n<p>2) count the occurences of '-ion'</p>\n\n<p>Any suggestions?</p>\n",
    "score": 15,
    "creation_date": 1331571453,
    "view_count": 17857,
    "answer_count": 4,
    "tags": "r;nlp"
  },
  {
    "question_id": 15777201,
    "title": "Why vector normalization can improve the accuracy of clustering and classification?",
    "body": "<p>It is described in Mahout in Action that normalization can slightly improve the accuracy.\nCan anyone explain the reason, thanks!</p>\n",
    "score": 15,
    "creation_date": 1364951330,
    "view_count": 16886,
    "answer_count": 3,
    "tags": "machine-learning;nlp;classification;mahout"
  },
  {
    "question_id": 43795249,
    "title": "How does spacy lemmatizer works?",
    "body": "<p>For lemmatization spacy has a <a href=\"https://github.com/explosion/spaCy/tree/master/spacy/en/lemmatizer\" rel=\"noreferrer\">lists of words</a>:  adjectives, adverbs, verbs... and also lists for exceptions: adverbs_irreg... for the regular ones there is a set of <a href=\"https://github.com/explosion/spaCy/blob/master/spacy/en/lemmatizer/_lemma_rules.py\" rel=\"noreferrer\">rules</a></p>\n\n<p>Let's take as example the word \"wider\"</p>\n\n<p>As it is an adjective the rule for lemmatization should be take from this list:</p>\n\n<pre><code>ADJECTIVE_RULES = [\n    [\"er\", \"\"],\n    [\"est\", \"\"],\n    [\"er\", \"e\"],\n    [\"est\", \"e\"]\n] \n</code></pre>\n\n<p>As I understand the process will be like this:</p>\n\n<p>1) Get the POS tag of the word to know whether it is a noun, a verb...<br>\n2) If the word is in the list of irregular cases is replaced directly if not one of the rules is applied.</p>\n\n<p>Now, how is decided to use \"er\" -> \"e\" instead of \"er\"-> \"\" to get \"wide\" and not \"wid\"? </p>\n\n<p><a href=\"http://textanalysisonline.com/spacy-word-lemmatize\" rel=\"noreferrer\">Here</a> it can be tested.</p>\n",
    "score": 15,
    "creation_date": 1493949039,
    "view_count": 6240,
    "answer_count": 3,
    "tags": "python;nlp;wordnet;spacy;lemmatization"
  },
  {
    "question_id": 41517595,
    "title": "nltk stemmer: string index out of range",
    "body": "<p>I have a set of pickled text documents which I would like to stem using nltk's <code>PorterStemmer</code>. For reasons specific to my project, I would like to do the stemming inside of a django app view.</p>\n\n<p>However, when stemming the documents inside the django view, I receive an <code>IndexError: string index out of range</code> exception from <code>PorterStemmer().stem()</code> for the string <code>'oed'</code>. As a result, running the following:</p>\n\n<pre><code># xkcd_project/search/views.py\nfrom nltk.stem.porter import PorterStemmer\n\ndef get_results(request):\n    s = PorterStemmer()\n    s.stem('oed')\n    return render(request, 'list.html')\n</code></pre>\n\n<p>raises the mentioned error:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"//anaconda/envs/xkcd/lib/python2.7/site-packages/django/core/handlers/exception.py\", line 39, in inner\n    response = get_response(request)\n  File \"//anaconda/envs/xkcd/lib/python2.7/site-packages/django/core/handlers/base.py\", line 187, in _get_response\n    response = self.process_exception_by_middleware(e, request)\n  File \"//anaconda/envs/xkcd/lib/python2.7/site-packages/django/core/handlers/base.py\", line 185, in _get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/Users/jkarimi91/Projects/xkcd_search/xkcd_project/search/views.py\", line 15, in get_results\n    s.stem('oed')\n  File \"//anaconda/envs/xkcd/lib/python2.7/site-packages/nltk/stem/porter.py\", line 665, in stem\n    stem = self._step1b(stem)\n  File \"//anaconda/envs/xkcd/lib/python2.7/site-packages/nltk/stem/porter.py\", line 376, in _step1b\n    lambda stem: (self._measure(stem) == 1 and\n  File \"//anaconda/envs/xkcd/lib/python2.7/site-packages/nltk/stem/porter.py\", line 258, in _apply_rule_list\n    if suffix == '*d' and self._ends_double_consonant(word):\n  File \"//anaconda/envs/xkcd/lib/python2.7/site-packages/nltk/stem/porter.py\", line 214, in _ends_double_consonant\n    word[-1] == word[-2] and\nIndexError: string index out of range\n</code></pre>\n\n<p>Now what is really odd is running the same stemmer on the same string outside django (be it a seperate python file or an interactive python console) produces no error. In other words:</p>\n\n<pre><code># test.py\nfrom nltk.stem.porter import PorterStemmer\ns = PorterStemmer()\nprint s.stem('oed')\n</code></pre>\n\n<p>followed by:</p>\n\n<pre><code>python test.py\n# successfully prints 'o'\n</code></pre>\n\n<p>what is causing this issue?</p>\n",
    "score": 15,
    "creation_date": 1483760923,
    "view_count": 4122,
    "answer_count": 2,
    "tags": "nlp;nltk;stemming;porter-stemmer"
  },
  {
    "question_id": 27517924,
    "title": "Extract Word from Synset using Wordnet in NLTK 3.0",
    "body": "<p>Some time ago, someone on SO asked <a href=\"https://stackoverflow.com/questions/24664250/how-do-i-print-out-just-the-word-itself-in-a-wordnet-synset-using-python-nltk\">how to retrieve a list of words for a given synset</a> using NLTK's wordnet wrapper. Here is one of the suggested responses:</p>\n\n<pre><code>for synset in wn.synsets('dog'):\n    print synset.lemmas[0].name\n</code></pre>\n\n<p>Running this code with NLTK 3.0 yields <code>TypeError: 'instancemethod' object is not subscriptable</code>. </p>\n\n<p>I tried each of the previously-proposed solutions (each of the solutions described on the page linked above), but each throws an error. I therefore wanted to ask: Is it possible to print the words for a list of synsets with NLTK 3.0? I would be thankful for any advice others can offer on this question.</p>\n",
    "score": 15,
    "creation_date": 1418787518,
    "view_count": 29028,
    "answer_count": 3,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 5598713,
    "title": "Computer AI algorithm to write sentences?",
    "body": "<p>I am searching for information on algorithms to process text sentences or to follow a structure when creating sentences that are valid in a normal human language such as English. I would like to know if there are projects working in this field that I can go learn from or start using.</p>\n\n<p>For example, if I gave a program a noun, provided it with a thesaurus (for related words) and part-of-speech (so it understood where each word belonged in a sentence) - could it create a random, valid sentence?</p>\n\n<p>I'm sure there are many sub-sections of this kind of research so any leads into this would be great.</p>\n",
    "score": 15,
    "creation_date": 1302284071,
    "view_count": 19705,
    "answer_count": 4,
    "tags": "parsing;artificial-intelligence;nlp"
  },
  {
    "question_id": 30318530,
    "title": "How apache UIMA is different from Apache Opennlp",
    "body": "<p>I have been doing some capability testing with Apache OpenNLP, Which has the capability to Sentence detection, Tokenization, Name entity recognition. Now when i started looking at UIMA documents it is mentioned on the UIMA home page -  \"language identification\" => \"language specific segmentation\" => \"sentence boundary detection\" => \"entity detection (person/place names etc.)\".  </p>\n\n<p>Which says that i can use UIMA to do the same task as done by OpenNLP. What added feature both have ? I am new to this area, Please help me to understand the uses and capability perspective of both. </p>\n",
    "score": 15,
    "creation_date": 1432017997,
    "view_count": 4951,
    "answer_count": 1,
    "tags": "nlp;opennlp;uima"
  },
  {
    "question_id": 58712418,
    "title": "Replace entity with its label in SpaCy",
    "body": "<p>Is there anyway by SpaCy to replace entity detected by SpaCy NER with its label?\nFor example:\n<strong>I am eating an apple while playing with my Apple Macbook.</strong></p>\n\n<p>I have trained NER model with SpaCy to detect \"FRUITS\" entity and the model successfully detects the first \"apple\" as \"FRUITS\", but not the second \"Apple\".</p>\n\n<p>I want to do post-processing of my data by replacing each entity with its label, so I want to replace the first \"apple\" with \"FRUITS\". The sentence will be \"<strong>I am eating an FRUITS while playing with my Apple Macbook.</strong>\"</p>\n\n<p>If I simply use regex, it will replace the second \"Apple\" with \"FRUITS\" as well, which is incorrect. Is there any smart way to do this?</p>\n\n<p>Thanks!</p>\n",
    "score": 15,
    "creation_date": 1572960671,
    "view_count": 7139,
    "answer_count": 3,
    "tags": "nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 1410408,
    "title": "Natural Language date and time parser for java",
    "body": "<p>I am working on a Natural Language parser which examines a sentence in english and extracts some information like name, date etc.</p>\n\n<p>for example: \"<em>Lets meet next tuesday at 5 PM at the beach.</em>\"</p>\n\n<p>So the output will be something like : \"<em>Lets meet 15/09/2009 at 1700 hr at the beach</em>\"</p>\n\n<p>So basically, what i want to know is that <strong>is there any framework or library available for JAVA to do these kind of operations like parsing dates from a sentence and give a output with some specified format.</strong> </p>\n\n<p>Regards,\nPranav</p>\n\n<hr>\n\n<p>Thanks for the replies. I have looked on few NLPs like <a href=\"http://alias-i.com/lingpipe/index.html\" rel=\"noreferrer\">LingPipe</a>, OpenPL, <a href=\"http://nlp.stanford.edu/index.shtml\" rel=\"noreferrer\">Stanford NLP</a>. I wanted to ask do they hav anything for date parsing for java.</p>\n",
    "score": 15,
    "creation_date": 1252669377,
    "view_count": 10076,
    "answer_count": 8,
    "tags": "java;datetime;parsing;nlp"
  },
  {
    "question_id": 39813890,
    "title": "Extract words from PDF with golang?",
    "body": "<p>I don't understand type conversion. I know this isn't right, all I get is a bunch of hieroglyphs.</p>\n\n<p><code>f, _ := os.Open(\"test.pdf\")\ndefer f.Close()\nio.Copy(os.Stdout, f)</code></p>\n\n<p>I want to work with the strings....</p>\n",
    "score": 15,
    "creation_date": 1475382814,
    "view_count": 35771,
    "answer_count": 4,
    "tags": "pdf;go;text-analysis"
  },
  {
    "question_id": 13555021,
    "title": "Supervised Latent Dirichlet Allocation for Document Classification?",
    "body": "<p>I have a bunch of already human-classified documents in some groups. </p>\n\n<p>Is there a modified version of lda which I can use to train a model and then later classify unknown documents with it?</p>\n",
    "score": 15,
    "creation_date": 1353874340,
    "view_count": 18472,
    "answer_count": 3,
    "tags": "machine-learning;nlp;classification;document-classification;lda"
  },
  {
    "question_id": 63302027,
    "title": "How to avoid double-extracting of overlapping patterns in SpaCy with Matcher?",
    "body": "<p>I need to extract item combination from 2 lists by means of python Spacy Matcher. The problem is following:\nLet us have 2 lists:</p>\n<pre><code>colors=['red','bright red','black','brown','dark brown']\nanimals=['fox','bear','hare','squirrel','wolf']\n</code></pre>\n<p>I match the sequences by the following code:</p>\n<pre><code>first_color=[]\nlast_color=[]\nonly_first_color=[]\nfor color in colors:\n    if ' ' in color:\n        first_color.append(color.split(' ')[0])\n        last_color.append(color.split(' ')[1])\n    else:\n        only_first_color.append(color)\nmatcher = Matcher(nlp.vocab)\n\npattern1 = [{&quot;TEXT&quot;: {&quot;IN&quot;: only_first_color}},{&quot;TEXT&quot;:{&quot;IN&quot;: animals}}]\npattern2 = [{&quot;TEXT&quot;: {&quot;IN&quot;: first_color}},{&quot;TEXT&quot;: {&quot;IN&quot;: last_color}},{&quot;TEXT&quot;:{&quot;IN&quot;: animals}}]\n\nmatcher.add(&quot;ANIMALS&quot;, None, pattern1,pattern2)\n\ndoc = nlp('bright red fox met black wolf')\n\nmatches = matcher(doc)\n\nfor match_id, start, end in matches:\n    string_id = nlp.vocab.strings[match_id]  # Get string representation\n    span = doc[start:end]  # The matched span\n    print(start, end, span.text)\n</code></pre>\n<p>It gives the output:</p>\n<pre><code>0 3 bright red fox\n1 3 red fox\n4 6 black wolf\n</code></pre>\n<p>How can i extract only 'bright red fox' and 'black wolf'? Should i change the patterns rules or post-process the matches?</p>\n<p>Any thoughts appreciate!</p>\n",
    "score": 15,
    "creation_date": 1596803839,
    "view_count": 4679,
    "answer_count": 2,
    "tags": "python;nlp;spacy;matcher"
  },
  {
    "question_id": 14820590,
    "title": "Trying to get tf-idf weighting working in R",
    "body": "<p>I am trying to do some very basic text analysis with the tm package and get some tf-idf scores; I'm running OS X (though I've tried this on Debian Squeeze with the same result); I've got a directory (which is my working directory) with a couple text files in it (the first containing the first three episodes of <em>Ulysses</em>, the second containing the second three episodes, if you must know).</p>\n\n<p>R Version: 2.15.1\nSessionInfo() Reports this about tm: [1] tm_0.5-8.3</p>\n\n<p>Relevant bit of code:</p>\n\n<pre><code>library('tm')\ncorpus &lt;- Corpus(DirSource('.'))\ndtm &lt;- DocumentTermMatrix(corpus,control=list(weight=weightTfIdf))\n\nstr(dtm)\nList of 6\n $ i       : int [1:12456] 1 1 1 1 1 1 1 1 1 1 ...\n $ j       : int [1:12456] 2 10 12 17 20 24 29 30 32 34 ...\n $ v       : num [1:12456] 1 1 1 1 1 1 1 1 1 1 ...\n $ nrow    : int 2\n $ ncol    : int 10646\n $ dimnames:List of 2\n  ..$ Docs : chr [1:2] \"bloom.txt\" \"telemachiad.txt\"\n  ..$ Terms: chr [1:10646] \"_--c'est\" \"_--et\" \"_--for\" \"_--goodbye,\" ...\n - attr(*, \"class\")= chr [1:2] \"DocumentTermMatrix\" \"simple_triplet_matrix\"\n - attr(*, \"Weighting\")= chr [1:2] \"term frequency\" \"tf\"\n</code></pre>\n\n<p>You will note, that the weighting appears to still be the default term frequency (tf) rather than the weighted tf-idf scores that I'd like.</p>\n\n<p>Apologies if I'm missing something obvious, but based on the documentation I've read, this <em>should</em> work. The fault, no doubt, lies not in the stars...</p>\n",
    "score": 15,
    "creation_date": 1360615766,
    "view_count": 24385,
    "answer_count": 1,
    "tags": "r;tm;tf-idf;text-analysis"
  },
  {
    "question_id": 8974090,
    "title": "Finding meaningful sub-sentences from a sentence",
    "body": "<p>Is there a way to to find all the sub-sentences of a sentence that still are meaningful and contain at least one subject, verb, and a predicate/object?</p>\n\n<p>For example, if we have a sentence like \"I am going to do a seminar on NLP at SXSW in Austin next month\". We can extract the following meaningful sub-sentences from this sentence: \"I am going to do a seminar\", \"I am going to do a seminar on NLP\", \"I am going to do a seminar on NLP at SXSW\", \"I am going to do a seminar at SXSW\", \"I am going to do a seminar in Austin\", \"I am going to do a seminar on NLP next month\", etc.</p>\n\n<p>Please note that there is no deduced sentences here (e.g. \"There will be a NLP seminar at SXSW next month\". Although this is true, we don't need this as part of this problem.) . All generated sentences are strictly part of the given sentence.</p>\n\n<p>How can we approach solving this problem? I was thinking of creating annotated training data that has a set of legal sub-sentences for each sentence in the training data set. And then write some supervised learning algorithm(s) to generate a model.</p>\n\n<p>I am quite new to NLP and Machine Learning, so it would be great if you guys could suggest some ways to solve this problem.</p>\n",
    "score": 15,
    "creation_date": 1327332588,
    "view_count": 5019,
    "answer_count": 4,
    "tags": "parsing;artificial-intelligence;nlp;machine-learning;grammar"
  },
  {
    "question_id": 3693323,
    "title": "How do I manipulate parse trees?",
    "body": "<p>I've been playing around with natural language parse trees and manipulating them in various ways. I've been using Stanford's Tregex and Tsurgeon tools but the code is a mess and doesn't fit in well with my mostly Python environment (those tools are Java and aren't ideal for tweaking). I'd like to have a toolset that would allow for easy hacking when I need more functionality. Are there any other tools that are well suited for doing pattern matching on trees and then manipulation of those matched branches?</p>\n\n<p>For example, I'd like to take the following tree as input:</p>\n\n<pre><code>(ROOT\n  (S\n    (NP\n      (NP (NNP Bank))\n      (PP (IN of)\n        (NP (NNP America))))\n    (VP (VBD used)\n      (S\n        (VP (TO to)\n          (VP (VB be)\n            (VP (VBN called)\n              (NP\n                (NP (NNP Bank))\n                (PP (IN of)\n                  (NP (NNP Italy)))))))))))\n</code></pre>\n\n<p>and (this is a simplified example):</p>\n\n<ol>\n<li>Find any node with the label NP that has a first child with the label NP and some descendent named \"Bank\", and a second child with the label PP.</li>\n<li>If that matches, then take all of the children of the PP node and move them to end of the matched NP's children.</li>\n</ol>\n\n<p>For example, take this part of the tree:</p>\n\n<pre><code>(NP\n  (NP (NNP Bank))\n  (PP (IN of)\n    (NP (NNP America))))\n</code></pre>\n\n<p>and turn it into this:</p>\n\n<pre><code>(NP\n  (NP (NNP Bank) (IN of) (NP (NNP America))))\n</code></pre>\n\n<p>Since my input trees are S-expressions I've considered using Lisp (embedded into my Python program) but it's been so long that I've written anything significant in Lisp that I have no idea where to even start.</p>\n\n<p>What would be a good way to describe the patterns? What would be a good way to describe the manipulations? What's a good way to think about this problem?</p>\n",
    "score": 15,
    "creation_date": 1284253390,
    "view_count": 3978,
    "answer_count": 3,
    "tags": "lisp;nlp;pattern-matching;stanford-nlp;s-expression"
  },
  {
    "question_id": 48549670,
    "title": "Pooling vs Pooling-over-time",
    "body": "<p>I understand conceptually what is happening in a max/sum pool as a CNN layer operation, but I see this term \"max pool over time\", or \"sum pool over time\" thrown around (e.g., <a href=\"https://arxiv.org/pdf/1408.5882.pdf\" rel=\"noreferrer\">\"Convolutional Neural Networks for Sentence Classification\"</a> paper by Yoon Kim). What is the difference?</p>\n",
    "score": 15,
    "creation_date": 1517425618,
    "view_count": 7934,
    "answer_count": 2,
    "tags": "machine-learning;neural-network;nlp;convolution;max-pooling"
  },
  {
    "question_id": 11141194,
    "title": "Sentiment Analysis of Entity (Entity-level Sentiment Analysis)",
    "body": "<p>I've been working on document level sentiment analysis since past 1 year. <em>Document level sentiment analysis</em> provides the sentiment of the complete document. For example - The text \"<em>Nokia is good but vodafone sucks big time</em>\" would have a negative polarity associated with it as it would be agnostic to the entities Nokia and Vodafone. <em>How would it be possible to get entity level sentiment, like positive for Nokia but negative for Vodafone</em> ? Are there any research papers providing a solution to such problems ?</p>\n",
    "score": 15,
    "creation_date": 1340291462,
    "view_count": 6829,
    "answer_count": 4,
    "tags": "nlp;sentiment-analysis;named-entity-recognition"
  },
  {
    "question_id": 3237624,
    "title": "How to use NLP to separate a unstructured text content into distinct paragraphs?",
    "body": "<p>The following unstructured text has three distinct themes -- Stallone, Philadelphia and the American Revolution.  But which algorithm or technique would you use to separate this content into distinct paragraphs?</p>\n\n<p>Classifiers won't work in this situation. I also tried to use Jaccard Similarity analyzer to find distance between successive sentences and tried to group successive sentences into one paragraph if the distance  between them was less than a given value. Is there a better method?</p>\n\n<p>This is my text sample:</p>\n\n<blockquote>\n  <p>Sylvester Gardenzio Stallone , nicknamed Sly Stallone,  is an American actor, filmmaker and screenwriter. Stallone is known for his machismo  and Hollywood action roles. Stallone's film Rocky was inducted into the National Film Registry as well as having its film props placed in the Smithsonian Museum. Stallone's use of the front entrance to the Philadelphia Museum of Art in the Rocky series led the area to be nicknamed the Rocky Steps.A commercial, educational, and cultural center, Philadelphia was once the second-largest city in the British Empire  (after London), and the social and geographical center of the original 13 American colonies. It was a centerpiece of early American history, host to many of the ideas and actions that gave birth to the American Revolution and independence.The American Revolution was the political upheaval during the last half of the 18th century in which thirteen colonies in North America joined together to break free from the British Empire, combining to become the United States of America. They first rejected the authority of the Parliament of Great Britain to govern them from overseas without representation, and then expelled all royal officials. By 1774 each colony had established a Provincial Congress, or an equivalent governmental institution, to form individual self-governing states.</p>\n</blockquote>\n",
    "score": 15,
    "creation_date": 1279027820,
    "view_count": 5281,
    "answer_count": 3,
    "tags": "text;nlp;classification;cluster-analysis;text-segmentation"
  },
  {
    "question_id": 33558709,
    "title": "The similar method from the nltk module produces different results on different machines. Why?",
    "body": "<p>I have taught a few introductory classes to text mining with Python, and the class tried the similar method with the provided practice texts.  Some students got different results for text1.similar() than others.  </p>\n\n<p>All versions and etc. were the same.</p>\n\n<p>Does anyone know why these differences would occur?  Thanks.</p>\n\n<p>Code used at command line.</p>\n\n<pre><code>python\n&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; nltk.download() #here you use the pop-up window to download texts\n&gt;&gt;&gt; from nltk.book import *\n*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n&gt;&gt;&gt;&gt;&gt;&gt; text1.similar(\"monstrous\")\nmean part maddens doleful gamesome subtly uncommon careful untoward\nexasperate loving passing mouldy christian few true mystifying\nimperial modifies contemptible\n&gt;&gt;&gt; text2.similar(\"monstrous\")\nvery heartily so exceedingly remarkably as vast a great amazingly\nextremely good sweet\n</code></pre>\n\n<p>Those lists of terms returned by the similar method differ from user to user, they have many words in common, but they are not identical lists.  All users were using the same OS, and the same versions of python and nltk.</p>\n\n<p>I hope that makes the question clearer.  Thanks.</p>\n",
    "score": 15,
    "creation_date": 1446778664,
    "view_count": 3177,
    "answer_count": 2,
    "tags": "python;nlp;nltk;similarity;corpus"
  },
  {
    "question_id": 12821201,
    "title": "What are ngram counts and how to implement using nltk?",
    "body": "<p>I've read a paper that uses ngram counts as feature for a classifier, and I was wondering what this exactly means.</p>\n\n<p>Example text: \"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam\"</p>\n\n<p>I can create unigrams, bigrams, trigrams, etc. out of this text, where I have to define on which \"level\" to create these unigrams. The \"level\" can be character, syllable, word, ...</p>\n\n<p>So creating unigrams out of the sentence above would simply create a list of all words? </p>\n\n<p>Creating bigrams would result in word pairs bringing together words that follow each other? </p>\n\n<p>So if the paper talks about ngram counts, it simply creates unigrams, bigrams, trigrams, etc. out of the text, and counts how often which ngram occurs? </p>\n\n<p>Is there an existing method in python's nltk package? Or do I have to implement a version of my own?</p>\n",
    "score": 15,
    "creation_date": 1349877668,
    "view_count": 24860,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 37043598,
    "title": "Use brain.js neural network to do text analysis",
    "body": "<p>I'm trying to do some text analysis to determine if a given string is... talking about politics. I'm thinking I could create a neural network where the input is either a string or a list of words (ordering might matter?) and the output is whether the string is about politics.</p>\n\n<p>However the brain.js library only takes inputs of a number between 0 and 1 or an array of numbers between 0 and 1. How can I coerce my data in such a way that I can achieve the task?</p>\n",
    "score": 15,
    "creation_date": 1462428623,
    "view_count": 11858,
    "answer_count": 3,
    "tags": "neural-network;text-analysis;brain.js"
  },
  {
    "question_id": 62261602,
    "title": "Downloading transformers models to use offline",
    "body": "<p>I have a trained transformers NER model that I want to use on a machine not connected to the internet. When loading such a model, currently it downloads cache files to the .cache folder.</p>\n<p>To load and run the model offline, you need to copy the files in the .cache folder to the offline machine. However, these files have long, non-descriptive names, which makes it really hard to identify the correct files if you have multiple models you want to use. Any thoughts on this?</p>\n<p>Example of model files</p>\n<p><a href=\"https://i.sstatic.net/0CFZj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/0CFZj.png\" alt=\"enter image description here\" /></a></p>\n",
    "score": 15,
    "creation_date": 1591617640,
    "view_count": 28649,
    "answer_count": 3,
    "tags": "python;nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 42373747,
    "title": "Is there a more efficient way to find most common n-grams?",
    "body": "<p>I'm trying to find k most common n-grams from a large corpus. I've seen lots of places suggesting the naïve approach - simply scanning through the entire corpus and keeping a dictionary of the count of all n-grams. Is there a better way to do this?</p>\n",
    "score": 15,
    "creation_date": 1487697146,
    "view_count": 17731,
    "answer_count": 1,
    "tags": "algorithm;nlp;n-gram"
  },
  {
    "question_id": 49274650,
    "title": "Directly load spacy model from packaged tar.gz file",
    "body": "<p>Is it possible to load a packaged spacy model (i.e. <code>foo.tar.gz</code>) directly from the tar file instead of installing it beforehand? I would imagine something like:</p>\n\n<pre><code>import spacy \n\nnlp = spacy.load(/some/path/foo.tar.gz)\n</code></pre>\n",
    "score": 15,
    "creation_date": 1521021556,
    "view_count": 12067,
    "answer_count": 2,
    "tags": "python;model;nlp;load;spacy"
  },
  {
    "question_id": 3156256,
    "title": "how do I create my own training corpus for stanford tagger?",
    "body": "<p>I have to analyze informal english text with lots of short hands and local lingo. Hence I was thinking of creating the model for the stanford tagger.</p>\n\n<p>How do i create my own set of labelled corpus for the stanford tagger to train on?</p>\n\n<p>What is the syntax of the corpus and how long should my corpus be in order to achieve a desirable performance?</p>\n",
    "score": 15,
    "creation_date": 1277974186,
    "view_count": 7645,
    "answer_count": 4,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 52570805,
    "title": "How to identify abbreviations/acronyms and expand them in spaCy?",
    "body": "<p>I have a large (~50k) term list and a number of these key phrases / terms have corresponding acronyms / abbreviations. I need a fast way of finding either the abbreviation or the expanded abbreviation ( i.e. MS -> Microsoft ) and then replacing that with the full expanded abbreviation + abbreviation ( i.e. Microsoft -> Microsoft (MS) or MS -> Microsoft (MS) ).</p>\n\n<p>I am very new to spaCy, so my naive approach was going to be to use <a href=\"https://spacy.io/universe/?id=spacy-lookup\" rel=\"noreferrer\">spacy_lookup</a> and use both the abbreviation and the expanded abbreviation as keywords and then using some kind of <a href=\"https://explosion.ai/blog/spacy-v2-pipelines-extensions\" rel=\"noreferrer\">pipeline extension</a> to then go through the matches and replace them with the full expanded abbreviation + abbreviation.</p>\n\n<p>Is there a better way of tagging and resolving acronyms/abbreviations in spaCy?</p>\n",
    "score": 15,
    "creation_date": 1538241885,
    "view_count": 13100,
    "answer_count": 1,
    "tags": "python-3.x;nlp;spacy"
  },
  {
    "question_id": 50583254,
    "title": "Explain bpe (Byte Pair Encoding) with examples?",
    "body": "<p>Can somebody help to explain the basic concept behind the <strong>bpe model</strong>? Except <a href=\"https://arxiv.org/abs/1508.07909\" rel=\"noreferrer\">this paper</a>, there is no so many explanations about it yet.  </p>\n\n<p>What I have known so far is that it enables NMT model translation on open-vocabulary by encoding rare and unknown words as sequences of subword units.</p>\n\n<p>But I want to get a general idea of how it works without going through the paper.</p>\n",
    "score": 15,
    "creation_date": 1527593331,
    "view_count": 7001,
    "answer_count": 2,
    "tags": "algorithm;nlp;tokenize"
  },
  {
    "question_id": 39981980,
    "title": "PDFminer: PDFTextExtractionNotAllowed Error",
    "body": "<p>I'm trying to extract text from pdfs I've scraped off the internet, but when I attempt to download them I get the error:</p>\n\n<pre><code>File \"/usr/local/lib/python2.7/dist-packages/pdfminer/pdfpage.py\", line 124, in get_pages\n    raise PDFTextExtractionNotAllowed('Text extraction is not allowed: %r' % fp)\nPDFTextExtractionNotAllowed: Text extraction is not allowed &lt;cStringIO.StringO object at 0x7f79137a1ab0&gt;\n</code></pre>\n\n<p>I've checked stackoverflow and <a href=\"https://stackoverflow.com/questions/28192977/how-to-unlock-a-secured-read-protected-pdf-in-python\">someone else who had this error</a> found their pdfs to be secured with a password.  However, I'm able to access the pdfs through preview on my mac.  </p>\n\n<p>Someone mentioned that preview may view secured pdfs anyway, so I opened the files in Adobe Acrobat Reader as well and was still able to access the pdf.</p>\n\n<p>Here's an example from the site I'm downloading pdfs from:\n<a href=\"http://www.sophia-project.org/uploads/1/3/9/5/13955288/aristotle_firstprinciples.pdf\" rel=\"noreferrer\">http://www.sophia-project.org/uploads/1/3/9/5/13955288/aristotle_firstprinciples.pdf</a></p>\n\n<p>I discovered that if I open the pdf manually and re-export it as a pdf to the same filepath (basically replacing the original with a 'new' file), then I am able to extract text from it.  I'm guessing it has something to do with downloading them from the site.  I'm simply using urllib to download the pdfs as follows:</p>\n\n<pre><code>if not os.path.isfile(filepath):\n    print '\\nDownloading pdf'\n    urllib.urlretrieve(link, filepath)\nelse:\n    print '\\nFile {} already exists!'.format(title)\n</code></pre>\n\n<p>I also tried rewriting the file to a new filepath, but it still resulted in the same error.</p>\n\n<pre><code>if not os.path.isfile(filepath):\n    print '\\nDownloading pdf'\n    urllib.urlretrieve(link, filepath)\n\n    with open(filepath) as f:\n        new_filepath = re.split(r'\\.', filepath)[0] + '_.pdf'\n        new_f = file(new_filepath, 'w')\n        new_f.write(f.read())\n        new_f.close()\n\n    os.remove(filepath)\n    filepath = new_filepath\nelse:\n    print '\\nFile {} already exists!'.format(title)\n</code></pre>\n\n<p>Lastly, here is the function I'm using to extract the text.</p>\n\n<pre><code>def convert(fname, pages=None):\n    '''\n    Get text from pdf\n    '''\n    if not pages:\n        pagenums = set()\n    else:\n        pagenums = set(pages)\n\n    output = StringIO()\n    manager = PDFResourceManager()\n    converter = TextConverter(manager, output, laparams=LAParams())\n    interpreter = PDFPageInterpreter(manager, converter)\n\n    infile = file(fname, 'rb')\n    try:\n        for page in PDFPage.get_pages(infile, pagenums):\n            interpreter.process_page(page)\n    except PDFTextExtractionNotAllowed:\n        print 'This pdf won\\'t allow text extraction!'\n\n    infile.close()\n    converter.close()\n    text = output.getvalue()\n    output.close\n\n    return text\n</code></pre>\n\n<p>Is there any way I can programmatically solve this rather than manually re-exporting the files in preview?</p>\n",
    "score": 15,
    "creation_date": 1476202696,
    "view_count": 12305,
    "answer_count": 4,
    "tags": "python;pdf;text;nlp;pdfminer"
  },
  {
    "question_id": 22586658,
    "title": "How to train the Stanford NLP Sentiment Analysis tool",
    "body": "<p>Hell everyone! I'm using the Stanford Core NLP package and my goal is to perform sentiment analysis on a live-stream of tweets. </p>\n\n<p>Using the sentiment analysis tool as is returns a very poor analysis of text's 'attitude' .. many positives are labeled neutral, many negatives rated positive. I've gone ahead an acquired well over a million tweets in a text file, but I haven't a clue how to actually <em>train</em> the tool and create my own model.</p>\n\n<p><a href=\"http://nlp.stanford.edu/sentiment/code.html\">Link to Stanford Sentiment Analysis page</a></p>\n\n<p>\"Models can be retrained using the following command using the PTB format dataset:\"</p>\n\n<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath     dev.txt -train -model model.ser.gz\n</code></pre>\n\n<p>Sample from dev.txt (The leading 4 represents polarity out of 5 ... 4/5 positive)</p>\n\n<pre><code>(4 (4 (2 A) (4 (3 (3 warm) (2 ,)) (3 funny))) (3 (2 ,) (3 (4 (4 engaging) (2 film)) (2 .))))\n</code></pre>\n\n<p>Sample from test.txt</p>\n\n<pre><code>(3 (3 (2 If) (3 (2 you) (3 (2 sometimes) (2 (2 like) (3 (2 to) (3 (3 (2 go) (2 (2 to) (2 (2 the) (2 movies)))) (3 (2 to) (3 (2 have) (4 fun))))))))) (2 (2 ,) (2 (2 Wasabi) (3 (3 (2 is) (2 (2 a) (2 (3 good) (2 (2 place) (2 (2 to) (2 start)))))) (2 .)))))\n</code></pre>\n\n<p>Sample from train.txt</p>\n\n<pre><code>(3 (2 (2 The) (2 Rock)) (4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 Century) (2 's)) (2 (3 new) (2 (2 ``) (2 Conan)))))))) (2 '')) (2 and)) (3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash)) (2 (2 even) (3 greater)))) (2 (2 than) (2 (2 (2 (2 (1 (2 Arnold) (2 Schwarzenegger)) (2 ,)) (2 (2 Jean-Claud) (2 (2 Van) (2 Damme)))) (2 or)) (2 (2 Steven) (2 Segal))))))))))))) (2 .)))\n</code></pre>\n\n<p>I have two questions going forward.</p>\n\n<p>What is the significance and difference between each file? Train.txt/Dev.txt/Test.txt ?</p>\n\n<p>How would I train my own model with a raw, unparsed text file full of tweets?</p>\n\n<p>I'm very new to NLP so if I am missing any required information or anything at all please critique! Thank you!</p>\n",
    "score": 15,
    "creation_date": 1395544871,
    "view_count": 15020,
    "answer_count": 3,
    "tags": "java;nlp;stanford-nlp;sentiment-analysis"
  },
  {
    "question_id": 10046407,
    "title": "Logical fallacy detection and/or identification with natural-language-processing",
    "body": "<p>Is there a package or methodology in existence for the detection of flawed logical arguments in text? </p>\n\n<p>I was hoping for something that would work for text that is not written in an academic setting (such as a logic class). It might be a stretch but I would like something that can identify where logic is trying to be used and identify the logical error. A possible use for this would be marking errors in editorial articles.</p>\n\n<p>I don't need anything that is polished. I wouldn't mind working to develop something either so I'm really looking for what's out there in the wild now.</p>\n",
    "score": 15,
    "creation_date": 1333730397,
    "view_count": 3827,
    "answer_count": 3,
    "tags": "nlp;logic;machine-learning;sentiment-analysis"
  },
  {
    "question_id": 57059458,
    "title": "SQL: Most Overdue pair of numbers?",
    "body": "<p>We have a this table and random data load: </p>\n\n<pre><code>CREATE TABLE [dbo].[webscrape](\n    [id] [int] IDENTITY(1,1) NOT NULL,\n    [date] [date] NULL,\n    [value1] [int] NULL,\n    [value2] [int] NULL,\n    [value3] [int] NULL,\n    [value4] [int] NULL,\n    [value5] [int] NULL,\n    [sumnumbers] AS ([value1]+[value2]+[value3]+[value4]+[value5])\n) ON [PRIMARY]\n\n\ndeclare @date date = '1990-01-01',\n@endDate date = Getdate()\n\nwhile @date&lt;=@enddate\nbegin\ninsert into [dbo].[webscrape](date,value1,value2,value3,value4,value5)\nSELECT @date date,FLOOR(RAND()*(36-1)+1) value1,\nFLOOR(RAND()*(36-1)+1) value2,\nFLOOR(RAND()*(36-1)+1) value3,\nFLOOR(RAND()*(36-1)+1) value4,\nFLOOR(RAND()*(36-1)+1) value5\n\nset @date = DATEADD(day,1,@date)\nend\n\nselect * from [dbo].[webscrape] \n</code></pre>\n\n<p><strong><em>In SQL how can we return pair of values that have gone the longest without occurring on a given date?</em></strong></p>\n\n<p><strong><em>And (if you happen to know) in Power BI Q&amp;A NLP, how do we map so that so we can ask in natural language \"when have the most overdue pairs occurred?\"</em></strong></p>\n\n<p>Overdue being the pair of numbers with the longest stretch of time since occurring as of the given date. </p>\n\n<p>UPDATE:  I am trying this very ugly code.  Any ideas: </p>\n\n<pre><code>  select *\n    from (\n      select date,value1 number1,value2 number2 from webscrape union all  \n      select date,value1,value3 from webscrape union all\n      select date,value1,value4 from webscrape union all\n      select date,value1,value5 from webscrape union all\n      select date,value2,value3 from webscrape union all\n      select date,value2,value4 from webscrape union all\n      select date,value2,value5 from webscrape union all\n      select date,value3,value4 from webscrape union all\n      select date,value3,value5 from webscrape union all\n      select date,value4,value5 from webscrape \n\n    ) t order by date\n\n\n    ----------------------------------\n\n    select t.number1,t.number2, count(*)\n     as counter\n    from (\n      select value1 number1,value2 number2 from webscrape union all  \n      select value1,value3 from webscrape union all\n      select value1,value4  from webscrape union all\n      select value1,value5 from webscrape union all\n      select value2,value3 from webscrape union all\n      select value2,value4  from webscrape union all\n      select value2,value5 from webscrape union all\n      select value3,value4  from webscrape union all\n      select value3,value5 from webscrape union all\n      select value4,value5 from webscrape \n    ) t\n\ngroup by t.number1,number2\norder by counter\n</code></pre>\n\n<p>Thanks for any help.</p>\n",
    "score": 15,
    "creation_date": 1563286813,
    "view_count": 442,
    "answer_count": 1,
    "tags": "sql;sql-server;t-sql;nlp;powerbi"
  },
  {
    "question_id": 43596745,
    "title": "Python parse text from multiple txt file",
    "body": "<p>Seeking advice on how to mine items from multiple text files to build a dictionary. </p>\n\n<p>This text file: <a href=\"https://pastebin.com/Npcp3HCM\" rel=\"noreferrer\">https://pastebin.com/Npcp3HCM</a></p>\n\n<p>Was manually transformed into this required data structure: <a href=\"https://drive.google.com/file/d/0B2AJ7rliSQubV0J2Z0d0eXF3bW8/view\" rel=\"noreferrer\">https://drive.google.com/file/d/0B2AJ7rliSQubV0J2Z0d0eXF3bW8/view</a></p>\n\n<p>There are thousands of such text files and they may have different section headings as shown in these examples:</p>\n\n<ol>\n<li><a href=\"https://pastebin.com/wWSPGaLX\" rel=\"noreferrer\">https://pastebin.com/wWSPGaLX</a> </li>\n<li><a href=\"https://pastebin.com/9Up4RWHu\" rel=\"noreferrer\">https://pastebin.com/9Up4RWHu</a></li>\n</ol>\n\n<p>I started off by reading the files</p>\n\n<pre><code>from glob import glob\n\ntxtPth = '../tr-txt/*.txt'\ntxtFiles = glob(txtPth)\n\nwith open(txtFiles[0],'r') as tf:\n    allLines = [line.rstrip() for line in tf]\n\nsectionHeading = ['Corporate Participants',\n                  'Conference Call Participiants',\n                  'Presentation',\n                  'Questions and Answers']\n\nfor lineNum, line in enumerate(allLines):\n    if line in sectionHeading:\n        print(lineNum,allLines[lineNum])\n</code></pre>\n\n<p>My idea was to look for the line numbers where section headings existed and try to extract the content in between those line numbers, then strip out separators like dashes. That didn't work and I got stuck in trying to create a dictionary of this kind so that I can later run various natural language processing algorithms on quarried items. </p>\n\n<pre><code>{file-name1:{\n    {date-time:[string]},\n    {corporate-name:[string]},\n    {corporate-participants:[name1,name2,name3]},\n    {call-participants:[name4,name5]},\n    {section-headings:{\n        {heading1:[\n            {name1:[speechOrderNum, text-content]},\n            {name2:[speechOrderNum, text-content]},\n            {name3:[speechOrderNum, text-content]}],\n        {heading2:[\n            {name1:[speechOrderNum, text-content]},\n            {name2:[speechOrderNum, text-content]},\n            {name3:[speechOrderNum, text-content]},\n            {name2:[speechOrderNum, text-content]},\n            {name1:[speechOrderNum, text-content]},\n            {name4:[speechOrderNum, text-content]}],\n        {heading3:[text-content]},\n        {heading4:[text-content]}\n        }\n    }\n}\n</code></pre>\n\n<p>The challenge is that different files may have different headings and number of headings. But there will always be a section called \"Presentation\" and very likely to have \"Question and Answer\" section. These section headings are always separated by a string of equal-to signs. And content of different speaker is always separated by string of dashes. The \"speech order\" for Q&amp;A section is indicated with a number in square brackets. The participants are are always indicated in the beginning of the document with an asterisks before their name and their tile is always on the next line. </p>\n\n<p>Any suggestion on how to parse the text files is appreciated. The ideal help would be to provide guidance on how to produce such a dictionary (or other suitable data structure) for each file that can then be written to a database. </p>\n\n<p>Thanks</p>\n\n<p>--EDIT--</p>\n\n<p>One of the files looks like this: <a href=\"https://pastebin.com/MSvmHb2e\" rel=\"noreferrer\">https://pastebin.com/MSvmHb2e</a></p>\n\n<p>In which the \"Question &amp; Answer\" section is mislabeled as \"Presentation\" and there is no other \"Question &amp; Answer\" section. </p>\n\n<p>And final sample text: <a href=\"https://pastebin.com/jr9WfpV8\" rel=\"noreferrer\">https://pastebin.com/jr9WfpV8</a></p>\n",
    "score": 15,
    "creation_date": 1493064052,
    "view_count": 2881,
    "answer_count": 2,
    "tags": "python;parsing;dictionary;nlp"
  },
  {
    "question_id": 4072020,
    "title": "Synchronizing text and audio. Is there a NLP/speech-to-text library to do this?",
    "body": "<p>I would like to synchronize a spoken recording against a known text.  Is there a speech-to-text / natural language processing library that would facilitate this?  I imagine I'd want to detect word boundaries and compute candidate matches from a dictionary.  Most of the questions I've found on SO concern written language.</p>\n\n<p>Desired, but not required:</p>\n\n<ul>\n<li>Open Source</li>\n<li>Compatible with American English out-of-the-box</li>\n<li>Cross-platform</li>\n<li>Thoroughly documented</li>\n</ul>\n\n<p>Edit: I realize this is a very broad, even naive, question, so thanks in advance for your guidance.</p>\n\n<p>What I've found so far:</p>\n\n<ul>\n<li><a href=\"http://www.politepix.com/openears/\" rel=\"noreferrer\">OpenEars</a> (iOS Sphinx/Flite wrapper)</li>\n</ul>\n",
    "score": 15,
    "creation_date": 1288637181,
    "view_count": 5776,
    "answer_count": 1,
    "tags": "nlp;speech-recognition;pattern-recognition"
  },
  {
    "question_id": 40413866,
    "title": "How does gensim calculate doc2vec paragraph vectors",
    "body": "<p>i am going thorugh this paper <a href=\"http://cs.stanford.edu/~quocle/paragraph_vector.pdf\">http://cs.stanford.edu/~quocle/paragraph_vector.pdf</a></p>\n\n<p>and it states that</p>\n\n<blockquote>\n  <p>\" Theparagraph vector and word vectors are averaged or concatenated\n  to predict the next word in a context. In the experiments, we use\n  concatenation as the method to combine the vectors.\"</p>\n</blockquote>\n\n<p>How does concatenation or averaging work?</p>\n\n<p>example (if paragraph 1 contain word1 and word2):</p>\n\n<pre><code>word1 vector =[0.1,0.2,0.3]\nword2 vector =[0.4,0.5,0.6]\n\nconcat method \ndoes paragraph vector = [0.1+0.4,0.2+0.5,0.3+0.6] ?\n\nAverage method \ndoes paragraph vector = [(0.1+0.4)/2,(0.2+0.5)/2,(0.3+0.6)/2] ?\n</code></pre>\n\n<p>Also from this image:</p>\n\n<p>It is stated that :</p>\n\n<blockquote>\n  <p>The paragraph token can be thought of as another word. It acts as a\n  memory that remembers what is missing from the current context – or\n  the topic of the paragraph. For this reason, we often call this model\n  the Distributed Memory Model of Paragraph Vectors (PV-DM).</p>\n</blockquote>\n\n<p>Is the paragraph token equal to the paragraph vector which is equal to <code>on</code>?</p>\n\n<p><a href=\"https://i.sstatic.net/EQO9m.png\"><img src=\"https://i.sstatic.net/EQO9m.png\" alt=\"enter image description here\"></a></p>\n",
    "score": 15,
    "creation_date": 1478222282,
    "view_count": 2290,
    "answer_count": 2,
    "tags": "nlp;vectorization;gensim;word2vec;doc2vec"
  },
  {
    "question_id": 39224236,
    "title": "word2vec: CBOW &amp; skip-gram performance wrt training dataset size",
    "body": "<p>The question is simple. Which of the CBOW &amp; skip-gram works better for a big dataset? (And the answer for small dataset follows.)</p>\n\n<p>I am confused since, by Mikolov himself, <a href=\"https://groups.google.com/forum/#!searchin/word2vec-toolkit/c-bow/word2vec-toolkit/NLvYXU99cAM/E5ld8LcDxlAJ\" rel=\"noreferrer\">[Link]</a></p>\n\n<blockquote>\n  <p>Skip-gram: works well with <strong>small amount of the training data</strong>, represents well even rare words or phrases. <br/> <br/>\n  CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>\n</blockquote>\n\n<p>but, according to Google TensorFlow, <a href=\"https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/index.html\" rel=\"noreferrer\">[Link]</a></p>\n\n<blockquote>\n  <p>CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets.<br/><br/>However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have <strong>larger datasets</strong>. We will focus on the skip-gram model in the rest of this tutorial.</p>\n</blockquote>\n\n<p>Here is a Quora post which supports the first thought <a href=\"https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms\" rel=\"noreferrer\">[Link]</a>, and then there is the other Quora post which suggests the second thought <a href=\"https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms\" rel=\"noreferrer\">[Link]</a>--both seem derivable from the aforementioned credible sources.</p>\n\n<p>Or is it like what Mikolov said:</p>\n\n<blockquote>\n  <p>Overall, the best practice is to try few experiments and see what works the best for you, as different applications have different requirements.</p>\n</blockquote>\n\n<p>But surely there is an empirical or analytical verdict or final saying on this matter?</p>\n",
    "score": 15,
    "creation_date": 1472550659,
    "view_count": 5460,
    "answer_count": 1,
    "tags": "nlp;word2vec;word-embedding"
  },
  {
    "question_id": 34995139,
    "title": "NLP : Is Gazetteer a cheat",
    "body": "<p>In NLP there is a concept of <code>Gazetteer</code> which can be quite useful for creating annotations. As far as i understand, </p>\n\n<p><code>A gazetteer consists of a set of lists containing names of entities such as cities, organisations, days of the week, etc. These lists are used to ﬁnd occurrences of these names in text, e.g. for the task of named entity recognition.</code></p>\n\n<p>So it is essentially a lookup. Isn't this kind of a cheat? If we use a <code>Gazetteer</code> for detecting named entities, then there is not much <code>Natural Language Processing</code> going on. Ideally, i would want to detect named entities using <code>NLP</code> techniques. Otherwise how is it any better than a regex pattern matcher.</p>\n\n<p>Does that make sense?</p>\n",
    "score": 15,
    "creation_date": 1453732543,
    "view_count": 6613,
    "answer_count": 1,
    "tags": "nlp;named-entity-recognition"
  },
  {
    "question_id": 26569592,
    "title": "How to use vector representation of words (as obtained from Word2Vec,etc) as features for a classifier?",
    "body": "<p>I am familiar with using BOW features for text classification, wherein we first find the size of the vocabulary for the corpus which becomes the size of our feature vector. For each sentence/document, and for all its constituent words, we then put 0/1 depending on the absence/presence of that word in that sentence/document. </p>\n\n<p>However, now that I am trying to use vector representation of each word, is creating a global vocabulary essential? </p>\n",
    "score": 15,
    "creation_date": 1414295128,
    "view_count": 3619,
    "answer_count": 2,
    "tags": "text;vector;nlp;text-classification;word2vec"
  },
  {
    "question_id": 25228219,
    "title": "Clustering of news articles",
    "body": "<p>My scenario is pretty straightforwrd: I have a bunch of news articles (~1k at the moment) for which I know that some cover the same story/topic. I now would like to group these articles based on shared story/topic, i.e., based on their similarity.</p>\n\n<p>What I did so far is to apply basic NLP techniques including stopword removal and stemming. I also calculated the tf-idf vector for each article, and with this can also calculate the, e.g., cosine similarity based on these tf-idf-vectors. But now with the grouping of the articles I struggles a bit. I see two principle ways -- probably related -- to do it:</p>\n\n<p>1) Machine Learning / Clustering: I already played a bit with existing clustering libraries, with more or less success; see <a href=\"https://stackoverflow.com/questions/25217065/scikit-learn-clustering-text-documents-using-dbscan\">here</a>. On the one hand, algorithms such as k-means require the number of clusters as input, which I don't know. Other algorithms require parameters that are also not intuitive to specify (for me that is).</p>\n\n<p>2) Graph algorithms: I can represent my data as a graph with the articles being the nodes and weighted adges representing the pairwise (cosine) similarity between the articles. With that, for example, I can first remove all edges that fall below a certain threshold and then might apply graph algorithms to look for strongly-connected subgraphs.</p>\n\n<p>In short, I'm not sure where best to go from here -- I'm still pretty new in this area. I wonder if there some best practices for that, or some kind of guidelines which methods / algorithms can (not) be applied in certain scenarios.</p>\n\n<p>(EDIT: forgot to link to related question of mine)</p>\n",
    "score": 15,
    "creation_date": 1407670745,
    "view_count": 6218,
    "answer_count": 4,
    "tags": "machine-learning;nlp;cluster-analysis;information-retrieval;unsupervised-learning"
  },
  {
    "question_id": 4861619,
    "title": "A Viable Solution for Word Splitting Khmer?",
    "body": "<p>I am working on a solution to split long lines of Khmer (the Cambodian language) into individual words (in UTF-8).  Khmer does not use spaces between words.  There are a few solutions out there, but they are far from adequate (<a href=\"http://sourceforge.net/projects/khmer/files/Khmer%20Word%20Breaking/\" rel=\"nofollow noreferrer\">here</a> and <a href=\"http://www.panl10n.net/english/Outputs%20Phase%202/CCs/Cambodia/MoEYS/Software/2008/Windows/KhmerLineBreaking.zip\" rel=\"nofollow noreferrer\">here</a>), and those projects have fallen by the wayside.</p>\n\n<p>Here is a sample line of Khmer that needs to be split (they can be longer than this):</p>\n\n<blockquote>\n  <p>ចូរសរសើរដល់ទ្រង់ដែលទ្រង់បានប្រទានការទាំងអស់នោះមកដល់រូបអ្នកដោយព្រោះអង្គព្រះយេស៊ូវ ហើយដែលអ្នកមិនអាចរកការទាំងអស់នោះដោយសារការប្រព្រឹត្តរបស់អ្នកឡើយ។</p>\n</blockquote>\n\n<p>The goal of creating a viable solution that splits Khmer words is twofold: it will encourage those who used Khmer legacy (non-Unicode) fonts to convert over to Unicode (which has many benefits), and it will enable legacy Khmer fonts to be imported into Unicode to be used with a spelling checker quickly (rather than manually going through and splitting words which, with a large document, can take a very long time).</p>\n\n<p>I don't need 100% accuracy, but speed is important (especially since the line that needs to be split into Khmer words can be quite long).\nI am open to suggestions, but currently I have a large corpus of Khmer words that are correctly split (with a non-breaking space), and I have created a word probability dictionary file (frequency.csv) to use as a dictionary for the word splitter.</p>\n\n<p>I found this python code <a href=\"https://stackoverflow.com/questions/195010/how-can-i-split-multiple-joined-words\">here</a> that uses the <a href=\"http://en.wikipedia.org/wiki/Viterbi_algorithm\" rel=\"nofollow noreferrer\">Viterbi algorithm</a> and it supposedly runs fast.  </p>\n\n<pre><code>import re\nfrom itertools import groupby\n\ndef viterbi_segment(text):\n    probs, lasts = [1.0], [0]\n    for i in range(1, len(text) + 1):\n        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n                        for j in range(max(0, i - max_word_length), i))\n        probs.append(prob_k)\n        lasts.append(k)\n    words = []\n    i = len(text)\n    while 0 &lt; i:\n        words.append(text[lasts[i]:i])\n        i = lasts[i]\n    words.reverse()\n    return words, probs[-1]\n\ndef word_prob(word): return dictionary.get(word, 0) / total\ndef words(text): return re.findall('[a-z]+', text.lower()) \ndictionary = dict((w, len(list(ws)))\n                  for w, ws in groupby(sorted(words(open('big.txt').read()))))\nmax_word_length = max(map(len, dictionary))\ntotal = float(sum(dictionary.values()))\n</code></pre>\n\n<p>I also tried using the source java code from the author of this page: <a href=\"https://stackoverflow.com/questions/4580877/text-segmentation-dictionary-based-word-splitting\">Text segmentation: dictionary-based word splitting</a> but it ran too slow to be of any use (because my word probability dictionary has over 100k terms...).</p>\n\n<p>And here is another option in python from <a href=\"https://stackoverflow.com/questions/2174093/python-word-splitting\">Detect most likely words from text without spaces / combined words</a>:</p>\n\n<pre><code>WORD_FREQUENCIES = {\n    'file': 0.00123,\n    'files': 0.00124,\n    'save': 0.002,\n    'ave': 0.00001,\n    'as': 0.00555\n}\n\ndef split_text(text, word_frequencies, cache):\n    if text in cache:\n        return cache[text]\n    if not text:\n        return 1, []\n    best_freq, best_split = 0, []\n    for i in xrange(1, len(text) + 1):\n        word, remainder = text[:i], text[i:]\n        freq = word_frequencies.get(word, None)\n        if freq:\n            remainder_freq, remainder = split_text(\n                    remainder, word_frequencies, cache)\n            freq *= remainder_freq\n            if freq &gt; best_freq:\n                best_freq = freq\n                best_split = [word] + remainder\n    cache[text] = (best_freq, best_split)\n    return cache[text]\n\nprint split_text('filesaveas', WORD_FREQUENCIES, {})\n\n--&gt; (1.3653e-08, ['file', 'save', 'as'])\n</code></pre>\n\n<p>I am a newbee when it comes to python and I am really new to all real programming (outside of websites), so please bear with me.  Does anyone have any options that they feel would work well?</p>\n",
    "score": 15,
    "creation_date": 1296557330,
    "view_count": 2787,
    "answer_count": 3,
    "tags": "python;nlp;word-boundary;text-segmentation;southeast-asian-languages"
  },
  {
    "question_id": 12299724,
    "title": "List of Natural Language Processing Tools in Regards to Sentiment Analysis - Which one do you recommend",
    "body": "<p>first up sorry for my not so perfect English... I am from Germany ;) </p>\n\n<p>So, for a research project of mine (Bachelor thesis) I need to analyze the sentiment of tweets about certain companies and brands. For this purpose I will need to script my own program / use some sort of modified open source code (no APIs' - I need to understand what is happening). </p>\n\n<p>Below you will find a list of some of the NLP Applications I found. My Question now is which one and which approach would you recommend? And which one does not require long nights adjusting the code?</p>\n\n<p>For example: When I screen twitter for the music player >iPod&lt; and someone writes: \"It's a terrible day but at least my iPod makes me happy\" or even harder: \"It's a terrible day but at least my iPod makes up for it\" </p>\n\n<p>Which software is smart enough to understand that the focused is on iPod and not the weather? </p>\n\n<p>Also which software is scalable / resource efficient (I want to analyze several tweets and don't want to spend thousands of dollars)? </p>\n\n<p><strong>Machine learning and data mining</strong></p>\n\n<p><em>Weka</em> - is a collection of machine learning algorithms for data mining. It is one of the most popular text classification frameworks. It contains implementations of a wide variety of algorithms including Naive Bayes and Support Vector Machines (SVM, listed under SMO) [Note: Other commonly used non-Java SVM implementations are SVM-Light, LibSVM, and SVMTorch]. A related project is Kea (Keyphrase Extraction Algorithm) an algorithm for extracting keyphrases from text documents.</p>\n\n<p><em>Apache Lucene Mahout</em> - An incubator project to created highly scalable distributed implementations of common machine learning algorithms on top of the Hadoop map-reduce framework.</p>\n\n<p><strong>NLP Tools</strong></p>\n\n<p><em>LingPipe</em> - (not technically 'open-source, see below) Alias-I's Lingpipe is a suite of java tools for linguistic processing of text including entity extraction, speech tagging (pos) , clustering, classification, etc... It is one of the most mature and widely used open source NLP toolkits in industry. It is known for it's speed, stability, and scalability. One of its best features is the extensive collection of well-written tutorials to help you get started. They have a list of links to competition, both academic and industrial tools. Be sure to check out their blog. LingPipe is released under a royalty-free commercial license that includes the source code, but it's not technically 'open-source'.</p>\n\n<p><em>OpenNLP</em> - hosts a variety of java-based NLP tools which perform sentence detection, tokenization, part-of-speech tagging, chunking and parsing, named-entity detection, and co-reference analysis using the Maxent machine learning package.</p>\n\n<p><em>Stanford Parser and Part-of-Speech (POS) Tagger</em> - Java packages for sentence parsing and part of speech tagging from the Stanford NLP group. It has implementations of probabilistic natural language parsers, both highly optimized PCFG and lexicalized dependency parsers, and a lexicalized PCFG parser. It's has a full GNU GPL license.</p>\n\n<p><em>OpenFST</em> - A package for manipulating weighted finite state automata. These are often used to represented a probablistic model. They are used to model text for speech recognition, OCR error correction, machine translation, and a variety of other tasks. The library was developed by contributors from Google Research and NYU. It is a C++ library that is meant to be fast and scalable.</p>\n\n<p><em>NTLK</em> - The natural language toolkit is a tool for teaching and researching classification, clustering, speech tagging and parsing, and more. It contains a set of tutorials and data sets for experimentation. It is written by Steven Bird, from the University of Melbourne.</p>\n\n<p><em>Opinion Finder</em> - A system that performs subjectivity analysis, automatically identifying when opinions, sentiments, speculations and other private states are present in text. Specifically, OpinionFinder aims to identify subjective sentences and to mark various aspects of the subjectivity in these sentences, including the source (holder) of the subjectivity and words that are included in phrases expressing positive or negative sentiments.</p>\n\n<p><em>Tawlk/osae</em> - A python library for sentiment classification on social text. The end-goal is to have a simple library that \"just works\". It should have an easy barrier to entry and be thoroughly documented. We have acheived best accuracy using stopwords filtering with tweets collected on negwords.txt and poswords.txt</p>\n\n<p><em>GATE</em> - GATE is over 15 years old and is in active use for all types of computational task involving human language. GATE excels at text analysis of all shapes and sizes. From large corporations to small startups, from €multi-million research consortia to undergraduate projects, our user community is the largest and most diverse of any system of this type, and is spread across all but one of the continents1.</p>\n\n<p><em>textir</em> - A suite of tools for text and sentiment mining. This includes the ‘mnlm’ function, for sparse multinomial logistic regression, ‘pls’, a concise partial least squares routine, and the ‘topics’ function, for efficient estimation and dimension selection in latent topic models.</p>\n\n<p>NLP Toolsuite - The JULIE Lab here offers a comprehensive NLP tool suite for the application purposes of semantic search, information extraction and text mining. Most of our continuously expanding tool suite is based on machine learning methods and thus is domain- and language independent.</p>\n\n<p>...</p>\n\n<p>On a side note: Would you recommend the twitter streaming or the get API? </p>\n\n<p>As to me, I am a fan of python and java ;)</p>\n\n<p>Thanks a lot for your help!!!</p>\n",
    "score": 15,
    "creation_date": 1346933115,
    "view_count": 7267,
    "answer_count": 1,
    "tags": "twitter;nlp;nltk;sentiment-analysis"
  },
  {
    "question_id": 43158631,
    "title": "Can anyone explain how to get BIDMach&#39;s Word2vec to work?",
    "body": "<p>In a paper titled, \"<a href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363760\" rel=\"noreferrer\">Machine Learning at the Limit</a>,\" Canny, et. al. report substantial <a href=\"https://code.google.com/archive/p/word2vec/\" rel=\"noreferrer\">word2vec</a> processing speed improvements. </p>\n\n<p>I'm working with the <a href=\"https://github.com/BIDData/BIDMach\" rel=\"noreferrer\">BIDMach</a> library used in this paper, and cannot find any resource that explains how Word2Vec is implemented or how it should be used within this framework.</p>\n\n<p>There are several scripts in the repo:</p>\n\n<ul>\n<li><a href=\"https://github.com/BIDData/BIDMach/blob/master/scripts/getw2vdata.sh\" rel=\"noreferrer\">getw2vdata.sh</a></li>\n<li><a href=\"https://github.com/BIDData/BIDMach/blob/master/scripts/getw2vdata.ssc\" rel=\"noreferrer\">getwv2data.ssc</a></li>\n</ul>\n\n<p>I've tried running them (after building the referenced <code>tparse2.exe</code> file) with no success.</p>\n\n<p>I've tried modifying them to get them to run but have nothing but errors come back.</p>\n\n<p>I emailed the author, and posted <a href=\"https://github.com/BIDData/BIDMach/issues/96\" rel=\"noreferrer\">an issue on the github repo</a>, but have gotten nothing back. I only got somebody else having the same troubles, who says he got it to run but at much slower speeds than reported on newer GPU hardware.</p>\n\n<p>I've searched all over trying to find anyone that has used this library to achieve these speeds with no luck. There are multiple references floating around that point to this library as the fastest implementation out there, and cite the numbers in the paper:</p>\n\n<ul>\n<li><a href=\"https://pdfs.semanticscholar.org/cced/c38f68ffaf51cf8c31cd6c6b5c2cf033f91a.pdf\" rel=\"noreferrer\">Intel research references the reported numbers without running the code on GPU (they cite numbers reported in the original paper)</a></li>\n<li><a href=\"https://www.reddit.com/r/MachineLearning/comments/4p3enc/advice_library_for_training_word2vec_on_gpu/\" rel=\"noreferrer\">old reddit post pointing to BIDMach as the best</a> (but the OP says \"I haven't tested BIDMach myself yet\")</li>\n<li><a href=\"https://stackoverflow.com/questions/30573873/how-to-train-word2vec-on-very-large-datasets\">SO post citing BIDMach as the best</a> (OP doesn't actually run the library to make this claim...)</li>\n<li>many more not worth listing citing BIDMach as the best/fastest without example or claims of \"I haven't tested myself...\"</li>\n</ul>\n\n<p>When I search for a similiar library (gensim), and the <code>import</code> code required to run it, <a href=\"https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=%22from+gensim.models+import+Word2Vec%22&amp;*\" rel=\"noreferrer\">I find thousands of results and tutorials</a> but a similar search for the BIDMach code <a href=\"https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=%22import+BIDMach.networks.Word2Vec%22&amp;*\" rel=\"noreferrer\">yields only the BIDMach repo</a>.</p>\n\n<p>This BIDMach implementation certainly carries the reputation for being the best, but <strong>can anyone out there tell me how to use it</strong>?</p>\n\n<p>All I want to do is run a simple training process to compare it to a handful of other implementations on my own hardware.</p>\n\n<p>Every other implementation of this concept I can find either has works with the <a href=\"https://github.com/svn2github/word2vec/blob/master/demo-word.sh\" rel=\"noreferrer\">original shell script test file</a>, <a href=\"https://github.com/yindlib/cuda-word2vec\" rel=\"noreferrer\">provides actual instructions</a>, or <a href=\"https://github.com/IntelLabs/pWord2Vec/tree/master/sandbox\" rel=\"noreferrer\">provides shell scripts of their own</a> to <a href=\"https://github.com/facebookresearch/fastText/blob/master/word-vector-example.sh\" rel=\"noreferrer\">test</a>.</p>\n\n<hr>\n\n<p>UPDATE:\nThe author of the library has added additional shell scripts to get the previously mentioned scripts running, but exactly what they mean or how they work is still a total mystery and I can't understand how to get the word2vec training procedure to run on my own data.</p>\n\n<hr>\n\n<p><strong>EDIT (for bounty)</strong></p>\n\n<p>I'll give out the bounty to anywone that can explain how I'd use my own corpus (text8 would be great), and then train a model, and then save the ouput vectors and the vocabulary to files that can be read by <a href=\"https://bitbucket.org/omerlevy/hyperwords\" rel=\"noreferrer\">Omar Levy's Hyperwords</a>.</p>\n\n<p>This is exactly what the original C implementation would do with arguments <code>-binary 1 -output vectors.bin -save-vocab vocab.txt</code></p>\n\n<p>This is also what Intel's implementation does, and other CUDA implementations, etc, so this is a great way to generate something that can be easily compared with other versions...</p>\n\n<hr>\n\n<p><strong>UPDATE (bounty expired without answer)</strong>\nJohn Canny has updated a few scripts in the repo and added a <code>fmt.txt</code> file, thus making it possible to run test scripts that are package in the repo.</p>\n\n<p>However, my attempt to run this with the <strong>text8</strong> corpus yields near 0% accuracy on they hyperwords test.</p>\n\n<p>Running the training process on the billion word benchmark (which is what the repo scripts now do) also yields well-below-average accuracy on the hyperwords test.</p>\n\n<p>So, either the library never yielded accuracy on these tests, or I'm still missing something in my setup. </p>\n\n<p><a href=\"https://github.com/BIDData/BIDMach/issues/96\" rel=\"noreferrer\">The issue remains open on github</a>.</p>\n",
    "score": 15,
    "creation_date": 1491060304,
    "view_count": 453,
    "answer_count": 1,
    "tags": "machine-learning;nlp;word2vec"
  },
  {
    "question_id": 18902608,
    "title": "Generating the plural form of a noun",
    "body": "<p>Given a word, which may or may not be a singular-form noun, how would you generate its plural form?</p>\n\n<p>Based on this <a href=\"http://nltk.org/book/ch02.html\" rel=\"noreferrer\">NLTK tutorial</a> and this <a href=\"http://web2.uvcs.uvic.ca/elc/studyzone/330/grammar/irrplu.htm\" rel=\"noreferrer\">informal list</a> on pluralization rules, I wrote this simple function:</p>\n\n<pre><code>def plural(word):\n    \"\"\"\n    Converts a word to its plural form.\n    \"\"\"\n    if word in c.PLURALE_TANTUMS:\n        # defective nouns, fish, deer, etc\n        return word\n    elif word in c.IRREGULAR_NOUNS:\n        # foot-&gt;feet, person-&gt;people, etc\n        return c.IRREGULAR_NOUNS[word]\n    elif word.endswith('fe'):\n        # wolf -&gt; wolves\n        return word[:-2] + 'ves'\n    elif word.endswith('f'):\n        # knife -&gt; knives\n        return word[:-1] + 'ves'\n    elif word.endswith('o'):\n        # potato -&gt; potatoes\n        return word + 'es'\n    elif word.endswith('us'):\n        # cactus -&gt; cacti\n        return word[:-2] + 'i'\n    elif word.endswith('on'):\n        # criterion -&gt; criteria\n        return word[:-2] + 'a'\n    elif word.endswith('y'):\n        # community -&gt; communities\n        return word[:-1] + 'ies'\n    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n        return word + 'es'\n    elif word.endswith('an'):\n        return word[:-2] + 'en'\n    else:\n        return word + 's'\n</code></pre>\n\n<p>But I think this is incomplete. Is there a better way to do this?</p>\n",
    "score": 14,
    "creation_date": 1379616274,
    "view_count": 29747,
    "answer_count": 5,
    "tags": "python;nlp;wordnet;linguistics"
  },
  {
    "question_id": 47614999,
    "title": "How to stop NLTK from outputting to terminal when downloading data?",
    "body": "<p>When I run my program, which is using:</p>\n\n<pre><code>nltk.download('wordnet')\nfrom nltk.corpus import wordnet\n</code></pre>\n\n<p>I get the following output to my terminal:</p>\n\n<pre><code>[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/.../nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n</code></pre>\n\n<p>My program relies on not having this information saved to the terminal and a resulting output file, so how can I prevent the above lines from occurring, or write it to <code>sys.stderr</code> so it doesn't get included instead of it being through <code>print</code>?</p>\n",
    "score": 14,
    "creation_date": 1512272205,
    "view_count": 10687,
    "answer_count": 2,
    "tags": "python;nlp;nltk;stderr"
  },
  {
    "question_id": 5226202,
    "title": "Word frequencies from strings in Postgres?",
    "body": "<p>Is it possible to identify distinct words and a count for each, from fields containing text strings in Postgres? </p>\n",
    "score": 14,
    "creation_date": 1299538021,
    "view_count": 13840,
    "answer_count": 3,
    "tags": "postgresql;text;nlp;word-frequency"
  },
  {
    "question_id": 62181162,
    "title": "cannot import name &#39;open&#39; from &#39;smart_open&#39;",
    "body": "<p>I was doing this and got this error :</p>\n\n<pre><code>from gensim.models import Word2Vec\n\nImportError: cannot import name 'open' from 'smart_open' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\__init__.py)\n</code></pre>\n\n<p>Then I did this :</p>\n\n<pre><code>import smart_open\ndir(smart_open)\n\n['BZ2File','BytesIO','DEFAULT_ERRORS','IS_PY2','P','PATHLIB_SUPPORT','SSLError','SYSTEM_ENCODING','Uri','__builtins__','__cached__','__doc__','__file__','__loader__','__name__','__package__','__path__','__spec__','boto','codecs','collections','gzip','hdfs','http','importlib','io','logger','logging','os','pathlib','pathlib_module','requests','s3','s3_iter_bucket','six','smart_open','smart_open_hdfs','smart_open_http','smart_open_lib','smart_open_s3','smart_open_webhdfs','sys','urlparse','urlsplit','warnings','webhdfs']\n</code></pre>\n\n<p>As you can see there is no 'open' in it so how should I solve this. I tried to install different versions\nand I upgraded all version too.</p>\n",
    "score": 14,
    "creation_date": 1591212553,
    "view_count": 15701,
    "answer_count": 7,
    "tags": "deep-learning;nlp;importerror;gensim"
  },
  {
    "question_id": 65646925,
    "title": "How to train BERT from scratch on a new domain for both MLM and NSP?",
    "body": "<p>I’m trying to train BERT model from scratch using my own dataset using HuggingFace library. I would like to train the model in a way that it has the exact architecture of the original BERT model.</p>\n<p>In the original paper, it stated that: <em>“BERT is trained on two tasks: predicting randomly masked tokens (MLM) and predicting whether two sentences follow each other (NSP). SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text.”</em></p>\n<p>I’m trying to understand how to train the model on two tasks as above. At the moment, I initialised the model as below:</p>\n<pre><code>from transformers import BertForMaskedLM\nmodel = BertForMaskedLM(config=config)\n</code></pre>\n<p>However, it would just be for MLM and not NSP. How can I initialize and train the model with NSP as well or maybe my original approach was fine as it is?</p>\n<p>My assumptions would be either</p>\n<ol>\n<li><p>Initialize with <code>BertForPreTraining</code> (for both MLM and NSP), OR</p>\n</li>\n<li><p>After finish training with <code>BertForMaskedLM</code>,\ninitalize the same model and train again with\n<code>BertForNextSentencePrediction</code> (but this approach’s computation and\nresources would cost twice…)</p>\n</li>\n</ol>\n<p>I’m not sure which one is the correct way. Any insights or advice would be greatly appreciated.</p>\n",
    "score": 14,
    "creation_date": 1610221584,
    "view_count": 13239,
    "answer_count": 2,
    "tags": "deep-learning;nlp;bert-language-model;huggingface-transformers;transformer-model"
  },
  {
    "question_id": 47638877,
    "title": "Using PhraseMatcher in SpaCy to find multiple match types",
    "body": "<p>The SpaCy documentation and samples show that the PhraseMatcher class is useful to match sequences of tokens in documents. One must provide a vocabulary of sequences that will be matched.</p>\n\n<p>In my application, I have documents that are collections of tokens and phrases. There are entities of different types. The data is remotely natural language (documents are rather set of keywords with semi-random order). I am trying to find matches of multiple types.</p>\n\n<p>For example:</p>\n\n<pre><code>yellow boots for kids\n</code></pre>\n\n<p>How can I find the matches for colors (e.g. yellow), for product types (e.g. boots) and for the age (e.g. kids) using SpaCy's PhraseMatches? Is this a good use case? If the different entity matches overlap (e.g. color is matched in colors list and in materials list), is it possible to produce all unique cases?</p>\n\n<p>I cannot really use a sequence tagger as the data is loosely structured and is riddled with ambiguities. I have a list of entities (e.g. colors, ager, product types) and associated value lists.</p>\n\n<p>One idea would be to instantiate multiple PhraseMatcher objects, one for each entity, do the matches separately and then merge the results. Each entity type will get its own vocabulary. This sounds straightforward but can be not efficient, especially the merging part. The value lists are fairly large. Before going this route, I would like to know if this is a good idea or perhaps there are simpler ways to do that with SpaCy.</p>\n",
    "score": 14,
    "creation_date": 1512408769,
    "view_count": 18951,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 31543542,
    "title": "Hierarchical Dirichlet Process Gensim topic number independent of corpus size",
    "body": "<p>I am using the Gensim HDP module on a set of documents. </p>\n\n<pre><code>&gt;&gt;&gt; hdp = models.HdpModel(corpusB, id2word=dictionaryB)\n&gt;&gt;&gt; topics = hdp.print_topics(topics=-1, topn=20)\n&gt;&gt;&gt; len(topics)\n150\n&gt;&gt;&gt; hdp = models.HdpModel(corpusA, id2word=dictionaryA)\n&gt;&gt;&gt; topics = hdp.print_topics(topics=-1, topn=20)\n&gt;&gt;&gt; len(topics)\n150\n&gt;&gt;&gt; len(corpusA)\n1113\n&gt;&gt;&gt; len(corpusB)\n17\n</code></pre>\n\n<p>Why is the number of topics independent of corpus length?</p>\n",
    "score": 14,
    "creation_date": 1437492887,
    "view_count": 12655,
    "answer_count": 7,
    "tags": "python;nlp;lda;gensim"
  },
  {
    "question_id": 68461204,
    "title": "Continual pre-training vs. Fine-tuning a language model with MLM",
    "body": "<p>I have some custom data I want to use to <em><strong>further pre-train</strong></em> the BERT model. I’ve tried the two following approaches so far:</p>\n<ol>\n<li>Starting with a pre-trained BERT checkpoint and continuing the pre-training with Masked Language Modeling (<code>MLM</code>) + Next Sentence Prediction (<code>NSP</code>) heads (e.g. using <em><strong>BertForPreTraining</strong></em> model)</li>\n<li>Starting with a pre-trained BERT model with the <code>MLM</code> objective (e.g. using the <em><strong>BertForMaskedLM</strong></em> model assuming we don’t need NSP for the pretraining part.)</li>\n</ol>\n<p>But I’m still confused that if using either <em>BertForPreTraining</em> or <em>BertForMaskedLM</em> actually does the continual pre-training on BERT or these are just two models for fine-tuning that use MLM+NSP and MLM for fine-tuning BERT, respectively. Is there even any difference between fine-tuning BERT with MLM+NSP or continually pre-train it using these two heads or this is something we need to test?</p>\n<p>I've reviewed similar questions such as <a href=\"https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp\">this one</a> but still, I want to make sure that whether technically there's a difference between continual pre-training a model from an initial checkpoint and fine-tuning it using the same objective/head.</p>\n",
    "score": 14,
    "creation_date": 1626814364,
    "view_count": 19059,
    "answer_count": 3,
    "tags": "deep-learning;nlp;huggingface-transformers;bert-language-model;pre-trained-model"
  },
  {
    "question_id": 2339386,
    "title": "Python - pyparsing unicode characters",
    "body": "<p>:) I tried using w = Word(printables), but it isn't working. How should I give the spec for this. 'w' is meant to process Hindi characters (UTF-8)</p>\n\n<p>The code specifies the grammar and parses accordingly. </p>\n\n<pre><code>671.assess  :: अहसास  ::2\nx=number + \".\" + src + \"::\" + w + \"::\" + number + \".\" + number\n</code></pre>\n\n<p>If there is only english characters it is working so the code is correct for the ascii format but the code is not working for the unicode format.</p>\n\n<p>I mean that the code works when we have something of the form\n671.assess  :: ahsaas  ::2</p>\n\n<p>i.e. it parses words in the english format, but I am not sure how to parse and then print characters in the unicode format. I need this for English Hindi word alignment for purpose.</p>\n\n<p>The python code looks like this:</p>\n\n<pre><code># -*- coding: utf-8 -*-\nfrom pyparsing import Literal, Word, Optional, nums, alphas, ZeroOrMore, printables , Group , alphas8bit , \n# grammar \nsrc = Word(printables)\ntrans =  Word(printables)\nnumber = Word(nums)\nx=number + \".\" + src + \"::\" + trans + \"::\" + number + \".\" + number\n#parsing for eng-dict\nefiledata = open('b1aop_or_not_word.txt').read()\neresults = x.parseString(efiledata)\nedict1 = {}\nedict2 = {}\ncounter=0\nxx=list()\nfor result in eresults:\n  trans=\"\"#translation string\n  ew=\"\"#english word\n  xx=result[0]\n  ew=xx[2]\n  trans=xx[4]   \n  edict1 = { ew:trans }\n  edict2.update(edict1)\nprint len(edict2) #no of entries in the english dictionary\nprint \"edict2 has been created\"\nprint \"english dictionary\" , edict2 \n\n#parsing for hin-dict\nhfiledata = open('b1aop_or_not_word.txt').read()\nhresults = x.scanString(hfiledata)\nhdict1 = {}\nhdict2 = {}\ncounter=0\nfor result in hresults:\n  trans=\"\"#translation string\n  hw=\"\"#hin word\n  xx=result[0]  \n  hw=xx[2]\n  trans=xx[4]\n  #print trans\n  hdict1 = { trans:hw }\n  hdict2.update(hdict1)\n\nprint len(hdict2) #no of entries in the hindi dictionary\nprint\"hdict2 has been created\"\nprint \"hindi dictionary\" , hdict2\n'''\n#######################################################################################################################\n\ndef translate(d, ow, hinlist):\n   if ow in d.keys():#ow=old word d=dict\n    print ow , \"exists in the dictionary keys\"\n        transes = d[ow]\n    transes = transes.split()\n        print \"possible transes for\" , ow , \" = \", transes\n        for word in transes:\n            if word in hinlist:\n        print \"trans for\" , ow , \" = \", word\n                return word\n        return None\n   else:\n        print ow , \"absent\"\n        return None\n\nf = open('bidir','w')\n#lines = [\"'\\\n#5# 10 # and better performance in business in turn benefits consumers .  # 0 0 0 0 0 0 0 0 0 0 \\\n#5# 11 # vHyaapaar mEmn bEhtr kaam upbhOkHtaaomn kE lIe laabhpHrdd hOtaa hAI .  # 0 0 0 0 0 0 0 0 0 0 0 \\\n#'\"]\ndata=open('bi_full_2','rb').read()\nlines = data.split('!@#$%')\nloc=0\nfor line in lines:\n    eng, hin = [subline.split(' # ')\n                for subline in line.strip('\\n').split('\\n')]\n\n    for transdict, source, dest in [(edict2, eng, hin),\n                                    (hdict2, hin, eng)]:\n        sourcethings = source[2].split()\n        for word in source[1].split():\n            tl = dest[1].split()\n            otherword = translate(transdict, word, tl)\n            loc = source[1].split().index(word)\n            if otherword is not None:\n                otherword = otherword.strip()\n                print word, ' &lt;-&gt; ', otherword, 'meaning=good'\n                if otherword in dest[1].split():\n                    print word, ' &lt;-&gt; ', otherword, 'trans=good'\n                    sourcethings[loc] = str(\n                        dest[1].split().index(otherword) + 1)\n\n        source[2] = ' '.join(sourcethings)\n\n    eng = ' # '.join(eng)\n    hin = ' # '.join(hin)\n    f.write(eng+'\\n'+hin+'\\n\\n\\n')\nf.close()\n'''\n</code></pre>\n\n<p>if an example input sentence for the source file is:</p>\n\n<pre><code>1# 5 # modern markets : confident consumers  # 0 0 0 0 0 \n1# 6 # AddhUnIk baajaar : AshHvsHt upbhOkHtaa .  # 0 0 0 0 0 0 \n!@#$%\n</code></pre>\n\n<p>the ouptut would look like this :-</p>\n\n<pre><code>1# 5 # modern markets : confident consumers  # 1 2 3 4 5 \n1# 6 # AddhUnIk baajaar : AshHvsHt upbhOkHtaa .  # 1 2 3 4 5 0 \n!@#$%\n</code></pre>\n\n<p>Output Explanation:-\nThis achieves bidirectional alignment.\nIt means the first word of english 'modern' maps to the first word of hindi 'AddhUnIk' and vice versa. Here even characters are take as words as they also are an integral part of bidirectional mapping. Thus if you observe the hindi WORD '.' has a null alignment and it maps to nothing with respect to the English sentence as it doesn't have a full stop.\nThe 3rd line int the output basically represents a delimiter when we are working for a number of sentences for which your trying to achieve bidirectional mapping.</p>\n\n<p>What modification should i make for it to work if the I have the hindi sentences in Unicode(UTF-8) format.</p>\n",
    "score": 14,
    "creation_date": 1267156331,
    "view_count": 8192,
    "answer_count": 3,
    "tags": "python;unicode;nlp;pyparsing"
  },
  {
    "question_id": 17314506,
    "title": "Why do I need a tokenizer for each language?",
    "body": "<p>When processing text, why would one need a tokenizer specialized for the language? </p>\n\n<p>Wouldn't tokenizing by whitespace be enough? What are the cases where it is not good idea to use simply a white space tokenization?</p>\n",
    "score": 14,
    "creation_date": 1372233255,
    "view_count": 6641,
    "answer_count": 3,
    "tags": "text;lucene;nlp;semantics"
  },
  {
    "question_id": 36572221,
    "title": "How to find ngram frequency of a column in a pandas dataframe?",
    "body": "<p>Below is the input pandas dataframe I have.</p>\n\n<p><a href=\"https://i.sstatic.net/ltSrD.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ltSrD.png\" alt=\"enter image description here\"></a></p>\n\n<p>I want to find the frequency of unigrams &amp; bigrams. A sample of what I am expecting is shown below<a href=\"https://i.sstatic.net/7NOKk.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/7NOKk.png\" alt=\"enter image description here\"></a></p>\n\n<p>How to do this using nltk or scikit learn?</p>\n\n<p>I wrote the below code which takes a string as input. How to extend it to series/dataframe?</p>\n\n<pre><code>from nltk.collocations import *\ndesc='john is a guy person you him guy person you him'\ntokens = nltk.word_tokenize(desc)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\nfinder = BigramCollocationFinder.from_words(tokens)\nfinder.ngram_fd.viewitems()\n</code></pre>\n",
    "score": 14,
    "creation_date": 1460461169,
    "view_count": 16951,
    "answer_count": 1,
    "tags": "pandas;nlp;scikit-learn;nltk;text-mining"
  },
  {
    "question_id": 2163330,
    "title": "Difference between feature selection, feature extraction, feature weights ",
    "body": "<p>I am slightly confused as to what \"feature selection / extractor / weights\" mean and the difference between them. As I read the literature sometimes I feel lost as I find the term used quite loosely, my primary concerns are -- </p>\n\n<ol>\n<li><p>When people talk of Feature Frequency, Feature Presence - is it feature selection?</p></li>\n<li><p>When people talk of algorithms such as Information Gain, Maximum Entropy - is it still feature selection. </p></li>\n<li><p>If I train the classifier - with a feature set that asks the classifier to note the position of a word within a document as an example - would one still call this feature selection?</p></li>\n</ol>\n\n<p>Thanks\nRahul Dighe</p>\n",
    "score": 14,
    "creation_date": 1264782408,
    "view_count": 11439,
    "answer_count": 3,
    "tags": "parallel-processing;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 34102420,
    "title": "POS tagging using spaCy",
    "body": "<p>I am trying to do POS tagging using the spaCy module in Python. </p>\n\n<p>Here is my code for the same</p>\n\n<pre><code>from spacy.en import English, LOCAL_DATA_DIR\nimport spacy.en\nimport os\n\ndata_dir = os.environ.get('SPACY_DATA', LOCAL_DATA_DIR)\nnlp = English(parser=False, tagger=True, entity=False)\n\n\ndef print_fine_pos(token):\n    return (token.tag_)\n\ndef pos_tags(sentence):\n    sentence = unicode(sentence, \"utf-8\")\n    tokens = nlp(sentence)\n    tags = []\n    for tok in tokens:\n        tags.append((tok,print_fine_pos(tok)))\n    return tags\n\na = \"we had crispy dosa\"\nprint pos_tags(a)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[(We , u'PRP'), (had , u'VBD'), (crispy , u'NN'), (dosa, u'NN')]\n</code></pre>\n\n<p>Here it returns crispy as a noun instead of an adjective. However, if I use a test sentence like</p>\n\n<pre><code>a=\"we had crispy fries\"\n</code></pre>\n\n<p>It recognizes that crispy is an adjective. Here is the output:</p>\n\n<pre><code>[(we , u'PRP'), (had , u'VBD'), (crispy , u'JJ'), (fries, u'NNS')]\n</code></pre>\n\n<p>I think the primary reason why crispy wasn't tagged as an adjective in the first case was because dosa was tagged as 'NN' whereas fries was tagged as 'NNS' in the second case. </p>\n\n<p>Is there any way I can get crispy to be tagged as an adjective in the second case too?</p>\n",
    "score": 14,
    "creation_date": 1449300325,
    "view_count": 16945,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 32404666,
    "title": "Python - Generating the plural noun of a singular noun",
    "body": "<p>How could I use NLTK module to write both the noun's singular and plural form, or tell it not to differentiate between singular and plural when searching a txt file for a word? Can I use NLTK to make the program case insensitive?</p>\n",
    "score": 14,
    "creation_date": 1441392209,
    "view_count": 20122,
    "answer_count": 5,
    "tags": "python;nlp"
  },
  {
    "question_id": 13324144,
    "title": "How to know when to use a particular kind of Similarity index? Euclidean Distance vs. Pearson Correlation",
    "body": "<p>What are some of the deciding factors to take into consideration when choosing a similarity index. \nIn what cases is a Euclidean Distance preferred over Pearson and vice versa? </p>\n",
    "score": 14,
    "creation_date": 1352567493,
    "view_count": 15417,
    "answer_count": 2,
    "tags": "statistics;machine-learning;nlp;artificial-intelligence"
  },
  {
    "question_id": 15722802,
    "title": "How do I use python interface of Stanford NER(named entity recogniser)?",
    "body": "<p>I want to use Stanford NER in python using pyner library. Here is one basic code snippet.</p>\n\n<pre><code>import ner \ntagger = ner.HttpNER(host='localhost', port=80)\ntagger.get_entities(\"University of California is located in California, United States\")\n</code></pre>\n\n<p>When I run this on my local python console(IDLE). It should have given me an output like this</p>\n\n<pre><code>  {'LOCATION': ['California', 'United States'],\n 'ORGANIZATION': ['University of California']}\n</code></pre>\n\n<p>but when I execut this, it showed empty brackets. I am actually new to all this.</p>\n",
    "score": 14,
    "creation_date": 1364672915,
    "view_count": 9039,
    "answer_count": 1,
    "tags": "python-2.7;nlp;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 40173481,
    "title": "Celery message queue vs AWS Lambda task processing",
    "body": "<p>Currently I'm developing a system to analyse and visualise textual data based on NLP. </p>\n\n<p>The backend (<em>Python+Flask+AWS EC2</em>) handles the analysis, and uses an API to feed the result back to a frontend (<em>FLASK+D3+Heroku</em>) app that solely handles interactive visualisations.</p>\n\n<p>Right now the analysis in the prototype is a basic python function which means on large files the analysis take longer and thus resulting a request timeout during the API data bridging to frontend. As well as the analysis of many files is done in a linear blocking queue. </p>\n\n<p>So to scale this prototype, I need to modify the <code>Analysis(text)</code> function to be a background task so it does not block further execution and can do a callback once the function is done. The input text is fetched from AWS S3 and the output is a relatively large JSON format aiming to be stored in AWS S3 as well, so the API bridge will simply fetch this JSON that contains data for all the graphs in the frontend app. (I find S3 slightly easier to handle than creating a large relational database structure to store persistent data..)</p>\n\n<p>I'm doing simple examples with Celery and find it fitting as a solution, however i just did some reading in AWS Lambda which on paper seems like a better solution in terms of scaling...</p>\n\n<p>The <code>Analysis(text)</code> function uses a pre-built model and functions from relatively common NLP python packages. As my lack of experience in scaling a prototype I'd like to ask for your experiences and judgement of which solution would be most fitting for this scenario.</p>\n\n<p>Thank you :)</p>\n",
    "score": 14,
    "creation_date": 1477043449,
    "view_count": 11755,
    "answer_count": 1,
    "tags": "python-2.7;amazon-web-services;nlp;celery;aws-lambda"
  },
  {
    "question_id": 1033649,
    "title": "Very basic English grammar parser",
    "body": "<p>I'm writing a very basic parser(mostly just to better understand how they work) that takes a user's input of a select few words, detects whether the sentence structure is OK or Not OK, and outputs the result. The grammar is:</p>\n\n<p>Sentence:\nNoun Verb</p>\n\n<p>Article Sentence</p>\n\n<p>Sentence Conjunction Sentence</p>\n\n<p>Conjunction:\n\"and\"\n\"or\"\n\"but\"</p>\n\n<p>Noun:\n\"birds\"\n\"fish\"\n\"C++\"</p>\n\n<p>Verb:\n\"rules\"\n\"fly\"\n\"swim\"</p>\n\n<p>Article:\n\"the\"</p>\n\n<p>Writing the grammar was simple. It's implementing the code that is giving me some trouble. My psuedocode for it is:</p>\n\n<pre><code>main()\nget user input (string words;)\nwhile loop (cin &gt;&gt; words)\ncall sentence()\nend main()\n\nsentence()\ncall noun()\nif noun() call verb() (if verb is true return \"OK\" ???)(else \"not ok\"???)\nelse if not noun() call article()\n                if article() call sentence() (if sentence is true \"OK\"???)(else \"not\"?)\nelse if not noun() call conjunction()\n                   if sentence() conjunction() sentence() - no idea how to implement\n                                                             return \"OK\"\nelse \"not ok\"\n</code></pre>\n\n<p>So there is my extremely sloppy psuedo code. I have a few questions on implementing it.</p>\n\n<ol>\n<li><p>For the word functions (noun, verb, etc.) how should I go about checking if they are true? (as in checking if the user's input has birds, fish, fly, swim, etc.)</p></li>\n<li><p>How should I handle the conjunction call and the output?</p></li>\n<li><p>Should I handle the output from the main function or the call functions?</p></li>\n<li><p>None of the above questions matter if my psuedo code is completely wrong. Is there anything wrong with the basics?</p></li>\n</ol>\n\n<p>As an added note, I'm on a Chapter 6 exercise of Programming: Practice and Principles Using C++ so I'd prefer to use language syntax that I've already learned, so anything that falls into the category of advanced programming probably isn't very helpful. (The exercise specifically says not to use tokens, so count those out.)</p>\n\n<p>Thanks in advance</p>\n\n<p>Last Edit: In the book's public group I asked the same question and Bjarne Stroustrup commented back saying he put the exercise solution online. He basically had the input read into the sentence function and used if statements to return true or false. However, he didn't use articles so mine was much more complex. I guess if I've learned anything from this exercise its that when dealing with a lot of user input, tokenization is key (from what I know so far.) Here is my code for now. I may go back to it later because it is still very buggy and basically only returns if the sentence is OK and can't handle things like (noun, conjunction, sentence), but for now I'm moving on.</p>\n\n<pre><code>#include \"std_lib_facilities.h\"\n\nbool article(string words)\n{\n               if (words == \"the\")\n               return true;\n               else return false;        \n}\n\nbool verb(string words)\n{\n               if (words == \"rules\" || words == \"fly\" || words == \"swim\")\n               return true;\n               else return false;                   \n}\n\nbool noun(string words)\n{\n               if (words == \"birds\" || words == \"fish\" || words == \"c++\")\n               return true;\n               else return false;                   \n}\n\nbool conjunction(string words)\n{\n              if (words == \"and\" || words == \"but\" || words == \"or\")\n              return true;\n              else return false;                  \n}\n\nbool sentence()\n{\nstring w1;\nstring w2;\nstring w3;\nstring w4;\n\ncin &gt;&gt; w1;\nif (!noun(w1) &amp;&amp; !article(w1)) return false; // grammar of IFS!\n\ncin &gt;&gt; w2;\nif (noun(w1) &amp;&amp; !verb(w2)) return false;\nif (article(w1) &amp;&amp; !noun(w2)) return false;\n\ncin &gt;&gt; w3;\nif (noun(w1) &amp;&amp; verb(w2) &amp;&amp; (w3 == \".\")) return true;\nif (verb(w2) &amp;&amp; !conjunction(w3)) return false;\nif (noun(w2) &amp;&amp; !verb(w3)) return false;\nif (conjunction(w3)) return sentence();\n\ncin &gt;&gt; w4;\nif (article(w1) &amp;&amp; noun(w2) &amp;&amp; verb(w3) &amp;&amp; (w4 == \".\")) return true;\nif (!conjunction(w4)) return false;\nif (conjunction(w4)) return sentence();\n}\n\n\nint main()\n{                                   \ncout &lt;&lt; \"Enter sentence. Use space then period to end.\\n\";\n            bool test = sentence();\n            if (test)\n               cout &lt;&lt; \"OK\\n\";\n            else\n               cout &lt;&lt; \"not OK\\n\";\n</code></pre>\n\n<p>keep_window_open();\n    }</p>\n",
    "score": 14,
    "creation_date": 1245773821,
    "view_count": 10407,
    "answer_count": 9,
    "tags": "c++;parsing;nlp"
  },
  {
    "question_id": 608743,
    "title": "Strategies for recognizing proper nouns in NLP",
    "body": "<p>I'm interested in learning more about <a href=\"http://en.wikipedia.org/wiki/Natural_language_processing\" rel=\"noreferrer\">Natural Language Processing</a> (NLP) and am curious if there are currently any strategies for recognizing proper nouns in a text that aren't based on dictionary recognition? Also, could anyone explain or link to resources that explain the current dictionary-based methods? Who are the authoritative experts on NLP or what are the definitive resources on the subject?</p>\n",
    "score": 14,
    "creation_date": 1236124605,
    "view_count": 9087,
    "answer_count": 8,
    "tags": "nlp;named-entity-recognition;part-of-speech"
  },
  {
    "question_id": 26183145,
    "title": "How do I generate random text in NLTK 3.0?",
    "body": "<p>The generate method of nltk.text.Text seems to have been removed in NLTK 3.0.</p>\n\n<p>For example:</p>\n\n<pre><code>&gt;&gt;&gt; bible = nltk.corpus.gutenberg.words(u'bible-kjv.txt')\n&gt;&gt;&gt; bibleText = nltk.Text(bible)\n&gt;&gt;&gt; bibleText.generate()\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nAttributeError: 'Text' object has no attribute 'generate'\n</code></pre>\n\n<p>It may just be that I'm remembering wrongly how to do this, but everything I can find online seems to support the above method. Any ideas what I'm doing wrong?</p>\n",
    "score": 14,
    "creation_date": 1412354576,
    "view_count": 6963,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 8063334,
    "title": "Extract triplet subject, predicate, and object sentence",
    "body": "<p>I'm trying to extract triplet subject, predicate, and object from sentence.\nI need more references on how to do this.</p>\n",
    "score": 14,
    "creation_date": 1320833153,
    "view_count": 16857,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 2325588,
    "title": "Text similarity algorithm",
    "body": "<p>I have two subtitles files.\nI need a function that tells whether they represent the same text, or <em>the similar text</em></p>\n\n<p>Sometimes there are comments like \"The wind is blowing... the music is playing\" in one file only.\nBut 80% percent of the contents will be the same. The function must return TRUE (files represent the same text).\nAnd sometimes there are misspellings like 1 instead of l (one - L ) as here:\n<em>She 1eft the baggage</em>.\nOf course, it means function must return TRUE.</p>\n\n<p>My comments: <br>\nThe function should return percentage of the similarity of texts  - AGREE</p>\n\n<p>\"all the people were happy\" and \"all the people were not happy\" - here that'd be considered as a misspelling, so that'd be considered the same text. To be exact, the percentage the function returns will be lower, but high enough to say the phrases are similar</p>\n\n<p>Do consider whether you want to apply Levenshtein on a whole file or just a search string - not sure about Levenshtein, but the algorithm must be applied to the file as a whole. It'll be a very long string, though.</p>\n",
    "score": 14,
    "creation_date": 1267011266,
    "view_count": 17960,
    "answer_count": 6,
    "tags": "java;text;nlp;levenshtein-distance;similarity"
  },
  {
    "question_id": 18942096,
    "title": "How to conjugate a verb in NLTK given POS tag?",
    "body": "<p>Given a POS tag, such as VBD, how can I conjugate a verb to match with NLTK?</p>\n\n<p>e.g.</p>\n\n<pre><code>VERB: go\nPOS: VBD\nRESULT: went\n</code></pre>\n",
    "score": 14,
    "creation_date": 1379840728,
    "view_count": 8075,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 7331462,
    "title": "Check if a string is a possible abbrevation for a name",
    "body": "<p>I'm trying to develop a python algorithm to check if a string could be an abbrevation for another word. For example</p>\n\n<ul>\n<li><code>fck</code> is a match for <code>fc kopenhavn</code> because it matches the first characters of the word. <code>fhk</code> would not match.</li>\n<li><code>fco</code> should not match <code>fc kopenhavn</code> because no one irl would abbrevate FC Kopenhavn as FCO.</li>\n<li><code>irl</code> is a match for <code>in real life</code>.</li>\n<li><code>ifk</code> is a match for <code>ifk goteborg</code>.</li>\n<li><code>aik</code> is a match for <code>allmanna idrottskluben</code>.</li>\n<li><code>aid</code> is a match for <code>allmanna idrottsklubben</code>. This is not a real team name abbrevation, but I guess it is hard to exclude it unless you apply domain specific knowledge on how Swedish abbrevations are formed.</li>\n<li><code>manu</code> is a match for <code>manchester united</code>.</li>\n</ul>\n\n<p>It is hard to describe the exact rules of the algorithm, but I hope my examples show what I'm after.</p>\n\n<p><strong>Update</strong> I made a mistake in showing the strings with the matching letters uppercased. In the real scenario, all letters are lowercase so it is not as easy as just checking which letters are uppercased.</p>\n",
    "score": 14,
    "creation_date": 1315387202,
    "view_count": 14380,
    "answer_count": 5,
    "tags": "python;string-matching;slug;abbreviation;text-analysis"
  },
  {
    "question_id": 4011526,
    "title": "Medical information extraction using Python",
    "body": "<p>I am a nurse and I know python but I am not an expert, just used it to process DNA sequences<br>\nWe got hospital records written in human languages and I am supposed to insert these data into a database or csv file but they are more than 5000 lines and this can be so hard. All the data are written in a consistent format let me show you an example</p>\n\n<pre><code>11/11/2010 - 09:00am : He got nausea, vomiting and died 4 hours later\n</code></pre>\n\n<p>I should get the following data</p>\n\n<pre><code>Sex: Male\nSymptoms: Nausea\n    Vomiting\nDeath: True\nDeath Time: 11/11/2010 - 01:00pm\n</code></pre>\n\n<p>Another example</p>\n\n<pre><code>11/11/2010 - 09:00am : She got heart burn, vomiting of blood and died 1 hours later in the operation room\n</code></pre>\n\n<p>And I get</p>\n\n<pre><code>Sex: Female\nSymptoms: Heart burn\n    Vomiting of blood\nDeath: True\nDeath Time: 11/11/2010 - 10:00am\n</code></pre>\n\n<p>the order is not consistent by when I say in ....... so in is a keyword and all the text after is a place until i find another keyword<br>\nAt the beginnning He or She determine sex, got ........ whatever follows is a group of symptoms that i should split according to the separator which can be a comma, hypen or whatever but it's consistent for the same line<br>\ndied ..... hours later also should get how many hours, sometimes the patient is stil alive and discharged ....etc<br>\nThat's to say we have a lot of conventions and I think if i can tokenize the text with keywords and patterns i can get the job done. So please if you know a useful function/modules/tutorial/tool for doing that preferably in python (if not python so a gui tool would be nice)  </p>\n\n<p>Some few information:</p>\n\n<pre><code>there are a lot of rules to express various medical data but here are few examples\n- Start with the same date/time format followed by a space followd by a colon followed by a space followed by He/She followed space followed by rules separated by and\n- Rules:\n    * got &lt;symptoms&gt;,&lt;symptoms&gt;,....\n    * investigations were done &lt;investigation&gt;,&lt;investigation&gt;,&lt;investigation&gt;,......\n    * received &lt;drug or procedure&gt;,&lt;drug or procedure&gt;,.....\n    * discharged &lt;digit&gt; (hour|hours) later\n    * kept under observation\n    * died &lt;digit&gt; (hour|hours) later\n    * died &lt;digit&gt; (hour|hours) later in &lt;place&gt;\nother rules do exist but they follow the same idea\n</code></pre>\n",
    "score": 14,
    "creation_date": 1287973862,
    "view_count": 7816,
    "answer_count": 4,
    "tags": "python;parsing;machine-learning;nlp;information-extraction"
  },
  {
    "question_id": 50580262,
    "title": "How to use spaCy to create a new entity and learn only from keyword list",
    "body": "<p>I am trying to use <a href=\"https://spacy.io/\" rel=\"noreferrer\">spaCy</a> to create a new entity categorization 'Species' with a list of species names, example can he found <a href=\"https://a-z-animals.com/animals/scientific/\" rel=\"noreferrer\">here</a>.</p>\n<p>I found a tutorial for training new entity type from <a href=\"https://spacy.io/usage/training#example-new-entity-type\" rel=\"noreferrer\">this spaCy tutorial</a> (Github code <a href=\"https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py\" rel=\"noreferrer\">here</a>). However, the problem is, I don't want to manually create a sentence for each species name as it would be very time consuming.</p>\n<p>I created below training data, which looks like this:</p>\n<pre><code>TRAIN_DATA = [('Bombina',{'entities':[(0,6,'SPECIES')]}),\n ('Dermaptera',{'entities':[(0,9,'SPECIES')]}),\n  .... \n]\n</code></pre>\n<p>The way I created the training set is: instead of providing a full sentence and the location of the matched entity, I only provide the name of each species, and the start and end index are programmatically generated:</p>\n<blockquote>\n<p>[( 0, 6, 'SPECIES' )]</p>\n<p>[( 0, 9, 'SPECIES' )]</p>\n</blockquote>\n<p>Below training code is what I used to train the model. (Code copied from above hyperlink)</p>\n<pre><code>nlp = spacy.blank('en')  # create blank Language class\n\n # Add entity recognizer to model if it's not in the pipeline \n # nlp.create_pipe works for built-ins that are registered with spaCy \n if 'ner' not in nlp.pipe_names: \n     ner = nlp.create_pipe('ner') \n     nlp.add_pipe(ner) \n # otherwise, get it, so we can add labels to it \n else: \n     ner = nlp.get_pipe('ner') \n\n ner.add_label(LABEL)   # add new entity label to entity recognizer\n\n\n  if model is None: \n      optimizer = nlp.begin_training() \n  else: \n      # Note that 'begin_training' initializes the models, so it'll zero out \n      # existing entity types. \n      optimizer = nlp.entity.create_optimizer() \n\n     # get names of other pipes to disable them during training \n     other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] \n     with nlp.disable_pipes(*other_pipes):  # only train NER \n         for itn in range(n_iter): \n             random.shuffle(TRAIN_DATA) \n             losses = {} \n             for text, annotations in TRAIN_DATA: \n                 nlp.update([text], [annotations], sgd=optimizer, drop=0.35,  losses=losses) \n             print(losses) \n</code></pre>\n<p>I'm new to NLP and spaCy please let me know if I did it correctly or not. And why my attempt failed the training (when I ran it, it throws an error).</p>\n<hr />\n<p>[UPDATE]</p>\n<p>The reason I want to feed keyword only to the training model is that, ideally, I would hope the model to learn those key words first, and once it identifies a context which contains the keyword, it will learn the associated context, and therefore, enhance the current model.</p>\n<p>At the first glance, it is more like regex expression. But with more and more data feeding in, the model will continuous learn, and finally being able to identify new species names that previously not exists in the original training set.</p>\n<hr />\n<p>Thanks,\nKatie</p>\n",
    "score": 14,
    "creation_date": 1527584315,
    "view_count": 8504,
    "answer_count": 1,
    "tags": "python;python-3.x;machine-learning;nlp;spacy"
  },
  {
    "question_id": 14009330,
    "title": "How to use malt parser in python nltk",
    "body": "<p>As a part of my academic project I need to parse a bunch of arbitrary sentences into a dependency graph. After a searching a lot I got the solution that I can use Malt Parser for parsing text with its pre trained grammer.</p>\n\n<p>I have downloaded pre-trained model (engmalt.linear-1.7.mco) from <a href=\"http://www.maltparser.org/mco/mco.html\" rel=\"noreferrer\">http://www.maltparser.org/mco/mco.html</a>. BUt I don't know how to parse my sentences using this grammer file and malt parser (by the python interface for malt). I have downloaded latest version of malt parser (1.7.2) and moved it to '/usr/lib/' </p>\n\n<pre><code>import nltk; \nparser =nltk.parse.malt.MaltParser()\ntxt=\"This is a test sentence\"\nparser.train_from_file('/home/rohith/malt-1.7.2/engmalt.linear-1.7.mco')\nparser.raw_parse(txt)\n</code></pre>\n\n<p>after executing the last line the following eror message is dispalyed</p>\n\n<pre><code>Traceback (most recent call last):\nFile \"&lt;pyshell#7&gt;\", line 1, in &lt;module&gt;\nparser.raw_parse(txt)\nFile \"/usr/local/lib/python2.7/dist-packages/nltk-2.0b5-py2.7.egg/nltk/parse/malt.py\", line 88, in raw_parse\nreturn self.parse(words, verbose)\nFile \"/usr/local/lib/python2.7/dist-packages/nltk-2.0b5-py2.7.egg/nltk/parse/malt.py\", line 75, in parse\nreturn self.tagged_parse(taggedwords, verbose)\nFile \"/usr/local/lib/python2.7/dist-packages/nltk-2.0b5-py2.7.egg/nltk/parse/malt.py\", line 122, in tagged_parse\nreturn DependencyGraph.load(output_file)\nFile \"/usr/local/lib/python2.7/dist-packages/nltk-2.0b5-py2.7.egg/nltk/parse/dependencygraph.py\", line 121, in load\nreturn DependencyGraph(open(file).read())\nIOError: [Errno 2] No such file or directory: '/tmp/malt_output.conll'\n</code></pre>\n\n<p>Please help me to parse that sentence using this malt parser.</p>\n",
    "score": 14,
    "creation_date": 1356247082,
    "view_count": 10324,
    "answer_count": 1,
    "tags": "python;parsing;nlp;nltk"
  },
  {
    "question_id": 1967847,
    "title": "Detect Proper Nouns with WordNet?",
    "body": "<p>I'm using <a href=\"http://lyle.smu.edu/~tspell/jaws/index.html\" rel=\"noreferrer\">JAWS</a> to access <a href=\"http://wordnet.princeton.edu/\" rel=\"noreferrer\">WordNet</a>. Given a word, is there any way to detect if it is a proper noun? It looks like the synsets have pretty coarse lexical categories.</p>\n\n<p>To clarify, there is no context for the words - they are just presented individually. If a word could conceivably be used as a common noun, it is acceptable. So \"mark\" is fine, because although it could be someone's name it could also refer to a point. However, \"Africa\" is not.</p>\n",
    "score": 14,
    "creation_date": 1261970836,
    "view_count": 8222,
    "answer_count": 4,
    "tags": "java;nlp;wordnet"
  },
  {
    "question_id": 32740988,
    "title": "Multilingual NLTK for POS Tagging and Lemmatizer",
    "body": "<p>Recently I approached to the NLP and I tried to use <a href=\"http://www.nltk.org\" rel=\"noreferrer\">NLTK</a> and <a href=\"http://textblob.readthedocs.org\" rel=\"noreferrer\">TextBlob</a> for analyzing texts. I would like to develop an app that analyzes reviews made by travelers and so I have to manage a lot of texts written in different languages. I need to do two main operations: POS Tagging and lemmatization. I have seen that in NLTK there is a possibility to choice the the right language for sentences tokenization like this:</p>\n\n<pre><code>tokenizer = nltk.data.load('tokenizers/punkt/PY3/italian.pickle')\n</code></pre>\n\n<p>I haven't found the the right way to set the language for POS Tagging and Lemmatizer in different languages yet. How can I set the correct corpora/dictionary for non-english texts such as Italian, French, Spanish or German? I also see that there is a possibility to import the \"TreeBank\" or \"WordNet\" modules, but I don't understand how I can use them.  Otherwise, where can I find the respective corporas?</p>\n\n<p>Can you give me some suggestion or reference? Please take care that I'm not an expert of NLTK.</p>\n\n<p>Many Thanks. </p>\n",
    "score": 14,
    "creation_date": 1443014999,
    "view_count": 13853,
    "answer_count": 3,
    "tags": "python;nlp;nltk;pos-tagger;lemmatization"
  },
  {
    "question_id": 57960995,
    "title": "How are the TokenEmbeddings in BERT created?",
    "body": "<p>In the <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"noreferrer\">paper describing BERT</a>, there is this paragraph about WordPiece Embeddings. </p>\n\n<blockquote>\n  <p>We use WordPiece embeddings (Wu et al.,\n  2016) with a 30,000 token vocabulary. The first\n  token of every sequence is always a special classification\n  token ([CLS]). The final hidden state\n  corresponding to this token is used as the aggregate\n  sequence representation for classification\n  tasks. Sentence pairs are packed together into a\n  single sequence. We differentiate the sentences in\n  two ways. First, we separate them with a special\n  token ([SEP]). Second, we add a learned embedding\n  to every token indicating whether it belongs\n  to sentence A or sentence B. As shown in Figure 1,\n  we denote input embedding as E, the final hidden\n  vector of the special [CLS] token as C 2 RH,\n  and the final hidden vector for the ith input token\n  as Ti 2 RH.\n  For a given token, its input representation is\n  constructed by summing the corresponding token,\n  segment, and position embeddings. A visualization\n  of this construction can be seen in Figure 2.\n  <a href=\"https://i.sstatic.net/QCcYF.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/QCcYF.png\" alt=\"Fig 2 from the paper\"></a></p>\n</blockquote>\n\n<p>As I understand, WordPiece splits Words into wordpieces like #I #like #swim #ing, but it does not generate Embeddings. But I did not find anything in the paper and on other sources how those Token Embeddings are generated. Are they pretrained before the actual Pre-training? How? Or are they randomly initialized? </p>\n",
    "score": 14,
    "creation_date": 1568651397,
    "view_count": 10834,
    "answer_count": 2,
    "tags": "machine-learning;nlp;word-embedding"
  },
  {
    "question_id": 32674380,
    "title": "CountVectorizer: Vocabulary wasn&#39;t fitted",
    "body": "<p>I instantiated a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\" rel=\"noreferrer\"><code>sklearn.feature_extraction.text.CountVectorizer</code></a> object by passing a vocabulary through the <code>vocabulary</code> argument, but I get a <code>sklearn.utils.validation.NotFittedError: CountVectorizer - Vocabulary wasn't fitted.</code> error message. Why?</p>\n\n<p>Example:</p>\n\n<pre><code>import sklearn.feature_extraction\nimport numpy as np\nimport pickle\n\n# Save the vocabulary\nngram_size = 1\ndictionary_filepath = 'my_unigram_dictionary'\nvectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size), min_df=1)\n\ncorpus = ['This is the first document.',\n        'This is the second second document.',\n        'And the third one.',\n        'Is this the first document? This is right.',]\n\nvect = vectorizer.fit(corpus)\nprint('vect.get_feature_names(): {0}'.format(vect.get_feature_names()))\npickle.dump(vect.vocabulary_, open(dictionary_filepath, 'w'))\n\n# Load the vocabulary\nvocabulary_to_load = pickle.load(open(dictionary_filepath, 'r'))\nloaded_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size), min_df=1, vocabulary=vocabulary_to_load)\nprint('loaded_vectorizer.get_feature_names(): {0}'.format(loaded_vectorizer.get_feature_names()))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>vect.get_feature_names(): [u'and', u'document', u'first', u'is', u'one', u'right', u'second', u'the', u'third', u'this']\nTraceback (most recent call last):\n  File \"C:\\Users\\Francky\\Documents\\GitHub\\adobe\\dstc4\\test\\CountVectorizerSaveDic.py\", line 22, in &lt;module&gt;\n    print('loaded_vectorizer.get_feature_names(): {0}'.format(loaded_vectorizer.get_feature_names()))\n  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 890, in get_feature_names\n    self._check_vocabulary()\n  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 271, in _check_vocabulary\n    check_is_fitted(self, 'vocabulary_', msg=msg),\n  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 627, in check_is_fitted\n    raise NotFittedError(msg % {'name': type(estimator).__name__})\nsklearn.utils.validation.NotFittedError: CountVectorizer - Vocabulary wasn't fitted.\n</code></pre>\n",
    "score": 14,
    "creation_date": 1442707165,
    "view_count": 24795,
    "answer_count": 1,
    "tags": "python;nlp;scikit-learn"
  },
  {
    "question_id": 1080321,
    "title": "Open source spell check",
    "body": "<p>Was evaluating adding spell check to a product I own. As per my research the major decisions that need to be made: <BR></p>\n\n<ol start=\"2\">\n<li>The library to use. <BR></li>\n<li>Dictionary( this can be region specific, British english, American etc).<BR></li>\n<li>Exclusion lists. Anytime a typo is detected its possible that its not a typo but is \nverbiage specific to the user. At this point the users should be given the ability to<br>\nadd this to his custom exclusion list.<BR></li>\n<li>Besides a per user custom list also a list of exclusion based on the user space of the \nclients of the tool. That is terms/acronyms in the users work domain. For example FX will not be a typo for currency traders.</li>\n</ol>\n\n<p>The open questions I had are listed below and if I could get input into them that would be very useful.\nFor 1, I was thinking of hunspell, whcih is the open source library offered under MPL and is used by firefox and OpenOffice family of products. Any horror stories out there using this?\nAny grey areas with the licensing? The spell checking will happen on a windows client.</p>\n\n<p>Dictionaries are available from a variety of sources some free under MPL while some are not. Any suggestions on good sources for free dictionaries. </p>\n\n<p>Multi lingual support and what needs to be worked out to support them?</p>\n\n<p>For 4, how are custom dictionaries kept in sync with the server side and the clientside? The spell check needs to happen on the clientside so are they pushed down with the initial launch everytime or are they synced up ever so often?</p>\n",
    "score": 14,
    "creation_date": 1246643599,
    "view_count": 12920,
    "answer_count": 4,
    "tags": "nlp;spell-checking;languagetool"
  },
  {
    "question_id": 61962710,
    "title": "How to fine tune BERT on unlabeled data?",
    "body": "<p>I want to fine tune BERT on a specific domain. I have texts of that domain in text files. How can I use these to fine tune BERT?\nI am looking <a href=\"https://huggingface.co/transformers/model_doc/bert.html#bertforpretraining\" rel=\"noreferrer\">here</a> currently.</p>\n\n<p>My main objective is to get sentence embeddings using BERT.</p>\n",
    "score": 14,
    "creation_date": 1590176525,
    "view_count": 9682,
    "answer_count": 4,
    "tags": "nlp;pytorch;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 37886534,
    "title": "Extracting nationalities and countries from text",
    "body": "<p>I want to extract all country and nationality mentions from text using nltk, I used POS tagging to extract all GPE labeled tokens but the results were not satisfying. </p>\n\n<pre><code> abstract=\"Thyroid-associated orbitopathy (TO) is an autoimmune-mediated orbital inflammation that can lead to disfigurement and blindness. Multiple genetic loci have been associated with Graves' disease, but the genetic basis for TO is largely unknown. This study aimed to identify loci associated with TO in individuals with Graves' disease, using a genome-wide association scan (GWAS) for the first time to our knowledge in TO.Genome-wide association scan was performed on pooled DNA from an Australian Caucasian discovery cohort of 265 participants with Graves' disease and TO (cases) and 147 patients with Graves' disease without TO (controls). \"\n\n  sent = nltk.tokenize.wordpunct_tokenize(abstract)\n  pos_tag = nltk.pos_tag(sent)\n  nes = nltk.ne_chunk(pos_tag)\n  places = []\n  for ne in nes:\n      if type(ne) is nltk.tree.Tree:\n         if (ne.label() == 'GPE'):\n            places.append(u' '.join([i[0] for i in ne.leaves()]))\n      if len(places) == 0:\n          places.append(\"N/A\")\n</code></pre>\n\n<p>The results obtained are :</p>\n\n<pre><code>['Thyroid', 'Australian', 'Caucasian', 'Graves']\n</code></pre>\n\n<p>Some are nationalities but others are just nouns.</p>\n\n<p>So what am I doing wrong or is there another way to extract such info?</p>\n",
    "score": 14,
    "creation_date": 1466181870,
    "view_count": 12880,
    "answer_count": 4,
    "tags": "python;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 25332,
    "title": "What&#39;s a good natural language library to use for paraphrasing?",
    "body": "<p>I'm looking for an existing library to summarize or paraphrase content (I'm aiming at blog posts) - any experience with existing natural language processing libraries?</p>\n\n<p>I'm open to a variety of languages, so I'm more interested in the abilities &amp; accuracy.</p>\n",
    "score": 14,
    "creation_date": 1219611453,
    "view_count": 6486,
    "answer_count": 4,
    "tags": "language-agnostic;nlp"
  },
  {
    "question_id": 50828314,
    "title": "How does the Gensim Fasttext pre-trained model get vectors for out-of-vocabulary words?",
    "body": "<p>I am using gensim to load pre-trained fasttext model. I downloaded the English wikipedia trained model from fasttext <a href=\"https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\" rel=\"noreferrer\">website</a>. </p>\n\n<p>here is the code I wrote to load the pre-trained model: </p>\n\n<pre><code>from gensim.models import FastText as ft\nmodel=ft.load_fasttext_format(\"wiki.en.bin\")\n</code></pre>\n\n<p>I try to check if the following phrase exists in the vocal(which rare chance it would as these are pre-trained model). </p>\n\n<pre><code>print(\"internal executive\" in model.wv.vocab)\nprint(\"internal executive\" in model.wv)\n\nFalse\nTrue\n</code></pre>\n\n<p>So the phrase \"internal executive\" is not present in the vocabulary but we still have the word vector corresponding to that. </p>\n\n<pre><code>model.wv[\"internal executive\"]\nOut[46]:\narray([ 0.0210917 , -0.15233646, -0.1173932 , -0.06210957, -0.07288644,\n       -0.06304111,  0.07833624, -0.17026938, -0.21922196,  0.01146349,\n       -0.13639058,  0.17283678, -0.09251394, -0.17875175,  0.01339212,\n       -0.26683623,  0.05487974, -0.11843193, -0.01982722,  0.37037706,\n       -0.24370994,  0.14269598, -0.16363597,  0.00328478, -0.16560239,\n       -0.1450972 , -0.24787527, -0.01318423,  0.03277111,  0.16175713,\n       -0.19367714,  0.16955379,  0.1972683 ,  0.09044111,  0.01731548,\n       -0.0034324 , -0.04834719,  0.14321515,  0.01422525, -0.08803893,\n       -0.29411593, -0.1033244 ,  0.06278021,  0.16452256,  0.0650492 ,\n        0.1506474 , -0.14194389,  0.10778475,  0.16008648, -0.07853138,\n        0.2183501 , -0.25451994, -0.0345991 , -0.28843886,  0.19964759,\n       -0.10923116,  0.26665714, -0.02544454,  0.30637854,  0.04568949,\n       -0.04798719, -0.05769338,  0.25762403, -0.05158515, -0.04426906,\n       -0.19901046,  0.00894193, -0.17269588, -0.24747233, -0.19061406,\n        0.14322804, -0.10804397,  0.4002605 ,  0.01409482, -0.04675362,\n        0.10039093,  0.07260711, -0.0938239 , -0.20434211,  0.05741301,\n        0.07592541, -0.02921724,  0.21137556, -0.23188967, -0.23164661,\n       -0.4569614 ,  0.07434579,  0.10841205, -0.06514647,  0.01220404,\n        0.02679767,  0.11840229,  0.2247431 , -0.1946325 , -0.0990666 ,\n       -0.02524677,  0.0801085 ,  0.02437297,  0.00674876,  0.02088535,\n        0.21464555, -0.16240154,  0.20670174, -0.21640894,  0.03900698,\n        0.21772243,  0.01954809,  0.04541844,  0.18990673,  0.11806394,\n       -0.21336791, -0.10871669, -0.02197789, -0.13249406, -0.20440844,\n        0.1967368 ,  0.09804545,  0.1440366 , -0.08401451, -0.03715726,\n        0.27826542, -0.25195453, -0.16737154,  0.3561183 , -0.15756823,\n        0.06724873, -0.295487  ,  0.28395334, -0.04908851,  0.09448399,\n        0.10877471, -0.05020981, -0.24595442, -0.02822314,  0.17862654,\n        0.06452435, -0.15105674, -0.31911567,  0.08166212,  0.2634299 ,\n        0.17043628,  0.10063848,  0.0687021 , -0.12210461,  0.10803893,\n        0.13644943,  0.10755012, -0.09816817,  0.11873955, -0.03881042,\n        0.18548298, -0.04769253, -0.01511982, -0.08552645, -0.05218676,\n        0.05387992,  0.0497043 ,  0.06922272, -0.0089245 ,  0.24790663,\n        0.27209425, -0.04925154, -0.08621719,  0.15918174,  0.25831223,\n        0.01654229, -0.03617229, -0.13490392,  0.08033483,  0.34922174,\n       -0.01744722, -0.16894792, -0.10506647,  0.21708378, -0.22582002,\n        0.15625793, -0.10860757, -0.06058934, -0.25798836, -0.20142137,\n       -0.06613475, -0.08779443, -0.10732629,  0.05967236, -0.02455976,\n        0.2229451 , -0.19476262, -0.2720119 ,  0.03687386, -0.01220259,\n        0.07704347, -0.1674307 ,  0.2400516 ,  0.07338555, -0.2000631 ,\n        0.13897157, -0.04637206, -0.00874449, -0.32827383, -0.03435039,\n        0.41587186,  0.04643605,  0.03352945, -0.13700874,  0.16430037,\n       -0.13630766, -0.18546128, -0.04692861,  0.37308362, -0.30846512,\n        0.5535561 , -0.11573419,  0.2332801 , -0.07236694, -0.01018955,\n        0.05936847,  0.25877884, -0.2959846 , -0.13610311,  0.10905041,\n       -0.18220575,  0.06902339, -0.10624941,  0.33002165, -0.12087796,\n        0.06742091,  0.20762768, -0.34141317,  0.0884434 ,  0.11247049,\n        0.14748637,  0.13261876, -0.07357208, -0.11968047, -0.22124515,\n        0.12290633,  0.16602683,  0.01055585,  0.04445777, -0.11142147,\n        0.00004863,  0.22543314, -0.14342701, -0.23209116, -0.00003538,\n        0.19272381, -0.13767233,  0.04850799, -0.281997  ,  0.10343244,\n        0.16510887,  0.08671653, -0.24125539,  0.01201926,  0.0995285 ,\n        0.09807415, -0.06764816, -0.0206733 ,  0.04697794,  0.02000999,\n        0.05817033,  0.10478792,  0.0974884 , -0.01756372, -0.2466861 ,\n        0.02877498,  0.02499748, -0.00370895, -0.04728201,  0.00107118,\n       -0.21848503,  0.2033032 , -0.00076264,  0.03828803, -0.2929495 ,\n       -0.18218371,  0.00628893,  0.20586628,  0.2410889 ,  0.02364616,\n       -0.05220835, -0.07040054, -0.03744286, -0.06718048,  0.19264086,\n       -0.06490505,  0.27364203,  0.05527219, -0.27494466,  0.22256687,\n        0.10330909, -0.3076979 ,  0.04852265,  0.07411488,  0.23980476,\n        0.1590279 , -0.26712465,  0.07580928,  0.05644221, -0.18824042],\n</code></pre>\n\n<p>Now my confusion is that Fastext creates vectors for character ngrams of a word too. So for a word \"internal\" it will create vectors for all its character ngrams including the full word and then the final word vector for the word is the sum of its character ngrams. </p>\n\n<p>However, how it is still able to give me vector of a word or even the whole sentence? Isn't fastext vector is for a word and its ngram? So what are these vector I am seeing for the phrase when its clearly two words?</p>\n",
    "score": 14,
    "creation_date": 1528857199,
    "view_count": 11806,
    "answer_count": 1,
    "tags": "python;nlp;gensim;fasttext"
  },
  {
    "question_id": 42892617,
    "title": "Edit distance between two pandas columns",
    "body": "<p>I have a pandas DataFrame consisting of two columns of strings. I would like to create a third column containing the Edit Distance of the two columns.</p>\n\n<pre><code>from nltk.metrics import edit_distance    \ndf['edit'] = edit_distance(df['column1'], df['column2'])\n</code></pre>\n\n<p>For some reason this seems to go to some sort of infinite loop in the sense that it remains unresponsive for quite some time and then I have to terminate it manually. </p>\n\n<p>Any suggestions are welcome.</p>\n",
    "score": 14,
    "creation_date": 1489959556,
    "view_count": 9674,
    "answer_count": 1,
    "tags": "python;string;pandas;nlp;nltk"
  },
  {
    "question_id": 77452363,
    "title": "TypeError when chaining Runnables in LangChain: Expected a Runnable, callable or dict",
    "body": "<p>I'm working with LangChain to create a retrieval-based QA system. However, when I attempt to chain Runnables, I encounter a TypeError that I'm unable to resolve. The error occurs when I try to use the | (pipe) operator to chain a RunnablePassthrough with a custom prompt and a ChatOpenAI instance.</p>\n<p>Here is the error message I'm receiving:</p>\n<p><code>TypeError: Expected a Runnable, callable or dict. Instead got an unsupported type: &lt;class 'str'&gt; </code></p>\n<p>I've pinpointed the error to this part of the code:</p>\n<p><code>rag_chain = ( {&quot;context&quot;: context, &quot;question&quot;: RunnablePassthrough()} | rag_custom_prompt | llm ) </code></p>\n<p>I expect the RunnablePassthrough() to pass the context and question to the next step in the chain, but it seems to fail during the coercion to a Runnable.</p>\n<p>The following is pretty much my entire code:</p>\n<pre><code>## Convert the pdf into txt\ndef pdf_to_txt(inst_manuals):\n\n    txt = &quot;&quot;\n    for manual in inst_manuals:\n        reader = PdfReader(inst_manuals)\n        for page in reader.pages:\n            txt += page.extract_text()\n\n    return txt\n\n## Convert txt into chunks \ndef chunkify_txt(txt):\n\n    txt_splitter = CharacterTextSplitter(\n        separator= &quot;\\n&quot;,\n        chunk_size= 1000,\n        chunk_overlap= 200,\n        length_function= len\n    )\n\n    chunks = txt_splitter.split_text(txt)\n\n    return chunks\n\n## Obtain the vector store\ndef get_vector(chunks):\n    embeddings = OpenAIEmbeddings()\n\n    vectorstore = FAISS.from_texts(texts= chunks, embedding = embeddings)\n\n    return vectorstore\n\n## Retrieve useful info similar to user query\ndef retrieve(vectorstore, question):\n    logging.basicConfig()\n    logging.getLogger(&quot;langchain.retrievers.multi_query&quot;).setLevel(logging.INFO)\n\n    retriever_from_llm = MultiQueryRetriever.from_llm(\n        retriever=vectorstore.as_retriever(), llm=ChatOpenAI(temperature=0)\n    )\n    unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n    \n    print(f&quot;Number of unique documents retrieved: {len(unique_docs)}&quot;)\n    \n    return unique_docs\n    \n\n## Generate response for user query\n\ndef gen_resp(retriever, question):\n    llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;, temperature=0)\n    template = &quot;&quot;&quot;... [custom prompt template] ...&quot;&quot;&quot;\n    rag_custom_prompt = PromptTemplate.from_template(template)\n\n    context = &quot;\\n&quot;.join(doc.page_content for doc in retriever)\n\n    rag_chain = (\n        {&quot;context&quot;: context, &quot;question&quot;: RunnablePassthrough()} | rag_custom_prompt | llm\n    )\n\n    answer = rag_chain.invoke(question)\n\n    return answer\n\n</code></pre>\n<p>Has anyone encountered this before? Any suggestions on how to properly chain these Runnables or what I might be doing wrong?</p>\n<p>I have attempted the following:</p>\n<p>Using different retrievers (details of which could be provided upon request).\nI've checked the LangChain documentation for proper usage of Runnables and chaining operations.\nI've tried interchanging the context and question keys in the rag_chain dictionary to see if the order was the issue.</p>\n",
    "score": 14,
    "creation_date": 1699525780,
    "view_count": 32311,
    "answer_count": 8,
    "tags": "python;nlp;artificial-intelligence;langchain"
  },
  {
    "question_id": 11578533,
    "title": "metaphone versus soundex versus NYSIIS",
    "body": "<p>I'm trying to come up with an implicit spell checker that will use the mappings of input words to some kind of more general phonetic representation to account for typos that might occur, basically for a search bar that will automatically correct your spelling to a degree. Two things that I've been looking into are metaphone, nysiis and soundex, but I don't really know which would be better for this application. </p>\n\n<p>I would like there to be preferentially more matches than less matches, and I would like the matching to be a bit more general and so for that reason I was thinking of going with soundex which seems to be a more approximate mapping than the original metaphone, but I don't really know how large the difference in vagueness is. I know that nysiis is pretty similar to soundex, but I don't have a good idea of how similar they are, or how nysiis compares to metaphone.</p>\n\n<p>I am also looking for the solution that is quickest to execute. I know that these phonetic mappers are usually pretty quick, but I'm not sure which would be fastest, considering I would like to be able to check spelling without an increase in search time, speed is a consideration. Thoughts?</p>\n",
    "score": 14,
    "creation_date": 1342784100,
    "view_count": 10135,
    "answer_count": 1,
    "tags": "machine-learning;nlp;soundex;phonetics;metaphone"
  },
  {
    "question_id": 30195287,
    "title": "How to save Python NLTK alignment models for later use?",
    "body": "<p>In Python, I'm using <a href=\"http://www.nltk.org/api/nltk.align.html\" rel=\"nofollow\"><code>NLTK's alignment module</code></a> to create word alignments between parallel texts. Aligning bitexts can be a time-consuming process, especially when done over considerable corpora. It would be nice to do alignments in batch one day and use those alignments later on.</p>\n\n<pre><code>from nltk import IBMModel1 as ibm\nbiverses = [list of AlignedSent objects]\nmodel = ibm(biverses, 20)\n\nwith open(path + \"eng-taq_model.txt\", 'w') as f:\n    f.write(model.train(biverses, 20))  // makes empty file\n</code></pre>\n\n<p>Once I create a model, how can I (1) save it to disk and (2) reuse it later?</p>\n",
    "score": 14,
    "creation_date": 1431444351,
    "view_count": 2671,
    "answer_count": 4,
    "tags": "python;io;nlp;nltk;machine-translation"
  },
  {
    "question_id": 27604191,
    "title": "NLTK other language POS tagger",
    "body": "<p>I am using the nltk module in python and i am trying to use this for POS tagging different languages.</p>\n\n<p>There is a lot of information on how to train your own POS tagger in different languages - is there a database of really robust well built and tested NLTK POS taggers for different languages?\n(It is quite easy to export POS taggers using the pickle module)</p>\n",
    "score": 14,
    "creation_date": 1419256829,
    "view_count": 10786,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 39075339,
    "title": "Syntaxnet / Parsey McParseface python API",
    "body": "<p>I've installed syntaxnet and am able to run the parser with the provided demo script. Ideally, I would like to run it directly from python.\nThe only code I found was this:</p>\n\n<pre><code>import subprocess\nimport os\nos.chdir(r\"../models/syntaxnet\")\nsubprocess.call([    \n\"echo 'Bob brought the pizza to Alice.' | syntaxnet/demo.sh\"\n], shell = True)\n</code></pre>\n\n<p>which is a complete disaster - inefficient and over-complex (calling python from python should be done with python).</p>\n\n<p>How can I call the python APIs directly, without going through shell scripts, standard I/O, etc?</p>\n\n<p>EDIT - <strong>Why isn't this as easy as opening syntaxnet/demo.sh and reading it?</strong></p>\n\n<p>This shell script calls two python scripts (parser_eval and conll2tree) which are written as python scripts and can't be imported into a python module without causing multiple errors. A closer look yields additional script-like layers and native code. These upper layers need to be refactored in order to run the whole thing in a python context. Hasn't anyone forked syntaxnet with such a modification or intend to do so?</p>\n",
    "score": 14,
    "creation_date": 1471856655,
    "view_count": 4469,
    "answer_count": 4,
    "tags": "python;nlp;syntaxnet;parsey-mcparseface"
  },
  {
    "question_id": 45735357,
    "title": "What is UNK Token in Vector Representation of Words",
    "body": "<pre><code># Step 2: Build the dictionary and replace rare words with UNK token.\nvocabulary_size = 50000\n\n\ndef build_dataset(words, n_words):\n  \"\"\"Process raw inputs into a dataset.\"\"\"\n  count = [['UNK', -1]]\n  count.extend(collections.Counter(words).most_common(n_words - 1))\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    if word in dictionary:\n      index = dictionary[word]\n    else:\n      index = 0  # dictionary['UNK']\n      unk_count += 1\n    data.append(index)\n  count[0][1] = unk_count\n  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n  return data, count, dictionary, reversed_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n                                                            vocabulary_size)\n</code></pre>\n\n<p>I am learning the elementary example of Vector Representation of Words using Tensorflow.</p>\n\n<p>This Step 2 is titled as \"Build the dictionary and replace rare words with UNK token\", however, there's no prior defining process of what \"UNK\" refers to.</p>\n\n<p>To specify the question:</p>\n\n<p>0) What does UNK generally refer to in NLP?</p>\n\n<p>1) What does count = [['UNK', -1]] mean? I know the bracket [] refer to list in python, however, why do we collocating it with -1?</p>\n",
    "score": 14,
    "creation_date": 1502973508,
    "view_count": 20476,
    "answer_count": 1,
    "tags": "tensorflow;nlp"
  },
  {
    "question_id": 47524602,
    "title": "Wordcloud is cropping text",
    "body": "<p>I am using twitter API to generate sentiments. I am trying to generate a word-cloud based on tweets. </p>\n\n<p>Here is my code to generate a wordcloud </p>\n\n<pre><code>wordcloud(clean.tweets, random.order=F,max.words=80, col=rainbow(50), scale=c(3.5,1))\n</code></pre>\n\n<p>Result for this:</p>\n\n<p><a href=\"https://i.sstatic.net/BAfJE.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/BAfJE.png\" alt=\"enter image description here\"></a></p>\n\n<p>I also tried this:</p>\n\n<pre><code>pal &lt;- brewer.pal(8,\"Dark2\")\n\nwordcloud(clean.tweets,min.freq = 125,max.words = Inf,random.order  = TRUE,colors = pal)\n</code></pre>\n\n<p>Result for this:</p>\n\n<p><a href=\"https://i.sstatic.net/7d9bT.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/7d9bT.png\" alt=\"enter image description here\"></a></p>\n\n<p>Am I missing something?</p>\n\n<p>This is how I am getting and cleaning tweets:</p>\n\n<pre><code>#downloading tweets\ntweets &lt;- searchTwitter(\"#hanshtag\",n = 5000, lang = \"en\",resultType = \"recent\")\n# removing re tweets \nno_retweets &lt;- strip_retweets(tweets , strip_manual = TRUE)\n\n#converts to data frame\ndf &lt;- do.call(\"rbind\", lapply(no_retweets , as.data.frame))\n\n#remove odd characters\ndf$text &lt;- sapply(df$text,function(row) iconv(row, \"latin1\", \"ASCII\", sub=\"\")) #remove emoticon\ndf$text = gsub(\"(f|ht)tp(s?)://(.*)[.][a-z]+\", \"\", df$text) #remove URL\nsample &lt;- df$text\n\n\n    # Cleaning Tweets \n    sum_txt1 &lt;- gsub(\"(RT|via)((?:\\\\b\\\\w*@\\\\w+)+)\",\"\",sample)\n    sum_txt2 &lt;- gsub(\"http[^[:blank:]]+\",\"\",sum_txt1)\n    sum_tx3 &lt;- gsub(\"@\\\\w+\",\"\",sum_txt2)\n    sum_tx4 &lt;- gsub(\"[[:punct:]]\",\" \", sum_tx3)\n    sum_tex5 &lt;- gsub(\"[^[:alnum:]]\", \" \", sum_tx4)\n    sum_tx6 &lt;- gsub(\"RT  \",\"\", sum_tex5)\n\n    # WordCloud\n\n    # data frame is not good for text convert it corpus\n    corpus &lt;- Corpus(VectorSource(sum_tx6))\n    clean.tweets&lt;- tm_map(corpus , content_transformer(tolower)) #converting everything to lower cases\n    clean.tweets&lt;- tm_map(guj_clean,removeWords, stopwords(\"english\")) #stopword are words like of, the, a, as..\n    clean.tweets&lt;- tm_map(guj_clean, removeNumbers)\n    clean.tweets&lt;- tm_map(guj_clean, stripWhitespace)\n</code></pre>\n\n<p>Thanks in advance!</p>\n",
    "score": 14,
    "creation_date": 1511847075,
    "view_count": 8116,
    "answer_count": 3,
    "tags": "r;text-analysis;word-cloud;sttwitterapi"
  },
  {
    "question_id": 10419656,
    "title": "Natural language generator for dates (Java)",
    "body": "<p>I'm building a system that needs to provide a commentary on things in natural English. One thing that is of use is to be able to express dates in a casual format. What I'm looking for is essentially the inverse of <a href=\"https://github.com/samtingleff/jchronic\" rel=\"nofollow noreferrer\">Chronic</a>, <a href=\"http://natty.joestelmach.com/\" rel=\"nofollow noreferrer\">Natty</a>, or the task described in this question: <a href=\"https://stackoverflow.com/questions/1410408/natural-language-date-and-time-parser-for-java\">Natural Language date and time parser for java</a>.</p>\n\n<p>Is this too out-there to have been done? Should I try and roll my own simple hardwired piece for the date ranges that make sense to me? Or is there some clever way to reverse existing parsers to spit out (even garbled) sentences describing dates?</p>\n\n<p>EDIT - To clarify, although <em>any</em> kind of output is interesting and useful, I'm particularly interested in varied/creative output generation. i.e. \"Next week\", \"seven days from now\", \"next Thursday\" and \"late next week\" all for the same date.</p>\n",
    "score": 14,
    "creation_date": 1335983669,
    "view_count": 1251,
    "answer_count": 1,
    "tags": "java;date;nlp"
  },
  {
    "question_id": 7643512,
    "title": "NLP and Machine learning for sentiment analysis",
    "body": "<p>I'm trying to write a program that takes text(article) as input and outputs the polarity of this text, weather its a positive or a negative sentiment. I've read extensively about different approaches but i am still confused. I read about many techniques like classifiers and machine learning. I would like direction and clear instructions on where to start. For example, i have a classifier which requires a dataset but how do i convert the text(article) into a dataset for the classifier. If anyone can tell me the logical sequence to approach this problem that would be greet. Thanks in advance!\nPS: please mention any related algorithms or open-source implementation</p>\n\n<p>Regards,\nMike</p>\n",
    "score": 14,
    "creation_date": 1317704750,
    "view_count": 4185,
    "answer_count": 5,
    "tags": "artificial-intelligence;nlp;machine-learning;data-mining;classification"
  },
  {
    "question_id": 2511876,
    "title": "Open Source Library for Linguistic Inquiry and Word Count (LIWC)",
    "body": "<p>I am looking for an open source library for Linguistic Inquiry and Word Count <a href=\"http://liwc.net/index.php\" rel=\"noreferrer\">(LIWC)</a>. Something in java or python will be good, though I am open to use other language.</p>\n\n<p>Does anyone know where I can get one ?</p>\n\n<p>Cheers,</p>\n",
    "score": 14,
    "creation_date": 1269469461,
    "view_count": 12256,
    "answer_count": 2,
    "tags": "java;python;open-source;nlp"
  },
  {
    "question_id": 47692906,
    "title": "FastText using pre-trained word vector for text classification",
    "body": "<p>I am working on a text classification problem, that is, given some text, I need to assign to it certain given labels.</p>\n\n<p>I have tried using fast-text library by Facebook, which has two utilities of interest to me:</p>\n\n<p>A) Word Vectors with pre-trained models</p>\n\n<p>B) Text Classification utilities</p>\n\n<p>However, it seems that these are completely independent tools as I have been unable to find any tutorials that merge these two utilities.</p>\n\n<p>What I want is to be able to classify some text, by taking advantage of the pre-trained models of the Word-Vectors. Is there any way to do this?</p>\n",
    "score": 13,
    "creation_date": 1512642512,
    "view_count": 14078,
    "answer_count": 2,
    "tags": "nlp;word2vec;text-classification;fasttext"
  },
  {
    "question_id": 33091397,
    "title": "Sparse Efficiency Warning while changing the column",
    "body": "<pre><code>def tdm_modify(feature_names,tdm):\n    non_useful_words=['kill','stampede','trigger','cause','death','hospital'\\\n        ,'minister','said','told','say','injury','victim','report']\n    indexes=[feature_names.index(word) for word in non_useful_words]\n    for index in indexes:\n        tdm[:,index]=0   \n    return tdm\n</code></pre>\n\n<p>I want to manually set zero weights for some terms in tdm matrix. Using the above code I get the warning. I don't seem to understand why? Is there a better way to do this?</p>\n\n<pre><code>C:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\compressed.py:730: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n  SparseEfficiencyWarning)\n</code></pre>\n",
    "score": 13,
    "creation_date": 1444689916,
    "view_count": 17526,
    "answer_count": 2,
    "tags": "python;numpy;scipy;nlp"
  },
  {
    "question_id": 7627170,
    "title": "How do I replace the string exactly using gsub()",
    "body": "<p>I have a corpus:\ntxt = \"a patterned layer within a microelectronic pattern.\"\nI would like to replace the term \"pattern\" exactly by \"form\", I try to write a code:</p>\n\n<pre><code>txt_replaced = gsub(\"pattern\",\"form\",txt)\n</code></pre>\n\n<p>However, the responsed corpus in txt_replaced is:\n\"a formed layer within a microelectronic form.\"</p>\n\n<p>As you can see, the term \"patterned\" is wrongly replaced by \"formed\" because parts of characteristics in \"patterned\" matched to \"pattern\".</p>\n\n<p>I would like to query that if I can replace the string exactly using gsub()?\nThat is, only the term with exactly match should be replaced.</p>\n\n<p>I thirst for a responsed as below:\n\"a patterned layer within a microelectronic form.\"</p>\n\n<p>Many thanks!</p>\n",
    "score": 13,
    "creation_date": 1317568313,
    "view_count": 20188,
    "answer_count": 1,
    "tags": "r;nlp"
  },
  {
    "question_id": 66367447,
    "title": "SpaCy can&#39;t find table(s) lexeme_norm for language &#39;en&#39; in spacy-lookups-data",
    "body": "<p>I am trying to train a text categorization pipe in SpaCy:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\nnlp.add_pipe(&quot;textcat&quot;, last=True)\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\nwith nlp.disable_pipes(*other_pipes):\n    optimizer = nlp.begin_training()\n    # training logic\n</code></pre>\n<p>However, every time I call <code>nlp.begin_training()</code>, I get the error</p>\n<pre><code>ValueError: [E955] Can't find table(s) lexeme_norm for language 'en' in spacy-lookups-data. Make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language.\n</code></pre>\n<p>Running <code>python3 -m spacy validate</code> returns</p>\n<pre><code>✔ Loaded compatibility table\n\n================= Installed pipeline packages (spaCy v3.0.3) =================\nℹ spaCy installation:\n/xxx/xxx/xxx/env/lib/python3.8/site-packages/spacy\n\nNAME             SPACY            VERSION                            \nen_core_web_lg   &gt;=3.0.0,&lt;3.1.0   3.0.0   ✔\nen_core_web_sm   &gt;=3.0.0,&lt;3.1.0   3.0.0   ✔\n</code></pre>\n<p>Furthermore, I have tried installing <code>spacy-lookups-data</code> without success.</p>\n<p>How can I resolve this error?</p>\n",
    "score": 13,
    "creation_date": 1614251822,
    "view_count": 11004,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 46326173,
    "title": "Understanding LDA / topic modelling -- too much topic overlap",
    "body": "<p>I'm new to topic modelling / Latent Dirichlet Allocation and have trouble understanding how I can apply the concept to my dataset (or whether it's the correct approach).</p>\n\n<p>I have a small number of literary texts (novels) and would like to extract some general topics using LDA.</p>\n\n<p>I'm using the <code>gensim</code> module in Python along with some <code>nltk</code> features. For a test I've split up my original texts (just 6) into 30 chunks with 1000 words each. Then I converted the chunks into document-term matrices and ran the algorithm. This is the code (although I think it doesn't matter for the question) :</p>\n\n<pre><code># chunks is a 30x1000 words matrix\n\ndictionary = gensim.corpora.dictionary.Dictionary(chunks)\ncorpus = [ dictionary.doc2bow(chunk) for chunk in chunks ]\nlda = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = dictionary,\n    num_topics = 10)\ntopics = lda.show_topics(5, 5)\n</code></pre>\n\n<p>However the result is completely different from any example I've seen in that the topics are full of meaningless words that can be found in <em>all</em> source documents, e.g. \"I\", \"he\", \"said\", \"like\", ... example:</p>\n\n<pre><code>[(2, '0.009*\"I\" + 0.007*\"\\'s\" + 0.007*\"The\" + 0.005*\"would\" + 0.004*\"He\"'), \n(8, '0.012*\"I\" + 0.010*\"He\" + 0.008*\"\\'s\" + 0.006*\"n\\'t\" + 0.005*\"The\"'), \n(9, '0.022*\"I\" + 0.014*\"\\'s\" + 0.009*\"``\" + 0.007*\"\\'\\'\" + 0.007*\"like\"'), \n(7, '0.010*\"\\'s\" + 0.009*\"I\" + 0.006*\"He\" + 0.005*\"The\" + 0.005*\"said\"'), \n(1, '0.009*\"I\" + 0.009*\"\\'s\" + 0.007*\"n\\'t\" + 0.007*\"The\" + 0.006*\"He\"')]\n</code></pre>\n\n<p>I don't quite understand why that happens, or why it doesn't happen with the examples I've seen. How do I get the LDA model to find more distinctive topics with less overlap? Is it a matter of filtering out more common words first? How can I adjust how many times the model runs? Is the number of original texts too small?</p>\n",
    "score": 13,
    "creation_date": 1505921407,
    "view_count": 10349,
    "answer_count": 1,
    "tags": "python;nlp;gensim;lda;topic-modeling"
  },
  {
    "question_id": 42038337,
    "title": "What is the connection or difference between lemma and synset in wordnet?",
    "body": "<p>I am a complete beginner to NLP and NLTK. </p>\n\n<p>I was not able to understand the exact <strong><em>difference between lemmas and synsets in wordnet</em></strong>, because both are producing nearly the same output. for example for the word cake it produce this output.</p>\n\n<pre><code>lemmas :  [Lemma('cake.n.01.cake'), Lemma('patty.n.01.cake'), Lemma('cake.n.03.cake'), Lemma('coat.v.03.cake')]\n\nsynsets :  [Synset('cake.n.01'), Synset('patty.n.01'), Synset('cake.n.03'), Synset('coat.v.03')]\n</code></pre>\n\n<p>please help me to understand this concept.</p>\n\n<p>Thank you.</p>\n",
    "score": 13,
    "creation_date": 1486197884,
    "view_count": 12190,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 122595,
    "title": "NLP: Qualitatively &quot;positive&quot; vs &quot;negative&quot; sentence",
    "body": "<p>I need your help in determining the best approach for analyzing industry-specific sentences (i.e. movie reviews) for \"positive\" vs \"negative\". I've seen libraries such as OpenNLP before, but it's too low-level - it just gives me the basic sentence composition; what I need is a higher-level structure:\n- hopefully with wordlists\n- hopefully trainable on my set of data</p>\n\n<p>Thanks!</p>\n",
    "score": 13,
    "creation_date": 1222192307,
    "view_count": 13430,
    "answer_count": 2,
    "tags": "nlp;text-analysis"
  },
  {
    "question_id": 50906210,
    "title": "Confused with the return result of TfidfVectorizer.fit_transform",
    "body": "<p>I wanted to learn more about NLP. I came across this piece of code. But I was confused about the outcome of <code>TfidfVectorizer.fit_transform</code> when the result is printed. I am familiar with what tfidf is but I could not understand what the numbers mean.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport os\nimport io\nimport string\nimport requests\nimport csv\nimport nltk\nfrom zipfile import ZipFile\n\nsess = tf.Session()\n\nbatch_size = 100\nmax_features = 1000\n\nsave_file_name = os.path.join('smsspamcollection', 'SMSSpamCollection.csv')\nif os.path.isfile(save_file_name):\n    text_data = []\n    with open(save_file_name, 'r') as temp_output_file:\n        reader = csv.reader(temp_output_file)\n        for row in reader:\n            text_data.append(row)\n\nelse:\n    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n    r = requests.get(zip_url)\n    z = ZipFile(io.BytesIO(r.content))\n    file = z.read('SMSSpamCollection')\n\n    # Format data \n    text_data = file.decode()\n    text_data = text_data.encode('ascii', errors='ignore')\n    text_data = text_data.decode().split('\\n')\n    text_data = [x.split('\\t') for x in text_data if len(x) &gt;= 1]\n\n    # And write to csv \n    with open(save_file_name, 'w') as temp_output_file:\n        writer = csv.writer(temp_output_file)\n        writer.writerows(text_data)\n\ntexts = [x[1] for x in text_data]\ntarget = [x[0] for x in text_data]\ntarget = [1 if x == 'spam' else 0 for x in target]\n\n# Normalize the text\ntexts = [x.lower() for x in texts]  # lower\ntexts = [''.join(c for c in x if c not in string.punctuation) for x in texts]  # remove punctuation\ntexts = [''.join(c for c in x if c not in '0123456789') for x in texts]  # remove numbers\ntexts = [' '.join(x.split()) for x in texts]  # trim extra whitespace\n\n\ndef tokenizer(text):\n    words = nltk.word_tokenize(text)\n    return words\n\n\ntfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words='english', max_features=max_features)\nsparse_tfidf_texts = tfidf.fit_transform(texts)\nprint(sparse_tfidf_texts)\n</code></pre>\n<p>And the output is:</p>\n<blockquote>\n<p>(0, 630)  0.37172623140154337   (0, 160)  0.36805562944957004   (0,\n38)   0.3613966215413548   (0, 545)   0.2561101665717327   (0,\n326)  0.2645280991765623   (0, 967)   0.3277447602873963   (0,\n421)  0.3896274380321477   (0, 227)   0.28102915589024796   (0,\n323)  0.22032541100275282   (0, 922)  0.2709848154866997   (1,\n577)  0.4007895093299793   (1, 425)   0.5970064521899725   (1,\n943)  0.6310763941180291   (1, 878)   0.29102173465492637   (2,\n282)  0.1771481430848552   (2, 243)   0.5517018054305785   (2,\n955)  0.2920174942032025   (2, 138)   0.30143666813167863   (2,\n946)  0.2269933441326121   (2, 165)   0.3051095293405041   (2,\n268)  0.2820392223588522   (2, 780)   0.24119626642264894   (2,\n823)  0.1890454397278538   (2, 674)   0.256251970757827   (2,\n874)  0.19343834015314287   : :   (5569, 648) 0.24171652492226922<br />\n(5569, 123)   0.23011909339432202   (5569, 957)   0.24817919217662862<br />\n(5569, 549)   0.28583789844730134   (5569, 863)   0.3026729783085827<br />\n(5569, 844)   0.20228305447951195   (5569, 146)   0.2514415602877767<br />\n(5569, 595)   0.2463259875380789   (5569, 511)    0.3091904754885042<br />\n(5569, 230)   0.2872728684768659   (5569, 638)    0.34151390143548765<br />\n(5569, 83)    0.3464271621701711   (5570, 370)    0.4199910200421362<br />\n(5570, 46)    0.48234172093857797   (5570, 317)   0.4171646676697801<br />\n(5570, 281)   0.6456993475093024   (5572, 282)    0.25540827228532487<br />\n(5572, 385)   0.36945842040023935   (5572, 448)   0.25540827228532487<br />\n(5572, 931)   0.3031800542518209   (5572, 192)    0.29866989620926737<br />\n(5572, 303)   0.43990016711221736   (5572, 87)    0.45211284173737176<br />\n(5572, 332)   0.3924202767503492   (5573, 866)    1.0</p>\n</blockquote>\n<p>I would be more than happy if someone can explain about the output.</p>\n",
    "score": 13,
    "creation_date": 1529313566,
    "view_count": 12407,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp;tf-idf;tfidfvectorizer"
  },
  {
    "question_id": 2781752,
    "title": "Naive Bayesian for Topic detection using &quot;Bag of Words&quot; approach",
    "body": "<p>I am trying to implement a naive bayseian approach to find the topic of a given document or stream of words. Is there are Naive Bayesian approach that i might be able to look up for this ? </p>\n\n<p>Also, i am trying to improve my dictionary as i go along. Initially, i have a bunch of words that map to a topics (hard-coded). Depending on the occurrence of the words other than the ones that are already mapped. And depending on the occurrences of these words i want to add them to the mappings, hence improving and learning about new words that map to topic. And also changing the probabilities of words.</p>\n\n<p>How should i go about doing this ? Is my approach the right one ? </p>\n\n<p>Which programming language would be best suited for the implementation ?  </p>\n",
    "score": 13,
    "creation_date": 1273155497,
    "view_count": 6422,
    "answer_count": 1,
    "tags": "machine-learning;nlp;data-mining;naivebayes"
  },
  {
    "question_id": 27405942,
    "title": "Best way to extract keywords from input NLP sentence",
    "body": "<p>I'm working on a project where I need to extract important keywords from a sentence.  I've been using a rules based system based on the POS tags.  However, I run into some ambiguous terms that I've been unable to parse.  Is there some machine learning classifier that I can use to extract relevant keywords based on a training set of different sentences?</p>\n",
    "score": 13,
    "creation_date": 1418228524,
    "view_count": 17174,
    "answer_count": 5,
    "tags": "python;machine-learning;nlp"
  },
  {
    "question_id": 1889675,
    "title": "Extract Nouns from Text (Java)",
    "body": "<p>Does anyone know the easiest way to extract only nouns from a body of text?</p>\n\n<p>I've heard about the <a href=\"http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/\" rel=\"noreferrer\">TreeTagger tool</a> and I tried giving it a shot but couldn't get it to work for some reason.</p>\n\n<p>Any suggestions?</p>\n\n<p>Thanks Phil</p>\n\n<p><strong>EDIT:</strong></p>\n\n<pre> import org.annolab.tt4j.*; \nTreeTaggerWrapper tt = new TreeTaggerWrapper(); \n\ntry { tt.setModel(\"/Nouns/english.par\"); \n\ntt.setHandler(new TokenHandler() { \n     void token(String token, String pos, String lemma) {    \n     System.out.println(token+\"\\t\"+pos+\"\\t\"+lemma); } }); \n     tt.process(words); // words = list of words \n\n     } finally { tt.destroy(); \n} </pre> \n\n<p>That is my code, English is the language. I was getting the error : The type new TokenHandler(){} must implement the inherited abstract method TokenHandler.token. Am I doing something wrong?</p>\n",
    "score": 13,
    "creation_date": 1260553233,
    "view_count": 22434,
    "answer_count": 7,
    "tags": "java;nlp"
  },
  {
    "question_id": 49100615,
    "title": "NLTK. Detecting whether a sentence is Interrogative or Not?",
    "body": "<p>I want to create a python script using NLTK or whatever library is best to correctly identify given sentence is interrogative (a question) or not. I tried using regex but there are deeper scenarios where regex fails. so wanted to use Natural Language Processing can anybody help!</p>\n",
    "score": 13,
    "creation_date": 1520198374,
    "view_count": 14936,
    "answer_count": 4,
    "tags": "python;machine-learning;nlp;artificial-intelligence;nltk"
  },
  {
    "question_id": 40124476,
    "title": "How to set custom stop words for sklearn CountVectorizer?",
    "body": "<p>I'm trying to run LDA (Latent Dirichlet Allocation) on a non-English text dataset.</p>\n\n<p>From sklearn's tutorial, there's this part where you count term frequency of the words to feed into the LDA:</p>\n\n<pre><code>tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                            max_features=n_features,\n                            stop_words='english')\n</code></pre>\n\n<p>Which has built-in stop words feature which is only available for English I think. How could I use my own stop words list for this?</p>\n",
    "score": 13,
    "creation_date": 1476860850,
    "view_count": 21541,
    "answer_count": 1,
    "tags": "python;machine-learning;scikit-learn;nlp"
  },
  {
    "question_id": 58876392,
    "title": "What is the difference between token and span (a slice from a doc) in spaCy?",
    "body": "<p>I would like to know what is the difference between <strong>token</strong> and <strong>span</strong> in <strong>spaCy</strong>.</p>\n\n<p>Also what is the main reason when we have to work with span? Why can't we simply use token to do any NLP? Specially when we use <strong>spaCy matcher</strong>?</p>\n\n<p><strong>Brief Background</strong>:\nMy problem came up when I wanted to get index of span (its exact index in string doc not its ordered index in spaCy doc) after using <strong>spaCy matcher</strong> which returns '<em>match_id</em>', '<em>start</em>' and '<em>end</em>', and so I could get span out of this information, not a token.\nThen I needed to create a training_data which requires exact index of word in a sentence. If I had access to token, I could simply use token.idx but span does not have that! So I have to write extra codes to find the index of word (which is the same as span) in its sentence!</p>\n",
    "score": 13,
    "creation_date": 1573818378,
    "view_count": 16243,
    "answer_count": 2,
    "tags": "python;nlp;token;spacy"
  },
  {
    "question_id": 2645706,
    "title": "Is there any lib for python that will get me the synonyms of a word?",
    "body": "<p>Is there any api/lib for python that will get me the synonyms of a word?</p>\n\n<p>For example if i have the word \"house\" it will return \"building, domicile, mansion, etc...\"</p>\n",
    "score": 13,
    "creation_date": 1271338664,
    "view_count": 16404,
    "answer_count": 2,
    "tags": "python;nlp;synonym"
  },
  {
    "question_id": 44158910,
    "title": "NLP reverse tokenizing (going from tokens to nicely formatted sentence)",
    "body": "<p>Python's Spacy package has a statistical tokenizer that intelligently splits a sentence into tokens. My question is, is there a package that allows me to go backwards, i.e. from list of tokens to a nicely formatted sentence? Essentially, I want a function that lets me do the following:</p>\n\n<pre><code>&gt;&gt;&gt; toks = ['hello', ',', 'i', 'ca', \"n't\", 'feel', 'my', 'feet', '!']\n&gt;&gt;&gt; some_function(toks)\n\"Hello, I can't feel my feet!\"\n</code></pre>\n\n<p>It probably needs some sort of statistical/rules-based procedure to know how spacing, capitalization or contractions should work in a proper sentence.</p>\n",
    "score": 13,
    "creation_date": 1495630194,
    "view_count": 10693,
    "answer_count": 3,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 7059954,
    "title": "Latent Semantic Analysis concepts",
    "body": "<p>I've read about using Singular Value Decomposition (SVD) to do Latent Semantic Analysis (LSA) in corpus of texts. I've understood how to do that, also I understand mathematical concepts of SVD. </p>\n\n<p>But I don't understand why does it works applying to corpuses of texts <em>(I believe - there must be linguistical explanation)</em>. Could anybody explain me this with linguistic point of view?</p>\n\n<p>Thanks</p>\n",
    "score": 13,
    "creation_date": 1313358566,
    "view_count": 3079,
    "answer_count": 3,
    "tags": "algorithm;nlp;data-mining;text-mining;latent-semantic-indexing"
  },
  {
    "question_id": 66244123,
    "title": "Why use multi-headed attention in Transformers?",
    "body": "<p>I am trying to understand why transformers use multiple attention heads. I found the following <a href=\"https://towardsdatascience.com/simple-explanation-of-transformers-in-nlp-da1adfc5d64f\" rel=\"noreferrer\">quote</a>:</p>\n<blockquote>\n<p>Instead of using a single attention function where the attention can\nbe dominated by the actual word itself, transformers use multiple\nattention heads.</p>\n</blockquote>\n<p>What is meant by &quot;the attention being dominated by the word itself&quot; and how does the use of multiple heads address that?</p>\n",
    "score": 13,
    "creation_date": 1613572714,
    "view_count": 6770,
    "answer_count": 2,
    "tags": "nlp;transformer-model;attention-model"
  },
  {
    "question_id": 59877385,
    "title": "What is the difference between Sentence Encodings and Contextualized Word Embeddings?",
    "body": "<p>I have seen both terms used while reading papers about BERT and ELMo so I wonder if there is a difference between them.</p>\n",
    "score": 13,
    "creation_date": 1579778454,
    "view_count": 8886,
    "answer_count": 1,
    "tags": "nlp;word-embedding;elmo;bert-language-model"
  },
  {
    "question_id": 28720174,
    "title": "Negation handling in NLP",
    "body": "<p>I'm currently working on a project, where I want to extract emotion from text. As I'm using conceptnet5 (a semantic network), I can't however simply prefix words in a sentence that contains a negation-word, as those words would simply not show up in conceptnet5's API.</p>\n\n<p>Here's an example:</p>\n\n<blockquote>\n  <p>The movie wasn't that good.</p>\n</blockquote>\n\n<p>Hence, I figured that I could use wordnet's lemma functionality to replace adjectives in sentences that contain negation-words like (not, ...).</p>\n\n<p>In the previous example, the algorithm would detect <code>wasn't</code> and would replace it with <code>was not</code>.\nFurther, it would detect a negation-word <code>not</code>, and replace <code>good</code> with it's antonym <code>bad</code>.\nThe sentence would read:</p>\n\n<blockquote>\n  <p>The movie was that bad.</p>\n</blockquote>\n\n<p>While I see that this isn't the most elegant way, and it does probably in many cases produce the wrong result, I'd still like to handle negation that way as I frankly don't know any better approach.</p>\n\n<p><strong>Considering my problem:</strong>\nUnfortunately, I did not find any library that would allow me to replace all occurrences of appended negation-words (<code>wasn't</code> => <code>was not</code>).\nI mean I could do it manually, by replacing the occurrences with a regex, but then I would be stuck with the english language.</p>\n\n<p>Therefore I'd like to ask if some of you know a library, function or <strong>better method</strong> that could help me here.\nCurrently I'm using python <code>nltk</code>, still it doesn't seem that it contains such functionality, but I may be wrong.</p>\n\n<p>Thanks in advance :)</p>\n",
    "score": 13,
    "creation_date": 1424870663,
    "view_count": 15478,
    "answer_count": 1,
    "tags": "python;regex;nlp;nltk;text-processing"
  },
  {
    "question_id": 60120043,
    "title": "Optimizer and scheduler for BERT fine-tuning",
    "body": "<p>I'm trying to fine-tune a model with BERT (using <code>transformers</code> library), and I'm a bit unsure about the optimizer and scheduler.</p>\n\n<p>First, I understand that I should use <code>transformers.AdamW</code> instead of Pytorch's version of it. Also, we should use a warmup scheduler as suggested in the paper, so the scheduler is created using <code>get_linear_scheduler_with_warmup</code> function from <code>transformers</code> package.</p>\n\n<p>The main questions I have are:</p>\n\n<ol>\n<li><code>get_linear_scheduler_with_warmup</code> should be called with the warm up. Is it ok to use 2 for warmup out of 10 epochs? </li>\n<li>When should I call <code>scheduler.step()</code>? If I do after <code>train</code>, the learning rate is zero for the first epoch. Should I call it for each batch?</li>\n</ol>\n\n<p>Am I doing something wrong with this?</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AdamW\nfrom transformers.optimization import get_linear_scheduler_with_warmup\n\nN_EPOCHS = 10\n\nmodel = BertGRUModel(finetune_bert=True,...)\nnum_training_steps = N_EPOCHS+1\nnum_warmup_steps = 2\nwarmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n\noptimizer = AdamW(model.parameters())\ncriterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([class_weights[1]]))\n\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=num_warmup_steps, \n    num_training_steps=num_training_steps\n)\n\nfor epoch in range(N_EPOCHS):\n    scheduler.step() #If I do after train, LR = 0 for the first epoch\n    print(optimizer.param_groups[0][\"lr\"])\n\n    train(...) # here we call optimizer.step()\n    evaluate(...)\n</code></pre>\n\n<p>My model and train routine(quite similar to <a href=\"https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb\" rel=\"noreferrer\">this notebook</a>)</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>class BERTGRUSentiment(nn.Module):\n    def __init__(self,\n                 bert,\n                 hidden_dim,\n                 output_dim,\n                 n_layers=1, \n                 bidirectional=False,\n                 finetune_bert=False,\n                 dropout=0.2):\n\n        super().__init__()\n\n        self.bert = bert\n\n        embedding_dim = bert.config.to_dict()['hidden_size']\n\n        self.finetune_bert = finetune_bert\n\n        self.rnn = nn.GRU(embedding_dim,\n                          hidden_dim,\n                          num_layers = n_layers,\n                          bidirectional = bidirectional,\n                          batch_first = True,\n                          dropout = 0 if n_layers &lt; 2 else dropout)\n\n        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)        \n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, text):    \n        #text = [batch size, sent len]\n\n        if not self.finetune_bert:\n            with torch.no_grad():\n                embedded = self.bert(text)[0]\n        else:\n            embedded = self.bert(text)[0]\n        #embedded = [batch size, sent len, emb dim]\n        _, hidden = self.rnn(embedded)\n\n        #hidden = [n layers * n directions, batch size, emb dim]\n\n        if self.rnn.bidirectional:\n            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        else:\n            hidden = self.dropout(hidden[-1,:,:])\n\n        #hidden = [batch size, hid dim]\n\n        output = self.out(hidden)\n\n        #output = [batch size, out dim]\n\n        return output\n\n\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\ndef train(model, iterator, optimizer, criterion, max_grad_norm=None):\n    \"\"\"\n    Trains the model for one full epoch\n    \"\"\"\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train()\n\n    for i, batch in enumerate(iterator):\n        optimizer.zero_grad()\n        text, lens = batch.text\n\n        predictions = model(text)\n\n        target = batch.target\n\n        loss = criterion(predictions.squeeze(1), target)\n\n        prob_predictions = torch.sigmoid(predictions)\n\n        preds = torch.round(prob_predictions).detach().cpu()\n        acc = accuracy_score(preds, target.cpu())\n\n        loss.backward()\n        # Gradient clipping\n        if max_grad_norm:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\n\n</code></pre>\n",
    "score": 13,
    "creation_date": 1581104411,
    "view_count": 38584,
    "answer_count": 2,
    "tags": "nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 43990617,
    "title": "spaCy Documentation for [ orth , pos , tag, lema and text ]",
    "body": "<p>I am new to spaCy. I added this post for documentation and make it simple for new starters as me.</p>\n\n<pre><code>import spacy\nnlp = spacy.load('en')\ndoc = nlp(u'KEEP CALM because TOGETHER We Rock !')\nfor word in doc:\n    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)\n    print(word.orth_)\n</code></pre>\n\n<p>I am looking to understand what the meaning of orth, lemma, tag and pos ? This code print out the values also what the different between <code>print(word)</code> vs <code>print(word.orth_)</code></p>\n",
    "score": 13,
    "creation_date": 1494893748,
    "view_count": 10901,
    "answer_count": 2,
    "tags": "python;nlp;cython;spacy"
  },
  {
    "question_id": 11449115,
    "title": "Algorithms/theory behind predictive autocomplete?",
    "body": "<p>Simple word autocomplete just displays a list of words that match the characters that were already typed. But I would like to order the words in the autocomplete list according to the probability of the words occuring, depending on the words that were typed before, relying on a statistical model of a text corpus. What algorithms and data structures do I need for this? Can you give me links for good tutorials? </p>\n",
    "score": 13,
    "creation_date": 1342086198,
    "view_count": 12865,
    "answer_count": 2,
    "tags": "algorithm;text;autocomplete;nlp;probability"
  },
  {
    "question_id": 5556778,
    "title": "Is there a way to get the subject of a sentence using OpenNLP?",
    "body": "<p>Is there a way to get the subject of a sentence using OpenNLP? \nI'm trying to identify the most important part of a users sentence.  Generally, users will be submitting sentences to our \"engine\" and we want to know exactly what the core topic is of that sentence.</p>\n\n<p>Currently we are using openNlp to:</p>\n\n<ol>\n<li>Chunk the sentence</li>\n<li>Identify the noun-phrase, verbs, etc of the sentence</li>\n<li>Identify all  \"topics\" of the sentence</li>\n<li>(NOT YET DONE!) Identify the \"core topic\" of the sentence</li>\n</ol>\n\n<p>Please let me know if you have any bright ideas..</p>\n",
    "score": 13,
    "creation_date": 1302028725,
    "view_count": 10000,
    "answer_count": 2,
    "tags": "java;nlp;opennlp"
  },
  {
    "question_id": 15111183,
    "title": "What languages are supported for nltk.word_tokenize and nltk.pos_tag",
    "body": "<p>I need to conduct name entity extraction for text in multiple languages: spanish, portuguese, greek, czech, chinese.</p>\n\n<p>Is there somewhere a list of all supported languages for these two functions? And is there a method to use other corpora so that these languages can be included?</p>\n",
    "score": 13,
    "creation_date": 1361965220,
    "view_count": 19609,
    "answer_count": 2,
    "tags": "nlp;nltk"
  },
  {
    "question_id": 10974532,
    "title": "Extracting noun phrases from a text file using stanford typed parser",
    "body": "<p>I have a text which I want to extract the noun phrases from it. I can easily get the typed parser for the text that i have, but wondering how i can extract the noun phrases in the text ?</p>\n",
    "score": 13,
    "creation_date": 1339389202,
    "view_count": 18104,
    "answer_count": 3,
    "tags": "java;text;nlp;stanford-nlp;opennlp"
  },
  {
    "question_id": 60074110,
    "title": "Extract text information from PDF files with different layouts - machine learning",
    "body": "<p>I need assistance with a ML project I am currently trying to create.</p>\n\n<p>I receive a lot of invoices from a lot of different suppliers - all in their own unique layout. I need to extract <strong>3</strong> key elements from the invoices. These <strong>3</strong> elements are all located in a table/line items for all the invoices.</p>\n\n<p>The <strong>3</strong> elements are: </p>\n\n<ul>\n<li><strong>1</strong>: Tariff number (digit)</li>\n<li><strong>2</strong>: Quantity (always a digit)</li>\n<li><strong>3</strong>: Total line amount (monetary value)</li>\n</ul>\n\n<p>Please refer to below screenshot, where I have marked these field on a sample invoice.</p>\n\n<p><a href=\"https://i.sstatic.net/HYmGZ.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/HYmGZ.png\" alt=\"enter image description here\"></a></p>\n\n<p>I started this project with a template approach, based on <em>regular expressions</em>. This, however, was not scaleable at all and I ended up with tons of different rules.</p>\n\n<p>I am hoping that machine learning can help me here - or maybe a hybrid solution?</p>\n\n<h1>The common denominator</h1>\n\n<p>In <strong>all</strong> of my invoices, despite of the different layouts, each line item will <strong>always</strong> consist of one <strong>tariff number</strong>. This tariff number is always 8 digits, and is always formatted in one the ways like below:</p>\n\n<ul>\n<li>xxxxxxxx</li>\n<li>xxxx.xxxx</li>\n<li>xx.xx.xx.xx</li>\n</ul>\n\n<p>(Where \"x\" is a digit from 0 - 9).</p>\n\n<p><strong>Further</strong>, as you can see on the invoice there is both a Unit Price and a Total Amount per line. The amount I will need is <strong>always</strong> the highest for each line.</p>\n\n<h1>The output</h1>\n\n<p>For each invoice like the one above, I need the output for each line. This could for example be something like this:</p>\n\n<pre><code>{\n    \"line\":\"0\",\n    \"tariff\":\"85444290\",\n    \"quantity\":\"3\",\n    \"amount\":\"258.93\"\n},\n{\n    \"line\":\"1\",\n    \"tariff\":\"85444290\",\n    \"quantity\":\"4\",\n    \"amount\":\"548.32\"\n},\n{\n    \"line\":\"2\",\n    \"tariff\":\"76109090\",\n    \"quantity\":\"5\",\n    \"amount\":\"412.30\"\n}\n</code></pre>\n\n<h1>Where to go from here?</h1>\n\n<p>I am not sure of what I am looking to do falls under machine learning and if so, under which category. Is it computer vision? NLP? Named Entity Recognition?</p>\n\n<p>My initial thought was to:</p>\n\n<ol>\n<li>Convert the invoice to text. (The invoices are all in textable PDFs, so I can use something like <code>pdftotext</code> to get the exact textual values)</li>\n<li>Create custom <strong>named entities</strong> for <code>quantity</code>, <code>tariff</code> and <code>amount</code></li>\n<li>Export the found entities.</li>\n</ol>\n\n<p>However, I feel like I might be missing something. </p>\n\n<p><strong>Can anyone assist me in the right direction?</strong></p>\n\n<h1>Edit:</h1>\n\n<p>Please see below for a few more examples of how an invoice table section can look like:</p>\n\n<p>Sample invoice #2\n<a href=\"https://i.sstatic.net/0dvIC.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/0dvIC.png\" alt=\"enter image description here\"></a></p>\n\n<p>Sample invoice #3\n<a href=\"https://i.sstatic.net/8Wh6f.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/8Wh6f.png\" alt=\"enter image description here\"></a></p>\n\n<h1>Edit 2:</h1>\n\n<p>Please see below for the three sample images, <strong>without</strong> the borders/bounding boxes:</p>\n\n<p>Image 1:\n<a href=\"https://i.sstatic.net/5Iln5.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/5Iln5.png\" alt=\"Sample 1 without bbox\"></a></p>\n\n<p>Image 2:\n<a href=\"https://i.sstatic.net/cEabY.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/cEabY.png\" alt=\"Sample 2 without bbox\"></a></p>\n\n<p>Image 3:\n<a href=\"https://i.sstatic.net/cSHzh.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/cSHzh.png\" alt=\"Sample 3 without bbox\"></a></p>\n",
    "score": 13,
    "creation_date": 1580899715,
    "view_count": 7364,
    "answer_count": 2,
    "tags": "machine-learning;image-processing;neural-network;nlp;computer-vision"
  },
  {
    "question_id": 49009386,
    "title": "Train only some word embeddings (Keras)",
    "body": "<p>In my model, I use GloVe pre-trained embeddings. I wish to keep them non-trainable in order to decrease the number of model parameters and avoid overfit. However, I have a special symbol whose embedding I <em>do</em> want to train.</p>\n\n<p>Using the provided Embedding Layer, I can only use the parameter 'trainable' to set the trainability of <strong>all</strong> embeddings in the following way:</p>\n\n<pre><code>embedding_layer = Embedding(voc_size,\n                        emb_dim,\n                        weights=[embedding_matrix],\n                        input_length=MAX_LEN,\n                        trainable=False)\n</code></pre>\n\n<p>Is there a Keras-level solution to training only a subset of embeddings?  </p>\n\n<p>Please note:</p>\n\n<ol>\n<li>There is not enough data to generate new embeddings for all words.</li>\n<li><a href=\"https://stackoverflow.com/questions/35803425/update-only-part-of-the-word-embedding-matrix-in-tensorflow\">These</a> answers only relate to native TensorFlow.</li>\n</ol>\n",
    "score": 13,
    "creation_date": 1519736566,
    "view_count": 3494,
    "answer_count": 2,
    "tags": "python;nlp;keras;word-embedding"
  },
  {
    "question_id": 13892638,
    "title": "Extracting the relationship between entities in Stanford CoreNLP",
    "body": "<p>I want to extract the complete relationship between two entities using Stanford CoreNLP (or maybe other tools).</p>\n\n<p>For example:</p>\n\n<blockquote>\n  <p>Windows is <em>more popular than</em> Linux.</p>\n  \n  <p>This tool <em>requires</em> Java.</p>\n  \n  <p>Football is <em>the most popular game in</em> the World.</p>\n</blockquote>\n\n<p>What is the fastest way? And what is the best practice for that?</p>\n\n<p>Thanks in advance</p>\n",
    "score": 13,
    "creation_date": 1355578252,
    "view_count": 8486,
    "answer_count": 5,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 3778388,
    "title": "Java text analysis libraries",
    "body": "<p>I'm looking for a java driven solution to a requirement for analysing sentences to log whether a key word was used positively or negatively. </p>\n\n<p>Ie The key word might be 'cabbages' and the sentence:-</p>\n\n<p>'I like cabbages but not peas'</p>\n\n<p>And I'd like a java text analyser of some kind to log this as positive. Can the lucene (Hibernate-Search) libraries be utilized to for this?</p>\n\n<p>Any thoughts?</p>\n",
    "score": 13,
    "creation_date": 1285245237,
    "view_count": 17115,
    "answer_count": 3,
    "tags": "java;text;analysis;text-analysis"
  },
  {
    "question_id": 2303098,
    "title": "Java Open Source Text Mining Frameworks",
    "body": "<p>I want to know what is the best open source Java based framework for Text Mining, to use botg Machine Learning and dictionary Methods.</p>\n\n<p>I'm using Mallet but there are not that much documentation and I do not know if it will fit all my requirements.</p>\n",
    "score": 13,
    "creation_date": 1266689139,
    "view_count": 26827,
    "answer_count": 7,
    "tags": "java;frameworks;machine-learning;nlp;information-retrieval"
  },
  {
    "question_id": 21107075,
    "title": "Classification using movie review corpus in NLTK/Python",
    "body": "<p>I'm looking to do some classification in the vein of <a href=\"http://nltk.org/book/ch06.html\">NLTK Chapter 6</a>. The book seems to skip a step in creating the categories, and I'm not sure what I'm doing wrong. I have my script here with the response following. My issues primarily stem from the first part -- category creation based upon directory names. Some other questions on here have used filenames (i.e. <code>pos_1.txt</code> and <code>neg_1.txt</code>), but I would prefer to create directories I could dump files into.</p>\n\n<pre><code>from nltk.corpus import movie_reviews\n\nreviews = CategorizedPlaintextCorpusReader('./nltk_data/corpora/movie_reviews', r'(\\w+)/*.txt', cat_pattern=r'/(\\w+)/.txt')\nreviews.categories()\n['pos', 'neg']\n\ndocuments = [(list(movie_reviews.words(fileid)), category)\n            for category in movie_reviews.categories()\n            for fileid in movie_reviews.fileids(category)]\n\nall_words=nltk.FreqDist(\n    w.lower() \n    for w in movie_reviews.words() \n    if w.lower() not in nltk.corpus.stopwords.words('english') and w.lower() not in  string.punctuation)\nword_features = all_words.keys()[:100]\n\ndef document_features(document): \n    document_words = set(document) \n    features = {}\n    for word in word_features:\n        features['contains(%s)' % word] = (word in document_words)\n    return features\nprint document_features(movie_reviews.words('pos/11.txt'))\n\nfeaturesets = [(document_features(d), c) for (d,c) in documents]\ntrain_set, test_set = featuresets[100:], featuresets[:100]\nclassifier = nltk.NaiveBayesClassifier.train(train_set)\n\nprint nltk.classify.accuracy(classifier, test_set)\nclassifier.show_most_informative_features(5)\n</code></pre>\n\n<p>This returns:</p>\n\n<pre><code>File \"test.py\", line 38, in &lt;module&gt;\n    for w in movie_reviews.words()\n\nFile \"/usr/local/lib/python2.6/dist-packages/nltk/corpus/reader/plaintext.py\", line 184, in words\n    self, self._resolve(fileids, categories))\n\nFile \"/usr/local/lib/python2.6/dist-packages/nltk/corpus/reader/plaintext.py\", line 91, in words\n    in self.abspaths(fileids, True, True)])\n\nFile \"/usr/local/lib/python2.6/dist-packages/nltk/corpus/reader/util.py\", line 421, in concat\n    raise ValueError('concat() expects at least one object!')\n\nValueError: concat() expects at least one object!\n</code></pre>\n\n<p>---------UPDATE-------------\nThanks alvas for your detailed answer! I have two questions, however. </p>\n\n<ol>\n<li>Is it possible to grab the category from the filename as I was attempting to do? I was hoping to do it in the same vein as the <code>review_pos.txt</code> method, only grabbing the <code>pos</code> from the folder name rather than the file name. </li>\n<li><p>I ran your code and am experiencing a syntax error on</p>\n\n<p><code>train_set =[({i:(i in tokens) for i in word_features}, tag) for tokens,tag in\ndocuments[:numtrain]] \ntest_set = [({i:(i in tokens) for i in\nword_features}, tag) for tokens,tag in documents[numtrain:]]</code> </p></li>\n</ol>\n\n<p>with the carrot under the first <code>for</code>. I'm a beginner Python user and I'm not familiar enough with that bit of syntax to try to toubleshoot it.</p>\n\n<p>----UPDATE 2----\nError is </p>\n\n<pre><code>File \"review.py\", line 17\n  for i in word_features}, tag)\n    ^\nSyntaxError: invalid syntax`\n</code></pre>\n",
    "score": 13,
    "creation_date": 1389679691,
    "view_count": 19938,
    "answer_count": 1,
    "tags": "python;nlp;nltk;sentiment-analysis;corpus"
  },
  {
    "question_id": 9538425,
    "title": "Is there a database, API, or parsable text for getting verb conjugations?",
    "body": "<p>This isn't directly a programming question, so I apologize in advance. I've been working on a grammar-free random sentence generator for a typing game I'd like to make, and I've been having a difficult time finding any parsable (or callable) data for getting verb conjugations. Ultimately, if I can't find anything like this, I'm going to have to go through the dictionary I've created and add first-person singular and plural, second-person singular and plural, third-person singular and plural, simple past, past participle, and present participle forms for every irregular verb.</p>\n\n<p>This wouldn't be a problem in many languages, but there are so many irregular English verbs that this could take a long, long time to do manually. I'm not against the worse option, but I want to make sure I'm not going to be wasting obscene hours doing it myself when there is some database I can use instead.</p>\n\n<p>I've seen <a href=\"http://www.scientificpsychic.com/verbs1.html\" rel=\"nofollow noreferrer\">http://www.scientificpsychic.com/verbs1.html</a> and spoken with the creator, but he doesn't release his exact dictionary (just the classes for it). I've also seen sites like <a href=\"http://www.verbix.com/webverbix/English/find.html\" rel=\"nofollow noreferrer\">http://www.verbix.com/webverbix/English/find.html</a>, which would be great for scraping, but that's a bit of a pain as well.</p>\n\n<p>This question has been asked here before ( <a href=\"https://stackoverflow.com/questions/8424806/verb-conjugations-database\">Verb Conjugations Database</a> ), but the question was left unanswered, and the asker alluded to solving the problem but never said what the solution was.</p>\n",
    "score": 13,
    "creation_date": 1330712660,
    "view_count": 10004,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 48573174,
    "title": "How to combine TFIDF features with other features",
    "body": "<p>I have a classic NLP problem, I have to classify a news as fake or real.</p>\n\n<p>I have created two sets of features:</p>\n\n<p>A) Bigram Term Frequency-Inverse Document Frequency</p>\n\n<p>B) Approximately 20 Features associated to each document obtained using pattern.en (<a href=\"https://www.clips.uantwerpen.be/pages/pattern-en\" rel=\"noreferrer\">https://www.clips.uantwerpen.be/pages/pattern-en</a>) as subjectivity of the text, polarity, #stopwords, #verbs, #subject, relations grammaticals etc ...</p>\n\n<p>Which is the best way to combine the TFIDF features with the other features for a single prediction?\nThanks a lot to everyone.</p>\n",
    "score": 13,
    "creation_date": 1517526172,
    "view_count": 9758,
    "answer_count": 2,
    "tags": "machine-learning;nlp;text-analysis"
  },
  {
    "question_id": 48524817,
    "title": "spaCy 2.0: Save and Load a Custom NER model",
    "body": "<p>I've trained a custom NER model in spaCy with a custom tokenizer. I'd like to save the NER model without the tokenizer. I tried the following code with I found in the spaCy support forum:</p>\n\n<pre><code>import spacy\n\nnlp = spacy.load(\"en\")\nnlp.tokenizer = some_custom_tokenizer\n# Train the NER model...\nnlp.tokenizer = None\nnlp.to_disk('/tmp/my_model', disable=['tokenizer'])\n</code></pre>\n\n<p>When I try to load it, the pipeline is empty, and surprisingly, is has the default spaCy tokenizer.</p>\n\n<pre><code>nlp = spacy.blank('en').from_disk('/tmp/model', disable=['tokenizer'])\n</code></pre>\n\n<p>Any idea how can I load the model without the tokenizer, but get the full pipeline? thanks</p>\n",
    "score": 13,
    "creation_date": 1517325564,
    "view_count": 13261,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 56010551,
    "title": "pytorch embedding index out of range",
    "body": "<p>I'm following this tutorial here <a href=\"https://cs230-stanford.github.io/pytorch-nlp.html\" rel=\"noreferrer\">https://cs230-stanford.github.io/pytorch-nlp.html</a>. In there a neural model is created, using <code>nn.Module</code>, with an embedding layer, which is initialized here</p>\n\n<pre><code>self.embedding = nn.Embedding(params['vocab_size'], params['embedding_dim'])\n</code></pre>\n\n<p><code>vocab_size</code> is the total number of training samples, which is 4000.  <code>embedding_dim</code> is 50.  The relevant piece of the <code>forward</code> method is below</p>\n\n<pre><code>def forward(self, s):\n        # apply the embedding layer that maps each token to its embedding\n        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n</code></pre>\n\n<p>I get this exception when passing a batch to the model like so\n<code>model(train_batch)</code>\n<code>train_batch</code> is a numpy array of dimension <code>batch_size</code>x<code>batch_max_len</code>.  Each sample is a sentence, and each sentence is padded so that it has the length of the longest sentence in the batch.</p>\n\n<blockquote>\n  <p>File\n  \"/Users/liam_adams/Documents/cs512/research_project/custom/model.py\",\n  line 34, in forward\n      s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim   File\n  \"/Users/liam_adams/Documents/cs512/venv_research/lib/python3.7/site-packages/torch/nn/modules/module.py\",\n  line 493, in <strong>call</strong>\n      result = self.forward(*input, **kwargs)   File \"/Users/liam_adams/Documents/cs512/venv_research/lib/python3.7/site-packages/torch/nn/modules/sparse.py\",\n  line 117, in forward\n      self.norm_type, self.scale_grad_by_freq, self.sparse)   File \"/Users/liam_adams/Documents/cs512/venv_research/lib/python3.7/site-packages/torch/nn/functional.py\",\n  line 1506, in embedding\n      return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) RuntimeError: index out of range at\n  ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:193</p>\n</blockquote>\n\n<p>Is the problem here that the embedding is initialized with different dimensions than those of my batch array?  My <code>batch_size</code> will be constant but <code>batch_max_len</code> will change with every batch. This is how its done in the tutorial.</p>\n",
    "score": 13,
    "creation_date": 1557167290,
    "view_count": 36170,
    "answer_count": 4,
    "tags": "python;neural-network;nlp;pytorch;recurrent-neural-network"
  },
  {
    "question_id": 7693470,
    "title": "Strategy for parsing natural language descriptions into structured data",
    "body": "<p>I have a set of requirements and I'm looking for the best <strong>Java-based</strong> strategy / algorthm / software to use.  Basically, I want to take a set of recipe ingredients entered by real people in natural english and parse out the meta-data into a structured format (see requirements below to see what I'm trying to do).</p>\n<p>I've looked around here and other places, but have found nothing that gives a high-level advice on what direction follow.  So, I'll put it to the smart people :-):</p>\n<p>What's the best / simplest way to solve this problem?  Should I use a natural language parser, dsl, lucene/solr, or some other tool/technology?  NLP seems like it may work, but it looks really complex.  I'd rather not spend a whole lot of time doing a deep dive just to find out it can't do what I'm looking for or that there is a simpler solution.</p>\n<h1>Requirements</h1>\n<p>Given these recipe ingredient descriptions....</p>\n<ol>\n<li>&quot;8 cups of mixed greens (about 5 ounces)&quot;</li>\n<li>&quot;Eight skinless chicken thighs (about 1¼ lbs)&quot;</li>\n<li>&quot;6.5 tablespoons extra-virgin olive oil&quot;</li>\n<li>&quot;approximately 6 oz. thinly sliced smoked salmon, cut into strips&quot;</li>\n<li>&quot;2 whole chickens (3 .5 pounds each)&quot;</li>\n<li>&quot;20 oz each frozen chopped spinach, thawed&quot;</li>\n<li>&quot;.5 cup parmesan cheese, grated&quot;</li>\n<li>&quot;about .5 cup pecans, toasted and finely ground&quot;</li>\n<li>&quot;.5 cup Dixie Diner Bread Crumb Mix, plain&quot;</li>\n<li>&quot;8 garlic cloves, minced (4 tsp)&quot;</li>\n<li>&quot;8 green onions, cut into 2 pieces&quot;</li>\n</ol>\n<p>I want to turn it into this....</p>\n<pre>\n|-----|---------|-------------|-------------------------|--------|-----------|--------------------------------|-------------|\n|     | Measure |             |                         | weight | weight    |                                |             |\n| #   | value   | Measure     | ingredient              | value  | measure   | preparation                    | Brand Name  |\n|-----|---------|-------------|-------------------------|--------|-----------|--------------------------------|-------------|\n| 1.  | 8       | cups        | mixed greens            | 5      | ounces    | -                              | -           |\n| 2.  | 8       | -           | skinless chicken thigh  | 1.5    | pounds    | -                              | -           |\n| 3.  | 6.5     | tablespoons | extra-virgin olive oil  | -      | -         | -                              | -           |\n| 4.  | 6       | ounces      | smoked salmon           | -      | -         | thinly sliced, cut into strips | -           |\n| 5.  | 2       | -           | whole chicken           | 3.5    | pounds    | -                              | -           |\n| 6.  | 20      | ounces      | forzen chopped spinach  | -      |           | thawed                         | -           |\n| 7.  | .5      | cup         | parmesean cheese        | -      | -         | grated                         | -           |\n| 8.  | .5      | cup         | pecans                  | -      | -         | toasted, finely ground         | -           |\n| 9.  | .5      | cup         | Bread Crumb Mix, plain  | -      | -         | -                              | Dixie Diner |\n| 10. | 8       | -           | garlic clove            | 4      | teaspoons | minced                         | -           |\n| 11. | 8       | -           | green onions            | -      | -         | cut into 2 pieces              | -           |\n|-----|---------|-------------|-------------------------|--------|-----------|--------------------------------|-------------|\n</pre>\n<p>Note the diversity of the descriptions.  Some things are abbreviated, some are not.  Some numbers are numbers, some are spelled out.</p>\n<p>I would love something that does a perfect parse/translation.  But, would settle for something that does reasonably well to start.</p>\n<p>Bonus question:  after suggesting a strategy / tool, how would you go about it?</p>\n<p>Thanks!</p>\n<p>Joe</p>\n",
    "score": 13,
    "creation_date": 1318026651,
    "view_count": 1745,
    "answer_count": 5,
    "tags": "java;nlp;dsl;text-parsing"
  },
  {
    "question_id": 2587663,
    "title": "Natural Language parsing of an appointment?",
    "body": "<p>I'm looking for a Java library to help parse user entered text that represents an 'appointment' for a calendar application.  For instance:</p>\n\n<p>Lunch with Mike at 11:30 on Tuesday</p>\n\n<p>or</p>\n\n<p>5pm Happy hour on Friday </p>\n\n<p>I've found some promising leads like <a href=\"https://github.com/samtingleff/jchronic\" rel=\"nofollow noreferrer\">https://github.com/samtingleff/jchronic</a> and <a href=\"http://www.datejs.com/\" rel=\"nofollow noreferrer\">http://www.datejs.com/</a> which can parse dates - but I also need to be able to extract the title of the event like \"Lunch with Mike\".</p>\n\n<p>If such an API doesn't exist, I'm also interested in any thoughts on how best to approach the problem from a coding perspective.</p>\n",
    "score": 13,
    "creation_date": 1270581995,
    "view_count": 2728,
    "answer_count": 3,
    "tags": "java;datetime;parsing;nlp"
  },
  {
    "question_id": 71679626,
    "title": "what is so special about special tokens?",
    "body": "<p>what exactly is the difference between &quot;token&quot; and a &quot;special token&quot;?</p>\n<p>I understand the following:</p>\n<ul>\n<li>what is a typical token</li>\n<li>what is a typical special token: MASK, UNK, SEP, etc</li>\n<li>when do you add a token (when you want to expand your vocab)</li>\n</ul>\n<p>What I don't understand is, under what kind of capacity will you want to create a new special token, any examples what we need it for and when we want to create a special token other than those default special tokens? If an example uses a special token, why can't a normal token achieve the same objective?</p>\n<pre><code>tokenizer.add_tokens(['[EOT]'], special_tokens=True)\n</code></pre>\n<p>And I also dont quite understand the following description in the source documentation.\nwhat difference does it do to our model if we set add_special_tokens to False?</p>\n<pre><code>add_special_tokens (bool, optional, defaults to True) — Whether or not to encode the sequences with the special tokens relative to their model.\n</code></pre>\n",
    "score": 13,
    "creation_date": 1648652286,
    "view_count": 8440,
    "answer_count": 1,
    "tags": "nlp;tokenize;huggingface-transformers;bert-language-model;huggingface-tokenizers"
  },
  {
    "question_id": 14708047,
    "title": "How to extract the noun phrases using Open nlp&#39;s chunking parser",
    "body": "<p>I am newbie to Natural Language processing.I need to extract the noun phrases from the text.So far i have used open nlp's chunking parser for parsing my text to get the Tree structure.But i am not able to extract the noun phrases from the tree structure, is there any regular expression pattern in open nlp so that i can use it to extract the noun phrases.</p>\n\n<p>Below is the code that i am using </p>\n\n<pre><code>    InputStream is = new FileInputStream(\"en-parser-chunking.bin\");\n    ParserModel model = new ParserModel(is);\n    Parser parser = ParserFactory.create(model);\n    Parse topParses[] = ParserTool.parseLine(line, parser, 1);\n        for (Parse p : topParses){\n                 p.show();}\n</code></pre>\n\n<p>Here I am getting the output as</p>\n\n<p>(TOP (S (S (ADJP (JJ welcome) (PP (TO to) (NP (NNP Big) (NNP Data.))))) (S (NP (PRP We)) (VP (VP (VBP are) (VP (VBG working) (PP (IN on) (NP (NNP Natural) (NNP Language) (NNP Processing.can))))) (NP (DT some) (CD one) (NN help)) (NP (PRP us)) (PP (IN in) (S (VP (VBG extracting) (NP (DT the) (NN noun) (NNS phrases)) (PP (IN from) (NP (DT the) (NN tree) (WP stucture.))))))))))</p>\n\n<p>Can some one please help me in getting the noun phrases like NP,NNP,NN etc.Can some one tell me do I need to use any other NP Chunker to get the noun phrases?Is there any regex pattern to achieve the same.</p>\n\n<p>Please help me on this.</p>\n\n<p>Thanks in advance</p>\n\n<p>Gouse.</p>\n",
    "score": 13,
    "creation_date": 1360068832,
    "view_count": 11249,
    "answer_count": 3,
    "tags": "java;nlp;stanford-nlp;opennlp"
  },
  {
    "question_id": 62677651,
    "title": "OpenAI GPT-2 model use with TensorFlow JS",
    "body": "<p>Is that possible to generate texts from OpenAI GPT-2 using TensorFlowJS?</p>\n<p>If not what is the limitation, like model format or ...?</p>\n",
    "score": 13,
    "creation_date": 1593609121,
    "view_count": 3549,
    "answer_count": 2,
    "tags": "tensorflow;machine-learning;nlp;tensorflow.js;gpt-2"
  },
  {
    "question_id": 51372724,
    "title": "How to speed up spaCy lemmatization?",
    "body": "<p>I'm using spaCy (version 2.0.11) for lemmatization in the first step of my NLP pipeline but unfortunately it's taking a verrry long time. It is clearly the slowest part of my processing pipeline and I want to know if there are improvements I could be making. I am using a pipeline as:</p>\n\n<pre><code>nlp.pipe(docs_generator, batch_size=200, n_threads=6, disable=['ner'])\n</code></pre>\n\n<p>on a 8 core machine, and I have verified that the machine is using all the cores.</p>\n\n<p>On a corpus of about 3 million short texts totaling almost 2gb it takes close to 24hrs to lemmatize and write to disk. Reasonable?</p>\n\n<p>I have tried disabling a couple parts of the processing pipeline and found that it broke the lemmatization (parser, tagger). </p>\n\n<p><strong>Are there any parts of the default processing pipeline that are not required for lemmatization besides named entity recognition?</strong></p>\n\n<p><strong>Are there other ways of speeding up the spaCy lemmatization process?</strong></p>\n\n<p>Aside:</p>\n\n<p>It also appears that documentation doesn't list all the operations in the parsing pipeline. At the top of the spacy Language class we have:</p>\n\n<pre><code>factories = {\n    'tokenizer': lambda nlp: nlp.Defaults.create_tokenizer(nlp),\n    'tensorizer': lambda nlp, **cfg: Tensorizer(nlp.vocab, **cfg),\n    'tagger': lambda nlp, **cfg: Tagger(nlp.vocab, **cfg),\n    'parser': lambda nlp, **cfg: DependencyParser(nlp.vocab, **cfg),\n    'ner': lambda nlp, **cfg: EntityRecognizer(nlp.vocab, **cfg),\n    'similarity': lambda nlp, **cfg: SimilarityHook(nlp.vocab, **cfg),\n    'textcat': lambda nlp, **cfg: TextCategorizer(nlp.vocab, **cfg),\n    'sbd': lambda nlp, **cfg: SentenceSegmenter(nlp.vocab, **cfg),\n    'sentencizer': lambda nlp, **cfg: SentenceSegmenter(nlp.vocab, **cfg),\n    'merge_noun_chunks': lambda nlp, **cfg: merge_noun_chunks,\n    'merge_entities': lambda nlp, **cfg: merge_entities\n}\n</code></pre>\n\n<p>which includes some items not covered in the documentation here: \n<a href=\"https://spacy.io/usage/processing-pipelines\" rel=\"noreferrer\">https://spacy.io/usage/processing-pipelines</a></p>\n\n<p>Since they are not covered I don't really know which may be disabled, nor what their dependencies are.</p>\n",
    "score": 13,
    "creation_date": 1531797353,
    "view_count": 7293,
    "answer_count": 2,
    "tags": "performance;nlp;spacy"
  },
  {
    "question_id": 14084711,
    "title": "How can I extract datetime from freeform text?",
    "body": "<p>I'm trying to come up with something along the lines of Google Calendar (or even some gmail messages), where freeform text will be parsed and converted to specific dates/times.</p>\n\n<p>Some examples (assume for simplicity that right now is January 01, 2013 at 1am):</p>\n\n<pre><code>\"I should call Mom tomorrow to wish her a happy birthday\" -&gt; \"tomorrow\" = \"2013-01-02\"\n\"The super bowl is on Feb 3rd at 6:30pm\" -&gt; \"Feb 3rd at 6:30\" =&gt; \"2013-02-03T06:30:00Z\"\n\"Remind me to take out the trash on Friday\" =&gt; \"Friday\" =&gt; \"2013-01-04\"\n</code></pre>\n\n<p>First of all I'll ask this - are there any already existing open source libraries that this (or part of this). If not, what sort of approaches do you think I should take?</p>\n\n<p>I am thinking of a few different possibilities:</p>\n\n<ol>\n<li>Lots of regular expressions, as many as I can come up with for each different use case</li>\n<li>Some sort of Bayesian Net that looks at n-grams and categorizes them into different scenarios like \"relative date\", \"relative day of week\", \"specific date\", \"date and time\", and then runs it through a rules engine (maybe more regex) to figure out the actual date.</li>\n<li>Sending it to a Google search and try to extract meaningful information from the search results (this one is probably not realistic)</li>\n</ol>\n",
    "score": 13,
    "creation_date": 1356805996,
    "view_count": 1225,
    "answer_count": 1,
    "tags": "javascript;datetime;nlp"
  },
  {
    "question_id": 3888063,
    "title": "Parser for Wikipedia",
    "body": "<p>I downloaded a Wikipedia dump and I want to convert the wiki format into my object format. Is there a wiki parser available that converts the object into XML?</p>\n",
    "score": 13,
    "creation_date": 1286517736,
    "view_count": 9689,
    "answer_count": 6,
    "tags": "java;mediawiki;nlp;nsxmlparser;wikipedia"
  },
  {
    "question_id": 9459745,
    "title": "Find words and combinations of words that can be spoken the quickest",
    "body": "<p>I'm a big fan of discovering sentences that can be rapped very quickly.  For example, \"gotta read a little bit of Wikipedia\" or \"don't wanna wind up in the gutter with a bottle of malt.\" (George Watsky) </p>\n\n<p>I wanted to write a program in Python that would enable me to find words (or combinations of words) that can be articulated such that it sounds very fast when spoken.  </p>\n\n<p>I initially thought that words that had a high syllable to letter ratio would be the best, but upon writing a Python program to do find those words, I retrieved only very simple words that didn't really sound fast (e.g. \"iowa\").  </p>\n\n<p>So I'm at a loss at what actually makes words sound fast.  Is it the morpheme to letter ratio?  Is it the number of alternating vowel-consonant pairs?  </p>\n\n<p>How would you guys go about devising a python program to resolve this problem?</p>\n",
    "score": 13,
    "creation_date": 1330313887,
    "view_count": 1423,
    "answer_count": 2,
    "tags": "python;algorithm;cpu-word;nlp;linguistics"
  },
  {
    "question_id": 4185199,
    "title": "Perl or Java Sentiment Analysis",
    "body": "<p>I was wondering if anybody knew of any good Perl modules and/or Java classes for sentiment analysis.  I have read about LingPipe, but the program would eventually need to be used for commercial use so something open-source would be better.  I also looked into GATE, but their documentation on sentiment analysis is sparse at best.</p>\n",
    "score": 13,
    "creation_date": 1289830824,
    "view_count": 3945,
    "answer_count": 3,
    "tags": "java;perl;nlp;sentiment-analysis"
  },
  {
    "question_id": 42422593,
    "title": "Is it possible to add your own WordNet to a library?",
    "body": "<p>I have a .txt file of a Danish WordNet. Is there any way to use this with an NLP library for Python such as NLTK? If not, how might you go about natural language processing in a language that is not supported by a given library. Also say you want to do named entity recognition in a language other than English or Dutch in a library like spaCy. Is there any way to do this?</p>\n",
    "score": 13,
    "creation_date": 1487871340,
    "view_count": 2112,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;nltk;spacy"
  },
  {
    "question_id": 13690194,
    "title": "Spelling correction for person names (Python)",
    "body": "<p>I have a large collection of person names (e.g. \"john smith\"). I want to look up people by name in it. However, some of the queries will be misspelled (e.g. \"jon smth\", \"johnsm ith\"). Are there any spelling correction libraries with Python bindings that might find spelling-corrected matches for me?</p>\n\n<p>I'm aware of Whoosh and Python-aspell. Whoosh's spelling correction doesn't quite work for me because it writes the collection of correct spellings to disk rather than storing it in memory. That makes lookups too slow for my application. It doesn't seem to be trivial to change this behavior, because of how the code is structured. Also Whoosh does not weight different character-edits differently even though, say, a 'y' is much more likely to be confused with an 'i' (\"jim kazinsky\" -> \"jim kazinski\") than it is a 'z'. </p>\n\n<p>Aspell doesn't work well with person names, since names typically contain white space -- Aspell considers the word to be the fundamental unit of correction. Also, as I understand it, Aspell uses an n-gram model of spelling correction, rather than a character-edit distance model. While an n-gram model makes sense for dictionary words, it doesn't work as well for names: the people \"bob ruzatoxg\" and \"joe ruzatoxg\" have a lot of rare trigrams in common, since they have the same rare last name. But they're clearly different people.</p>\n\n<p>I should also mention that I can't just compare each query to all of the entries in the collection -- that would be too slow. Some index needs to get built beforehand.</p>\n\n<p>Thanks!</p>\n",
    "score": 13,
    "creation_date": 1354562587,
    "view_count": 4188,
    "answer_count": 1,
    "tags": "python;nlp;spell-checking;nltk;whoosh"
  },
  {
    "question_id": 395421,
    "title": "Is there open source software available that analyses a string and guesses the gender of the author?",
    "body": "<p>I can't find anything other than closed-source web applications. Are there any active projects? I'd be interested in using the software in something I'm developing and getting involved.</p>\n",
    "score": 13,
    "creation_date": 1230412820,
    "view_count": 613,
    "answer_count": 10,
    "tags": "string;file-io;nlp"
  },
  {
    "question_id": 7080309,
    "title": "Intelligent transliteration in PHP",
    "body": "<p>I'm interested in writing a PHP script (I do welcome language-agnostic suggestions) that would transliterate a sentence or word written in English (phoenetically) into the script of another language. Since I'm looking at English written phoenetically (i.e. by ear): I'd have to deal with variant spellings of the same word.</p>\n\n<p>It is assumed that no standard exists for romanization (for instance, in Chinese, you have the Simplified Wade, etc.) </p>\n\n<p>Does anyone have any advice on where I could start? </p>\n\n<p>EDIT: I'm doing this purely for educational purposes, and I was initially under the impression that in order to figure out the connection between variant spellings (which could be found in a corpus of IM messages, Facebook posts written in the romanized form of the language), you'd need some sort of machine learning tool. However, I'd like to know if I was on the right track, and I'd like some help in figuring out what next I should look into to get this working (for instance: which machine learning tool should I look into?). </p>\n",
    "score": 13,
    "creation_date": 1313506684,
    "view_count": 1018,
    "answer_count": 2,
    "tags": "php;nlp"
  },
  {
    "question_id": 41618119,
    "title": "Wit.ai recognizes numbers as location",
    "body": "<p>We are facing the issue that wit.ai recognizes almost every number as a location. Sometimes even as a DateTime, but almost never as a number. We tried to teach it that 1 is a number, 2 is a number, etc., but it doesn't seem to pick that up, see the screenshot below:</p>\n\n<p><a href=\"https://i.sstatic.net/R1tfn.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/R1tfn.png\" alt=\"enter image description here\"></a></p>\n\n<p>Are we doing something wrong?</p>\n",
    "score": 13,
    "creation_date": 1484238011,
    "view_count": 438,
    "answer_count": 1,
    "tags": "machine-learning;nlp;wit.ai"
  },
  {
    "question_id": 28575082,
    "title": "Classify a noun into abstract or concrete using NLTK or similar",
    "body": "<p>How can I categorize a list of nouns into abstract or concrete in Python?</p>\n\n<p>For example: </p>\n\n<pre><code>\"Have a seat in that chair.\"\n</code></pre>\n\n<p>In above sentence <code>chair</code> is noun and can be categorized as concrete.</p>\n",
    "score": 13,
    "creation_date": 1424227480,
    "view_count": 2687,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 15574915,
    "title": "Stanford Core NLP how to get the probability &amp; margin of error",
    "body": "<p>When using the parser or for the matter any of the Annotation in <strong>Core NLP</strong>, is there a way to access the probability or the margin of error?</p>\n<p>I am particularly interested in detecting ambiguity programmatically. For example, in the sentence below, <strong>desire</strong> is tagged as a noun, but it could also be a verb.</p>\n<p>I want to know if there is a way to retrieve a confidence score from the CoreNLP API that indicates ambiguity.</p>\n<pre><code>(NP (NP (NNP Whereas)) (, ,) (NP (NNP users) (NN desire) (S (VP (TO to) (VP (VB sell))))))\n</code></pre>\n<p>In this case, <strong>desire</strong> is labeled as NN (noun) instead of a verb. I need a way to check how confident <strong>CoreNLP</strong> is about this classification.</p>\n",
    "score": 13,
    "creation_date": 1363968557,
    "view_count": 655,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;pos-tagger"
  },
  {
    "question_id": 30150047,
    "title": "Find all locations / cities / places in a text",
    "body": "<p>If I have a text containing for example an article of a newspaper in Catalan language, how could I find all cities from that text?</p>\n\n<p>I have been looking at the package nltk for python and I have downloaded the corpus for catalan language (nltk.corpus.cess_cat).</p>\n\n<p>What I have at this moment:\nI have installed all necessary from nltk.download().  An example of what I have at this moment:</p>\n\n<pre><code>te = nltk.word_tokenize('Tots els gats son de Sant Cugat del Valles.')\n\nnltk.pos_tag(te)\n</code></pre>\n\n<p>The city is 'Sant Cugat del Valles'. What I get from the output is:</p>\n\n<pre><code>[('Tots', 'NNS'),\n ('els', 'NNS'),\n ('gats', 'NNS'),\n ('son', 'VBP'),\n ('de', 'IN'),\n ('Sant', 'NNP'),\n ('Cugat', 'NNP'),\n ('del', 'NN'),\n ('Valles', 'NNP')]\n</code></pre>\n\n<p>NNP seems to indicate nouns whose first letter is uppercase. Is there a way of getting places or cities and not all names? \n Thank you</p>\n",
    "score": 12,
    "creation_date": 1431252045,
    "view_count": 28138,
    "answer_count": 4,
    "tags": "python;nltk;corpus;text-analysis;tagged-corpus"
  },
  {
    "question_id": 60937617,
    "title": "How to reconstruct text entities with Hugging Face&#39;s transformers pipelines without IOB tags?",
    "body": "<p>I've been looking to use Hugging Face's Pipelines for NER (named entity recognition). However, it is returning the entity labels in inside-outside-beginning (IOB) format but <a href=\"https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\" rel=\"noreferrer\">without the IOB labels</a>. So I'm not able to map the output of the pipeline back to my original text. Moreover, the outputs are masked in BERT tokenization format (the default model is BERT-large).</p>\n\n<p>For example: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\nnlp_bert_lg = pipeline('ner')\nprint(nlp_bert_lg('Hugging Face is a French company based in New York.'))\n</code></pre>\n\n<p>The output is:</p>\n\n<pre><code>[{'word': 'Hu', 'score': 0.9968873858451843, 'entity': 'I-ORG'},\n{'word': '##gging', 'score': 0.9329522848129272, 'entity': 'I-ORG'},\n{'word': 'Face', 'score': 0.9781811237335205, 'entity': 'I-ORG'},\n{'word': 'French', 'score': 0.9981815814971924, 'entity': 'I-MISC'},\n{'word': 'New', 'score': 0.9987512826919556, 'entity': 'I-LOC'},\n{'word': 'York', 'score': 0.9976728558540344, 'entity': 'I-LOC'}]\n</code></pre>\n\n<p>As you can see, New York is broken up into two tags.</p>\n\n<p>How can I map Hugging Face's NER Pipeline back to my original text?</p>\n\n<p>Transformers version: 2.7</p>\n",
    "score": 12,
    "creation_date": 1585594683,
    "view_count": 13130,
    "answer_count": 4,
    "tags": "nlp;tokenize;transformer-model;named-entity-recognition;huggingface-transformers"
  },
  {
    "question_id": 12184304,
    "title": "Extracting text from garbled PDF",
    "body": "<p>I have a PDF file with valuable textual information.</p>\n\n<p>The problem is that I cannot extract the text, all I get is a bunch of garbled symbols. The same happens if I copy and paste the text from the PDF reader to a text file. Even <em>File -> Save as text</em> in Acrobat Reader fails.</p>\n\n<p>I have used all tools I could get my hands on and the result is the same. I believe that this has something to do with fonts embedding, but I don't know what exactly?</p>\n\n<p>My questions:</p>\n\n<ul>\n<li><strong>What is the culprit</strong> of this weird <strong>text garbling</strong>?</li>\n<li><strong>How to extract the text content</strong> from the PDF (programmatically, with a tool, manipulating the bits directly, etc.)?</li>\n<li>How to fix the PDF to not garble on copy?</li>\n</ul>\n",
    "score": 12,
    "creation_date": 1346265020,
    "view_count": 40156,
    "answer_count": 3,
    "tags": "pdf;file-format;text-analysis"
  },
  {
    "question_id": 54717449,
    "title": "Mapping word vector to the most similar/closest word using spaCy",
    "body": "<p>I am using spaCy as part of a topic modelling solution and I have a situation where I need to map a derived word vector to the \"closest\" or \"most similar\" word in a vocabulary of word vectors.</p>\n\n<p>I see gensim has a function (WordEmbeddingsKeyedVectors.similar_by_vector) to calculate this, but I was wondering if spaCy has something like this to map a vector to a word within its vocabulary (nlp.vocab)?</p>\n",
    "score": 12,
    "creation_date": 1550267001,
    "view_count": 8990,
    "answer_count": 5,
    "tags": "nlp;spacy;word2vec;word-embedding"
  },
  {
    "question_id": 728006,
    "title": "Crawling The Internet",
    "body": "<p>I want to crawl for specific things. Specifically events that are taking place like concerts, movies, art gallery openings, etc, etc. Anything that one might spend time going to.</p>\n\n<p>How do I implement a crawler?</p>\n\n<p>I have heard of Grub (grub.org -> Wikia) and Heritix (<a href=\"http://crawler.archive.org/\" rel=\"noreferrer\">http://crawler.archive.org/</a>)</p>\n\n<p>Are there others?</p>\n\n<p>What opinions does everyone have?</p>\n\n<p>-Jason</p>\n",
    "score": 12,
    "creation_date": 1239147548,
    "view_count": 3701,
    "answer_count": 10,
    "tags": "nlp;web-crawler;information-retrieval;text-mining"
  },
  {
    "question_id": 55111360,
    "title": "Using BERT for next sentence prediction",
    "body": "<p>Google's <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"noreferrer\">BERT</a> is pretrained on next sentence prediction tasks, but I'm wondering if it's possible to call the next sentence prediction function on new data. </p>\n\n<p>The idea is: given sentence A and given sentence B, I want a probabilistic label for whether or not sentence B follows sentence A. BERT is pretrained on a huge set of data, so I was hoping to use this next sentence prediction on new sentence data. I can't seem to figure out if this next sentence prediction function can be called and if so, how. Thanks for your help!</p>\n",
    "score": 12,
    "creation_date": 1552343379,
    "view_count": 15587,
    "answer_count": 2,
    "tags": "tensorflow;deep-learning;reproducible-research;nlp"
  },
  {
    "question_id": 40481348,
    "title": "Is it possible to edit NLTK&#39;s vader sentiment lexicon?",
    "body": "<p>I would like to add words to the <code>vader_lexicon.txt</code> to specify polarity scores to a word. What is the right way to do so?</p>\n\n<p>I saw this file in <code>AppData\\Roaming\\nltk_data\\sentiment\\vader_lexicon</code>. The file consists of the word, its polarity, intensity, and an array of 10 intensity scores given by \"10 independent human raters\". [1] However, when I edited it, nothing changed in the results of the following code:</p>\n\n<pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\ns = sia.polarity_scores(\"my string here\")\n</code></pre>\n\n<p>I think that this text file is accessed by my code when I called SentimentIntensityAnalyzer's constructor. [2] Do you have any ideas on how I can edit a pre-made lexicon?</p>\n\n<p>Sources:</p>\n\n<p>[1] <a href=\"https://github.com/cjhutto/vaderSentiment\" rel=\"noreferrer\">https://github.com/cjhutto/vaderSentiment</a></p>\n\n<p>[2] <a href=\"http://www.nltk.org/api/nltk.sentiment.html\" rel=\"noreferrer\">http://www.nltk.org/api/nltk.sentiment.html</a></p>\n",
    "score": 12,
    "creation_date": 1478590465,
    "view_count": 12683,
    "answer_count": 2,
    "tags": "python;nlp;nltk;sentiment-analysis;vader"
  },
  {
    "question_id": 66149878,
    "title": "E053 Could not read config.cfg Resumeparser",
    "body": "<pre><code>spacy.load('en_core_web_sm')\n\nfrom pyresparser import ResumeParser\ndata = ResumeParser('Resume.pdf').get_extracted_data()\n\nOSError: [E053] Could not read config.cfg from C:\\Users\\Kumaran\\anaconda3\\envs\\nlp\\lib\\site-packages\\pyresparser\\config.cfg\n\nOSError                                   Traceback (most recent call last)\n&lt;ipython-input-3-941548297df0&gt; in &lt;module&gt;\n      1 from pyresparser import ResumeParser\n----&gt; 2 data = ResumeParser('Resume.pdf').get_extracted_data()\n\n~\\anaconda3\\envs\\nlp\\lib\\site-packages\\pyresparser\\resume_parser.py in __init__(self, resume, skills_file, custom_regex)\n     19     ):\n     20         nlp = spacy.load('en_core_web_sm')\n---&gt; 21         custom_nlp = spacy.load(os.path.dirname(os.path.abspath(__file__)))\n     22         self.__skills_file = skills_file\n     23         self.__custom_regex = custom_regex\n\n~\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\__init__.py in load(name, disable, exclude, config)\n     45     RETURNS (Language): The loaded nlp object.\n     46     &quot;&quot;&quot;\n---&gt; 47     return util.load_model(name, disable=disable, exclude=exclude, config=config)\n     48 \n     49 \n</code></pre>\n",
    "score": 12,
    "creation_date": 1613025577,
    "view_count": 39245,
    "answer_count": 2,
    "tags": "python;parsing;nlp;spacy"
  },
  {
    "question_id": 16630627,
    "title": "How to recreate same DocumentTermMatrix with new (test) data",
    "body": "<p>Suppose I have text based training data and testing data. To be more specific, I have two data sets - training and testing - and both of them have one column which contains text and is of interest for the job at hand.</p>\n\n<p>I used tm package in R to process the text column in the training data set. After removing the white spaces, punctuation, and stop words, I stemmed the corpus and finally created a document term matrix of 1 grams containing the frequency/count of the words in each document. I then took a pre-determined cut-off of, say, 50 and kept only those terms that have a count of greater than 50.  </p>\n\n<p>Following this, I train a, say, GLMNET model using the DTM and the dependent variable (which was present in the training data). Everything runs smooth and easy till now.</p>\n\n<p>However, how do I proceed when I want to score/predict the model on the testing data or any new data that might come in the future?</p>\n\n<p>Specifically, what I am trying to find out is that how do I create the exact DTM on new data? </p>\n\n<p>If the new data set does not have any of the similar words as the original training data then all the terms should have a count of zero (which is fine). But I want to be able to replicate the exact same DTM (in terms of structure) on any new corpus.</p>\n\n<p>Any ideas/thoughts?</p>\n",
    "score": 12,
    "creation_date": 1368927009,
    "view_count": 8920,
    "answer_count": 2,
    "tags": "r;machine-learning;nlp;text-mining;tm"
  },
  {
    "question_id": 76963311,
    "title": "llama-cpp-python not using NVIDIA GPU CUDA",
    "body": "<p>I have been playing around with <a href=\"https://github.com/oobabooga/text-generation-webui\" rel=\"noreferrer\">oobabooga text-generation-webui </a> on my Ubuntu 20.04 with my NVIDIA GTX 1060 6GB for some weeks without problems. I have been using llama2-chat models sharing memory between my RAM and NVIDIA VRAM. I installed without much problems following the intructions on its repository.</p>\n<p>So what I want now is to use the model loader <code>llama-cpp</code> with its package <code>llama-cpp-python</code> bindings to play around with it by myself. So using the same miniconda3 environment that oobabooga text-generation-webui uses I started a jupyter notebook and I could make inferences and everything is working well <em>BUT ONLY for CPU</em>.</p>\n<p>A working example bellow,</p>\n<pre><code>from llama_cpp import Llama\n\nllm = Llama(model_path=&quot;/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin&quot;, \n            n_gpu_layers=32, n_threads=6, n_ctx=3584, n_batch=521, verbose=True), \n\nprompt = &quot;&quot;&quot;[INST] &lt;&lt;SYS&gt;&gt;\nName the planets in the solar system? \n&lt;&lt;/SYS&gt;&gt;\n[/INST] \n&quot;&quot;&quot;\noutput = llm(prompt, max_tokens=350, echo=True)\nprint(output['choices'][0]['text'].split('[/INST]')[-1])\n</code></pre>\n<blockquote>\n<p>Of course! Here are the eight planets in our solar system, listed in order from closest to farthest from the Sun:</p>\n<ol>\n<li>Mercury</li>\n<li>Venus</li>\n<li>Earth</li>\n<li>Mars</li>\n<li>Jupiter</li>\n<li>Saturn</li>\n<li>Uranus</li>\n<li>Neptune</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>Note that Pluto was previously considered a planet but is now classified as a dwarf planet due to its small size and unique orbit.</p>\n</blockquote>\n<p>I want to make inference using GPU as well. What is wrong?\nWhy can't I offload to gpu like the parameter <code>n_gpu_layers=32</code> specifies and also like <code>oobabooga text-generation-webui</code> already does on the same miniconda environment whithout any problems?</p>\n",
    "score": 12,
    "creation_date": 1692808557,
    "view_count": 62009,
    "answer_count": 9,
    "tags": "python;python-3.x;nlp;llama;llama-cpp-python"
  },
  {
    "question_id": 70185150,
    "title": "Return all possible entity types from spaCy model?",
    "body": "<p>Is there a method to extract all possible named entity types from a model in spaCy? You can manually figure it out by running on sample text, but I imagine there is a more programmatic way to do this?\nFor example:</p>\n<pre><code>import spacy\nmodel=spacy.load(&quot;en_core_web_sm&quot;)\nmodel.*returns_entity_types*\n</code></pre>\n",
    "score": 12,
    "creation_date": 1638365620,
    "view_count": 10054,
    "answer_count": 2,
    "tags": "python;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 27588702,
    "title": "Need a good relation extractor",
    "body": "<p>I'm doing a NLP project.</p>\n\n<p>The purpose of the project is to extract possible relationship between two things. For example, for a pair \"location\" and \"person\" the extracted results would be \"near\", \"lives in\", \"works in\", etc.</p>\n\n<p>Is there any existing NLP tool capable of doing this?</p>\n",
    "score": 12,
    "creation_date": 1419159457,
    "view_count": 7772,
    "answer_count": 3,
    "tags": "nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 39410282,
    "title": "Coreference resolution in python nltk using Stanford coreNLP",
    "body": "<p>Stanford CoreNLP provides coreference resolution <a href=\"http://nlp.stanford.edu/software/dcoref.shtml\" rel=\"noreferrer\">as mentioned here</a>, also <a href=\"https://stackoverflow.com/questions/30954649/coreference-resolution-using-stanford-corenlp\">this thread</a>, <a href=\"https://stackoverflow.com/questions/30362691/stanford-corenlp-wrong-coreference-resolution\">this</a>,   provides some insights about its implementation in Java.</p>\n\n<p>However, I am using python and NLTK and I am not sure how can I use Coreference resolution functionality of CoreNLP in my python code. I have been able to set up StanfordParser in NLTK, this is my code so far.</p>\n\n<pre><code>from nltk.parse.stanford import StanfordDependencyParser\nstanford_parser_dir = 'stanford-parser/'\neng_model_path = stanford_parser_dir  + \"stanford-parser-models/edu/stanford/nlp/models/lexparser/englishRNN.ser.gz\"\nmy_path_to_models_jar = stanford_parser_dir  + \"stanford-parser-3.5.2-models.jar\"\nmy_path_to_jar = stanford_parser_dir  + \"stanford-parser.jar\"\n</code></pre>\n\n<p>How can I use coreference resolution of CoreNLP in python?</p>\n",
    "score": 12,
    "creation_date": 1473419567,
    "view_count": 18744,
    "answer_count": 4,
    "tags": "python;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 13725861,
    "title": "Generate all word forms using Lucene &amp; Hunspell",
    "body": "<p>In an application I work on, we use Lucene Analyzer, especially it's Hunspell part. The problem I face is: I need to generate all word forms of a word, using a set of affix rules.</p>\n<p>E.g. having the word 'educate' and affix rules ABC, generate all forms of word 'educate.' - educates, educated, educative, etc.</p>\n<p>What I'd like to know is: is it possible to do this using Lucene's Hunspell implementation (we use a Hunspell dictionary (.dic) and affix file (.aff), so it has to be a Hunspell API)? Lucene's Hunspell API isn't that big, I went through it, and didn't find something suitable.</p>\n<p>Nearest I could find on SO was <a href=\"https://stackoverflow.com/questions/893436/best-practices-for-seaching-for-alternate-forms-of-a-word-with-lucene\">this</a>, but there are no answers related to hunspell.</p>\n",
    "score": 12,
    "creation_date": 1354719103,
    "view_count": 6178,
    "answer_count": 5,
    "tags": "lucene;nlp;cpu-word;hunspell"
  },
  {
    "question_id": 22118350,
    "title": "Python - Sentiment Analysis using Pointwise Mutual Information",
    "body": "<pre><code>from __future__ import division\nimport urllib\nimport json\nfrom math import log\n\n\ndef hits(word1,word2=\"\"):\n    query = \"http://ajax.googleapis.com/ajax/services/search/web?v=1.0&amp;q=%s\"\n    if word2 == \"\":\n        results = urllib.urlopen(query % word1)\n    else:\n        results = urllib.urlopen(query % word1+\" \"+\"AROUND(10)\"+\" \"+word2)\n    json_res = json.loads(results.read())\n    google_hits=int(json_res['responseData']['cursor']['estimatedResultCount'])\n    return google_hits\n\n\ndef so(phrase):\n    num = hits(phrase,\"excellent\")\n    #print num\n    den = hits(phrase,\"poor\")\n    #print den\n    ratio = num / den\n    #print ratio\n    sop = log(ratio)\n    return sop\n\nprint so(\"ugly product\")\n</code></pre>\n\n<p>I need this code to calculate the Point wise Mutual Information which can be used to classify reviews as positive or negative. Basically I am using the technique specified by Turney(2002): <a href=\"http://acl.ldc.upenn.edu/P/P02/P02-1053.pdf\" rel=\"noreferrer\">http://acl.ldc.upenn.edu/P/P02/P02-1053.pdf</a> as an example for an unsupervised classification method for sentiment analysis.</p>\n\n<p>As explained in the paper, the semantic orientation of a phrase is negative if the phrase is more strongly associated with the word \"poor\" and positive if it is more strongly associated with the word \"excellent\".</p>\n\n<p>The code above calculates the SO of a phrase. I use Google to calculate the number of hits and calculate the SO.(as AltaVista is now not there)</p>\n\n<p>The values computed are very erratic. They don't stick to a particular pattern.\nFor example SO(\"ugly product\") turns out be 2.85462098541 while SO(\"beautiful product\") is 1.71395061117. While the former is expected to be negative and the other positive.</p>\n\n<p>Is there something wrong with the code? Is there an easier way to calculate SO of a phrase (using PMI) with any Python library,say NLTK? I tried NLTK but was not able to find any explicit method which computes the PMI.</p>\n",
    "score": 12,
    "creation_date": 1393697836,
    "view_count": 23570,
    "answer_count": 3,
    "tags": "python;nlp;nltk;sentiment-analysis"
  },
  {
    "question_id": 15730473,
    "title": "Wordnet Find Synonyms",
    "body": "<p>I am searching for a way to find all the synonyms of a particular word using wordnet. I am using JAWS.</p>\n\n<p><strong>For example:</strong></p>\n\n<p><strong>love(v):</strong> admire, adulate, be attached to, be captivated by, be crazy about, be enamored of, be enchanted by, be fascinated with, be fond of, be in love with, canonize, care for, cherish, choose, deify, delight in, dote on, esteem, exalt, fall for, fancy, glorify, go for, gone on....</p>\n\n<p><strong>love(n):</strong> \nSynonym : adulation, affection, allegiance, amity, amorousness, amour, appreciation, ardency, ardor, attachment, case*, cherishing, crush, delight, devotedness, devotion, emotion, enchantment, enjoyment, fervor, fidelity, flame, fondness, friendship, hankering, idolatry, inclination, infatuation, involvement</p>\n\n<p>In a related <a href=\"https://stackoverflow.com/questions/13745631/trying-to-find-synonyms-using-wordnet-java-api\">question</a> user Ram has pointed to some code but that does not suffice as it just gives a vastly different output:</p>\n\n<p>love, passion: any object of warm affection or devotion beloved, dear, dearest, honey, \nlove: a beloved person; used as terms of endearment\nlove, sexual love, erotic love: a deep feeling of sexual desire and attraction\nlove: a score of zero in tennis or squash\nsexual love, lovemaking, making love, love, love life: sexual activities (often including sexual intercourse) between two people\nlove: have a great affection or liking for</p>\n\n<p>So how do I achieve it and is wordnet suited for what I want to do?</p>\n",
    "score": 12,
    "creation_date": 1364740339,
    "view_count": 25070,
    "answer_count": 4,
    "tags": "nlp;wordnet;synonym;jaws-wordnet"
  },
  {
    "question_id": 63633534,
    "title": "Is it necessary to do stopwords removal ,Stemming/Lemmatization for text classification while using Spacy,Bert?",
    "body": "<p>Is stopwords removal ,Stemming and Lemmatization necessary   for text classification while using Spacy,Bert or other advanced NLP models for getting the vector embedding of the text ?</p>\n<p>text=&quot;The food served in the wedding was very delicious&quot;</p>\n<p>1.since Spacy,Bert were trained on huge raw datasets are there any benefits of apply stopwords removal ,Stemming and Lemmatization on these text before generating the embedding using bert/spacy for text classification task ?</p>\n<p>2.I can understand stopwords removal ,Stemming and Lemmatization will be good when we use countvectorizer,tfidf vectorizer to get embedding of sentences .</p>\n",
    "score": 12,
    "creation_date": 1598616619,
    "view_count": 17409,
    "answer_count": 4,
    "tags": "nlp;spacy;text-classification;bert-language-model"
  },
  {
    "question_id": 27673527,
    "title": "How should I vectorize the following list of lists with scikit learn?",
    "body": "<p>I would like to vectorize with scikit learn a list who has lists. I go to the path where I have the training texts I read them and then I obtain something like this:</p>\n\n<pre><code>corpus = [[\"this is spam, 'SPAM'\"],[\"this is ham, 'HAM'\"],[\"this is nothing, 'NOTHING'\"]]\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(analyzer='word')\nvect_representation= vect.fit_transform(corpus)\nprint vect_representation.toarray()\n</code></pre>\n\n<p>And I get the following:</p>\n\n<pre><code>return lambda x: strip_accents(x.lower())\nAttributeError: 'list' object has no attribute 'lower'\n</code></pre>\n\n<p>Also the problem with this are the labels at the end of each document, how should I treat them in order to do a correct classification?.</p>\n",
    "score": 12,
    "creation_date": 1419738323,
    "view_count": 19730,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;scikit-learn"
  },
  {
    "question_id": 3901266,
    "title": "Find the words in a long stream of characters. Auto-tokenize",
    "body": "<p>How would you find the correct words in a long stream of characters?</p>\n\n<p>Input :</p>\n\n<pre><code>\"The revised report onthesyntactictheoriesofsequentialcontrolandstate\"\n</code></pre>\n\n<p>Google's Output: </p>\n\n<pre><code>\"The revised report on syntactic theories sequential controlandstate\"\n</code></pre>\n\n<p>(which is close enough considering the time that they produced the output)</p>\n\n<p>How do you think Google does it? \nHow would you increase the accuracy? </p>\n",
    "score": 12,
    "creation_date": 1286730505,
    "view_count": 2469,
    "answer_count": 5,
    "tags": "algorithm;computer-science;nlp;string-algorithm"
  },
  {
    "question_id": 18911589,
    "title": "How to test whether a word is in singular form or not in python?",
    "body": "<p>I am trying to get whether a word is in singular form or in plural form by using nltk pos_tag. But the results are not accurate.</p>\n\n<p>So, I need a way to find how can get whether a word is in singular form or in plural form? moreover I need it without using any python package.</p>\n",
    "score": 12,
    "creation_date": 1379663638,
    "view_count": 9374,
    "answer_count": 2,
    "tags": "python-2.7;nlp;nltk;wordnet"
  },
  {
    "question_id": 37443138,
    "title": "Python stemming (with pandas dataframe)",
    "body": "<p>I created a dataframe with sentences to be stemmed.\nI would like to use a Snowballstemmer to obtain higher accuracy with my classification algorithm. How can I achieve this?</p>\n<pre><code>import pandas as pd\nfrom nltk.stem.snowball import SnowballStemmer\n\n# Use English stemmer.\nstemmer = SnowballStemmer(&quot;english&quot;)\n\n# Sentences to be stemmed.\ndata = [&quot;programmers program with programming languages&quot;, &quot;my code is working so there must be a bug in the interpreter&quot;] \n    \n# Create the Pandas dataFrame.\ndf = pd.DataFrame(data, columns = ['unstemmed']) \n\n# Split the sentences to lists of words.\ndf['unstemmed'] = df['unstemmed'].str.split()\n\n# Make sure we see the full column.\npd.set_option('display.max_colwidth', -1)\n\n# Print dataframe.\ndf \n\n+----+---------------------------------------------------------------+\n|    | unstemmed                                                     |\n|----+---------------------------------------------------------------|\n|  0 | ['programmers', 'program', 'with', 'programming', 'languages']|\n|  1 | ['my', 'code', 'is', 'working', 'so', 'there', 'must',        |  \n|    |  'be', 'a', 'bug', 'in', 'the', 'interpreter']                |\n+----+---------------------------------------------------------------+\n</code></pre>\n",
    "score": 12,
    "creation_date": 1464194481,
    "view_count": 35024,
    "answer_count": 1,
    "tags": "python;pandas;nlp;stemming"
  },
  {
    "question_id": 21600196,
    "title": "How do I use sklearn CountVectorizer with both &#39;word&#39; and &#39;char&#39; analyzer? - python",
    "body": "<p>How do I use sklearn CountVectorizer with both 'word' and 'char' analyzer?\n<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\" rel=\"noreferrer\">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a></p>\n\n<p>I could extract the text features by word or char separately but how do i create a <code>charword_vectorizer</code>? Is there a way to combine the vectorizers? or use more than one analyzer?</p>\n\n<pre><code>&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer\n&gt;&gt;&gt; word_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=1)\n&gt;&gt;&gt; char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 2), min_df=1)\n&gt;&gt;&gt; x = ['this is a foo bar', 'you are a foo bar black sheep']\n&gt;&gt;&gt; word_vectorizer.fit_transform(x)\n&lt;2x15 sparse matrix of type '&lt;type 'numpy.int64'&gt;'\n    with 18 stored elements in Compressed Sparse Column format&gt;\n&gt;&gt;&gt; char_vectorizer.fit_transform(x)\n&lt;2x47 sparse matrix of type '&lt;type 'numpy.int64'&gt;'\n    with 64 stored elements in Compressed Sparse Column format&gt;\n&gt;&gt;&gt; char_vectorizer.get_feature_names()\n[u' ', u' a', u' b', u' f', u' i', u' s', u'a', u'a ', u'ac', u'ar', u'b', u'ba', u'bl', u'c', u'ck', u'e', u'e ', u'ee', u'ep', u'f', u'fo', u'h', u'he', u'hi', u'i', u'is', u'k', u'k ', u'l', u'la', u'o', u'o ', u'oo', u'ou', u'p', u'r', u'r ', u're', u's', u's ', u'sh', u't', u'th', u'u', u'u ', u'y', u'yo']\n&gt;&gt;&gt; word_vectorizer.get_feature_names()\n[u'are', u'are foo', u'bar', u'bar black', u'black', u'black sheep', u'foo', u'foo bar', u'is', u'is foo', u'sheep', u'this', u'this is', u'you', u'you are']\n</code></pre>\n",
    "score": 12,
    "creation_date": 1391682447,
    "view_count": 14533,
    "answer_count": 2,
    "tags": "python;machine-learning;scikit-learn;analyzer;text-analysis"
  },
  {
    "question_id": 2664629,
    "title": "Where can I find a list of English phrases?",
    "body": "<p>I'm tasked with searching for the use of cliches and common phrases in text. The phrases are similar to the phrases you might see for the phrase puzzles on Wheel of Fortune. Here are a few examples:</p>\n\n<ul>\n<li>Easy Come Easy Go</li>\n<li>Too Good To be True</li>\n<li>Winning Isn't Everything</li>\n</ul>\n\n<p>I cannot find a list of phrases however. Does anybody know of such a list?</p>\n\n<p>Seriously, even a list of all Wheel of Fortune solutions would suffice.</p>\n",
    "score": 12,
    "creation_date": 1271638180,
    "view_count": 4430,
    "answer_count": 5,
    "tags": "nlp;linguistics"
  },
  {
    "question_id": 60463829,
    "title": "Training TFBertForSequenceClassification with custom X and Y data",
    "body": "<p>I am working on a TextClassification problem, for which I am trying to traing my model on TFBertForSequenceClassification given in huggingface-transformers library.</p>\n\n<p>I followed the example given on their <a href=\"https://github.com/huggingface/transformers#quick-tour-tf-20-training-and-pytorch-interoperability\" rel=\"noreferrer\">github</a> page, I am able to run the sample code with given sample data using <code>tensorflow_datasets.load('glue/mrpc')</code>.\nHowever, I am unable to find an example on how to load my own custom data and pass it in \n<code>model.fit(train_dataset, epochs=2, steps_per_epoch=115, validation_data=valid_dataset, validation_steps=7)</code>. </p>\n\n<p>How can I define my own X, do tokenization of my X and prepare train_dataset with my X and Y. Where X represents my input text and Y represents classification category of given X.</p>\n\n<p>Sample Training dataframe : </p>\n\n<pre><code>    text    category_index\n0   Assorted Print Joggers - Pack of 2 ,/ Gray Pri...   0\n1   \"Buckle\" ( Matt ) for 35 mm Width Belt  0\n2   (Gagam 07) Barcelona Football Jersey Home 17 1...   2\n3   (Pack of 3 Pair) Flocklined Reusable Rubber Ha...   1\n4   (Summer special Offer)Firststep new born baby ...   0\n</code></pre>\n",
    "score": 12,
    "creation_date": 1582969786,
    "view_count": 12455,
    "answer_count": 4,
    "tags": "nlp;pytorch;tensorflow2.0;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 56036037,
    "title": "Text normalization : Text similarity in Python. How to normalize Text spelling mismatch?",
    "body": "<p>i have a dataframe with a column A as below :</p>\n\n<pre><code>Column A\nCarrefour supermarket\nCarrefour hypermarket\nCarrefour\ncarrefour\nCarrfour downtown\nCarrfor market\nLulu\nLulu Hyper\nLulu dxb\nlulu airport\nk.m trading\nKM Trading\nKM trade\nK.M.  Trading\nKM.Trading\n</code></pre>\n\n<p>I wanted to derive at the below \"column A\" :</p>\n\n<pre><code>Column A\nCarrefour\nCarrefour\nCarrefour\nCarrefour\nCarrefour\nCarrefour\nLulu\nLulu\nLulu\nLulu\nKM Trading\nKM Trading\nKM Trading\nKM Trading\nKM Trading\n</code></pre>\n\n<p>To do this, i code as below :</p>\n\n<pre><code>MERCHANT_NAME_DICT = {\"lulu\": \"Lulu\", \"carrefour\": \"Carrefour\",  \"km\": \"KM Trading\"}\n\ndef replace_merchant_name(row):\n    \"\"\"Provided a long merchant name replace it with short name.\"\"\"\n    processed_row = re.sub(r'\\s+|\\.', '', row.lower()).strip()\n    for key, value in MERCHANT_NAME_DICT.items():\n        if key in processed_row:\n            return value\n\n    return row\n\nframe['MERCHANT_NAME'] = frame['MERCHANT_NAME'].astype(str)\nframe.MERCHANT_NAME = frame.MERCHANT_NAME.apply(lambda row: replace_merchant_name(row))\n</code></pre>\n\n<p>But i wanted to use NLP Logic and make it a generic function ( Instead of using values for mapping ). Just call the generic function and run it on any similar data column and get the desired results. I am pretty new to NLP Concepts, so looking for some help on it friends.</p>\n\n<p>NOTE : Basically i wanted a generic NLP way coding to find all similar words from a given column ( or in a list ).</p>\n",
    "score": 12,
    "creation_date": 1557302228,
    "view_count": 3640,
    "answer_count": 2,
    "tags": "python-3.x;nlp"
  },
  {
    "question_id": 37875614,
    "title": "How to use syntaxnet output",
    "body": "<p>I started playing with Syntaxnet two days ago and I'm wondering <strong>how to use/export</strong> the output (ascii tree or conll ) in a format that is easy to parse (ie : Json, XML, python graph).</p>\n\n<p>Thanks for your help ! </p>\n",
    "score": 12,
    "creation_date": 1466148556,
    "view_count": 5255,
    "answer_count": 3,
    "tags": "nlp;syntaxnet"
  },
  {
    "question_id": 36283391,
    "title": "difflib.get_close_matches GET SCORE",
    "body": "<p>I am trying to get the score of the best match using <code>difflib.get_close_matches</code>:</p>\n\n<pre><code>import difflib\n\nbest_match = difflib.get_close_matches(str,str_list,1)[0]\n</code></pre>\n\n<p>I know of the option to add '<code>cutoff</code>' parameter, but couldn't find out how to get the actual score after setting the threshold.\nAm I missing something? Is there a better solution to match unicode strings?</p>\n",
    "score": 12,
    "creation_date": 1459252054,
    "view_count": 22519,
    "answer_count": 3,
    "tags": "python-2.7;text;text-analysis"
  },
  {
    "question_id": 48925328,
    "title": "How to get all noun phrases in Spacy",
    "body": "<p>I am new to <code>Spacy</code> and I would like to extract \"all\" the noun phrases from a sentence. I'm wondering how I can do it. I have the following code:</p>\n\n<pre><code>import spacy\n\nnlp = spacy.load(\"en\")\n\nfile = open(\"E:/test.txt\", \"r\")\ndoc = nlp(file.read())\nfor np in doc.noun_chunks:\n    print(np.text)\n</code></pre>\n\n<p>But it returns only the base noun phrases, that is, phrases which don't have any other <code>NP</code> in them. That is, for the following phrase, I get the result below:</p>\n\n<p>Phrase: <code>We try to explicitly describe the geometry of the edges of the images.</code></p>\n\n<p>Result: <code>We, the geometry, the edges, the images</code>.</p>\n\n<p>Expected result: <code>We, the geometry, the edges, the images, the geometry of the edges of the images, the edges of the images.</code></p>\n\n<p>How can I get all the noun phrases, including nested phrases?</p>\n",
    "score": 12,
    "creation_date": 1519296064,
    "view_count": 14818,
    "answer_count": 5,
    "tags": "python;python-3.x;nlp;spacy"
  },
  {
    "question_id": 48362530,
    "title": "Doc2Vec.infer_vector keeps giving different result everytime on a particular trained model",
    "body": "<p>I am trying to follow the official Doc2Vec Gensim tutorial mentioned here - <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb\" rel=\"noreferrer\">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a></p>\n\n<p>I modified the code in line 10 to determine best matching document for the given query and everytime I run, I get a completely different resultset. My new code iin line 10 of the notebook is:</p>\n\n<p><code>\ninferred_vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\nsims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\nrank = [docid for docid, sim in sims]\nprint(rank)\n</code></p>\n\n<p>Everytime I run the piece of code, I get different set of documents that are matching with this query: \"only you can prevent forest fires\". The difference is stark and just does not seem to match.</p>\n\n<p>Is Doc2Vec not a suitable match for querying and information extraction? Or are there bugs?</p>\n",
    "score": 12,
    "creation_date": 1516494708,
    "view_count": 5485,
    "answer_count": 2,
    "tags": "nlp;word2vec;gensim;doc2vec"
  },
  {
    "question_id": 6482046,
    "title": "Why are these words considered stopwords?",
    "body": "<p>I do not have a formal background in Natural Language Processing was wondering if someone from the NLP side can shed some light on this. I am playing around with the <a href=\"http://www.nltk.org\" rel=\"nofollow\">NLTK</a> library and I was specifically looking into the stopwords function provided by this package:</p>\n\n<blockquote>\n  <p>In [80]:\n  nltk.corpus.stopwords.words('english')</p>\n  \n  <p>Out[80]: </p>\n  \n  <p>['i',  'me',  'my', \n  'myself',  'we',  'our',  'ours', \n  'ourselves',  'you',  'your', \n  'yours',  'yourself',  'yourselves', \n  'he',  'him',  'his',  'himself', \n  'she',  'her',  'hers',  'herself', \n  'it',  'its',  'itself',  'they', \n  'them',  'their',  'theirs', \n  'themselves',  'what',  'which', \n  'who',  'whom',  'this',  'that', \n  'these',  'those',  'am',  'is', \n  'are',  'was',  'were',  'be', \n  'been',  'being',  'have',  'has', \n  'had',  'having',  'do',  'does', \n  'did',  'doing',  'a',  'an',  'the', \n  'and',  'but',  'if',  'or', \n  'because',  'as',  'until',  'while', \n  'of',  'at',  'by',  'for',  'with', \n  'about',  'against',  'between', \n  'into',  'through',  'during', \n  'before',  'after',  'above', \n  'below',  'to',  'from',  'up', \n  'down',  'in',  'out',  'on',  'off', \n  'over',  'under',  'again', \n  'further',  'then',  'once',  'here', \n  'there',  'when',  'where',  'why', \n  'how',  'all',  'any',  'both', \n  'each',  'few',  'more',  'most', \n  'other',  'some',  'such',  'no', \n  'nor',  'not',  'only',  'own', \n  'same',  'so',  'than',  'too', \n  'very',  's',  't',  'can',  'will', \n  'just',  'don',  'should',  'now']</p>\n</blockquote>\n\n<p>What I don't understand is, why is the word \"not\" present? Isn't that necessary to determine the sentiment inside a sentence? For instance, a sentence like this:</p>\n\n<blockquote>\n  <p>I am not sure what the problem is.</p>\n</blockquote>\n\n<p>is totally different once the stopword <code>not</code> is removed changing the meaning of the sentence to its opposite (<code>I am sure what the problem is</code>). If that is the case, is there a set of rules that I am missing on when not to use these stopwords?</p>\n",
    "score": 12,
    "creation_date": 1309060556,
    "view_count": 2797,
    "answer_count": 1,
    "tags": "language-agnostic;machine-learning;nlp;nltk;stop-words"
  },
  {
    "question_id": 29099621,
    "title": "How to find out wether a word exists in english using nltk",
    "body": "<p>I am looking for a proper solution to this question. This question has been asked many times before and I didn't find a single answer that suited.\nI need to use a corpus in NLTK to detect whether a word is an English word</p>\n<p>I have tried to do :</p>\n<pre><code>wordnet.synsets(word)\n</code></pre>\n<p>This doesn't work for many common words.\nUsing a list of words in English and performing lookup in a file is not an option.\nUsing enchant is not an option either.\nIf there is another library that can do the same, please provide the usage of the api.\nIf not, please provide a corpus in nltk which has all the words in English.</p>\n",
    "score": 12,
    "creation_date": 1426596872,
    "view_count": 20264,
    "answer_count": 3,
    "tags": "python;python-3.x;nlp;nltk;wordnet"
  },
  {
    "question_id": 61560056,
    "title": "Extracting Key-Phrases from text based on the Topic with Python",
    "body": "<p>I have a large dataset with 3 columns, columns are text, phrase and topic. \nI want to find a way to extract key-phrases (phrases column) based on the topic.\nKey-Phrase can be part of the text value or the whole text value.</p>\n\n<pre><code>import pandas as pd\n\n\ntext = [\"great game with a lot of amazing goals from both teams\",\n        \"goalkeepers from both teams made misteke\",\n        \"he won all four grand slam championchips\",\n        \"the best player from three-point line\",\n        \"Novak Djokovic is the best player of all time\",\n        \"amazing slam dunks from the best players\",\n        \"he deserved yellow-card for this foul\",\n        \"free throw points\"]\n\nphrase = [\"goals\", \"goalkeepers\", \"grand slam championchips\", \"three-point line\", \"Novak Djokovic\", \"slam dunks\", \"yellow-card\", \"free throw points\"]\n\ntopic = [\"football\", \"football\", \"tennis\", \"basketball\", \"tennis\", \"basketball\", \"football\", \"basketball\"]\n\ndf = pd.DataFrame({\"text\":text,\n                   \"phrase\":phrase,\n                   \"topic\":topic})\n\nprint(df.text)\nprint(df.phrase)\n</code></pre>\n\n<p>I'm having big trouble with finding a path to do something like this, because I have more than 50000 rows in my dataset and around 48000 of unique values of phrases, and 3 different topics.</p>\n\n<p>I guess that building a dataset with all football, basketball and tennis topics are not really the best solution. So I was thinking about making some kind of ML model for this, but again that  means that I will have 2 features (text and topic) and one result (phrase), but I will have more than 48000 of different classes in my result, and that is not a good approach.</p>\n\n<p>I was thinking about using text column as a feature and applying classification model in order to find sentiment. After that I can use predicted sentiment to extract key features, but I do not know how to extract them. </p>\n\n<p>One more problem is that I get only 66% accuracy when I try to classify sentiment by using <code>CountVectorizer</code> or <code>TfidfTransformer</code> with Random Forest, Decision Tree, or any other classifying algorithm, and also 66% of accuracy if Im using <code>TextBlob</code> for sentiment analysis.</p>\n\n<p>Any help?</p>\n",
    "score": 12,
    "creation_date": 1588425650,
    "view_count": 12338,
    "answer_count": 3,
    "tags": "python;machine-learning;nlp;nltk"
  },
  {
    "question_id": 46924452,
    "title": "What to do when Seq2Seq network repeats words over and over in output?",
    "body": "<p>So, I've been working on a project for a while, we have <em>very</em> little data, I know it would become much better if we were able to put together a much much larger dataset. That aside, my issue at the moment is when I have a sentence input, my outputs look like this right now:</p>\n\n<blockquote>\n  <p>contactid contactid contactid contactid</p>\n</blockquote>\n\n<p>A single word is focused on and repeated over and over again. What can I do to overcome this hurdle?</p>\n\n<p>Things I've tried:</p>\n\n<ol>\n<li>Double checked I was appending start/stop tokens and make sure the tokens were properly placed in the top of their vocab files, I am sharing vocab.</li>\n<li>I found something saying it could be due to poor word embeddings. To that end I checked with tensorboard and sure enough PCA showed a very dense cluster of points. Seeing that I grabbed Facebook's public pre trained word vectors and loaded them in as the embedding. Trained again and this time tensorboard PCA showed a much better picture.</li>\n<li>Switched my training scheduler from basic to SampledScheduling to occasionally replace a training output with the ground truth.</li>\n<li>Switched my decoder to use the beam search decoder I figured this may give more robust responses if the word choices were close together in the intermediary feature space.</li>\n</ol>\n\n<p>For certain my perplexity is steadily decreasing.</p>\n\n<p>Here is my dataset preperation code:</p>\n\n<pre><code>class ModelInputs(object):\n\"\"\"Factory to construct various input hooks and functions depending on mode \"\"\"\n\ndef __init__(\n    self, vocab_files, batch_size,\n    share_vocab=True, src_eos_id=1, tgt_eos_id=2\n):\n    self.batch_size = batch_size\n    self.vocab_files = vocab_files\n    self.share_vocab = share_vocab\n    self.src_eos_id = src_eos_id\n    self.tgt_eos_id = tgt_eos_id\n\ndef get_inputs(self, file_path, num_infer=None, mode=tf.estimator.ModeKeys.TRAIN):\n    self.mode = mode\n    if self.mode == tf.estimator.ModeKeys.TRAIN:\n        return self._training_input_hook(file_path)\n    if self.mode == tf.estimator.ModeKeys.EVAL:\n        return self._validation_input_hook(file_path)\n    if self.mode == tf.estimator.ModeKeys.PREDICT:\n        if num_infer is None:\n            raise ValueError('If performing inference must supply number of predictions to be made.')\n        return self._infer_input_hook(file_path, num_infer)\n\ndef _prepare_data(self, dataset, out=False):\n    prep_set = dataset.map(lambda string: tf.string_split([string]).values)\n    prep_set = prep_set.map(lambda words: (words, tf.size(words)))\n    if out == True:\n        return prep_set.map(lambda words, size: (self.vocab_tables[1].lookup(words), size))\n    return prep_set.map(lambda words, size: (self.vocab_tables[0].lookup(words), size))\n\ndef _batch_data(self, dataset, src_eos_id, tgt_eos_id):\n    batched_set = dataset.padded_batch(\n            self.batch_size,\n            padded_shapes=((tf.TensorShape([None]), tf.TensorShape([])), (tf.TensorShape([None]), tf.TensorShape([]))),\n            padding_values=((src_eos_id, 0), (tgt_eos_id, 0))\n    )\n    return batched_set\n\ndef _batch_infer_data(self, dataset, src_eos_id):\n    batched_set = dataset.padded_batch(\n        self.batch_size,\n        padded_shapes=(tf.TensorShape([None]), tf.TensorShape([])),\n        padding_values=(src_eos_id, 0)\n    )\n    return batched_set\n\ndef _create_vocab_tables(self, vocab_files, share_vocab=False):\n    if vocab_files[1] is None and share_vocab == False:\n        raise ValueError('If share_vocab is set to false must provide target vocab. (src_vocab_file, \\\n                target_vocab_file)')\n\n    src_vocab_table = lookup_ops.index_table_from_file(\n        vocab_files[0],\n        default_value=UNK_ID\n    )\n\n    if share_vocab:\n        tgt_vocab_table = src_vocab_table\n    else:\n        tgt_vocab_table = lookup_ops.index_table_from_file(\n            vocab_files[1],\n            default_value=UNK_ID\n        )\n\n    return src_vocab_table, tgt_vocab_table\n\ndef _prepare_iterator_hook(self, hook, scope_name, iterator, file_path, name_placeholder):\n    if self.mode == tf.estimator.ModeKeys.TRAIN or self.mode == tf.estimator.ModeKeys.EVAL:\n        feed_dict = {\n                name_placeholder[0]: file_path[0],\n                name_placeholder[1]: file_path[1]\n        }\n    else:\n        feed_dict = {name_placeholder: file_path}\n\n    with tf.name_scope(scope_name):\n        hook.iterator_initializer_func = \\\n                lambda sess: sess.run(\n                    iterator.initializer,\n                    feed_dict=feed_dict,\n                )\n\ndef _set_up_train_or_eval(self, scope_name, file_path):\n    hook = IteratorInitializerHook()\n    def input_fn():\n        with tf.name_scope(scope_name):\n            with tf.name_scope('sentence_markers'):\n                src_eos_id = tf.constant(self.src_eos_id, dtype=tf.int64)\n                tgt_eos_id = tf.constant(self.tgt_eos_id, dtype=tf.int64)\n            self.vocab_tables = self._create_vocab_tables(self.vocab_files, self.share_vocab)\n            in_file = tf.placeholder(tf.string, shape=())\n            in_dataset = self._prepare_data(tf.contrib.data.TextLineDataset(in_file).repeat(None))\n            out_file = tf.placeholder(tf.string, shape=())\n            out_dataset = self._prepare_data(tf.contrib.data.TextLineDataset(out_file).repeat(None))\n            dataset = tf.contrib.data.Dataset.zip((in_dataset, out_dataset))\n            dataset = self._batch_data(dataset, src_eos_id, tgt_eos_id)\n            iterator = dataset.make_initializable_iterator()\n            next_example, next_label = iterator.get_next()\n            self._prepare_iterator_hook(hook, scope_name, iterator, file_path, (in_file, out_file))\n            return next_example, next_label\n\n    return (input_fn, hook)\n\ndef _training_input_hook(self, file_path):\n    input_fn, hook = self._set_up_train_or_eval('train_inputs', file_path)\n\n    return (input_fn, hook)\n\ndef _validation_input_hook(self, file_path):\n    input_fn, hook = self._set_up_train_or_eval('eval_inputs', file_path)\n\n    return (input_fn, hook)\n\ndef _infer_input_hook(self, file_path, num_infer):\n    hook = IteratorInitializerHook()\n\n    def input_fn():\n        with tf.name_scope('infer_inputs'):\n            with tf.name_scope('sentence_markers'):\n                src_eos_id = tf.constant(self.src_eos_id, dtype=tf.int64)\n            self.vocab_tables = self._create_vocab_tables(self.vocab_files, self.share_vocab)\n            infer_file = tf.placeholder(tf.string, shape=())\n            dataset = tf.contrib.data.TextLineDataset(infer_file)\n            dataset = self._prepare_data(dataset)\n            dataset = self._batch_infer_data(dataset, src_eos_id)\n            iterator = dataset.make_initializable_iterator()\n            next_example, seq_len = iterator.get_next()\n            self._prepare_iterator_hook(hook, 'infer_inputs', iterator, file_path, infer_file)\n            return ((next_example, seq_len), None)\n\n    return (input_fn, hook)\n</code></pre>\n\n<p>And here is my model:</p>\n\n<pre><code>class Seq2Seq():\n\ndef __init__(\n    self, batch_size, inputs,\n    outputs, inp_vocab_size, tgt_vocab_size,\n    embed_dim, mode, time_major=False,\n    enc_embedding=None, dec_embedding=None, average_across_batch=True,\n    average_across_timesteps=True, vocab_path=None, embedding_path='./data_files/wiki.simple.vec'\n):\n    embed_np = self._get_embedding(embedding_path)\n    if not enc_embedding:\n        self.enc_embedding = tf.contrib.layers.embed_sequence(\n            inputs,\n            inp_vocab_size,\n            embed_dim,\n            trainable=True,\n            scope='embed',\n            initializer=tf.constant_initializer(value=embed_np, dtype=tf.float32)\n        )\n    else:\n        self.enc_embedding = enc_embedding\n    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n        if not dec_embedding:\n            embed_outputs = tf.contrib.layers.embed_sequence(\n                outputs,\n                tgt_vocab_size,\n                embed_dim,\n                trainable=True,\n                scope='embed',\n                reuse=True\n            )\n            with tf.variable_scope('embed', reuse=True):\n                dec_embedding = tf.get_variable('embeddings')\n            self.embed_outputs = embed_outputs\n            self.dec_embedding = dec_embedding\n\n        else:\n            self.dec_embedding = dec_embedding\n    else:\n        with tf.variable_scope('embed', reuse=True):\n            self.dec_embedding = tf.get_variable('embeddings')\n\n    if mode == tf.estimator.ModeKeys.PREDICT and vocab_path is None:\n        raise ValueError('If mode is predict, must supply vocab_path')\n    self.vocab_path = vocab_path\n    self.inp_vocab_size = inp_vocab_size\n    self.tgt_vocab_size = tgt_vocab_size\n    self.average_across_batch = average_across_batch\n    self.average_across_timesteps = average_across_timesteps\n    self.time_major = time_major\n    self.batch_size = batch_size\n    self.mode = mode\n\ndef _get_embedding(self, embedding_path):\n    model = KeyedVectors.load_word2vec_format(embedding_path)\n    vocab = model.vocab\n    vocab_len = len(vocab)\n    return np.array([model.word_vec(k) for k in vocab.keys()])\n\ndef _get_lstm(self, num_units):\n    return tf.nn.rnn_cell.BasicLSTMCell(num_units)\n\ndef encode(self, num_units, num_layers, seq_len, cell_fw=None, cell_bw=None):\n    if cell_fw and cell_bw:\n        fw_cell = cell_fw\n        bw_cell = cell_bw\n    else:\n        fw_cell = self._get_lstm(num_units)\n        bw_cell = self._get_lstm(num_units)\n    encoder_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n        fw_cell,\n        bw_cell,\n        self.enc_embedding,\n        sequence_length=seq_len,\n        time_major=self.time_major,\n        dtype=tf.float32\n    )\n    c_state = tf.concat([bi_encoder_state[0].c, bi_encoder_state[1].c], axis=1)\n    h_state = tf.concat([bi_encoder_state[0].h, bi_encoder_state[1].h], axis=1)\n    encoder_state = tf.contrib.rnn.LSTMStateTuple(c=c_state, h=h_state)\n    return tf.concat(encoder_outputs, -1), encoder_state\n\ndef _train_decoder(self, decoder_cell, out_seq_len, encoder_state, helper):\n    if not helper:\n        helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n            self.embed_outputs,\n            out_seq_len,\n            self.dec_embedding,\n            0.3,\n        )\n        # helper = tf.contrib.seq2seq.TrainingHelper(\n        #     self.dec_embedding,\n        #     out_seq_len,\n        # )\n    projection_layer = layers_core.Dense(self.tgt_vocab_size, use_bias=False)\n    decoder = tf.contrib.seq2seq.BasicDecoder(\n        decoder_cell,\n        helper,\n        encoder_state,\n        output_layer=projection_layer\n    )\n    return decoder\n\ndef _predict_decoder(self, cell, encoder_state, beam_width, length_penalty_weight):\n    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(\n        encoder_state, multiplier=beam_width\n    )\n    with tf.name_scope('sentence_markers'):\n        sos_id = tf.constant(1, dtype=tf.int32)\n        eos_id = tf.constant(2, dtype=tf.int32)\n    start_tokens = tf.fill([self.batch_size], sos_id)\n    end_token = eos_id\n    projection_layer = layers_core.Dense(self.tgt_vocab_size, use_bias=False)\n    emb = tf.squeeze(self.dec_embedding)\n    decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n        cell=cell,\n        embedding=self.dec_embedding,\n        start_tokens=start_tokens,\n        end_token=end_token,\n        initial_state=tiled_encoder_state,\n        beam_width=beam_width,\n        output_layer=projection_layer,\n        length_penalty_weight=length_penalty_weight\n    )\n    return decoder\n\ndef decode(\n    self, num_units, out_seq_len,\n    encoder_state, cell=None, helper=None,\n    beam_width=None, length_penalty_weight=None\n):\n    with tf.name_scope('Decode'):\n        if cell:\n            decoder_cell = cell\n        else:\n            decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(2*num_units)\n        if self.mode != estimator.ModeKeys.PREDICT:\n            decoder = self._train_decoder(decoder_cell, out_seq_len, encoder_state, helper)\n        else:\n            decoder = self._predict_decoder(decoder_cell, encoder_state, beam_width, length_penalty_weight)\n        outputs = tf.contrib.seq2seq.dynamic_decode(\n            decoder,\n            maximum_iterations=20,\n            swap_memory=True,\n        )\n        outputs = outputs[0]\n        if self.mode != estimator.ModeKeys.PREDICT:\n            return outputs.rnn_output, outputs.sample_id\n        else:\n            return outputs.beam_search_decoder_output, outputs.predicted_ids\n\ndef prepare_predict(self, sample_id):\n    rev_table = lookup_ops.index_to_string_table_from_file(\n        self.vocab_path, default_value=UNK)\n    predictions = rev_table.lookup(tf.to_int64(sample_id))\n    return tf.estimator.EstimatorSpec(\n        predictions=predictions,\n        mode=tf.estimator.ModeKeys.PREDICT\n    )\n\ndef prepare_train_eval(\n    self, t_out,\n    out_seq_len, labels, lr,\n    train_op=None, loss=None\n):\n    if not loss:\n        weights = tf.sequence_mask(\n            out_seq_len,\n            dtype=t_out.dtype\n        )\n        loss = tf.contrib.seq2seq.sequence_loss(\n            t_out,\n            labels,\n            weights,\n            average_across_batch=self.average_across_batch,\n        )\n\n    if not train_op:\n        train_op = tf.contrib.layers.optimize_loss(\n            loss,\n            tf.train.get_global_step(),\n            optimizer='SGD',\n            learning_rate=lr,\n            summaries=['loss', 'learning_rate']\n        )\n\n    return tf.estimator.EstimatorSpec(\n        mode=self.mode,\n        loss=loss,\n        train_op=train_op,\n    )\n</code></pre>\n",
    "score": 12,
    "creation_date": 1508908383,
    "view_count": 6851,
    "answer_count": 2,
    "tags": "machine-learning;tensorflow;nlp;translation"
  },
  {
    "question_id": 36587702,
    "title": "Convert sparse matrix (csc_matrix) to pandas dataframe",
    "body": "<p>I want to convert this matrix into a pandas dataframe.\n<a href=\"https://i.sstatic.net/qYkPp.png\" rel=\"noreferrer\">csc_matrix</a></p>\n\n<p>The <strong>first</strong> number in the bracket should be the <strong>index</strong>, the <strong>second</strong> number being <strong>columns</strong> and the <strong>number in the end</strong> being the <strong>data</strong>.</p>\n\n<p>I want to do this to do feature selection in text analysis, the first number represents the document, the second being the feature of word and the last number being the TFIDF score.</p>\n\n<p>Getting a dataframe helps me to transform the text analysis problem into data analysis.</p>\n",
    "score": 12,
    "creation_date": 1460515989,
    "view_count": 15118,
    "answer_count": 1,
    "tags": "python;pandas;dataframe;text-analysis;word-frequency"
  },
  {
    "question_id": 2910338,
    "title": "Python/YACC Lexer: Token priority?",
    "body": "<p>I'm trying to use reserved words in my grammar:</p>\n\n<pre><code>reserved = {\n   'if' : 'IF',\n   'then' : 'THEN',\n   'else' : 'ELSE',\n   'while' : 'WHILE',\n}\n\ntokens = [\n 'DEPT_CODE',\n 'COURSE_NUMBER',\n 'OR_CONJ',\n 'ID',\n] + list(reserved.values())\n\nt_DEPT_CODE = r'[A-Z]{2,}'\nt_COURSE_NUMBER  = r'[0-9]{4}'\nt_OR_CONJ = r'or'\n\nt_ignore = ' \\t'\n\ndef t_ID(t):\n r'[a-zA-Z_][a-zA-Z_0-9]*'\n if t.value in reserved.values():\n  t.type = reserved[t.value]\n  return t\n return None\n</code></pre>\n\n<p>However, the t_ID rule somehow swallows up DEPT_CODE and OR_CONJ. How can I get around this? I'd like those two to take higher precedence than the reserved words.</p>\n",
    "score": 12,
    "creation_date": 1274852759,
    "view_count": 4368,
    "answer_count": 2,
    "tags": "python;parsing;nlp;yacc"
  },
  {
    "question_id": 49564176,
    "title": "Python (NLTK) - more efficient way to extract noun phrases?",
    "body": "<p>I've got a machine learning task involving a large amount of text data. I want to identify, and extract, noun-phrases in the training text so I can use them for feature construction later on in the pipeline. \nI've extracted the type of noun-phrases I wanted from text but I'm fairly new to NLTK, so I approached this problem in a way where I can break down each step in list comprehensions like you can see below. </p>\n\n<p>But my real question is, am I reinventing the wheel here? Is there a faster way to do this that I'm not seeing?</p>\n\n<pre><code>import nltk\nimport pandas as pd\n\nmyData = pd.read_excel(\"\\User\\train_.xlsx\")\ntexts = myData['message']\n\n# Defining a grammar &amp; Parser\nNP = \"NP: {(&lt;V\\w+&gt;|&lt;NN\\w?&gt;)+.*&lt;NN\\w?&gt;}\"\nchunkr = nltk.RegexpParser(NP)\n\ntokens = [nltk.word_tokenize(i) for i in texts]\n\ntag_list = [nltk.pos_tag(w) for w in tokens]\n\nphrases = [chunkr.parse(sublist) for sublist in tag_list]\n\nleaves = [[subtree.leaves() for subtree in tree.subtrees(filter = lambda t: t.label == 'NP')] for tree in phrases]\n</code></pre>\n\n<p>flatten the list of lists of lists of tuples that we've ended up with, into\njust a list of lists of tuples</p>\n\n<pre><code>leaves = [tupls for sublists in leaves for tupls in sublists]\n</code></pre>\n\n<p>Join the extracted terms into one bigram</p>\n\n<pre><code>nounphrases = [unigram[0][1]+' '+unigram[1][0] in leaves]\n</code></pre>\n",
    "score": 12,
    "creation_date": 1522353845,
    "view_count": 22585,
    "answer_count": 4,
    "tags": "python-3.x;pandas;nlp;nltk;text-chunking"
  },
  {
    "question_id": 42824129,
    "title": "Dependency parsing tree in Spacy",
    "body": "<p>I have a sentence <strong>John saw a flashy hat at the store</strong><br>\nHow to represent this as a dependency tree as shown below?</p>\n\n<pre><code>(S\n      (NP (NNP John))\n      (VP\n        (VBD saw)\n        (NP (DT a) (JJ flashy) (NN hat))\n        (PP (IN at) (NP (DT the) (NN store)))))\n</code></pre>\n\n<p>I got this script from <a href=\"https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy\">here</a></p>\n\n<pre><code>import spacy\nfrom nltk import Tree\nen_nlp = spacy.load('en')\n\ndoc = en_nlp(\"John saw a flashy hat at the store\")\n\ndef to_nltk_tree(node):\n    if node.n_lefts + node.n_rights &gt; 0:\n        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n    else:\n        return node.orth_\n\n\n[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n</code></pre>\n\n<p>I am getting the following but I am looking for a tree(NLTK) format. </p>\n\n<pre><code>     saw                 \n  ____|_______________    \n |        |           at \n |        |           |   \n |       hat        store\n |     ___|____       |   \nJohn  a      flashy  the\n</code></pre>\n",
    "score": 12,
    "creation_date": 1489630386,
    "view_count": 7453,
    "answer_count": 2,
    "tags": "nlp;spacy;dependency-parsing"
  },
  {
    "question_id": 11428601,
    "title": "NLP Parser in Haskell",
    "body": "<p>Does Haskell have a good \n(a) natural language parser\n(b) part of speech tagger\n(c) nlp library (a la python's nltk)</p>\n",
    "score": 12,
    "creation_date": 1341995581,
    "view_count": 2984,
    "answer_count": 2,
    "tags": "haskell;nlp"
  },
  {
    "question_id": 2839548,
    "title": "Corpus/data set of English words with syllabic stress information?",
    "body": "<p>I know this is a long shot, but does anyone know of a dataset of English words that has stress information by syllable?  Something as simple as the following would be fantastic:</p>\n\n<pre><code>AARD vark\nA ble\na BOUT\nac COUNT\nAC id\nad DIC tion\nad VERT ise ment\n...\n</code></pre>\n",
    "score": 12,
    "creation_date": 1273914309,
    "view_count": 2334,
    "answer_count": 1,
    "tags": "dataset;nlp;corpus"
  },
  {
    "question_id": 1876103,
    "title": "I have a list of country codes and a list of language codes. How do I map from country code to language code?",
    "body": "<p>When the user visits the site, I can get their country code.  I want to use this to set the default language (which they can later modify if necessary, just a general guess as to what language they might speak based on what country they are in).  </p>\n\n<p>Is there a definitive mapping from country codes to language codes that exists somewhere?  I could not find it.  I know that not everyone in a particular country speaks the same language, but I just need a general mapping, the user can select their language manually later.</p>\n",
    "score": 12,
    "creation_date": 1260385597,
    "view_count": 11938,
    "answer_count": 2,
    "tags": "localization;internationalization;nlp;country-codes"
  },
  {
    "question_id": 4994899,
    "title": "Encoding for Multilingual .py Files",
    "body": "<p>I am writing a .py file that contains strings from multiple charactersets, including English, Spanish, and Russian. For example, I have something like:</p>\n\n<pre><code>string_en = \"The quick brown fox jumped over the lazy dog.\"  \nstring_es = \"El veloz murciélago hindú comía feliz cardillo y kiwi.\"\nstring_ru = \"В чащах юга жил бы цитрус? Да, но фальшивый экземпляр!\"\n</code></pre>\n\n<p>I am having trouble figuring out how to encode my file to avoid generating syntax errors like the one below when my file is run:</p>\n\n<pre><code>SyntaxError: Non-ASCII character '\\xc3' in file example.py on line 128, but no encoding\ndeclared; see http://www.python.org/peps/pep-0263.html for details\n</code></pre>\n\n<p>I've tried adding <code># -*- coding: utf-8 -*-</code> to the beginning of my file, but without any luck.  I've also tried marking my strings as unicode (i.e. <code>string_en = u'The quick brown fox jumped over the lazy dog.\"</code>), again unsuccessfully.  </p>\n\n<p>Is it possible to include characters from different Python codecs in one file, or am I attempting to do something that is not allowed?</p>\n",
    "score": 12,
    "creation_date": 1297702810,
    "view_count": 7824,
    "answer_count": 2,
    "tags": "python;unicode;encoding;nlp"
  },
  {
    "question_id": 57062456,
    "title": "Function call stack: keras_scratch_graph Error",
    "body": "<p>I am reimplementing a text2speech project.\nI am facing a <strong>Function call stack : keras_scratch_graph</strong> error in decoder part.\nThe network architecture is from Deep Voice 3 paper.</p>\n\n<p>I am using keras from TF 2.0 on Google Colab.\nBelow is the code for Decoder Keras Model.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>y1 = tf.ones(shape = (16, 203, 320))\ndef Decoder(name = \"decoder\"):\n    # Decoder Prenet\n    din = tf.concat((tf.zeros_like(y1[:, :1, -hp.mel:]), y1[:, :-1, -hp.mel:]), 1)\n    keys = K.Input(shape = (180, 256), batch_size = 16, name = \"keys\")\n    vals = K.Input(shape = (180, 256), batch_size = 16, name = \"vals\")\n    prev_max_attentions_li = tf.ones(shape=(hp.dlayer, hp.batch_size), dtype=tf.int32)\n    #prev_max_attentions_li = K.Input(tensor = prev_max_attentions_li)\n    for i in range(hp.dlayer):\n        dpout = K.layers.Dropout(rate = 0 if i == 0 else hp.dropout)(din)\n        fc_out = K.layers.Dense(hp.char_embed, activation = 'relu')(dpout)\n\n    print(\"=======================================================================================================\")\n    print(\"The FC value is \", fc_out)\n    print(\"=======================================================================================================\")\n\n    query_pe = K.layers.Embedding(hp.Ty, hp.char_embed)(tf.tile(tf.expand_dims(tf.range(hp.Ty // hp.r), 0), [hp.batch_size, 1]))\n    key_pe = K.layers.Embedding(hp.Tx, hp.char_embed)(tf.tile(tf.expand_dims(tf.range(hp.Tx), 0), [hp.batch_size, 1]))\n\n    alignments_li, max_attentions_li = [], []\n    for i in range(hp.dlayer):\n        dpout = K.layers.Dropout(rate = 0)(fc_out)\n        queries = K.layers.Conv1D(hp.datten_size, hp.dfilter, padding = 'causal', dilation_rate = 2**i)(dpout)\n        fc_out = (queries + fc_out) * tf.math.sqrt(0.5)\n        print(\"=======================================================================================================\")\n        print(\"The FC value is \", fc_out)\n        print(\"=======================================================================================================\")\n        queries = fc_out + query_pe\n        keys += key_pe\n\n        tensor, alignments, max_attentions = Attention(name = \"attention\")(queries, keys, vals, prev_max_attentions_li[i])\n\n        fc_out = (tensor + queries) * tf.math.sqrt(0.5)\n\n        alignments_li.append(alignments)\n        max_attentions_li.append(max_attentions)\n\n    decoder_output = fc_out\n\n    dpout = K.layers.Dropout(rate = 0)(decoder_output)\n    mel_logits = K.layers.Dense(hp.mel * hp.r)(dpout)\n\n    dpout = K.layers.Dropout(rate = 0)(fc_out)\n    done_output = K.layers.Dense(2)(dpout)\n\n    return K.Model(inputs = [keys, vals], outputs = [mel_logits, done_output, decoder_output, alignments_li, max_attentions_li], name = name)\n\n</code></pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>decode = Decoder()\nkin = tf.ones(shape = (16, 180, 256))\nvin = tf.ones(shape = (16, 180, 256))\nprint(decode(kin, vin))\ntf.keras.utils.plot_model(decode, to_file = \"decoder.png\", show_shapes = True)\n\n</code></pre>\n\n<p>When I test with some data, it shows the error messages below.\nIt's going to be some problem with \"fc_out\", but I dun know how to pass \"fc_out\" output from the first for loop to the second for loop?\nAny answer would be appreciated.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>File \"Decoder.py\", line 60, in &lt;module&gt;\n    decode = Decoder()\n  File \"Decoder.py\", line 33, in Decoder\n    dpout = K.layers.Dropout(rate = 0)(fc_out)\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 596, in __call__\n    base_layer_utils.create_keras_history(inputs)\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 199, in create_keras_history\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 245, in _create_keras_history_helper\n    layer_inputs, processed_ops, created_layers)\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 245, in _create_keras_history_helper\n    layer_inputs, processed_ops, created_layers)\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 245, in _create_keras_history_helper\n    layer_inputs, processed_ops, created_layers)\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 243, in _create_keras_history_helper\n    constants[i] = backend.function([], op_input)([])\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3510, in __call__\n    outputs = self._graph_fn(*converted_inputs)\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 572, in __call__\n    return self._call_flat(args)\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 671, in _call_flat\n    outputs = self._inference_function.call(ctx, args)\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 445, in call\n    ctx=ctx)\n  File \"/Users/ydc/dl-npm/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 67, in quick_execute\n    six.raise_from(core._status_to_exception(e.code, message), None)\n  File \"&lt;string&gt;\", line 3, in raise_from\ntensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable _AnonymousVar19 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar19/N10tensorflow3VarE does not exist.\n     [[node dense_7/BiasAdd/ReadVariableOp (defined at Decoder.py:33) ]] [Op:__inference_keras_scratch_graph_566]\n\nFunction call stack:\nkeras_scratch_graph\n\n</code></pre>\n",
    "score": 12,
    "creation_date": 1563297442,
    "view_count": 28967,
    "answer_count": 6,
    "tags": "python;tensorflow;keras;nlp;tensorflow2.0"
  },
  {
    "question_id": 44786174,
    "title": "How combine word embedded vectors to one vector?",
    "body": "<p>I know the meaning and methods of word embedding(skip-gram, CBOW) completely. And I know, that Google has a word2vector API that by getting the word can produce the vector. \nbut my problem is this: we have a clause that includes the subject, object, verb... that each word is previously embedded by the Google API, now \"How we can combine these vectors together to create a vector that is equal to the clause?\" \nExample: \nClause: V= \"dog bites man\"\nafter word embedding by the Google, we have V1, V2, V3 that each of them maps to the dog, bites, man. and we know that:\nV = V1+ V2 +V3\nHow can we provide V?\nI will appreciate if you explain it by taking an example of real vectors.</p>\n",
    "score": 12,
    "creation_date": 1498583556,
    "view_count": 10297,
    "answer_count": 3,
    "tags": "nlp;information-retrieval;word2vec;google-api-python-client;word-embedding"
  },
  {
    "question_id": 11876740,
    "title": "R stemming a string/document/corpus",
    "body": "<p>I'm trying to do some stemming in R but it only seems to work on individual documents.  My end goal is a term document matrix that shows the frequency of each term in the document.</p>\n\n<p>Here's an example:</p>\n\n<pre><code>require(RWeka)\nrequire(tm)\nrequire(Snowball)\n\nworder1&lt;- c(\"I am taking\",\"these are the samples\",\n\"He speaks differently\",\"This is distilled\",\"It was placed\")\ndf1 &lt;- data.frame(id=1:5, words=worder1)\n\n&gt; df1\n  id                 words\n1  1           I am taking\n2  2 these are the samples\n3  3 He speaks differently\n4  4     This is distilled\n5  5         It was placed\n</code></pre>\n\n<p>This method works for the stemming part but not the term document matrix part:</p>\n\n<pre><code>&gt; corp1 &lt;- Corpus(VectorSource(df1$words))\n&gt; inspect(corp1)\nA corpus with 5 text documents\n\nThe metadata consists of 2 tag-value pairs and a data frame\nAvailable tags are:\n  create_date creator \nAvailable variables in the data frame are:\n  MetaID \n\n[[1]]\nI am taking\n\n[[2]]\nthese are the samples\n\n[[3]]\nHe speaks differently\n\n[[4]]\nThis is distilled\n\n[[5]]\nIt was placed\n\n&gt; corp1 &lt;- tm_map(corp1, SnowballStemmer)\n&gt; inspect(corp1)\nA corpus with 5 text documents\n\nThe metadata consists of 2 tag-value pairs and a data frame\nAvailable tags are:\n  create_date creator \nAvailable variables in the data frame are:\n  MetaID \n\n[[1]]\n[1] I am tak\n\n[[2]]\n[1] these are the sampl\n\n[[3]]\n[1] He speaks differ\n\n[[4]]\n[1] This is distil\n\n[[5]]\n[1] It was plac\n\n&gt;  class(corp1)\n[1] \"VCorpus\" \"Corpus\"  \"list\"   \n&gt; tdm1 &lt;- TermDocumentMatrix(corp1)\nError in UseMethod(\"Content\", x) : \n  no applicable method for 'Content' applied to an object of class \"character\"\n</code></pre>\n\n<p>So instead I tried creating the term document matrix first but this time the words don't get stemmed:</p>\n\n<pre><code>&gt; corp1 &lt;- Corpus(VectorSource(df1$words))\n&gt; tdm1 &lt;- TermDocumentMatrix(corp1, control=list(stemDocument=TRUE))\n&gt;  as.matrix(tdm1)\n             Docs\nTerms         1 2 3 4 5\n  are         0 1 0 0 0\n  differently 0 0 1 0 0\n  distilled   0 0 0 1 0\n  placed      0 0 0 0 1\n  samples     0 1 0 0 0\n  speaks      0 0 1 0 0\n  taking      1 0 0 0 0\n  the         0 1 0 0 0\n  these       0 1 0 0 0\n  this        0 0 0 1 0\n  was         0 0 0 0 1\n</code></pre>\n\n<p>Here the words are obviously not stemmed.</p>\n\n<p>Any suggestions?</p>\n",
    "score": 12,
    "creation_date": 1344485829,
    "view_count": 11341,
    "answer_count": 4,
    "tags": "r;nlp;stemming;tm"
  },
  {
    "question_id": 69181078,
    "title": "SpaCy: how do you add custom NER labels to a pre-trained model?",
    "body": "<p>I am new to SpaCy and NLP. I am using SpaCy v 3.1 and Python 3.9.7 64-bit.</p>\n<p><strong>My objective</strong>: to use a pre-trained SpaCy model (<code>en_core_web_sm</code>) and add a set of custom labels to the existing NER labels (<code>GPE</code>, <code>PERSON</code>, <code>MONEY</code>, etc.) so that the model can recognize both the default AND the custom entities.</p>\n<p>I've looked at the SpaCy documentation and what I need seems to be an <a href=\"https://spacy.io/api/entityrecognizer\" rel=\"noreferrer\">EntityRecogniser</a>, specifically a new pipe.</p>\n<p>However, it is not really clear to me at what point in my workflow I should add this new pipe, since in SpaCy 3 the training happens in CLI, and from the docs it's not even clear to me where the pre-trained model is called.</p>\n<p>Any tutorials or pointers you might have are highly appreciated.</p>\n<p>This is what I think should be done, but I am not sure how:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy import displacy\nfrom spacy_langdetect import LanguageDetector\nfrom spacy.language import Language\nfrom spacy.pipeline import EntityRecognizer\n\n# Load model\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\n# Register custom component and turn a simple function into a pipeline component\n@Language.factory('new-ner')\ndef create_bespoke_ner(nlp, name):\n    \n    # Train the new pipeline with custom labels here??\n    \n    return LanguageDetector()\n\n# Add custom pipe\ncustom = nlp.add_pipe(&quot;new-ner&quot;)\n</code></pre>\n<p>This is what my config file looks like so far. I suspect my new pipe needs to go next to &quot;tok2vec&quot; and &quot;ner&quot;.</p>\n<pre><code>[paths]\ntrain = null\ndev = null\nvectors = null\ninit_tok2vec = null\n\n[system]\ngpu_allocator = null\nseed = 0\n\n[nlp]\nlang = &quot;en&quot;\npipeline = [&quot;tok2vec&quot;,&quot;ner&quot;]\nbatch_size = 1000\ndisabled = []\nbefore_creation = null\nafter_creation = null\nafter_pipeline_creation = null\ntokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}\n\n[components]\n\n[components.ner]\nfactory = &quot;ner&quot;\nincorrect_spans_key = null\nmoves = null\nupdate_with_oracle_cut_size = 100\n</code></pre>\n",
    "score": 12,
    "creation_date": 1631635367,
    "view_count": 8038,
    "answer_count": 1,
    "tags": "python;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 43288147,
    "title": "how do I use a very large (&gt;2M) word embedding in tensorflow?",
    "body": "<p>I am running a model with a very big word embedding (>2M words).  When I use tf.embedding_lookup, it expects the matrix, which is big.  When I run, I subsequently get out of GPU memory error.  If I reduce the size of the embedding, everything works fine.</p>\n\n<p>Is there a way to deal with larger embedding?</p>\n",
    "score": 12,
    "creation_date": 1491605258,
    "view_count": 3721,
    "answer_count": 1,
    "tags": "tensorflow;nlp;deep-learning;embedding;embedding-lookup"
  },
  {
    "question_id": 39241709,
    "title": "How to generate bi/tri-grams using spacy/nltk",
    "body": "<p>The input text are always list of dish names where there are 1~3 adjectives  and a noun</p>\n\n<p>Inputs</p>\n\n<pre><code>thai iced tea\nspicy fried chicken\nsweet chili pork\nthai chicken curry\n</code></pre>\n\n<p>outputs:</p>\n\n<pre><code>thai tea, iced tea\nspicy chicken, fried chicken\nsweet pork, chili pork\nthai chicken, chicken curry, thai curry\n</code></pre>\n\n<p>Basically, I am looking to parse the sentence tree and try to generate bi-grams by pairing an adjective with the noun.</p>\n\n<p>And I would like to achieve this with spacy or nltk</p>\n",
    "score": 12,
    "creation_date": 1472622794,
    "view_count": 16987,
    "answer_count": 3,
    "tags": "python;nlp;nltk;n-gram;spacy"
  },
  {
    "question_id": 35117491,
    "title": "Is it possible to re-train a word2vec model (e.g. GoogleNews-vectors-negative300.bin) from a corpus of sentences in python?",
    "body": "<p>I am using pre-trained Google news dataset for getting word vectors by using Gensim library in python</p>\n\n<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n</code></pre>\n\n<p>After loading the model I am converting training reviews sentence words into vectors</p>\n\n<pre><code>#reading all sentences from training file\nwith open('restaurantSentences', 'r') as infile:\nx_train = infile.readlines()\n#cleaning sentences\nx_train = [review_to_wordlist(review,remove_stopwords=True) for review in x_train]\ntrain_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_train])\n</code></pre>\n\n<p>During word2Vec process i get a lot of errors for the words in my corpus, that are not in the model. Problem is how can i retrain already pre-trained model (e.g GoogleNews-vectors-negative300.bin'), in order to get word vectors for those missing words.</p>\n\n<p>Following is what I have tried:\nTrained a new model from training sentences that I had</p>\n\n<pre><code># Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 10   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window    size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\nsentences = gensim.models.word2vec.LineSentence(\"restaurantSentences\")\n# Initialize and train the model (this will take some time)\nprint \"Training model...\"\nmodel = gensim.models.Word2Vec(sentences, workers=num_workers,size=num_features, min_count = min_word_count, \n                      window = context, sample = downsampling)\n\n\nmodel.build_vocab(sentences)\nmodel.train(sentences)\nmodel.n_similarity([\"food\"], [\"rice\"])\n</code></pre>\n\n<p>It worked! but the problem is that I have a really small dataset and less resources to train a large model.</p>\n\n<p>Second way that I am looking at is to extend the already trained model such as GoogleNews-vectors-negative300.bin.</p>\n\n<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\nsentences = gensim.models.word2vec.LineSentence(\"restaurantSentences\")\nmodel.train(sentences)\n</code></pre>\n\n<p>Is it possible and is that a good way to use, please help me out</p>\n",
    "score": 12,
    "creation_date": 1454264272,
    "view_count": 9470,
    "answer_count": 3,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 55179517,
    "title": "Python: Check if string and its substring are existing in the same list",
    "body": "<p>I've extracted keywords based on 1-gram, 2-gram, 3-gram within a tokenized sentence</p>\n\n<pre><code>list_of_keywords = []\nfor i in range(0, len(stemmed_words)):\n    temp = []\n    for j in range(0, len(stemmed_words[i])):\n        temp.append([' '.join(x) for x in list(everygrams(stemmed_words[i][j], 1, 3)) if ' '.join(x) in set(New_vocabulary_list)])\n    list_of_keywords.append(temp)\n</code></pre>\n\n<p>I've obtained keywords list as </p>\n\n<pre><code>['blood', 'pressure', 'high blood', 'blood pressure', 'high blood pressure']\n['sleep', 'anxiety', 'lack of sleep']\n</code></pre>\n\n<p>How can I simply the results by removing all substring within the list and remain:</p>\n\n<pre><code>['high blood pressure']\n['anxiety', 'lack of sleep']\n</code></pre>\n",
    "score": 12,
    "creation_date": 1552642587,
    "view_count": 1087,
    "answer_count": 5,
    "tags": "python;nlp"
  },
  {
    "question_id": 42982159,
    "title": "Differences between using Lex and Alexa",
    "body": "<p>I'm building an Alexa skill that will allow Alexa users to interact with a consumer facing e-commerce site.  There is functionality to call a representative that already exists on the site.  Now, I want to build out a voice app as a side project that extends that same option via a conversation.  There will be a need for slots like location, category of call, etc.  It's basically an Application/Transactional bot.</p>\n\n<p>In the future, if this is successful, I'd like that same general app to be accessible on different IoT devices (like Google Home Assistant, etc.)  Therefore, I'd like to abstract out the voice interactions and have the same (general) flow and API to interact with.</p>\n\n<p>This leaves me doing some research on different technologies like api.ai, wit.ai, Lex, etc.</p>\n\n<p>But, since this is an app for Alexa and I already rely on AWS and Amazon in general, I think I'd prefer to use Lex or just write a native Alexa app for now.</p>\n\n<p>I'm having a hard time understanding the differences between the two.  I understand that Alexa was built using Lex and I see that they have similar concepts like intent, slots, etc.</p>\n\n<p>But, I'm looking for any differences between the two services:</p>\n\n<ol>\n<li><p>Would using Lex allow me to more easily integrate with other devices?  Or is there any benefit?</p></li>\n<li><p>Would using Lex allow me greater flexibility in designing/modifying the flow of a conversation?  It seems like Lex is a little more complex and, therefore, might allow greater functionality.  </p></li>\n<li><p>Or is it just that Lex offers nearly the exact same functionality and is just meant for devices that aren't Alexa?</p></li>\n<li><p>Does Lex offer any more analytics processing than Alexa?  In Alexa I can only see intents/slots, but if I could see the actual text in Lex, that would be ideal.</p></li>\n</ol>\n",
    "score": 12,
    "creation_date": 1490287529,
    "view_count": 7098,
    "answer_count": 4,
    "tags": "amazon-web-services;nlp;alexa-skills-kit;dialogflow-es"
  },
  {
    "question_id": 9653815,
    "title": "How to get inflections for a word using Wordnet",
    "body": "<p>I want to get inflectional forms for a word using Wordnet.</p>\n\n<p>E.g. If the word is <code>make</code>, then its inflections are </p>\n\n<pre><code>made, makes, making\n</code></pre>\n\n<p>I tried all the options of the <code>wn</code> command but I did not get the inflections for a word.</p>\n\n<p>Any idea how to get these?</p>\n",
    "score": 12,
    "creation_date": 1331458222,
    "view_count": 3119,
    "answer_count": 2,
    "tags": "nlp;wordnet"
  },
  {
    "question_id": 26474847,
    "title": "Estimate Phonemic Similarity Between Two Words",
    "body": "<p>I am working on detecting rhymes in Python using the Carnegie Mellon University dictionary of pronunciation, and would like to know: How can I estimate the phonemic similarity between two words? In other words, is there an algorithm that can identify the fact that \"hands\" and \"plans\" are closer to rhyming than are \"hands\" and \"fries\"? </p>\n\n<p>Some context: At first, I was willing to say that two words rhyme if their primary stressed syllable and all subsequent syllables are identical (<a href=\"http://www.greenteapress.com/thinkpython/code/c06d\">c06d</a> if you want to replicate in Python):</p>\n\n<pre><code>def create_cmu_sound_dict():\n\n    final_sound_dict = {}\n\n    with open('resources/c06d/c06d') as cmu_dict:\n        cmu_dict = cmu_dict.read().split(\"\\n\")\n        for i in cmu_dict:\n            i_s = i.split()\n            if len(i_s) &gt; 1:\n                word = i_s[0]\n                syllables = i_s[1:]\n\n                final_sound = \"\"\n                final_sound_switch = 0\n\n                for j in syllables:\n                    if \"1\" in j:\n                        final_sound_switch = 1\n                        final_sound += j\n                    elif final_sound_switch == 1:\n                        final_sound += j\n\n            final_sound_dict[word.lower()] = final_sound\n\n    return final_sound_dict\n</code></pre>\n\n<p>If I then run </p>\n\n<pre><code>print cmu_final_sound_dict[\"hands\"]\nprint cmu_final_sound_dict[\"plans\"]\n</code></pre>\n\n<p>I can see that hands and plans sound <em>very</em> similar. I could work towards an estimation of this similarity on my own, but I thought I should ask: Are there sophisticated algorithms that can tie a mathematical value to this degree of sonic (or auditory) similarity? That is, what algorithms or packages can one use to mathematize the degree of phonemic similarity between two words? I realize this is a large question, but I would be most grateful for any advice others can offer on this question.</p>\n",
    "score": 12,
    "creation_date": 1413838934,
    "view_count": 5855,
    "answer_count": 2,
    "tags": "python;algorithm;nlp;linguistics;phoneme"
  },
  {
    "question_id": 21367779,
    "title": "How Can I Add More Languages to Stopwords in NLTK?",
    "body": "<p>I'm using NLTK with stopwords to detect the language of a document using the method described by Alejandro Nolla at <a href=\"http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/\" rel=\"noreferrer\">http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/</a> , and it works reasonably well.</p>\n\n<p>I'm also working with some additional languages not included in the NLTK stopwords package, such as Czech and Romanian, and they get false matches as other languages. These are the languages in stopwords:</p>\n\n<p>['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']</p>\n\n<p>How can I expand the list of languages supported by NLTK? Are there other stopword lists available that I can add? Is there a documented method I can use to create an add my own stopword lists?</p>\n",
    "score": 12,
    "creation_date": 1390762708,
    "view_count": 9446,
    "answer_count": 1,
    "tags": "python;nlp;nltk;stop-words"
  },
  {
    "question_id": 8183891,
    "title": "NLP for extracting actions from text",
    "body": "<p>I'm hoping somebody can point me in the right direction to learn about separating out actions from a bunch of text. </p>\n\n<p>Suppose I have this text</p>\n\n<pre>\nDrop off the dry cleaning, and go to the corner store and pick-up a jug of milk and get a pint of strawberries.\nThen, go pick up the kids from school. First, get John who is in the daycare next to the library, and then get Sam who is two blocks away. \nBy the time you've got the kids, you'll need to stop by the doctors office for the perscription. Tim's flight arrives at 4pm. \nIt's American Airlines flight 331 arriving from Dallas. It will be getting close to rush hour, so make sure you leave yourself enough time.\n</pre>\n\n<p>I'm trying to have it split up into </p>\n\n<pre>\nDrop off the dry cleaning,\n</pre>\n\n<pre>\n and go to the corner store and pick-up a jug of milk and get a pint of strawberries.\n</pre>\n\n<pre>\nThen, go pick up the kids from school. First, get John who is in the daycare next to the library, and then get Sam who is two blocks away. \n</pre>\n\n<pre>\nBy the time you've got the kids, you'll need to stop by the doctors office for the perscription.</pre>\n\n<pre> Tim's flight arrives at 4pm. \nIt's American Airlines flight 331 arriving from Dallas. It will be getting close to rush hour, so make sure you leave yourself enough time.\n</pre>\n\n<p>I haven't been able to find anything in my searches that is specifically action based. It would need to be smarter than just picking out verbs, as there are multiple verbs that are sometimes associated with one action for, instance the second item has 'go','pick-up' and 'get', but that is all part of a single action. Of course, \"Tim's flight\" is only suggests an action with the present participle, with the verb coming toward the end of the segment. </p>\n\n<p>Any suggestions on where to look to do this kind of thing? Things to watch-out for, recommended readings, etc. etc.</p>\n",
    "score": 12,
    "creation_date": 1321626072,
    "view_count": 5913,
    "answer_count": 2,
    "tags": "machine-learning;nlp;information-extraction;sentence;pos-tagger"
  },
  {
    "question_id": 4698229,
    "title": "Compose synthetic English phrase that would contain 160 bits of recoverable information",
    "body": "<p>I have 160 bits of random data.</p>\n\n<p>Just for fun, I want to generate pseudo-English phrase to \"store\" this information in. I want to be able to recover this information from the phrase. </p>\n\n<p><strong><em>Note:</strong> This is not a security question, I don't care if someone else will be able to recover the information or even detect that it is there or not.</em></p>\n\n<p>Criteria for better phrases, from most important to the least: </p>\n\n<ul>\n<li>Short</li>\n<li>Unique</li>\n<li>Natural-looking</li>\n</ul>\n\n<p>The current approach, suggested <a href=\"https://stackoverflow.com/questions/4683551/generating-a-pseudo-natural-phrase-from-a-big-integer-in-a-reversible-way/4684842#4684842\">here</a>:</p>\n\n<p>Take three lists of 1024 nouns, verbs and adjectives each (picking most popular ones). Generate a phrase by the following pattern, reading 20 bits for each word:</p>\n\n<pre>\nNoun verb adjective verb,\nNoun verb adjective verb,\nNoun verb adjective verb,\nNoun verb adjective verb.\n</pre>\n\n<p>Now, this seems to be a good approach, but the phrase is a bit too long and a bit too dull.</p>\n\n<p>I have found a corpus of words <a href=\"http://wordlist.sourceforge.net/\" rel=\"nofollow noreferrer\">here</a> (Part of Speech Database).</p>\n\n<p>After some ad-hoc filtering, I calculated that this corpus contains, approximately</p>\n\n<ul>\n<li>50690 usable adjectives</li>\n<li>123585 nouns</li>\n<li>15301 verbs</li>\n<li>13010 adverbs (not included in pattern, but mentioned in answers)</li>\n</ul>\n\n<p>This allows me to use up to</p>\n\n<ul>\n<li>16 bits per adjective (actually 16.9, but I can't figure how to use fractional bits)</li>\n<li>15 bits per noun</li>\n<li>13 bits per verb</li>\n<li>13 bits per adverb</li>\n</ul>\n\n<p>For noun-verb-adjective-verb pattern this gives 57 bits per \"sentence\" in phrase. This means that, if I'll use all words I can get from this corpus, I can generate three sentences instead of four (160 / 57 ≈ 2.8).</p>\n\n<pre>\nNoun verb adjective verb,\nNoun verb adjective verb,\nNoun verb adjective verb.\n</pre>\n\n<p>Still a bit too long and dull.</p>\n\n<p>Any hints how can I improve it?</p>\n\n<p>What I see that I can try:</p>\n\n<ul>\n<li><p>Try to compress my data somehow before encoding. But since the data is completely random, only some phrases would be shorter (and, I guess, not by much).</p></li>\n<li><p>Improve phrase pattern, so it would look better.</p></li>\n<li><p>Use several patterns, using the first word in phrase to somehow indicate for future decoding which pattern was used. (For example, use the last letter or even the length of the word.) Pick pattern according to the first bytes of the data.</p></li>\n</ul>\n\n<p>...I'm not that good with English to come up with better phrase patterns. Any suggestions?</p>\n\n<ul>\n<li>Use more linguistics in the pattern. Different tenses etc. </li>\n</ul>\n\n<p>...I guess, I would need much better word corpus than I have now for that. Any hints where can I get a suitable one?</p>\n",
    "score": 12,
    "creation_date": 1295070099,
    "view_count": 512,
    "answer_count": 1,
    "tags": "nlp;steganography"
  },
  {
    "question_id": 48783876,
    "title": "What&#39;s the ideal way to include dictionaries (gazetteer) in spaCy to improve NER?",
    "body": "<p>I'm currently working on replacing a system based on nltk entity extraction combined with regexp matching where I have several named entity dictionaries. The dictionary entities are both of common type (PERSON (employees) etc.) as well as custom types (e.g. SKILL). I want to use the pre-trained spaCy model and include my dictionaries somehow, to increase the NER accuracy. Here are my thoughts on possible methods:</p>\n\n<ul>\n<li><p>Use spaCy's Matcher API, iterate through the dictionary and add each phrase with a callback to add the entity?</p></li>\n<li><p>I've just found spacy-lookup, which seems like an easy way to provide long lists of words/phrases to match.</p></li>\n<li><p>But what if I want to have fuzzy matching? Is there a way to add directly to the Vocab and thus have some fuzzy matching through Bloom filter / n-gram word vectors, or is there some extension out there that suits this need? Otherwise I guess I could copy spacy-lookup and replace the flashtext machinery with something else, e.g. Levenshtein distance.</p></li>\n<li><p>While playing around with spaCy I did try just training the NER directly with a single word from the dictionary (without any sentence context), and this did \"work\". But I would, of course, have to take much care to keep the model from forgetting everything. </p></li>\n</ul>\n\n<p>Any help appreciated, I feel like this must be a pretty common requirement and would love to hear what's working best for people out there. </p>\n",
    "score": 12,
    "creation_date": 1518601170,
    "view_count": 2942,
    "answer_count": 2,
    "tags": "python;nlp;named-entity-recognition;spacy"
  },
  {
    "question_id": 32789841,
    "title": "How to encode dependency path as a feature for classification?",
    "body": "<p>I am trying to implement relation extraction between verb pairs. I want to use dependency path from one verb to the other as a feature for my classifier (predicts if relation X exists or not). But I am not sure how to encode the dependency path as a feature. Following are some example dependency paths, as space separated relation annotations from StanfordCoreNLP Collapsed Dependencies:</p>\n\n<pre><code>nsubj acl nmod:from acl nmod:by conj:and\nnsubj nmod:into\nnsubj acl:relcl advmod nmod:of\n</code></pre>\n\n<p>It is important to keep in mind that these path are of <strong>variable length</strong> and a relation could <strong>reappear</strong> without any restriction. </p>\n\n<p>Two compromising ways of encoding this feature that come to my mind are: </p>\n\n<p>1) Ignore the sequence, and just have one feature for each relation with its value being the number of times it appears in the path</p>\n\n<p>2) Have a sliding window of length n, and have one feature for each possible pair of relations with the value being the number of times those two relations appeared consecutively. I suppose this is how one encodes n-grams. However, the number of possible relations is 50, which means I cannot really go with this approach. </p>\n\n<p>Any suggestions are welcomed.</p>\n",
    "score": 12,
    "creation_date": 1443211466,
    "view_count": 1104,
    "answer_count": 1,
    "tags": "machine-learning;nlp;stanford-nlp;feature-extraction;information-extraction"
  },
  {
    "question_id": 31986466,
    "title": "Probability tree for sentences in nltk employing both lookahead and lookback dependencies",
    "body": "<p>Does nltk or any other NLP tool allow to construct probability trees based on input sentences thus storing the language model of the input text in a dictionary tree, the following <a href=\"https://stackoverflow.com/a/31835523/2305993\">example</a> gives the rough idea, but I need the same functionality such that a word Wt does not just probabilistically modelled  on past input words(history) Wt-n but also on lookahead words like Wt+m. Also the lookback and lookahead word count should also be 2 or more i.e. bigrams or more. Are there any other libraries in python which achieve this?</p>\n\n<pre><code>from collections import defaultdict\nimport nltk\nimport math\n\nngram = defaultdict(lambda: defaultdict(int))\ncorpus = \"The cat is cute. He jumps and he is happy.\"\nfor sentence in nltk.sent_tokenize(corpus):\n    tokens = map(str.lower, nltk.word_tokenize(sentence))\n    for token, next_token in zip(tokens, tokens[1:]):\n        ngram[token][next_token] += 1\nfor token in ngram:\n    total = math.log10(sum(ngram[token].values()))\n    ngram[token] = {nxt: math.log10(v) - total for nxt, v in ngram[token].items()}\n</code></pre>\n\n<p>the solution  requires both lookahead and lookback and a specially sub classed dictionary may help in solving this problem. Can also point to relevant resources which talk about implementing such a system. A nltk.models seemed to be doing something similar but is no longer available. Are there any existing design patterns in NLP which implement this idea? skip gram based models are similar to this idea too but I feel this has should have been implemented already somewhere.</p>\n",
    "score": 12,
    "creation_date": 1439463941,
    "view_count": 1437,
    "answer_count": 2,
    "tags": "python;dictionary;nlp;nltk;linguistics"
  },
  {
    "question_id": 6664556,
    "title": "Base word stemming instead of root word stemming in R",
    "body": "<p>Is there any way to get base word instead of root word in stemming using NLP in R? </p>\n\n<p>Code:</p>\n\n<pre><code>&gt; #Loading libraries\n&gt; library(tm)\n&gt; library(slam)\n&gt; \n&gt; #Vector\n&gt; Vec=c(\"happyness happies happys\",\"sky skies\")\n&gt; \n&gt; #Creating Corpus\n&gt; Txt=Corpus(VectorSource(Vec))\n&gt; \n&gt; #Stemming\n&gt; Txt=tm_map(Txt, stemDocument)\n&gt; \n&gt; #Checking result\n&gt; inspect(Txt)\nA corpus with 2 text documents\n\nThe metadata consists of 2 tag-value pairs and a data frame\nAvailable tags are:\n  create_date creator \nAvailable variables in the data frame are:\n  MetaID \n\n[[1]]\nhappi happi happi\n\n[[2]]\nsky sky\n\n&gt; \n</code></pre>\n\n<p>Can I get base word \"happy\" (base word) instead of \"happi\" (root word) for \"happyness happies happys\" using R.</p>\n",
    "score": 12,
    "creation_date": 1310475722,
    "view_count": 6107,
    "answer_count": 4,
    "tags": "r;nlp;stemming"
  },
  {
    "question_id": 49764842,
    "title": "How much space and processing will be optimized in Lucene index by storing a field as Byte instead of String for billions of documents",
    "body": "<p>I understand the concept of inverted-index and how Dictionary storage optimization could help to load entire dictionary in main memory for the faster query.</p>\n\n<p>I am trying to understand how Lucene index work. </p>\n\n<p>Suppose I have a String type field which has only four distinct values for the 200 billion documents indexed in Lucene. This field is a Stored field.</p>\n\n<p>If I change the field to Byte or Int type to represent all 4 distinct values and re-index and store all the 200 billion documents.</p>\n\n<p>What would be storage and query optimization for this data type change? If there would be any.</p>\n\n<p>Please suggest if I can do some test on my laptop to get a sense.</p>\n",
    "score": 12,
    "creation_date": 1523408072,
    "view_count": 307,
    "answer_count": 1,
    "tags": "algorithm;lucene;nlp;information-retrieval"
  },
  {
    "question_id": 10638597,
    "title": "Minimum Edit Distance Reconstruction",
    "body": "<p>I know there are similar answer to this on stack, as well as online, but I feel I'm missing something. Given the code below, we need to reconstruct the sequence of events that led to the resulting minimum edit distance. For the code below, we need to write a function that outputs:</p>\n\n<pre><code>Equal, L, L\nDelete, E\nEqual, A, A\nSubstitute, D, S\nInsert, T\n</code></pre>\n\n<p><strong>EDIT: CODE IS UPDATED WITH MY (PARTIALLY CORRECT) SOLUTION</strong></p>\n\n<p>Here is the code, with my partial solution. It works for example I was given (\"lead\" -> \"last\"), but doesn't work for the example below (\"hint\" -> \"isnt\"). I suspect this is because the first character is equal, which is throwing off my code. Any tips or pointers in the right direction would be great!</p>\n\n<pre><code>def printMatrix(M):\n        for row in M:\n                print row\n        print\n\ndef med(s, t):  \n        k = len(s) + 1\n        l = len(t) + 1\n\n        M = [[0 for i in range(k)] for j in range(l)]\n        MTrace = [[\"\" for i in range(k)] for j in range(l)]\n\n        M[0][0] = 0\n\n\n        for i in xrange(0, k):\n                M[i][0] = i\n                MTrace[i][0] = s[i-1]\n\n        for j in xrange(0, l):\n                M[0][j] = j\n                MTrace[0][j] = t[j-1]\n\n        MTrace[0][0] = \"DONE\"\n\n        for i in xrange(1, k):\n                for j in xrange(1, l):\n\n                        sub = 1\n                        sub_op = \"sub\"\n                        if s[i-1] == t[j-1]:\n                                # equality\n                                sub = 0\n                                sub_op = \"eq\"\n\n\n                        # deletion\n                        min_value = M[i-1][j] + 1\n                        op = \"del\"\n                        if min_value &gt; M[i][j-1] + 1:\n                                # insertion\n                                min_value = M[i][j-1] + 1\n                                op = \"ins\"\n                        if min_value &gt; M[i-1][j-1] + sub:\n                                # substitution\n                                min_value = M[i-1][j-1] + sub\n                                op = sub_op\n\n\n                        M[i][j] = min_value\n                        MTrace[i][j] = op                        \n\n        print \"final Matrix\"\n        printMatrix(M)\n        printMatrix(MTrace)\n\n############ MY PARTIAL SOLUTION\n\n        def array_append(array,x,y):\n            ops_string = MTrace[x][y]\n            if ops_string == 'ins':\n                array.append((\"Insert\",MTrace[0][y]))\n            elif ops_string == 'sub':\n                array.append((\"Substitute\",MTrace[x][0],MTrace[0][y]))\n            elif ops_string == 'eq':\n                array.append((\"Equal\",MTrace[x][0],MTrace[0][y]))\n            elif ops_string == 'del':\n                array.append((\"Delete\",MTrace[x][0]))\n\n\n        i = len(s)\n        j = len(t)\n\n        ops_array = []\n        base = M[i][j]\n        array_append(ops_array,i,j)\n\n\n        while MTrace[i][j] != \"DONE\":\n            base = M[i][j]\n            local_min = min(M[i][j-1],M[i-1][j],M[i-1][j-1])\n            if base == local_min:\n                i = i - 1\n                j = j - 1\n                array_append(ops_array,i,j)\n            elif M[i][j-1] &lt; M[i-1][j]:\n                j = j -1\n                array_append(ops_array,i,j)\n            elif M[i-1][j] &lt; M[i][j-1]:\n                i = i - 1\n                array_append(ops_array,i,j)\n            else:\n                i = i - 1\n                j = j - 1\n                array_append(ops_array,i,j)\n\n        print ops_array\n#########\n\n        return M[k-1][l-1]      \n\nprint med('lead', 'last')\n</code></pre>\n",
    "score": 11,
    "creation_date": 1337268082,
    "view_count": 16472,
    "answer_count": 3,
    "tags": "python;matrix;nlp;dynamic-programming"
  },
  {
    "question_id": 3067377,
    "title": "What programming language is the most English-like?",
    "body": "<p>I'm mainly a Python programmer, and it is often described as being \"executable pseudo-code\".  I have used a little bit of AppleScript, which seems to be the most English-like programming language I have ever seen, because almost operators can be words, and it lets you use \"the\" anywhere (for example, this stupid example I just came up with:</p>\n\n<pre><code>set the firstnumber to 1\nset the secondnumber to 2\nif the firstnumber is equal to the secondnumber then \n    set the sum to 5\nend if\n</code></pre>\n\n<p>is a valid AppleScript program.  Are there any programming languages that are even more English-like than these?</p>\n",
    "score": 11,
    "creation_date": 1276838846,
    "view_count": 17081,
    "answer_count": 11,
    "tags": "applescript;programming-languages;nlp"
  },
  {
    "question_id": 20359346,
    "title": "Executing and testing stanford core nlp example",
    "body": "<p>I downloaded stanford core nlp packages and tried to test it on my machine.</p>\n\n<p>Using command: <code>java -cp \"*\" -mx1g edu.stanford.nlp.sentiment.SentimentPipeline -file input.txt</code></p>\n\n<p>I got sentiment result in form of <code>positive</code> or <code>negative</code>. <code>input.txt</code> contains the sentence to be tested.</p>\n\n<p>On more command: <code>java -cp stanford-corenlp-3.3.0.jar;stanford-corenlp-3.3.0-models.jar;xom.jar;joda-time.jar -Xmx600m edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse -file input.txt</code> when executed gives follwing lines :</p>\n\n<pre><code>H:\\Drive E\\Stanford\\stanfor-corenlp-full-2013~&gt;java -cp stanford-corenlp-3.3.0.j\nar;stanford-corenlp-3.3.0-models.jar;xom.jar;joda-time.jar -Xmx600m edu.stanford\n.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse -file\ninput.txt\nAdding annotator tokenize\nAdding annotator ssplit\nAdding annotator pos\nReading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3wo\nrds/english-left3words-distsim.tagger ... done [36.6 sec].\nAdding annotator lemma\nAdding annotator parse\nLoading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCF\nG.ser.gz ... done [13.7 sec].\n\nReady to process: 1 files, skipped 0, total 1\nProcessing file H:\\Drive E\\Stanford\\stanfor-corenlp-full-2013~\\input.txt ... wri\nting to H:\\Drive E\\Stanford\\stanfor-corenlp-full-2013~\\input.txt.xml {\n  Annotating file H:\\Drive E\\Stanford\\stanfor-corenlp-full-2013~\\input.txt [13.6\n81 seconds]\n} [20.280 seconds]\nProcessed 1 documents\nSkipped 0 documents, error annotating 0 documents\nAnnotation pipeline timing information:\nPTBTokenizerAnnotator: 0.4 sec.\nWordsToSentencesAnnotator: 0.0 sec.\nPOSTaggerAnnotator: 1.8 sec.\nMorphaAnnotator: 2.2 sec.\nParserAnnotator: 9.1 sec.\nTOTAL: 13.6 sec. for 10 tokens at 0.7 tokens/sec.\nPipeline setup: 58.2 sec.\nTotal time for StanfordCoreNLP pipeline: 79.6 sec.\n\nH:\\Drive E\\Stanford\\stanfor-corenlp-full-2013~&gt;\n</code></pre>\n\n<p>Could understand. No informative result.</p>\n\n<p>I got one example at : <a href=\"https://stackoverflow.com/questions/11832490/stanford-core-nlp-java-output\">stanford core nlp java output</a></p>\n\n<pre><code>import java.io.*;\nimport java.util.*;\n\nimport edu.stanford.nlp.io.*;\nimport edu.stanford.nlp.ling.*;\nimport edu.stanford.nlp.pipeline.*;\nimport edu.stanford.nlp.trees.*;\nimport edu.stanford.nlp.util.*;\n\npublic class StanfordCoreNlpDemo {\n\n  public static void main(String[] args) throws IOException {\n    PrintWriter out;\n    if (args.length &gt; 1) {\n      out = new PrintWriter(args[1]);\n    } else {\n      out = new PrintWriter(System.out);\n    }\n    PrintWriter xmlOut = null;\n    if (args.length &gt; 2) {\n      xmlOut = new PrintWriter(args[2]);\n    }\n\n    StanfordCoreNLP pipeline = new StanfordCoreNLP();\n    Annotation annotation;\n    if (args.length &gt; 0) {\n      annotation = new Annotation(IOUtils.slurpFileNoExceptions(args[0]));\n    } else {\n      annotation = new Annotation(\"Kosgi Santosh sent an email to Stanford University. He didn't get a reply.\");\n    }\n\n    pipeline.annotate(annotation);\n    pipeline.prettyPrint(annotation, out);\n    if (xmlOut != null) {\n      pipeline.xmlPrint(annotation, xmlOut);\n    }\n    // An Annotation is a Map and you can get and use the various analyses individually.\n    // For instance, this gets the parse tree of the first sentence in the text.\n    List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);\n    if (sentences != null &amp;&amp; sentences.size() &gt; 0) {\n      CoreMap sentence = sentences.get(0);\n      Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);\n      out.println();\n      out.println(\"The first sentence parsed is:\");\n      tree.pennPrint(out);\n    }\n  }\n\n}\n</code></pre>\n\n<p>Tried to execute it in netbeans with including necessary library. But it always stuck in between or gives exception <code>Exception in thread “main” java.lang.OutOfMemoryError: Java heap space</code></p>\n\n<p>Thou I set the memory to be allocated in <code>property/run/VM box</code></p>\n\n<p>Any idea how can I run above java example using command line?</p>\n\n<p><strong>I want to get sentiment score of the example</strong></p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>output of :  <code>java -cp \"*\" -mx1g edu.stanford.nlp.sentiment.SentimentPipeline -file input.txt</code></p>\n\n<p><img src=\"https://i.sstatic.net/Xy8ms.png\" alt=\"enter image description here\"></p>\n\n<p>out put of: <code>java -cp stanford-corenlp-3.3.0.j\nar;stanford-corenlp-3.3.0-models.jar;xom.jar;joda-time.jar -Xmx600m edu.stanford\n.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse -file\ninput.txt</code></p>\n\n<p><img src=\"https://i.sstatic.net/HRbp4.png\" alt=\"Out of of above command\"></p>\n",
    "score": 11,
    "creation_date": 1386096943,
    "view_count": 40141,
    "answer_count": 4,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 10677020,
    "title": "real word count in NLTK",
    "body": "<p>The NLTK book has a couple of examples of word counts, but in reality they are not word counts but token counts. For instance, Chapter 1, Counting Vocabulary says that the following gives a word count:</p>\n\n<pre><code>text = nltk.Text(tokens)\nlen(text)\n</code></pre>\n\n<p>However, it doesn't - it gives a word and punctuation count.\nHow can you get a real word count (ignoring punctuation)?</p>\n\n<p>Similarly, how can you get the average number of characters in a word? \nThe obvious answer is:</p>\n\n<pre><code>word_average_length =(len(string_of_text)/len(text))\n</code></pre>\n\n<p>However, this would be off because:</p>\n\n<ol>\n<li>len(string_of_text) is a character count, including spaces</li>\n<li>len(text) is a token count, excluding spaces but including punctuation marks, which aren't words.</li>\n</ol>\n\n<p>Am I missing something here? This must be a very common NLP task...</p>\n",
    "score": 11,
    "creation_date": 1337546446,
    "view_count": 53985,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 10072744,
    "title": "Remove repeating characters from words",
    "body": "<p>I was wondering what is the best way to convert something like \"haaaaapppppyyy\" to \"haappyy\". </p>\n\n<p>Basically, when parsing slang, people sometimes repeat characters for added emphasis. </p>\n\n<p>I was wondering what the best way to do this is? Using <code>set()</code> doesn't work because the order of the letters is obviously important.</p>\n\n<p>Any ideas? I'm using Python + nltk. </p>\n",
    "score": 11,
    "creation_date": 1333972200,
    "view_count": 18060,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 12497252,
    "title": "How can i cluster document using k-means (Flann with python)?",
    "body": "<p>I want to cluster documents based on similarity.</p>\n\n<p>I haved tried ssdeep (similarity hashing), very fast but i was told that k-means is faster and flann is fastest of all implementations, and more accurate so i am trying flann with python bindings but i can't find any example how to do it on text (it only support array of numbers).</p>\n\n<p>I am very very new to this field (k-means, natural language processing). What i need is speed and accuracy.</p>\n\n<p>My questions are: </p>\n\n<ol>\n<li>Can we do document similarity grouping / Clustering using KMeans (Flann do not allow any text input it seems )</li>\n<li>Is Flann the right choice? If not please suggest me High performance library that support text/docs clustering, that have python wrapper/API.</li>\n<li>Is k-means the right algorithm?</li>\n</ol>\n",
    "score": 11,
    "creation_date": 1348066273,
    "view_count": 13714,
    "answer_count": 2,
    "tags": "nlp;cluster-analysis;data-mining;k-means;text-mining"
  },
  {
    "question_id": 1140908,
    "title": "Parsing Meaning from Text",
    "body": "<p>I realize this is a broad topic, but I'm looking for a good primer on parsing meaning from text, ideally in Python. As an example of what I'm looking to do, if a user makes a blog post like:</p>\n\n<p>\"Manny Ramirez makes his return for the Dodgers today against the Houston Astros\",</p>\n\n<p>what's a light-weight/ easy way of getting the nouns out of a sentence? To start, I think I'd limit it to proper nouns, but I wouldn't want to be limited to just that (and I don't want to rely on a simple regex that assumes anything Title Capped is a proper noun).</p>\n\n<p>To make this question even worse, what are the things I'm not asking that I should be? Do I need a corpus of existing words to get started? What lexical analysis stuff do I need to know to make this work? I did come across <a href=\"https://stackoverflow.com/questions/220187/algorithms-or-libraries-for-textual-analysis-specifically-dominant-words-phras\">one other question</a> on the topic and I'm digging through those resources now.</p>\n",
    "score": 11,
    "creation_date": 1247789109,
    "view_count": 14356,
    "answer_count": 7,
    "tags": "python;parsing;nlp;lexical-analysis"
  },
  {
    "question_id": 32031353,
    "title": "Replace single quotes with double with exclusion of some elements",
    "body": "<p>I want to replace all single quotes in the string with double with the exception of occurrences such as \"n't\", \"'ll\", \"'m\" etc.</p>\n\n<pre><code>input=\"the stackoverflow don\\'t said, \\'hey what\\'\"\noutput=\"the stackoverflow don\\'t said, \\\"hey what\\\"\"\n</code></pre>\n\n<p>Code 1:(@<a href=\"https://stackoverflow.com/users/918959/antti-haapala\">https://stackoverflow.com/users/918959/antti-haapala</a>)</p>\n\n<pre><code>def convert_regex(text): \n     return re.sub(r\"(?&lt;!\\w)'(?!\\w)|(?&lt;!\\w)'(?=\\w)|(?&lt;=\\w)'(?!\\w)\", '\"', text)\n</code></pre>\n\n<p>There are 3 cases: ' is NOT preceded and is NOT followed by a alphanumeric character; or is not preceded, but followed by an alphanumeric character; or is preceded and not followed by an alphanumeric character.</p>\n\n<p>Issue: That doesn't work on words that end in an apostrophe, i.e. \nmost possessive plurals, and it also doesn't work on informal \nabbreviations that start with an apostrophe.</p>\n\n<p>Code 2:(@<a href=\"https://stackoverflow.com/users/953482/kevin\">https://stackoverflow.com/users/953482/kevin</a>)</p>\n\n<pre><code>def convert_text_func(s):\n    c = \"_\" #placeholder character. Must NOT appear in the string.\n    assert c not in s\n    protected = {word: word.replace(\"'\", c) for word in [\"don't\", \"it'll\", \"I'm\"]}\n    for k,v in protected.iteritems():\n        s = s.replace(k,v)\n    s = s.replace(\"'\", '\"')\n    for k,v in protected.iteritems():\n        s = s.replace(v,k)\n    return s\n</code></pre>\n\n<p>Too large set of words to specify, as how can one specify persons' etc.\nPlease help.</p>\n\n<p><strong>Edit 1:</strong>\n I am using @anubhava's brillant answer. I am facing this issue. Sometimes, there language translations which the approach fail. \nCode=</p>\n\n<pre><code>text=re.sub(r\"(?&lt;!s)'(?!(?:t|ll|e?m|s|d|ve|re|clock)\\b)\", '\"', text)\n</code></pre>\n\n<p>Problem:</p>\n\n<p>In text, 'Kumbh melas' melas is a Hindi to English translation not plural possessive nouns.</p>\n\n<pre><code>Input=\"Similar to the 'Kumbh melas', celebrated by the banks of the holy rivers of India,\"\nOutput=Similar to the \"Kumbh melas', celebrated by the banks of the holy rivers of India,\nExpected Output=Similar to the \"Kumbh melas\", celebrated by the banks of the holy rivers of India,\n</code></pre>\n\n<p>I am looking maybe to add a condition that somehow fixes it. Human-level intervention is the last option.</p>\n\n<p><strong>Edit 2:</strong>\nNaive and long approach to fix:</p>\n\n<pre><code>def replace_translations(text):\n    d = enchant.Dict(\"en_US\")\n    words=tokenize_words(text)\n    punctuations=[x for x in string.punctuation]\n    for i,word in enumerate(words):\n        print i,word\n        if(i!=len(words) and word not in punctuations and d.check(word)==False and words[i+1]==\"'\"):\n            text=text.replace(words[i]+words[i+1],words[i]+\"\\\"\")\n    return text\n</code></pre>\n\n<p>Are there any corner cases I am missing or are there any better approaches?</p>\n",
    "score": 11,
    "creation_date": 1439693228,
    "view_count": 2681,
    "answer_count": 5,
    "tags": "python;regex;replace;nlp"
  },
  {
    "question_id": 2821575,
    "title": "Java text classification problem",
    "body": "<p>I have a set of Books objects, classs <strong>Book</strong> is defined as following :</p>\n\n<pre><code>Class Book{\n\nString title;\nArrayList&lt;tags&gt; taglist;\n\n}\n</code></pre>\n\n<p>Where <strong>title</strong> is the title of the book, example : <em>Javascript for dummies</em>.</p>\n\n<p>and <strong>taglist</strong> is a list of tags for our example : <em>Javascript, jquery, \"web dev\", ..</em></p>\n\n<p>As I said a have a set of books talking about different things : IT, BIOLOGY, HISTORY, ...\nEach book has a title and a set of tags describing it..</p>\n\n<p>I have to classify automaticaly those books into separated sets by topic, example :</p>\n\n<p>IT BOOKS :</p>\n\n<ul>\n<li>Java for dummies</li>\n<li>Javascript for dummies</li>\n<li>Learn flash in 30 days</li>\n<li>C++ programming</li>\n</ul>\n\n<p>HISTORY BOOKS :</p>\n\n<ul>\n<li>World wars</li>\n<li>America in 1960</li>\n<li>Martin luther king's life</li>\n</ul>\n\n<p>BIOLOGY BOOKS :</p>\n\n<ul>\n<li>....</li>\n</ul>\n\n<p>Do you guys know a classification algorithm/method to apply for that kind of problems ?</p>\n\n<p>A solution is to use an external API to define the category of the text, but the problem here is that books are in different languages : french, spanish, english ..</p>\n",
    "score": 11,
    "creation_date": 1273688189,
    "view_count": 11887,
    "answer_count": 4,
    "tags": "java;machine-learning;nlp;text-processing;classification"
  },
  {
    "question_id": 76678783,
    "title": "langchain&#39;s chroma vectordb.similarity_search_with_score() and vectordb.similarity_search_with_relevancy_scores() returning the same output",
    "body": "<p>I have been working with langchain's chroma vectordb. It has two methods for running similarity search with scores.</p>\n<ol>\n<li><code>vectordb.similarity_search_with_score()</code></li>\n<li><code>vectordb.similarity_search_with_relevance_scores()</code></li>\n</ol>\n<p>According to the documentation, the first one should return a cosine distance in <code>float</code>.\n<a href=\"https://i.sstatic.net/mpvnZ.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/mpvnZ.png\" alt=\"enter image description here\" /></a></p>\n<p>Smaller the better.</p>\n<p>And the second one should return a score from 0 to 1, 0 means dissimilar and 1 means similar.\n<a href=\"https://i.sstatic.net/Vomd3.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Vomd3.png\" alt=\"enter image description here\" /></a></p>\n<p>But when I tried the same it is giving me exactly same results with same scores which overflows the upperlimit 1, which should not be the case for the second function.</p>\n<p>What's going on here?</p>\n",
    "score": 11,
    "creation_date": 1689247181,
    "view_count": 46916,
    "answer_count": 5,
    "tags": "nlp;langchain"
  },
  {
    "question_id": 35281691,
    "title": "scikit cosine_similarity vs pairwise_distances",
    "body": "<p>What is the difference between Scikit-learn's sklearn.metrics.pairwise.cosine_similarity and sklearn.metrics.pairwise.pairwise_distances(.. metric=\"cosine\")?</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\ndocuments = (\n    \"Macbook Pro 15' Silver Gray with Nvidia GPU\",\n    \"Macbook GPU\"    \n)\n\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nprint(cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)[0,1])\n</code></pre>\n\n<p>0.37997836</p>\n\n<pre><code>from sklearn.metrics.pairwise import pairwise_distances\nprint(pairwise_distances(tfidf_matrix[0:1], tfidf_matrix, metric='cosine')[0,1])\n</code></pre>\n\n<p>0.62002164</p>\n\n<p>Why are these different?</p>\n",
    "score": 11,
    "creation_date": 1454976394,
    "view_count": 13442,
    "answer_count": 2,
    "tags": "python;nlp;scikit-learn"
  },
  {
    "question_id": 57826164,
    "title": "How to save a dataframe result into a table in databricks?",
    "body": "<p>I am trying to save a list of words that I have converted to a dataframe into a table in databricks so that I can view or refer to it later when my cluster restarts. </p>\n\n<p>I have tried the below code but it keeps giving me an error or does run but I can't see the table in the database</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>myWords_External=[['this', 'is', 'my', 'world'],['this', 'is', 'the', 'problem']]\ndf1 = pd.DataFrame(myWords_External)\ndf1.write.mode(\"overwrite\").saveAsTable(\"temp.eehara_trial_table_9_5_19\")\n</code></pre>\n\n<p>the last line gives me the following error</p>\n\n<pre><code>AttributeError: 'DataFrame' object has no attribute 'write'\n</code></pre>\n",
    "score": 11,
    "creation_date": 1567789870,
    "view_count": 96195,
    "answer_count": 1,
    "tags": "python;nlp;databricks"
  },
  {
    "question_id": 32957895,
    "title": "WordNetLemmatizer not returning the right lemma unless POS is explicit - Python NLTK",
    "body": "<p>I'm lemmatizing the Ted Dataset Transcript. There's something strange I notice:\nNot all words are being lemmatized. To say,</p>\n\n<pre><code>selected -&gt; select\n</code></pre>\n\n<p>Which is right. </p>\n\n<p>However, <code>involved !-&gt; involve</code> and <code>horsing !-&gt; horse</code> unless I explicitly input the 'v' (Verb) attribute.</p>\n\n<p>On the python terminal, I get the right output but not in my <a href=\"http://pastebin.com/midgUJ02\" rel=\"noreferrer\">code</a>:</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer\n&gt;&gt;&gt; from nltk.corpus import wordnet\n&gt;&gt;&gt; lem = WordNetLemmatizer()\n&gt;&gt;&gt; lem.lemmatize('involved','v')\nu'involve'\n&gt;&gt;&gt; lem.lemmatize('horsing','v')\nu'horse'\n</code></pre>\n\n<p>The relevant section of the code is this:</p>\n\n<pre><code>for l in LDA_Row[0].split('+'):\n    w=str(l.split('*')[1])\n    word=lmtzr.lemmatize(w)\n    wordv=lmtzr.lemmatize(w,'v')\n    print wordv, word\n    # if word is not wordv:\n    #   print word, wordv\n</code></pre>\n\n<p>The whole code is <a href=\"http://pastebin.com/midgUJ02\" rel=\"noreferrer\">here</a>. </p>\n\n<p>What is the problem?</p>\n",
    "score": 11,
    "creation_date": 1444079204,
    "view_count": 17741,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet;lemmatization"
  },
  {
    "question_id": 51648046,
    "title": "Difference or Relation between RASA and Spacy",
    "body": "<p>I'm really new to Chatbots and starting to learn these stuff using frameworks. I'm starting to use this opensource framework RASA and learning about it. Then I found that this entity extraction tool Spacy, is used by RASA. </p>\n\n<p>Can anybody explain what's the actual relation between these ? What's the role of Spacy within RASA ?</p>\n",
    "score": 11,
    "creation_date": 1533195427,
    "view_count": 5453,
    "answer_count": 1,
    "tags": "python;nlp;spacy;rasa-nlu;rasa-core"
  },
  {
    "question_id": 34361725,
    "title": "nltk StanfordNERTagger : NoClassDefFoundError: org/slf4j/LoggerFactory (In Windows)",
    "body": "<p>NOTE: I am using Python 2.7 as part of Anaconda distribution. I hope this is not a problem for nltk 3.1.</p>\n\n<p>I am trying to use nltk for NER as</p>\n\n<pre><code>import nltk\nfrom nltk.tag.stanford import StanfordNERTagger \n#st = StanfordNERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\nst = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \nprint st.tag(str)\n</code></pre>\n\n<p>but i get </p>\n\n<pre><code>Exception in thread \"main\" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory\n    at edu.stanford.nlp.io.IOUtils.&lt;clinit&gt;(IOUtils.java:41)\n    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1117)\n    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1076)\n    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1057)\n    at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3088)\nCaused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 5 more\n\nTraceback (most recent call last):\n  File \"X:\\jnk.py\", line 47, in &lt;module&gt;\n    print st.tag(str)\n  File \"X:\\Anaconda2\\lib\\site-packages\\nltk\\tag\\stanford.py\", line 66, in tag\n    return sum(self.tag_sents([tokens]), []) \n  File \"X:\\Anaconda2\\lib\\site-packages\\nltk\\tag\\stanford.py\", line 89, in tag_sents\n    stdout=PIPE, stderr=PIPE)\n  File \"X:\\Anaconda2\\lib\\site-packages\\nltk\\internals.py\", line 134, in java\n    raise OSError('Java command failed : ' + str(cmd))\nOSError: Java command failed : ['X:\\\\PROGRA~1\\\\Java\\\\JDK18~1.0_6\\\\bin\\\\java.exe', '-mx1000m', '-cp', 'X:\\\\stanford\\\\stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 'X:\\\\stanford\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz', '-textFile', 'x:\\\\appdata\\\\local\\\\temp\\\\tmpqjsoma', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '\"tokenizeNLs=false\"', '-encoding', 'utf8']\n</code></pre>\n\n<p>but i can see that the slf4j jar is there in my lib folder. do i need to update an environment variable?</p>\n\n<p><strong>Edit</strong></p>\n\n<p>Thanks everyone for their help, but i still get the same error. Here is what i tried recently</p>\n\n<pre><code>import nltk\nfrom nltk.tag import StanfordNERTagger \nprint(nltk.__version__)\nstanford_ner_dir = 'X:\\\\stanford\\\\'\neng_model_filename= stanford_ner_dir + 'classifiers\\\\english.all.3class.distsim.crf.ser.gz'\nmy_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'\nst = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) \nprint st._stanford_model\nprint st._stanford_jar\n\nst.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n</code></pre>\n\n<p>and also</p>\n\n<pre><code>import nltk\nfrom nltk.tag import StanfordNERTagger \nprint(nltk.__version__)\nst = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \nprint st._stanford_model\nprint st._stanford_jar\nst.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n</code></pre>\n\n<p>i get</p>\n\n<pre><code>3.1\nX:\\stanford\\classifiers\\english.all.3class.distsim.crf.ser.gz\nX:\\stanford\\stanford-ner.jar\n</code></pre>\n\n<p>after that it goes on to print the same stacktrace as before. <code>java.lang.ClassNotFoundException: org.slf4j.LoggerFactory</code></p>\n\n<p>any idea why this might be happening? I updated my CLASSPATH as well. I even added all the relevant folders to my PATH environment variable.for example the folder where i unzipped the stanford jars, the place where i unzipped slf4j and even the lib folder inside the stanford folder. i have no idea why this is happening :(</p>\n\n<p><strong>Could it be windows? i have had problems with windows paths before</strong></p>\n\n<p><strong>Update</strong></p>\n\n<ol>\n<li><p>The Stanford NER version i have is 3.6.0. The zip file says <code>stanford-ner-2015-12-09.zip</code></p></li>\n<li><p>I also tried using the <code>stanford-ner-3.6.0.jar</code> instead of <code>stanford-ner.jar</code> but still get the same error</p></li>\n<li><p>When i right click on the <code>stanford-ner-3.6.0.jar</code>, i notice   </p></li>\n</ol>\n\n<p><a href=\"https://i.sstatic.net/Z8Jlo.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Z8Jlo.png\" alt=\"jar properties\"></a></p>\n\n<p><strong>i see this for all the files that i have extracted, even the slf4j files.could this be causing the problem?</strong></p>\n\n<ol start=\"4\">\n<li>Finally, why does the error message say</li>\n</ol>\n\n<p><code>java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory</code></p>\n\n<p>i do not see any folder named <code>org</code> anywhere</p>\n\n<p><strong>Update: Env variables</strong></p>\n\n<p>Here are my env variables</p>\n\n<pre><code>CLASSPATH\n.;\nX:\\jre1.8.0_60\\lib\\rt.jar;\nX:\\stanford\\stanford-ner-3.6.0.jar;\nX:\\stanford\\stanford-ner.jar;\nX:\\stanford\\lib\\slf4j-simple.jar;\nX:\\stanford\\lib\\slf4j-api.jar;\nX:\\slf4j\\slf4j-1.7.13\\slf4j-1.7.13\\slf4j-log4j12-1.7.13.jar\n\nSTANFORD_MODELS\nX:\\stanford\\classifiers\n\nJAVA_HOME\nX:\\PROGRA~1\\Java\\JDK18~1.0_6\n\nPATH\nX:\\PROGRA~1\\Java\\JDK18~1.0_6\\bin;\nX:\\stanford;\nX:\\stanford\\lib;\nX:\\slf4j\\slf4j-1.7.13\\slf4j-1.7.13\n</code></pre>\n\n<p>anything wrong here?</p>\n",
    "score": 11,
    "creation_date": 1450462822,
    "view_count": 5790,
    "answer_count": 9,
    "tags": "python;windows;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 61919670,
    "title": "How nltk.TweetTokenizer different from nltk.word_tokenize?",
    "body": "<p>I am unable to understand the difference between the two. Though, I come to know that word_tokenize uses Penn-Treebank for tokenization purposes. But nothing on TweetTokenizer is available. For which sort of data should I be using TweetTokenizer over word_tokenize?</p>\n",
    "score": 11,
    "creation_date": 1589997195,
    "view_count": 12699,
    "answer_count": 2,
    "tags": "python;nlp;artificial-intelligence;nltk;tokenize"
  },
  {
    "question_id": 33266956,
    "title": "NLTK package to estimate the (unigram) perplexity",
    "body": "<p>I am trying to calculate the perplexity for the data I have. The code I am using is: </p>\n\n<pre><code> import sys\n sys.path.append(\"/usr/local/anaconda/lib/python2.7/site-packages/nltk\")\n\nfrom nltk.corpus import brown\nfrom nltk.model import NgramModel\nfrom nltk.probability import LidstoneProbDist, WittenBellProbDist\nestimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)\nlm = NgramModel(3, brown.words(categories='news'), True, False, estimator)\nprint lm\n</code></pre>\n\n<p>But I am receiving the error, </p>\n\n<pre><code>File \"/usr/local/anaconda/lib/python2.7/site-packages/nltk/model/ngram.py\", line 107, in __init__\ncfd[context][token] += 1\nTypeError: 'int' object has no attribute '__getitem__'\n</code></pre>\n\n<p>I have already performed Latent Dirichlet Allocation for the data I have and I have generated the unigrams and their respective probabilities (they are normalized as the sum of total probabilities of the data is 1).</p>\n\n<p>My unigrams and their probability looks like:</p>\n\n<pre><code>Negroponte 1.22948976891e-05\nAndreas 7.11290670484e-07\nRheinberg 7.08255885794e-07\nJoji 4.48481435106e-07\nHelguson 1.89936727391e-07\nCAPTION_spot 2.37395965468e-06\nMortimer 1.48540253778e-07\nyellow 1.26582575863e-05\nSugar 1.49563800878e-06\nfour 0.000207196011781\n</code></pre>\n\n<p>This is just a fragment of the unigrams file I have. The same format is followed for about 1000s of lines. The total probabilities (second column) summed gives 1. </p>\n\n<p>I am a budding programmer. This ngram.py belongs to the nltk package and I am confused as to how to rectify this. The sample code I have here is from the nltk documentation and I don't know what to do now. Please help on what I can do. Thanks in advance!</p>\n",
    "score": 11,
    "creation_date": 1445453311,
    "view_count": 16446,
    "answer_count": 2,
    "tags": "python-2.7;nlp;nltk;n-gram;language-model"
  },
  {
    "question_id": 17797922,
    "title": "How to calculate bits per character of a string? (bpc)",
    "body": "<p>A paper I was reading, <a href=\"http://www.cs.toronto.edu/~ilya/pubs/2011/LANG-RNN.pdf\" rel=\"noreferrer\">http://www.cs.toronto.edu/~ilya/pubs/2011/LANG-RNN.pdf</a>, uses bits per character as a test metric for estimating the quality of generative computer models of text but doesn't reference how it was calculated. Googling around, I can't really find anything about it.</p>\n\n<p>Does anyone know how to calculate it? Python preferably, but pseudo-code or anything works. Thanks!</p>\n",
    "score": 11,
    "creation_date": 1374529205,
    "view_count": 5636,
    "answer_count": 2,
    "tags": "python;algorithm;machine-learning;nlp;entropy"
  },
  {
    "question_id": 56555066,
    "title": "Can&#39;t import bert.tokenization",
    "body": "<p>I am using Google Colab and the following import doesn't work somehow:</p>\n\n<pre><code>from bert.tokenization import FullTokenizer\n</code></pre>\n\n<p>I am getting this error:</p>\n\n<pre><code>ModuleNotFoundError: No module named 'bert.tokenization'\n</code></pre>\n\n<p>I tried to install bert by running the following command:</p>\n\n<pre><code>!pip install  --upgrade bert\n</code></pre>\n\n<p>Any idea how to resolve this error?</p>\n",
    "score": 11,
    "creation_date": 1560316495,
    "view_count": 22848,
    "answer_count": 8,
    "tags": "python-3.x;deep-learning;nlp"
  },
  {
    "question_id": 50742516,
    "title": "How to get the index of a token in a sentence in spaCy?",
    "body": "<p>Is there an elegant way to get the index of a word/token in its sentence?\nI am aware of the attributes for tokens <a href=\"https://spacy.io/api/token#attributes\" rel=\"noreferrer\">https://spacy.io/api/token#attributes</a>\nThe <code>i</code> attribute returns the index within the whole parent document. But the parent document can contain multiple sentences.</p>\n<p>Example:</p>\n<blockquote>\n<p>&quot;This is an example. This is another example.&quot;</p>\n</blockquote>\n<p>What I need is both <code>&quot;This&quot;</code> to be returned as index <code>0</code>, both <code>&quot;is&quot;</code> to be returned as index <code>1</code> etc...</p>\n",
    "score": 11,
    "creation_date": 1528378052,
    "view_count": 15434,
    "answer_count": 1,
    "tags": "nlp;spacy;dependency-parsing"
  },
  {
    "question_id": 4565334,
    "title": "Is there a programming language with semantics close to English?",
    "body": "<p>Most languages allow to 'tweek' to certain extend parts of the syntax (<a href=\"https://stackoverflow.com/questions/829125/c-loop-macros\">C++</a>,<a href=\"http://msdn.microsoft.com/en-us/library/aa664765(v=vs.71).aspx\" rel=\"nofollow noreferrer\">C#</a>) and/or semantics that you will be using in your code (<a href=\"http://news.ycombinator.com/item?id=1945282\" rel=\"nofollow noreferrer\">Katahdin</a>, <a href=\"http://en.wikipedia.org/wiki/Lua_(programming_language)\" rel=\"nofollow noreferrer\">lua</a>). But I have not heard of a language that can just completely define how your code will look like. So  isn't there some language which already exists that has such capabilities to override <strong>all</strong> syntax &amp; define semantics ? </p>\n\n<p>Example of what I want to do is basically from the C# code below:</p>\n\n<pre><code>foreach(Fruit fruit in Fruits)\n{\n  if(fruit is Apple)\n  {\n    fruit.Price =  fruit.Price/2;\n  }\n}\n</code></pre>\n\n<p>I want do be able to to write the above code in my perfect language like this:</p>\n\n<pre><code>Check if any fruits are Macintosh apples and discount the price by 50%.\n</code></pre>\n\n<p>The advantages that come to my mind looking from a coder's perspective in this \"imaginary\" language are:</p>\n\n<ol>\n<li>It's very clear what is going on (self descriptive) - it's plain English after all even kid would understand my program</li>\n<li>Hides all complexities which I have to write in C#. But why should I care to learn that\nif statements, arithmetic operators etc since there are already implemented</li>\n</ol>\n\n<p>The disadvantages that I see for a coder who will maintain this program are:</p>\n\n<ol>\n<li>Maybe you would express  this program differently from me so you may not get all the\ninformation that I've expressed in my sentence</li>\n<li>Programs can be quite verbose and hard to debug but  if possible to even proximate this type of syntax above maybe more people would start programming right? That would be amazing I think. I can go to work and just write an essay to draw a square on a winform like this: </li>\n</ol>\n\n<p><code>Create a form called MyGreetingForm. Draw a square with in the middle of \nMyGreetingFormwith a side of 100 points. In the middle of the square write \"Hello! Click here to continue\" in Arial font.</code></p>\n\n<p>In the above code the parser must basically guess that I want to use\n    the unnamed square from the previous sentence, it'd be hard to write such a smart parser I guess, yet it's so simple what I want to do.</p>\n\n<p><code>If the user clicks on square in the middle of MyGreetingForm show MyMainForm.</code>  </p>\n\n<p>In the above code 'basically' the compiler must: 1)generate an event handler 2) check if there is any square in the middle of the form and if there is - 3) hide the form and show another form  </p>\n\n<p>It looks very hard to do but it doesn't look impossible IMO to me at least approximate this (I can personally generate a parser to perform the 3 steps above np &amp; it's basically the same that it has to do any way when you add even in c# <code>a.MyEvent=+handler;</code> so I don't see a problem here) so I'm thinking maybe somebody already did something like this ? Or is there some practical burden of complexity to create such a 'essay style' programming language which I can't see ? I mean what's the worse that can happen if the parser is not that good? - your program will crash so you have to re-word it:)</p>\n",
    "score": 11,
    "creation_date": 1293734977,
    "view_count": 4608,
    "answer_count": 6,
    "tags": "parsing;compiler-construction;programming-languages;nlp"
  },
  {
    "question_id": 43554124,
    "title": "Is possible to keep spacy in memory to reduce the load time?",
    "body": "<p>I want to use spacy as for NLP for an online service.\nEach time a user makes a request I call the script \"my_script.py\"</p>\n\n<p>which starts with:</p>\n\n<pre><code>from spacy.en import English\nnlp = English()\n</code></pre>\n\n<p>The problem I'm having is that those two lines take over 10 seconds, is it possible to keep English() in the ram or some other option to reduce this load time to less than a second?</p>\n",
    "score": 11,
    "creation_date": 1492823303,
    "view_count": 5663,
    "answer_count": 5,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 34090734,
    "title": "How to use nltk regex pattern to extract a specific phrase chunk?",
    "body": "<p>I have written the following regex to tag certain phrases pattern</p>\n\n<pre><code>pattern = \"\"\"\n        P2: {&lt;JJ&gt;+ &lt;RB&gt;? &lt;JJ&gt;* &lt;NN&gt;+ &lt;VB&gt;* &lt;JJ&gt;*}\n        P1: {&lt;JJ&gt;? &lt;NN&gt;+ &lt;CC&gt;? &lt;NN&gt;* &lt;VB&gt;? &lt;RB&gt;* &lt;JJ&gt;+}\n        P3: {&lt;NP1&gt;&lt;IN&gt;&lt;NP2&gt;}\n        P4: {&lt;NP2&gt;&lt;IN&gt;&lt;NP1&gt;}\n\n    \"\"\"\n</code></pre>\n\n<p>This pattern would correctly tag a phrase such as:</p>\n\n<pre><code>a = 'The pizza was good but pasta was bad'\n</code></pre>\n\n<p>and give the desired output with 2 phrases: </p>\n\n<ol>\n<li>pizza was good</li>\n<li>pasta was bad</li>\n</ol>\n\n<p>However, if my sentence is something like:</p>\n\n<pre><code>a = 'The pizza was awesome and brilliant'\n</code></pre>\n\n<p>matches only the phrase:</p>\n\n<pre><code>'pizza was awesome' \n</code></pre>\n\n<p>instead of the desired:</p>\n\n<pre><code>'pizza was awesome and brilliant'\n</code></pre>\n\n<p><strong>How do I incorporate the regex pattern for my second example as well?</strong></p>\n",
    "score": 11,
    "creation_date": 1449239833,
    "view_count": 11733,
    "answer_count": 1,
    "tags": "python;regex;nlp;nltk;text-chunking"
  },
  {
    "question_id": 7706696,
    "title": "How to determine the correct capitalization for a words in a sentence?",
    "body": "<p>I have a database containing sentences which only contain capitalized letters. The database is technical, containing medical terms, and I want to normalize it so that the capitalization is (close to) what the user expects. What is the best way to achieve this? Is there a freely available data-set I can use to help with the process?</p>\n",
    "score": 11,
    "creation_date": 1318195974,
    "view_count": 5440,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 63244030,
    "title": "Can&#39;t find model &#39;en_core_web_md&#39;. It doesn&#39;t seem to be a shortcut link, a Python package or a valid path to a data directory",
    "body": "<p>I have installed spacy and downloaded en_core_web_sm with:\npip install spacy\npython -m spacy download en_core_web_sm\nAlso tried\npip3 install <a href=\"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\" rel=\"noreferrer\">https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz</a></p>\n<p>My spaCy version: 2.2.0\nMy Python version: 3.7.4</p>\n<p>However, it still shows the error:\n<a href=\"https://i.sstatic.net/yuObm.png\" rel=\"noreferrer\">OSError: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.</a></p>\n<pre><code>*import aqgFunction\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n# Main Function\ndef main():\n    # Create AQG object\n    aqg = aqgFunction.AutomaticQuestionGenerator()\n    inputTextPath = &quot;E:\\Automatic-Question-Generator-master\\Automatic-Question-Generator-master\\AutomaticQuestionGenerator\\DB\\db.txt&quot;\n    readFile = open(inputTextPath, 'r+', encoding=&quot;utf8&quot;)\n    #readFile = open(inputTextPath, 'r+', encoding=&quot;utf8&quot;, errors = 'ignore')\n    inputText = readFile.read()\n    #inputText = '''I am Dipta. I love codding. I build my carrier with this.'''\n    questionList = aqg.aqgParse(inputText)\n    aqg.display(questionList)\n    #aqg.DisNormal(questionList)\n    return 0\n# Call Main Function\nif __name__ == &quot;__main__&quot;:\n    main()*\n</code></pre>\n",
    "score": 11,
    "creation_date": 1596533767,
    "view_count": 31942,
    "answer_count": 3,
    "tags": "python;python-3.x;text;nlp;spacy"
  },
  {
    "question_id": 58123393,
    "title": "How to use Transformers for text classification?",
    "body": "<p>I have two questions about how to use Tensorflow implementation of the Transformers for text classifications. </p>\n\n<ul>\n<li><strong>First</strong>, it seems people mostly used only the encoder layer to do the text classification task. However, encoder layer generates one prediction for each input word. Based on my understanding of transformers, the input to the encoder each time is one word from the input sentence. Then, the attention weights and the output is calculated using the current input word. And we can repeat this process for all of the words in the input sentence. As a result we'll end up with pairs of (attention weights, outputs) for each word in the input sentence. Is that correct? Then how would you use this pairs to perform a text classification?  </li>\n<li><strong>Second</strong>, based on the Tensorflow implementation of transformer <a href=\"https://www.tensorflow.org/beta/tutorials/text/transformer\" rel=\"noreferrer\">here</a>, they embed the whole input sentence to one vector and feed a batch of these vectors to the Transformer. However, I expected the input to be a batch of words instead of sentences based on what I've learned from <a href=\"http://jalammar.github.io/illustrated-transformer/\" rel=\"noreferrer\">The Illustrated Transformer</a></li>\n</ul>\n\n<p>Thank you!</p>\n",
    "score": 11,
    "creation_date": 1569525495,
    "view_count": 14785,
    "answer_count": 2,
    "tags": "tensorflow;nlp;transformer-model;bert-language-model"
  },
  {
    "question_id": 53525994,
    "title": "How to find &quot;num_words&quot; or vocabulary size of Keras tokenizer when one is not assigned?",
    "body": "<p>So if I were to not pass <code>num_words</code> argument when initializing <code>Tokenizer()</code>, how do I find the vocabulary size after it is used to tokenize the training dataset?</p>\n\n<p>Why this way, I don't want to limit the tokenizer vocabulary size to know how well my Keras model perform without it. But then I need to pass on this vocabulary size as the argument in the model's first layer definition.</p>\n",
    "score": 11,
    "creation_date": 1543430220,
    "view_count": 13924,
    "answer_count": 1,
    "tags": "machine-learning;keras;deep-learning;nlp;tokenize"
  },
  {
    "question_id": 44213549,
    "title": "Python NLP Intent Identification",
    "body": "<p>I am novice in Python and NLP, and my problem is how to finding out Intent of given questions, for example I have sets of questions and answers like this :</p>\n\n<p><code>question:What is NLP; answer: NLP stands for Natural Language Processing</code></p>\n\n<p>I did some basic <code>POS tagger</code> on given questions in above question I get <code>entety [NLP]</code> I also did <code>String Matching</code> <a href=\"http://anhaidgroup.github.io/py_stringmatching/v0.2.x/SimilarityMeasure.html\" rel=\"noreferrer\">using this algo</a>. </p>\n\n<p>Basically I faced following issues :</p>\n\n<ol>\n<li>If user ask <code>what is NLP</code> then it will return exact answers</li>\n<li>If user ask <code>meaning of NLP</code> then it fail</li>\n<li>If user ask <code>Definition of NLP</code> then it fail</li>\n<li>If user ask <code>What is Natural Language Processing</code> then it fail</li>\n</ol>\n\n<p>So how I should identify user intent of given questions because in my case String matching or pattern matching not works.  </p>\n",
    "score": 11,
    "creation_date": 1495865224,
    "view_count": 28661,
    "answer_count": 4,
    "tags": "python;machine-learning;nlp"
  },
  {
    "question_id": 5827439,
    "title": "Any tools to programmatically convert Japanese sentence into its romaji (phonetical reading)?",
    "body": "<p>Input:</p>\n\n<blockquote>\n  <p>日本が好きです.</p>\n</blockquote>\n\n<p>Output:</p>\n\n<blockquote>\n  <p>Nippon ga sukidesu.</p>\n</blockquote>\n\n<p>Phonetical reading is unfortunately not available through Google Translate API.</p>\n",
    "score": 11,
    "creation_date": 1304046704,
    "view_count": 7223,
    "answer_count": 2,
    "tags": "unicode;nlp;translation;cjk"
  },
  {
    "question_id": 30266502,
    "title": "How to un-stem a word in Python?",
    "body": "<p>I want to know if there is anyway that I can un-stem them to a normal form?</p>\n\n<p>The problem is that I have thousands of words in different forms e.g. eat, eaten, ate, eating and so on and I need to count the frequency of each word. All of these - eat, eaten, ate, eating etc will count towards eat and hence, I used stemming.</p>\n\n<p>But the next part of the problem requires me to find similar words in data and I am using nltk's synsets to calculate Wu-Palmer Similarity among the words. The problem is that nltk's synsets wont work on stemmed words, or at least in this code they won't. <a href=\"https://stackoverflow.com/questions/18871706/check-if-two-words-are-related-to-each-other\">check if two words are related to each other</a></p>\n\n<p>How should I do it? Is there a way to un-stem a word?</p>\n",
    "score": 11,
    "creation_date": 1431714884,
    "view_count": 9234,
    "answer_count": 5,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 3856630,
    "title": "How to separate words in a &quot;sentence&quot; with spaces?",
    "body": "<h2>Background</h2>\n\n<p>Looking to automate creating Domains in JasperServer. Domains are a \"view\" of data for creating ad hoc reports. The names of the columns must be presented to the user in a human readable fashion.</p>\n\n<h2>Problem</h2>\n\n<p>There are over 2,000 possible pieces of data from which the organization could theoretically want to include on a report. The data are sourced from non-human-friendly names such as:</p>\n\n<blockquote>\n  <p>payperiodmatchcode\n  labordistributioncodedesc\n  dependentrelationship actionendoption\n  actionendoptiondesc addresstype\n  addresstypedesc historytype\n  psaddresstype rolename\n  bankaccountstatus\n  bankaccountstatusdesc bankaccounttype\n  bankaccounttypedesc beneficiaryamount\n  beneficiaryclass beneficiarypercent\n  benefitsubclass beneficiaryclass\n  beneficiaryclassdesc benefitactioncode\n  benefitactioncodedesc\n  benefitagecontrol\n  benefitagecontroldesc\n  ageconrolagelimit\n  ageconrolnoticeperiod</p>\n</blockquote>\n\n<h2>Question</h2>\n\n<p>How would you automatically change such names to:</p>\n\n<ul>\n<li>pay period match code</li>\n<li>labor distribution code desc</li>\n<li>dependent relationship</li>\n</ul>\n\n<h2>Ideas</h2>\n\n<ul>\n<li><p>Use Google's <a href=\"http://www.google.co.uk/search?q=caseactioncode&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla%3aen-US%3aofficial&amp;client=firefox-a#sclient=psy&amp;hl=en&amp;client=firefox-a&amp;rls=org.mozilla%3aen-US%3Aofficial&amp;q=labordistributioncodedesc&amp;aq=f&amp;aqi=&amp;aql=&amp;oq=labordistributioncodedesc&amp;gs_rfai=&amp;pbx=1&amp;fp=1&amp;bav=on.2,or.r_gc.r_pw.&amp;cad=b\" rel=\"nofollow\">Did you mean</a> engine, however I think it violates their TOS:</p>\n\n<p><code>lynx -dump «url» | grep \"Did you mean\" | awk ...</code></p></li>\n</ul>\n\n<h2>Languages</h2>\n\n<p>Any language is fine, but text parsers such as Perl would probably be well-suited. (The column names are English-only.)</p>\n\n<h2>Unnecessary Prefection</h2>\n\n<p>The goal is not 100% perfection in breaking words apart; the following outcome is acceptable:</p>\n\n<ul>\n<li>enrollmenteffectivedate -> Enrollment Effective Date</li>\n<li>enrollmentenddate -> Enroll Men Tend Date</li>\n<li>enrollmentrequirementset -> Enrollment Requirement Set</li>\n</ul>\n\n<p>No matter what, a human will need to double-check the results and correct many. Whittling a set of 2,000 results down to 600 edits would be a dramatic time savings. To fixate on <em>some</em> cases having multiple possibilities (e.g., therapistname) is to miss the point altogether.</p>\n",
    "score": 11,
    "creation_date": 1286205882,
    "view_count": 2085,
    "answer_count": 6,
    "tags": "bash;perl;awk;nlp;text-segmentation"
  },
  {
    "question_id": 2310536,
    "title": "Language recognition in Java",
    "body": "<p>Is there any language recognition open-source for Java? Found only for c/c++.</p>\n\n<p><strong>UPD:</strong></p>\n\n<p>I`m talking about human text language. Example:</p>\n\n<p>Input: My name is John.\nOutput: English.</p>\n\n<p>Input: Ich heisse John.\nOutput: German.</p>\n\n<p>Input: Меня зовут Джон.\nOutput: Russian.</p>\n",
    "score": 11,
    "creation_date": 1266838128,
    "view_count": 9981,
    "answer_count": 4,
    "tags": "java;open-source;nlp"
  },
  {
    "question_id": 75901231,
    "title": "How can I make sentence-BERT throw an exception if the text exceeds max_seq_length, and what is the max possible max_seq_length for all-MiniLM-L6-v2?",
    "body": "<p>I'm using sentence-BERT from Huggingface in the following way:</p>\n<pre><code>from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nmodel.max_seq_length = 512\nmodel.encode(text)\n</code></pre>\n<p>When <code>text</code> is long and contains more than 512 tokens, it does not throw an exception. I assume it automatically truncates the input to 512 tokens.</p>\n<p>How can I make it throw an exception when the input length is larger than <code>max_seq_length</code>?</p>\n<p>Further, what is the maximum possible <code>max_seq_length</code> for <code>all-MiniLM-L6-v2</code>?</p>\n",
    "score": 11,
    "creation_date": 1680283794,
    "view_count": 15525,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;bert-language-model;huggingface-tokenizers;sentence-transformers"
  },
  {
    "question_id": 67412925,
    "title": "what is the difference between len(tokenizer) and tokenizer.vocab_size",
    "body": "<p>I'm trying to add a few new words to the vocabulary of a pretrained HuggingFace Transformers model. I did the following to change the vocabulary of the tokenizer and also increase the embedding size of the model:</p>\n<pre><code>tokenizer.add_tokens(['word1', 'word2', 'word3', 'word4'])\nmodel.resize_token_embeddings(len(tokenizer))\nprint(len(tokenizer)) # outputs len_vocabulary + 4\n</code></pre>\n<p>But after training the model on my corpus and saving it, I found out that the saved tokenizer vocabulary size hasn't changed. After checking again I found out that the abovementioned code does not change the vocabulary size (tokenizer.vocab_size is still the same) and only the len(tokenizer) has changed.</p>\n<p>So now my question is; what is the difference between tokenizer.vocab_size and len(tokenizer)?</p>\n",
    "score": 11,
    "creation_date": 1620282655,
    "view_count": 12254,
    "answer_count": 1,
    "tags": "nlp;tokenize;huggingface-transformers;huggingface-tokenizers"
  },
  {
    "question_id": 62204109,
    "title": "`return_sequences = False` equivalent in pytorch LSTM",
    "body": "<p>In tensorflow/keras, we can simply set <code>return_sequences = False</code> for the last LSTM layer before the classification/fully connected/activation (softmax/sigmoid) layer to get rid of the temporal dimension.</p>\n\n<p>In PyTorch, I don't find anything similar. For the classification task, I don't need a sequence to sequence model but many to one architecture like this:</p>\n\n<p><a href=\"https://i.sstatic.net/pWTLS.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/pWTLS.png\" alt=\"enter image description here\"></a></p>\n\n<p>Here's my simple bi-LSTM model.</p>\n\n<pre><code>import torch\nfrom torch import nn\n\nclass BiLSTMClassifier(nn.Module):\n    def __init__(self):\n        super(BiLSTMClassifier, self).__init__()\n        self.embedding = torch.nn.Embedding(num_embeddings = 65000, embedding_dim = 64)\n        self.bilstm = torch.nn.LSTM(input_size = 64, hidden_size = 8, num_layers = 2,\n                                    batch_first = True, dropout = 0.2, bidirectional = True)\n        # as we have 5 classes\n        self.linear = nn.Linear(8*2*512, 5) # last dimension\n    def forward(self, x):\n        x = self.embedding(x)\n        print(x.shape)\n        x, _ = self.bilstm(x)\n        print(x.shape)\n        x = self.linear(x.reshape(x.shape[0], -1))\n        print(x.shape)\n\n# create our model\n\nbilstmclassifier = BiLSTMClassifier()\n</code></pre>\n\n<p>If I observe the shapes after each layer,</p>\n\n<pre><code>xx = torch.tensor(X_encoded[0]).reshape(1,512)\nprint(xx.shape) \n# torch.Size([1, 512])\nbilstmclassifier(xx)\n#torch.Size([1, 512, 64])\n#torch.Size([1, 512, 16])\n#torch.Size([1, 5])\n</code></pre>\n\n<p>What can I do so that the last LSTM returns a tensor with shape <code>(1, 16)</code> instead of <code>(1, 512, 16)</code>?</p>\n",
    "score": 11,
    "creation_date": 1591304669,
    "view_count": 6724,
    "answer_count": 1,
    "tags": "python-3.x;tensorflow;nlp;pytorch;lstm"
  },
  {
    "question_id": 50152856,
    "title": "spacy create new language model with data from corpus",
    "body": "<p>I am trying to create a new language model (Luxembourgish) in spaCy, but I am confused on how to do this. </p>\n\n<p>I followed the <a href=\"https://spacy.io/usage/adding-languages\" rel=\"noreferrer\">instructions on their website</a> and did a similar thing as in <a href=\"https://stackoverflow.com/questions/47196799/how-write-code-and-run-pythons-files-using-spacy-using-windows/47233007#47233007\">this post</a>. But what I do not understand is, how to add data like a vocab or wordvectors. (e.g. \"fill\" the language template) </p>\n\n<p>I get that there are some <a href=\"https://github.com/explosion/spacy-dev-resources\" rel=\"noreferrer\">dev tools</a> for same of these operations, but their execution is poorly documented so I do not get how to install and use them properly, especially as they seem to be in python 2.7 which clashes with my spacy installation as it uses python 3.</p>\n\n<p>As for now I have a <code>corpus.txt</code> (from a wikipediadump) on which I want to train and a language template with the defaults like <code>stop_words.py</code>, <code>tokenizer_exceptions.py</code> etc. that I created and filled by hand.</p>\n\n<p>Anyone ever done this properly and could help me here?</p>\n",
    "score": 11,
    "creation_date": 1525343543,
    "view_count": 4582,
    "answer_count": 1,
    "tags": "python;windows;nlp;spacy"
  },
  {
    "question_id": 18416561,
    "title": "POS tagging in Scala",
    "body": "<p>I tried to POS tag a sentence in Scala using Stanford parser like below</p>\n\n<pre><code>val lp:LexicalizedParser = LexicalizedParser.loadModel(\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\");\nlp.setOptionFlags(\"-maxLength\", \"50\", \"-retainTmpSubcategories\")\nval s = \"I love to play\"\nval parse :Tree =  lp.apply(s)\nval taggedWords = parse.taggedYield()\nprintln(taggedWords)\n</code></pre>\n\n<p>I got an error <strong>type mismatch; found : java.lang.String required: java.util.List[_ &lt;: edu.stanford.nlp.ling.HasWord]</strong> in the line <strong>val parse :Tree =  lp.apply(s)</strong></p>\n\n<p>I don't know whether this is the right way of doing it or not. Are there any other easy ways of POS tagging a sentence in Scala?</p>\n",
    "score": 11,
    "creation_date": 1377332968,
    "view_count": 2906,
    "answer_count": 3,
    "tags": "scala;nlp;stanford-nlp;pos-tagger"
  },
  {
    "question_id": 12290667,
    "title": "Identifying a person&#39;s name vs. a dictionary word",
    "body": "<p>Is there some way to recognize that a word is likely to be/is not likely to be a person's name?</p>\n\n<p>So if I see the word \"understanding\" I would get a probability of 0.01, whereas the word \"Johnson\" would return a probability of 0.99, while a word like Smith would return 0.75 and a word like Apple 0.15.</p>\n\n<p>Is there any way to do this? </p>\n\n<p>The goal is, if someone searches for, say <code>Charles Darwin galapagos</code>, the search engine guesses that it should search the author field for <code>Charles</code> and <code>Darwin</code> and the title and abstract fields for <code>galapagos</code>.</p>\n",
    "score": 11,
    "creation_date": 1346884029,
    "view_count": 4148,
    "answer_count": 3,
    "tags": "algorithm;search;nlp;dictionary"
  },
  {
    "question_id": 9149709,
    "title": "Extracting Words using nltk from German Text",
    "body": "<p>I am trying to extract words from a german document, when I use th following method as described in the nltk tutorial, I fail to get the words with language specific special characters.</p>\n\n<pre><code>ptcr = nltk.corpus.PlaintextCorpusReader(Corpus, '.*');\nwords = nltk.Text(ptcr.words(DocumentName))\n</code></pre>\n\n<p>What should I do to get the list of words in the document?</p>\n\n<p>An example with <code>nltk.tokenize.WordPunctTokenizer()</code> for the german phrase <code>Veränderungen über einen Walzer</code> looks like:</p>\n\n<pre><code>In [231]: nltk.tokenize.WordPunctTokenizer().tokenize(u\"Veränderungen über einen Walzer\")\n\nOut[231]: [u'Ver\\xc3', u'\\xa4', u'nderungen', u'\\xc3\\xbcber', u'einen', u'Walzer']\n</code></pre>\n\n<p>In this example \"ä\" is treated as a delimiter,even though \"ü\" is not. </p>\n",
    "score": 11,
    "creation_date": 1328449389,
    "view_count": 24348,
    "answer_count": 3,
    "tags": "python;nlp;nltk;text-mining"
  },
  {
    "question_id": 24192979,
    "title": "How to generate a list of antonyms for adjectives in WordNet using Python",
    "body": "<p>I want to do the following in Python (I have the NLTK library, but I'm not great with Python, so I've written the following in a weird pseudocode):</p>\n\n<pre><code>from nltk.corpus import wordnet as wn  #Import the WordNet library\nfor each adjective as adj in wn        #Get all adjectives from the wordnet dictionary\n    print adj &amp; antonym                #List all antonyms for each adjective \nonce list is complete then export to txt file\n</code></pre>\n\n<p>This is so I can generate a complete dictionary of antonyms for adjectives. I think it should be doable, but I don't know how to create the Python script. I'd like to do it in Python as that's the NLTK's native language.</p>\n",
    "score": 11,
    "creation_date": 1402603141,
    "view_count": 10402,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 22076055,
    "title": "Hindi to English Transliteration",
    "body": "<p>Is there a python library for transliterating Hindi to English?</p>\n<p>e.g. &quot;खाया&quot; should be converted to &quot;khaya&quot;</p>\n",
    "score": 11,
    "creation_date": 1393522531,
    "view_count": 6128,
    "answer_count": 1,
    "tags": "python;nlp;nltk;transliteration"
  },
  {
    "question_id": 21902411,
    "title": "How to get domain of words using WordNet in Python?",
    "body": "<p>How can I find domain of words using nltk Python module and <a href=\"https://en.wikipedia.org/wiki/WordNet\" rel=\"noreferrer\">WordNet</a>?</p>\n\n<p>Suppose I have words like (transaction, Demand Draft, cheque, passbook) and the domain for all these words is \"BANK\". How can we get this using nltk and WordNet in Python?</p>\n\n<p>I am trying through hypernym and hyponym relationship:</p>\n\n<p>For example:</p>\n\n<pre><code>from nltk.corpus import wordnet as wn\nsports = wn.synset('sport.n.01')\nsports.hyponyms()\n[Synset('judo.n.01'), Synset('athletic_game.n.01'), Synset('spectator_sport.n.01'),    Synset('contact_sport.n.01'), Synset('cycling.n.01'), Synset('funambulism.n.01'), Synset('water_sport.n.01'), Synset('riding.n.01'), Synset('gymnastics.n.01'), Synset('sledding.n.01'), Synset('skating.n.01'), Synset('skiing.n.01'), Synset('outdoor_sport.n.01'), Synset('rowing.n.01'), Synset('track_and_field.n.01'), Synset('archery.n.01'), Synset('team_sport.n.01'), Synset('rock_climbing.n.01'), Synset('racing.n.01'), Synset('blood_sport.n.01')]\n</code></pre>\n\n<p>and</p>\n\n<pre><code>bark = wn.synset('bark.n.02')\nbark.hypernyms()\n[Synset('noise.n.01')]\n</code></pre>\n",
    "score": 11,
    "creation_date": 1392885720,
    "view_count": 8432,
    "answer_count": 4,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 18496925,
    "title": "How to parse product titles (unstructured) into structured data?",
    "body": "<p>I am looking to parse unstructured product titles like “Canon D1000 4MP Camera 2X Zoom LCD” into structured data like <code>{brand: canon, model number: d1000, lens: 4MP zoom: 2X, display type: LCD}</code>.</p>\n\n<p>So far I have:</p>\n\n<ol>\n<li>Removed stopwords and cleaned up (remove characters like <code>-</code> <code>;</code> <code>:</code> <code>/</code>)</li>\n<li>Tokenizing long strings into words.</li>\n</ol>\n\n<p>Any techniques/library/methods/algorithms would be much appreciated!</p>\n\n<p>EDIT: There is no heuristic for the product titles. A seller can input <strong>anything</strong> as a title. For eg: 'Canon D1000' can just be the title. Also, this exercise is not only for camera datasets, the title can be of any product.  </p>\n",
    "score": 11,
    "creation_date": 1377719113,
    "view_count": 4684,
    "answer_count": 5,
    "tags": "parsing;machine-learning;e-commerce;nlp;artificial-intelligence"
  },
  {
    "question_id": 17879551,
    "title": "NLTK. Find if a sentence is in a questioning form",
    "body": "<p>I am trying to detect if a sentence is a question or a statement. Apart from looking for a question mark at the end of the sentence, is there another way to detect this? I am processing Twitter posts and people are not necessarily following good practises like question marks on Twitter.</p>\n\n<p>Reference to other libraries is also ok with me if <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">nltk</a> does now work.</p>\n",
    "score": 11,
    "creation_date": 1374836144,
    "view_count": 12830,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 10079163,
    "title": "Training Naive Bayes Classifier on ngrams",
    "body": "<p>I've been using the <a href=\"http://classifier.rubyforge.org/\" rel=\"nofollow noreferrer\">Ruby Classifier library</a> to <a href=\"https://stackoverflow.com/questions/9709293/interesting-nlp-machine-learning-style-project-analyzing-privacy-policies\">classify privacy policies</a>.  I've come to the conclusion that the simple bag-of-words approach built into this library is not enough.  To increase my classification accuracy, I want to train the classifier on n-grams in addition to individual words.</p>\n\n<p>I was wondering whether there's a library out there for preprocessing documents to get relevant n-grams (and properly deal with punctuation).  One thought was that I could preprocess the documents and feed pseudo-ngrams into the Ruby Classifier like:</p>\n\n<blockquote>\n  <p>wordone_wordtwo_wordthree</p>\n</blockquote>\n\n<p>Or maybe there's a better way to be doing this, such as a library that has ngram based Naive Bayes Classification built into it from the getgo.  I'm open to using languages other than Ruby here if they get the job done (Python seems like a good candidate if need be).</p>\n",
    "score": 11,
    "creation_date": 1334002264,
    "view_count": 3202,
    "answer_count": 2,
    "tags": "python;ruby;nlp;machine-learning;classification"
  },
  {
    "question_id": 45403390,
    "title": "Lemmatizing Italian sentences for frequency counting",
    "body": "<p>I would like to lemmatize some Italian text in order to perform some frequency counting of words and further investigations on the output of this lemmatized content.</p>\n\n<p>I am preferring lemmatizing than stemming because I could extract the word meaning from the context in the sentence (e.g. distinguish between a verb and a noun) and obtain words that exist in the language, rather than roots of those words that don't usually have a meaning.</p>\n\n<p>I found out this library called <code>pattern</code> (<code>pip2 install pattern</code>) that should complement <code>nltk</code> in order to perform lemmatization of the <strong>Italian language</strong>, however I am not sure the approach below is correct because each word is lemmatized by itself, not in the context of a sentence.</p>\n\n<p>Probably I should give <code>pattern</code> the responsibility to tokenize a sentence (so also annotating each word with the metadata regarding verbs/nouns/adjectives etc), then retrieving the lemmatized word, but I am not able to do this and I am not even sure it is possible at the moment?</p>\n\n<p>Also: in Italian some articles are rendered with an apostrophe so for example \"l'appartamento\" (in English \"the flat\") is actually 2 words: \"lo\" and \"appartamento\". Right now I am not able to find a way to split these 2 words with a combination of <code>nltk</code> and <code>pattern</code> so then I am not able to count the frequency of the words in the correct way.</p>\n\n<pre><code>import nltk\nimport string\nimport pattern\n\n# dictionary of Italian stop-words\nit_stop_words = nltk.corpus.stopwords.words('italian')\n# Snowball stemmer with rules for the Italian language\nita_stemmer = nltk.stem.snowball.ItalianStemmer()\n\n# the following function is just to get the lemma\n# out of the original input word (but right now\n# it may be loosing the context about the sentence\n# from where the word is coming from i.e.\n# the same word could either be a noun/verb/adjective\n# according to the context)\ndef lemmatize_word(input_word):\n    in_word = input_word#.decode('utf-8')\n    # print('Something: {}'.format(in_word))\n    word_it = pattern.it.parse(\n        in_word, \n        tokenize=False,  \n        tag=False,  \n        chunk=False,  \n        lemmata=True \n    )\n    # print(\"Input: {} Output: {}\".format(in_word, word_it))\n    the_lemmatized_word = word_it.split()[0][0][4]\n    # print(\"Returning: {}\".format(the_lemmatized_word))\n    return the_lemmatized_word\n\nit_string = \"Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo. Stasera mangio la pizza con le verdure.\"\n\n# 1st tokenize the sentence(s)\nword_tokenized_list = nltk.tokenize.word_tokenize(it_string)\nprint(\"1) NLTK tokenizer, num words: {} for list: {}\".format(len(word_tokenized_list), word_tokenized_list))\n\n# 2nd remove punctuation and everything lower case\nword_tokenized_no_punct = [string.lower(x) for x in word_tokenized_list if x not in string.punctuation]\nprint(\"2) Clean punctuation, num words: {} for list: {}\".format(len(word_tokenized_no_punct), word_tokenized_no_punct))\n\n# 3rd remove stop words (for the Italian language)\nword_tokenized_no_punct_no_sw = [x for x in word_tokenized_no_punct if x not in it_stop_words]\nprint(\"3) Clean stop-words, num words: {} for list: {}\".format(len(word_tokenized_no_punct_no_sw), word_tokenized_no_punct_no_sw))\n\n# 4.1 lemmatize the words\nword_tokenize_list_no_punct_lc_no_stowords_lemmatized = [lemmatize_word(x) for x in word_tokenized_no_punct_no_sw]\nprint(\"4.1) lemmatizer, num words: {} for list: {}\".format(len(word_tokenize_list_no_punct_lc_no_stowords_lemmatized), word_tokenize_list_no_punct_lc_no_stowords_lemmatized))\n\n# 4.2 snowball stemmer for Italian\nword_tokenize_list_no_punct_lc_no_stowords_stem = [ita_stemmer.stem(i) for i in word_tokenized_no_punct_no_sw]\nprint(\"4.2) stemmer, num words: {} for list: {}\".format(len(word_tokenize_list_no_punct_lc_no_stowords_stem), word_tokenize_list_no_punct_lc_no_stowords_stem))\n\n# difference between stemmer and lemmatizer\nprint(\n    \"For original word(s) '{}' and '{}' the stemmer: '{}' '{}' (count 1 each), the lemmatizer: '{}' '{}' (count 2)\"\n    .format(\n        word_tokenized_no_punct_no_sw[1],\n        word_tokenized_no_punct_no_sw[6],\n        word_tokenize_list_no_punct_lc_no_stowords_stem[1],\n        word_tokenize_list_no_punct_lc_no_stowords_stem[6],\n        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1],\n        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1]\n    )\n)\n</code></pre>\n\n<p>Gives this output:</p>\n\n<pre><code>1) NLTK tokenizer, num words: 20 for list: ['Ieri', 'sono', 'andato', 'in', 'due', 'supermercati', '.', 'Oggi', 'volevo', 'andare', \"all'ippodromo\", '.', 'Stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure', '.']\n2) Clean punctuation, num words: 17 for list: ['ieri', 'sono', 'andato', 'in', 'due', 'supermercati', 'oggi', 'volevo', 'andare', \"all'ippodromo\", 'stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure']\n3) Clean stop-words, num words: 12 for list: ['ieri', 'andato', 'due', 'supermercati', 'oggi', 'volevo', 'andare', \"all'ippodromo\", 'stasera', 'mangio', 'pizza', 'verdure']\n4.1) lemmatizer, num words: 12 for list: [u'ieri', u'andarsene', u'due', u'supermercato', u'oggi', u'volere', u'andare', u\"all'ippodromo\", u'stasera', u'mangiare', u'pizza', u'verdura']\n4.2) stemmer, num words: 12 for list: [u'ier', u'andat', u'due', u'supermerc', u'oggi', u'vol', u'andar', u\"all'ippodrom\", u'staser', u'mang', u'pizz', u'verdur']\nFor original word(s) 'andato' and 'andare' the stemmer: 'andat' 'andar' (count 1 each), the lemmatizer: 'andarsene' 'andarsene' (count 2)\n</code></pre>\n\n<ul>\n<li>How to effectively lemmatize some sentences with <code>pattern</code> using their tokenizer? (assuming lemmas are recognized as nouns/verbs/adjectives etc.)</li>\n<li>Is there a python alternative to <code>pattern</code> to use for Italian lemmatization with <code>nltk</code>?  </li>\n<li>How to split articles that are bound to the next word using apostrophes?</li>\n</ul>\n",
    "score": 11,
    "creation_date": 1501440093,
    "view_count": 12945,
    "answer_count": 3,
    "tags": "python-2.7;nlp;nltk;stemming;lemmatization"
  },
  {
    "question_id": 27629130,
    "title": "Chunking Stanford Named Entity Recognizer (NER) outputs from NLTK format",
    "body": "<p>I am using NER in NLTK to find persons, locations, and organizations in sentences. I am able to produce the results like this:</p>\n\n<pre><code>[(u'Remaking', u'O'), (u'The', u'O'), (u'Republican', u'ORGANIZATION'), (u'Party', u'ORGANIZATION')]\n</code></pre>\n\n<p>Is that possible to chunk things together by using it?\nWhat I want is like this:</p>\n\n<pre><code>u'Remaking'/ u'O', u'The'/u'O', (u'Republican', u'Party')/u'ORGANIZATION'\n</code></pre>\n\n<p>Thanks!</p>\n",
    "score": 11,
    "creation_date": 1419374775,
    "view_count": 2546,
    "answer_count": 4,
    "tags": "python;nlp;nltk;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 11279054,
    "title": "sharpNLP as .nbin file extension",
    "body": "<p>I've downloaded SharpNLP from this site <a href=\"http://sharpnlp.codeplex.com/\">http://sharpnlp.codeplex.com/</a>\nbut it downloaded .nbin file, which i don't know how to deal with.\nAny help pleeeeeeeease?</p>\n",
    "score": 11,
    "creation_date": 1341107795,
    "view_count": 10775,
    "answer_count": 1,
    "tags": "c#;nlp"
  },
  {
    "question_id": 9663918,
    "title": "How can I tag and chunk French text using NLTK and Python?",
    "body": "<p>I have 30,000+ French-language articles in a JSON file.  I would like to perform some text analysis on both individual articles and on the set as a whole.  Before I go further, I'm starting with simple goals:</p>\n\n<ul>\n<li>Identify important entities (people, places, concepts)</li>\n<li>Find significant changes in the importance (~=frequency) of those entities over time (using the article sequence number as a proxy for time)</li>\n</ul>\n\n<p>The steps I've taken so far:</p>\n\n<ol>\n<li><p>Imported the data into a python list:</p>\n\n<pre><code>import json\njson_articles=open('articlefile.json')\narticlelist = json.load(json_articles)\n</code></pre></li>\n<li><p>Selected a single article to test, and concatenated the body text into a single string:</p>\n\n<pre><code>txt =  ' '.join(data[10000]['body'])\n</code></pre></li>\n<li><p>Loaded a French sentence tokenizer and split the string into a list of sentences:</p>\n\n<pre><code>nltk.data.load('tokenizers/punkt/french.pickle')\ntokens = [french_tokenizer.tokenize(s) for s in sentences]\n</code></pre></li>\n<li><p>Attempted to split the sentences into words using the WhiteSpaceTokenizer:</p>\n\n<pre><code>from nltk.tokenize import WhitespaceTokenizer\nwst = WhitespaceTokenizer()\ntokens = [wst.tokenize(s) for s in sentences]\n</code></pre></li>\n</ol>\n\n<p>This is where I'm stuck, for the following reasons:</p>\n\n<ul>\n<li>NLTK doesn't have a built-in tokenizer which can split French into words.  White space doesn't work well, particular due to the fact it won't correctly separate on apostrophes.</li>\n<li>Even if I were to use regular expressions to split into individual words, there's no French PoS (parts of speech) tagger that I can use to tag those words, and no way to chunk them into logical units of meaning</li>\n</ul>\n\n<p>For English, I could tag and chunk the text like so:</p>\n\n<pre><code>    tagged = [nltk.pos_tag(token) for token in tokens]\n    chunks = nltk.batch_ne_chunk(tagged)\n</code></pre>\n\n<p>My main options (in order of current preference) seem to be:</p>\n\n<ol>\n<li>Use <a href=\"https://github.com/japerk/nltk-trainer\">nltk-trainer</a> to train my own tagger and chunker.</li>\n<li>Use the python wrapper for TreeTagger for just this part, as TreeTagger can already tag French, and someone has written a wrapper which calls the TreeTagger binary and parses the results.</li>\n<li>Use a different tool altogether.</li>\n</ol>\n\n<p>If I were to do (1), I imagine I would need to create my own tagged corpus.  Is this correct, or would it be possible (and premitted) to use the French Treebank?</p>\n\n<p>If the French Treebank corpus format (<a href=\"http://www.llf.cnrs.fr/Gens/Abeille/example1.txt\">example here</a>) is not suitable for use with nltk-trainer, is it feasible to convert it into such a format?</p>\n\n<p>What approaches have French-speaking users of NLTK taken to PoS tag and chunk text?</p>\n",
    "score": 11,
    "creation_date": 1331541730,
    "view_count": 16342,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 59830168,
    "title": "layer Normalization in pytorch?",
    "body": "<p>shouldn't the layer normalization of <code>x = torch.tensor([[1.5,0,0,0,0]])</code> be <code>[[1.5,-0.5,-0.5,-0.5]]</code> ? according to this paper <a href=\"https://arxiv.org/pdf/1607.06450.pdf\" rel=\"noreferrer\">paper</a> and the equation from the <a href=\"https://pytorch.org/docs/stable/nn.html#layernorm\" rel=\"noreferrer\">pytorch doc</a>. But the <code>torch.nn.LayerNorm</code> gives <code>[[ 1.7320, -0.5773, -0.5773, -0.5773]]</code></p>\n\n<p>Here is the example code:</p>\n\n<pre><code>x = torch.tensor([[1.5,.0,.0,.0]])\nlayerNorm = torch.nn.LayerNorm(4, elementwise_affine = False)\n\ny1 = layerNorm(x)\n\nmean = x.mean(-1, keepdim = True)\nvar = x.var(-1, keepdim = True)\ny2 = (x-mean)/torch.sqrt(var+layerNorm.eps)\n</code></pre>\n\n<p>where:</p>\n\n<pre><code>y1 == tensor([[ 1.7320, -0.5773, -0.5773, -0.5773]])\ny2 == tensor([[ 1.5000, -0.5000, -0.5000, -0.5000]])\n</code></pre>\n",
    "score": 11,
    "creation_date": 1579550825,
    "view_count": 18663,
    "answer_count": 4,
    "tags": "machine-learning;deep-learning;nlp;pytorch"
  },
  {
    "question_id": 56643503,
    "title": "Efficient metrics evaluation in PyTorch",
    "body": "<p>I am new to PyTorch and want to efficiently evaluate among others F1 during my Training and my Validation Loop.</p>\n\n<p>So far, my approach was to calculate the predictions on GPU, then push them to CPU and append them to a vector for both Training and Validation. After Training and Validation, I would evaluate both for each epoch using sklearn. However, profiling my code it showed, that pushing to cpu is quite a bottleneck. </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>for epoch in range(n_epochs):\n    model.train()\n    avg_loss = 0\n    avg_val_loss = 0\n    train_pred = np.array([])\n    val_pred = np.array([])\n    # Training loop (transpose X_batch to fit pretrained (features, samples) style)\n    for X_batch, y_batch in train_loader:\n        scores = model(X_batch)\n        y_pred = F.softmax(scores, dim=1)\n        train_pred = np.append(train_pred, self.get_vector(y_pred.detach().cpu().numpy()))\n\n        loss = loss_fn(scores, self.get_vector(y_batch))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        avg_loss += loss.item() / len(train_loader)\n\n    model.eval()\n    # Validation loop\n    for X_batch, y_batch in val_loader:\n        with torch.no_grad():\n            scores = model(X_batch)\n            y_pred = F.softmax(scores, dim=1)\n            val_pred = np.append(val_pred, self.get_vector(y_pred.detach().cpu().numpy()))\n            loss = loss_fn(scores, self.get_vector(y_batch))\n            avg_val_loss += loss.item() / len(val_loader)\n\n    # Model Checkpoint for best validation f1\n    val_f1 = self.calculate_metrics(train_targets[val_index], val_pred, f1_only=True)\n    if val_f1 &gt; best_val_f1:\n        prev_best_val_f1 = best_val_f1\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), self.PATHS['xlm'])\n        evaluated_epoch = epoch\n\n    # Calc the metrics\n    self.save_metrics(train_targets[train_index], train_pred, avg_loss, 'train')\n    self.save_metrics(train_targets[val_index], val_pred, avg_val_loss, 'val')\n</code></pre>\n\n<p>I am certain there is a more efficient way to \na) store the predictions without having to push them to cpu each batch. b) calculate the metrics on GPU directly?</p>\n\n<p>As I am new to PyTorch, I am very grateful for any hints and feedback :)</p>\n",
    "score": 11,
    "creation_date": 1560841651,
    "view_count": 24882,
    "answer_count": 1,
    "tags": "python;deep-learning;nlp;pytorch"
  },
  {
    "question_id": 51663068,
    "title": "Tensorflow.js tokenizer",
    "body": "<p>I'm new to Machine Learning and Tensorflow, since I don't know python so I decide to use there javascript version (maybe more like a wrapper). </p>\n\n<p>The <strong>problem</strong> is I tried to build a model that process the Natural Language. So the first step is tokenizer the text in order to feed the data to model. I did a lot research, but most of them are using python version of tensorflow that use method like: <code>tf.keras.preprocessing.text.Tokenizer</code> which I can't find similar in tensorflow.js. I'm stuck in this step and don't know how can I transfer text to vector that can feed to model. Please help :)</p>\n",
    "score": 11,
    "creation_date": 1533249612,
    "view_count": 7522,
    "answer_count": 2,
    "tags": "javascript;machine-learning;tensorflow.js;nlp"
  },
  {
    "question_id": 63979544,
    "title": "Using trained BERT Model and Data Preprocessing",
    "body": "<p>When using a pre-trained BERT embeddings from pytorch (which are then fine-tuned), should the text data fed into the model be pre-processed like in any standard NLP task?</p>\n<p>For instance, should  stemming, removing low frequency words, de-captilisation, be performed or should the raw text simply be passed to `transformers.BertTokenizer'?</p>\n",
    "score": 11,
    "creation_date": 1600608839,
    "view_count": 9616,
    "answer_count": 3,
    "tags": "nlp;pytorch;bert-language-model"
  },
  {
    "question_id": 16262016,
    "title": "How to predict the topic of a new query using a trained LDA model using gensim?",
    "body": "<p>I have trained a corpus for LDA topic modelling using gensim.</p>\n\n<p>Going through the tutorial on the gensim website (this is not the whole code):</p>\n\n<pre><code>question = 'Changelog generation from Github issues?';\n\ntemp = question.lower()\nfor i in range(len(punctuation_string)):\n    temp = temp.replace(punctuation_string[i], '')\n\nwords = re.findall(r'\\w+', temp, flags = re.UNICODE | re.LOCALE)\nimportant_words = []\nimportant_words = filter(lambda x: x not in stoplist, words)\nprint important_words\ndictionary = corpora.Dictionary.load('questions.dict')\nques_vec = []\nques_vec = dictionary.doc2bow(important_words)\nprint dictionary\nprint ques_vec\nprint lda[ques_vec]\n</code></pre>\n\n<p>This is the output that I get:</p>\n\n<pre><code>['changelog', 'generation', 'github', 'issues']\nDictionary(15791 unique tokens)\n[(514, 1), (3625, 1), (3626, 1), (3627, 1)]\n[(4, 0.20400000000000032), (11, 0.20400000000000032), (19, 0.20263215848547525), (29, 0.20536784151452539)]\n</code></pre>\n\n<p>I don't know how the last output is going to help me find the possible topic for the <code>question</code> !!!</p>\n\n<p>Please help!</p>\n",
    "score": 11,
    "creation_date": 1367145583,
    "view_count": 17385,
    "answer_count": 3,
    "tags": "python;nlp;lda;topic-modeling;gensim"
  },
  {
    "question_id": 7814815,
    "title": "Natural Language Processing in PHP",
    "body": "<p>Given, say, a recipe (list of ingredients, steps, etc.) in free text form, how could I parse that in such a way I can pull out the ingredients (e.g. quantity, unit of measurements, ingredient name, etc.) usin PHP?</p>\n\n<p>Assume that the free text is <em>somewhat</em> formatted.</p>\n",
    "score": 11,
    "creation_date": 1318979126,
    "view_count": 12225,
    "answer_count": 5,
    "tags": "php;algorithm;nlp"
  },
  {
    "question_id": 68349833,
    "title": "Pip can&#39;t install pyicu",
    "body": "<p>I am running a AWS that runs Ubuntu 20.04. I am trying to install the package <code>pyicu</code>, but I am facing problems. I tried running <code>sudo apt install libicu-dev</code>, but I still can't install pyicu. I am not able to install brew on the aws server. Any other suggestions? This is the erro message:</p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: /root/dev/big5_rest/venv/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-se360cxw/pyicu/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-se360cxw/pyicu/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-install-se360cxw/pyicu/pip-egg-info\n         cwd: /tmp/pip-install-se360cxw/pyicu/\n    Complete output (53 lines):\n    Traceback (most recent call last):\n      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 63, in &lt;module&gt;\n        ICU_VERSION = os.environ['ICU_VERSION']\n      File &quot;/usr/lib/python3.8/os.py&quot;, line 675, in __getitem__\n        raise KeyError(key) from None\n    KeyError: 'ICU_VERSION'\n    \n    During handling of the above exception, another exception occurred:\n    \n    Traceback (most recent call last):\n      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 66, in &lt;module&gt;\n        ICU_VERSION = check_output(('icu-config', '--version')).strip()\n      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 19, in check_output\n        return subprocess_check_output(popenargs)\n      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 411, in check_output\n        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 489, in run\n        with Popen(*popenargs, **kwargs) as process:\n      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 854, in __init__\n        self._execute_child(args, executable, preexec_fn, close_fds,\n      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 1702, in _execute_child\n        raise child_exception_type(errno_num, err_msg, err_filename)\n    FileNotFoundError: [Errno 2] No such file or directory: 'icu-config'\n    \n    During handling of the above exception, another exception occurred:\n    \n    Traceback (most recent call last):\n      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 69, in &lt;module&gt;\n        ICU_VERSION = check_output(('pkg-config', '--modversion', 'icu-i18n')).strip()\n      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 19, in check_output\n        return subprocess_check_output(popenargs)\n      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 411, in check_output\n        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 489, in run\n        with Popen(*popenargs, **kwargs) as process:\n      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 854, in __init__\n        self._execute_child(args, executable, preexec_fn, close_fds,\n      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 1702, in _execute_child\n        raise child_exception_type(errno_num, err_msg, err_filename)\n    FileNotFoundError: [Errno 2] No such file or directory: 'pkg-config'\n    \n    During handling of the above exception, another exception occurred:\n    \n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 71, in &lt;module&gt;\n        raise RuntimeError('''\n    RuntimeError:\n    Please install pkg-config on your system or set the ICU_VERSION environment\n    variable to the version of ICU you have installed.\n    \n    (running 'icu-config --version')\n    (running 'pkg-config --modversion icu-i18n')\n    ----------------------------------------\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n</code></pre>\n",
    "score": 11,
    "creation_date": 1626102970,
    "view_count": 10891,
    "answer_count": 5,
    "tags": "python;pip;nlp;polyglot;pyicu"
  },
  {
    "question_id": 47036409,
    "title": "Keras - how to get unnormalized logits instead of probabilities",
    "body": "<p>I am creating a model in Keras and want to compute my own metric (perplexity). This requires using the unnormalized probabilities/logits. However, the keras model only returns the softmax probabilties:</p>\n\n<pre><code>model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(LSTM(n_hidden, return_sequences=False))\nmodel.add(Dropout(dropout_keep_prob))\nmodel.add(Dense(vocab_size))\nmodel.add(Activation('softmax'))\noptimizer = RMSprop(lr=self.lr)\n\nmodel.compile(optimizer=optimizer, \nloss='sparse_categorical_crossentropy')\n</code></pre>\n\n<p>The Keras FAQ have a solution to get the output of intermediate layers <a href=\"https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer\" rel=\"noreferrer\">here</a>. Another solution is given <a href=\"https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer\">here</a>. However, these answers store the intermediate outputs in a different model which is not what I need.\nI want to use the logits for my custom metric. The custom metric should be included in the <code>model.compile()</code> function such that it's evaluated and displayed during training. So I don't need the output of the <code>Dense</code> layer separated in a different model, but as part of my original model.</p>\n\n<p>In short, my questions are:</p>\n\n<ul>\n<li><p>When defining a custom metric as outlined <a href=\"https://keras.io/metrics/\" rel=\"noreferrer\">here</a> using <code>def custom_metric(y_true, y_pred)</code>, does the <code>y_pred</code> contain logits or normalized probabilities? </p></li>\n<li><p>If it contains normalized probabilities, how can I get the unnormalized probabilities, i.e. the logits output by the <code>Dense</code> layer?</p></li>\n</ul>\n",
    "score": 11,
    "creation_date": 1509455742,
    "view_count": 10079,
    "answer_count": 3,
    "tags": "python;machine-learning;keras;neural-network;nlp"
  },
  {
    "question_id": 30182138,
    "title": "How to replace a word by its most representative mention using Stanford CoreNLP Coreferences module",
    "body": "<p>I am trying to figure out the way to rewrite sentences by \"resolving\" (replacing words with) their coreferences using Stanford Corenlp's Coreference module.</p>\n\n<p>The idea is to rewrite a sentence like the following :</p>\n\n<blockquote>\n  <p>John drove to Judy’s house. He made her dinner.</p>\n</blockquote>\n\n<p>into </p>\n\n<blockquote>\n  <p>John drove to Judy’s house. John made Judy dinner.</p>\n</blockquote>\n\n<p>Here's the code I've been fooling around with : </p>\n\n<pre><code>    private void doTest(String text){\n    Annotation doc = new Annotation(text);\n    pipeline.annotate(doc);\n\n\n    Map&lt;Integer, CorefChain&gt; corefs = doc.get(CorefChainAnnotation.class);\n    List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);\n\n\n    List&lt;String&gt; resolved = new ArrayList&lt;String&gt;();\n\n    for (CoreMap sentence : sentences) {\n\n        List&lt;CoreLabel&gt; tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);\n\n        for (CoreLabel token : tokens) {\n\n            Integer corefClustId= token.get(CorefCoreAnnotations.CorefClusterIdAnnotation.class);\n            System.out.println(token.word() +  \" --&gt; corefClusterID = \" + corefClustId);\n\n\n            CorefChain chain = corefs.get(corefClustId);\n            System.out.println(\"matched chain = \" + chain);\n\n\n            if(chain==null){\n                resolved.add(token.word());\n            }else{\n\n                int sentINdx = chain.getRepresentativeMention().sentNum -1;\n                CoreMap corefSentence = sentences.get(sentINdx);\n                List&lt;CoreLabel&gt; corefSentenceTokens = corefSentence.get(TokensAnnotation.class);\n\n                String newwords = \"\";\n                CorefMention reprMent = chain.getRepresentativeMention();\n                System.out.println(reprMent);\n                for(int i = reprMent.startIndex; i&lt;reprMent.endIndex; i++){\n                    CoreLabel matchedLabel = corefSentenceTokens.get(i-1); //resolved.add(tokens.get(i).word());\n                    resolved.add(matchedLabel.word());\n\n                    newwords+=matchedLabel.word()+\" \";\n\n                }\n\n\n\n\n                System.out.println(\"converting \" + token.word() + \" to \" + newwords);\n            }\n\n\n            System.out.println();\n            System.out.println();\n            System.out.println(\"-----------------------------------------------------------------\");\n\n        }\n\n    }\n\n\n    String resolvedStr =\"\";\n    System.out.println();\n    for (String str : resolved) {\n        resolvedStr+=str+\" \";\n    }\n    System.out.println(resolvedStr);\n\n\n}\n</code></pre>\n\n<p>The best output I was able to achieve for now is </p>\n\n<blockquote>\n  <p>John drove to Judy 's 's Judy 's house . John made Judy 's her dinner . </p>\n</blockquote>\n\n<p>which is not very brilliant ... </p>\n\n<p>I'm pretty sure there is a MUCH easier way to do what I am trying to achieve.</p>\n\n<p>Ideally, I would like to reorganize the sentence as a list of CoreLabels, so that I could keep the other data they have attached to them. </p>\n\n<p>Any help appreciated.</p>\n",
    "score": 11,
    "creation_date": 1431406851,
    "view_count": 2376,
    "answer_count": 2,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 4457830,
    "title": "NLP programming tools using PHP?",
    "body": "<p>Since big web applications came into existence, searching for data (and doing it lightning fast and accurate) has been one of the most important problems in web applications. For a while, I've worked using <a href=\"http://incubator.apache.org/lucene.net/\">Lucene.NET</a>, which is a C# port of the <a href=\"http://lucene.apache.org/java/\">Lucene project</a>. </p>\n\n<p>I also work using PHP using <a href=\"http://framework.zend.com/manual/en/zend.search.lucene.html\">Zend Framework's Lucene API</a>, which brings me to my question. Most times for providing good indexing we need to perform some NLP tools like <strong>tokenizing</strong>, <strong>lemmatizing</strong>, and many more, the question is:</p>\n\n<p>Do you know of any good NLP programming framework/toolset using PHP?</p>\n\n<p>PS: I'm very aware of the Zend API for Lucene, but indexing data properly is not just storing and relying in Lucene, you need to perform some extra tasks, like those above.</p>\n",
    "score": 11,
    "creation_date": 1292478686,
    "view_count": 5912,
    "answer_count": 3,
    "tags": "php;lucene;nlp"
  },
  {
    "question_id": 55707577,
    "title": "How does TfidfVectorizer compute scores on test data",
    "body": "<p>In scikit-learn <code>TfidfVectorizer</code> allows us to fit over training data, and later use the same vectorizer to transform over our test data.\nThe output of the transformation over the train data is a matrix that represents a tf-idf score for each word for a given document.</p>\n\n<p>However, how does the fitted vectorizer compute the score for new inputs? I have guessed that either:</p>\n\n<ol>\n<li>The score of a word in a new document computed by some aggregation of the scores of the same word over documents in the training set.</li>\n<li>The new document is 'added' to the existing corpus and new scores are calculated.</li>\n</ol>\n\n<p>I have tried deducing the operation from scikit-learn's source <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py\" rel=\"noreferrer\">code</a> but could not quite figure it out. Is it one of the options I've previously mentioned or something else entirely?\nPlease assist.</p>\n",
    "score": 11,
    "creation_date": 1555415733,
    "view_count": 4342,
    "answer_count": 1,
    "tags": "scikit-learn;nlp;tf-idf;tfidfvectorizer"
  },
  {
    "question_id": 35721503,
    "title": "Gensim word2vec on predefined dictionary and word-indices data",
    "body": "<p>I need to train a word2vec representation on tweets using gensim. Unlike most tutorials and code I've seen on gensim my data is not raw, but has already been preprocessed. I have a dictionary in a text document containing 65k words (incl. an \"unknown\" token and a EOL token) and the tweets are saved as a numpy matrix with indices into this dictionary. A simple example of the data format can be seen below:</p>\n\n<p><strong>dict.txt</strong></p>\n\n<pre><code>you\nlove\nthis\ncode\n</code></pre>\n\n<p><strong>tweets (5 is unknown and 6 is EOL)</strong></p>\n\n<pre><code>[[0, 1, 2, 3, 6],\n [3, 5, 5, 1, 6],\n [0, 1, 3, 6, 6]]\n</code></pre>\n\n<p>I'm unsure how I should handle the indices representation. An easy way is just to convert the list of indices to a list of strings (i.e. [0, 1, 2, 3, 6] -> ['0', '1', '2', '3', '6']) as I read it into the word2vec model. However, this must be inefficient as gensim then will try to look up the internal index used for e.g. '2'.</p>\n\n<p>How do I load this data and create the word2vec representation in an efficient manner using gensim? </p>\n",
    "score": 11,
    "creation_date": 1456831222,
    "view_count": 3966,
    "answer_count": 2,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 30675935,
    "title": "Transporting Sparse Matrix from Python to R",
    "body": "<p>I am doing some text analysis work in Python. Unfortunately, I need to switch to R  in order to use a particular package (unfortunately, the package cannot be replicated in Python easily). </p>\n\n<p>Currently the text is parsed into bigram counts, reduced to a vocabulary of about 11,000 bigrams, and then stored as a dictionary: </p>\n\n<pre><code>{id1: {'bigrams':[(bigram1, count), (bigram2, count), ...]},\nid2: {'bigrams': ...} \n</code></pre>\n\n<p>I need to get this into a dgCMatrix in R, where the rows are id1, id2, ... and the columns are the different bigrams such that a cell represents the 'count' for that id-bigram. </p>\n\n<p>Any suggestions? I thought about expanding it just to a massive CSV, but that seems super inefficient plus probably infeasible due to memory constraints. </p>\n",
    "score": 11,
    "creation_date": 1433538903,
    "view_count": 3039,
    "answer_count": 1,
    "tags": "python;r;sparse-matrix;text-analysis"
  },
  {
    "question_id": 62466514,
    "title": "Shall we lower case input data for (pre) training a BERT uncased model using huggingface?",
    "body": "<p>Shall we lower case input data for (pre) training a BERT uncased model using huggingface? I looked into this response from Thomas Wolf (<a href=\"https://github.com/huggingface/transformers/issues/92#issuecomment-444677920\" rel=\"noreferrer\">https://github.com/huggingface/transformers/issues/92#issuecomment-444677920</a>) but not entirely sure if he meant that. </p>\n\n<p>What happens if we lowercase the text ? </p>\n",
    "score": 11,
    "creation_date": 1592556096,
    "view_count": 7906,
    "answer_count": 2,
    "tags": "deep-learning;nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 53533515,
    "title": "Retraining an existing machine learning model with new data",
    "body": "<p>I have a ML model which is trained on a million data set (supervised classification on text) , however I want the <strong>same model</strong> to get trained again as soon as a new data comes in (training data).</p>\n<p>This process is continuous and I <strong>don't</strong> want to <strong>loose</strong> the power of the <strong>model's prediction</strong> every time it receives a new data set. I don't want to merge the new data with my history data (~1 million samples) to train again.</p>\n<p>So the ideal would be for this model to grow up gradually training on all data over a period of time and preserving the intelligence of the model every time it receives a new training set data. What is the best way to <strong>avoid retraining all historical data</strong>? A Code sample would help me.</p>\n",
    "score": 11,
    "creation_date": 1543475010,
    "view_count": 22301,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;training-data;supervised-learning"
  },
  {
    "question_id": 52193581,
    "title": "Remove a word in a span from SpaCy?",
    "body": "<p>I am parsing a sentence with Spacy like following:</p>\n\n<pre><code>import spacy\nnlp = spacy.load(\"en\")\nspan = nlp(\"This is some text.\")\n</code></pre>\n\n<p>I am wondering if there is a way to delete a word in the span, while still keep the remaining words format like a sentence. Such as </p>\n\n<pre><code>del span[3]\n</code></pre>\n\n<p>which could yield a sentence like </p>\n\n<blockquote>\n  <p>This is some.</p>\n</blockquote>\n\n<p>If some other methods without SpaCy could achieve the same effect that will be great too. </p>\n",
    "score": 11,
    "creation_date": 1536182662,
    "view_count": 3925,
    "answer_count": 2,
    "tags": "python-3.x;nlp;spacy"
  },
  {
    "question_id": 39351215,
    "title": "What does tokens and vocab mean in glove embeddings?",
    "body": "<p>I am using glove embeddings and I am quite confused about <code>tokens</code> and <code>vocab</code> in the embeddings. Like this one:</p>\n\n<pre><code>Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n</code></pre>\n\n<p>what does <code>tokens</code> and <code>vocab</code> mean, respectively? What is the difference?</p>\n",
    "score": 11,
    "creation_date": 1473171215,
    "view_count": 2570,
    "answer_count": 2,
    "tags": "nlp;embedding"
  },
  {
    "question_id": 33671453,
    "title": "&quot;g++ not detected&quot; while data set goes larger, is there any limit to matrix size in GPU?",
    "body": "<p>I got this message in using Keras to train an RNN for language model with a big 3D tensor (generated from a text, one hot encoded, and results a shape of (165717, 25, 7631)):</p>\n\n<pre><code>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to \nexecute optimized C-implementations (for both CPU and GPU) and will default to \nPython implementations. Performance will be severely degraded. To remove this \nwarning, set Theano flags cxx to an empty string.\nERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc \ninstallation and try again.\n</code></pre>\n\n<p>But everything goes well while I limit the size of data set into small. Thus I wonder that does Theano or CUDA limit the size of matrix? </p>\n\n<p>Besides, do I have a better way to do one hot representation? I mean, in the large 3D tensor, most elements are 0 due to the one-hot representation. However, I didn't found a layer which accepts index representation of words.</p>\n",
    "score": 11,
    "creation_date": 1447331481,
    "view_count": 18710,
    "answer_count": 2,
    "tags": "nlp;theano;deep-learning;keras"
  },
  {
    "question_id": 6583160,
    "title": "Are there published generative grammars for natural languages?",
    "body": "<p>I have some ideas to do with natural language processing. I will need some grammars of the</p>\n\n<pre><code>S -&gt; NP VP\n</code></pre>\n\n<p>variety in order to play with them.</p>\n\n<p>If I try to write these rules myself it will be a tedious and error-prone business. <strong>Has anyone ever typed up and released comprehensive rule sets for English and other natural languages?</strong> Ideally written in BNF, Prolog or similar syntax.</p>\n\n<p>My project only relates to context-free grammars, I'm not interested in statistical methods or machine learning -- I need to systematically <em>produce</em> Engligh-like and Foobarian-like sentences.</p>\n\n<p>If you know where to find such materiel, I'd very much appreciate it.</p>\n",
    "score": 11,
    "creation_date": 1309871338,
    "view_count": 1514,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 4808702,
    "title": "Is there any kind of statistical natural language processing library for Haskell?",
    "body": "<p>Currently I'm reading Natural Language Processing for the Working Programmer (a work in progress book <a href=\"http://nlpwp.org/\" rel=\"noreferrer\">http://nlpwp.org/</a>) and wondering if there is a decent library for statistical natural language processing tasks.</p>\n",
    "score": 11,
    "creation_date": 1296068315,
    "view_count": 1032,
    "answer_count": 1,
    "tags": "haskell;nlp"
  },
  {
    "question_id": 2644345,
    "title": "Which is better? OpenCyc or ConceptNet?",
    "body": "<p>I'm doing a NLP project where I need to recognise concepts in sentences to find other similar concepts. I do this to infer word valences from a list I already have. I started using WordNet, but it gave many contradictory results. By contradictory results I mean word expansions that had contradictory valences.</p>\n<p>So now I'm looking into ConceptNet and OpenCyc. I've already implemented ConceptNet and it was all very easy and I love it. Problem is that OpenCyc appears to have a much larger and more logically rigid database, which is important when I found so many &quot;contradictions&quot; on WordNet... But I wouldn't know because I haven't tried it.</p>\n<p>Could someone tell me if it's worth going through the (considerable, for me) effort to implement OpenCyc, or is ConceptNet good enough to infer word valences? Are they that different?</p>\n",
    "score": 11,
    "creation_date": 1271326204,
    "view_count": 3278,
    "answer_count": 1,
    "tags": "nlp;wordnet;conceptnet"
  },
  {
    "question_id": 72861962,
    "title": "Using HuggingFace pipeline on pytorch mps device M1 pro",
    "body": "<p>i want to run the pipeline abstract for zero-shot-classification task on the mps device. Here is my code</p>\n<pre><code>pipe = pipeline('zero-shot-classification', device = mps_device)\nseq = &quot;i love watching the office show&quot;\nlabels = ['negative', 'positive']\npipe(seq, labels)\n</code></pre>\n<p>The error generated is</p>\n<pre><code>RuntimeError: Placeholder storage has not been allocated on MPS device!\n</code></pre>\n<p>Which my guess is because seq is on my cpu and not mps. How can i fix this ?\nIs there a way to send seq to the mps device so that i can pass it to the pipe for inference?</p>\n<p>Thanks</p>\n",
    "score": 11,
    "creation_date": 1656968239,
    "view_count": 7845,
    "answer_count": 1,
    "tags": "nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 54411020,
    "title": "Why use cosine similarity in Word2Vec when its trained using dot-product similarity",
    "body": "<p>According to several posts I found on stackoverflow (for instance this <a href=\"https://stackoverflow.com/questions/38423387/why-does-word2vec-use-cosine-similarity\">Why does word2Vec use cosine similarity?</a>), it's common practice to calculate the cosine similarity between two word vectors after we have trained a word2vec (either CBOW or Skip-gram) model. However, this seems a little odd to me since the model is actually trained with dot-product as a similarity score. One evidence of this is that the norm of the word vectors we get after training are actually meaningful. So why is it that people still use cosine-similarity instead of dot-product when calculating the similarity between two words?</p>\n",
    "score": 11,
    "creation_date": 1548713401,
    "view_count": 4807,
    "answer_count": 1,
    "tags": "nlp;word2vec;cosine-similarity;word-embedding;dot-product"
  },
  {
    "question_id": 47649396,
    "title": "Handling \\u200b (Zero width space) character in text preprocessing for NLP task",
    "body": "<p>I'm preprocessing some text for a NER model I'm training, and I'm encountering this character quite a lot. This character is not removed with <code>strip()</code>:</p>\n\n<pre><code>&gt;&gt;&gt; 'Hello world!\\u200b'.strip()\n'Hello world!\\u200b'\n</code></pre>\n\n<p>It is not considered a whitespace for regular expressions:</p>\n\n<pre><code>&gt;&gt;&gt; re.sub('\\s+', ' ', \"hello\\u200bworld!\")\n'hello\\u200bworld!'\n</code></pre>\n\n<p>and spaCy's tokenizer does not split tokens upon it:</p>\n\n<pre><code>&gt;&gt;&gt; [t.text for t in nlp(\"hello\\u200bworld!\")]\n['hello\\u200bworld', '!']\n</code></pre>\n\n<p>So, how should I handle it? I can simply replace it, however I don't want to make a special case for this character, but rather replace all characters with similar characteristics.</p>\n\n<p>Thanks.</p>\n",
    "score": 11,
    "creation_date": 1512463568,
    "view_count": 8640,
    "answer_count": 2,
    "tags": "python;nlp;removing-whitespace;spacy"
  },
  {
    "question_id": 44382941,
    "title": "Loss on masked tensors",
    "body": "<p>Suppose I have logits like </p>\n\n<pre><code>[[4.3, -0.5, -2.7, 0, 0],\n[0.5, 2.3, 0, 0, 0]]\n</code></pre>\n\n<p>where clearly the last two in the first example and last three in the second example are masked (that is, they are zero) and should not affect loss and gradient computations. </p>\n\n<p>How do I compute cross-entropy loss between this logits and corresponding labels? For sanity, the labels for this example can be something like </p>\n\n<pre><code>[[1, 0, 0, 0, 0],\n[0, 1, 0, 0, 0]]\n</code></pre>\n\n<p>(One issue: Softmax, followed by log, on the logits will be applicable for the masked zeroes too and tf's cross-entropy method will consider the loss for those elements too.)</p>\n\n<p>(Also, you can think about the problem like this: I have logits of varying lengths in a batch, i.e. my logits were length 3 and 2 for eg.1 and eg.2 respectively. Same is followed by the labels.)</p>\n",
    "score": 11,
    "creation_date": 1496731069,
    "view_count": 5400,
    "answer_count": 4,
    "tags": "python;tensorflow;machine-learning;nlp"
  },
  {
    "question_id": 37581346,
    "title": "How to deal with length variations for text classification using CNN (Keras)",
    "body": "<p>It has been proved that CNN (convolutional neural network) is quite useful for text/document classification. I wonder how to deal with the length differences as the lengths of articles are different in most cases. Are there any examples in Keras?  Thanks!! </p>\n",
    "score": 11,
    "creation_date": 1464831648,
    "view_count": 7540,
    "answer_count": 4,
    "tags": "nlp;deep-learning;text-classification;keras"
  },
  {
    "question_id": 31148582,
    "title": "How to automatically label a cluster of words using semantics?",
    "body": "<p>The context is : I already have clusters of words (phrases actually) resulting from kmeans applied to internet search queries and using common urls in the results of the search engine as a distance (co-occurrence of urls rather than words if I simplify a lot).</p>\n\n<p>I would like to automatically label the clusters using semantics, in other words I'd like to extract the main concept surrounding a group of phrases considered together. </p>\n\n<p>For example - sorry for the subject of my example - if I have the following bunch of queries : ['my husband attacked me','he was arrested by the police','the trial is still going on','my husband can go to jail for harrassing me ?','free lawyer']\nMy study deals with domestic violence, but clearly this cluster is focused on the legal aspect of the problem so the label could be \"legal\" for example.</p>\n\n<p>I am new to NPL but I have to precise that I don't want to extract words using POS tagging (or at least this is not the expected final outcome but maybe a necessary preliminary step).</p>\n\n<p>I read about Wordnet for sense desambiguation and I think that might be a good track, but I don't want to calculate similarity between two queries (since the clusters are the input) nor obtain the definition of one selected word thanks to the context provided by the whole bunch of words (which word to select in this case ?). I want to use the whole bunch of words to provide a context (maybe using synsets or categorization with the xml structure of the wordnet) and then summarize the context in one or few words.</p>\n\n<p>Any ideas ? I can use R or python, I read a little about nltk but I don't find a way to use it in my context.</p>\n",
    "score": 11,
    "creation_date": 1435698677,
    "view_count": 4423,
    "answer_count": 5,
    "tags": "python;r;nlp;nltk;wordnet"
  },
  {
    "question_id": 30551549,
    "title": "Tabulating characters with diacritics in R",
    "body": "<p>I'm trying to tabulate phones (characters) occurrences in a string, but diacritics are tabulated as characters on their own. Ideally, I have a wordlist in International Phonetic Alphabet, with a fair amount of diacritics and several combinations of them with base characters. I give here a MWE with just one word, but the same goes with list of words and more types of combinations.</p>\n\n<pre><code>&gt; word &lt;- \"n̥ana\" # word constituted by 4 phones: [n̥],[a],[n],[a]\n&gt; table(strsplit(word, \"\"))\n ̥ a n \n1 2 2\n</code></pre>\n\n<p>But the wanted result is:</p>\n\n<pre><code>a n n̥\n2 1 1\n</code></pre>\n\n<p>How can I manage to get this kind of result?</p>\n",
    "score": 11,
    "creation_date": 1433021186,
    "view_count": 233,
    "answer_count": 1,
    "tags": "r;unicode;nlp;linguistics"
  },
  {
    "question_id": 27171298,
    "title": "Nltk stanford pos tagger error : Java command failed",
    "body": "<p>I'm trying to use <a href=\"http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford\" rel=\"nofollow\"><code>nltk.tag.stanford module</code></a>  for tagging a sentence (first like wiki's example) but i keep getting the following error :</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"test.py\", line 28, in &lt;module&gt;\n    print st.tag(word_tokenize('What is the airspeed of an unladen swallow ?'))\n  File \"/usr/local/lib/python2.7/dist-packages/nltk/tag/stanford.py\", line 59, in tag\n    return self.tag_sents([tokens])[0]\n  File \"/usr/local/lib/python2.7/dist-packages/nltk/tag/stanford.py\", line 81, in tag_sents\n    stdout=PIPE, stderr=PIPE)\n  File \"/usr/local/lib/python2.7/dist-packages/nltk/internals.py\", line 160, in java\n    raise OSError('Java command failed!')\nOSError: Java command failed!\n</code></pre>\n\n<p>or following <code>LookupError</code> error :</p>\n\n<pre><code>LookupError: \n\n===========================================================================\nNLTK was unable to find the java file!\nUse software specific configuration paramaters or set the JAVAHOME environment variable.\n===========================================================================\n</code></pre>\n\n<p>this is the exapmle code :</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.tag.stanford import POSTagger\n&gt;&gt;&gt; st = POSTagger('/usr/share/stanford-postagger/models/english-bidirectional-distsim.tagger',\n...                '/usr/share/stanford-postagger/stanford-postagger.jar') \n&gt;&gt;&gt; st.tag('What is the airspeed of an unladen swallow ?'.split()) \n</code></pre>\n\n<p>I also used <code>word_tokenize</code> instead <code>split</code> but it doesn't made any difference.</p>\n\n<p>I also installed java again or <code>jdk</code>! and my all searches were unsuccessful! something like <code>nltknltk.internals.config_java()</code> or ... ! </p>\n\n<p>Note : I use linux (Xubuntu)!   </p>\n",
    "score": 11,
    "creation_date": 1417092871,
    "view_count": 8143,
    "answer_count": 2,
    "tags": "python;nlp;nltk;stanford-nlp;text-processing"
  },
  {
    "question_id": 23172832,
    "title": "Using node.js and natural language processing to handle multiple word phrases",
    "body": "<p>I'm using the very cool <a href=\"https://github.com/NaturalNode/natural\">natural</a> library for node.js.  </p>\n\n<p>I'm trying to train my classifier to match the phrase <code>user experience</code>.  My issue is if I do something like this:</p>\n\n<pre><code>classifier.addDocument(['user experience'], 'ux');\n</code></pre>\n\n<p>It doesn't match 2 word phrases, I believe because it tokenizes the words.  If I do something like this:</p>\n\n<pre><code>classifier.addDocument(['user', 'experience'], 'ux');\n</code></pre>\n\n<p>It works like I want it to, but my issue is, I don't want to just match on the word <code>user</code> because an article could mention include the word <code>user</code> multiple times and it would potentially have nothing to do with user experience, which would lead to inaccurate classifications.  So, my question is how does one match 2 or more word phrases using NLP?    </p>\n\n<p>Thanks for you help in advance. </p>\n",
    "score": 11,
    "creation_date": 1397926504,
    "view_count": 5222,
    "answer_count": 3,
    "tags": "javascript;node.js;nlp"
  },
  {
    "question_id": 745334,
    "title": "How to analyze simple English sentences",
    "body": "<p>Is there any library that can be used for analyzing (nlp) simple english text. For example it would be perfect if it can do that;\nInput: \"I am going\"\nOutput: I, go, present continuous tense</p>\n",
    "score": 11,
    "creation_date": 1239655468,
    "view_count": 1959,
    "answer_count": 4,
    "tags": "java;nlp"
  },
  {
    "question_id": 33374010,
    "title": "Does Word2Vec has a hidden layer?",
    "body": "<p>When I am reading one of papers of Tomas Mikolov: <a href=\"http://arxiv.org/pdf/1301.3781.pdf\" rel=\"nofollow noreferrer\">http://arxiv.org/pdf/1301.3781.pdf</a></p>\n\n<p>I have one concern on the Continuous Bag-of-Words Model section：</p>\n\n<blockquote>\n  <p>The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).</p>\n</blockquote>\n\n<p>I find some people mention that there is a hidden layer in Word2Vec model, but from my understanding, there is only one projection layer in that model. Does this projection layer do the same work as hidden layer?</p>\n\n<p>The another question is that how to project input data into the projection layer? </p>\n\n<p>\"the projection layer is shared for all words (not just the projection matrix)\", what does that mean?</p>\n",
    "score": 11,
    "creation_date": 1445965054,
    "view_count": 3103,
    "answer_count": 1,
    "tags": "machine-learning;nlp;neural-network;word2vec"
  },
  {
    "question_id": 32423369,
    "title": "What&#39;s the difference between WordNet 3.1 and WordNet 3.0?",
    "body": "<p>There doesn't seem to be a changelog or something of that sort available at wordnet.princeton.edu</p>\n",
    "score": 11,
    "creation_date": 1441541179,
    "view_count": 2889,
    "answer_count": 2,
    "tags": "nlp;wordnet"
  },
  {
    "question_id": 17247874,
    "title": "What is the difference between Information Extraction and Text Mining?",
    "body": "<p>It may be looking easy. But I am confused. </p>\n\n<p>What is the difference between Text Mining and Information Extraction ? </p>\n",
    "score": 11,
    "creation_date": 1371881404,
    "view_count": 8520,
    "answer_count": 2,
    "tags": "nlp;information-retrieval;text-mining;information-extraction"
  },
  {
    "question_id": 14011447,
    "title": "Natural Language Processing in Windows 8",
    "body": "<p>I'm a newbie to windows 8 programming, C# and NLP.</p>\n\n<p>I'm looking for a library that allows me to use NLP in windows 8.</p>\n\n<p>I found SharpNLP but it is very poorly documented with no tutorials. I've also come across the Antelope framework but this seems to have even worse documentation.</p>\n\n<p>Is there any resource that'll help me (either tutorials or a better documented framework)?</p>\n",
    "score": 11,
    "creation_date": 1356270363,
    "view_count": 2382,
    "answer_count": 3,
    "tags": "c#;windows-8;nlp"
  },
  {
    "question_id": 2935544,
    "title": "A PHP Library / Class to Count Words in Various Languages?",
    "body": "<p>Some time in the near future I will need to implement a cross-language word count, or if that is not possible, a cross-language character count.</p>\n\n<p>By word count I mean an accurate count of the words contained within the given text, taking the language of the text. The language of the text is set by a user, and will be assumed to be correct.</p>\n\n<p>By character count I mean a count of the \"possibly in a word\" characters contained within the given text, with the same language information described above.</p>\n\n<p>I would much prefer the former count, but I am aware of the difficulties involved. I am also aware that the latter count is much easier, but very much prefer the former, if at all possible.</p>\n\n<p>I'd love it if I just had to look at English, but I need to consider every language here, Chinese, Korean, English, Arabic, Hindi, and so on.</p>\n\n<p>I would like to know if Stack Overflow has any leads on where to start looking for an existing product / method to do this in PHP, as I am a good lazy programmer*</p>\n\n<p><a href=\"http://www.ecpod.com/blog/TEMP/word-count/index.php?string=%E4%B8%80%E4%B8%AA%E6%9C%89%E5%8D%81%E7%9A%84%E5%AD%97%E7%AC%A6%E7%9A%84%E5%8F%A5%E5%AD%90&amp;locale=zh_CN.UTF8\" rel=\"noreferrer\">A simple test</a> showing how str_word_count with set_locale doesn't work, and a function from php.net's str_word_count page.</p>\n\n<p>*<a href=\"http://blogoscoped.com/archive/2005-08-24-n14.html\" rel=\"noreferrer\">http://blogoscoped.com/archive/2005-08-24-n14.html</a></p>\n",
    "score": 11,
    "creation_date": 1275145358,
    "view_count": 3838,
    "answer_count": 3,
    "tags": "php;nlp;utf-8;word-count"
  },
  {
    "question_id": 579203,
    "title": "Binarization in Natural Language Processing",
    "body": "<p>Binarization is the act of transforming colorful features of of an entity into vectors of numbers, most often binary vectors, to make good examples for classifier algorithms.</p>\n\n<p>If we where to binarize the sentence \"The cat ate the dog\", we could start by assigning every word an ID (for example cat-1, ate-2, the-3, dog-4) and then simply replace the word by it's ID giving the vector &lt;3,1,2,3,4>. </p>\n\n<p>Given these IDs we could also create a binary vector by giving each word four possible slots, and setting the slot corresponding to a specific word with to one, giving the vector &lt;0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1>. The latter method is, as far as I know, is commonly referred to as the bag-of-words-method.</p>\n\n<p>Now for my question, what is the <i>best</i> binarization method when it comes to describe features for natural language processing in general, and transition-based <i>dependency parsing</i> (with Nivres algorithm) in particular? </p>\n\n<p>In this context, we do not want to encode the whole sentence, but rather the current state of the parse, for example the top word on the stack en the first word in the input queue. Since order is highly relevant, this rules out the bag-of-words-method. </p>\n\n<p>With <i>best</i>, I am referring to the method that makes the data the most intelligible for the classifier, without using up unnecessary memory. For example I don't want a word bigram to use 400 million features for 20000 unique words, if only 2% the bigrams actually exist.</p>\n\n<p>Since the answer is also depending on the particular classifier, I am mostly interested in maximum entropy models (liblinear), support vector machines (libsvm) and perceptrons, but answers that apply to other models are also welcome.</p>\n",
    "score": 11,
    "creation_date": 1235421115,
    "view_count": 3328,
    "answer_count": 3,
    "tags": "machine-learning;nlp;classification;libsvm"
  },
  {
    "question_id": 58620282,
    "title": "Updating a BERT model through Huggingface transformers",
    "body": "<p>I am attempting to update the pre-trained BERT model using an in house corpus. I have looked at the Huggingface transformer docs and I am a little stuck as you will see below.My goal is to compute simple similarities between sentences using the cosine distance but I need to update the pre-trained model for my specific use case. </p>\n\n<p>If you look at the code below, which is precisely from the Huggingface docs. I am attempting to \"retrain\" or update the model and I assumed that special_token_1 and special_token_2 represent \"new sentences\" from my \"in house\" data or corpus. Is this correct? In summary, I like the already pre-trained BERT model but I would like to update it or retrain it using another in house dataset. Any leads will be appreciated. </p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_datasets\nfrom transformers import *\n\nmodel = BertModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nSPECIAL_TOKEN_1=\"dogs are very cute\"\nSPECIAL_TOKEN_2=\"dogs are cute but i like cats better and my \nbrother thinks they are more cute\"\n\ntokenizer.add_tokens([SPECIAL_TOKEN_1, SPECIAL_TOKEN_2])\nmodel.resize_token_embeddings(len(tokenizer))\n#Train our model\nmodel.train()\nmodel.eval()\n</code></pre>\n",
    "score": 11,
    "creation_date": 1572419964,
    "view_count": 4074,
    "answer_count": 1,
    "tags": "tensorflow;nlp;pytorch;spacy;huggingface-transformers"
  },
  {
    "question_id": 40244101,
    "title": "Implement word2vec in Keras",
    "body": "<p>I would like to implement word2vec algorithm in keras, Is this possible? \nHow can I fit the model? Should I use custom loss function?</p>\n",
    "score": 11,
    "creation_date": 1477409718,
    "view_count": 9665,
    "answer_count": 1,
    "tags": "nlp;deep-learning;keras;theano;word2vec"
  },
  {
    "question_id": 35103191,
    "title": "NLTK ViterbiParser fails in parsing words that are not in the PCFG rule",
    "body": "<pre><code>import nltk\nfrom nltk.parse import ViterbiParser\n\ndef pcfg_chartparser(grammarfile):\n    f=open(grammarfile)\n    grammar=f.read()\n    f.close()\n    return nltk.PCFG.fromstring(grammar)\n\ngrammarp = pcfg_chartparser(\"wsjp.cfg\")\n\nVP = ViterbiParser(grammarp)\nprint VP\nfor w in sent:\n    for tree in VP.parse(nltk.word_tokenize(w)):\n        print tree\n</code></pre>\n\n<p>When I run the above code, it produces the following output for the sentence, \"turn off the lights\"-  </p>\n\n<blockquote>\n  <p>(S\n    (VP (VB turn) (PRT (RP off)) (NP (DT the) (NNS lights)))) (p=2.53851e-14)</p>\n</blockquote>\n\n<p>However, it raises the following error for the sentence, \"please turn off the lights\"-  </p>\n\n<blockquote>\n  <p>ValueError: Grammar does not cover some of the input words: u\"'please'\"</p>\n</blockquote>\n\n<p>I am building a ViterbiParser by supplying it a probabilistic context free grammar. It works well in parsing sentences that have words which are already in the rules of the grammar. It fails to parse sentences in which the Parser has not seen the word in the grammar rules. How to get around this limitation?<br>\nI am referring to this <a href=\"http://web.mit.edu/6.863/www/fall2012/labs/assignment5.pdf\">assignment</a>.</p>\n",
    "score": 11,
    "creation_date": 1454165449,
    "view_count": 2376,
    "answer_count": 1,
    "tags": "python;nlp;nltk;context-free-grammar;viterbi"
  },
  {
    "question_id": 7036902,
    "title": "Convert plural nouns into singular nouns",
    "body": "<p>How can plural nouns be converted into singular nouns using R? I use the the tagPOS function which tags each text and then extract all of plural nouns which were tagged as \"NNS\". But what to do in case I want to convert those plural nouns into singular ones.?</p>\n\n<hr>\n\n<pre><code>library(\"openNLP\")\nlibrary(\"tm\")\nacq_o &lt;- \"Gulf Applied Technologies Inc said it sold its subsidiaries engaged in pipelines and terminal operations for 12.2 mln dlrs. The company said the sale is subject to certain post closing adjustments, which it did not explain. Reuter.\"\n\nacq = tm_map(Corpus(DataframeSource(data.frame(acq_o))), removePunctuation)\nacqTag &lt;- tagPOS(acq)\nacqTagSplit = strsplit(acqTag,\" \")\nqq = 0\ntag = 0\nfor (i in 1:length(acqTagSplit[[1]])){\n        qq[i] &lt;-strsplit(acqTagSplit[[1]][i],'/')\n        tag[i] = qq[i][[1]][2]\n}\n\nindex = 0\nk = 0\nfor (i in 1:(length(acqTagSplit[[1]]))) { \n    if (tag[i] == \"NNS\"){\n        k = k +1             \n        index[k] = i     \n    } \n}\nindex\n</code></pre>\n",
    "score": 11,
    "creation_date": 1313134199,
    "view_count": 6713,
    "answer_count": 1,
    "tags": "r;nlp"
  },
  {
    "question_id": 58362316,
    "title": "How do I go from Pandas DataFrame to Tensorflow BatchDataset for NLP?",
    "body": "<p>I'm honestly trying to figure out how to convert a dataset (format: pandas <code>DataFrame</code> or numpy array) to a form that a simple text-classification tensorflow model can train on for sentiment analysis. The dataset I'm using is similar to IMDB (containing both text and labels (positive or negative)). Every tutorial I've looked at has either prepared data differently, or didn't bother with data preparation and left it to your imagination. (For instance, all the IMDB tutorials import a preprocessed Tensorflow <code>BatchDataset</code> from <code>tensorflow_datasets</code>, which isn't helpful when I'm using my own set of data). My own attempts to convert a Pandas <code>DataFrame</code> to Tensorflow's <code>Dataset</code> types have resulted in ValueErrors or a negative loss during training. Any help would be appreciated.</p>\n\n<p>I had originally prepared my data as follows, where <code>training</code> and <code>validation</code> are already shuffled Pandas <code>DataFrame</code>s containing <code>text</code> and <code>label</code> columns:</p>\n\n<pre><code># IMPORT STUFF\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf # (I'm using tensorflow 2.0)\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport pandas as pd\nimport numpy as np\n# ... [code for importing and preparing the pandas dataframe omitted]\n\n# TOKENIZE\n\ntrain_text = training['text'].to_numpy()\ntok = Tokenizer(oov_token='&lt;unk&gt;')\ntok.fit_on_texts(train_text)\ntok.word_index['&lt;pad&gt;'] = 0\ntok.index_word[0] = '&lt;pad&gt;'\n\ntrain_seqs = tok.texts_to_sequences(train_text)\ntrain_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n\ntrain_labels = training['label'].to_numpy().flatten()\n\nvalid_text = validation['text'].to_numpy()\nvalid_seqs = tok.texts_to_sequences(valid_text)\nvalid_seqs = tf.keras.preprocessing.sequence.pad_sequences(valid_seqs, padding='post')\n\nvalid_labels = validation['label'].to_numpy().flatten()\n\n# CONVERT TO TF DATASETS\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_seqs,train_labels))\nvalid_ds = tf.data.Dataset.from_tensor_slices((valid_seqs,valid_labels))\n\ntrain_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\nvalid_ds = valid_ds.batch(BATCH_SIZE)\n\n# PREFETCH\n\ntrain_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nvalid_ds = valid_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n</code></pre>\n\n<p>This resulted train_ds and valid_ds being tokenized and of type <code>PrefetchDataset</code> or <code>&lt;PrefetchDataset shapes: ((None, None, None, 118), (None, None, None)), types: (tf.int32, tf.int64)&gt;</code>.</p>\n\n<p>I then trained as follows, but got a <em>large negative loss and an accuracy of 0</em>.</p>\n\n<pre><code>model = keras.Sequential([\n    layers.Embedding(vocab_size, embedding_dim),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(1, activation='sigmoid') # also tried activation='softmax'\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy', # binary_crossentropy\n              metrics=['accuracy'])\n\nhistory = model.fit(\n    train_ds,\n    epochs=1,\n    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)\n</code></pre>\n\n<p>If I don't do the fancy prefetch stuff, <code>train_ds</code> would be of type <code>BatchDataset</code> or <code>&lt;BatchDataset shapes: ((None, 118), (None,)), types: (tf.int32, tf.int64)&gt;</code>, but that also is getting me a negative loss and an accuracy of 0.</p>\n\n<p>And if I just do the following: </p>\n\n<pre><code>x, y = training['text'].to_numpy(), training['label'].to_numpy()\nx, y = tf.convert_to_tensor(x),tf.convert_to_tensor(y)\n</code></pre>\n\n<p>then <code>x</code> and <code>y</code> are of type <code>EagerTensor</code>, but I can't seem to figure out how to Batch an <code>EagerTensor</code>.</p>\n\n<p><strong>What types and shapes do I really need for <code>train_ds</code>? What am I missing or doing wrong?</strong> </p>\n\n<p>The <a href=\"https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\" rel=\"noreferrer\">text_classification_with_hub tutorial</a> trains an already prepared imdb dataset as shown:</p>\n\n<pre><code>model = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=20,\n                    validation_data=validation_data.batch(512),\n                    verbose=1)\n</code></pre>\n\n<p>In this example, <code>train_data</code> is of form <code>tensorflow.python.data.ops.dataset_ops._OptionsDataset</code>, and <code>train_data.shuffle(1000).batch(512)</code> is <code>tensorflow.python.data.ops.dataset_ops.BatchDataset</code> (or <code>&lt;BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int64)&gt;</code>). </p>\n\n<p>They apparently didn't bother with tokenization with this dataset, but I doubt tokenization is my issue. Why does their <code>train_data.shuffle(10000).batch(512)</code> work but my <code>train_ds</code> not work? </p>\n\n<p>It's possible the issue is with the model setup, the <code>Embedding</code> layer, or with tokenization, but I'm not so sure that's the case. I've already looked at the following tutorials for inspiration:</p>\n\n<p><a href=\"https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\" rel=\"noreferrer\">https://www.tensorflow.org/tutorials/keras/text_classification_with_hub</a></p>\n\n<p><a href=\"https://www.kaggle.com/drscarlat/imdb-sentiment-analysis-keras-and-tensorflow\" rel=\"noreferrer\">https://www.kaggle.com/drscarlat/imdb-sentiment-analysis-keras-and-tensorflow</a></p>\n\n<p><a href=\"https://www.tensorflow.org/tutorials/text/image_captioning\" rel=\"noreferrer\">https://www.tensorflow.org/tutorials/text/image_captioning</a></p>\n\n<p><a href=\"https://www.tensorflow.org/tutorials/text/word_embeddings#learning_embeddings_from_scratch\" rel=\"noreferrer\">https://www.tensorflow.org/tutorials/text/word_embeddings#learning_embeddings_from_scratch</a></p>\n\n<p><a href=\"https://thedatafrog.com/word-embedding-sentiment-analysis/\" rel=\"noreferrer\">https://thedatafrog.com/word-embedding-sentiment-analysis/</a></p>\n",
    "score": 11,
    "creation_date": 1570959504,
    "view_count": 5002,
    "answer_count": 1,
    "tags": "tensorflow;keras;deep-learning;nlp;tensorflow-datasets"
  },
  {
    "question_id": 36764191,
    "title": "Extracting one-hot vector from text",
    "body": "<p>In <code>pandas</code> or <code>numpy</code>, I can do the following to get one-hot vectors:</p>\n\n<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; x = [0,2,1,4,3]\n&gt;&gt;&gt; pd.get_dummies(x).values\narray([[ 1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.,  0.]])\n\n&gt;&gt;&gt; np.eye(len(set(x)))[x]\narray([[ 1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.,  0.]])\n</code></pre>\n\n<p>From text, with <code>gensim</code>, I can do:</p>\n\n<pre><code>&gt;&gt;&gt; from gensim.corpora import Dictionary\n&gt;&gt;&gt; sent1 = 'this is a foo bar sentence .'.split()\n&gt;&gt;&gt; sent2 = 'this is another foo bar sentence .'.split()\n&gt;&gt;&gt; texts = [sent1, sent2]\n&gt;&gt;&gt; vocab = Dictionary(texts)\n&gt;&gt;&gt; [[vocab.token2id[word] for word in sent] for sent in texts]\n[[3, 4, 0, 6, 1, 2, 5], [3, 4, 7, 6, 1, 2, 5]]\n</code></pre>\n\n<p>Then I'll have to do the same <code>pd.get_dummies</code> or <code>np.eyes</code> to get the one-hot vector but I get an error where there's one dimension missing from my one-hot vector I have 8 unique words but the one-hot vector lengths are only 7:</p>\n\n<pre><code>&gt;&gt;&gt; [pd.get_dummies(sent).values for sent in texts_idx]\n[array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.]]), array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.],\n       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.]])]\n</code></pre>\n\n<p>It seems like it's doing one-hot vector individually as it iterates through each sentence, instead of using the global vocabulary.</p>\n\n<p>Using <code>np.eye</code>, I do get the right vectors:</p>\n\n<pre><code>&gt;&gt;&gt; [np.eye(len(vocab))[sent] for sent in texts_idx]\n[array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]]), array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]])]\n</code></pre>\n\n<p>Also, currently, I have to do several things from using <code>gensim.corpora.Dictionary</code> to converting the words to their ids then getting the one-hot vector.</p>\n\n<p><strong>Are there other ways to achieve the same one-hot vector from texts?</strong></p>\n",
    "score": 11,
    "creation_date": 1461227989,
    "view_count": 3933,
    "answer_count": 3,
    "tags": "python;numpy;pandas;vector;nlp"
  },
  {
    "question_id": 30460713,
    "title": "Parsing multiple sentences with MaltParser using NLTK",
    "body": "<p>There have been many MaltParser and/or NLTK related questions:</p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/20091698/malt-parser-throwing-class-not-found-exception\">Malt Parser throwing class not found exception</a></li>\n<li><a href=\"https://stackoverflow.com/questions/14009330/how-to-use-malt-parser-in-python-nltk\">How to use malt parser in python nltk</a></li>\n<li><a href=\"https://stackoverflow.com/questions/29513187/maltparser-not-working-in-python-nltk\">MaltParser Not Working in Python NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/9524553/nltk-maltparser-wont-parse\">NLTK MaltParser won&#39;t parse</a></li>\n<li><a href=\"https://stackoverflow.com/questions/21815891/dependency-parser-using-nltk-and-maltparser\">Dependency parser using NLTK and MaltParser</a></li>\n<li><a href=\"https://stackoverflow.com/questions/23463519/dependency-parsing-using-maltparser-and-nltk\">Dependency Parsing using MaltParser and NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/3944269/parsing-with-maltparser-engmalt\">Parsing with MaltParser engmalt</a></li>\n<li><a href=\"https://stackoverflow.com/questions/17392790/parse-raw-text-with-maltparser-in-java\">Parse raw text with MaltParser in Java</a></li>\n</ul>\n\n<p>Now, there's a more stabilized version of MaltParser API in NLTK: <a href=\"https://github.com/nltk/nltk/pull/944\" rel=\"nofollow noreferrer\">https://github.com/nltk/nltk/pull/944</a> but there are issues when it comes to parsing multiple sentences at the same time. </p>\n\n<p>Parsing one sentence at a time seems fine:</p>\n\n<pre><code>_path_to_maltparser = '/home/alvas/maltparser-1.8/dist/maltparser-1.8/'\n_path_to_model= '/home/alvas/engmalt.linear-1.7.mco'     \n&gt;&gt;&gt; mp = MaltParser(path_to_maltparser=_path_to_maltparser, model=_path_to_model)\n&gt;&gt;&gt; sent = 'I shot an elephant in my pajamas'.split()\n&gt;&gt;&gt; sent2 = 'Time flies like banana'.split()\n&gt;&gt;&gt; print(mp.parse_one(sent).tree())\n(pajamas (shot I) an elephant in my)\n</code></pre>\n\n<p>But parsing a list of sentences doesn't return a DependencyGraph object:</p>\n\n<pre><code>_path_to_maltparser = '/home/alvas/maltparser-1.8/dist/maltparser-1.8/'\n_path_to_model= '/home/alvas/engmalt.linear-1.7.mco'     \n&gt;&gt;&gt; mp = MaltParser(path_to_maltparser=_path_to_maltparser, model=_path_to_model)\n&gt;&gt;&gt; sent = 'I shot an elephant in my pajamas'.split()\n&gt;&gt;&gt; sent2 = 'Time flies like banana'.split()\n&gt;&gt;&gt; print(mp.parse_one(sent).tree())\n(pajamas (shot I) an elephant in my)\n&gt;&gt;&gt; print(next(mp.parse_sents([sent,sent2])))\n&lt;listiterator object at 0x7f0a2e4d3d90&gt; \n&gt;&gt;&gt; print(next(next(mp.parse_sents([sent,sent2]))))\n[{u'address': 0,\n  u'ctag': u'TOP',\n  u'deps': [2],\n  u'feats': None,\n  u'lemma': None,\n  u'rel': u'TOP',\n  u'tag': u'TOP',\n  u'word': None},\n {u'address': 1,\n  u'ctag': u'NN',\n  u'deps': [],\n  u'feats': u'_',\n  u'head': 2,\n  u'lemma': u'_',\n  u'rel': u'nn',\n  u'tag': u'NN',\n  u'word': u'I'},\n {u'address': 2,\n  u'ctag': u'NN',\n  u'deps': [1, 11],\n  u'feats': u'_',\n  u'head': 0,\n  u'lemma': u'_',\n  u'rel': u'null',\n  u'tag': u'NN',\n  u'word': u'shot'},\n {u'address': 3,\n  u'ctag': u'AT',\n  u'deps': [],\n  u'feats': u'_',\n  u'head': 11,\n  u'lemma': u'_',\n  u'rel': u'nn',\n  u'tag': u'AT',\n  u'word': u'an'},\n {u'address': 4,\n  u'ctag': u'NN',\n  u'deps': [],\n  u'feats': u'_',\n  u'head': 11,\n  u'lemma': u'_',\n  u'rel': u'nn',\n  u'tag': u'NN',\n  u'word': u'elephant'},\n {u'address': 5,\n  u'ctag': u'NN',\n  u'deps': [],\n  u'feats': u'_',\n  u'head': 11,\n  u'lemma': u'_',\n  u'rel': u'nn',\n  u'tag': u'NN',\n  u'word': u'in'},\n {u'address': 6,\n  u'ctag': u'NN',\n  u'deps': [],\n  u'feats': u'_',\n  u'head': 11,\n  u'lemma': u'_',\n  u'rel': u'nn',\n  u'tag': u'NN',\n  u'word': u'my'},\n {u'address': 7,\n  u'ctag': u'NNS',\n  u'deps': [],\n  u'feats': u'_',\n  u'head': 11,\n  u'lemma': u'_',\n  u'rel': u'nn',\n  u'tag': u'NNS',\n  u'word': u'pajamas'},\n {u'address': 8,\n  u'ctag': u'NN',\n  u'deps': [],\n  u'feats': u'_',\n  u'head': 11,\n  u'lemma': u'_',\n  u'rel': u'nn',\n  u'tag': u'NN',\n  u'word': u'Time'},\n {u'address': 9,\n  u'ctag': u'NNS',\n  u'deps': [],\n  u'feats': u'_',\n  u'head': 11,\n  u'lemma': u'_',\n  u'rel': u'nn',\n  u'tag': u'NNS',\n  u'word': u'flies'},\n {u'address': 10,\n  u'ctag': u'NN',\n  u'deps': [],\n  u'feats': u'_',\n  u'head': 11,\n  u'lemma': u'_',\n  u'rel': u'nn',\n  u'tag': u'NN',\n  u'word': u'like'},\n {u'address': 11,\n  u'ctag': u'NN',\n  u'deps': [3, 4, 5, 6, 7, 8, 9, 10],\n  u'feats': u'_',\n  u'head': 2,\n  u'lemma': u'_',\n  u'rel': u'dep',\n  u'tag': u'NN',\n  u'word': u'banana'}]\n</code></pre>\n\n<p><strong>Why is that using <code>parse_sents()</code> don't return an iterable of <code>parse_one</code>?</strong></p>\n\n<p>I could however, just get lazy and do:</p>\n\n<pre><code>_path_to_maltparser = '/home/alvas/maltparser-1.8/dist/maltparser-1.8/'\n_path_to_model= '/home/alvas/engmalt.linear-1.7.mco'     \n&gt;&gt;&gt; mp = MaltParser(path_to_maltparser=_path_to_maltparser, model=_path_to_model)\n&gt;&gt;&gt; sent1 = 'I shot an elephant in my pajamas'.split()\n&gt;&gt;&gt; sent2 = 'Time flies like banana'.split()\n&gt;&gt;&gt; sentences = [sent1, sent2]\n&gt;&gt;&gt; for sent in sentences:\n&gt;&gt;&gt; ...    print(mp.parse_one(sent).tree())\n</code></pre>\n\n<p>But this is not the solution I'm looking for. <strong>My question is how to answer why doesn't the <code>parse_sent()</code> return an iterable of <code>parse_one()</code>. and how could it be fixed in the NLTK code?</strong></p>\n\n<hr>\n\n<p>After @NikitaAstrakhantsev answered, I've tried it outputs a parse tree now but it seems to be confused and puts both sentences into one before parsing it.</p>\n\n<pre><code># Initialize a MaltParser object with a pre-trained model.\nmp = MaltParser(path_to_maltparser=path_to_maltparser, model=path_to_model) \nsent = 'I shot an elephant in my pajamas'.split()\nsent2 = 'Time flies like banana'.split()\n# Parse a single sentence.\nprint(mp.parse_one(sent).tree())\nprint(next(next(mp.parse_sents([sent,sent2]))).tree())\n</code></pre>\n\n<p>[out]:</p>\n\n<pre><code>(pajamas (shot I) an elephant in my)\n(shot I (banana an elephant in my pajamas Time flies like))\n</code></pre>\n\n<p>From the code it seems to be doing something weird: <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/parse/api.py#L45\" rel=\"nofollow noreferrer\">https://github.com/nltk/nltk/blob/develop/nltk/parse/api.py#L45</a></p>\n\n<p><strong>Why is it that the parser abstract class in NLTK is swooshing two sentences into one before parsing? Am I calling the <code>parse_sents()</code> incorrectly? If so, what is the correct way to call <code>parse_sents()</code>?</strong> </p>\n",
    "score": 11,
    "creation_date": 1432648713,
    "view_count": 1391,
    "answer_count": 1,
    "tags": "java;python;parsing;nlp;nltk"
  },
  {
    "question_id": 14673902,
    "title": "Is there a C# utility for matching patterns in (syntactic parse) trees?",
    "body": "<p>I'm working on a Natural Language Processing (NLP) project in which I use a syntactic parser to create a syntactic parse tree out of a given sentence.</p>\n\n<p><strong><em>Example Input:</em></strong> I ran into Joe and Jill and then we went shopping<br>\n<strong><em>Example Output:</em></strong> [TOP [S [S [NP [PRP I]] [VP [VBD ran] [PP [IN into] [NP [NNP Joe] [CC and] [NNP Jill]]]]] [CC and] [S [ADVP [RB then]] [NP [PRP we]] [VP [VBD went] [NP [NN shopping]]]]]]\n<img src=\"https://i.sstatic.net/EedCb.png\" alt=\"enter image description here\"></p>\n\n<p>I'm looking for a C# utility that will let me do complex queries like:</p>\n\n<ul>\n<li>Get the first VBD related to 'Joe'</li>\n<li>Get the NP closest to 'Shopping'</li>\n</ul>\n\n<p>Here's a <a href=\"http://nlp.stanford.edu/software/tregex.shtml\" rel=\"noreferrer\">Java utility</a> that does this, I'm looking for a C# equivalent.<br>\nAny help would be much appreciated.</p>\n",
    "score": 11,
    "creation_date": 1359904265,
    "view_count": 1851,
    "answer_count": 2,
    "tags": "c#;tree;nlp;stanford-nlp;s-expression"
  },
  {
    "question_id": 11408557,
    "title": "How to search a String in Class in c#",
    "body": "<p>I am developing an app in which i have some data fetched from net into a class.\nClass is</p>\n\n<pre><code>public class Detail\n{\n        public string name { get; set; }\n        public List&lt;Education&gt; education { get; set; }\n        public City city { get; set; }\n        public List&lt;Work&gt; work { get; set; }\n}\n\npublic class Education\n{\n        public string DegreeName { get; set; }\n}\n\npublic class City \n    {\n        public string name { get; set; }\n    }\npublic class Work\n    {\n        public string name { get; set; }\n    }\n</code></pre>\n\n<p>Data is stored for a person in the above class.</p>\n\n<p>Now i want to search for a string say <code>q=\" Which Manager Graduated From USA ?\"</code></p>\n\n<p>So i want it to search for the above query...</p>\n\n<p>Based on how much words matched, i want to give the Name of user. So searching for person if he is <em>a Manager Graduated From USA ?</em> (may be less words, for search like some <em>Director from India</em>)</p>\n\n<p>The approach i am trying to look for words like <em>Manager</em> in <code>Work</code> and <em>Graduate</em> in <code>Education</code> and <em>Location</em> for <code>USA</code> </p>\n\n<p>I am making an array of search string</p>\n\n<pre><code>string[] qList = q.Split(' ');\n</code></pre>\n\n<p>and then traverse through the class. But i don't have any idea of how to (efficiently) look for data in the class.</p>\n\n<p>And is my approach good enough for search or is there any better option ?</p>\n",
    "score": 11,
    "creation_date": 1341905431,
    "view_count": 2531,
    "answer_count": 4,
    "tags": "c#;asp.net;asp.net-mvc;string;nlp"
  },
  {
    "question_id": 8541447,
    "title": "Some NLP stuff to do with grammar, tagging, stemming, and word sense disambiguation in Python",
    "body": "<h1>Background (TLDR; provided for the sake of completion)</h1>\n<p><strong>Seeking advice on an optimal solution to an odd requirement.</strong>\nI'm a (literature) student in my fourth year of college with only my own guidance in programming. I'm competent enough with Python that I won't have trouble implementing solutions I find (most of the time) and developing upon them, but because of my newbness, I'm seeking advice on the <em>best</em> ways I might tackle this peculiar problem.</p>\n<p><strong>Already using NLTK, but differently from the examples in the NLTK book.</strong> I'm already utilizing a lot of stuff from NLTK, particularly WordNet, so that material is not foreign to me. I've read most of the NLTK book.</p>\n<p><strong>I'm working with fragmentary, atomic language.</strong> Users input words and sentence fragments, and WordNet is used to find connections between the inputs, and generate new words and sentences/fragments. My question is about turning an uninflected word from WordNet (a synset) into something that makes sense contextually.</p>\n<p><strong>The problem: How to inflect the result in a grammatically sensible way?</strong> Without any kind of grammatical processing, the results are just a list of dictionary-searchable words, without agreement between words. First step is for my application to stem/pluralize/conjugate/inflect root-words according to context. (The &quot;root words&quot; I'm speaking of are synsets from WordNet and/or their human-readable equivalents.)</p>\n<h1>Example scenario</h1>\n<p>Let's assume we have a chunk of a poem, to which users are adding new inputs to. The new results need to be inflected in a grammatically sensible way.</p>\n<pre><code>The river bears no empty bottles, sandwich papers,   \nSilk handkerchiefs, cardboard boxes, cigarette ends  \nOr other testimony of summer nights. The sprites\n</code></pre>\n<p>Let's say now, it needs to print 1 of 4 possible next words/synsets: <code>['departure', 'to have', 'blue', 'quick']</code>. It seems to me that <code>'blue'</code> should be discarded; <code>'The sprites blue'</code> seems grammatically odd/unlikely. From there it could use either of these verbs.</p>\n<p>If it picks <code>'to have'</code> the result could be sensibly inflected as <code>'had'</code>, <code>'have'</code>, <code>'having'</code>, <code>'will have'</code>, <code>'would have'</code>, etc. (but not <code>'has'</code>). (The resulting line would be something like <code>'The sprites have'</code> and the sensibly-inflected result will provide better context for future results ...)</p>\n<p>I'd like for <code>'depature'</code> to be a valid possibility in this case; while <code>'The sprites departure'</code> doesn't make sense (it's not <code>&quot;sprites'&quot;</code>), <code>'The sprites departed'</code> (or other <em>verb</em> conjugations) would.</p>\n<p>Seemingly <code>'The sprites quick'</code> wouldn't make sense, but something like <code>'The sprites quickly [...]'</code> or <code>'The sprites quicken'</code> could, so <code>'quick'</code> is also a possibility for sensible inflection.</p>\n<h2>Breaking down the tasks</h2>\n<ol>\n<li><strong>Tag part of speech, plurality, tense, etc.</strong> -- of original inputs. Taking note of this could help to select from the several possibilities (i.e. choosing between had/have/having could be more directed than random if a user had inputted <code>'having'</code> rather than some other tense). I've heard the Stanford POS tagger is good, which has an implementation in NLTK. I am not sure how to handle tense detection here.</li>\n<li><strong>Consider context in order to rule out grammatically peculiar possibilities.</strong> Consider the last couple words and their part-of-speech tags (and tense?), as well as sentence boundaries if any, and from that, drop things that wouldn't make sense. After <code>'The sprites'</code> we don't want another article (or determiner, as far as I can tell), nor an adjective, but an adverb or verb could work. Comparison of the current stuff with sequences in tagged corpora (and/or Markov chains?) -- or consultation of grammar-checking functions -- could provide a solution for this.</li>\n<li><strong>Select a word from the remaining possibilities</strong> (those that could be inflected sensibly). This isn't something I need an answer for -- I've got my methods for this. Let's say it's randomly selected.</li>\n<li><strong>Transform the selected word as needed</strong>. If the information from #1 can be folded in (for example, perhaps the &quot;pluralize&quot; flag was set to True), do so. If there are several possibilities (e.g. picked word is a verb, but a few tenses are possible) select, randomly. Regardless I'm going to need to morph/inflect the word.</li>\n</ol>\n<p>I'm looking for advice on the soundness of this routine, as well as suggestions for steps to add. Ways to break down these steps further would also be helpful. Finally I'm looking for suggestions on what tool might best accomplish each task.</p>\n",
    "score": 11,
    "creation_date": 1324078629,
    "view_count": 2735,
    "answer_count": 1,
    "tags": "python;nlp;grammar;tagging;nltk"
  },
  {
    "question_id": 4698279,
    "title": "Generate a pseudo-poem that would contain 160 bits of recoverable information",
    "body": "<p>I have 160 bits of random data.</p>\n<p>Just for fun, I want to generate an English pseudo-poem to &quot;store&quot; this information in. I want to be able to recover this information from the poem. (&quot;Poem&quot; here is a vague term for any kind of poetry.)</p>\n<p><em><strong>Note:</strong> This is not a security question, I don't care if someone else will be able to recover the information or even detect that it is there or not.</em></p>\n<p>Criteria for a better poem:</p>\n<ul>\n<li>Better aestetics</li>\n<li>Better rhyme and foot</li>\n<li>Uniqueness</li>\n<li>Shorter length</li>\n</ul>\n<p>I'd say that the acceptable poem is no longer than three stanzas of four lines each. (But the other, established forms of poetry, like sonnets are good as well.)</p>\n<p>I like this idea, but, I'm afraid, that I'm completely clueless in how to do English computer-generated poetry. (I programmed that for Russian when I was young, but looks like that experience will not help me here.)</p>\n<p>So, any clues?</p>\n<p><em><strong>Note:</strong> I already <a href=\"https://stackoverflow.com/q/4698229/6236\">asked a similar question</a>. I want to try both approaches. Note how good poem criteria are different from the good phrase in parallel question. Remember, this is &quot;just for fun&quot;.</em></p>\n<p><em>Also, I have to note this: There is an <a href=\"https://www.rfc-editor.org/rfc/rfc1605\" rel=\"nofollow noreferrer\">RFC 1605</a> on somewhat related matters. But it do not suggest any implementation details, so it is not quite useful for me, sorry. &lt;g&gt;</em></p>\n",
    "score": 11,
    "creation_date": 1295071594,
    "view_count": 388,
    "answer_count": 2,
    "tags": "nlp;steganography"
  },
  {
    "question_id": 23529123,
    "title": "How to determine the language(English, Chinese...) of a given string in Oracle?",
    "body": "<p>How to determine the language (English, Chinese...) of a given sting (table column value) in Oracle(multi language environment)?</p>\n",
    "score": 11,
    "creation_date": 1399499392,
    "view_count": 4587,
    "answer_count": 4,
    "tags": "java;oracle-database;plsql;nlp"
  },
  {
    "question_id": 20953495,
    "title": "Is there a good stemmer for Hebrew?",
    "body": "<p>I am looking for a good stemmer for Hebrew - I found nothing at all using Google...</p>\n\n<p>On the <a href=\"http://code972.com/blog/2010/06/19-finding-hebrew-lemmas-hebmorph-part-2\" rel=\"noreferrer\">HebMorph site</a> it says that:</p>\n\n<p><code>Stem and Lemma originally have different meanings, but for Semitic languages they seem to be used interchangeably.</code></p>\n\n<p>Does that mean that for NLP purposes, I could use lemmas instead of stems? Keeping in mind that: <code>Stemmers are much simpler, smaller and usually faster then lemmatizers, and for many applications their results are good enough. Using a lemmatizer for that is a waste of resources.</code> (<a href=\"https://stackoverflow.com/questions/17317418/stemmers-vs-lemmatizers\">source</a> )</p>\n\n<p>Thank you.</p>\n",
    "score": 11,
    "creation_date": 1389022787,
    "view_count": 4661,
    "answer_count": 1,
    "tags": "nlp;hebrew;stemming;lemmatization"
  },
  {
    "question_id": 59867929,
    "title": "InvalidArgumentError: 2 root error(s) found. Incompatible shapes in Tensorflow text-classification model",
    "body": "<p>I am trying to get code working from the following <a href=\"https://github.com/YanWenqiang/HBLSTM-CRF/blob/master/HBLSTM-CRF.py\" rel=\"noreferrer\">repo</a>, which is based off this <a href=\"https://arxiv.org/pdf/1709.04250.pdf\" rel=\"noreferrer\">paper</a>. It had a lot of errors, but I mostly got it working. However, I keep getting the same problem and I really do not understand how to troubleshoot this/what is even going wrong. </p>\n\n<p>The error occurs the second time the validation if statement critera is met. The first time is always works, then breaks on the second. I'm including the output it prints before breaking if its helpful. See error below:</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>step = 1, train_loss = 1204.7784423828125, train_accuracy = 0.13725490868091583\r\ncounter = 1, dev_loss = 1188.6639287274584, dev_accuacy = 0.2814199453625912\r\nstep = 2, train_loss = 1000.983154296875, train_accuracy = 0.26249998807907104\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-&gt; 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n7 frames\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: Incompatible shapes: [2,185] vs. [2,229]\r\n\t [[{{node loss/cond/add_1}}]]\r\n\t [[viterbi_decode/cond/rnn_1/while/Switch_3/_541]]\r\n  (1) Invalid argument: Incompatible shapes: [2,185] vs. [2,229]\r\n\t [[{{node loss/cond/add_1}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-&gt; 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: Incompatible shapes: [2,185] vs. [2,229]\r\n\t [[node loss/cond/add_1 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n\t [[viterbi_decode/cond/rnn_1/while/Switch_3/_541]]\r\n  (1) Invalid argument: Incompatible shapes: [2,185] vs. [2,229]\r\n\t [[node loss/cond/add_1 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'loss/cond/add_1':\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in &lt;module&gt;\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"&lt;ipython-input-11-90859dc83f76&gt;\", line 66, in &lt;module&gt;\r\n    main()\r\n  File \"&lt;ipython-input-11-90859dc83f76&gt;\", line 12, in main\r\n    model = DAModel()\r\n  File \"&lt;ipython-input-9-682db36e2a23&gt;\", line 148, in __init__\r\n    self.logits, self.labels, self.dialogue_lengths)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py\", line 257, in crf_log_likelihood\r\n    transition_params)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py\", line 116, in crf_sequence_score\r\n    false_fn=_multi_seq_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/utils.py\", line 202, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/smart_cond.py\", line 59, in smart_cond\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 1235, in cond\r\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 1061, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py\", line 104, in _multi_seq_fn\r\n    unary_scores = crf_unary_score(tag_indices, sequence_lengths, inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py\", line 287, in crf_unary_score\r\n    flattened_tag_indices = array_ops.reshape(offsets + tag_indices, [-1])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\", line 899, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\", line 1197, in _add_dispatch\r\n    return gen_math_ops.add_v2(x, y, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 549, in add_v2\r\n    \"AddV2\", x=x, y=y, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>Here is the code (which is slightly different from the repo in order to get it to run:</p>\n\n<p>Versions:\nPython 3</p>\n\n<p>tensorflow == 1.15.0</p>\n\n<p>pandas == 0.25.3</p>\n\n<p>numpy == 1.17.5</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>import glob\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n\r\n# preprocess data\r\n\r\nfile_list = []\r\nfor f in glob.glob('swda/*'):\r\n  file_list.append(f)\r\n\r\ndf_list = []\r\nfor i in file_list:\r\n  df = pd.read_csv(i)\r\n  df_list.append(df)\r\n\r\ntext_list = []\r\nlabel_list = []\r\n\r\nfor df in df_list:\r\n  df['utterance_no_specialchar_'] = df.utterance_no_specialchar.astype(str)\r\n  text = df.utterance_no_specialchar_.tolist()\r\n  labels = df.da_category.tolist()\r\n  text_list.append(text)\r\n  label_list.append(labels)\r\n\r\n### new preprocessing step  \r\ntext_list = [[[j] for j in i] for i in text_list]\r\n\r\ntok_data = [y[0] for x in text_list for y in x]\r\n\r\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\r\ntokenizer.fit_on_texts(tok_data)\r\n\r\nsequences = []\r\nfor x in text_list:\r\n  tmp = []\r\n  for y in x:\r\n    tmp.append(tokenizer.texts_to_sequences(y)[0])\r\n  sequences.append(tmp)\r\n\r\ndef _pad_sequences(sequences, pad_tok, max_length):\r\n    \"\"\"\r\n    Args:\r\n        sequences: a generator of list or tuple\r\n        pad_tok: the char to pad with\r\n    Returns:\r\n        a list of list where each sublist has same length\r\n    \"\"\"\r\n    sequence_padded, sequence_length = [], []\r\n\r\n    for seq in sequences:\r\n        seq = list(seq)\r\n        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\r\n        sequence_padded +=  [seq_]\r\n        sequence_length += [min(len(seq), max_length)]\r\n\r\n    return sequence_padded, sequence_length\r\n\r\ndef pad_sequences(sequences, pad_tok, nlevels=1):\r\n    \"\"\"\r\n    Args:\r\n        sequences: a generator of list or tuple\r\n        pad_tok: the char to pad with\r\n        nlevels: \"depth\" of padding, for the case where we have characters ids\r\n    Returns:\r\n        a list of list where each sublist has same length\r\n    \"\"\"\r\n    if nlevels == 1:\r\n        max_length = max(map(lambda x : len(x), sequences))\r\n        sequence_padded, sequence_length = _pad_sequences(sequences,\r\n                                            pad_tok, max_length)\r\n\r\n    elif nlevels == 2:\r\n        max_length_word = max([max(map(lambda x: len(x), seq))\r\n                               for seq in sequences])\r\n        sequence_padded, sequence_length = [], []\r\n        for seq in sequences:\r\n            # all words are same length now\r\n            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\r\n            sequence_padded += [sp]\r\n            sequence_length += [sl]\r\n\r\n        max_length_sentence = max(map(lambda x : len(x), sequences))\r\n        \r\n        sequence_padded, _ = _pad_sequences(sequence_padded,\r\n                [pad_tok]*max_length_word, max_length_sentence)\r\n        sequence_length, _ = _pad_sequences(sequence_length, 0,\r\n                max_length_sentence)\r\n\r\n    return sequence_padded, sequence_length\r\n\r\ndef minibatches(data, labels, batch_size):\r\n  data_size = len(data)\r\n  start_index = 0\r\n\r\n  num_batches_per_epoch = int((len(data) + batch_size - 1) / batch_size)\r\n  for batch_num in range(num_batches_per_epoch):\r\n      start_index = batch_num * batch_size\r\n      end_index = min((batch_num + 1) * batch_size, data_size)\r\n      yield data[start_index: end_index], labels[start_index: end_index]\r\n\r\ndef select(parameters, length):\r\n  \"\"\"Select the last valid time step output as the sentence embedding\r\n  :params parameters: [batch, seq_len, hidden_dims]\r\n  :params length: [batch]\r\n  :Returns : [batch, hidden_dims]\r\n  \"\"\"\r\n  shape = tf.shape(parameters)\r\n  idx = tf.range(shape[0])\r\n  idx = tf.stack([idx, length - 1], axis = 1)\r\n  return tf.gather_nd(parameters, idx)\r\n\r\n\r\nclass DAModel():\r\n    def __init__(self):\r\n        with tf.variable_scope(\"placeholder\"):\r\n\r\n            self.dialogue_lengths = tf.placeholder(tf.int32, shape = [None], name = \"dialogue_lengths\")\r\n            self.word_ids = tf.placeholder(tf.int32, shape = [None,None,None], name = \"word_ids\")\r\n            self.utterance_lengths = tf.placeholder(tf.int32, shape = [None, None], name = \"utterance_lengths\")\r\n            self.labels = tf.placeholder(tf.int32, shape = [None, None], name = \"labels\")\r\n            self.clip = tf.placeholder(tf.float32, shape = [], name = 'clip')\r\n\r\n######################## EMBEDDINGS ###########################################\r\n\r\n        with tf.variable_scope(\"embeddings\"):\r\n            _word_embeddings = tf.get_variable(\r\n                name = \"_word_embeddings\",\r\n                dtype = tf.float32,\r\n                shape = [words, word_dim],\r\n                initializer = tf.random_uniform_initializer()\r\n                )\r\n            word_embeddings = tf.nn.embedding_lookup(_word_embeddings,self.word_ids, name=\"word_embeddings\")\r\n            self.word_embeddings = tf.nn.dropout(word_embeddings, 0.8)\r\n                    \r\n        with tf.variable_scope(\"utterance_encoder\"):\r\n            s = tf.shape(self.word_embeddings)\r\n            batch_size = s[0] * s[1]\r\n            \r\n            time_step = s[-2]\r\n            word_embeddings = tf.reshape(self.word_embeddings, [batch_size, time_step, word_dim])\r\n            length = tf.reshape(self.utterance_lengths, [batch_size])\r\n\r\n            fw = tf.nn.rnn_cell.LSTMCell(hidden_size_lstm_1, forget_bias=0.8, state_is_tuple= True)\r\n            bw = tf.nn.rnn_cell.LSTMCell(hidden_size_lstm_1, forget_bias=0.8, state_is_tuple= True)\r\n            \r\n            output, _ = tf.nn.bidirectional_dynamic_rnn(fw, bw, word_embeddings,sequence_length=length, dtype = tf.float32)\r\n            output = tf.concat(output, axis = -1) # [batch_size, time_step, dim]\r\n            # Select the last valid time step output as the utterance embedding, \r\n            # this method is more concise than TensorArray with while_loop\r\n            # output = select(output, self.utterance_lengths) # [batch_size, dim]\r\n            output = select(output, length) # [batch_size, dim]\r\n\r\n            # output = tf.reshape(output, s[0], s[1], 2 * hidden_size_lstm_1)\r\n            output = tf.reshape(output, [s[0], s[1], 2 * hidden_size_lstm_1])\r\n\r\n            output = tf.nn.dropout(output, 0.8)\r\n\r\n        with tf.variable_scope(\"bi-lstm\"):\r\n            cell_fw = tf.contrib.rnn.BasicLSTMCell(hidden_size_lstm_2, state_is_tuple = True)\r\n            cell_bw = tf.contrib.rnn.BasicLSTMCell(hidden_size_lstm_2, state_is_tuple = True)\r\n            \r\n            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, output, sequence_length = self.dialogue_lengths, dtype = tf.float32)\r\n            outputs = tf.concat([output_fw, output_bw], axis = -1)\r\n            outputs = tf.nn.dropout(outputs, 0.8)\r\n        \r\n        with tf.variable_scope(\"proj1\"):\r\n            output = tf.reshape(outputs, [-1, 2 * hidden_size_lstm_2])\r\n            W = tf.get_variable(\"W\", dtype = tf.float32, shape = [2 * hidden_size_lstm_2, proj1], initializer= tf.contrib.layers.xavier_initializer())\r\n            b = tf.get_variable(\"b\", dtype = tf.float32, shape = [proj1], initializer=tf.zeros_initializer())\r\n            output = tf.nn.relu(tf.matmul(output, W) + b)\r\n\r\n        with tf.variable_scope(\"proj2\"):\r\n            W = tf.get_variable(\"W\", dtype = tf.float32, shape = [proj1, proj2], initializer= tf.contrib.layers.xavier_initializer())\r\n            b = tf.get_variable(\"b\", dtype = tf.float32, shape = [proj2], initializer=tf.zeros_initializer())\r\n            output = tf.nn.relu(tf.matmul(output, W) + b)\r\n\r\n        with tf.variable_scope(\"logits\"):\r\n            nstep = tf.shape(outputs)[1]\r\n            W = tf.get_variable(\"W\", dtype = tf.float32,shape=[proj2, tags], initializer = tf.random_uniform_initializer())\r\n            b = tf.get_variable(\"b\", dtype = tf.float32,shape = [tags],initializer=tf.zeros_initializer())\r\n\r\n            pred = tf.matmul(output, W) + b\r\n            self.logits = tf.reshape(pred, [-1, nstep, tags])\r\n        \r\n        with tf.variable_scope(\"loss\"):\r\n            log_likelihood, self.trans_params = tf.contrib.crf.crf_log_likelihood(\r\n                        self.logits, self.labels, self.dialogue_lengths)\r\n            self.loss = tf.reduce_mean(-log_likelihood) + tf.nn.l2_loss(W) + tf.nn.l2_loss(b)\r\n            #tf.summary.scalar(\"loss\", self.loss)\r\n        \r\n\r\n        with tf.variable_scope(\"viterbi_decode\"):\r\n            viterbi_sequence, _ = tf.contrib.crf.crf_decode(self.logits, self.trans_params,  self.dialogue_lengths)\r\n            \r\n            batch_size = tf.shape(self.dialogue_lengths)[0]\r\n\r\n            output_ta = tf.TensorArray(dtype = tf.float32, size = 1, dynamic_size = True)\r\n            def body(time, output_ta_1):\r\n                length = self.dialogue_lengths[time]\r\n                vcode = viterbi_sequence[time][:length]\r\n                true_labs = self.labels[time][:length]\r\n                accurate = tf.reduce_sum(tf.cast(tf.equal(vcode, true_labs), tf.float32))\r\n\r\n                output_ta_1 = output_ta_1.write(time, accurate)\r\n                return time + 1, output_ta_1\r\n\r\n\r\n            def condition(time, output_ta_1):\r\n                return time &lt; batch_size\r\n\r\n            i = 0\r\n            [time, output_ta] = tf.while_loop(condition, body, loop_vars = [i, output_ta])\r\n            output_ta = output_ta.stack()\r\n            accuracy = tf.reduce_sum(output_ta)\r\n            self.accuracy = accuracy / tf.reduce_sum(tf.cast(self.dialogue_lengths, tf.float32))\r\n            #tf.summary.scalar(\"accuracy\", self.accuracy)\r\n\r\n        with tf.variable_scope(\"train_op\"):\r\n            optimizer = tf.train.AdagradOptimizer(0.1)\r\n            #if tf.greater(self.clip , 0):\r\n            grads, vs = zip(*optimizer.compute_gradients(self.loss))\r\n            grads, gnorm = tf.clip_by_global_norm(grads, self.clip)\r\n            self.train_op = optimizer.apply_gradients(zip(grads, vs))\r\n            #else:\r\n            #    self.train_op = optimizer.minimize(self.loss)\r\n        #self.merged = tf.summary.merge_all()\r\n\r\n\r\n### Set model variables\r\n\r\nhidden_size_lstm_1 = 200\r\nhidden_size_lstm_2 = 200\r\ntags = 39 # assuming number of classes to predict?\r\nword_dim = 300\r\nproj1 = 200\r\nproj2 = 100\r\nwords = 20001 \r\n# words = 8759 + 1 # max(num_unique_word_tokens)\r\nbatchSize = 2\r\nlog_dir = \"train\"\r\nmodel_dir = \"DAModel\"\r\nmodel_name = \"ckpt\"\r\n\r\n### Run model\r\n\r\ndef main():\r\n    # tokenize and vectorize text data to prepare for embedding\r\n    train_data = sequences[:75]\r\n    train_labels = label_list[:75]\r\n    dev_data = sequences[75:]\r\n    dev_labels = label_list[75:]\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.per_process_gpu_memory_fraction = 0.4\r\n    \r\n    with tf.Session(config = config) as sess:\r\n        model = DAModel()\r\n        sess.run(tf.global_variables_initializer())\r\n        clip = 2\r\n        saver = tf.train.Saver()\r\n        #writer = tf.summary.FileWriter(\"D:\\\\Experimemts\\\\tensorflow\\\\DA\\\\train\", sess.graph)\r\n        writer = tf.summary.FileWriter(\"train\", sess.graph)\r\n        counter = 0\r\n        for epoch in range(10):\r\n            for dialogues, labels in minibatches(train_data, train_labels, batchSize):\r\n                _, dialogue_lengthss = pad_sequences(dialogues, 0)\r\n                word_idss, utterance_lengthss = pad_sequences(dialogues, 0, nlevels = 2)\r\n                true_labs = labels\r\n                labs_t, _ = pad_sequences(true_labs, 0)\r\n                counter += 1\r\n                train_loss, train_accuracy, _ = sess.run([model.loss, model.accuracy,model.train_op], feed_dict = {model.word_ids: word_idss, model.utterance_lengths: utterance_lengthss, model.dialogue_lengths: dialogue_lengthss, model.labels:labs_t, model.clip :clip} )\r\n                #writer.add_summary(summary, global_step = counter)\r\n                print(\"step = {}, train_loss = {}, train_accuracy = {}\".format(counter, train_loss, train_accuracy))\r\n                \r\n                train_precision_summ = tf.Summary()\r\n                train_precision_summ.value.add(\r\n                    tag='train_accuracy', simple_value=train_accuracy)\r\n                writer.add_summary(train_precision_summ, counter)\r\n\r\n                train_loss_summ = tf.Summary()\r\n                train_loss_summ.value.add(\r\n                    tag='train_loss', simple_value=train_loss)\r\n                writer.add_summary(train_loss_summ, counter)\r\n                \r\n                if counter % 1 == 0:\r\n                    loss_dev = []\r\n                    acc_dev = []\r\n                    for dev_dialogues, dev_labels in minibatches(dev_data, dev_labels, batchSize):\r\n                        _, dialogue_lengthss = pad_sequences(dev_dialogues, 0)\r\n                        word_idss, utterance_lengthss = pad_sequences(dev_dialogues, 0, nlevels = 2)\r\n                        true_labs = dev_labels\r\n                        labs_t, _ = pad_sequences(true_labs, 0)\r\n                        dev_loss, dev_accuacy = sess.run([model.loss, model.accuracy], feed_dict = {model.word_ids: word_idss, model.utterance_lengths: utterance_lengthss, model.dialogue_lengths: dialogue_lengthss, model.labels:labs_t})\r\n                        loss_dev.append(dev_loss)\r\n                        acc_dev.append(dev_accuacy)\r\n                    valid_loss = sum(loss_dev) / len(loss_dev)\r\n                    valid_accuracy = sum(acc_dev) / len(acc_dev)\r\n\r\n                    dev_precision_summ = tf.Summary()\r\n                    dev_precision_summ.value.add(\r\n                        tag='dev_accuracy', simple_value=valid_accuracy)\r\n                    writer.add_summary(dev_precision_summ, counter)\r\n\r\n                    dev_loss_summ = tf.Summary()\r\n                    dev_loss_summ.value.add(\r\n                        tag='dev_loss', simple_value=valid_loss)\r\n                    writer.add_summary(dev_loss_summ, counter)\r\n                    print(\"counter = {}, dev_loss = {}, dev_accuacy = {}\".format(counter, valid_loss, valid_accuracy))\r\nif __name__ == \"__main__\":\r\n    tf.reset_default_graph()\r\n    main()</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>The data comes from <a href=\"https://github.com/cgpotts/swda\" rel=\"noreferrer\">here</a> and looks like this:</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>[[['what  '],\r\n ['do you want to start '],\r\n ['f uh  laughter  you hit  you hit  f uh   '],\r\n ['it doesnt matter  '],\r\n ['f um  were discussing the capital punishment i believe '],\r\n ['right  '],\r\n ['you are right  '],\r\n ['yeah  '],\r\n ['  i  i  suppose i should have '],\r\n ['f uh  which  '],\r\n ['i  am  am  pro capital punishment except that i dont like the way its done '],\r\n ['uhhuh  '],\r\n ['f uh  yeah  '],\r\n ['f uh   i  f uh  i  guess  i  i  hate to see anyone die f uh   ']\r\n ...\r\n ]]</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>The dataset to train the model can be found here:\n<a href=\"https://github.com/cmeaton/Hierarchical_BiLSTM-CRF_Encoder/tree/master/swda_parsed\" rel=\"noreferrer\">https://github.com/cmeaton/Hierarchical_BiLSTM-CRF_Encoder/tree/master/swda_parsed</a></p>\n\n<p>I'm having a hard time understanding what this error even means and how to approach understanding it. Any advice would be much appreciated. Thanks.</p>\n",
    "score": 11,
    "creation_date": 1579725756,
    "view_count": 26448,
    "answer_count": 2,
    "tags": "python;tensorflow;deep-learning;nlp;text-classification"
  },
  {
    "question_id": 35482943,
    "title": "Computing symmetric Kullback-Leibler divergence between two documents",
    "body": "<p>I have followed the paper <a href=\"http://users.softlab.ntua.gr/facilities/public/AD/Text%20Categorization/Using%20Kullback-Leibler%20Distance%20for%20Text%20Categorization.pdf\" rel=\"nofollow noreferrer\">here</a> and the code <a href=\"https://gist.github.com/mrorii/961963\" rel=\"nofollow noreferrer\">here</a> (it is implemented using the symmetric kld and a back-off model proposed in the paper in the 1st link) for computing KLD between two text data sets. I have changed the for-loop in the end to return the probability distribution of two data sets to test if both sum to 1:</p>\n\n<pre><code>import re, math, collections\n\ndef tokenize(_str):\n    stopwords = ['and', 'for', 'if', 'the', 'then', 'be', 'is', \\\n                 'are', 'will', 'in', 'it', 'to', 'that']\n    tokens = collections.defaultdict(lambda: 0.)\n    for m in re.finditer(r\"(\\w+)\", _str, re.UNICODE):\n        m = m.group(1).lower()\n        if len(m) &lt; 2: continue\n        if m in stopwords: continue\n        tokens[m] += 1\n\n    return tokens\n#end of tokenize\n\ndef kldiv(_s, _t):\n    if (len(_s) == 0):\n        return 1e33\n\n    if (len(_t) == 0):\n        return 1e33\n\n    ssum = 0. + sum(_s.values())\n    slen = len(_s)\n\n    tsum = 0. + sum(_t.values())\n    tlen = len(_t)\n\n    vocabdiff = set(_s.keys()).difference(set(_t.keys()))\n    lenvocabdiff = len(vocabdiff)\n\n    \"\"\" epsilon \"\"\"\n    epsilon = min(min(_s.values())/ssum, min(_t.values())/tsum) * 0.001\n\n    \"\"\" gamma \"\"\"\n    gamma = 1 - lenvocabdiff * epsilon\n\n    \"\"\" Check if distribution probabilities sum to 1\"\"\"\n    sc = sum([v/ssum for v in _s.itervalues()])\n    st = sum([v/tsum for v in _t.itervalues()])\n\n    ps=[] \n    pt = [] \n    for t, v in _s.iteritems(): \n        pts = v / ssum \n        ptt = epsilon \n        if t in _t: \n            ptt = gamma * (_t[t] / tsum) \n        ps.append(pts) \n        pt.append(ptt)\n    return ps, pt\n</code></pre>\n\n<p>I have tested with</p>\n\n<p><code>d1 = \"\"\"Many research publications want you to use BibTeX, which better\norganizes the whole process. Suppose for concreteness your source\nfile is x.tex. Basically, you create a file x.bib containing the\nbibliography, and run bibtex on that file.\"\"\"\nd2 = \"\"\"In this case you must supply both a \\left and a \\right because the\ndelimiter height are made to match whatever is contained between the\ntwo commands. But, the \\left doesn't have to be an actual 'left\ndelimiter', that is you can use '\\left)' if there were some reason\nto do it.\"\"\"</code></p>\n\n<p><code>sum(ps)</code> = 1  but <code>sum(pt)</code> is way smaller than 1 when:</p>\n\n<p><img src=\"https://i.sstatic.net/25rcv.png\" alt=\"This should be the case.\"></p>\n\n<p>Is there something that is not correct in the code or else? Thanks!</p>\n\n<p><strong>Update:</strong></p>\n\n<p>In order to make both pt and ps sum to 1, I had to change the code to:</p>\n\n<pre><code>    vocab = Counter(_s)+Counter(_t)\n    ps=[] \n    pt = [] \n    for t, v in vocab.iteritems(): \n        if t in _s:\n            pts = gamma * (_s[t] / ssum) \n        else: \n            pts = epsilon\n\n        if t in _t: \n            ptt = gamma * (_t[t] / tsum) \n        else:\n            ptt = epsilon\n\n        ps.append(pts) \n        pt.append(ptt)\n\n    return ps, pt\n</code></pre>\n",
    "score": 11,
    "creation_date": 1455802533,
    "view_count": 2697,
    "answer_count": 2,
    "tags": "python;nlp;similarity;information-retrieval"
  },
  {
    "question_id": 12349211,
    "title": "C# library to build correct english sentences",
    "body": "<p>I am working on a C# application. I need to construct english sentences correctly. I will give it the nouns verbs and objects and I need to construct a correct english phrase.\nFor example I am looking to do something like this:</p>\n\n<pre><code>PhraseBuilder p = new PhraseBuilder ();\np.Subject(\"Tom\");\np.Verb(\"eat\");\np.Object(\"the apple\");\n</code></pre>\n\n<p>and then use</p>\n\n<pre><code>p.BuildPhrase()\n</code></pre>\n\n<p>and I need to get this as an output:</p>\n\n<blockquote>\n  <p>Tom eats the apple. </p>\n</blockquote>\n\n<p>Notice the 's' added to eat and the full stop at the end</p>\n\n<p>Is there any library that can do above? I need it to have correct English and punctuation.</p>\n",
    "score": 11,
    "creation_date": 1347270716,
    "view_count": 1852,
    "answer_count": 1,
    "tags": "c#;nlp;artificial-intelligence"
  },
  {
    "question_id": 8695552,
    "title": "Algorithm (or C# library) for identifying &#39;keywords&#39; in a set of messages?",
    "body": "<p>I want to build a list of ~6 keywords (or even better: couple word keyphrases) for each message in a message forum.</p>\n\n<ul>\n<li>The primary use of keywords is to replace subject lines in some instances. For example: <em>Message from Terry sent Dec 5, keywords: <strong>norweigan blue, plumage, not dead</em></strong></li>\n<li>In a super ideal world keywords would identify both unique phases, and phrases that cluster the discussion into \"topics\", i.e. words that are highly relevant to the message in question, and a few other messages in the forum, but not found frequently in the forum as a whole.</li>\n<li>I expect junk phrases to show up, no big deal.</li>\n<li>Can't be too computationally expensive: I need something that can handle several hundred messages in several seconds, as I'll need to re-run this every time a new message comes in.</li>\n</ul>\n\n<p>Anyone know a good C# library for accomplishing this? Maybe there's a way to bend Lucene.NET into providing this sort of info?</p>\n\n<p>Or, failing that, can anyone suggest an algorithm (or set of algos) to read up on? If I'm implementing myself I need something not terribly complex, I can only tackle this if its tractable in about a week. Right now, the best I've found in terms of simple-but-effective is <a href=\"http://en.wikipedia.org/wiki/Tf%E2%80%93idf\" rel=\"nofollow\">TF-IDF</a>.</p>\n\n<p><strong>UPDATE:</strong> I've uploaded the results of using TF-IDF to select the top 5 keywords from a real dataset here: <a href=\"http://jsbin.com/oxanoc/2/edit#preview\" rel=\"nofollow\">http://jsbin.com/oxanoc/2/edit#preview</a> </p>\n\n<p>The results are mediocre, but not totally useless... maybe with the addition of detecting multi-word phrases, this would be good enough.</p>\n",
    "score": 11,
    "creation_date": 1325452138,
    "view_count": 3875,
    "answer_count": 1,
    "tags": "c#;algorithm;search;nlp;text-mining"
  },
  {
    "question_id": 7773907,
    "title": "Web/browser-oriented open source machine learning projects?",
    "body": "<p>Applying machine learning techniques, more specifically text mining techniques, in browser environment (mainly Javascript) or as a web application is not a very widely discussed topic.</p>\n\n<p>I want to build my own web application / browser extension that can accomplish certain level of text classification / visualization techniques. I would like to know, if there is any open source projects that apply text mining techniques in web application or even better as browser extensions? </p>\n\n<p>So far, these are the projects/discussions I gathered with days of random searching:</p>\n\n<p><strong>For text mining in web application:</strong></p>\n\n<ul>\n<li><a href=\"http://text-processing.com/\" rel=\"nofollow noreferrer\">http://text-processing.com/</a> with <a href=\"http://text-processing.com/demo/\" rel=\"nofollow noreferrer\">demo</a> (Close source, with limited api)</li>\n<li><a href=\"http://www.uclassify.com/Default.aspx\" rel=\"nofollow noreferrer\">uClassify</a> (close source, no info about library base)<br> </li>\n</ul>\n\n<p><strong>For machine learning in Javascript:</strong></p>\n\n<ul>\n<li><a href=\"http://news.ycombinator.com/item?id=1704648\" rel=\"nofollow noreferrer\">Discussion</a> on the possibility about Machine learning in\nJavaScript. (mainly about saying Node.js is going to change the landscape)</li>\n<li><a href=\"http://harthur.github.com/brain/\" rel=\"nofollow noreferrer\">brain - javascript supervised machine learning</a></li>\n<li>A <a href=\"http://www.dusbabek.org/~garyd/bayes/\" rel=\"nofollow noreferrer\">demo project</a> with Naive Bayes implemented in Javascript</li>\n</ul>\n\n<p>For web application text mining, the architect that I can think of:</p>\n\n<ul>\n<li>Python libraries (e.g. NLTK or scikit-learn) + Django</li>\n<li>Java libraries (a lot) + Play! framework</li>\n<li>Even R based + <a href=\"http://rapache.net/\" rel=\"nofollow noreferrer\">rApache</a></li>\n</ul>\n",
    "score": 11,
    "creation_date": 1318628706,
    "view_count": 2447,
    "answer_count": 2,
    "tags": "machine-learning;nlp;classification;weka;nltk"
  },
  {
    "question_id": 26344933,
    "title": "Extracting (subject,predicate,object) from dependency tree",
    "body": "<p>I'm interested in extracting triples (subject,predicate,object) from questions.</p>\n\n<p>For example, I would like to transform the following question : </p>\n\n<blockquote>\n  <p>Who is the wife of the president of the USA?</p>\n</blockquote>\n\n<p>to : </p>\n\n<blockquote>\n  <p>(x,isWifeOf,y) &wedge; (y,isPresidentof,USA)</p>\n</blockquote>\n\n<p>x and y are unknows that we have to find in order to answer the question (/\\ denotes the conjunction).</p>\n\n<p>I have read a lot of papers about this topic and I would like to perform this task using existing parsers such as Stanford parser. I know that parsers output 2 types of data : </p>\n\n<ul>\n<li>parse structure tree (constituency relations)</li>\n<li>dependency tree (dependency relations)</li>\n</ul>\n\n<p>Some papers try to build triples from the parse structure tree (e.g., <a href=\"http://ailab.ijs.si/delia_rusu/Papers/is_2007.pdf\" rel=\"nofollow noreferrer\">Triple Extraction from Sentences</a>), however this approach seems to be too weak to deal with complicated questions.</p>\n\n<p>On the other hand, dependency trees contain a lot of relevant information to perform the triple extraction. A lot of papers claim to do that, however I didn't find any of them that gives explicitely a detailed procedure or an algorithm. Most of the time, authors say they analyze the dependencies to produce triples according to some rules they didn't give.</p>\n\n<p>Does anyone know any paper with more information on extracting (subject,predicate,object) from dependency tree of a question?</p>\n",
    "score": 11,
    "creation_date": 1413218748,
    "view_count": 2317,
    "answer_count": 1,
    "tags": "nlp;rdf;stanford-nlp;nlp-question-answering;dependency-parsing"
  },
  {
    "question_id": 29110950,
    "title": "Python concordance command in NLTK",
    "body": "<p>I have a question regarding Python concordance command in NLTK. First, I came through an easy example: </p>\n\n<pre><code>from nltk.book import *\n\ntext1.concordance(\"monstrous\")\n</code></pre>\n\n<p>which worked just fine. Now, I have my own .txt file and I would like to perform the same command. I have a list called \"textList\" and want to find the word \"CNA\" so I put command</p>\n\n<pre><code>textList.concordance('CNA') \n</code></pre>\n\n<p>Yet, I got the error </p>\n\n<pre><code>AttributeError: 'list' object has no attribute 'concordance'. \n</code></pre>\n\n<p>In the example, is the text1 NOT a list? I wonder what is going on here.    </p>\n",
    "score": 10,
    "creation_date": 1426631482,
    "view_count": 29820,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 12413705,
    "title": "Parsing natural language ingredient quantities for recipes",
    "body": "<p>I'm building a ruby recipe management application, and as part of it, I want to be able to parse ingredient quantities into a form I can compare and scale. I'm wondering what the best tools are for doing this.</p>\n\n<p>I originally planned on a complex regex, then on some other code that converts human readable numbers like <code>two</code> or <code>five</code> into integers, and finally code that will convert say <code>1 cup</code> and <code>3 teaspoons</code> into some base measurement. I control the input, so I kept the actual ingredient separate. However, I noticed users inputting abstract measurements like <code>to taste</code> and <code>1 package</code>. At least with the abstract measurements, I think I could just ignore them and scale and just scrape any number preceding them.</p>\n\n<p>Here are some more examples</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>1 tall can\n1/4 cup\n2 Leaves\n1 packet\nTo Taste\nOne\nTwo slices\n3-4 fillets\nHalf-bunch\n2 to 3 pinches (optional)\n</code></pre>\n\n<p>Are there any tricks to this? I have noticed users seem somewhat confused of what constitutes a quantity. I could try to enforce stricter rules and push things like <code>tall can</code> and <code>leaves</code> into the ingredient part. However, in order to enforce that, I need to be able to convey what's invalid.</p>\n\n<p>I'm also not sure what the \"base\" measurement I should convert quantities into.</p>\n\n<p>These are my goals.</p>\n\n<ol>\n<li><p><strong>To be able to scale recipes.</strong>  Arbitrary units of measurement like\n<code>packages</code> don't have to be scaled but precise ones like <code>cups</code> or\n<code>ounces</code> need to be.</p></li>\n<li><p><strong>Figure out the \"main\" ingredients.</strong>  In the context of this question, this will be done largely by figuring out what the largest ingredient is in the recipe. In production, there will have to be some sort of modifier based on the type of ingredient because, obviously, <code>flour</code> is almost never considered the \"main\" ingredient. However, <code>chocolate</code> can be used sparingly, and it can still be said a <code>chocolate cake</code>.</p></li>\n<li><p><strong>Normalize input.</strong>  To keep some consistency on the site, I want to keep consistent abbreviations. For example, instead of <code>pounds</code>, it should be <code>lbs</code>.</p></li>\n</ol>\n",
    "score": 10,
    "creation_date": 1347565727,
    "view_count": 7879,
    "answer_count": 4,
    "tags": "regex;ruby;nlp"
  },
  {
    "question_id": 59865719,
    "title": "How to find the closest word to a vector using BERT",
    "body": "<p>I am trying to get textual representation(or the closest word) of given word embedding using BERT. Basically I am trying to get similar functionality as in gensim:</p>\n\n<pre><code>&gt;&gt;&gt; your_word_vector = array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32)\n&gt;&gt;&gt; model.most_similar(positive=[your_word_vector], topn=1))\n</code></pre>\n\n<p>So far, I have been able to generate contextual word embedding using <a href=\"https://github.com/hanxiao/bert-as-service#getting-elmo-like-contextual-word-embedding\" rel=\"noreferrer\">bert-as-service</a> but can't figure out how to get closest words to this embedding. I have used pre-trained bert model (uncased_L-12_H-768_A-12) and haven't done any fine tuning.</p>\n",
    "score": 10,
    "creation_date": 1579716005,
    "view_count": 12396,
    "answer_count": 2,
    "tags": "nlp;word-embedding;bert-language-model"
  },
  {
    "question_id": 6030291,
    "title": "Python or Java for text processing (text mining, information retrieval, natural language processing)",
    "body": "<p>I'm soon to start on a new project where I am going to do lots of text processing tasks like searching, categorization/classifying, clustering, and so on. </p>\n\n<p>There's going to be a huge amount of documents that need to be processed; probably millions of documents. After the initial processing, it also has to be able to be updated daily with multiple new documents.</p>\n\n<p>Can I use Python to do this, or is Python too slow? Is it best to use Java?</p>\n\n<p>If possible, I would prefer Python since that's what I have been using lately. Plus, I would finish the coding part much faster. But it all depends on Python's speed. I have used Python for some small scale text processing tasks with only a couple of thousand documents, but I am not sure how well it scales up.</p>\n",
    "score": 10,
    "creation_date": 1305632779,
    "view_count": 10801,
    "answer_count": 4,
    "tags": "java;python;nlp;information-retrieval;text-mining"
  },
  {
    "question_id": 22139866,
    "title": "Finding Tense of A sentence using stanford nlp",
    "body": "<p>Q1.I am trying to get tense of a complete sentence,just don't know how to do it using nlp.\nAny help appreciated.</p>\n\n<p>Q2 .What all information can be extracted from a sentence using nlp?</p>\n\n<p>Currently I can,\nI get : 1.Voice of sentence\n        2.subject object verb\n        3.POS tags.</p>\n\n<p>Any more info can be extracted please let me know.</p>\n",
    "score": 10,
    "creation_date": 1393827124,
    "view_count": 12131,
    "answer_count": 4,
    "tags": "nlp;stanford-nlp;linguistics"
  },
  {
    "question_id": 64485777,
    "title": "How is the number of parameters be calculated in BERT model?",
    "body": "<p>The paper &quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&quot; by Devlin &amp; Co. calculated for the base model size 110M parameters (i.e. L=12, H=768, A=12) where L = number of layers, H = hidden size and A = number of self-attention operations. As far as I know parameters in a neural network are usually the count of &quot;weights and biases&quot; between the layers. So how is this calculated based on the given information? 12<em>768</em>768*12?</p>\n",
    "score": 10,
    "creation_date": 1603381278,
    "view_count": 22096,
    "answer_count": 1,
    "tags": "neural-network;nlp;bert-language-model"
  },
  {
    "question_id": 33536022,
    "title": "How can I print the entire contents of Wordnet (preferably with NLTK)?",
    "body": "<p>NLTK provides functions for printing all the words in the Brown (or Gutenberg) corpus. But the equivalent function does not seem to work on Wordnet. </p>\n\n<p>Is there a way to do this through NLTK? If there is not, how might one do it?</p>\n\n<p>This works: </p>\n\n<pre><code>from nltk.corpus import brown as b\nprint b.words()\n</code></pre>\n\n<p>This causes an AttributeError:</p>\n\n<pre><code>from nltk.corpus import wordnet as wn\nprint wn.words()\n</code></pre>\n",
    "score": 10,
    "creation_date": 1446693136,
    "view_count": 9346,
    "answer_count": 3,
    "tags": "python;nlp;nltk;wordnet;corpus"
  },
  {
    "question_id": 54978443,
    "title": "Predicting Missing Words in a sentence - Natural Language Processing Model",
    "body": "<p>I have the sentence below :</p>\n\n<pre><code>I want to ____ the car because it is cheap.\n</code></pre>\n\n<p>I want to predict the missing word ,using an NLP model. What NLP model shall I use? Thanks.</p>\n",
    "score": 10,
    "creation_date": 1551684432,
    "view_count": 11770,
    "answer_count": 2,
    "tags": "machine-learning;neural-network;nlp;predict"
  },
  {
    "question_id": 1318197,
    "title": "How to determine subject, object and other words?",
    "body": "<p>I'm trying to implement application that can determine meaning of sentence, by dividing it to smaller pieces. So I need to know what words are subject, object etc. so that my program can know how to handle this sentence.</p>\n",
    "score": 10,
    "creation_date": 1251021802,
    "view_count": 6312,
    "answer_count": 5,
    "tags": "artificial-intelligence;nlp"
  },
  {
    "question_id": 2649474,
    "title": "How to perform FST (Finite State Transducer) composition",
    "body": "<p>Consider the following FSTs :</p>\n\n<pre><code>T1 \n\n0 1 a : b\n0 2 b : b\n2 3 b : b\n0 0 a : a\n1 3 b : a\n\nT2\n\n0 1 b : a\n1 2 b : a\n1 1 a : d\n1 2 a : c\n</code></pre>\n\n<p>How do I perform the composition operation on these two FSTs (i.e. T1 o T2)\nI saw some algorithms but couldn't understand much. If anyone could explain it in a easy way it would be a major help.</p>\n\n<p>Please note that this is NOT a homework. The example is taken from the lecture slides where the solution is given but I couldn't figure out how to get to it.</p>\n",
    "score": 10,
    "creation_date": 1271371227,
    "view_count": 8585,
    "answer_count": 3,
    "tags": "nlp;finite-automata;state-machine"
  },
  {
    "question_id": 46580932,
    "title": "Calculate TF-IDF using sklearn for n-grams in python",
    "body": "<p>I have a vocabulary list that include n-grams as follows. </p>\n\n<pre><code>myvocabulary = ['tim tam', 'jam', 'fresh milk', 'chocolates', 'biscuit pudding']\n</code></pre>\n\n<p>I want to use these words to calculate TF-IDF values.</p>\n\n<p>I also have a dictionary of corpus as follows (key = recipe number, value = recipe).</p>\n\n<pre><code>corpus = {1: \"making chocolates biscuit pudding easy first get your favourite biscuit chocolates\", 2: \"tim tam drink new recipe that yummy and tasty more thicker than typical milkshake that uses normal chocolates\", 3: \"making chocolates drink different way using fresh milk egg\"}\n</code></pre>\n\n<p>I am currently using the following code.</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(vocabulary = myvocabulary, stop_words = 'english')\ntfs = tfidf.fit_transform(corpus.values())\n</code></pre>\n\n<p>Now I am printing tokens or n-grams of the recipe 1 in <code>corpus</code> along with the tF-IDF value as follows.</p>\n\n<pre><code>feature_names = tfidf.get_feature_names()\ndoc = 0\nfeature_index = tfs[doc,:].nonzero()[1]\ntfidf_scores = zip(feature_index, [tfs[doc, x] for x in feature_index])\nfor w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n  print(w, s)\n</code></pre>\n\n<p>The results I get is <code>chocolates 1.0</code>. However, my code does not detect n-grams (bigrams) such as <code>biscuit pudding</code> when calculating TF-IDF values. Please let me know where I make the code wrong.</p>\n\n<p>I want to get the TD-IDF matrix for <code>myvocabulary</code> terms by using the recipe documents in the <code>corpus</code>. In other words, the rows of the matrix represents <code>myvocabulary</code> and the columns of the matrix represents the recipe documents of my <code>corpus</code>. Please help me.</p>\n",
    "score": 10,
    "creation_date": 1507191526,
    "view_count": 24970,
    "answer_count": 2,
    "tags": "python;scikit-learn;nlp;tf-idf"
  },
  {
    "question_id": 24398536,
    "title": "Named Entity Recognition with Regular Expression: NLTK",
    "body": "<p>I have been playing with NLTK toolkit. I come across this problem a lot and searched for solution online but nowhere I got a satisfying answer. So I am putting my query here. </p>\n\n<p>Many times NER doesn't tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.</p>\n\n<p>Example:</p>\n\n<p>Input: </p>\n\n<blockquote>\n  <p>Barack Obama is a great person.</p>\n</blockquote>\n\n<p>Output:  </p>\n\n<blockquote>\n  <p>Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('ORGANIZATION', [('Obama', 'NNP')]), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('person', 'NN'), ('.', '.')])</p>\n</blockquote>\n\n<p>where as </p>\n\n<p>input: </p>\n\n<blockquote>\n  <p>Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he \"was honored\" to be compared to Darth Vader while in office.</p>\n</blockquote>\n\n<p>Output: </p>\n\n<blockquote>\n  <p>Tree('S', [('Former', 'JJ'), ('Vice', 'NNP'), ('President', 'NNP'), Tree('NE', [('Dick', 'NNP'), ('Cheney', 'NNP')]), ('told', 'VBD'), ('conservative', 'JJ'), ('radio', 'NN'), ('host', 'NN'), Tree('NE', [('Laura', 'NNP'), ('Ingraham', 'NNP')]), ('that', 'IN'), ('he', 'PRP'), ('<code>', '</code>'), ('was', 'VBD'), ('honored', 'VBN'), (\"''\", \"''\"), ('to', 'TO'), ('be', 'VB'), ('compared', 'VBN'), ('to', 'TO'), Tree('NE', [('Darth', 'NNP'), ('Vader', 'NNP')]), ('while', 'IN'), ('in', 'IN'), ('office', 'NN'), ('.', '.')])</p>\n</blockquote>\n\n<p>Here Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP) , is correctly extracted. </p>\n\n<p>So I think if nltk.ne_chunk is used first and then if two consecutive trees are NNP there are high chances that both refers to one entity. </p>\n\n<p>Any suggestion will be really appreciated. I am looking for flaws in my approach.</p>\n\n<p>Thanks.</p>\n",
    "score": 10,
    "creation_date": 1403657105,
    "view_count": 15318,
    "answer_count": 3,
    "tags": "regex;nlp;nltk;named-entity-recognition"
  },
  {
    "question_id": 6952512,
    "title": "How I train an Named Entity Recognizer identifier in OpenNLP?",
    "body": "<p>Ok, I have the following code to train the NER Identifier from OpenNLP</p>\n\n<pre><code>FileReader fileReader = new FileReader(\"train.txt\");\nObjectStream fileStream = new PlainTextByLineStream(fileReader);\nObjectStream sampleStream = new NameSampleDataStream(fileStream);\nTokenNameFinderModel model = NameFinderME.train(\"pt-br\", \"train\", sampleStream, Collections.&lt;String, Object&gt;emptyMap());\nnfm = new NameFinderME(model); \n</code></pre>\n\n<p>I don't know if I'm doing something wrong of if something is missing, but the classifying is not working. I'm supposing that the train.txt is wrong.</p>\n\n<p><strong>The error</strong> that occurs is that all tokens are classified to only one type.</p>\n\n<p>My train.txt data is something like the following example, but with a lot more of variation and quantity of entries. Another thing is that I'm classifind word by word from a text per time, and not all tokens.</p>\n\n<pre><code>&lt;START:distance&gt; 8000m &lt;END&gt;\n&lt;START:temperature&gt; 100ºC &lt;END&gt;\n&lt;START:weight&gt; 50kg &lt;END&gt;\n&lt;START:name&gt; Renato &lt;END&gt;\n</code></pre>\n\n<p>Somebody can show what I doing wrong?</p>\n",
    "score": 10,
    "creation_date": 1312527078,
    "view_count": 10197,
    "answer_count": 1,
    "tags": "java;nlp;opennlp;named-entity-recognition"
  },
  {
    "question_id": 75203036,
    "title": "Flan T5 - How to give the correct prompt/question?",
    "body": "<p>Giving the right kind of prompt to Flan T5 Language model in order to get the correct/accurate responses for a chatbot/option matching use case.</p>\n<p>I am trying to use a Flan T5 model for the following task. Given a chatbot that presents the user with a list of options, the model has to do semantic option matching. For instance, if the options are &quot;Barbeque Chicken, Smoked Salmon&quot;, if the user says &quot;I want fish&quot;, the model should select smoked salmon.\nAnother use case could be &quot;The first one&quot; in which case the model should select Barbeque Chicken.\nA third use case could be &quot;The BBQ one&quot; in which case the model should select Barbeque chicken.</p>\n<p>I am using some code from the huggingface docs to play around with flan-t5 but I did not get the correct output.</p>\n<pre><code>\nmodel = AutoModelForSeq2SeqLM.from_pretrained(&quot;google/flan-t5-small&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;google/flan-t5-small&quot;)\n\ninputs = tokenizer('''Q:Select from the following options \n(a) Quinoa Salad \n(b) Kale Smoothie \nA:Select the first one\n''', return_tensors=&quot;pt&quot;)\noutputs = model.generate(**inputs)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n</code></pre>\n<p>The output is</p>\n<pre><code>['(b) Kale Smoothie']\n</code></pre>\n<p>How should I give the correct prompt/question to elicit the correct response from Flan t5 ?</p>\n",
    "score": 10,
    "creation_date": 1674413709,
    "view_count": 15563,
    "answer_count": 2,
    "tags": "nlp;huggingface-transformers"
  },
  {
    "question_id": 29290107,
    "title": "Detecting language using Stanford NLP",
    "body": "<p>I'm wondering if it is possible to use <code>Stanford CoreNLP</code> to detect which language a sentence is written in? If so, how precise can those algorithms be?</p>\n",
    "score": 10,
    "creation_date": 1427409240,
    "view_count": 6333,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 1167262,
    "title": "Automatically determine the natural language of a website page given its URL",
    "body": "<p>I'm looking for a way to automatically determine the natural language used by a website page, given its URL.</p>\n\n<p>In Python, a function like:</p>\n\n<pre><code>def LanguageUsed (url):\n    #stuff\n</code></pre>\n\n<p>Which returns a language specifier (e.g. 'en' for English, 'jp' for Japanese, etc...)</p>\n\n<p>Summary of Results:\nI have a reasonable solution working in Python using <a href=\"http://pypi.python.org/pypi/oice.langdet/1.0dev-r781\" rel=\"noreferrer\">code from the PyPi for oice.langdet</a>.\nIt does a decent job in discriminating English vs. Non-English, which is all I require at the moment.  Note that you have to fetch the html using Python urllib.  Also, oice.langdet is GPL license.</p>\n\n<p>For a more general solution using Trigrams in Python as others have suggested, see this <a href=\"http://code.activestate.com/recipes/326576/\" rel=\"noreferrer\">Python Cookbook Recipe from ActiveState</a>.</p>\n\n<p>The Google Natural Language Detection API works very well (if not the best I've seen).  However, it is Javascript and their TOS forbids automating its use.</p>\n",
    "score": 10,
    "creation_date": 1248287626,
    "view_count": 6044,
    "answer_count": 7,
    "tags": "python;url;web;nlp"
  },
  {
    "question_id": 51412095,
    "title": "Spacy - Save custom pipeline",
    "body": "<p>I'm trying to integrate a custom <code>PhraseMatcher()</code> component into my nlp pipeline in a way that will allow me to load the custom Spacy model without having to re-add my custom components to a generic model on each load.</p>\n\n<p>How can I load a Spacy model containing custom pipeline components?</p>\n\n<p>I create the component, add it to my pipeline and save it with the following:</p>\n\n<pre><code>import requests\nfrom spacy.lang.en import English\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.tokens import Doc, Span, Token\n\nclass RESTCountriesComponent(object):\n    name = 'countries'\n    def __init__(self, nlp, label='GPE'):\n        self.countries = [u'MyCountry', u'MyOtherCountry']\n        self.label = nlp.vocab.strings[label]\n        patterns = [nlp(c) for c in self.countries]\n        self.matcher = PhraseMatcher(nlp.vocab)\n        self.matcher.add('COUNTRIES', None, *patterns)        \n    def __call__(self, doc):\n        matches = self.matcher(doc)\n        spans = []\n        for _, start, end in matches:\n            entity = Span(doc, start, end, label=self.label)\n            spans.append(entity)\n        doc.ents = list(doc.ents) + spans\n        for span in spans:\n            span.merge()\n        return doc\n\nnlp = English()\nrest_countries = RESTCountriesComponent(nlp)\nnlp.add_pipe(rest_countries)\nnlp.to_disk('myNlp')\n</code></pre>\n\n<p>I then attempt to load my model with,</p>\n\n<pre><code>nlp = spacy.load('myNlp')\n</code></pre>\n\n<p>But get this error message:</p>\n\n<blockquote>\n  <p>KeyError: u\"[E002] Can't find factory for 'countries'. This usually\n  happens when spaCy calls <code>nlp.create_pipe</code> with a component name\n  that's not built in - for example, when constructing the pipeline from\n  a model's meta.json. If you're using a custom component, you can write\n  to <code>Language.factories['countries']</code> or remove it from the model meta\n  and add it via <code>nlp.add_pipe</code> instead.\"</p>\n</blockquote>\n\n<p>I can't just add my custom components to a generic pipeline in my programming environment. How can I do what I'm trying to do?</p>\n",
    "score": 10,
    "creation_date": 1531954746,
    "view_count": 7635,
    "answer_count": 2,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 5642139,
    "title": "PHP Syllable Detection",
    "body": "<p>I would like to find a way to be able to split a word into syllables with PHP. For example, the word \"nevermore\" ran through detect_syllables(), would return \"nev-er-more.\" Are there any good APIs or something out there?</p>\n",
    "score": 10,
    "creation_date": 1302645732,
    "view_count": 6429,
    "answer_count": 3,
    "tags": "php;nlp"
  },
  {
    "question_id": 42329766,
    "title": "Python NLP British English vs American English",
    "body": "<p>I'm currently working on NLP in python. However, in my corpus, there are both British and American English(realize/realise) I'm thinking to convert British to American. However, I did not find a good tool/package to do that. Any suggestions?</p>\n",
    "score": 10,
    "creation_date": 1487521729,
    "view_count": 9469,
    "answer_count": 4,
    "tags": "python;nlp;nltk;gensim;linguistics"
  },
  {
    "question_id": 10463898,
    "title": "Creating a custom categorized corpus in NLTK and Python",
    "body": "<p>I'm experiencing a bit of a problem which has to do with regular expressions and <code>CategorizedPlaintextCorpusReader</code> in Python.</p>\n\n<p>I want to create a custom categorized corpus and train a Naive-Bayes classifier on it. My issue is the following: I want to have two categories, \"pos\" and \"neg\". The positive files are all in one directory, <code>main_dir/pos/*.txt</code>, and the negative ones are in a separate directory, <code>main_dir/neg/*.txt</code>.</p>\n\n<p>How can I use the <code>CategorizedPlaintextCorpusReader</code> to load and label all the positive files in the pos directory, and do the same for the negative ones?</p>\n\n<p>NB: The setup is absolutely the same as the <code>Movie_reviews</code> corpus (<code>~nltk_data\\corpora\\movie_reviews</code>).</p>\n",
    "score": 10,
    "creation_date": 1336236282,
    "view_count": 4962,
    "answer_count": 1,
    "tags": "python;regex;nlp;nltk"
  },
  {
    "question_id": 76314229,
    "title": "How to download spaCy models in a Poetry managed environment",
    "body": "<p>I am writing a Python Jupyter notebook that does some NLP processing on Italian texts.</p>\n<p>I have installed spaCy 3.5.3 via Poetry and then attempt to run the following code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nload_model = spacy.load('it_core_news_sm')\n</code></pre>\n<p>The <code>import</code> line works as expected, but running <code>spacy.load</code> produces the following error:</p>\n<blockquote>\n<p>OSError: [E050] Can't find model 'it_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\nThe model name is correct as shown on <a href=\"https://spacy.io/models/it\" rel=\"noreferrer\">https://spacy.io/models/it</a></p>\n</blockquote>\n<p>After a web search, I see that a solution is to issue the following command:</p>\n<pre class=\"lang-none prettyprint-override\"><code>python3 -m spacy download it_core_news_sm\n</code></pre>\n<p>After running this command the above code works as expected, however, is there a more 'kosher' way of doing this via Poetry?</p>\n",
    "score": 10,
    "creation_date": 1684841350,
    "view_count": 5411,
    "answer_count": 1,
    "tags": "python;nlp;spacy;python-poetry;virtual-environment"
  },
  {
    "question_id": 50154568,
    "title": "which th best RASA NLU or SNIPS NLU?",
    "body": "<p>I would like to know the difference between <strong>Snips NLU</strong> and <strong>Rasa NLU</strong> ? \nwhich is simpler and powerful ?</p>\n",
    "score": 10,
    "creation_date": 1525348685,
    "view_count": 3813,
    "answer_count": 1,
    "tags": "nlp;rasa-nlu;snips;nlu"
  },
  {
    "question_id": 52205475,
    "title": "Custom sentence segmentation using Spacy",
    "body": "<p>I am new to Spacy and NLP. I'm facing the below issue while doing sentence segmentation using Spacy.</p>\n<p>The text I am trying to tokenise into sentences contains numbered lists (with space between numbering and actual text), like below.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nnlp = spacy.load('en_core_web_sm')\ntext = &quot;This is first sentence.\\nNext is numbered list.\\n1. Hello World!\\n2. Hello World2!\\n3. Hello World!&quot;\ntext_sentences = nlp(text)\nfor sentence in text_sentences.sents:\n    print(sentence.text)\n</code></pre>\n<p>Output (1.,2.,3. are considered as separate lines) is:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>This is first sentence.\n  \nNext is numbered list.\n    \n1.\nHello World!\n \n2.\nHello World2!\n  \n3.\nHello World!\n</code></pre>\n<p>But if there is no space between numbering and actual text, then sentence tokenisation is fine. Like below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nnlp = spacy.load('en_core_web_sm')\ntext = &quot;This is first sentence.\\nNext is numbered list.\\n1.Hello World!\\n2.Hello World2!\\n3.Hello World!&quot;\ntext_sentences = nlp(text)\nfor sentence in text_sentences.sents:\n    print(sentence.text)\n</code></pre>\n<p>Output(desired) is:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>This is first sentence.\n    \nNext is numbered list.\n   \n1.Hello World!\n    \n2.Hello World2!\n    \n3.Hello World!\n</code></pre>\n<p>Please suggest whether we can customise sentence detector to do this.</p>\n",
    "score": 10,
    "creation_date": 1536241414,
    "view_count": 18839,
    "answer_count": 1,
    "tags": "nlp;tokenize;spacy;sentence"
  },
  {
    "question_id": 45679331,
    "title": "Reading .eml files with Python 3.6 using emaildata 0.3.4",
    "body": "<p>I am using python 3.6.1 and I want to read in email files (.eml) for processing. I am using the <a href=\"https://pypi.python.org/pypi/emaildata/0.3.4\" rel=\"noreferrer\">emaildata 0.3.4</a> package, however whenever I try to import the Text class as in the documentation, I get the module errors:</p>\n\n<pre><code>import email\nfrom email.text import Text\n&gt;&gt;&gt; ModuleNotFoundError: No module named 'cStringIO'\n</code></pre>\n\n<p>When I tried to correct using <a href=\"https://stackoverflow.com/questions/28200366/python-3-4-0-email-package-install-importerror-no-module-named-cstringio\">this update</a>, I get the next error relating to <code>mimetools</code></p>\n\n<pre><code>&gt;&gt;&gt; ModuleNotFoundError: No module named 'mimetools'\n</code></pre>\n\n<p>Is it possible to use emaildata 0.3.4 with python 3.6 to parse .eml files? Or are there any other packages I can use to parse .eml files? Thanks</p>\n",
    "score": 10,
    "creation_date": 1502729554,
    "view_count": 16845,
    "answer_count": 1,
    "tags": "python;parsing;nlp;eml"
  },
  {
    "question_id": 34502517,
    "title": "What does NER model to find person names inside a resume/CV?",
    "body": "<p>i just have started with Stanford CoreNLP, I would like to build a custom NER model to find <strong>persons</strong>.</p>\n\n<p>Unfortunately, I did not find a good ner model for italian. I need to find these entities inside a resume/CV document.</p>\n\n<p>The problem here is that document like those can have different structure, for example i can have:</p>\n\n<p><strong>CASE 1</strong></p>\n\n<pre><code>- Name: John\n\n- Surname: Travolta\n\n- Last name: Travolta\n\n- Full name: John Travolta\n\n(so many labels that can represent the entity of the person i need to extract)\n</code></pre>\n\n<p><strong>CASE 2</strong></p>\n\n<pre><code>My name is John Travolta and I was born ...\n</code></pre>\n\n<p>Basically, i can have structured data (with different labels) or a context where i should find these entities.</p>\n\n<p>What is the best approach for this kind of documents? Can a maxent model work in this case?</p>\n\n<hr>\n\n<h2><strong>EDIT @vihari-piratla</strong></h2>\n\n<p>At the moment, i adopt the strategy to find a pattern that has something on the left and something on the right, following this method i have 80/85% to find the entity.</p>\n\n<p>Example:</p>\n\n<pre><code>Name: John\nBirthdate: 2000-01-01\n</code></pre>\n\n<p>It means that i have \"Name:\" on the left of the pattern and a <strong>\\n</strong> on the right (until it finds the <strong>\\n</strong>).\nI can create a very long list of patterns like those. I thought about patterns because i do not need names inside \"other\" context. </p>\n\n<p>For example, if the user writes other names inside a <strong>job experience</strong> i do not need them. Because i am looking for the personal name, not others. With this method i can reduce false positives because i will look at specific patterns not \"general names\".</p>\n\n<p>A problem with this method is that i have a big list of patterns (1 pattern = 1 regex), so it does not scale so well if i add others.</p>\n\n<p>If i can train a NER model with all those patterns it will be awesome, but i should use tons of documents to train it well.</p>\n",
    "score": 10,
    "creation_date": 1451346883,
    "view_count": 4275,
    "answer_count": 4,
    "tags": "nlp;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 33748554,
    "title": "how to speed up NE recognition with stanford NER with python nltk",
    "body": "<p>First I tokenize the file content into sentences and then call Stanford NER on each of the sentences. But this process is really slow. I know if I call it on the whole file content if would be faster, but I'm calling it on each sentence as I want to index each sentence before and after NE recognition. </p>\n\n<pre><code>st = NERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\nfor filename in filelist:\n    sentences = sent_tokenize(filecontent) #break file content into sentences\n    for j,sent in enumerate(sentences): \n        words = word_tokenize(sent) #tokenize sentences into words\n        ne_tags = st.tag(words) #get tagged NEs from Stanford NER\n</code></pre>\n\n<p>This is probably due to calling <code>st.tag()</code> for each sentence, but is there any way to make it run faster?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>The reason that I want to tag sentences separate is that I want to write sentences to a file (like sentence indexing) so that given the ne tagged sentence at a later stage, i can get the unprocessed sentence (i'm also doing lemmatizing here)</p>\n\n<p>file format:</p>\n\n<blockquote>\n  <p>(sent_number, orig_sentence, NE_and_lemmatized_sentence)</p>\n</blockquote>\n",
    "score": 10,
    "creation_date": 1447730264,
    "view_count": 5955,
    "answer_count": 4,
    "tags": "python;nlp;nltk;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 9604460,
    "title": "How to find out the entropy of the English language",
    "body": "<p>How to find out the entropy of the English language by using isolated symbol probabilities of the language?</p>\n",
    "score": 10,
    "creation_date": 1331134806,
    "view_count": 10893,
    "answer_count": 1,
    "tags": "nlp;entropy"
  },
  {
    "question_id": 59730960,
    "title": "Convert NER SpaCy format to IOB format",
    "body": "<p>I have data which is already labelled in SpaCy format. For example:</p>\n\n<pre><code>(\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n(\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]})\n</code></pre>\n\n<p>But I want to try training it with any other NER model, such as BERT-NER, which requires IOB tagging instead. Is there any conversion code from SpaCy data format to IOB?</p>\n\n<p>Thanks!</p>\n",
    "score": 10,
    "creation_date": 1578994541,
    "view_count": 10056,
    "answer_count": 5,
    "tags": "nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 57606940,
    "title": "How to get Index of an Entity in a Sentence in Spacy?",
    "body": "<p>I want to know if there is an elegant way to get the index of an Entity with respect to a Sentence. I know I can get the index of an Entity in a string using <code>ent.start_char</code> and <code>ent.end_char</code>, but that value is with respect to the entire string.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion. Apple just launched a new Credit Card.\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n</code></pre>\n\n<p>I want the Entity <code>Apple</code> in both the sentences to point to start and end indexes 0 and 5 respectively. How can I do that?</p>\n",
    "score": 10,
    "creation_date": 1566468618,
    "view_count": 4173,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 53594690,
    "title": "Is it possible to use spacy with already tokenized input?",
    "body": "<p>I have a sentence that has already been tokenized into words. I want to get the part of speech tag for each word in the sentence. When I check the documentation in SpaCy I realized it starts with the raw sentence. I don't want to do that because in that case, the spacy might end up with a different tokenization. Therefore, I wonder if using spaCy with the list of words (rather than a string) is possible or not ? </p>\n\n<p>Here is an example about my question:</p>\n\n<pre><code># I know that it does the following sucessfully :\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nraw_text = 'Hello, world.'\ndoc = nlp(raw_text)\nfor token in doc:\n    print(token.pos_)\n</code></pre>\n\n<p>But I want to do something similar to the following:</p>\n\n<pre><code>import spacy\nnlp = spacy.load('en_core_web_sm')\ntokenized_text = ['Hello',',','world','.']\ndoc = nlp(tokenized_text)\nfor token in doc:\n    print(token.pos_)\n</code></pre>\n\n<p>I know, it doesn't work, but is it possible to do something similar to that ?</p>\n",
    "score": 10,
    "creation_date": 1543843053,
    "view_count": 3706,
    "answer_count": 3,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 60043276,
    "title": "ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters",
    "body": "<p>I wrote a text classification program. When I run the program it crashes with an error as seen in this screenshot:</p>\n<p><a href=\"https://i.sstatic.net/wDoXv.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/wDoXv.jpg\" alt=\"as seen in this screenshot\" /></a></p>\n<blockquote>\n<p>ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.</p>\n</blockquote>\n<p>Here is my code:</p>\n<pre><code>from sklearn.model_selection import train_test_split\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.preprocessing import scale\nfrom sklearn.linear_model import SGDClassifier\nimport nltk, string, json\nimport numpy as np\n\ndef cleanText(corpus):\n    reviews = []\n    for dd in corpus:\n        #for d in dd:\n        try:\n            words = nltk.word_tokenize(dd['description'])\n            words = [w.lower() for w in words]\n            reviews.append(words)\n            #break\n        except:\n            pass\n    return reviews\n\nwith open('C:\\\\NLP\\\\bad.json') as fin:\n    text = json.load(fin)\n    neg_rev = cleanText(text)\n\nwith open('C:\\\\NLP\\\\good.json') as fin:\n    text = json.load(fin)\n    pos_rev = cleanText(text)\n\n#1 for positive sentiment, 0 for negative\ny = np.concatenate((np.ones(len(pos_rev)), np.zeros(len(neg_rev))))\n\nx_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos_rev, neg_rev)), y, test_size=0.2)\n</code></pre>\n<p>The data I am using is available here:</p>\n<ol>\n<li><p><a href=\"https://github.com/SilverYar/TransportDataMiner/blob/master/bad.json\" rel=\"nofollow noreferrer\">Bad</a>;</p>\n</li>\n<li><p><a href=\"https://github.com/SilverYar/TransportDataMiner/blob/master/good.json\" rel=\"nofollow noreferrer\">Good</a></p>\n</li>\n</ol>\n<p>How would I go about fixing this error?</p>\n",
    "score": 10,
    "creation_date": 1580747106,
    "view_count": 72645,
    "answer_count": 8,
    "tags": "python;scikit-learn;nlp"
  },
  {
    "question_id": 44066264,
    "title": "how to choose parameters in TfidfVectorizer in sklearn during unsupervised clustering",
    "body": "<p>TfidfVectorizer provides an easy way to encode &amp; transform texts into vectors.</p>\n\n<p>My question is how to choose the proper values for parameters such as min_df, max_features, smooth_idf, sublinear_tf?</p>\n\n<p>update:</p>\n\n<p>Maybe I should have put more details on the question:</p>\n\n<p>What if I am doing unsupervised clustering with bunch of texts. and I don't have any labels for the texts &amp; I don't know how many clusters there might be (which is actually what I am trying to figure out)</p>\n",
    "score": 10,
    "creation_date": 1495186008,
    "view_count": 19301,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp;tf-idf;tfidfvectorizer"
  },
  {
    "question_id": 37235932,
    "title": "Python langdetect: choose between one language or the other only",
    "body": "<p>I'm using <code>langdetect</code> to determine the language of a set of strings which I know are either in English or French.</p>\n\n<p>Sometimes, <code>langdetect</code> tells me the language is Romanian for a string I know is in French.</p>\n\n<p>How can I make <code>langdetect</code> choose between English or French only, and not all other languages?</p>\n\n<p>Thanks!</p>\n",
    "score": 10,
    "creation_date": 1463299862,
    "view_count": 12381,
    "answer_count": 2,
    "tags": "python;nlp;language-detection"
  },
  {
    "question_id": 28033882,
    "title": "Determining whether a word is a noun or not",
    "body": "<p>Given an input word, I want to determine whether it is a noun or not (in case of ambiguity, for instance <code>cook</code> can be a noun or a verb, the word must be identified as a noun).</p>\n\n<p>Actually I use the POS tagger from the Stanford Parser (i give it a single word as input, and i extract only the POS tag from the result). The results are quite good but it takes a very long time.</p>\n\n<p>Is there a way (in python, please :) to perform this task quicker than what I do actually?</p>\n",
    "score": 10,
    "creation_date": 1421703419,
    "view_count": 18073,
    "answer_count": 4,
    "tags": "python;nlp;stanford-nlp"
  },
  {
    "question_id": 19966345,
    "title": "Identifying verb tenses in python",
    "body": "<p>How can I use Python + NLTK to identify whether a sentence refers to the past/present/future ?</p>\n\n<p>Can I do this only using POS tagging? This seems a bit inaccurate, seems to me that I need to consider the sentence context and not only the words alone.</p>\n\n<p>Any suggestion for another library that can do that?</p>\n",
    "score": 10,
    "creation_date": 1384384127,
    "view_count": 8853,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;nltk;text-processing"
  },
  {
    "question_id": 2829303,
    "title": "Given a document, select a relevant snippet",
    "body": "<p>When I ask a question here, the tool tips for the question returned by the auto search given the first little bit of the question, but a decent percentage of them don't give any text that is any more useful for understanding the question than the title. Does anyone have an idea about how to make a filter to trim out useless bits of a question?</p>\n\n<p>My first idea is to trim any leading sentences that contain only words in some list (for instance, stop words, plus words from the title, plus words from the SO corpus that have very weak correlation with tags, that is that are equally likely to occur in any question regardless of it's tags)</p>\n",
    "score": 10,
    "creation_date": 1273775420,
    "view_count": 3265,
    "answer_count": 1,
    "tags": "statistics;nlp;text-processing;heuristics"
  },
  {
    "question_id": 46536132,
    "title": "How to access topic words only in gensim",
    "body": "<p>I built LDA model using Gensim and I want to get the topic words only How can I get the words of the topics only no probabilities and no IDs.words only </p>\n\n<p>I tried print_topics() and show_topics() functions in gensim but I can't get clean words ! </p>\n\n<p>This is the code I used</p>\n\n<pre><code>dictionary = corpora.Dictionary(doc_clean)\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\nLda = gensim.models.ldamodel.LdaModel\nldamodel = Lda(doc_term_matrix, num_topics=12, id2word = dictionary, passes = 100, alpha='auto', update_every=5)\nx = ldamodel.print_topics(num_topics=12, num_words=5)\nfor i in x:\n    print(i[1])\n    #print('\\n' + str(i))\n\n0.045*تعرض + 0.045*الماضية + 0.045*السنوات + 0.045*وءسرته + 0.045*لءحمد\n0.021*مصر + 0.021*الديمقراطية + 0.021*حرية + 0.021*باسم + 0.021*الحكومة\n0.068*المواطنة + 0.068*الطاءفية + 0.068*وانهيارات + 0.068*رابطة + 0.005*طبول\n0.033*عربية + 0.033*انكسارات + 0.033*رهابيين + 0.033*بحقوق + 0.033*ل\n0.007*وحريات + 0.007*ممنهج + 0.007*قواءم + 0.007*الناس + 0.007*دراج\n0.116*طبول + 0.116*الوطنية + 0.060*يكتب + 0.060*مصر + 0.005*عربية\n0.064*قيم + 0.064*وهن + 0.064*عربيا + 0.064*والتعددية + 0.064*الديمقراطية\n0.036*تضامنا + 0.036*الشخصية + 0.036*مع + 0.036*التفتيش + 0.036*الءخلاق\n0.052*تضامنا + 0.052*كل + 0.052*محمد + 0.052*الخلوق + 0.052*مظلوم\n0.034*بمواطنين + 0.034*رهابية + 0.034*لم + 0.034*عليهم + 0.034*يثبت\n0.035*مع + 0.035*ومستشار + 0.035*يستعيدا + 0.035*ءرهقهما + 0.035*حريتهما\n0.064*للقمع + 0.064*قريبة + 0.064*لا + 0.064*نهاية + 0.064*مصر\n</code></pre>\n\n<p>I tried show_topics and it gave the same output</p>\n\n<pre><code>y = np.array(ldamodel.show_topics(num_topics=12, num_words=5))\nfor i in y[:,1]:\n    #if i != '%d':\n    #print([str(word) for word in i])\n    print(i)\n</code></pre>\n\n<p>If I have the topic ID how can I access its words and other informations </p>\n\n<p>Thanks in Advance</p>\n",
    "score": 10,
    "creation_date": 1506995904,
    "view_count": 9326,
    "answer_count": 6,
    "tags": "python;nlp;gensim;lda;topic-modeling"
  },
  {
    "question_id": 46192144,
    "title": "Named Entity Extraction of dates",
    "body": "<p>I am absolutely new to the NER and Extraction and programming in general. I am trying to figure out a way where I can extract due dates and start date of certain documents. Is there a way to do this? A place where I can start? I have been looking around but the problem  I run into is the same. Can extract dates but not whether the date is due or post. If it only has 1 date, is it post or due. Stuff like that. Any help would be appreciated.</p>\n\n<p>Example: </p>\n\n<p>\"Essay on Medieval Asia was due on September 3rd.\"</p>\n\n<p>\"Your last assignment that was given on April 6th was supposed to be submitted in 10 days.\"</p>\n\n<p>\"The bid is due no later than a month from the date it was posted(today).\"</p>\n",
    "score": 10,
    "creation_date": 1505289918,
    "view_count": 5637,
    "answer_count": 1,
    "tags": "nlp;named-entity-recognition"
  },
  {
    "question_id": 41956362,
    "title": "clustering list of words in python",
    "body": "<p>I am a newbie in text mining, here is my situation.\nSuppose i have a list of words ['car', 'dog', 'puppy', 'vehicle'], i would like to cluster words into k groups, I want the output to be [['car', 'vehicle'], ['dog', 'puppy']].\nI first calculate similarity score of each pairwise word to obtain a 4x4 matrix(in this case) M, where Mij is the similarity score of word i and j. \nAfter transforming the words into numeric data, i utilize different clustering library(such as sklearn) or implement it by myself to get the word clusters.</p>\n\n<p>I want to know does this approach makes sense? Besides, how do I determine the value of k? More importantly, i know that there exist different clustering technique, i am thinking whether i should use k-means or k-medoids for word clustering?</p>\n",
    "score": 10,
    "creation_date": 1485861925,
    "view_count": 18520,
    "answer_count": 3,
    "tags": "python;nlp;cluster-analysis;text-mining"
  },
  {
    "question_id": 21241602,
    "title": "maximum entropy model and logistic regression",
    "body": "<p>I am doing a project that has some Natural Language Processing to do. I am using <a href=\"http://nlp.stanford.edu/downloads/classifier.shtml\" rel=\"noreferrer\">stanford MaxEnt Classifier</a> for the purpose.But I am not sure, whether Maximum entropy model and logistic regression are one at the same or is it some special kind of logistic regression?</p>\n\n<p>Can anyone come up with an explanation?</p>\n",
    "score": 10,
    "creation_date": 1390242981,
    "view_count": 7835,
    "answer_count": 3,
    "tags": "machine-learning;nlp;stanford-nlp;logistic-regression"
  },
  {
    "question_id": 16274623,
    "title": "Can NLTK be used in a Postgres Python Stored Procedure",
    "body": "<p>Has anyone done or even no if its possible to use NLTK within a Postgres Python Stored Procedure or trigger</p>\n",
    "score": 10,
    "creation_date": 1367225397,
    "view_count": 2011,
    "answer_count": 1,
    "tags": "postgresql;nlp;nltk"
  },
  {
    "question_id": 9560972,
    "title": "Render linguistic syntax tree in browser",
    "body": "<p>The input is <strong>either:</strong></p>\n\n<p>(1) a bracketed representation of a tree with labeled internal nodes such as:</p>\n\n<pre><code>(S (N John) (VP (V hit) (NP (D the) (N ball))))\n</code></pre>\n\n<p>with output:</p>\n\n<p><img src=\"https://i.sstatic.net/ZD8S2.jpg\" alt=\"enter image description here\"></p>\n\n<p>(Whether the lines are dashed and whether the caption is present are not significant.)</p>\n\n<p><strong>Or the input could be:</strong></p>\n\n<p>(2) a bracketing on words without labels e.g.:</p>\n\n<pre><code>((John) ((hit) ((the) (ball))))\n</code></pre>\n\n<p>with output same as above (no internal labels this time, just the tree structure).</p>\n\n<p>Another component of the input is whether the tree is labeled as in (1) or unlabeled as in (2).</p>\n\n<hr>\n\n<p><strong>My question:</strong> What is the best way (fastest development time) to render these trees in the browser in javascript? Everything should happen on the client side.</p>\n\n<p>I'm imagining a simple interface with just a textbox (and a radio button specifying whether it is a labeled tree or not), that, when changed, triggers a tree to render (if the input does not have any syntax errors). </p>\n",
    "score": 10,
    "creation_date": 1330914808,
    "view_count": 1224,
    "answer_count": 2,
    "tags": "javascript;rendering;nlp;linguistics"
  },
  {
    "question_id": 47580326,
    "title": "Extracting a part of a Spacy document as a new document",
    "body": "<p>I have a rather long text parsed by <code>Spacy</code> into a <code>Doc</code> instance:</p>\n\n<pre><code>import spacy\n\nnlp = spacy.load('en_core_web_lg')\ndoc = nlp(content)\n</code></pre>\n\n<p><code>doc</code> here becomes a <a href=\"https://spacy.io/api/doc\" rel=\"noreferrer\"><code>Doc</code> class instance</a>. </p>\n\n<p>Now, since the text is huge, I would like to process, experiment and visualize in a Jupyter notebook using only just one part of the document - for instance, first 100 sentences. </p>\n\n<p>How can I slice and create a new <code>Doc</code> instance from a part of the existing document?</p>\n",
    "score": 10,
    "creation_date": 1512066666,
    "view_count": 2464,
    "answer_count": 3,
    "tags": "python;nlp;document;spacy"
  },
  {
    "question_id": 43601358,
    "title": "Empty vocabulary for single letter by CountVectorizer",
    "body": "<p>Trying to convert string into numeric vector, </p>\n\n<pre><code>### Clean the string\ndef names_to_words(names):\n    print('a')\n    words = re.sub(\"[^a-zA-Z]\",\" \",names).lower().split()\n    print('b')\n\n    return words\n\n\n### Vectorization\ndef Vectorizer():\n    Vectorizer= CountVectorizer(\n                analyzer = \"word\",  \n                tokenizer = None,  \n                preprocessor = None, \n                stop_words = None,  \n                max_features = 5000)\n    return Vectorizer  \n\n\n### Test a string\ns = 'abc...'\nr = names_to_words(s)\nfeature = Vectorizer().fit_transform(r).toarray()\n</code></pre>\n\n<p>But when I encoutered:</p>\n\n<pre><code> ['g', 'o', 'm', 'd']\n</code></pre>\n\n<p>There's error:</p>\n\n<pre><code>ValueError: empty vocabulary; perhaps the documents only contain stop words\n</code></pre>\n\n<p>It seems there's a problem with such single-letter string.\nwhat should I do？\nThx</p>\n",
    "score": 10,
    "creation_date": 1493092942,
    "view_count": 4629,
    "answer_count": 1,
    "tags": "python;nlp;vectorization;feature-extraction;countvectorizer"
  },
  {
    "question_id": 42731970,
    "title": "Regex add character to matched string",
    "body": "<p>I have a long string which is a paragraph, however there is no white space after periods. For example:</p>\n\n<pre><code>para = \"I saw this film about 20 years ago and remember it as being particularly nasty. I believe it is based on a true incident: a young man breaks into a nurses\\' home and rapes, tortures and kills various women.It is in black and white but saves the colour for one shocking shot.At the end the film seems to be trying to make some political statement but it just comes across as confused and obscene.Avoid.\"\n</code></pre>\n\n<p>I am trying to use re.sub to solve this problem, but the output is not what I expected.</p>\n\n<p>This is what I did:</p>\n\n<pre><code>re.sub(\"(?&lt;=\\.).\", \" \\1\", para)\n</code></pre>\n\n<p>I am matching the first char of each sentence, and I want to put a white space before it. My match pattern is <code>(?&lt;=\\.).</code>, which (supposedly) checks for any character that appears after a period. I learned from other stackoverflow questions that \\1 matches the last matched pattern, so I wrote my replace pattern as <code>\\1</code>, a space followed by the previously matched string. </p>\n\n<p>Here is the output:</p>\n\n<pre><code>\"I saw this film about 20 years ago and remember it as being particularly nasty. \\x01I believe it is based on a true incident: a young man breaks into a nurses\\' home and rapes, tortures and kills various women. \\x01t is in black and white but saves the colour for one shocking shot. \\x01t the end the film seems to be trying to make some political statement but it just comes across as confused and obscene. \\x01void. \\x01\n</code></pre>\n\n<p>Instead of matching any character preceded by a period and adding a space before it, <code>re.sub</code> replaced the matched character with <code>\\x01</code>. Why? How do I add a character before a matched string?</p>\n",
    "score": 10,
    "creation_date": 1489212422,
    "view_count": 17466,
    "answer_count": 5,
    "tags": "python;regex;nlp"
  },
  {
    "question_id": 32652725,
    "title": "&quot;ImportError: cannot import name StanfordNERTagger&quot; in NLTK",
    "body": "<p>I'm unable to import the NER Stanford Tagger in NLTK. This is what I have done:</p>\n\n<p>Downloaded the java code from <a href=\"http://nlp.stanford.edu/software/CRF-NER.shtml\" rel=\"nofollow\">here</a> \nand added an environment variable <code>STANFORD_MODELS</code> with the path to the folder where the java code is stored.</p>\n\n<p>That should be sufficient according to the information that is provided on the NLTK site. It says: </p>\n\n<p>\"Tagger models need to be downloaded from <a href=\"http://nlp.stanford.edu/software\" rel=\"nofollow\">http://nlp.stanford.edu/software</a> and the STANFORD_MODELS environment variable set (a colon-separated list of paths).\"</p>\n\n<p>Would anybody be kind enough to help me, please?</p>\n\n<p>EDIT: The downloaded folder is located at /Users/-----------/Documents/JavaJuno/stanford-ner-2015-04-20 and contains these files:</p>\n\n<pre><code>LICENSE.txt         lib             ner.sh              stanford-ner-3.5.2-javadoc.jar\nNERDemo.java            ner-gui.bat         sample-conll-file.txt       stanford-ner-3.5.2-sources.jar\nREADME.txt          ner-gui.command         sample-w-time.txt       stanford-ner-3.5.2.jar\nbuild.xml           ner-gui.sh          sample.ner.txt          stanford-ner.jar\nclassifiers         ner.bat             sample.txt\n</code></pre>\n\n<p>Then I have added an environment variable STANFORD_MODELS:</p>\n\n<pre><code>os.environ[\"STANFORD_MODELS\"] = \"/Users/-----------/Documents/JavaJuno/stanford-ner-2015-04-20\"\n</code></pre>\n\n<p>Calling from nltk.tag import StanfordNERTagger yields the error: </p>\n\n<pre><code>ImportError                               Traceback (most recent call last)\n&lt;ipython-input-356-f4287e573edc&gt; in &lt;module&gt;()\n----&gt; 1 from nltk.tag import StanfordNERTagger\n\nImportError: cannot import name StanfordNERTagger\n</code></pre>\n\n<p>Also in case that this may be relevant, this is what is in my nltk.tag folder:</p>\n\n<pre><code>__init__.py api.pyc     crf.py      hmm.pyc     senna.py    sequential.pyc  stanford.py tnt.pyc\n__init__.pyc    brill.py    crf.pyc     hunpos.py   senna.pyc   simplify.py stanford.pyc    util.py\napi.py      brill.pyc   hmm.py      hunpos.pyc  sequential.py   simplify.pyc    tnt.py      util.pyc\n</code></pre>\n\n<p>EDIT2: I have managed to import the NER Tagger, by using:</p>\n\n<pre><code>from nltk.tag.stanford import NERTagger\n</code></pre>\n\n<p>but now when calling an example call from the NLTK website, I get:</p>\n\n<pre><code>In [360]: st = NERTagger('english.all.3class.distsim.crf.ser.gz')\n---------------------------------------------------------------------------\nLookupError                               Traceback (most recent call last)\n&lt;ipython-input-360-0c0ab770b0ff&gt; in &lt;module&gt;()\n----&gt; 1 st = NERTagger('english.all.3class.distsim.crf.ser.gz')\n\n/Library/Python/2.7/site-packages/nltk/tag/stanford.pyc in __init__(self, *args, **kwargs)\n    158 \n    159     def __init__(self, *args, **kwargs):\n--&gt; 160         super(NERTagger, self).__init__(*args, **kwargs)\n    161 \n    162     @property\n\n/Library/Python/2.7/site-packages/nltk/tag/stanford.pyc in __init__(self, path_to_model, path_to_jar, encoding, verbose, java_options)\n     40                 self._JAR, path_to_jar,\n     41                 searchpath=(), url=_stanford_url,\n---&gt; 42                 verbose=verbose)\n     43 \n     44         self._stanford_model = find_file(path_to_model,\n\n/Library/Python/2.7/site-packages/nltk/__init__.pyc in find_jar(name, path_to_jar, env_vars, searchpath, url, verbose)\n    595                     (name, url))\n    596     div = '='*75\n--&gt; 597     raise LookupError('\\n\\n%s\\n%s\\n%s' % (div, msg, div))\n    598 \n    599 ##########################################################################\n\nLookupError: \n\n===========================================================================\n  NLTK was unable to find stanford-ner.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-ner.jar, see:\n    &lt;http://nlp.stanford.edu/software&gt;\n===========================================================================\n</code></pre>\n\n<p>So I have incorrectly set the environment variable. Can anybody help me with that?</p>\n",
    "score": 10,
    "creation_date": 1442581348,
    "view_count": 17530,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 9520501,
    "title": "How do you get the past tense of a verb?",
    "body": "<p>What is the most efficient way to get the past tense of a verb, preferably without using memory heavy NLP frameworks?</p>\n\n<p>e.g. </p>\n\n<ul>\n<li>live to: lived </li>\n<li>try to: tried </li>\n<li>tap to: tapped </li>\n<li>boil to: boiled </li>\n<li>sell to: sold</li>\n</ul>\n\n<p>I wrote something quick myself (stack overflow won't let me self answer) which seems to work for regular verbs (e.g. the first 4 of that list), but not irregular verbs: <a href=\"http://pastebin.com/Txh76Dnb\" rel=\"noreferrer\">http://pastebin.com/Txh76Dnb</a></p>\n\n<p>edit: Thanks for all the responses, it looks like it can't be done properly without a dictionary due to irregular verbs.</p>\n",
    "score": 10,
    "creation_date": 1330620908,
    "view_count": 8437,
    "answer_count": 3,
    "tags": "java;nlp"
  },
  {
    "question_id": 5941580,
    "title": "Is there a search engine that will give a direct answer?",
    "body": "<p>I've been wondering about this for a while and I can't see why Google haven't tried it yet - or maybe they have and I just don't know about it.</p>\n\n<p>Is there a search engine that you can type a question into which will give you a single answer rather than a list of results which you then have to trawl through yourself to find what you want to know?</p>\n\n<p>For example, this is how I would design the system:</p>\n\n<p>User’s input: “Where do you go to get your eyes tested?”</p>\n\n<p>System output: “Opticians. Certainty: 95%”</p>\n\n<p>This would be calculated as follows:</p>\n\n<ol>\n<li>The input is parsed from natural language into a simple search string, probably something like “eye testing” in this case.  The term “Where do you go” would also be interpreted by the system and used when comparing results.</li>\n<li>The search string would be fed into a search engine.</li>\n<li>The system would then compare the contents of the results to find matching words or phrases taking note of what the question is asking (i.e. what, where, who, how etc.)</li>\n<li>Once a suitable answer is determined, the system displays it to the user along with a measure of how sure it is that the answer is correct.</li>\n</ol>\n\n<p>Due to the dispersed nature of the Internet, a correct answer is likely to appear multiple times, especially for simple questions.  For this particular example, it wouldn’t be too hard for the system to recognise that this word keeps cropping up in the results and that it is almost certainly the answer being searched for.</p>\n\n<p>For more complicated questions, a lower certainty would be shown, and possibly multiple results with different levels of certainty.  The user would also be offered the chance to see the sources which the system calculated the results from.</p>\n\n<p>The point of this system is that it simplifies searching.  Many times when we use a search engine, we’re just looking for something really simple or trivial.  Returning a long list of results doesn’t seem like the most efficient way of answering the question, even though the answer is almost certainly hidden away in those results.  </p>\n\n<p>Just take a look at the Google results for the above question to see my point:\n<a href=\"http://www.google.co.uk/webhp?sourceid=chrome-instant&amp;ie=UTF-8&amp;ion=1&amp;nord=1#sclient=psy&amp;hl=en&amp;safe=off&amp;nord=1&amp;site=webhp&amp;source=hp&amp;q=Where%20do%20you%20go%20to%20get%20your%20eyes%20tested%3F&amp;aq=&amp;aqi=&amp;aql=&amp;oq=&amp;pbx=1&amp;fp=72566eb257565894&amp;fp=72566eb257565894&amp;ion=1\" rel=\"nofollow noreferrer\">http://www.google.co.uk/webhp?sourceid=chrome-instant&amp;ie=UTF-8&amp;ion=1&amp;nord=1#sclient=psy&amp;hl=en&amp;safe=off&amp;nord=1&amp;site=webhp&amp;source=hp&amp;q=Where%20do%20you%20go%20to%20get%20your%20eyes%20tested%3F&amp;aq=&amp;aqi=&amp;aql=&amp;oq=&amp;pbx=1&amp;fp=72566eb257565894&amp;fp=72566eb257565894&amp;ion=1</a></p>\n\n<p>The results given don't immediately answer the question - they need to be searched through by the user before the answer they really want is found.  Search engines are great directories.  They're really good for giving you more information about a subject, or telling you where to find a service, but they're not so good at answering direct questions.</p>\n\n<p>There are many aspects that would have to be considered when creating the system – for example a website’s accuracy would have to be taken into account when calculating results.</p>\n\n<p>Although the system should work well for simple questions, it may be quite a task to make it work for more complicated ones.  For example, common misconceptions would need to be handled as a special case.  If the system finds evidence that the user’s question has a common misconception as an answer, it should either point this out when providing the answer, or even simply disregard the most common answer in favour of the one provided by the website that points out that it is a common misconception.  This would all have to be weighed up by comparing the accuracy and quality of conflicting sources.</p>\n\n<p>It's an interesting question and would involve a lot of research, but surely it would be worth the time and effort?  It wouldn't always be right, but it would make simple queries a lot quicker for the user.</p>\n",
    "score": 10,
    "creation_date": 1304969685,
    "view_count": 10958,
    "answer_count": 5,
    "tags": "search;nlp;search-engine;information-retrieval;nlp-question-answering"
  },
  {
    "question_id": 5532363,
    "title": "Python: Tokenizing with phrases",
    "body": "<p>I have blocks of text I want to tokenize, but I don't want to tokenize on whitespace and punctuation, as seems to be the standard with tools like <a href=\"http://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html\" rel=\"noreferrer\">NLTK</a>. There are particular phrases that I want to be tokenized as a single token, instead of the regular tokenization.  </p>\n\n<p>For example, given the sentence \"The West Wing is an American television serial drama created by Aaron Sorkin that was originally broadcast on NBC from September 22, 1999 to May 14, 2006,\" and adding the phrase to the tokenizer \"<a href=\"http://en.wikipedia.org/wiki/The_West_Wing\" rel=\"noreferrer\">the west wing</a>,\" the resulting tokens would be:  </p>\n\n<ul>\n<li>the west wing</li>\n<li>is</li>\n<li>an</li>\n<li>american </li>\n<li>...</li>\n</ul>\n\n<p>What's the best way to accomplish this?  I'd prefer to stay within the bounds of tools like NLTK.</p>\n",
    "score": 10,
    "creation_date": 1301863332,
    "view_count": 11072,
    "answer_count": 3,
    "tags": "python;nlp;tokenize;nltk"
  },
  {
    "question_id": 71481711,
    "title": "OSError for huggingface model",
    "body": "<p>I am trying to use a huggingface model (<a href=\"https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-ca\" rel=\"noreferrer\">CamelBERT</a>), but I am getting an error when loading the tokenizer:\nCode:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(&quot;CAMeL-Lab/bert-base-arabic-camelbert-ca&quot;)\nmodel = AutoModelForMaskedLM.from_pretrained(&quot;CAMeL-Lab/bert-base-arabic-camelbert-ca&quot;)\n</code></pre>\n<p>Error:</p>\n<pre><code>OSError: Can't load config for 'CAMeL-Lab/bert-base-arabic-camelbert-ca'. Make sure that:\n\n- 'CAMeL-Lab/bert-base-arabic-camelbert-ca' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'CAMeL-Lab/bert-base-arabic-camelbert-ca' is the correct path to a directory containing a config.json file\n</code></pre>\n<p>I couldn't run the model because of this error.</p>\n",
    "score": 10,
    "creation_date": 1647344879,
    "view_count": 44301,
    "answer_count": 3,
    "tags": "python;deep-learning;nlp;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 69740911,
    "title": "How to get all documents per topic in bertopic modeling",
    "body": "<p>I have a dataset and trying to convert it to topics using berTopic modeling but the problem is,  i cant get all the docoments of a topic. berTopic is only return 3 docoments per topic.</p>\n<pre><code>topic_model  = BERTopic(verbose=True, embedding_model=embedding_model,\n                                nr_topics = 'auto',\n                                n_gram_range = (3,3),\n                                top_n_words = 10,\n                               calculate_probabilities=True, \n                              seed_topic_list = topic_list,\n                              )\ntopics, probs = topic_model.fit_transform(docs_test)\nrepresentative_doc = topic_model.get_representative_docs(topic#1)\nrepresentative_doc\n</code></pre>\n<p><a href=\"https://i.sstatic.net/KqjRd.png\" rel=\"noreferrer\">this topic contain more then 300 documents but bertopic only shows 3 of them with <code>.get_representative_docs</code></a></p>\n",
    "score": 10,
    "creation_date": 1635346377,
    "view_count": 8526,
    "answer_count": 2,
    "tags": "nlp;text-classification;bert-language-model;topic-modeling"
  },
  {
    "question_id": 64158898,
    "title": "What does Keras Tokenizer num_words specify?",
    "body": "<p>Given this piece of code:</p>\n<pre><code>from tensorflow.keras.preprocessing.text import Tokenizer\n\nsentences = [\n    'i love my dog',\n    'I, love my cat',\n    'You love my dog!'\n]\n\ntokenizer = Tokenizer(num_words = 1)\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nprint(word_index)\n</code></pre>\n<p>whether <code>num_words=1</code> or <code>num_words=100</code>, I get the same output when I run this cell on my jupyter notebook, and I can't seem to understand what difference it makes in tokenization.</p>\n<blockquote>\n<p>{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}</p>\n</blockquote>\n",
    "score": 10,
    "creation_date": 1601567388,
    "view_count": 9422,
    "answer_count": 1,
    "tags": "python;tensorflow;machine-learning;keras;nlp"
  },
  {
    "question_id": 48143769,
    "title": "Spacy NLP library: what is maximum reasonable document size",
    "body": "<p>The following question is about the Spacy NLP library for Python, but I would be surprised if the answer for other libraries differed substantially.</p>\n\n<p>What is the maximum document size that Spacy can handle under reasonable memory conditions (e.g. a 4 GB VM in my case)? I had hoped to use Spacy to search for matches in book-size documents (100K+ tokens), but I'm repeatedly getting crashes that point to memory exhaustion as the cause.</p>\n\n<p>I'm an NLP noob - I know the concepts academically, but I don't really know what to expect out of the state of the art libraries in practice. So I don't know if what I'm asking the library to do is ridiculously hard, or so easy that must be something I've screwed up in my environment.</p>\n\n<p>As far as why I'm using an NLP library instead of something specifically oriented toward document search (e.g. solr), I'm using it because I would like to do lemma-based matching, rather than string-based.</p>\n",
    "score": 10,
    "creation_date": 1515380833,
    "view_count": 10133,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 30309124,
    "title": "software to extract word functions like subject, predicate, object etc",
    "body": "<p>I need to extract relations of the words in a sentence. I'm mostly interested in identifying a subject, predicate and an object. For example, for the follwoing sentence:</p>\n\n<pre><code>She gave him a pen\n</code></pre>\n\n<p>I'd like to have:</p>\n\n<pre><code>She_subject gave_predicate him a pen_object.\n</code></pre>\n\n<p>Is Stanford NLP can do that? I've tried their <code>relation</code> annotator but it didn't seem to work as I expected? Maybe there's other software that can produce this result?</p>\n",
    "score": 10,
    "creation_date": 1431969018,
    "view_count": 19165,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 27807333,
    "title": "Sentence annotation in text without punctuation",
    "body": "<p>I'm having difficulty getting the CoreNLP system to correctly find where one sentence ends and another begins in a corpus of poetry. </p>\n\n<p>The reasons why it's struggling:</p>\n\n<ul>\n<li>some poems have no punctuation throughout their entire length (and sometimes no case)</li>\n<li>some poems have sentences that run from one paragraph into another</li>\n<li>some poems have capitalization at the beginning of every line</li>\n</ul>\n\n<p><a href=\"http://www.poetryfoundation.org/poem/180418\" rel=\"noreferrer\">This is a particularly tricky one</a>\n (The system thought the first sentence ended at the \".\" at the beginning of the second stanza)</p>\n\n<p>Given the lack of capitals and punctuation to go on, I thought that I would try using <em>-tokenizeNLs</em> to see if that improved it, but it went overboard, and cut off any sentence that ran between blank lines (which there are a few of)</p>\n\n<p>These sentences often end at the end of a line, but not always, so what would be slick is if the system could look at a line ending as a potential candidate for a sentence break, and maybe weigh the likelihood of those being the endpoints, but I don't know how I would implement that.</p>\n\n<p>Is there an elegant way to do this? Or an alternative?</p>\n\n<p>Thanks in advance!</p>\n\n<p>(expected sentence output <a href=\"https://www.dropbox.com/s/pt8yoaxvbnucmjy/sentences.txt?dl=0\" rel=\"noreferrer\">here</a>)</p>\n",
    "score": 10,
    "creation_date": 1420579090,
    "view_count": 4577,
    "answer_count": 3,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 4778089,
    "title": "Ways to store and access large (~10 GB) lists in Python?",
    "body": "<p>I have a large set of strings that I'm using for natural language processing research, and I'd like a nice way to store it in Python.</p>\n\n<p>I could use pickle, but loading the entire list into memory would then be an impossibility (I believe), as it's about 10 GB large, and I don't have that much main memory. Currently I have the list stored with the shelve library... The shelf is indexed by strings, \"0\", \"1\", ..., \"n\" which is a bit clunky.</p>\n\n<p>Are there nicer ways to store such an object in a single file, and still have random (ish) access to it?</p>\n\n<p>It may be that the best option is to split it into multiple lists.</p>\n\n<p>Thanks!</p>\n",
    "score": 10,
    "creation_date": 1295836287,
    "view_count": 1786,
    "answer_count": 3,
    "tags": "python;list;nlp;pickle;shelve"
  },
  {
    "question_id": 56675662,
    "title": "How does Beam Search operate on the output of The Transformer?",
    "body": "<p>According to my understanding (please correct me if I'm wrong), Beam Search is BFS where it only explores the \"graph\" of possibilities down <strong>b</strong> the most likely options, where <strong>b</strong> is the beam size. </p>\n\n<p>To calculate/score each option, especially for the work that I'm doing which is in the field of NLP, we basically calculate the score of a possibility by calculating the probability of a token, given everything that comes before it.</p>\n\n<p>This makes sense in a recurrent architecture, where you simply run the model you have with your decoder through the best <strong>b</strong> first tokens, to get the probabilities of the second tokens, for each of the first tokens. Eventually, you get sequences with probabilities and you just pick the one with the highest probability.</p>\n\n<p>However, in the Transformer architecture, where the model doesn't have that recurrence, the output is the entire probability for each word in the vocabulary, for each position in the sequence (batch size, max sequence length, vocab size). How do I interpret this output for Beam Search? I can get the encodings for the input sequence, but since there isn't that recurrence of using the previous output as input for the next token's decoding, how do I go about calculating the probability of all the possible sequences that stems from the best <strong>b</strong> tokens?</p>\n",
    "score": 10,
    "creation_date": 1560977483,
    "view_count": 7477,
    "answer_count": 2,
    "tags": "machine-learning;nlp;beam-search"
  },
  {
    "question_id": 52049511,
    "title": "How to perform clustering on Word2Vec",
    "body": "<p>I have a semi-structured dataset, each row pertains to a single user:</p>\n\n<pre><code>id, skills\n0,\"java, python, sql\"\n1,\"java, python, spark, html\"\n2, \"business management, communication\"\n</code></pre>\n\n<p>Why semi-structured is because the followings skills can only be selected from a list of 580 unique values.</p>\n\n<p>My goal is to cluster users, or find similar users based on similar skillsets. I have tried using a Word2Vec model, which gives me very good results to identify similar skillsets - For eg. </p>\n\n<pre><code>model.most_similar([\"Data Science\"])\n</code></pre>\n\n<p>gives me -</p>\n\n<pre><code>[('Data Mining', 0.9249375462532043),\n ('Data Visualization', 0.9111810922622681),\n ('Big Data', 0.8253220319747925),...\n</code></pre>\n\n<p>This gives me a very good model for identifying <strong>individual</strong> skills and not group of skills. how do I make use of the vector provided from the Word2Vec model to successfully cluster groups of similar users?</p>\n",
    "score": 10,
    "creation_date": 1535425652,
    "view_count": 16786,
    "answer_count": 1,
    "tags": "python;nlp;cluster-analysis;data-mining;word2vec"
  },
  {
    "question_id": 17762516,
    "title": "Methods for extracting locations from text?",
    "body": "<p>What are the recommended methods for extracting locations from free text? </p>\n\n<p>What I can think of is to use regex rules like \"words ... in location\". But are there better approaches than this?</p>\n\n<p>Also I can think of having a lookup hash table table with names for countries and cities and then compare every extracted token from the text to that of the hash table.</p>\n\n<p>Does anybody know of better approaches?</p>\n\n<p>Edit: I'm trying to extract locations from tweets text. So the issue of high number of tweets might also affect my choice for a method.</p>\n",
    "score": 10,
    "creation_date": 1374325127,
    "view_count": 6627,
    "answer_count": 3,
    "tags": "nlp;text-mining;information-extraction;named-entity-recognition;named-entity-extraction"
  },
  {
    "question_id": 17695611,
    "title": "NLTK Context Free Grammar Genaration",
    "body": "<p>I'm working on a non-English parser with Unicode characters. For that, I decided to use NLTK.</p>\n\n<p>But it requires a predefined context-free grammar as below: </p>\n\n<pre><code>  S -&gt; NP VP\n  VP -&gt; V NP | V NP PP\n  PP -&gt; P NP\n  V -&gt; \"saw\" | \"ate\" | \"walked\"\n  NP -&gt; \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n  Det -&gt; \"a\" | \"an\" | \"the\" | \"my\"\n  N -&gt; \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n  P -&gt; \"in\" | \"on\" | \"by\" | \"with\" \n</code></pre>\n\n<p>In my app, I am supposed to minimize hard coding with the use of a rule-based grammar. \nFor example, I can assume any word ending with <strong>-ed</strong> or <strong>-ing</strong> as a verb. So it should work for any given context.</p>\n\n<p>How can I feed such grammar rules to NLTK? Or generate them dynamically using Finite State Machine?</p>\n",
    "score": 10,
    "creation_date": 1374052013,
    "view_count": 17375,
    "answer_count": 5,
    "tags": "python;parsing;nlp;nltk;context-free-grammar"
  },
  {
    "question_id": 5578791,
    "title": "what is the MeCab output and the tagset?",
    "body": "<p>Can someone enlighten me on the MeCab default output? what annotation does the MeCab output and where can i find the tagset for the morpho analyzer</p>\n\n<p><a href=\"http://mecab.sourceforge.net/\" rel=\"noreferrer\">http://mecab.sourceforge.net/</a></p>\n\n<p>can anyone decipher this output from MeCab?</p>\n\n<pre><code>&lt;s&gt;\nブギス・ジャンクション ブギス・ジャンクション ブギス・ジャンクション 名詞-一般       \nに   ニ   に   助詞-格助詞-一般       \nは   ハ   は   助詞-係助詞      \n最も  モットモ    最も  副詞-一般       \n買い  カイ  買う  動詞-自立   五段・ワ行促音便    連用形\n物慣れ モノナレ    物慣れる    動詞-自立   一段  連用形\nし   シ   する  動詞-自立   サ変・スル   連用形\nた   タ   た   助動詞 特殊・タ    基本形\n人々  ヒトビト    人々  名詞-一般       \nを   ヲ   を   助詞-格助詞-一般       \nも   モ   も   助詞-係助詞      \n魅了  ミリョウ    魅了  名詞-サ変接続     \nする  スル  する  動詞-自立   サ変・スル   基本形\n品   シナ  品   名詞-一般       \n揃え  ソロエ 揃える 動詞-自立   一段  連用形\nが   ガ   が   助詞-格助詞-一般       \nあり  アリ  ある  動詞-自立   五段・ラ行   連用形\nます  マス  ます  助動詞 特殊・マス   基本形\n。   。   。   記号-句点       \n&lt;/s&gt;\n</code></pre>\n",
    "score": 10,
    "creation_date": 1302167995,
    "view_count": 2279,
    "answer_count": 1,
    "tags": "nlp;translation;nltk;pos-tagger;mecab"
  },
  {
    "question_id": 74228640,
    "title": "Which HuggingFace summarization models support more than 1024 tokens? Which model is more suitable for programming related articles?",
    "body": "<p>If this is not the best place to ask this question, please lead me to the most accurate one.</p>\n<p>I am planning to use one of the Huggingface summarization models (<a href=\"https://huggingface.co/models?pipeline_tag=summarization\" rel=\"noreferrer\">https://huggingface.co/models?pipeline_tag=summarization</a>) to summarize my lecture video transcriptions.</p>\n<p>So far I have tested <code>facebook/bart-large-cnn</code> and <code>sshleifer/distilbart-cnn-12-6</code>, but they only support a maximum of 1,024 tokens as input.</p>\n<p>So, here are my questions:</p>\n<ol>\n<li><p>Are there any summarization models that support longer inputs such as 10,000 word articles?</p>\n</li>\n<li><p>What are the optimal output lengths for given input lengths? Let's say for a 1,000 word input, what is the optimal (minimum) output length (the min. length of the summarized text)?</p>\n</li>\n<li><p>Which model would likely work on programming related articles?</p>\n</li>\n</ol>\n",
    "score": 10,
    "creation_date": 1666907104,
    "view_count": 6051,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;summarization;huggingface;mlmodel"
  },
  {
    "question_id": 47095673,
    "title": "How to tie word embedding and softmax weights in keras?",
    "body": "<p>Its commonplace for various neural network architectures in NLP and vision-language problems to tie the weights of an initial word embedding layer to that of an output softmax. Usually this produces a boost to sentence generation quality. (see example <a href=\"https://arxiv.org/pdf/1608.05859.pdf\" rel=\"noreferrer\">here</a>)</p>\n\n<p>In Keras its typical to embed word embedding layers using the Embedding class, however there seems to be no easy way to tie the weights of this layer to the output softmax. Would anyone happen to know how this could be implemented ?</p>\n",
    "score": 10,
    "creation_date": 1509711638,
    "view_count": 4052,
    "answer_count": 2,
    "tags": "machine-learning;neural-network;nlp;deep-learning;keras"
  },
  {
    "question_id": 46452020,
    "title": "Sinusoidal embedding - Attention is all you need",
    "body": "<p>In <a href=\"https://arxiv.org/pdf/1706.03762.pdf\" rel=\"noreferrer\">Attention Is All You Need</a>, the authors implement a positional embedding (which adds information about where a word is in a sequence). For this, they use a sinusoidal embedding: </p>\n\n<pre><code>PE(pos,2i) = sin(pos/10000**(2*i/hidden_units))\nPE(pos,2i+1) = cos(pos/10000**(2*i/hidden_units))\n</code></pre>\n\n<p>where pos is the position and i is the dimension. It must result in an embedding matrix of shape [max_length, embedding_size], i.e., given a position in a sequence, it returns the tensor of PE[position,:].</p>\n\n<p>I found the <a href=\"https://github.com/Kyubyong/transformer/blob/master/modules.py#L143\" rel=\"noreferrer\">Kyubyong's</a> implementation, but I do not fully understand it.</p>\n\n<p>I tried to implement it in numpy the following way:</p>\n\n<pre><code>hidden_units = 100 # Dimension of embedding\nvocab_size = 10 # Maximum sentence length\n# Matrix of [[1, ..., 99], [1, ..., 99], ...]\ni = np.tile(np.expand_dims(range(hidden_units), 0), [vocab_size, 1])\n# Matrix of [[1, ..., 1], [2, ..., 2], ...]\npos = np.tile(np.expand_dims(range(vocab_size), 1), [1, hidden_units])\n# Apply the intermediate funcitons\npos = np.multiply(pos, 1/10000.0)\ni = np.multiply(i, 2.0/hidden_units)\nmatrix = np.power(pos, i)\n# Apply the sine function to the even colums\nmatrix[:, 1::2] = np.sin(matrix[:, 1::2]) # even\n# Apply the cosine function to the odd columns\nmatrix[:, ::2] = np.cos(matrix[:, ::2]) # odd\n# Plot\nim = plt.imshow(matrix, cmap='hot', aspect='auto')\n</code></pre>\n\n<p><a href=\"https://i.sstatic.net/1kexj.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/1kexj.png\" alt=\"Result of matrix plot\"></a></p>\n\n<p>I don't understand how this matrix can give information on the position of inputs. Could someone first tell me if this is the right way to compute it and second what is the rationale behind it?</p>\n\n<p>Thank you.</p>\n",
    "score": 10,
    "creation_date": 1506526804,
    "view_count": 13858,
    "answer_count": 2,
    "tags": "python;machine-learning;tensorflow;nlp;deep-learning"
  },
  {
    "question_id": 30585760,
    "title": "Where can I find a corpus of search engine queries?",
    "body": "<p>I'm interested in training a question-answering system on top of user-generated search queries but so far it looks like such data is not made available. Are there some research centers or industry labs that have compiled corpora of search-engine queries?  </p>\n",
    "score": 10,
    "creation_date": 1433207356,
    "view_count": 7236,
    "answer_count": 3,
    "tags": "nlp;search-engine;google-search;bing"
  },
  {
    "question_id": 19626737,
    "title": "Where can I find a text list or library that contains a list of common foods?",
    "body": "<p>I'm writing a Python script that parses emails which involves searching the text of the email for any words that are common food items. I need some way to determine whether words are indeed food items.</p>\n\n<p>I've looked at several natural language processing APIs (such as AlchemyAPI and NLTK 2.0) and they appear to have Named Entity Extraction (which is what I want), but I can't find an entity type for food in particular.</p>\n\n<p>It's quite possible that natural language processing is overkill for what I need-- I just want a list of foods that I can match to. Where can I find such a word list? Do I need to write my own scraper to parse it off some online source, or is there an easier way?</p>\n",
    "score": 10,
    "creation_date": 1382930695,
    "view_count": 10984,
    "answer_count": 3,
    "tags": "python;nlp;nltk;alchemyapi"
  },
  {
    "question_id": 12212015,
    "title": "How to setup neo4j with dBpedia ontop of ruby-on-rails application?",
    "body": "<p>I am trying to use <code>dBpedia</code> with <code>neo4j</code> ontop of <code>ruby on rails</code>.</p>\n\n<p>Assuming I have installed <a href=\"http://www.neo4j.org/download/\" rel=\"nofollow\">neo4j</a> and downloaded one of the <a href=\"http://wiki.dbpedia.org/Downloads38\" rel=\"nofollow\">dBpedia datasets</a>.</p>\n\n<p><strong>How do I import the <code>dbpedia</code> dataset into <code>neo4j</code> ?</strong></p>\n",
    "score": 10,
    "creation_date": 1346403369,
    "view_count": 1785,
    "answer_count": 2,
    "tags": "ruby-on-rails;nlp;jruby;neo4j;dbpedia"
  },
  {
    "question_id": 7851937,
    "title": "extract relationships using NLTK",
    "body": "<p>This is a <a href=\"https://stackoverflow.com/questions/7757554/extract-business-titles-and-time-periods-from-string\">follow-up of my question</a>. I am using nltk to parse out persons, organizations, and their relationships. Using <a href=\"https://gist.github.com/322906/90dea659c04570757cccf0ce1e6d26c9d06f9283\" rel=\"nofollow noreferrer\">this example</a>, I was able to create chunks of persons and organizations; however, I am getting an error in the nltk.sem.extract_rel command:</p>\n\n<pre><code>AttributeError: 'Tree' object has no attribute 'text'\n</code></pre>\n\n<p>Here is the complete code:</p>\n\n<pre><code>import nltk\nimport re\n#billgatesbio from http://www.reuters.com/finance/stocks/officerProfile?symbol=MSFT.O&amp;officerId=28066\nwith open('billgatesbio.txt', 'r') as f:\n    sample = f.read()\n\nsentences = nltk.sent_tokenize(sample)\ntokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\ntagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\nchunked_sentences = nltk.batch_ne_chunk(tagged_sentences)\n\n# tried plain ne_chunk instead of batch_ne_chunk as given in the book\n#chunked_sentences = [nltk.ne_chunk(sentence) for sentence in tagged_sentences]\n\n# pattern to find &lt;person&gt; served as &lt;title&gt; in &lt;org&gt;\nIN = re.compile(r'.+\\s+as\\s+')\nfor doc in chunked_sentences:\n    for rel in nltk.sem.extract_rels('ORG', 'PERSON', doc,corpus='ieer', pattern=IN):\n        print nltk.sem.show_raw_rtuple(rel)\n</code></pre>\n\n<p>This example is very similar to the one <a href=\"http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html#sec-relextract\" rel=\"nofollow noreferrer\">given in the book</a>, but the example uses prepared 'parsed docs,' which appears of nowhere and I don't know where to find its object type. I scoured thru the git libraries as well. Any help is appreciated.</p>\n\n<p>My ultimate goal is to extract persons, organizations, titles (dates) for some companies; then create network maps of persons and organizations.</p>\n",
    "score": 10,
    "creation_date": 1319211970,
    "view_count": 10493,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 26955305,
    "title": "Recognize partial/complete address with NLP framework",
    "body": "<p>I was wondering the amount of work on NLP framework to get partial (without city) or complete postal address extraction with NLP frameworks from unstructured text? Are NLP frameworks efficient to do this? Also, how difficult is it to \"train\" Named Entity Recognition modules to match new locations ?</p>\n",
    "score": 10,
    "creation_date": 1416127833,
    "view_count": 5488,
    "answer_count": 1,
    "tags": "location;nlp;named-entity-recognition"
  },
  {
    "question_id": 15364975,
    "title": "Is there an easy way generate a probable list of words from an unspaced sentence in python?",
    "body": "<p>I have some text: </p>\n\n<pre><code> s=\"Imageclassificationmethodscan beroughlydividedinto two broad families of approaches:\"\n</code></pre>\n\n<p>I'd like to parse this into its individual words. I quickly looked into the enchant and nltk, but didn't see anything that looked immediately useful. If I had time to invest in this, I'd look into writing a dynamic program with enchant's ability to check if a word was english or not. I would have thought there'd be something to do this online, am I wrong? </p>\n",
    "score": 10,
    "creation_date": 1363101136,
    "view_count": 714,
    "answer_count": 2,
    "tags": "python;nlp"
  },
  {
    "question_id": 3058039,
    "title": "Natural Language Processing Solution in Java?",
    "body": "<p>Are there any equally great packages like Python's NTLK in Java world ?</p>\n",
    "score": 10,
    "creation_date": 1276732897,
    "view_count": 1572,
    "answer_count": 5,
    "tags": "java;nlp"
  },
  {
    "question_id": 59590993,
    "title": "where can i download a pretrained word2vec map?",
    "body": "<p>I have been learning about NLP models and came across word embedding, and saw the examples in which it is possible to see relations between words by calculating their dot products and such.</p>\n\n<p>What I am looking for is just a dictionary, mapping words to their representative vectors, so I can play around with it. I know that I can build a model and train it and create my own map but I just want the already trained map as a python variable. </p>\n",
    "score": 10,
    "creation_date": 1578143419,
    "view_count": 12078,
    "answer_count": 1,
    "tags": "python;nlp;word2vec;word-embedding"
  },
  {
    "question_id": 49933974,
    "title": "&#39;No module named spacy&#39; in ipython, but works fine in regular python interpretter",
    "body": "<p>I am currently trying to import spacy using Jupyter Notebooks and running into a problem. Every time I try to import it, it says that it cannot find the module, even though the regular python shell interpreter works just fine.</p>\n\n<p>Information:</p>\n\n<ul>\n<li><p>Conda Environment</p></li>\n<li><p>installed using conda install -c conda-forge spacy</p></li>\n<li><p>shows up in conda list | grep spacy</p></li>\n<li><p>Jupyter can find other packages in the conda env, just not spacy</p></li>\n</ul>\n\n<p>Thank you for any help you can provide.</p>\n\n<p>EDIT: Terminal Commands:</p>\n\n<p><code>1. cd into project directory\n2. conda create -n &lt;env name&gt;\n3. source activate &lt;env name&gt;\n4. conda install -c conda-forge spacy\n5. python -m spacy download en\n6. python\n- import spacy            #works!\n- nlp = spacy.load('en')  #works!\n- quit()\n7. ipython\n- import spacy\nModuleNotFoundError: No module named 'spacy'\n</code></p>\n\n<p>EDIT2:\nFigured it out. My sys.path was different in ipython and wasn't searching through the conda env. I had to run conda install jupyter in the env and then everything worked. Apparently the root jupyter doesn't detect if you're inside an environment or not.</p>\n",
    "score": 10,
    "creation_date": 1524198274,
    "view_count": 14118,
    "answer_count": 4,
    "tags": "python;path;nlp;anaconda;jupyter-notebook"
  },
  {
    "question_id": 44865840,
    "title": "How to treat numbers inside text strings when vectorizing words?",
    "body": "<p>If I have a text string to be vectorized, how should I handle numbers inside it? Or if I feed a Neural Network with numbers and words, how can I keep the numbers as numbers? </p>\n\n<p>I am planning on making a dictionary of all my words (<a href=\"https://github.com/tflearn/tflearn/issues/124#issuecomment-223478145\" rel=\"noreferrer\">as suggested here</a>). In this case all strings will become arrays of numbers. How should I handle characters that are numbers? how to output a vector that does not mix the word index with the number character?</p>\n\n<p>Does converting numbers to strings weakens the information i feed the network?</p>\n",
    "score": 10,
    "creation_date": 1498947360,
    "view_count": 7155,
    "answer_count": 3,
    "tags": "tensorflow;nlp;word2vec;word-embedding"
  },
  {
    "question_id": 37193199,
    "title": "Does NLTK have any pre-trained classifiers for Sentiment Analysis",
    "body": "<p>I was comparing NLTK and Stanford CoreNLP and found out that the latter one had an RNTN (Recursive Tensor Neural Network) implementation provided for Sentiment Analysis. The examples available online show that we do not need to train it as it has already been trained using large datasets like the Penn TreeBank</p>\n\n<p>Does NLTK provide a similar kind of feature?\nThe reason I am asking about it is because whatever implementations for NLTK I found online included training a particular classifer like Naive-Bayes or the MaxEnt.</p>\n\n<p>P.S.: Is it just because Python is easy to use that NLTK is more popular? I am more comfortable in Java so should I opt for Stanford CoreNLP or switch to NLTK + python</p>\n",
    "score": 10,
    "creation_date": 1463072476,
    "view_count": 7461,
    "answer_count": 1,
    "tags": "nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 22897794,
    "title": "Rewriting sentences while retaining semantic meaning",
    "body": "<p>Is it possible to use <a href=\"http://wordnet.princeton.edu/\" rel=\"noreferrer\">WordNet</a> to rewrite a sentence so that the semantic meaning of the sentence still ways the same (or mostly the same)?</p>\n\n<p>Let's say I have this sentence:</p>\n\n<pre><code>Obama met with Putin last week.\n</code></pre>\n\n<ol>\n<li><p>Is it possible to use WordNet to rephrase the sentence into alternatives like:</p>\n\n<pre><code>Obama and Putin met the previous week.\nObama and Putin met each other a week ago.\n</code></pre></li>\n<li><p>If changing the sentence structure is not possible, can WordNet be used to replace only the relevant synonyms?</p>\n\n<p>For example:</p>\n\n<pre><code>Obama met Putin the previous week.\n</code></pre></li>\n</ol>\n",
    "score": 10,
    "creation_date": 1396807169,
    "view_count": 8226,
    "answer_count": 2,
    "tags": "algorithm;nlp;semantics;wordnet"
  },
  {
    "question_id": 1795410,
    "title": "can NLTK/pyNLTK work &quot;per language&quot; (i.e. non-english), and how?",
    "body": "<p>How can I tell NLTK to treat the text in a particular language?</p>\n\n<p>Once in a while I write a specialized NLP routine to do POS tagging, tokenizing and etc. on a non-english (but still hindo-European) text domain.</p>\n\n<p>This question seem to address only different corpora, not the change in code/settings:\n<a href=\"https://stackoverflow.com/questions/1639855/nltk-tagging-in-german\">POS tagging in German</a></p>\n\n<p>Alternatively,are there any specialized Hebrew/Spanish/Polish NLP modules for python?</p>\n",
    "score": 10,
    "creation_date": 1259137052,
    "view_count": 10290,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 57099613,
    "title": "How is teacher-forcing implemented for the Transformer training?",
    "body": "<p>In <a href=\"https://www.tensorflow.org/beta/tutorials/text/transformer#training_and_checkpointing\" rel=\"nofollow noreferrer\">this part</a> of Tensorflow's tutorial, they mentioned that they are training with teacher-forcing. To my knowledge, teacher-forcing involves feeding the target output into the model so that it converges faster. So I'm curious as to how this is done here? The real target is <code>tar_real</code>, and as far as I can see, it is only used to calculate loss and accuracy.</p>\n<p>How is this code implementing teacher-forcing?</p>\n",
    "score": 10,
    "creation_date": 1563469945,
    "view_count": 11726,
    "answer_count": 3,
    "tags": "tensorflow;machine-learning;nlp;transformer-model"
  },
  {
    "question_id": 46173502,
    "title": "Using Keras Tokenizer to generate n-grams",
    "body": "<p>Is it possible to use n-grams in Keras?</p>\n\n<p>E.g., sentences contain in X_train dataframe with \"sentences\" column.</p>\n\n<p>I use tokenizer from Keras in the following manner:</p>\n\n<pre><code>tokenizer = Tokenizer(lower=True, split=' ')\ntokenizer.fit_on_texts(X_train.sentences)\nX_train_tokenized = tokenizer.texts_to_sequences(X_train.sentences)\n</code></pre>\n\n<p>And later I pad the sentences thus:</p>\n\n<pre><code>X_train_sequence = sequence.pad_sequences(X_train_tokenized)\n</code></pre>\n\n<p>Also I use a simple LSTM network:</p>\n\n<pre><code>model = Sequential()\nmodel.add(Embedding(MAX_FEATURES, 128))\nmodel.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2,\n               activation='tanh', return_sequences=True))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2, activation='tanh'))\nmodel.add(Dense(number_classes, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop',\n              metrics=['accuracy'])\n</code></pre>\n\n<p>In this case, tokenizer execution.\nIn Keras docs: <a href=\"https://keras.io/preprocessing/text/\" rel=\"noreferrer\">https://keras.io/preprocessing/text/</a>\nI see character processing is possible, but that is not appropriate for my case.</p>\n\n<p>My main question: Can I use n-grams for NLP tasks (not only Sentiment Analysis but rather any NLP task)</p>\n\n<p>For clarification: I'd like to consider not just words but combination of words.  I'd like to try and see if it helps to model my task.</p>\n",
    "score": 10,
    "creation_date": 1505210533,
    "view_count": 6394,
    "answer_count": 2,
    "tags": "nlp;keras;tokenize;text-processing;n-gram"
  },
  {
    "question_id": 35747245,
    "title": "Bigram vector representations using word2vec",
    "body": "<p>I want to construct word embeddings for documents using the word2vec tool. I know how to find a vector embedding corresponding to a single word (unigram). Now, I want to find a vector for a bigram. Is it possible to construct a bigram word embedding using word2vec? If yes, how?</p>\n",
    "score": 10,
    "creation_date": 1456921621,
    "view_count": 6738,
    "answer_count": 1,
    "tags": "nlp;word2vec;word-embedding"
  },
  {
    "question_id": 15326694,
    "title": "Sentiment Analysis on LARGE collection of online conversation text",
    "body": "<p>The title says it all; I have an SQL database bursting at the seams with online conversation text. I've already done most of this project in Python, so I would like to do this using Python's NLTK library (unless there's a <strong>strong</strong> reason not to).</p>\n\n<p>The data is organized by <strong>Thread</strong>, <strong>Username</strong>, and <strong>Post</strong>. Each thread more or less focuses on discussing one \"product\" of the Category that I am interested in analyzing. Ultimately,  when this is finished, I would like to have an estimated opinion (like/dislike sort of deal) from each user for any of the products they had discussed at some point.</p>\n\n<p>So, what I would like to know:</p>\n\n<p>1) <strong>How can I go about determining what product each thread is about?</strong> I was reading about keyword extraction... is that the correct method?</p>\n\n<p>2) <strong>How do I determine a specific users sentiment based on their posts?</strong> From my limited understanding, I must first \"train\" NLTK to recognize certain indicators of opinion, and then do I simply determine the context of those words when they appear in the text?</p>\n\n<p>As you may have guessed by now, I have no prior experience with NLP. From my reading so far, I think I can handle learning it though. Even just a basic and crude working model for now would be great if someone can point me in the right direction. Google was not very helpful to me.</p>\n\n<p><strong>P.S.</strong> I have permission to analyze this data (in case it matters)</p>\n",
    "score": 10,
    "creation_date": 1362944660,
    "view_count": 5836,
    "answer_count": 2,
    "tags": "python;nlp;nltk;text-mining;sentiment-analysis"
  },
  {
    "question_id": 6136436,
    "title": "Natural Language Processing Toolkit for .NET",
    "body": "<p>Can you give me some toolkits and libraries for natural language processing in .NET.</p>\n\n<p>Are there tools like UIMA for .NET?</p>\n",
    "score": 10,
    "creation_date": 1306402267,
    "view_count": 3796,
    "answer_count": 1,
    "tags": ".net;nlp"
  },
  {
    "question_id": 61049310,
    "title": "How to avoid reloading ML model every time when I call python script?",
    "body": "<p>I have two files, <code>file1.py</code> which have ML model size of 1GB and <code>file2.py</code> which calls <code>get_vec()</code> method from file1 and receives vectors in return. ML <code>model</code> is being loaded everytime when file1 get_vec() method is called. This is where it is taking lots of time (around 10s) to load the model from disk.</p>\n\n<p>I want to tell file1 somehow not to reload model every time but utilize loaded model from earlier calls.</p>\n\n<p>Sample code is as follows</p>\n\n<pre><code># File1.py\n\nimport spacy\nnlp = spacy.load('model')\n\ndef get_vec(post):\n    doc = nlp(post)\n    return doc.vector\n\n</code></pre>\n\n<pre><code>File2.py\n\nfrom File1 import get_vec\n\ndf['vec'] = df['text'].apply(lambda x: get_vec(x))\n\n</code></pre>\n\n<p>So here, it is taking 10 to 12 seconds in each call. This seems small code but it is a part of a large project and I can not put both in the same file. </p>\n\n<p><strong>Update1:</strong></p>\n\n<p>I have done some research and came to know that I can use Redis to store model in cache first time it runs and thereafter I can read the model from cache directly. I tried it for testing with Redis as follows</p>\n\n<pre><code>import spacy\nimport redis\n\nnlp = spacy.load('en_core_web_lg')\nr = redis.Redis(host = 'localhost', port = 6379, db = 0)\nr.set('nlp', nlp)\n</code></pre>\n\n<p>It throws an error</p>\n\n<pre><code>DataError: Invalid input of type: 'English'. Convert to a bytes, string, int or float first.\n</code></pre>\n\n<p>Seems, <code>type(nlp)</code> is <code>English()</code> and it need to convert in a suitable format. So I tried to use pickle as well to convert it. But again, pickle is taking lots of time in encoding and decoding. Is there anyway to store this in Redis?</p>\n\n<p>Can anybody suggest me how can I make it faster? Thanks.</p>\n",
    "score": 10,
    "creation_date": 1586119753,
    "view_count": 7366,
    "answer_count": 5,
    "tags": "python;machine-learning;redis;nlp;spacy"
  },
  {
    "question_id": 59384146,
    "title": "Why do Transformers in Natural Language Processing need a stack of encoders?",
    "body": "<p>I am following this blog on transformers</p>\n\n<p><a href=\"http://jalammar.github.io/illustrated-transformer/\" rel=\"noreferrer\">http://jalammar.github.io/illustrated-transformer/</a></p>\n\n<p>The only thing I don't understand is why there needs to be a stack of encoders or decoders. I understand that the multi-headed attention layers capture different representation spaces of the problem. I don't understand why there needs to be a vertical stack of encoders and decoders. Wouldn't one encoder/decoder layer work?</p>\n",
    "score": 10,
    "creation_date": 1576630646,
    "view_count": 3277,
    "answer_count": 3,
    "tags": "machine-learning;deep-learning;nlp;transformer-model"
  },
  {
    "question_id": 54085997,
    "title": "Is it possible to get a confidence score on Spacy Named-entity recognition",
    "body": "<p>I need to get a confidence score on the predictions done by Spacy NER.<br><br>\n<strong>CSV file</strong></p>\n\n<pre><code>Text,Amount &amp; Nature,Percent of Class\n\"T. Rowe Price Associates, Inc.\",\"28,223,360 (1)\",8.7% (1)\n100 E. Pratt Street,Not Listed,Not Listed\n\"Baltimore, MD 21202\",Not Listed,Not Listed\n\"BlackRock, Inc.\",\"21,871,854 (2)\",6.8% (2)\n55 East 52nd Street,Not Listed,Not Listed\n\"New York, NY 10022\",Not Listed,Not Listed\nThe Vanguard Group,\"21,380,085 (3)\",6.64% (3)\n100 Vanguard Blvd.,Not Listed,Not Listed\n\"Malvern, PA 19355\",Not Listed,Not Listed\nFMR LLC,\"20,784,414 (4)\",6.459% (4)\n245 Summer Street,Not Listed,Not Listed\n\"Boston, MA 02210\",Not Listed,Not Listed\n</code></pre>\n\n<p><strong>Code</strong></p>\n\n<pre><code>import pandas as pd\nimport spacy\nwith open('/path/table.csv') as csvfile:\n    reader1 = csv.DictReader(csvfile)\n    data1 =[[\"Text\",\"Amount &amp; Nature\",\"Prediction\"]]\n    for row in reader1:\n        AmountNature = row[\"Amount &amp; Nature\"]\n        nlp = spacy.load('en_core_web_sm') \n        doc1 = nlp(row[\"Text\"])\n\n        for ent in doc1.ents:\n            #output = [ent.text, ent.start_char, ent.end_char, ent.label_]\n            label1 = ent.label_\n            text1 = ent.text\n        data1.append([str(doc1),AmountNature,label1])\nmy_df1 = pd.DataFrame(data1)\nmy_df1.columns = my_df1.iloc[0]\nmy_df1 = my_df1.drop(my_df1.index[[0]])\nmy_df1.to_csv('/path/output.csv', index=False, header=[\"Text\",\"Amount &amp; Nature\",\"Prediction\"])\n</code></pre>\n\n<p><strong>Output CSV</strong></p>\n\n<pre><code>Text,Amount &amp; Nature,Prediction\n\"T. Rowe Price Associates, Inc.\",\"28,223,360 (1)\",ORG\n100 E. Pratt Street,Not Listed,FAC\n\"Baltimore, MD 21202\",Not Listed,CARDINAL\n\"BlackRock, Inc.\",\"21,871,854 (2)\",ORG\n55 East 52nd Street,Not Listed,LOC\n\"New York, NY 10022\",Not Listed,DATE\nThe Vanguard Group,\"21,380,085 (3)\",ORG\n100 Vanguard Blvd.,Not Listed,FAC\n\"Malvern, PA 19355\",Not Listed,DATE\nFMR LLC,\"20,784,414 (4)\",ORG\n245 Summer Street,Not Listed,CARDINAL\n\"Boston, MA 02210\",Not Listed,GPE\n</code></pre>\n\n<p><strong>Here on the above output, is it possible to get a Confident Score on the Spacy NER prectiction. If yes, how do I achieve that?</strong> \n<p>\nCan someone please help me on this?</p>\n",
    "score": 10,
    "creation_date": 1546927334,
    "view_count": 10841,
    "answer_count": 3,
    "tags": "python;pandas;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 36079383,
    "title": "how could I use complete penn treebank dataset inside python/nltk",
    "body": "<p>I'm trying to learn using <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">NLTK</a> package in python. In particular, I need to use penn tree bank dataset in NLTK. As far as I know, If I call <code>nltk.download('treebank')</code> I can get the 5% of the dataset. However, I have a complete dataset in tar.gz file and I want to use it. In   <a href=\"http://www.nltk.org/howto/corpus.html#parsed-corpora\" rel=\"noreferrer\">here</a> it is said that: </p>\n\n<blockquote>\n  <p>If you have access to a full installation of the Penn Treebank, NLTK\n  can be configured to load it as well. Download the ptb package, and in\n  the directory nltk_data/corpora/ptb place the BROWN and WSJ\n  directories of the Treebank installation (symlinks work as well). Then\n  use the ptb module instead of treebank:</p>\n</blockquote>\n\n<p>So, I opened the python from terminal, imported nltk and typed <code>nltk.download('ptb')</code> . With this command, \"ptb\" directory has been created under my <code>~/nltk_data</code> directory. At the end, now I have <code>~/nltk_data/ptb</code> directory. Inside there, as suggested in the link I gave above, I've put my dataset folder. So this is my final directory hierarchy.</p>\n\n<pre><code>    $: pwd\n    $: ~/nltk_data/corpora/ptb/WSJ\n    $: ls\n    $:00  02  04  06  08  10  12  14  16  18  20  22  24\n      01  03  05  07  09  11  13  15  17  19  21  23  merge.log\n</code></pre>\n\n<p>Inside all of the folders from 00 to 24, there are many  <code>.mrg</code> files such as <code>wsj_0001.mrg , wsj_0002.mrg</code>  and so on.</p>\n\n<p>Now, Lets return my question. Again, according to <a href=\"http://www.nltk.org/howto/corpus.html#parsed-corpora\" rel=\"noreferrer\">here</a> :</p>\n\n<p>I should be able to obtain the file ids if I write the followings:</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.corpus import ptb\n&gt;&gt;&gt; print(ptb.fileids()) # doctest: +SKIP\n['BROWN/CF/CF01.MRG', 'BROWN/CF/CF02.MRG', 'BROWN/CF/CF03.MRG', 'BROWN/CF/CF04.MRG', ...]\n</code></pre>\n\n<p>Unfortunately, when I type <code>print(ptb.fileids())</code> I got empty array.</p>\n\n<pre><code>&gt;&gt;&gt; print(ptb.fileids())\n[]\n</code></pre>\n\n<p>Is there anyone who could help me ? </p>\n\n<p><strong>EDIT</strong>\nhere is the content of my ptb directory and some of allcats.txt file : </p>\n\n<pre><code>   $: pwd\n    $: ~/nltk_data/corpora/ptb\n    $: ls\n    $: allcats.txt  WSJ\n    $: cat allcats.txt\n    $: WSJ/00/WSJ_0001.MRG news\n    WSJ/00/WSJ_0002.MRG news\n    WSJ/00/WSJ_0003.MRG news\n    WSJ/00/WSJ_0004.MRG news\n    WSJ/00/WSJ_0005.MRG news\n\n    and so on ..\n</code></pre>\n",
    "score": 10,
    "creation_date": 1458289269,
    "view_count": 4783,
    "answer_count": 1,
    "tags": "python;nlp;nltk;corpus;penn-treebank"
  },
  {
    "question_id": 13068386,
    "title": "Causal Sentences Extraction Using NLTK python",
    "body": "<p>I am extracting causal sentences from the accident reports on water. I am using NLTK as a tool here. I manually created my regExp grammar by taking 20 causal sentence structures [see examples below]. The constructed grammar is of the type </p>\n\n<pre><code>grammar = r'''Cause: {&lt;DT|IN|JJ&gt;?&lt;NN.*|PRP|EX&gt;&lt;VBD&gt;&lt;NN.*|PRP|VBD&gt;?&lt;.*&gt;+&lt;VBD|VBN&gt;?&lt;.*&gt;+}'''\n</code></pre>\n\n<p>Now the grammar has 100% recall on the test set ( I built my own toy dataset with 50 causal and 50 non causal sentences) but a low precision. I would like to ask about: </p>\n\n<ol>\n<li>How to train NLTK to build the regexp grammar automatically for\nextracting particular type of sentences.</li>\n<li><p>Has any one ever tried to extract causal sentences. Example\ncausal sentences are:  </p>\n\n<ul>\n<li><p>There was poor sanitation in the village, as a consequence, she had\nhealth problems.</p></li>\n<li><p>The water was impure in her village, For this reason, she suffered\nfrom parasites.</p></li>\n<li><p>She had health problems because of poor sanitation in the village.\nI would want to extract only the above  type of sentences from a\nlarge text.</p></li>\n</ul></li>\n</ol>\n",
    "score": 10,
    "creation_date": 1351167428,
    "view_count": 2807,
    "answer_count": 1,
    "tags": "nlp;nltk"
  },
  {
    "question_id": 48266070,
    "title": "Why word2vec doesn&#39;t use regularization?",
    "body": "<p>ML models with huge number of parameters will tend to overfit (since they have a large variance). In my opinion, <code>word2vec</code> is one such models. One of the ways to reduce the model variance is to apply a regularization technique, which is very common thing for the other embedding models, such as matrix factorization. However, the basic version of <code>word2vec</code> doesn't have any regularization part. Is there a reason for this?</p>\n",
    "score": 10,
    "creation_date": 1516030430,
    "view_count": 2303,
    "answer_count": 1,
    "tags": "machine-learning;nlp;word2vec;embedding;regularized"
  },
  {
    "question_id": 31386224,
    "title": "What created `maxent_treebank_pos_tagger/english.pickle`?",
    "body": "<p>The <code>nltk</code> package's built-in part-of-speech tagger does not seem to be optimized for my use-case (<a href=\"https://stackoverflow.com/questions/31349851/provoke-the-nltk-part-of-speech-tagger-to-report-a-plural-proper-noun\">here, for instance</a>).  The <a href=\"http://www.nltk.org/_modules/nltk/tag.html\" rel=\"nofollow noreferrer\">source code here</a> shows that it's using a saved, pre-trained classifier called <code>maxent_treebank_pos_tagger</code>.  </p>\n\n<p>What created <code>maxent_treebank_pos_tagger/english.pickle</code>?  I'm guessing that there is a tagged corpus out there somewhere that was used to train this tagger, so I think I'm looking for (a) that tagged corpus and (b) the exact code that trains the tagger based on the tagged corpus.</p>\n\n<p>In addition to lots of googling, so far I tried to look at the <code>.pickle</code> object directly to find any clues inside it, starting like this</p>\n\n<pre><code>from nltk.data import load\nx = load(\"nltk_data/taggers/maxent_treebank_pos_tagger/english.pickle\")\ndir(x)\n</code></pre>\n",
    "score": 10,
    "creation_date": 1436798034,
    "view_count": 1819,
    "answer_count": 1,
    "tags": "python-2.7;nlp;nltk;part-of-speech"
  },
  {
    "question_id": 23862526,
    "title": "Python interface to ARPA files",
    "body": "<p>I'm looking for a pythonic interface to load ARPA files (back-off language models) and use them to evaluate some text, e.g. get its log-probability, perplexity etc.</p>\n\n<p>I don't need to generate the ARPA file in Python, only to use it for querying.</p>\n\n<p>Does anybody have a recommended package?\nI already saw <a href=\"https://github.com/kpu/kenlm\">kenlm</a> and <a href=\"https://github.com/desilinguist/swig-srilm\">swig-srilm</a>, but the first is very hard to set up in Windows and the second seems un-maintained anymore.</p>\n",
    "score": 10,
    "creation_date": 1401077103,
    "view_count": 2683,
    "answer_count": 2,
    "tags": "python;nlp;n-gram;language-model"
  },
  {
    "question_id": 3216440,
    "title": "Probabilistic Generation of Semantic Networks",
    "body": "<p>I've studied some simple semantic network implementations and basic techniques for parsing natural language. However, I haven't seen many projects that try and bridge the gap between the two.</p>\n\n<p>For example, consider the dialog:</p>\n\n<pre><code>\"the man has a hat\"\n\"he has a coat\"\n\"what does he have?\" =&gt; \"a hat and coat\"\n</code></pre>\n\n<p>A simple semantic network, based on the grammar tree parsing of the above sentences, might look like:</p>\n\n<pre><code>the_man = Entity('the man')\nhas = Entity('has')\na_hat = Entity('a hat')\na_coat = Entity('a coat')\nRelation(the_man, has, a_hat)\nRelation(the_man, has, a_coat)\nprint the_man.relations(has) =&gt; ['a hat', 'a coat']\n</code></pre>\n\n<p>However, this implementation assumes the prior knowledge that the text segments \"the man\" and \"he\" refer to the same network entity.</p>\n\n<p>How would you design a system that \"learns\" these relationships between segments of a semantic network? I'm used to thinking about ML/NL problems based on creating a simple training set of attribute/value pairs, and feeding it to a classification or regression algorithm, but I'm having trouble formulating this problem that way.</p>\n\n<p>Ultimately, it seems I would need to overlay probabilities on top of the semantic network, but that would drastically complicate an implementation. Is there any prior art along these lines? I've looked at a few libaries, like NLTK and OpenNLP, and while they have decent tools to handle symbolic logic and parse natural language, neither seems to have any kind of proabablilstic framework for converting one to the other.</p>\n",
    "score": 10,
    "creation_date": 1278708436,
    "view_count": 769,
    "answer_count": 3,
    "tags": "machine-learning;data-mining;nlp"
  },
  {
    "question_id": 54992220,
    "title": "Text classification beyond the keyword dependency and inferring the actual meaning",
    "body": "<p>I am trying to develop a text classifier that will classify a piece of text as <strong>Private</strong> or <strong>Public</strong>. Take medical or health information as an example domain. A typical classifier that I can think of considers keywords as the main distinguisher, right? What about a scenario like bellow? What if both of the pieces of text contains similar keywords but carry a different meaning. </p>\n\n<p><strong>Following piece of text is revealing someone's private (health) situation (the patient has cancer):</strong>  </p>\n\n<p>I've been to two <code>clinics</code> and my <code>pcp</code>. I've had an <code>ultrasound</code> only to be told it's a resolving <code>cyst</code> or a <code>hematoma</code>, but it's getting larger and starting to make my leg <code>ache</code>. The <code>PCP</code> said it can't be a <code>cyst</code> because it started out way too big and I swear I have NEVER <code>injured</code> my leg, not even a <code>bump</code>. I am now scared and afraid of <code>cancer</code>. I noticed a slightly uncomfortable sensation only when squatting down about 9 months ago. 3 months ago I went to squat down to put away laundry and it kinda <code>hurt</code>. The <code>pain</code> prompted me to examine my <code>leg</code> and that is when I noticed a <code>lump</code> at the bottom of my calf <code>muscle</code> and flexing only made it more noticeable. Eventually after four <code>clinic</code> visits, an <code>ultrasound</code> and one <code>pcp</code> the result seems to be positive and the mass is getting larger.<br>\n<strong>[Private] (Correct Classification)</strong></p>\n\n<p><strong>Following piece of text is a comment from a doctor which is definitely not revealing is health situation. It introduces the weaknesses of a typical classifier model:</strong>  </p>\n\n<p>Don’t be scared and do not assume anything bad as <code>cancer</code>. I have gone through several cases in my <code>clinic</code> and it seems familiar to me. As you mentioned it might be a <code>cyst</code> or a <code>hematoma</code> and it's getting larger, it must need some additional <code>diagnosis</code> such as <code>biopsy</code>. Having an <code>ache</code> in that area or the size of the <code>lump</code> does not really tells anything <code>bad</code>. You should visit specialized <code>clinics</code> few more times and go under some specific tests such as <code>biopsy</code>, <code>CT scan</code>, <code>pcp</code> and <code>ultrasound</code> before that <code>lump</code> become more larger.<br>\n<strong>[Private] (Which is the Wrong Classification. It should be [Public])</strong>  </p>\n\n<p>The second paragraph was classified as private by all of my current classifiers, for obvious reason. Similar keywords, valid word sequences, the presence of subjects seemed to make the classifier very confused. Even, both of the content contains subjects like <code>I</code>, <code>You</code> (Noun, Pronouns) etc. I thought about from Word2Vec to Doc2Vec, from Inferring meaning to semantic embeddings but can't think about a solution approach that best suits this problem.</p>\n\n<p>Any idea, which way I should handle the classification problem? Thanks in advance. </p>\n\n<p><strong>Progress so Far:</strong><br>\nThe data, I have collected from a public source where patients/victims usually post their own situation and doctors/well-wishers reply to those. I assumed while crawling is that - posts belongs to my private class and comments belongs to public class. All to gether I started with 5K+5K posts/comments and got around 60% with a naive bayes classifier without any major preprocessing. I will try Neural Network soon. But before feeding into any classifier, I just want to know how I can preprocess better to put reasonable weights to either class for better distinction.</p>\n",
    "score": 10,
    "creation_date": 1551736824,
    "view_count": 656,
    "answer_count": 3,
    "tags": "python;text-classification;nlp"
  },
  {
    "question_id": 43649359,
    "title": "Machine Learning Libraries For Android",
    "body": "<p>I am trying build a small text mining tool for my android app. I am checking for a machine learning library that will allow me to cluster, classify etc.</p>\n\n<p>Are there any machine learning libraries for android? I came across tensorflow but I need a bit more access to common ML functions.</p>\n",
    "score": 10,
    "creation_date": 1493271232,
    "view_count": 5475,
    "answer_count": 3,
    "tags": "android;machine-learning;nlp;artificial-intelligence;text-mining"
  },
  {
    "question_id": 26795820,
    "title": "Getting the basic form of the english word",
    "body": "<p>I am trying to get the basic english word for an english word which is modified from its base form. This question had been asked here, but I didnt see a proper answer, so I am trying to put it this way. I tried 2 stemmers and one lemmatizer from NLTK package which are porter stemmer, snowball stemmer, and wordnet lemmatiser.</p>\n\n<p>I tried this code:</p>\n\n<pre><code>from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nwords = ['arrival','conclusion','ate']\n\nfor word in words:\n    print \"\\n\\nOriginal Word =&gt;\", word\n    print \"porter stemmer=&gt;\", PorterStemmer().stem(word)\n    snowball_stemmer = SnowballStemmer(\"english\")\n    print \"snowball stemmer=&gt;\", snowball_stemmer.stem(word)\n    print \"WordNet Lemmatizer=&gt;\", WordNetLemmatizer().lemmatize(word)\n</code></pre>\n\n<p>This is the output I get:</p>\n\n<pre><code>Original Word =&gt; arrival\nporter stemmer=&gt; arriv\nsnowball stemmer=&gt; arriv\nWordNet Lemmatizer=&gt; arrival\n\n\nOriginal Word =&gt; conclusion\nporter stemmer=&gt; conclus\nsnowball stemmer=&gt; conclus\nWordNet Lemmatizer=&gt; conclusion\n\n\nOriginal Word =&gt; ate\nporter stemmer=&gt; ate\nsnowball stemmer=&gt; ate\nWordNet Lemmatizer=&gt; ate\n</code></pre>\n\n<p>but I want this output</p>\n\n<pre><code>    Input : arrival\n    Output: arrive\n\n    Input : conclusion\n    Output: conclude\n\n    Input : ate\n    Output: eat \n</code></pre>\n\n<p>How can I achieve this? Are there any tools already available for this? This is called as morphological analysis. I am aware of that, but there must be some tools which are already achieving this. Help is appreciated :)</p>\n\n<p><strong>First Edit</strong></p>\n\n<p>I tried this code</p>\n\n<pre><code>import nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet as wn\n\nquery = \"The Indian economy is the worlds tenth largest by nominal GDP and third largest by purchasing power parity\"\n\ndef is_noun(tag):\n    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n\ndef is_verb(tag):\n    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n\ndef is_adverb(tag):\n    return tag in ['RB', 'RBR', 'RBS']\n\ndef is_adjective(tag):\n    return tag in ['JJ', 'JJR', 'JJS']\n\ndef penn_to_wn(tag):\n    if is_adjective(tag):\n        return wn.ADJ\n    elif is_noun(tag):\n        return wn.NOUN\n    elif is_adverb(tag):\n        return wn.ADV\n    elif is_verb(tag):\n        return wn.VERB\n    return wn.NOUN\n\ntags = nltk.pos_tag(word_tokenize(query))\nfor tag in tags:\n    wn_tag = penn_to_wn(tag[1])\n    print tag[0]+\"---&gt; \"+WordNetLemmatizer().lemmatize(tag[0],wn_tag)\n</code></pre>\n\n<p>Here, I tried to use wordnet lemmatizer by providing proper tags. Here is the output:</p>\n\n<pre><code>The---&gt; The\nIndian---&gt; Indian\neconomy---&gt; economy\nis---&gt; be\nthe---&gt; the\nworlds---&gt; world\ntenth---&gt; tenth\nlargest---&gt; large\nby---&gt; by\nnominal---&gt; nominal\nGDP---&gt; GDP\nand---&gt; and\nthird---&gt; third\nlargest---&gt; large\nby---&gt; by\npurchasing---&gt; purchase\npower---&gt; power\nparity---&gt; parity\n</code></pre>\n\n<p>Still, words like \"arrival\" and \"conclusion\" wont get processed with this approach. Is there any solution for this?</p>\n",
    "score": 10,
    "creation_date": 1415343706,
    "view_count": 8936,
    "answer_count": 2,
    "tags": "python;nlp;text-processing;stemming;morphological-analysis"
  },
  {
    "question_id": 15860695,
    "title": "replacing pronoun with its antecedent using python2.7 and nltk",
    "body": "<p>As the title shows I am trying to look for pronouns in a string and replace it with it's antecedent  like: </p>\n\n<pre><code>[in]: \"the princess looked from the palace, she was happy\".\n[out]: \"the princess looked from the palace, the princess was happy\". \n</code></pre>\n\n<p>I use pos tag to return pronouns  and nouns. I need to know  how to replace without knowing the sentence, meaning how to specify the subject in the sentence to replace the pronoun with it. Any suggestions?</p>\n",
    "score": 10,
    "creation_date": 1365325568,
    "view_count": 3852,
    "answer_count": 1,
    "tags": "python;python-2.7;nlp;nltk"
  },
  {
    "question_id": 4007558,
    "title": "is there is any stemmer available for indian language",
    "body": "<p>is there is any implementation of stemmers for indian languages like(hindi,telugu) are available ....</p>\n",
    "score": 10,
    "creation_date": 1287909485,
    "view_count": 4137,
    "answer_count": 4,
    "tags": "nlp;stemming;indic"
  },
  {
    "question_id": 47725035,
    "title": "Lemmatization with apache lucene",
    "body": "<p>I'm developing a text analysis project using apache lucene. I need to lemmatize some text (transform the words to their canonical forms). I've already written the code that makes stemming. Using it, I am able to convert the following sentence</p>\n<blockquote>\n<p>The stem is the part of the word that never changes even when morphologically inflected; a lemma is the base form of the word. For example, from &quot;produced&quot;, the lemma is &quot;produce&quot;, but the stem is &quot;produc-&quot;. This is because there are words such as production</p>\n</blockquote>\n<p>into</p>\n<blockquote>\n<p>stem part word never chang even when morpholog inflect lemma base form word exampl from produc lemma produc stem produc becaus word product</p>\n</blockquote>\n<p>However, I need to get the base forms of the words: <em>example</em> instead of <em>exampl</em>, <em>produce</em> instead of <em>produc</em>, and so on.</p>\n<p>I am using lucene because it has analyzers for many languages (I need at least English and Russian). I know about <a href=\"https://stanfordnlp.github.io/CoreNLP/\" rel=\"nofollow noreferrer\">Stanford NLP</a> library, but it has no Russian language support.</p>\n<p>So is there any way to do lemmatization for several languages like I do stemming using lucene?</p>\n<p>The simplified version of my code responsible for stemming:</p>\n<pre><code>//Using apache tika to identify the language\nLanguageIdentifier identifier = new LanguageIdentifier(text);\n//getting analyzer according to the language (eg, EnglishAnalyzer for 'en')\nAnalyzer analyzer = getAnalyzer(identifier.getLanguage());\nTokenStream stream = analyzer.tokenStream(&quot;field&quot;, text);\nstream.reset();\nwhile (stream.incrementToken()) {\n    String stem = stream.getAttribute(CharTermAttribute.class).toString();\n    // doing something with the stem\n    System.out.print(stem+ &quot; &quot;);\n}\nstream.end();\nstream.close();\n</code></pre>\n<p><strong>UPDATE:</strong> I found the <a href=\"https://github.com/AKuznetsov/russianmorphology\" rel=\"nofollow noreferrer\">library</a> that does almost what I need (for English and Russian languages) and uses apache lucene (although in its own way), it's definitely worth exploring.</p>\n",
    "score": 10,
    "creation_date": 1512790159,
    "view_count": 4266,
    "answer_count": 2,
    "tags": "java;lucene;nlp;stemming;lemmatization"
  },
  {
    "question_id": 36698769,
    "title": "Chunking with rule-based grammar in spacy",
    "body": "<p>I have this simple example of chunking in nltk.</p>\n\n<p>My data:</p>\n\n<pre><code>data = 'The little yellow dog will then walk to the Starbucks, where he will introduce them to Michael.'\n</code></pre>\n\n<p>...pre-processing ...</p>\n\n<pre><code>data_tok = nltk.word_tokenize(data) #tokenisation\ndata_pos = nltk.pos_tag(data_tok) #POS tagging\n</code></pre>\n\n<p>CHUNKING:</p>\n\n<pre><code>cfg_1 = \"CUSTOMCHUNK: {&lt;VB&gt;&lt;.*&gt;*?&lt;NNP&gt;}\" #should return `walk to the Starbucks`, etc.\nchunker = nltk.RegexpParser(cfg_1)\ndata_chunked = chunker.parse(data_pos)\n</code></pre>\n\n<p>This returns (among other stuff): <code>(CUSTOMCHUNK walk/VB to/TO the/DT Starbucks/NNP)</code>, so it did what I wanted it to do.</p>\n\n<p>Now my question: I want to switch to spacy for my projects. How would I do this in spacy?</p>\n\n<p>I come as far as to tag it (the coarser <code>.pos</code> method will do for me):</p>\n\n<pre><code>from spacy.en import English    \nparser = English()\nparsed_sent = parser(u'The little yellow dog will then walk to the Starbucks, where')\n\ndef print_coarse_pos(token):\n  print(token, token.pos_)\n\nfor sentence in parsed_sent.sents:\n  for token in sentence:\n    print_coarse_pos(token)\n</code></pre>\n\n<p>... which returns the tags and tokens\n<code>The  DET\nlittle  ADJ\nyellow  ADJ\ndog  NOUN\nwill  VERB\nthen  ADV\nwalk  VERB\n...</code></p>\n\n<p>How could I extract chunks with my own grammar?</p>\n",
    "score": 10,
    "creation_date": 1460994645,
    "view_count": 10199,
    "answer_count": 1,
    "tags": "nlp;nltk;text-parsing;spacy"
  },
  {
    "question_id": 11277379,
    "title": "Parsing HTML into sentences - how to handle tables/lists/headings/etc?",
    "body": "<p>How do you go about parsing an HTML page with free text, lists, tables, headings, etc., into sentences?</p>\n\n<p>Take <a href=\"http://en.wikipedia.org/wiki/Neurotransmitter\" rel=\"noreferrer\">this wikipedia page</a> for example. There is/are:</p>\n\n<ul>\n<li>free text: <a href=\"http://en.wikipedia.org/wiki/Neurotransmitter#Discovery\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Neurotransmitter#Discovery</a></li>\n<li>lists: <a href=\"http://en.wikipedia.org/wiki/Neurotransmitter#Actions\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Neurotransmitter#Actions</a></li>\n<li>tables: <a href=\"http://en.wikipedia.org/wiki/Neurotransmitter#Common_neurotransmitters\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Neurotransmitter#Common_neurotransmitters</a></li>\n</ul>\n\n<p>After messing around with the python <a href=\"http://nltk.org/\" rel=\"noreferrer\">NLTK</a>, I want to test out all of these different corpus annotation methods (from <a href=\"http://nltk.googlecode.com/svn/trunk/doc/book/ch11.html#deciding-which-layers-of-annotation-to-include\" rel=\"noreferrer\">http://nltk.googlecode.com/svn/trunk/doc/book/ch11.html#deciding-which-layers-of-annotation-to-include</a>):</p>\n\n<ul>\n<li><strong>Word Tokenization</strong>: The orthographic form of text does not unambiguously identify its tokens. A tokenized and normalized version, in addition to the conventional orthographic version, may be a very convenient resource.</li>\n<li><strong>Sentence Segmentation</strong>: As we saw in Chapter 3, sentence segmentation can be more difficult than it seems. Some corpora therefore use explicit annotations to mark sentence segmentation.</li>\n<li><strong>Paragraph Segmentation</strong>: Paragraphs and other structural elements (headings, chapters, etc.) may be explicitly annotated.</li>\n<li><strong>Part of Speech</strong>: The syntactic category of each word in a document.</li>\n<li><strong>Syntactic Structure</strong>: A tree structure showing the constituent structure of a sentence.</li>\n<li><strong>Shallow Semantics</strong>: Named entity and coreference annotations, semantic role labels.</li>\n<li><strong>Dialogue and Discourse</strong>: dialogue act tags, rhetorical structure</li>\n</ul>\n\n<p>Once you break a document into sentences it seems pretty straightforward. But how do you go about breaking down something like the HTML from that Wikipedia page? I am very familiar with using HTML/XML parsers and traversing the tree, and I have tried just stripping the HTML tags to get the plain text, but because punctuation is missing after HTML is removed, NLTK doesn't parse things like table cells, or even lists, correctly.</p>\n\n<p>Is there some best-practice or strategy for parsing that stuff with NLP? Or do you just have to manually write a parser specific to that individual page?</p>\n\n<p>Just looking for some pointers in the right direction, really want to try this NLTK out!</p>\n",
    "score": 10,
    "creation_date": 1341087604,
    "view_count": 3568,
    "answer_count": 5,
    "tags": "python;html;nlp;nltk;text-segmentation"
  },
  {
    "question_id": 2532285,
    "title": "Is vim able to detect the natural language of a file, then load the correct dictionary?",
    "body": "<p>I am using several languages, and currently I am obliged to indicate to vim with which of these the spell check must be done. Is there a way to set up vim so that it automatically detects the correct one? I vaguely remember that in a previous version of vim, when the spell check was not integrated, the vimspell script made this possible.</p>\n\n<p>It would be even better if this could apply not only to a file but also to a portion of a file, since I frequently mix several languages in a single file. Of course, I would like to avoid to load several dictionaries simultaneously.</p>\n",
    "score": 10,
    "creation_date": 1269763969,
    "view_count": 878,
    "answer_count": 2,
    "tags": "vim;nlp;spell-checking"
  },
  {
    "question_id": 52102961,
    "title": "LDA Topic Model Performance - Topic Coherence Implementation for scikit-learn",
    "body": "<p>I have a question around measuring/calculating topic coherence for LDA models built in scikit-learn. </p>\n\n<p>Topic Coherence is a useful metric for measuring the human interpretability of a given LDA topic model. Gensim's <a href=\"https://radimrehurek.com/gensim/models/coherencemodel.html\" rel=\"noreferrer\">CoherenceModel</a> allows Topic Coherence to be calculated for a given LDA model (several variants are included). </p>\n\n<p>I am interested in leveraging <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\" rel=\"noreferrer\">scikit-learn's LDA</a> rather than <a href=\"https://radimrehurek.com/gensim/models/ldamodel.html\" rel=\"noreferrer\">gensim's LDA</a> for ease of use and documentation (<em>note: I would like to avoid using the gensim to scikit-learn wrapper i.e. actually leverage sklearn’s LDA</em>). From my research, there is seemingly no scikit-learn equivalent to Gensim’s CoherenceModel. </p>\n\n<p>Is there a way to either:</p>\n\n<p><strong>1</strong> - Feed scikit-learn’s LDA model into gensim’s CoherenceModel pipeline, either through manually converting the scikit-learn model into gensim format or through a scikit-learn to gensim wrapper (I have seen the wrapper the other way around) to generate Topic Coherence?</p>\n\n<p>Or</p>\n\n<p><strong>2</strong> - Manually calculate topic coherence from scikit-learn’s LDA model and CountVectorizer/Tfidf matrices?</p>\n\n<p>I have done quite a bit of research on implementations for this use case online but haven’t seen any solutions. The only leads I have are the documented equations from scientific literature.</p>\n\n<p>If anyone has any knowledge on any similar implementations, or if you could point me in the right direction for creating a manual method for this, that would be great. Thank you!</p>\n\n<p>*Side note: I understand that perplexity and log-likelihood are available in scikit-learn for performance measurements, but these are not as predictive from what I have read.</p>\n",
    "score": 10,
    "creation_date": 1535652115,
    "view_count": 10763,
    "answer_count": 1,
    "tags": "scikit-learn;nlp;gensim;lda;topic-modeling"
  },
  {
    "question_id": 76432971,
    "title": "How can I use LangChain Callbacks to log the model calls and answers into a variable",
    "body": "<p>I'm using <a href=\"https://python.langchain.com/en/latest/index.html\" rel=\"nofollow noreferrer\">LangChain</a> to build a NL application. I want the interactions with the LLM to be recorded in a variable I can use for logging and debugging purposes. I have created a very simple chain:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from typing import Any, Dict\n\nfrom langchain import PromptTemplate\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\n\nllm = OpenAI()\nprompt = PromptTemplate.from_template(&quot;1 + {number} = &quot;)\nhandler = MyCustomHandler()\n\nchain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\nchain.run(number=2)\n</code></pre>\n<p>To record what's going on, I have created a custom <a href=\"https://python.langchain.com/en/latest/modules/callbacks/getting_started.html#creating-a-custom-handler\" rel=\"nofollow noreferrer\">CallbackHandler</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyCustomHandler(BaseCallbackHandler):\n    def on_text(self, text: str, **kwargs: Any) -&gt; Any:\n        print(f&quot;Text: {text}&quot;)\n        self.log = text\n  \n    def on_chain_start(\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n    ) -&gt; Any:\n        &quot;&quot;&quot;Run when chain starts running.&quot;&quot;&quot;\n        print(&quot;Chain started running&quot;)\n</code></pre>\n<p>This works more or less as expected, but it has some side effects that I cannot figure out where they are coming from. The output is:</p>\n<p><a href=\"https://i.sstatic.net/EgrL2.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/EgrL2.png\" alt=\"enter image description here\" /></a></p>\n<p>And the <code>handler.log</code> variable contains:</p>\n<p><code>'Prompt after formatting:\\n\\x1b[32;1m\\x1b[1;3m1 + 2 = \\x1b[0m'</code></p>\n<p>Where are the &quot;Prompt after formatting&quot; and the ANSI codes setting the text as green coming from? Can I get rid of them?</p>\n<p>Overall, is there a better way I'm missing to use the callback system to log the application? This seems to be poorly documented.</p>\n",
    "score": 10,
    "creation_date": 1686235018,
    "view_count": 8531,
    "answer_count": 1,
    "tags": "python;nlp;openai-api;langchain"
  },
  {
    "question_id": 13099193,
    "title": "How to integrate &quot;WordNet Domains&quot; into WordNet DB?",
    "body": "<p>I am using <code>WordNet 2.1</code> <code>tool</code> and accessing it <code>pro-grammatically</code> via <code>JAWS</code>(<code>Java API for WordNet Searching</code>).</p>\n\n<p>Today I came across this new thing called <strong>WordNet Domains</strong> which has assigned <code>DOMAIN</code> labels to each word in <code>WordNet</code>.</p>\n\n<p>link:- <a href=\"http://wndomains.fbk.eu/labels.html\" rel=\"nofollow\">http://wndomains.fbk.eu/labels.html</a></p>\n\n<p>I have downloaded the same from above link. Its a zip file.</p>\n\n<p><strong>My question is:-\nHow do I use \"WordNet Domains\" along with \"WordNet\" in Java?</strong></p>\n",
    "score": 10,
    "creation_date": 1351332693,
    "view_count": 1897,
    "answer_count": 2,
    "tags": "java;nlp;wordnet;jaws-wordnet;word-sense-disambiguation"
  },
  {
    "question_id": 57123405,
    "title": "Fastai - how to prediction after use load_learner in cpu",
    "body": "<p>I'm doing a Text Classification (NLP) model using fastai train on googlecolab (gpu) after I load the model using load_learner without any error but when I change the cpu usage, I get an error \"RuntimeError: _th_index_select not supported on CPUType for Half\"\nIs there any way for me to predict cpu usage results?</p>\n\n<pre><code>from fastai import *\nfrom fastai.text import *\nfrom sklearn.metrics import f1_score\ndefaults.device = torch.device('cpu')\n@np_func\ndef f1(inp,targ): return f1_score(targ, np.argmax(inp, axis=-1))\npath = Path('/content/drive/My Drive/Test_fast_ai')\nlearn = load_learner(path)\nlearn.predict(\"so sad\")\n</code></pre>\n\n<hr>\n\n<pre><code>RuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-13-3775eb2bfe91&gt; in &lt;module&gt;()\n----&gt; 1 learn.predict(\"so sad\")\n\n11 frames\n/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   1504         # remove once script supports set_grad_enabled\n   1505         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 1506     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   1507 \n   1508 \n\nRuntimeError: _th_index_select not supported on CPUType for Half\n</code></pre>\n",
    "score": 10,
    "creation_date": 1563614931,
    "view_count": 2325,
    "answer_count": 1,
    "tags": "python;deep-learning;nlp;fast-ai"
  },
  {
    "question_id": 62467650,
    "title": "Re-implementing TF 1.0 sampled_softmax_loss funtion for seq2seq model in to TF 2 Keras model",
    "body": "<p>I have a TF 1.0.1 code of seq2seq model. I am trying to rewrite it using Tensorflow Keras.</p>\n<p><strong>TF 1.0.1 code has following decoder architecure:</strong></p>\n<pre><code>with tf.variable_scope(&quot;decoder_scope&quot;) as decoder_scope:\n\n    # output projection\n    # we need to specify output projection manually, because sampled softmax needs to have access to the the projection matrix \n\n    output_projection_w_t = tf.get_variable(&quot;output_projection_w&quot;, [vocabulary_size, state_size], dtype=DTYPE)\n    output_projection_w = tf.transpose(output_projection_w_t)\n    output_projection_b = tf.get_variable(&quot;output_projection_b&quot;, [vocabulary_size], dtype=DTYPE)\n    \n    decoder_cell = tf.contrib.rnn.LSTMCell(num_units=state_size)\n    decoder_cell = DtypeDropoutWrapper(cell=decoder_cell, output_keep_prob=tf_keep_probabiltiy, dtype=DTYPE)\n    decoder_cell = contrib_rnn.MultiRNNCell(cells=[decoder_cell] * num_lstm_layers, state_is_tuple=True)   \n    \n    # define decoder train netowrk\n    decoder_outputs_tr, _ , _ = dynamic_rnn_decoder( \n        cell=decoder_cell, \n        decoder_fn= simple_decoder_fn_train(last_encoder_state, name=None),\n        inputs=decoder_inputs, \n        sequence_length=decoder_sequence_lengths,\n        parallel_iterations=None,\n        swap_memory=False,\n        time_major=False)\n    \n    # define decoder inference network\n    decoder_scope.reuse_variables()    \n</code></pre>\n<p><strong>Here is how the sampled_softmax_loss is calculated:</strong></p>\n<pre><code>decoder_forward_outputs = tf.reshape(decoder_outputs_tr,[-1, state_size])\ndecoder_target_labels  = tf.reshape(decoder_labels ,[-1, 1]) #decoder_labels is target sequnce of decoder\n\nsampled_softmax_losses = tf.nn.sampled_softmax_loss(\n    weights = output_projection_w_t,\n    biases = output_projection_b,\n    inputs = decoder_forward_outputs,\n    labels = decoder_target_labels , \n    num_sampled = 500,\n    num_classes=vocabulary_size,\n    num_true = 1,\n)    \ntotal_loss_op = tf.reduce_mean(sampled_softmax_losses) \n</code></pre>\n<p><strong>And, this is my decoder in Keras:</strong></p>\n<pre><code>decoder_inputs = tf.keras.Input(shape=(None,), name='decoder_input')\nemb_layer = tf.keras.layers.Embedding(vocabulary_size, state_size)\nx_d = emb_layer(decoder_inputs)\n\nd_lstm_layer = tf.keras.layers.LSTM(embed_dim, return_sequences=True)\nd_lstm_out = d_lstm_layer(x_d, initial_state=encoder_states)\n</code></pre>\n<p><strong>This is my sampled_softmax_loss function I use for Keras model:</strong></p>\n<pre><code>class SampledSoftmaxLoss(object):\n\n  def __init__(self, model):\n    self.model = model\n    output_layer = model.layers[-1]\n    self.input = output_layer.input\n    self.weights = output_layer.weights\n\n  def loss(self, y_true, y_pred, **kwargs):\n    loss = tf.nn.sampled_softmax_loss(\n        weights=self.weights[0],\n        biases=self.weights[1],\n        labels=tf.reshape(y_true ,[-1, 1]),\n        inputs=tf.reshape(d_lstm_out,[-1, state_size]),\n        num_sampled = 500,\n        num_classes = vocabulary_size\n    )\n</code></pre>\n<p>But, it does not work.\nCan anyone help me to implement sampled_loss_funtion in Keras correctly.</p>\n",
    "score": 10,
    "creation_date": 1592560215,
    "view_count": 238,
    "answer_count": 0,
    "tags": "python;tensorflow;keras;nlp"
  },
  {
    "question_id": 501675,
    "title": "Algorithm for separating nonsense text from meaningful text",
    "body": "<p>I provided some of my programs with a feedback function. Unfortunately I forgot to include some sort of spam-protection - so users could send  anything they wanted to my server - where every feedback is stored in a huge db.  </p>\n\n<p>In the beginning I periodically checked those feedbacks - I filtered out what was usable and deleted garbage. The problem is: I get 900 feedbacks per day. Only 4-5 are really useful, the other messages are mostly 2 type of gibberish:</p>\n\n<ul>\n<li>nonsense: jfvgasdjkfahs kdlfjhasdf  (People smashing their heads on the keyboard)</li>\n<li>language i don't understand</li>\n</ul>\n\n<p>What I did so far: </p>\n\n<ol>\n<li><p>I installed a filter to delete any feedback containing \"asdf\", \"qwer\" etc... -> only 700 per day</p></li>\n<li><p>I installed a word filter to delte anything containing bad language -> 600 per day (don't ask - but there are many strange people out there)</p></li>\n<li>I filter out any messages containing letters not being used in my language -> 400 per day</li>\n</ol>\n\n<p>But 400 per day is still way too much. So I'm wondering if anybody has dealt with such a problem before and knows some sort of algorithm to filter out senseless messages.</p>\n\n<p>Any help would really be appreciated!</p>\n",
    "score": 9,
    "creation_date": 1233525948,
    "view_count": 4225,
    "answer_count": 11,
    "tags": "algorithm;filter;cpu-word;nlp;spam"
  },
  {
    "question_id": 12206276,
    "title": "How to check if given word is in plural or singular form?",
    "body": "<p>Question like in topic - I'm trying to do that in python for app in Google App Engine. I know PyEnchant library is used for natural language recognition but I don't see if I can use it for my problem and how.</p>\n",
    "score": 9,
    "creation_date": 1346364557,
    "view_count": 23251,
    "answer_count": 3,
    "tags": "python;nlp"
  },
  {
    "question_id": 10542937,
    "title": "What is Oracle experiment?",
    "body": "<p>I have read a paper about machine learning and it contains an Oracle experiment to compare between his study and another study?\nBut it does not seem to be so clear what is Oracle experiment?</p>\n",
    "score": 9,
    "creation_date": 1336688126,
    "view_count": 6019,
    "answer_count": 1,
    "tags": "nlp;machine-learning"
  },
  {
    "question_id": 196924,
    "title": "How to ensure user submit only english text",
    "body": "<p>I am building a project involving natural language processing, since the nlp module currently only deal with english text, so I have to make sure the user submitted content (not long, only several words) is in english. Are there established ways to achieve this? Python or Javascript way preferred.</p>\n",
    "score": 9,
    "creation_date": 1223883132,
    "view_count": 1250,
    "answer_count": 10,
    "tags": "javascript;python;nlp"
  },
  {
    "question_id": 4663379,
    "title": "implementing a perceptron classifier",
    "body": "<p>Hi I'm pretty new to Python and to NLP. I need to implement a perceptron classifier. I searched through some websites but didn't find enough information. For now I have a number of documents which I grouped according to category(sports, entertainment etc). I also have a list of the most used words in these documents along with their frequencies. On a particular website there was stated that I must have some sort of a decision function accepting arguments x and w. x apparently is some sort of vector ( i dont know what w is). But I dont know how to use the information I have to build the perceptron algorithm and how to use it to classify my documents. Have you got any ideas? Thanks :)</p>\n",
    "score": 9,
    "creation_date": 1294784823,
    "view_count": 8041,
    "answer_count": 5,
    "tags": "python;artificial-intelligence;nlp;machine-learning;perceptron"
  },
  {
    "question_id": 74000154,
    "title": "How do I make sure answers are from a customized (fine-tuning) dataset?",
    "body": "<p>I'm using customized text with 'Prompt' and 'Completion' to train new model.</p>\n<p>Here's the tutorial I used to create customized model from my data:</p>\n<p><a href=\"https://beta.openai.com/docs/guides/fine-tuning/advanced-usage\" rel=\"nofollow noreferrer\">beta.openai.com/docs/guides/fine-tuning/advanced-usage</a></p>\n<p>However even after training the model and sending prompt text to the model, I'm still getting generic results which are not always suitable for me.</p>\n<p>How I can make sure completion results for my prompts will be only from the text I used for the model and not from the generic OpenAI models?</p>\n<p>Can I use some flags to eliminate results from generic models?</p>\n",
    "score": 9,
    "creation_date": 1665259087,
    "view_count": 9410,
    "answer_count": 1,
    "tags": "nlp;customization;openai-api;gpt-3"
  },
  {
    "question_id": 47890052,
    "title": "Improving Gensim Doc2vec results",
    "body": "<p>I tried to apply doc2vec on 600000 rows of sentences: Code as below:</p>\n\n<pre><code>from gensim import models\nmodel = models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1, workers = 5)\nmodel.build_vocab(res)\ntoken_count = sum([len(sentence) for sentence in res])\ntoken_count\n\n%%time\nfor epoch in range(100):\n    #print ('iteration:'+str(epoch+1))\n    #model.train(sentences)\n    model.train(res, total_examples = token_count,epochs = model.iter)\n    model.alpha -= 0.0001  # decrease the learning rate`\n    model.min_alpha = model.alpha  # fix the learning rate, no decay\n</code></pre>\n\n<p>I am getting very poor results with the above implementation. \nthe change I made apart from what was suggested in the tutorial was change the below line:</p>\n\n<pre><code>  model.train(sentences)\n</code></pre>\n\n<p>As:</p>\n\n<pre><code> token_count = sum([len(sentence) for sentence in res])\nmodel.train(res, total_examples = token_count,epochs = model.iter)\n</code></pre>\n",
    "score": 9,
    "creation_date": 1513696820,
    "view_count": 6396,
    "answer_count": 1,
    "tags": "python;nlp;gensim;doc2vec"
  },
  {
    "question_id": 32016545,
    "title": "How does nltk.pos_tag() work?",
    "body": "<p>How does <code>nltk.pos_tag()</code> work? Does it involve any corpus use? I found a source code (<code>nltk.tag</code> - NLTK 3.0 documentation) and it says </p>\n\n<pre><code>_POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'.\n</code></pre>\n\n<p>Loading _POS_TAGGER gives an object:</p>\n\n<pre><code>nltk.tag.sequential.ClassifierBasedPOSTagger\n</code></pre>\n\n<p>, which seems to have no training from corpus. The tagging is incorrect when I use a few adjective in series before a noun (e.g. <em>the quick brown fox</em>). I wonder if I can improve the result by using better tagging method or somehow training with better corpus. Any suggestions? </p>\n",
    "score": 9,
    "creation_date": 1439576885,
    "view_count": 7275,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 8640246,
    "title": "NLTK Performance",
    "body": "<p>Alright, I've been pretty interested in natural language processing recently: however, I've used C until now for most of my work. I heard of NLTK, and I didn't know Python, but it seems quite easy to learn, and it's looking like a really powerful and interesting language. In particular, the NLTK module seems very, very adapted to what I need to do.</p>\n\n<p>However, when using <a href=\"https://stackoverflow.com/questions/526469/practical-examples-of-nltk-use\">sample code for NLTK</a> and pasting that into a file called <code>test.py</code>, I've noticed it takes a very, very long time to run ! </p>\n\n<p>I'm calling it from the shell like so:</p>\n\n<pre><code>time python ./test.py\n</code></pre>\n\n<p>And on a 2.4 GHz machine with 4 GBs of RAM, it takes 19.187 seconds ! </p>\n\n<p>Now, maybe this is absolutely normal, but I was under the impression that NTLK was <em>extremely</em> fast; I may have been mistaken, but is there anything obvious that I'm clearly doing wrong here?</p>\n",
    "score": 9,
    "creation_date": 1324946916,
    "view_count": 5883,
    "answer_count": 2,
    "tags": "python;performance;nlp;nltk"
  },
  {
    "question_id": 53453559,
    "title": "Similarity in Spacy",
    "body": "<p>I am trying to understand how similarity in Spacy works. I tried using Melania Trump's <a href=\"http://time.com/4412008/republican-convention-melania-trump-2/\" rel=\"noreferrer\">speech</a> and Michelle Obama's <a href=\"https://www.npr.org/templates/story/story.php?storyId=93963863\" rel=\"noreferrer\">speech</a> to see how similar they were. </p>\n\n<p>This is my code. </p>\n\n<pre><code>import spacy\nnlp = spacy.load('en_core_web_lg')\n\nfile1 = open(\"melania.txt\").read().decode('ascii', 'ignore')\nfile2 = open(\"michelle.txt\").read().decode('ascii', 'ignore')\n\ndoc1 = nlp(unicode(file1))\ndoc2 = nlp(unicode(file2))\nprint doc1.similarity(doc2)\n</code></pre>\n\n<p>I get the similarity score as 0.9951584208511974. This similarity score looks very high to me. Is this correct? Am I doing something wrong?</p>\n",
    "score": 9,
    "creation_date": 1543012529,
    "view_count": 18790,
    "answer_count": 2,
    "tags": "nlp;similarity;spacy"
  },
  {
    "question_id": 71822208,
    "title": "How do I calculate similarity between two words to detect if they are duplicates?",
    "body": "<p>I have two words and I want to calculate the similarity between them in order to rank them if they are duplicates or not.</p>\n<p>How do I achieve that using deep learning / NLP methods?</p>\n",
    "score": 9,
    "creation_date": 1649645268,
    "view_count": 12453,
    "answer_count": 4,
    "tags": "python;deep-learning;nlp;similarity"
  },
  {
    "question_id": 680907,
    "title": "Is there a free library for morphological analysis of the German language?",
    "body": "<p>I'm looking for a library which can perform a morphological analysis on German words, i.e. it converts any word into its root form and providing meta information about the analysed word.</p>\n\n<p>For example:</p>\n\n<pre><code>gegessen -&gt; essen\nwurde [...] gefasst -&gt; fassen\nHäuser -&gt; Haus\nHunde -&gt; Hund\n</code></pre>\n\n<p>My wishlist:</p>\n\n<ul>\n<li>It has to work with both nouns and verbs.</li>\n<li>I'm aware that this is a very hard task given the complexity of the German language, so I'm also looking for libaries which provide only approximations or may only be 80% accurate.</li>\n<li>I'd prefer libraries which don't work with dictionaries, but again I'm open to compromise given the cirumstances.</li>\n<li>I'd also prefer C/C++/Delphi Windows libraries, because that would make them easier to integrate but .NET, Java, ... will also do.</li>\n<li>It has to be a free library. (L)GPL, MPL, ...</li>\n</ul>\n\n<p><strong>EDIT:</strong> I'm aware that there is no way to perform a morphological analysis without any dictionary at all, because of the irregular words. \nWhen I say, I prefer a library without a dictionary I mean those full blown dictionaries which map each and every word:</p>\n\n<pre><code>arbeite -&gt; arbeiten\narbeitest -&gt; arbeiten\narbeitet -&gt; arbeiten\narbeitete -&gt; arbeiten\narbeitetest -&gt; arbeiten\narbeiteten -&gt; arbeiten\narbeitetet -&gt; arbeiten\ngearbeitet -&gt; arbeiten\narbeite -&gt; arbeiten\n... \n</code></pre>\n\n<p>Those dictionaries have several drawbacks, including the huge size and the inability to process unknown words.</p>\n\n<p>Of course all exceptions can only be handled with a dictionary:</p>\n\n<pre><code>esse -&gt; essen\nisst -&gt; essen\neßt -&gt; essen\naß -&gt; essen\naßt -&gt; essen\naßen -&gt; essen\n...\n</code></pre>\n\n<p>(My mind is spinning right now :) )</p>\n",
    "score": 9,
    "creation_date": 1237974717,
    "view_count": 4142,
    "answer_count": 8,
    "tags": "nlp;languagetool;morphological-analysis"
  },
  {
    "question_id": 60381170,
    "title": "Which Deep Learning Algorithm does Spacy uses when we train Custom model?",
    "body": "<p>When we train custom model, I do see we have dropout and n_iter parameters to tune, but which deep learning algorithm does Spacy Uses to train Custom Models? Also, when Adding new Entity type is it good to create blank or train it on existing model?</p>\n",
    "score": 9,
    "creation_date": 1582565632,
    "view_count": 10611,
    "answer_count": 2,
    "tags": "nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 38839924,
    "title": "How to combine n-grams into one vocabulary in Spark?",
    "body": "<p>Wondering if there is a built-in Spark feature to combine 1-, 2-, n-gram features into a single vocabulary. Setting <code>n=2</code> in <code>NGram</code> followed by invocation of <code>CountVectorizer</code> results in a dictionary containing only 2-grams. What I really want is to combine all frequent 1-grams, 2-grams, etc into one dictionary for my corpus.</p>\n",
    "score": 9,
    "creation_date": 1470698958,
    "view_count": 6247,
    "answer_count": 1,
    "tags": "python;apache-spark;nlp;pyspark;apache-spark-ml"
  },
  {
    "question_id": 27943396,
    "title": "Using WN-Affect to detect emotion/mood of a string",
    "body": "<p>I  downloaded <a href=\"http://wndomains.fbk.eu/wnaffect.html\" rel=\"noreferrer\">WN-Affect</a>. I am however not sure how to use it to detect the mood of a sentence. For example if I have a string \"I hate football.\" I want to be able to detect whether the mood is bad and the emotion is fear. WN-Affect has no tutorial on how to do it, and I am kind of new to python. Any help would be great!</p>\n",
    "score": 9,
    "creation_date": 1421239891,
    "view_count": 9073,
    "answer_count": 2,
    "tags": "python;nlp;nltk;sentiment-analysis;wordnet"
  },
  {
    "question_id": 51369858,
    "title": "Spacy - nlp.pipe() returns generator",
    "body": "<p>I am using Spacy for NLP in Python. I am trying to use <code>nlp.pipe()</code> to generate a list of Spacy doc objects, which I can then analyze. Oddly enough, <code>nlp.pipe()</code> returns an object of the class <code>&lt;generator object pipe at 0x7f28640fefa0&gt;</code>. How can I get it to return a list of docs, as intended?</p>\n<pre><code>import Spacy\nnlp = spacy.load('en_depent_web_md', disable=['tagging', 'parser'])\nmatches = ['one', 'two', 'three']\ndocs = nlp.pipe(matches)\ndocs\n</code></pre>\n",
    "score": 9,
    "creation_date": 1531773915,
    "view_count": 13978,
    "answer_count": 3,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 2940489,
    "title": "PyParsing: What does Combine() do?",
    "body": "<p>What is the difference between:</p>\n\n<pre><code>foo = TOKEN1 + TOKEN2\n</code></pre>\n\n<p>and</p>\n\n<pre><code>foo = Combine(TOKEN1 + TOKEN2)\n</code></pre>\n\n<p>Thanks. </p>\n\n<p><strong>UPDATE</strong>: Based on my experimentation, it seems like <code>Combine()</code> is for terminals, where you're trying to build an expression to match on, whereas plain <code>+</code> is for non-terminals. But I'm not sure.</p>\n",
    "score": 9,
    "creation_date": 1275256264,
    "view_count": 4223,
    "answer_count": 1,
    "tags": "python;parsing;nlp;pyparsing"
  },
  {
    "question_id": 76379440,
    "title": "How to see the Embedding of the documents with Chroma (or any other DB) saved in Lang Chain?",
    "body": "<p>I can see everything but the Embedding of the documents when I used <code>Chroma</code> with <code>Langchain</code> and <code>OpenAI</code> embeddings. It always show me <code>None</code> for that</p>\n<p>Here is the code:</p>\n<pre><code>for db_collection_name in tqdm([&quot;class1-sub2-chap3&quot;, &quot;class2-sub3-chap4&quot;]):\n    documents = []\n    doc_ids = []\n\n    for doc_index in range(3):\n        cl, sub, chap = db_collection_name.split(&quot;-&quot;)\n        content = f&quot;This is {db_collection_name}-doc{doc_index}&quot;\n        doc = Document(page_content=content, metadata={&quot;chunk_num&quot;: doc_index, &quot;chapter&quot;:chap, &quot;class&quot;:cl, &quot;subject&quot;:sub})\n        documents.append(doc)\n        doc_ids.append(str(doc_index))\n\n\n    # # Initialize a Chroma instance with the original document\n    db = Chroma.from_documents(\n         collection_name=db_collection_name,\n         documents=documents, ids=doc_ids,\n         embedding=embeddings, \n         persist_directory=&quot;./data&quot;)\n    \n     db.persist()\n</code></pre>\n<p>when I do <code>db.get()</code>, I see everything as expected except <code>embedding</code> is <code>None</code>.</p>\n<pre><code>{'ids': ['0', '1', '2'],\n 'embeddings': None,\n 'documents': ['This is class1-sub2-chap3-doc0',\n  'This is class1-sub2-chap3-doc1',\n  'This is class1-sub2-chap3-doc2'],\n 'metadatas': [{'chunk_num': 0,\n   'chapter': 'chap3',\n   'class': 'class1',\n   'subject': 'sub2'},\n  {'chunk_num': 1, 'chapter': 'chap3', 'class': 'class1', 'subject': 'sub2'},\n  {'chunk_num': 2, 'chapter': 'chap3', 'class': 'class1', 'subject': 'sub2'}]}\n</code></pre>\n<p>My <code>embeddings</code> is also working fine as it returns:</p>\n<pre><code>len(embeddings.embed_documents([&quot;EMBED THIS&quot;])[0])\n&gt;&gt; 1536\n</code></pre>\n<p>also, in my <code>./data</code> directory I have Embedding file as <code>chroma-embeddings.parquet</code></p>\n<hr />\n<p>I tried the example with example given in document but it shows <code>None</code> too</p>\n<pre><code># Import Document class\nfrom langchain.docstore.document import Document\n\n# Initial document content and id\ninitial_content = &quot;This is an initial document content&quot;\ndocument_id = &quot;doc1&quot;\n\n# Create an instance of Document with initial content and metadata\noriginal_doc = Document(page_content=initial_content, metadata={&quot;page&quot;: &quot;0&quot;})\n\n# Initialize a Chroma instance with the original document\nnew_db = Chroma.from_documents(\n    collection_name=&quot;test_collection&quot;,\n    documents=[original_doc],\n    embedding=OpenAIEmbeddings(),  # using the same embeddings as before\n    ids=[document_id],\n)\n</code></pre>\n<p>Here also <code>new_db.get()</code> gives me <code>None</code></p>\n",
    "score": 9,
    "creation_date": 1685603260,
    "view_count": 25437,
    "answer_count": 2,
    "tags": "python;nlp;openai-api;langchain;chromadb"
  },
  {
    "question_id": 1902967,
    "title": "NLTK - how to find out what corpora are installed from within python?",
    "body": "<p>I'm trying to load some corpora I installed with the NLTK installer but I got a:</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.corpus import machado\n      Traceback (most recent call last):\n      File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n      ImportError: cannot import name machado\n</code></pre>\n\n<p>But in the download manager (<code>nltk.download()</code>) the package machado is marked as installed and I have a <code>nltk_data/corpus/machado</code> folder.</p>\n\n<p>How can I see from inside the python intepreter what are the installed corpora?</p>\n\n<p>Also, what package should I install to work with this how-to?\n<a href=\"http://nltk.googlecode.com/svn/trunk/doc/howto/portuguese_en.html\" rel=\"noreferrer\">http://nltk.googlecode.com/svn/trunk/doc/howto/portuguese_en.html</a></p>\n\n<p>I can't find the module <code>nltk.examples</code> refered to in the how-to.</p>\n",
    "score": 9,
    "creation_date": 1260819145,
    "view_count": 10896,
    "answer_count": 2,
    "tags": "python;nlp;nltk;corpus"
  },
  {
    "question_id": 74279005,
    "title": "Tokenizer.from_file() HUGGINFACE : Exception: data did not match any variant of untagged enum ModelWrapper",
    "body": "<p>I am having issue loading a <code>Tokenizer.from_file()</code> BPE tokenizer.\nWhen I try I am encountering this error where the line 11743 is the last last one:\n<code>Exception: data did not match any variant of untagged enum ModelWrapper at line 11743 column 3</code>\nI have no idea what is the problem and how to solve it\ndoes anyone have some clue?\nI did not train directly the BPE but the structure is the correct one so vocab and merges in a json. What I did was from a BPE trained by me (that was working) change completely the vocab and the merges based on something manually created by me (without a proper train). But I don't see the problem since the structure should be the same as the original one.\nMy tokenizer version is: <code>0.13.1</code></p>\n<pre><code>{\n  &quot;version&quot;:&quot;1.0&quot;,\n  &quot;truncation&quot;:null,\n  &quot;padding&quot;:null,\n  &quot;added_tokens&quot;:[\n    {\n      &quot;id&quot;:0,\n      &quot;content&quot;:&quot;[UNK]&quot;,\n      &quot;single_word&quot;:false,\n      &quot;lstrip&quot;:false,\n      &quot;rstrip&quot;:false,\n      &quot;normalized&quot;:false,\n      &quot;special&quot;:true\n    },\n    {\n      &quot;id&quot;:1,\n      &quot;content&quot;:&quot;[CLS]&quot;,\n      &quot;single_word&quot;:false,\n      &quot;lstrip&quot;:false,\n      &quot;rstrip&quot;:false,\n      &quot;normalized&quot;:false,\n      &quot;special&quot;:true\n    },\n    {\n      &quot;id&quot;:2,\n      &quot;content&quot;:&quot;[SEP]&quot;,\n      &quot;single_word&quot;:false,\n      &quot;lstrip&quot;:false,\n      &quot;rstrip&quot;:false,\n      &quot;normalized&quot;:false,\n      &quot;special&quot;:true\n    },\n    {\n      &quot;id&quot;:3,\n      &quot;content&quot;:&quot;[PAD]&quot;,\n      &quot;single_word&quot;:false,\n      &quot;lstrip&quot;:false,\n      &quot;rstrip&quot;:false,\n      &quot;normalized&quot;:false,\n      &quot;special&quot;:true\n    },\n    {\n      &quot;id&quot;:4,\n      &quot;content&quot;:&quot;[MASK]&quot;,\n      &quot;single_word&quot;:false,\n      &quot;lstrip&quot;:false,\n      &quot;rstrip&quot;:false,\n      &quot;normalized&quot;:false,\n      &quot;special&quot;:true\n    }\n  ],\n  &quot;normalizer&quot;:null,\n  &quot;pre_tokenizer&quot;:{\n    &quot;type&quot;:&quot;Whitespace&quot;\n  },\n  &quot;post_processor&quot;:null,\n  &quot;decoder&quot;:null,\n  &quot;model&quot;:{\n    &quot;type&quot;:&quot;BPE&quot;,\n    &quot;dropout&quot;:null,\n    &quot;unk_token&quot;:&quot;[UNK]&quot;,\n    &quot;continuing_subword_prefix&quot;:null,\n    &quot;end_of_word_suffix&quot;:null,\n    &quot;fuse_unk&quot;:false,\n    &quot;vocab&quot;:{\n      &quot;[UNK]&quot;:0,\n      &quot;[CLS]&quot;:1,\n      &quot;[SEP]&quot;:2,\n      &quot;[PAD]&quot;:3,\n      &quot;[MASK]&quot;:4,\n      &quot;AA&quot;:5,\n      &quot;A&quot;:6,\n      &quot;C&quot;:7,\n      &quot;D&quot;:8,\n.....\n</code></pre>\n<p>merges:</p>\n<pre><code>....\n      &quot;QD FLPDSITF&quot;,\n      &quot;QPHY AS&quot;,\n      &quot;LR SE&quot;,\n      &quot;A DRV&quot;\n    ] #11742\n  } #11743\n} #11744\n</code></pre>\n",
    "score": 9,
    "creation_date": 1667320481,
    "view_count": 21568,
    "answer_count": 3,
    "tags": "json;nlp;huggingface-transformers;huggingface-tokenizers;huggingface"
  },
  {
    "question_id": 65034771,
    "title": "How to truncate a Bert tokenizer in Transformers library",
    "body": "<p>I am using the Scibert pretrained model to get embeddings for various texts. The code is as follows:</p>\n<pre><code>from transformers import *\n\ntokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', model_max_length=512, truncation=True)\nmodel = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n</code></pre>\n<p>I have added both the max length and truncation parameters to tokenizers, but unfortunately, they don't truncate the results.If I run a longer text through the tokenizer:</p>\n<pre><code>inputs = tokenizer(&quot;&quot;&quot;long text&quot;&quot;&quot;)\n</code></pre>\n<p>I get the following error:</p>\n<blockquote>\n<p>Token indices sequence length is longer than the specified maximum\nsequence length for this model (605 &gt; 512). Running this sequence\nthrough the model will result in indexing errors</p>\n</blockquote>\n<p>Now obviously I can't run this through the model due to having too long sequences of tensors. What is the easiest way to truncate the input to fit the maximum sequence length of 512?</p>\n",
    "score": 9,
    "creation_date": 1606468784,
    "view_count": 12161,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers"
  },
  {
    "question_id": 51699001,
    "title": "tokenizer.texts_to_sequences Keras Tokenizer gives almost all zeros",
    "body": "<p>I am working to create a text classification code but I having problems in encoding documents using the tokenizer. </p>\n\n<p>1) I started by fitting a tokenizer on my document as in here: </p>\n\n<pre><code>vocabulary_size = 20000\ntokenizer = Tokenizer(num_words= vocabulary_size, filters='')\ntokenizer.fit_on_texts(df['data'])\n</code></pre>\n\n<p>2) Then I wanted to check if my data is fitted correctly so I converted into sequence as in here: </p>\n\n<pre><code>sequences = tokenizer.texts_to_sequences(df['data'])\ndata = pad_sequences(sequences, maxlen= num_words) \nprint(data) \n</code></pre>\n\n<p>which gave me fine output. i.e. encoded words into numbers </p>\n\n<pre><code>[[ 9628  1743    29 ...   161    52   250]\n [14948     1    70 ...    31   108    78]\n [ 2207  1071   155 ... 37607 37608   215]\n ...\n [  145    74   947 ...     1    76    21]\n [   95 11045  1244 ...   693   693   144]\n [   11   133    61 ...    87    57    24]]\n</code></pre>\n\n<p>Now, I wanted to convert a text into a sequence using the same method. \nLike this: </p>\n\n<pre><code>sequences = tokenizer.texts_to_sequences(\"physics is nice \")\ntext = pad_sequences(sequences, maxlen=num_words)\nprint(text)\n</code></pre>\n\n<p>it gave me weird output: </p>\n\n<pre><code>[[   0    0    0    0    0    0    0    0    0  394]\n [   0    0    0    0    0    0    0    0    0 3136]\n [   0    0    0    0    0    0    0    0    0 1383]\n [   0    0    0    0    0    0    0    0    0  507]\n [   0    0    0    0    0    0    0    0    0    1]\n [   0    0    0    0    0    0    0    0    0 1261]\n [   0    0    0    0    0    0    0    0    0    0]\n [   0    0    0    0    0    0    0    0    0 1114]\n [   0    0    0    0    0    0    0    0    0    1]\n [   0    0    0    0    0    0    0    0    0 1261]\n [   0    0    0    0    0    0    0    0    0  753]]\n</code></pre>\n\n<p>According to Keras documentation (<a href=\"https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/\" rel=\"noreferrer\">Keras</a>): </p>\n\n<blockquote>\n  <p>texts_to_sequences(texts)</p>\n  \n  <p>Arguments: texts: list of texts to turn to sequences. </p>\n  \n  <p>Return: list of\n  sequences (one per text input).</p>\n</blockquote>\n\n<p>is it not supposed to encode each word to its corresponding number? then pad the text if it shorter than 50 to 50? \nWhere is the mistake ? </p>\n",
    "score": 9,
    "creation_date": 1533511713,
    "view_count": 36048,
    "answer_count": 6,
    "tags": "python;keras;nlp;deep-learning;tokenize"
  },
  {
    "question_id": 3641152,
    "title": "How to efficiently filter a string against a long list of words in Python/Django?",
    "body": "<p>Stackoverflow implemented its \"Related Questions\" feature by taking the title of the current question being asked and removing from it the 10,000 most common English words according to Google. The remaining words are then submitted as a fulltext search to find related questions.</p>\n\n<p>I want to do something similar in my Django site. What is the best way to filter a string (the question title in this case) against a long list of words in Python? Any libraries that would enable me to do that efficiently?</p>\n",
    "score": 9,
    "creation_date": 1283581516,
    "view_count": 4876,
    "answer_count": 6,
    "tags": "python;django;string;nlp"
  },
  {
    "question_id": 1468962,
    "title": "Correlation clustering in R",
    "body": "<p>I'd like to use <code>correlation clustering</code> and I figure <code>R</code> is a good place to start.</p>\n\n<p>I can present the data to <code>R</code> as a set of large, sparse vectors or as a table with a pre-computed dissimilarity matrix.</p>\n\n<p>My questions are:</p>\n\n<ul>\n<li>are there existing <code>R</code> functions to turn this into a <code>hierarchical cluster</code> with <code>agnes</code> that uses <code>correlation clustering</code>?</li>\n<li>will I have to implement the (admittedly simple) <code>correlation clustering</code>function by hand, if so how do I make it play well with <code>agnes</code>?</li>\n</ul>\n",
    "score": 9,
    "creation_date": 1253747022,
    "view_count": 14250,
    "answer_count": 4,
    "tags": "r;cluster-analysis;nlp"
  },
  {
    "question_id": 55350811,
    "title": "In language modeling, why do I have to init_hidden weights before every new epoch of training? (pytorch)",
    "body": "<p>I have a question about the following code in pytorch language modeling:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>print(\"Training and generating...\")\n    for epoch in range(1, config.num_epochs + 1): \n        total_loss = 0.0\n        model.train()  \n        hidden = model.init_hidden(config.batch_size)  \n\n        for ibatch, i in enumerate(range(0, train_len - 1, seq_len)):\n            data, targets = get_batch(train_data, i, seq_len)          \n            hidden = repackage_hidden(hidden)\n            model.zero_grad()\n\n            output, hidden = model(data, hidden)\n            loss = criterion(output.view(-1, config.vocab_size), targets)\n            loss.backward()  \n</code></pre>\n\n<p>Please check line 5.</p>\n\n<p>And the init_hidden function is as follows:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def init_hidden(self, bsz):\n    weight = next(self.parameters()).data\n    if self.rnn_type == 'LSTM':  # lstm：(h0, c0)\n        return (Variable(weight.new(self.n_layers, bsz, self.hi_dim).zero_()),\n                Variable(weight.new(self.n_layers, bsz, self.hi_dim).zero_()))\n    else:  # gru &amp; rnn：h0\n        return Variable(weight.new(self.n_layers, bsz, self.hi_dim).zero_())\n</code></pre>\n\n<p>My question is:</p>\n\n<p>Why do we need to init_hidden every epoch? Shouldn't it be that the model inherit the hidden parameters from last epoch and continue training on them.</p>\n",
    "score": 9,
    "creation_date": 1553580627,
    "view_count": 3846,
    "answer_count": 3,
    "tags": "machine-learning;nlp;pytorch;recurrent-neural-network"
  },
  {
    "question_id": 38365389,
    "title": "Compare similarity between names",
    "body": "<p>I have to make a cross-validation for some data based on names.</p>\n\n<p>The problem I'm facing is that depending on the source, names have slight variations, for example:</p>\n\n<pre><code>L &amp; L AIR CONDITIONING   vs L &amp; L AIR CONDITIONING Service\n\nBEST ROOFING vs ROOFING INC\n</code></pre>\n\n<p>I have several thousands of records so do it manually will be very time demanding, I want to automate the process as much as possible.</p>\n\n<p>Since there are additional words it wouldn't be enough to lowercase the names. </p>\n\n<p>Which are good algorithms to handle this?</p>\n\n<p>Maybe to calculate the correlation giving low weight to words like 'INC' or 'Service'</p>\n\n<p>Edit:</p>\n\n<p>I tried the difflib library</p>\n\n<pre><code>difflib.SequenceMatcher(None,name_1.lower(),name_2.lower()).ratio()\n</code></pre>\n\n<p>I'm getting a decent result with it.</p>\n",
    "score": 9,
    "creation_date": 1468469809,
    "view_count": 14537,
    "answer_count": 4,
    "tags": "python;machine-learning;nlp"
  },
  {
    "question_id": 8589005,
    "title": "Difference between named entity recognition and resolution?",
    "body": "<p>What is the difference between named entity recognition and named entity resolution? Would appreciate a practical example.</p>\n",
    "score": 9,
    "creation_date": 1324466534,
    "view_count": 2612,
    "answer_count": 1,
    "tags": "nlp;named-entity-recognition;named-entity-extraction"
  },
  {
    "question_id": 3733587,
    "title": "How to get POS tagging using Stanford Parser",
    "body": "<p>I'm using Stanford Parser to parse the dependence relations between pair of words, but I also need the tagging of words. However, in the ParseDemo.java, the program only output the Tagging Tree. I need each word's tagging like this:</p>\n\n<pre><code>My/PRP$ dog/NN also/RB likes/VBZ eating/VBG bananas/NNS ./.\n</code></pre>\n\n<p>not like this:</p>\n\n<pre><code>(ROOT\n  (S\n    (NP (PRP$ My) (NN dog))\n    (ADVP (RB also))\n    (VP (VBZ likes)\n      (S\n        (VP (VBG eating)\n          (S\n            (ADJP (NNS bananas))))))\n    (. .)))\n</code></pre>\n\n<p>Who can help me? thanks a lot.</p>\n",
    "score": 9,
    "creation_date": 1284710484,
    "view_count": 7825,
    "answer_count": 4,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 1787755,
    "title": "Python and .NET integration",
    "body": "<p>I'm currently looking at python because I really like the text parsing capabilities and the nltk library, but traditionally I am a .Net/C# programmer.  I don't think IronPython is an integration point for me because I am using NLTK and presumably would need a port of that library to the CLR.  I've looked a little at <a href=\"http://pythonnet.sourceforge.net/readme.html\" rel=\"nofollow noreferrer\">Python for .NET</a> and was wondering if this was a good place to start.  Is there a way to marshal a python class into C#?  Also, is this solution still being used?  Better yet, has anyone done this?  One thing I am considering is just using a persistence medium as a go-between (parse in Python, store in MongoDB, and run site in .NET).</p>\n",
    "score": 9,
    "creation_date": 1259036488,
    "view_count": 8088,
    "answer_count": 3,
    "tags": "python;.net;nlp;nltk;python.net"
  },
  {
    "question_id": 1687510,
    "title": "What is the default chunker for NLTK toolkit in Python?",
    "body": "<p>I am using their default POS tagging and default tokenization..and it seems sufficient.  I'd like their default chunker too.</p>\n\n<p>I am reading the NLTK toolkit book, but it does not seem like they have a default chunker?</p>\n",
    "score": 9,
    "creation_date": 1257513020,
    "view_count": 4674,
    "answer_count": 2,
    "tags": "python;nlp;nltk;chunking"
  },
  {
    "question_id": 55114128,
    "title": "Uni-directional Transformer VS Bi-directional BERT",
    "body": "<p>I just finished reading the <a href=\"https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\" rel=\"noreferrer\">Transformer</a> paper and <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"noreferrer\">BERT</a> paper. But couldn't figure out why Transformer is uni-directional and BERT is bi-directional as mentioned in BERT paper. As they don't use recurrent networks, it's not so straightforward to interpret the directions. Can anyone give some clue? Thanks.</p>\n",
    "score": 9,
    "creation_date": 1552364613,
    "view_count": 5097,
    "answer_count": 1,
    "tags": "nlp;transformer-model;pre-trained-model;bert-language-model"
  },
  {
    "question_id": 10180445,
    "title": "OpenNLP Name Finder",
    "body": "<p>I am using the <a href=\"http://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html#tools.namefind\">NameFinder</a> API example doc of OpenNLP. After initializing the Name Finder the documentation uses the following code for the input text:</p>\n\n<pre><code>for (String document[][] : documents) {\n\n  for (String[] sentence : document) {\n    Span nameSpans[] = nameFinder.find(sentence);\n    // do something with the names\n  }\n\n  nameFinder.clearAdaptiveData()\n}\n</code></pre>\n\n<p>However when I bring this into eclipse the 'documents' (not 'document') variable is giving me an error saying the <em>variable documents cannot be resolved</em>. What is the documentation referring to with the 'documents' array variable? Do I need to initialize an array called 'documents' which hold txt files for this error to go away?</p>\n\n<p>Thank you for your help.</p>\n",
    "score": 9,
    "creation_date": 1334604818,
    "view_count": 7425,
    "answer_count": 1,
    "tags": "apache;nlp;data-mining;opennlp"
  },
  {
    "question_id": 5833030,
    "title": "Simple Natural Language Processing Startup for Java",
    "body": "<p>I am willing to start developing a project on NLP. I dont know much of the tools available. After googling for about a month. I realized that openNLP can be my solution.</p>\n\n<p>Unfortunately i dont see any complete tutorial over using the API. All of them are lacking of some general steps. I need a tutorial from ground level. I have seen a lot of downloads over the site but dont know how to use them? do i need to train or something?.. Here is what i want to know-</p>\n\n<p>How to install / set up a nlp system which can-</p>\n\n<ol>\n<li>parse a English sentence words</li>\n<li>identify the different parts of speech</li>\n</ol>\n",
    "score": 9,
    "creation_date": 1304085584,
    "view_count": 12496,
    "answer_count": 4,
    "tags": "java;nlp"
  },
  {
    "question_id": 2236858,
    "title": "Build a natural language model that fixes misspellings",
    "body": "<p>What are books about how to build a natural language parsing program like this:</p>\n\n<pre>\ninput: I got to TALL you\noutput: I got to TELL you\n\ninput: Big RAT box\noutput: Big RED box\n\nin: hoo un thum zend three\nout: one thousand three\n\n</pre>\n\n<p>It must have the language model that allows to predict what words are misspelled !</p>\n\n<p>What are the best books on how to build such a tool??</p>\n\n<p>p.s. Are there free webservices to spell-check? From Google maybe?..</p>\n",
    "score": 9,
    "creation_date": 1265806435,
    "view_count": 1310,
    "answer_count": 5,
    "tags": "java;parsing;nlp;linguistics"
  },
  {
    "question_id": 69064948,
    "title": "How to import gensim summarize",
    "body": "<p>I got gensim to work in Google Collab by following this process:</p>\n<pre><code>!pip install gensim\nfrom gensim.summarization import summarize\n</code></pre>\n<p>Then I was able to call <code>summarize(some_text)</code></p>\n<p>Now I'm trying to run the same thing in VS code:</p>\n<p>I've installed gensim:\n<code>pip3 install gensim</code></p>\n<p>but when I run</p>\n<pre><code>from gensim.summarization import summarize\n</code></pre>\n<p>I get the error</p>\n<pre><code>Import &quot;gensim.summarization&quot; could not be resolvedPylancereportMissingImports\n</code></pre>\n<p>I've also tried <code>from gensim.summarization.summarizer import summarize</code> with same error. Regardless I haven't been able to call the function <code>summarize(some_text)</code> outside of Google Collab.</p>\n",
    "score": 9,
    "creation_date": 1630857262,
    "view_count": 26018,
    "answer_count": 2,
    "tags": "python;visual-studio-code;nlp;gensim"
  },
  {
    "question_id": 52645459,
    "title": "How to evaluate Word2Vec model",
    "body": "<p>Hi have my own corpus and I train several Word2Vec models on it.\nWhat is the best way to evaluate them one against each-other and choose the best one? (Not manually obviously - I am looking for various measures).</p>\n\n<p>It worth noting that the embedding is for items and not word, therefore I can't use any existing benchmarks.</p>\n\n<p>Thanks!</p>\n",
    "score": 9,
    "creation_date": 1538652157,
    "view_count": 10808,
    "answer_count": 3,
    "tags": "python;nlp;word2vec;embedding;word-embedding"
  },
  {
    "question_id": 52602071,
    "title": "Dataframe as datasource in torchtext",
    "body": "<p>I have a dataframe, which has two columns (review and sentiment). I am using pytorch and torchtext library for preprocessing data.\nIs it possible to use dataframe as source to read data from, in torchtext?\nI am looking for something similar to, but not  </p>\n\n<pre><code>data.TabularDataset.splits(path='./data')\n</code></pre>\n\n<p>I have performed some operation (clean, change to required format) on data and final data is in a dataframe.</p>\n\n<p>If not torchtext, what other package would you suggest that would help in preprocessing text data present in a datarame. I could not find anything online. Any help would be great.</p>\n",
    "score": 9,
    "creation_date": 1538454791,
    "view_count": 7885,
    "answer_count": 2,
    "tags": "dataframe;nlp;pytorch;torchtext"
  },
  {
    "question_id": 50362506,
    "title": "Why Doc2vec gives 2 different vectors for the same texts",
    "body": "<p>I am using <code>Doc2vec</code> to get vectors from words.\nPlease see my below code:</p>\n\n<pre><code>from gensim.models.doc2vec import TaggedDocument\nf = open('test.txt','r')\n\ntrainings = [TaggedDocument(words = data.strip().split(\",\"),tags = [i]) for i,data in enumerate(f)\n\n\nmodel = Doc2Vec(vector_size=5,  epochs=55, seed = 1, dm_concat=1)\n\nmodel.build_vocab(trainings)\nmodel.train(trainings, total_examples=model.corpus_count, epochs=model.epochs)\n\nmodel.save(\"doc2vec.model\")\n\nmodel = Doc2Vec.load('doc2vec.model')\nfor i in range(len(model.docvecs)):\n    print(i,model.docvecs[i])\n</code></pre>\n\n<p>I have a <code>test.txt</code> file that its content has 2 lines and contents of these 2 lines is the same (they are \"a\")\nI trained with doc2vec and got the model, but the problem is although the contents of 2 lines is the same, doc2vec gave me 2 different vectors.</p>\n\n<pre><code>0 [ 0.02730868  0.00393569 -0.08150548 -0.04009786 -0.01400406]\n1 [ 0.03916578 -0.06423566 -0.05350181 -0.00726833 -0.08292392]\n</code></pre>\n\n<p>I dont know why this happened. I thought that these vectors would be the same.\nCan you explain that? And if I want to make the same vectors for the sames words, what should I do in this case?</p>\n",
    "score": 9,
    "creation_date": 1526445126,
    "view_count": 2668,
    "answer_count": 2,
    "tags": "python;nlp;word2vec;gensim;doc2vec"
  },
  {
    "question_id": 47493897,
    "title": "In spacy, Is it possible to get the corresponding rule id in a match of matches",
    "body": "<p>In Spacy 2.x, I use the matcher to find specific tokens in my text corpus. Each rule has an ID (<code>'class-1_0'</code> for example). During parse, I use the callback <code>on_match</code> to handle each match. Is there a solution to retrieve the rule used to find the match directly in the callback.</p>\n\n<p>Here is my sample code.</p>\n\n<pre><code>txt = (\"Aujourd'hui, je vais me faire une tartine au beurre \"\n       \"de cacahuète, c'est un pilier de ma nourriture \"\n       \"quotidienne.\")\n\nnlp = spacy.load('fr')\n\ndef on_match(matcher, doc, id, matches):\n    span = doc[matches[id][1]:matches[id][2]]\n    print(span)\n    # find a way to get the corresponding rule without fuzz\n\nmatcher = Matcher(nlp.vocab)\nmatcher.add('class-1_0', on_match, [{'LEMMA': 'pilier'}])\nmatcher.add('class-1_1', on_match, [{'LEMMA': 'beurre'}, {'LEMMA': 'de'}, {'LEMMA': 'cacahuète'}])\n\ndoc = nlp(txt)\nmatches = matcher(doc)\n</code></pre>\n\n<p>In this case <code>matches</code> return :</p>\n\n<pre><code>[(12071893341338447867, 9, 12), (4566231695725171773, 16, 17)]\n</code></pre>\n\n<p><code>12071893341338447867</code> is a unique ID based on <code>class-1_0</code>. I cannot find the original rule name, even if I do some introspection in <code>matcher._patterns</code>.</p>\n\n<p>It would be great if someone can help me.\nThank you very much.</p>\n",
    "score": 9,
    "creation_date": 1511680897,
    "view_count": 3097,
    "answer_count": 3,
    "tags": "nlp;matcher;spacy"
  },
  {
    "question_id": 43227938,
    "title": "Keras embedding layer masking. Why does input_dim need to be |vocabulary| + 2?",
    "body": "<p>In the Keras docs for <code>Embedding</code> <a href=\"https://keras.io/layers/embeddings/\" rel=\"noreferrer\">https://keras.io/layers/embeddings/</a>, the explanation given for <code>mask_zero</code> is </p>\n\n<blockquote>\n  <p>mask_zero: Whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal |vocabulary| + 2).</p>\n</blockquote>\n\n<p>Why does input_dim need to be 2 + number of words in vocabulary? Assuming 0 is masked and can't be used, shouldn't it just be 1 + number of words? What is the other extra entry for?</p>\n",
    "score": 9,
    "creation_date": 1491386520,
    "view_count": 2597,
    "answer_count": 2,
    "tags": "python;nlp;deep-learning;keras;keras-layer"
  },
  {
    "question_id": 29570207,
    "title": "Does NLTK have TF-IDF implemented?",
    "body": "<p>There are TF-IDF implementations in <code>scikit-learn</code> and <code>gensim</code>. </p>\n\n<p>There are simple implementations <a href=\"https://stackoverflow.com/questions/2380394/simple-implementation-of-n-gram-tf-idf-and-cosine-similarity-in-python/22577329#22577329\">Simple implementation of N-Gram, tf-idf and Cosine similarity in Python</a></p>\n\n<p>To avoid reinventing the wheel, </p>\n\n<ul>\n<li><strong>Is there really no TF-IDF in NLTK?</strong> </li>\n<li><strong>Are there sub-packages that we can manipulate to implement TF-IDF in NLTK? If there are how?</strong></li>\n</ul>\n\n<p>In this blogpost, it says NLTK doesn't have it. <strong>Is that true?</strong> <a href=\"http://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php\" rel=\"noreferrer\">http://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php</a></p>\n",
    "score": 9,
    "creation_date": 1428698056,
    "view_count": 29590,
    "answer_count": 2,
    "tags": "python;nlp;nltk;tf-idf"
  },
  {
    "question_id": 24688116,
    "title": "How to filter out words with low tf-idf in a corpus with gensim?",
    "body": "<p>I am using <code>gensim</code> for some NLP task. I've created a corpus from <code>dictionary.doc2bow</code> where <code>dictionary</code> is an object of <code>corpora.Dictionary</code>. Now I want to filter out the terms with low tf-idf values before running an LDA model. I looked into the <a href=\"http://radimrehurek.com/gensim/corpora/mmcorpus.html\" rel=\"noreferrer\">documentation</a> of the corpus class but cannot find a way to access the terms. Any ideas? Thank you.</p>\n",
    "score": 9,
    "creation_date": 1405036425,
    "view_count": 10734,
    "answer_count": 4,
    "tags": "python;nlp;gensim"
  },
  {
    "question_id": 1200980,
    "title": "Identifying geographical locations in text",
    "body": "<p>What kind of work has been done to determine whether a specific string pertains to a geographical location?  For example:</p>\n\n<pre><code>'troy, ny'\n'austin, texas'\n'hotels in las vegas, nv'\n</code></pre>\n\n<p>I guess what I'm sort of expecting is a statistical approach that gives a degree of confidence that the first two are locations.  The last one would probably require a heuristic which grabs \"%s, %s\" and then uses the same technique.  I'm specifically looking for approaches that don't rely too heavily on the proposition 'in', seeing as it's not an entirely unambiguous or consistently available indicator of location.</p>\n\n<p>Can anyone point me to approaches, papers, or existing utilities?  Thanks!</p>\n",
    "score": 9,
    "creation_date": 1248880115,
    "view_count": 3829,
    "answer_count": 4,
    "tags": "nlp;geography"
  },
  {
    "question_id": 163923,
    "title": "Methods for Geotagging or Geolabelling Text Content",
    "body": "<p>What are some good algorithms for automatically labeling text with the city / region  or origin?  That is, if a blog is about New York, how can I tell programatically.  Are there packages / papers that claim to do this with any degree of certainty?  </p>\n\n<p>I have looked at some tfidf based approaches, proper noun intersections, but so far, no spectacular successes, and I'd appreciate ideas!  </p>\n\n<p>The more general question is about assigning texts to topics, given some list of topics.</p>\n\n<p>Simple / naive approaches preferred to full on Bayesian approaches, but I'm open.</p>\n",
    "score": 9,
    "creation_date": 1222973072,
    "view_count": 6591,
    "answer_count": 2,
    "tags": "algorithm;statistics;nlp;named-entity-recognition"
  },
  {
    "question_id": 62710872,
    "title": "How to store Word vector Embeddings?",
    "body": "<p>I am using BERT Word Embeddings for sentence classification task with 3 labels. I am using Google Colab for coding. My problem is, since I will have to execute the embedding part every time I restart the kernel, is there any way to save these word embeddings once it is generated? Because, it takes a lot of time to generate those embeddings.</p>\n<p>The code I am using to generate BERT Word Embeddings is -</p>\n<pre><code>[get_features(text_list[i]) for text_list[i] in text_list]\n</code></pre>\n<p>Here, gen_features is a function which returns word embedding for each i in my list text_list.</p>\n<p>I read that converting embeddings into bumpy tensors and then using np.save can do it. But I actually don't know how to code it.</p>\n",
    "score": 9,
    "creation_date": 1593762693,
    "view_count": 17981,
    "answer_count": 1,
    "tags": "python-3.x;keras;nlp;word-embedding;bert-language-model"
  },
  {
    "question_id": 47028576,
    "title": "How can I compute TF/IDF with SQL (BigQuery)",
    "body": "<p>I'm doing text analysis over reddit comments, and I want to calculate the TF-IDF within BigQuery.</p>\n",
    "score": 9,
    "creation_date": 1509428531,
    "view_count": 8582,
    "answer_count": 3,
    "tags": "sql;google-bigquery;text-analysis"
  },
  {
    "question_id": 15260212,
    "title": "NLP to find relationship between entities",
    "body": "<p>My current understanding is that it's possible to extract entities from a text document using toolkits such as OpenNLP, Stanford NLP. </p>\n\n<p>However, is there a way to find <em>relationships</em> between these entities? </p>\n\n<p>For example consider the following text : </p>\n\n<p><em>\"As some of you may know, I spent last week at CERN, the European high-energy physics laboratory where the famous Higgs boson was discovered last July. Every time I go to CERN I feel a deep sense of reverence. Apart from quick visits over the years, I was there for three months in the late 1990s as a visiting scientist, doing work on early Universe physics, trying to figure out how to connect the Universe we see today with what may have happened in its infancy.\"</em> </p>\n\n<p>Entities: <strong>I</strong> (author), <strong>CERN</strong>, <strong>Higgs boson</strong></p>\n\n<p>Relationships : \n- I \"<strong>visited</strong>\" CERN\n- CERN \"<strong>discovered</strong>\" Higgs boson</p>\n\n<p>Thanks. </p>\n",
    "score": 9,
    "creation_date": 1362612430,
    "view_count": 10960,
    "answer_count": 5,
    "tags": "text;nlp;stanford-nlp;opennlp;information-extraction"
  },
  {
    "question_id": 68018745,
    "title": "Not able to import from `gensim.summarization` module in Django",
    "body": "<p>I have included the 2 import statements in my views.py</p>\n<pre><code>from gensim.summarization.summarizer import summarizer\nfrom gensim.summarization import keywords\n</code></pre>\n<p>However, even after I installed gensim using pip, I am getting the error:</p>\n<pre><code>ModuleNotFoundError: No module named 'gensim.summarization'\n</code></pre>\n",
    "score": 9,
    "creation_date": 1623930579,
    "view_count": 23571,
    "answer_count": 3,
    "tags": "python;django;nlp;gensim"
  },
  {
    "question_id": 67821137,
    "title": "Spacy: How to get all words that describe a noun?",
    "body": "<p>I am new to spacy and to nlp overall.</p>\n<p>To understand how spacy works I would like to create a function which takes a sentence and returns a dictionary,tuple or list with the noun and the words describing it.</p>\n<p>I know that spacy creates a tree of the sentence and knows the use of each word (shown in displacy).</p>\n<p>But what's the right way to get from:</p>\n<blockquote>\n<p>&quot;A large room with two yellow dishwashers in it&quot;</p>\n</blockquote>\n<p>To:</p>\n<blockquote>\n<p>{noun:&quot;room&quot;,adj:&quot;large&quot;}\n{noun:&quot;dishwasher&quot;,adj:&quot;yellow&quot;,adv:&quot;two&quot;}</p>\n</blockquote>\n<p>Or any other solution that gives me all related words in a usable bundle.</p>\n<p>Thanks in advance!</p>\n",
    "score": 9,
    "creation_date": 1622721801,
    "view_count": 3518,
    "answer_count": 2,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 59030907,
    "title": "NLP Transformers: Best way to get a fixed sentence embedding-vector shape?",
    "body": "<p>I'm loading a language model from torch hub (<a href=\"https://camembert-model.fr/#about\" rel=\"nofollow noreferrer\">CamemBERT</a> a French RoBERTa-based model) and using it do embed some french sentences:  </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import torch\ncamembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')\ncamembert.eval()  # disable dropout (or leave in train mode to finetune)\n\n\ndef embed(sentence):\n   tokens = camembert.encode(sentence)\n   # Extract all layer's features (layer 0 is the embedding layer)\n   all_layers = camembert.extract_features(tokens, return_all_hiddens=True)\n   embeddings = all_layers[0]\n   return embeddings\n\n# Here we see that the shape of the embedding vector depends on the number of tokens in the sentence\n\nu = embed(sentence=\"Bonjour, ça va ?\")\nu.shape # torch.Size([1, 7, 768])\nv = embed(sentence=\"Salut, comment vas-tu ?\")\nv.shape # torch.Size([1, 9, 768])\n\n</code></pre>\n\n<p>Imagine now in order to do some <strong>semantic search</strong>, I want to calculate the <code>cosine distance</code> between the vectors (tensors in our case) <code>u</code> and <code>v</code> : </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>cos = torch.nn.CosineSimilarity(dim=1)\ncos(u, v) # will throw an error since the shape of `u` is different from the shape of `v`\n</code></pre>\n\n<p>I'm asking what is the best method to use in order to always get the <strong>same embedding shape</strong> for a sentence <strong>regardless the count of its tokens</strong>?</p>\n\n<p>=> The first solution I'm thinking of is calculating the <code>mean on axis=1</code> (embedding of a sentence is the mean embedding its tokens) since axis=0 and axis=2 have always the same size:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>cos = torch.nn.CosineSimilarity(dim=1)\ncos(u.mean(axis=1), v.mean(axis=1)) # works now and gives 0.7269\n</code></pre>\n\n<p>But, I'm afraid that I'm hurting the embedding of the sentence when calculating the mean since it gives the same weight for each token (maybe multiplying by <strong>TF-IDF</strong>?).</p>\n\n<p>=> The second solution is to pad shorter sentences out. That means:  </p>\n\n<ul>\n<li>giving a list of sentences to embed at a time (instead of embedding sentence by sentence)</li>\n<li>look up for the sentence with the longest tokens and embed it, get its shape <code>S</code></li>\n<li>for the rest of sentences embed then pad zero to get the same shape <code>S</code> (the sentence has 0 in the rest of dimensions)</li>\n</ul>\n\n<p>What are your thoughts?\nWhat other techniques would you use and why?</p>\n\n<p>Thanks in advance!</p>\n",
    "score": 9,
    "creation_date": 1574681772,
    "view_count": 11382,
    "answer_count": 3,
    "tags": "machine-learning;deep-learning;nlp;pytorch;word-embedding"
  },
  {
    "question_id": 56639938,
    "title": "BERT output not deterministic",
    "body": "<p>BERT output is not deterministic.\nI expect the output values are deterministic when I put a same input, but my bert model the values are changing. Sounds awkwardly, the same value is returned twice, once. That is, once another value comes out, the same value comes out and it repeats.\nHow I can make the output deterministic?\nlet me show snippets of my code.\nI use the model as below.</p>\n\n<p>For the BERT implementation, I use huggingface implemented BERT pytorch implementation. which is quite fameous model ri implementation in the pytorch area. [link] <a href=\"https://github.com/huggingface/pytorch-pretrained-BERT/\" rel=\"noreferrer\">https://github.com/huggingface/pytorch-pretrained-BERT/</a></p>\n\n<pre><code>        tokenizer = BertTokenizer.from_pretrained(self.bert_type, do_lower_case=self.do_lower_case, cache_dir=self.bert_cache_path)\n        pretrain_bert = BertModel.from_pretrained(self.bert_type, cache_dir=self.bert_cache_path)\n        bert_config = pretrain_bert.config\n</code></pre>\n\n<p>Get the output like this</p>\n\n<pre><code>        all_encoder_layer, pooled_output = self.model_bert(all_input_ids, all_segment_ids, all_input_mask)\n\n        # all_encoder_layer: BERT outputs from all layers.\n        # pooled_output: output of [CLS] vec.\n\n</code></pre>\n\n<p>pooled_output</p>\n\n<pre><code>tensor([[-3.3997e-01,  2.6870e-01, -2.8109e-01, -2.0018e-01, -8.6849e-02,\n\ntensor([[ 7.4340e-02, -3.4894e-03, -4.9583e-03,  6.0806e-02,  8.5685e-02,\n\ntensor([[-3.3997e-01,  2.6870e-01, -2.8109e-01, -2.0018e-01, -8.6849e-02,\n\ntensor([[ 7.4340e-02, -3.4894e-03, -4.9583e-03,  6.0806e-02,  8.5685e-02,\n</code></pre>\n\n<p>for the all encoder layer, the situation is same, - same in twice an once.</p>\n\n<p>I extract word embedding feature from the bert, and the situation is same. </p>\n\n<pre><code>wemb_n\ntensor([[[ 0.1623,  0.4293,  0.1031,  ..., -0.0434, -0.5156, -1.0220],\n\ntensor([[[ 0.0389,  0.5050,  0.1327,  ...,  0.3232,  0.2232, -0.5383],\n\ntensor([[[ 0.1623,  0.4293,  0.1031,  ..., -0.0434, -0.5156, -1.0220],\n\ntensor([[[ 0.0389,  0.5050,  0.1327,  ...,  0.3232,  0.2232, -0.5383],\n</code></pre>\n",
    "score": 9,
    "creation_date": 1560813432,
    "view_count": 3472,
    "answer_count": 2,
    "tags": "deep-learning;nlp;transformer-model;bert-language-model"
  },
  {
    "question_id": 47274540,
    "title": "How to improve NLTK sentence segmentation?",
    "body": "<p>Given the paragraph from Wikipedia:</p>\n<blockquote>\n<p>An ambitious campus expansion plan was proposed by Fr. Vernon F.\nGallagher in 1952. Assumption Hall, the first student dormitory, was\nopened in 1954, and Rockwell Hall was dedicated in November 1958,\nhousing the schools of business and law. It was during the tenure of\nF. Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to\naction.</p>\n</blockquote>\n<p>I run NLTK <code>nltk.sent_tokenize</code> to get the sentences. This returns:</p>\n<pre><code>['An ambitious campus expansion plan was proposed by Fr.', \n'Vernon F. Gallagher in 1952.', \n'Assumption Hall, the first student dormitory, was opened in 1954, and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.', \n'It was during the tenure of Fr.', \n'Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to action.'\n ] \n</code></pre>\n<p>While NTLK could handle <strong>F. Henry J. McAnulty</strong> as one entity,\nIt failed for <strong>Fr. Vernon F. Gallagher</strong>, and this broke the sentence into two.</p>\n<p>The correct tokenization should be:</p>\n<pre><code>[\n'An ambitious campus expansion plan was proposed by Fr. Vernon F. Gallagher in 1952.', \n'Assumption Hall, the first student dormitory, was opened in 1954, and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.', \n'It was during the tenure of Fr. Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to action.'\n ] \n</code></pre>\n<p>How can I improve the tokenizer performance?</p>\n",
    "score": 9,
    "creation_date": 1510611678,
    "view_count": 7749,
    "answer_count": 1,
    "tags": "python;nlp;nltk;tokenize;text-segmentation"
  },
  {
    "question_id": 43455043,
    "title": "How to vectorize whole text using fasttext?",
    "body": "<p>To get vector of a word, I can use: </p>\n\n<pre><code>model[\"word\"]\n</code></pre>\n\n<p>but if I want to get the vector of a sentence, I need to either sum vectors of all words or get average of all vectors.</p>\n\n<p>Does FastText provide a method to do this?</p>\n",
    "score": 9,
    "creation_date": 1492445174,
    "view_count": 13264,
    "answer_count": 3,
    "tags": "facebook;nlp;fasttext"
  },
  {
    "question_id": 42643074,
    "title": "Document similarity: Vector embedding versus Tf-Idf performance?",
    "body": "<p>I have a collection of documents, where each document is rapidly growing with time. The task is to find similar documents at any fixed time. I have two potential approaches:</p>\n\n<ol>\n<li><p>A vector embedding (word2vec, GloVe or fasttext), averaging over word vectors in a document, and using cosine similarity.</p></li>\n<li><p>Bag-of-Words: tf-idf or its variations such as BM25. </p></li>\n</ol>\n\n<p>Will one of these  yield a significantly better result? Has someone done a quantitative comparison of tf-idf versus averaging word2vec for document similarity? </p>\n\n<p>Is there another approach, that allows to dynamically refine the document's vectors as more text is added? </p>\n",
    "score": 9,
    "creation_date": 1488873591,
    "view_count": 10331,
    "answer_count": 3,
    "tags": "machine-learning;nlp;tf-idf;word2vec;doc2vec"
  },
  {
    "question_id": 40453503,
    "title": "Information extraction in Python",
    "body": "<p>I am attempting to extract this type of information from the following paragraph structure:</p>\n<pre><code> women_ran men_ran kids_ran walked\n         1       2        1      3\n         2       4        3      1\n         3       6        5      2\n\ntext = [&quot;On Tuesday, one women ran on the street while 2 men ran and 1 child ran on the sidewalk. Also, there were 3 people walking.&quot;, &quot;One person was walking yesterday, but there were 2 women running as well as 4 men and 3 kids running.&quot;, &quot;The other day, there were three women running and also 6 men and 5 kids running on the sidewalk. Also, there were 2 people walking in the park.&quot;]\n</code></pre>\n<p>I am using Python's <code>spaCy</code> as my NLP library. What would be the best way to extract this tabular information from such sentences?</p>\n<p>If it was simply a matter of identifying whether there were individuals running or walking, I would just use <code>sklearn</code> to fit a classification model, but the information that I need to extract is obviously more granular than that (I am trying to retrieve subcategories and values for each).</p>\n",
    "score": 9,
    "creation_date": 1478460080,
    "view_count": 5451,
    "answer_count": 1,
    "tags": "python;nlp;information-extraction;spacy"
  },
  {
    "question_id": 40112373,
    "title": "How to classify new documents with tf-idf?",
    "body": "<p>If I use the <code>TfidfVectorizer</code> from <code>sklearn</code> to generate feature vectors as:</p>\n\n<p><code>features = TfidfVectorizer(min_df=0.2, ngram_range=(1,3)).fit_transform(myDocuments)</code></p>\n\n<p>How would I then generate feature vectors to classify a new document? Since you cant calculate the tf-idf for a single document. </p>\n\n<p>Would it be a correct approach, to extract the feature names with:</p>\n\n<p><code>feature_names = TfidfVectorizer.get_feature_names()</code></p>\n\n<p>and then count the term frequency for the new document according to the <code>feature_names</code>?</p>\n\n<p>But then I won't get the weights that have the information of a words importance.</p>\n",
    "score": 9,
    "creation_date": 1476804764,
    "view_count": 5196,
    "answer_count": 2,
    "tags": "python;scikit-learn;text-mining;tf-idf;text-analysis"
  },
  {
    "question_id": 12935936,
    "title": "How to programmatically access wordnet hierarchy?",
    "body": "<p>Suppose for any word I want to access its IS-A parent value and HAS-A value then is it possible using any api?</p>\n",
    "score": 9,
    "creation_date": 1350481965,
    "view_count": 5360,
    "answer_count": 3,
    "tags": "nlp;wordnet;word-sense-disambiguation;jaws-wordnet"
  },
  {
    "question_id": 66778944,
    "title": "Address Splitting with NLP",
    "body": "<p>I am working currently on a project that should identify each part of an address, for example from &quot;str. Jack London 121, Corvallis, ARAD, ap. 1603, 973130 &quot; the output should be like this:</p>\n<pre><code>street name: Jack London; \nno: 121; city: Corvallis; \nstate: ARAD; \napartment: 1603; \nzip code: 973130\n</code></pre>\n<p>The problem is that not all of the input data are in the same format so some of the elements may be missing or in different order, but it is guaranteed to be an address.</p>\n<p>I checked some sources on the internet, but a lot of them are adapted for US addresses only - like Google API Places, the thing is that I will use this for another country.</p>\n<p>Regex is not an option since the address may variate too much.</p>\n<p>I also thought about NLP to use Named Entity Recognition model but I'm not sure that will work.</p>\n<p>Do you know what could a be a good way to start, and maybe help me with some tips?</p>\n",
    "score": 9,
    "creation_date": 1616581126,
    "view_count": 4807,
    "answer_count": 1,
    "tags": "python;nlp;street-address;named-entity-recognition"
  },
  {
    "question_id": 45959618,
    "title": "what is the minimum dataset size needed for good performance with doc2vec?",
    "body": "<p>How does doc2vec perform when trained on different sized datasets? There is no mention of dataset size in the original corpus, so I am wondering what is the minimum size required to get good performance out of doc2vec. </p>\n",
    "score": 9,
    "creation_date": 1504093703,
    "view_count": 4275,
    "answer_count": 1,
    "tags": "nlp;doc2vec"
  },
  {
    "question_id": 42459373,
    "title": "Reduce Google&#39;s Word2Vec model with Gensim",
    "body": "<p>Loading the complete pre-trained word2vec model by <a href=\"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\" rel=\"noreferrer\">Google</a> is time intensive and tedious, therefore I was wondering if there is a chance to remove words below a certain frequency to bring the <code>vocab</code> count down to e.g. 200k words.</p>\n\n<p>I found Word2Vec methods in the <code>gensim</code> package to determine the word frequency and to re-save the model again, but I am not sure how to <code>pop</code>/<code>remove</code> vocab from the pre-trained model before saving it again. I couldn't find any hint in the <code>KeyedVector class</code> and the <code>Word2Vec class</code> for such an operation?</p>\n\n<p><a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py\" rel=\"noreferrer\">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py</a>\n<a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py\" rel=\"noreferrer\">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py</a></p>\n\n<p><strong>How can I select a subset of the vocabulary of the pre-trained word2vec model?</strong></p>\n",
    "score": 9,
    "creation_date": 1488044286,
    "view_count": 4123,
    "answer_count": 2,
    "tags": "nlp;gensim;word2vec"
  },
  {
    "question_id": 30497428,
    "title": "What is the acl tag in Stanford dependency parsing?",
    "body": "<p>An <code>acl</code> tag appears in Stanford dependency parses with no explanation in the manual. For example, a sentence like \"are you going there\" you gives something like:</p>\n\n<pre><code>root(ROOT-0, are-1)\nnsubj(are-1, you-2)\nacl(you-2, going-3)     &lt;--\nadvmod(going-3, there-4)\n</code></pre>\n\n<p>Can someone explain what this tag is?</p>\n",
    "score": 9,
    "creation_date": 1432788294,
    "view_count": 2623,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 3501436,
    "title": "Break/Decompose complex and compound sentences in nltk",
    "body": "<p>Is there a way to decompose complex sentences into simple sentences in nltk or other natural language processing libraries?</p>\n\n<p>For example:</p>\n\n<p>The park is so wonderful when the sun is setting and a cool breeze is blowing ==> The sun is setting. a cool breeze is blowing.  The park is so wonderful.</p>\n",
    "score": 9,
    "creation_date": 1282040549,
    "view_count": 6568,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 50275623,
    "title": "Difference between most_similar and similar_by_vector in gensim word2vec?",
    "body": "<p>I was confused with the results of most_similar and similar_by_vector from gensim's Word2vecKeyedVectors. They are supposed to calculate cosine similarities in the same way - however:</p>\n\n<p>Running them with one word gives identical results, for example:\nmodel.most_similar(['obama']) and similar_by_vector(model['obama'])</p>\n\n<p>but if I give it an equation:</p>\n\n<pre><code>model.most_similar(positive=['king', 'woman'], negative=['man'])\n</code></pre>\n\n<p>gives:</p>\n\n<pre><code>[('queen', 0.7515910863876343), ('monarch', 0.6741327047348022), ('princess', 0.6713887453079224), ('kings', 0.6698989868164062), ('kingdom', 0.5971318483352661), ('royal', 0.5921063423156738), ('uncrowned', 0.5911505818367004), ('prince', 0.5909028053283691), ('lady', 0.5904011130332947), ('monarchs', 0.5884358286857605)]\n</code></pre>\n\n<p>while with:</p>\n\n<pre><code>q = model['king'] - model['man'] + model['woman']\nmodel.similar_by_vector(q)\n</code></pre>\n\n<p>gives:</p>\n\n<pre><code>[('king', 0.8655095100402832), ('queen', 0.7673765420913696), ('monarch', 0.695580005645752), ('kings', 0.6929547786712646), ('princess', 0.6909604668617249), ('woman', 0.6528975963592529), ('lady', 0.6286187767982483), ('prince', 0.6222133636474609), ('kingdom', 0.6208546161651611), ('royal', 0.6090123653411865)]\n</code></pre>\n\n<p>There is a noticable difference in cosine distance of the words queen, monarch... etc. I'm wondering why?</p>\n\n<p>Thanks!</p>\n",
    "score": 9,
    "creation_date": 1525963497,
    "view_count": 10919,
    "answer_count": 1,
    "tags": "nlp;word2vec;gensim"
  },
  {
    "question_id": 38068539,
    "title": "Finding conditional probability of trigram in python nltk",
    "body": "<p>I have started learning <code>NLTK</code> and I am following a tutorial from <a href=\"http://www.katrinerk.com/courses/python-worksheets/language-models-in-python\" rel=\"noreferrer\">here</a>, where they find conditional probability using bigrams like this.</p>\n\n<pre><code>import nltk\nfrom nltk.corpus import brown\ncfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(brown.words()))\n</code></pre>\n\n<p>However I want to find conditional probability using trigrams. When I try to change <code>nltk.bigrams</code> to <code>nltk.trigrams</code> I get the following error.</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"home/env/local/lib/python2.7/site-packages/nltk/probability.py\", line 1705, in __init__\n    for (cond, sample) in cond_samples:\nValueError: too many values to unpack (expected 2)\n</code></pre>\n\n<p>How can I calculate the conditional probability using trigrams? </p>\n",
    "score": 9,
    "creation_date": 1467095145,
    "view_count": 7411,
    "answer_count": 2,
    "tags": "python;nlp;nltk;n-gram"
  },
  {
    "question_id": 13769242,
    "title": "Clustering words into groups",
    "body": "<p>This is a Homework question. I have a huge document full of words. My challenge is to classify these words into different groups/clusters that adequately represent the words. My strategy to deal with it is using the K-Means algorithm, which as you know takes the following steps.</p>\n\n<ol>\n<li>Generate k random means for the entire group</li>\n<li>Create K clusters by associating each word with the nearest mean</li>\n<li>Compute centroid of each cluster, which becomes the new mean</li>\n<li>Repeat Step 2 and Step 3 until a certain benchmark/convergence has been reached.</li>\n</ol>\n\n<p>Theoretically, I kind of get it, but not quite. I think at each step, I have questions that correspond to it, these are:</p>\n\n<ol>\n<li><p>How do I decide on k random means, technically I could say 5, but that may not necessarily be a good random number. So is this k purely a random number or is it actually driven by heuristics such as size of the dataset, number of words involved etc</p></li>\n<li><p>How do you associate each word with the nearest mean? Theoretically I can conclude that each word is associated by its distance to the nearest mean, hence if there are 3 means, any word that belongs to a specific cluster is dependent on which mean it has the shortest distance to. However, how is this actually computed? Between two words \"group\", \"textword\" and assume a mean word \"pencil\", how do I create a similarity matrix.</p></li>\n<li><p>How do you calculate the centroid?</p></li>\n<li><p>When you repeat step 2 and step 3, you are assuming each previous cluster as a new data set?</p></li>\n</ol>\n\n<p>Lots of questions, and I am obviously not clear. If there are any resources that I can read from, it would be great. Wikipedia did not suffice :( </p>\n",
    "score": 9,
    "creation_date": 1354906433,
    "view_count": 12586,
    "answer_count": 3,
    "tags": "cluster-analysis;k-means;text-analysis"
  },
  {
    "question_id": 10286058,
    "title": "When are n-grams (n&gt;3) important as opposed to just bigrams or trigrams?",
    "body": "<p>I am just wondering what is the use of n-grams (n>3) (and their occurrence frequency) considering the computational overhead in computing them. Are there any applications where bigrams or trigrams are simply not enough? </p>\n\n<p>If so, what is the state-of-the-art in n-gram extraction? Any suggestions? I am aware of the following:</p>\n\n<ul>\n<li><a href=\"http://acl.ldc.upenn.edu/C/C94/C94-1101.pdf\">A new method of n-gram statistics for large number of n and automatic\nextraction of words and phrases from large text data of Japanese</a></li>\n<li><a href=\"http://acl.ldc.upenn.edu/J/J01/J01-1001.pdf\">Using suffix arrays to compute term frequency and document frequency\nfor all substrings in a corpus</a> </li>\n<li><a href=\"http://acl.ldc.upenn.edu/J/J90/J90-1003.pdf\">Word association norms, mutual information, and lexicography</a></li>\n<li><a href=\"http://acl.ldc.upenn.edu/J/J93/J93-1007.pdf\">Retrieving collocations from text: Xtract</a></li>\n</ul>\n",
    "score": 9,
    "creation_date": 1335205255,
    "view_count": 4645,
    "answer_count": 5,
    "tags": "nlp;data-mining;nltk;n-gram"
  },
  {
    "question_id": 3158132,
    "title": "Verbally format a number in Python",
    "body": "<p>How do pythonistas print a number as words, like the equivalent of the Common Lisp code:</p>\n\n<pre><code>[3]&gt; (format t \"~r\" 1e25)\nnine septillion, nine hundred and ninety-nine sextillion, nine hundred and ninety-nine quintillion, seven hundred and seventy-eight quadrillion, one hundred and ninety-six trillion, three hundred and eight billion, three hundred and sixty-one million, two hundred and sixteen thousand\n</code></pre>\n",
    "score": 9,
    "creation_date": 1277990231,
    "view_count": 3899,
    "answer_count": 3,
    "tags": "python;formatting;nlp;language-comparisons"
  },
  {
    "question_id": 74181750,
    "title": "A checklist for Spacy optimization?",
    "body": "<p>I have been trying to understand how to systematically make Spacy run as fast as possible for a long time and I would like this post to become a wiki-style public post if possible.</p>\n<p>Here is what I currently know, with subsidiary questions on each point:</p>\n<p><strong>1. Space will run faster on faster hardware. For example, try a computer with more CPU cores, or more RAM/primary memory.</strong></p>\n<p>What I do not know:</p>\n<ul>\n<li><em>What specific aspects of the execution of Spacy - especially the main one of instantiating the</em> <code>Doc</code> <em>object - depend more on CPU vs. RAM and why?</em></li>\n<li><em>Is the instantiation of a</em> <code>Doc</code> <em>object a sequence of arithmetical calculations (the compiled binary of the neural networks), so the more CPU cores, the more calculations can be done at once, therefore faster? Does that mean increasing RAM would not make this process faster?</em></li>\n<li><em>Are there any other aspects of CPUs or GPUs to watch out for, other than cores, that would make one chip better than another, for Spacy? Someone mentioned &quot;hyper threading&quot;.</em></li>\n<li><em>Is there any standard mathematical estimate of time per pipeline component, such as parser, relative to input string length? Like Parser, seconds = number of characters in input? / number of CPU cores</em></li>\n</ul>\n<p><strong>2. You can make Spacy run faster by removing <a href=\"https://spacy.io/usage/spacy-101#pipelines\" rel=\"noreferrer\">components</a> you don't need, for example by</strong> <code>nlp = spacy.load(&quot;en_core_web_sm&quot;, disable=['tagger', 'ner', 'lemmatizer', 'textcat'])</code></p>\n<ul>\n<li><em>Just loading the Spacy module itself with <code>import spacy</code> is slightly slow. If you haven't even loaded the language model yet, what are the most significant things being loaded here, apart from just adding functions to the namespace? Is it possible to only load a part of the module you need?</em></li>\n</ul>\n<p><strong>3. You can make Spacy faster by using certain options that simply make it run faster.</strong></p>\n<ul>\n<li><em>I have read about multiprocessing with</em> <code>nlp.pipe</code>, <code>n_process</code>, <code>batch_size</code> and <code>joblib</code><em>, but that's for multiple documents and I'm only doing a single document right now.</em></li>\n</ul>\n<p><strong>4. You can make Spacy faster by minimising the number of times it has to perform the same operations.</strong></p>\n<ul>\n<li><p><em>You can keep Spacy alive on a server and pass processing commands to it when you need to</em></p>\n</li>\n<li><p><em>You can serialize a</em> <code>Doc</code> <em>to reload it later, and you can further exclude attributes you don't need with</em> <code>doc.to_bytes(exclude=[&quot;tensor&quot;])</code> or <code>doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])</code></p>\n</li>\n</ul>\n<p><strong>5. Anything else?</strong></p>\n",
    "score": 9,
    "creation_date": 1666617784,
    "view_count": 3935,
    "answer_count": 1,
    "tags": "optimization;nlp;spacy;micro-optimization"
  },
  {
    "question_id": 53301916,
    "title": "Python/Gensim - What is the meaning of syn0 and syn0norm?",
    "body": "<p>I know that in <em>gensims</em> <em><code>KeyedVectors</code>-model</em>, one can access the embedding matrix by the attribute <code>model.syn0</code>. There is also a <code>syn0norm</code>, which doesn't seem to work for the <em>glove</em> model I recently loaded. I think I also have seen <code>syn1</code> somewhere previously. </p>\n\n<p>I haven't found a doc-string for this and I'm just wondering what's the logic behind this?</p>\n\n<p>So if <code>syn0</code> is the embedding matrix, what is <code>syn0norm</code>? What would then <code>syn1</code> be and generally, what does <code>syn</code> stand for?</p>\n",
    "score": 9,
    "creation_date": 1542203793,
    "view_count": 8692,
    "answer_count": 1,
    "tags": "python;deep-learning;nlp;gensim;word-embedding"
  },
  {
    "question_id": 41661801,
    "title": "Python - calculate the co-occurrence matrix",
    "body": "<p>I'm working on an NLP task and I need to calculate the co-occurrence matrix over documents. The basic formulation is as below:</p>\n\n<p>Here I have a matrix with shape <code>(n, length)</code>, where each row represents a sentence composed by <code>length</code> words. So there are <code>n</code> sentences with same length in all. Then with a defined context size, e.g., <code>window_size = 5</code>, I want to calculate the co-occurrence matrix <code>D</code>, where the entry in the <code>cth</code> row and <code>wth</code> column is <code>#(w,c)</code>, which means the number of times that a context word <code>c</code> appears in <code>w</code>'s context.</p>\n\n<p>An example can be referred here. <a href=\"https://linguistics.stackexchange.com/questions/3641/how-to-calculate-the-co-occurrence-between-two-words-in-a-window-of-text\">How to calculate the co-occurrence between two words in a window of text?</a></p>\n\n<p>I know it can be calculate by stacking loops, but I want to know if there exits an simple way or simple function? I have find some answers but they cannot work with a window sliding through the sentence. For example:<a href=\"https://stackoverflow.com/questions/35562789/word-word-co-occurrence-matrix\">word-word co-occurrence matrix</a></p>\n\n<p>So could anyone tell me is there any function in Python can deal with this problem concisely? Cause I think this task is quite common in NLP things.</p>\n",
    "score": 9,
    "creation_date": 1484488008,
    "view_count": 23492,
    "answer_count": 2,
    "tags": "python;matrix;machine-learning;nlp"
  },
  {
    "question_id": 36001230,
    "title": "doc2vec: How is PV-DBOW implemented",
    "body": "<p>I know that there exists already an implementation of PV-DBOW (paragraph vector) in python (gensim).\nBut I'm interested in knowing how to implement it myself.\nThe explanation from the <a href=\"https://cs.stanford.edu/~quocle/paragraph_vector.pdf\" rel=\"nofollow noreferrer\">official paper</a> for PV-DBOW is as follows:</p>\n\n<blockquote>\n  <p>Another way is to ignore the context words in the input, but force the model to predict words randomly sampled from the paragraph in the output. In reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window, then sample a random word from the text window and form a classification task given the Paragraph Vector.</p>\n</blockquote>\n\n<p>According to the paper the word vectors are not stored\nand PV-DBOW is said to work similar to skip gram in word2vec.</p>\n\n<p>Skip-gram is explained in <a href=\"http://arxiv.org/pdf/1411.2738v3.pdf\" rel=\"nofollow noreferrer\">word2vec Parameter Learning</a>.\nIn the skip gram model the word vectors are mapped to the hidden layer.\nThe matrix that performs this mapping is updated during the training.\nIn PV-DBOW the dimension of the hidden layer should be the dimension of one paragraph vector. When I want to multiply the word vector of a sampled example with the paragraph vector they should have the same size.\nThe original representation of a word is of size (vocabulary size x 1). Which mapping is performed to get the right size (paragraph dimension x 1)\nin the hidden layer. And how is this mapping performed when the word vectors are not stored?\nI assume that word and paragraph representation should have the same size in the hidden layer because of equation 26 in <a href=\"http://arxiv.org/pdf/1411.2738v3.pdf\" rel=\"nofollow noreferrer\">word2vec Parameter Learning</a></p>\n",
    "score": 9,
    "creation_date": 1458006134,
    "view_count": 2196,
    "answer_count": 1,
    "tags": "machine-learning;nlp;neural-network;gensim;word2vec"
  },
  {
    "question_id": 9575920,
    "title": "Unsupervised HMM training in NLTK",
    "body": "<p>I am just trying to do very simple unsupervised HMM training in <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">nltk</a>.</p>\n\n<p>Consider:</p>\n\n<pre><code>import nltk\ntrainer = nltk.tag.hmm.HiddenMarkovModelTrainer()\nfrom nltk.corpus import gutenberg\nemma = gutenberg.words('austen-emma.txt')\nm = trainer.train_unsupervised(emma)\nValueError: A Uniform probability distribution must have at least one sample.\n</code></pre>\n\n<p>Can I find an example of using <a href=\"http://nltk.googlecode.com/svn/trunk/doc/api/nltk.tag.hmm-pysrc.html#HiddenMarkovModelTrainer.train_supervised\" rel=\"noreferrer\"><code>nltk.tag.hmm.HiddenMarkovModelTrainer.train_unsupervised</code></a>?</p>\n",
    "score": 9,
    "creation_date": 1330991505,
    "view_count": 5759,
    "answer_count": 2,
    "tags": "nlp;nltk;hidden-markov-models"
  },
  {
    "question_id": 6612062,
    "title": "The relationship between latent Dirichlet allocation and documents clustering",
    "body": "<p>I would like to clarify the relationship between latent Dirichlet allocation (LDA) and the generic task of document clustering. </p>\n\n<p>The LDA analysis tends to output the topic proportions for each document. If my understanding is correct, this is not the direct result of document clustering. However, we can treat this probability proportions as a feature reprsentation for each document. Afterwards, we can invoke other established clustering method based on the feature configurations generated by LDA analysis.</p>\n\n<p>Is my understanding correct? Thanks.</p>\n",
    "score": 9,
    "creation_date": 1310048257,
    "view_count": 1965,
    "answer_count": 1,
    "tags": "nlp;machine-learning;data-mining;text-mining;lda"
  },
  {
    "question_id": 57216216,
    "title": "Keras Multitask learning with two different input sample size",
    "body": "<p>I am implementing multitask regression model using code from the Keras <a href=\"https://keras.io/getting-started/functional-api-guide/\" rel=\"noreferrer\">API</a> under the shared layers section.</p>\n\n<p>There are two data sets, Let's call them <code>data_1</code> and <code>data_2</code> as follows.</p>\n\n<pre><code>data_1 : shape(1434, 185, 37)\ndata_2 : shape(283, 185, 37)\n</code></pre>\n\n<p><code>data_1</code> is consists of 1434 samples, each sample is 185 characters long and 37 shows total number of unique characters is 37 or in another words the <code>vocab_size</code>. Comparatively <code>data_2</code> consists of 283 characters.</p>\n\n<p>I convert the <code>data_1</code> and <code>data_2</code> into two dimensional numpy array as follows before giving it to the Embedding layer.</p>\n\n<pre><code>data_1=np.argmax(data_1, axis=2)\ndata_2=np.argmax(data_2, axis=2)\n</code></pre>\n\n<p>That makes the shape of the data as follows.</p>\n\n<pre><code>print(np.shape(data_1)) \n(1434, 185)\n\nprint(np.shape(data_2)) \n(283, 185)\n</code></pre>\n\n<p>Each number in the matrix represents index integer.</p>\n\n<p>The multitask model is as under.</p>\n\n<pre><code>user_input = keras.layers.Input(shape=((185, )), name='Input_1')\nproducts_input =  keras.layers.Input(shape=((185, )), name='Input_2')\n\nshared_embed=(keras.layers.Embedding(vocab_size, 50, input_length=185))\n\nuser_vec_1 = shared_embed(user_input )\nuser_vec_2 = shared_embed(products_input )\n\n\ninput_vecs = keras.layers.concatenate([user_vec_1, user_vec_2], name='concat')\n\ninput_vecs_1=keras.layers.Flatten()(input_vecs)\ninput_vecs_2=keras.layers.Flatten()(input_vecs)\n\n# Task 1 FC layers\nnn = keras.layers.Dense(90, activation='relu',name='layer_1')(input_vecs_1)\nresult_a = keras.layers.Dense(1, activation='linear', name='output_1')(nn)\n\n\n\n# Task 2 FC layers\nnn1 = keras.layers.Dense(90, activation='relu', name='layer_2')(input_vecs_2)\nresult_b = keras.layers.Dense(1, activation='linear',name='output_2')(nn1) \n\nmodel = Model(inputs=[user_input , products_input], outputs=[result_a, result_b])\n\nmodel.compile(optimizer='rmsprop',\n              loss='mse',\n              metrics=['accuracy'])\n</code></pre>\n\n<p>The model is visualized as follows.\n<a href=\"https://i.sstatic.net/ni7nK.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ni7nK.png\" alt=\"enter image description here\"></a></p>\n\n<p>Then I fit the model as follows.</p>\n\n<pre><code>model.fit([data_1, data_2], [Y_1,Y_2], epochs=10)\n</code></pre>\n\n<p><strong>Error:</strong></p>\n\n<pre><code>ValueError: All input arrays (x) should have the same number of samples. Got array shapes: [(1434, 185), (283, 185)]\n</code></pre>\n\n<p>Is there any way in Keras where I can use two different sample size inputs or to some trick to avoid this error to achieve my goal of multitasking regression.</p>\n\n<p><strong>Here is the minimum working code for testing</strong>.</p>\n\n<pre><code>data_1=np.array([[25,  5, 11, 24,  6],\n       [25,  5, 11, 24,  6],\n       [25,  0, 11, 24,  6],\n       [25, 11, 28, 11, 24],\n       [25, 11,  6, 11, 11]])\n\ndata_2=np.array([[25, 11, 31,  6, 11],\n       [25, 11, 28, 11, 31],\n       [25, 11, 11, 11, 31]])\n\nY_1=np.array([[2.33],\n       [2.59],\n       [2.59],\n       [2.54],\n       [4.06]])\n\n\nY_2=np.array([[2.9],\n       [2.54],\n       [4.06]])\n\n\n\nuser_input = keras.layers.Input(shape=((5, )), name='Input_1')\nproducts_input =  keras.layers.Input(shape=((5, )), name='Input_2')\n\nshared_embed=(keras.layers.Embedding(37, 3, input_length=5))\nuser_vec_1 = shared_embed(user_input )\nuser_vec_2 = shared_embed(products_input )\n\ninput_vecs = keras.layers.concatenate([user_vec_1, user_vec_2], name='concat')\n\n\ninput_vecs_1=keras.layers.Flatten()(input_vecs) \ninput_vecs_2=keras.layers.Flatten()(input_vecs)\n\n    nn = keras.layers.Dense(90, activation='relu',name='layer_1')(input_vecs_1)\n    result_a = keras.layers.Dense(1, activation='linear', name='output_1')(nn)\n\n    # Task 2 FC layers\n    nn1 = keras.layers.Dense(90, activation='relu', name='layer_2')(input_vecs_2)\n\n    result_b = keras.layers.Dense(1, activation='linear',name='output_2')(nn1)\n\nmodel = Model(inputs=[user_input , products_input], outputs=[result_a, result_b])\n\nmodel.compile(optimizer='rmsprop',\n              loss='mse',\n              metrics=['accuracy'])\n\nmodel.fit([data_1, data_2], [Y_1,Y_2], epochs=10)\n</code></pre>\n",
    "score": 9,
    "creation_date": 1564130245,
    "view_count": 3962,
    "answer_count": 3,
    "tags": "python;tensorflow;machine-learning;keras;nlp"
  },
  {
    "question_id": 44653180,
    "title": "Are there examples of using reinforcement learning for text classification?",
    "body": "<p>Imagine a binary classification problem like sentiment analysis. Since we have the labels, cant we use the gap between actual - predicted as reward for RL ?</p>\n\n<p>I wish to try Reinforcement Learning for Classification Problems</p>\n",
    "score": 9,
    "creation_date": 1497961933,
    "view_count": 7126,
    "answer_count": 1,
    "tags": "machine-learning;nlp;deep-learning;reinforcement-learning"
  },
  {
    "question_id": 10688739,
    "title": "Resolve coreference using Stanford CoreNLP - unable to load parser model",
    "body": "<p>I want to do a very simple job: given a string containing pronouns, I want to resolve them.</p>\n\n<p>for example, I want to turn the sentence \"Mary has a little lamb. She is cute.\" in \"Mary has a little lamb. Mary is cute.\".</p>\n\n<p>I have tried to use Stanford CoreNLP. However, I seem unable to get the parser to start. I have imported all the included jars in my project using Eclipse, and I have allocated 3GB to the JVM (-Xmx3g).</p>\n\n<p>The error is very awkward:</p>\n\n<blockquote>\n  <p>Exception in thread \"main\" java.lang.NoSuchMethodError:\n  edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(Ljava/lang/String;[Ljava/lang/String;)Ledu/stanford/nlp/parser/lexparser/LexicalizedParser;</p>\n</blockquote>\n\n<p>I don't understand where that L comes from, I think it is the root of my problem... This is rather weird. I have tried to get inside the source files, but there is no wrong reference there.</p>\n\n<p>Code:</p>\n\n<pre><code>import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;\nimport edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;\nimport edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefGraphAnnotation;\nimport edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;\nimport edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;\nimport edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;\nimport edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;\nimport edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;\nimport edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;\nimport edu.stanford.nlp.ling.CoreLabel;\nimport edu.stanford.nlp.dcoref.CorefChain;\nimport edu.stanford.nlp.pipeline.*;\nimport edu.stanford.nlp.trees.Tree;\nimport edu.stanford.nlp.semgraph.SemanticGraph;\nimport edu.stanford.nlp.util.CoreMap;\nimport edu.stanford.nlp.util.IntTuple;\nimport edu.stanford.nlp.util.Pair;\nimport edu.stanford.nlp.util.Timing;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\n\nimport java.util.Properties;\n\npublic class Coref {\n\n/**\n * @param args the command line arguments\n */\npublic static void main(String[] args) throws IOException, ClassNotFoundException {\n    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution \n    Properties props = new Properties();\n    props.put(\"annotators\", \"tokenize, ssplit, pos, lemma, ner, parse, dcoref\");\n    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n\n    // read some text in the text variable\n    String text = \"Mary has a little lamb. She is very cute.\"; // Add your text here!\n\n    // create an empty Annotation just with the given text\n    Annotation document = new Annotation(text);\n\n    // run all Annotators on this text\n    pipeline.annotate(document);\n\n    // these are all the sentences in this document\n    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types\n    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);\n\n    for(CoreMap sentence: sentences) {\n      // traversing the words in the current sentence\n      // a CoreLabel is a CoreMap with additional token-specific methods\n      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {\n        // this is the text of the token\n        String word = token.get(TextAnnotation.class);\n        // this is the POS tag of the token\n        String pos = token.get(PartOfSpeechAnnotation.class);\n        // this is the NER label of the token\n        String ne = token.get(NamedEntityTagAnnotation.class);       \n      }\n\n      // this is the parse tree of the current sentence\n      Tree tree = sentence.get(TreeAnnotation.class);\n      System.out.println(tree);\n\n      // this is the Stanford dependency graph of the current sentence\n      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);\n    }\n\n    // This is the coreference link graph\n    // Each chain stores a set of mentions that link to each other,\n    // along with a method for getting the most representative mention\n    // Both sentence and token offsets start at 1!\n    Map&lt;Integer, CorefChain&gt; graph = \n      document.get(CorefChainAnnotation.class);\n    System.out.println(graph);\n  }\n}\n</code></pre>\n\n<p>Full stack trace:</p>\n\n<blockquote>\n  <p>Adding annotator tokenize\n  Adding annotator ssplit\n  Adding annotator pos\n  Loading POS Model [edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger] ... Loading default properties from trained tagger edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger\n  Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.1 sec].\n  done [2.2 sec].\n  Adding annotator lemma\n  Adding annotator ner\n  Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [4.0 sec].\n  Loading classifier from edu/stanford/nlp/models/ner/english.muc.distsim.crf.ser.gz ... done [3.0 sec].\n  Loading classifier from edu/stanford/nlp/models/ner/english.conll.distsim.crf.ser.gz ... done [3.3 sec].\n  Adding annotator parse\n  Exception in thread \"main\" java.lang.NoSuchMethodError: edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(Ljava/lang/String;[Ljava/lang/String;)Ledu/stanford/nlp/parser/lexparser/LexicalizedParser;\n      at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:115)\n      at edu.stanford.nlp.pipeline.ParserAnnotator.(ParserAnnotator.java:64)\n      at edu.stanford.nlp.pipeline.StanfordCoreNLP$12.create(StanfordCoreNLP.java:603)\n      at edu.stanford.nlp.pipeline.StanfordCoreNLP$12.create(StanfordCoreNLP.java:585)\n      at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:62)\n      at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:329)\n      at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:196)\n      at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:186)\n      at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:178)\n      at Coref.main(Coref.java:41)</p>\n</blockquote>\n",
    "score": 9,
    "creation_date": 1337616659,
    "view_count": 7785,
    "answer_count": 1,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 6448002,
    "title": "How do you find the subject of a sentence?",
    "body": "<p>I am new to NLP and was doing research about what language toolkit I should be using to do the following. I would like to do one of the two things which accomplishes the same thing:</p>\n\n<ol>\n<li><p>I basically would like to classify a text, usually one sentence that contains 15 words. Would like to classify if the sentence is talking about a specific subject.  </p></li>\n<li><p>Is there a tool that given a sentence, it finds out the subject of a sentence.  </p></li>\n</ol>\n\n<p>I am using PHP and Java but the tool can be anything that runs on Linux command line</p>\n\n<p>Thank you very much.</p>\n",
    "score": 9,
    "creation_date": 1308787745,
    "view_count": 5124,
    "answer_count": 2,
    "tags": "java;php;nlp"
  },
  {
    "question_id": 744237,
    "title": "Naive bayes calculation in sql",
    "body": "<p>I want to use naive bayes to classify documents into a relatively large number of classes. I'm looking to confirm whether an mention of an entity name in an article really is that entity, on the basis of whether that article is similar to articles where that entity has been correctly verified.</p>\n\n<p>Say, we find the text \"General Motors\" in an article. We have a set of data that contains articles and the correct entities mentioned within in. So, if we have found \"General Motors\" mentioned in a new article, should it fall into that class of articles in the prior data that contained a known genuine mention \"General Motors\" vs. the class of articles which did not mention that entity?</p>\n\n<p>(I'm not creating a class for every entity and trying to classify every new article into every possible class. I already have a heuristic method for finding plausible mentions of entity names, and I just want to verify the plausibility of the limited number of entity name mentions per article that the method already detects.)</p>\n\n<p>Given that the number of potential classes and articles was quite large and naive bayes is relatively simple, I wanted to do the whole thing in sql, but I'm having trouble with the scoring query...</p>\n\n<p>Here's what I have so far:</p>\n\n<pre><code>CREATE TABLE `each_entity_word` (\n  `word` varchar(20) NOT NULL,\n  `entity_id` int(10) unsigned NOT NULL,\n  `word_count` mediumint(8) unsigned NOT NULL,\n  PRIMARY KEY (`word`, `entity_id`)\n);\n\nCREATE TABLE `each_entity_sum` (\n  `entity_id` int(10) unsigned NOT NULL DEFAULT '0',\n  `word_count_sum` int(10) unsigned DEFAULT NULL,\n  `doc_count` mediumint(8) unsigned NOT NULL,\n  PRIMARY KEY (`entity_id`)\n);\n\nCREATE TABLE `total_entity_word` (\n  `word` varchar(20) NOT NULL,\n  `word_count` int(10) unsigned NOT NULL,\n  PRIMARY KEY (`word`)\n);\n\nCREATE TABLE `total_entity_sum` (\n  `word_count_sum` bigint(20) unsigned NOT NULL,\n  `doc_count` int(10) unsigned NOT NULL,\n  `pkey` enum('singleton') NOT NULL DEFAULT 'singleton',\n  PRIMARY KEY (`pkey`)\n);\n</code></pre>\n\n<p>Each article in the marked data is split into distinct words, and for each article for each entity every word is added to <code>each_entity_word</code> and/or its <code>word_count</code> is incremented and <code>doc_count</code> is incremented in <code>entity_word_sum</code>, both with respect to an <code>entity_id</code>. This is repeated for each entity known to be mentioned in that article.</p>\n\n<p>For each article regardless of the entities contained within for each word <code>total_entity_word</code> <code>total_entity_word_sum</code> are similarly incremented.</p>\n\n<ul>\n<li>P(word|any document) should equal the\n<code>word_count</code> in <code>total_entity_word</code> for that word over\n<code>doc_count</code> in <code>total_entity_sum</code></li>\n<li>P(word|document mentions entity <em>x</em>)\nshould equal <code>word_count</code> in\n<code>each_entity_word</code> for that word for <code>entity_id</code> <em>x</em> over <code>doc_count</code> in\n<code>each_entity_sum</code> for <code>entity_id</code> <em>x</em></li>\n<li>P(word|document does <em>not</em> mention entity <em>x</em>) should equal (the <code>word_count</code> in <code>total_entity_word</code> minus its <code>word_count</code> in <code>each_entity_word</code> for that word for that entity) over (the <code>doc_count</code> in <code>total_entity_sum</code> minus <code>doc_count</code> for that entity in <code>each_entity_sum</code>)</li>\n<li>P(document mentions entity <em>x</em>) should equal <code>doc_count</code> in <code>each_entity_sum</code> for that entity id over <code>doc_count</code> in <code>total_entity_word</code></li>\n<li>P(document does not mention entity <em>x</em>) should equal 1 minus (<code>doc_count</code> in <code>each_entity_sum</code> for <em>x</em>'s entity id over <code>doc_count</code> in <code>total_entity_word</code>).</li>\n</ul>\n\n<p>For a new article that comes in, split it into words and just select where word in ('I', 'want', 'to', 'use'...) against either <code>each_entity_word</code> or <code>total_entity_word</code>. In the db platform I'm working with (mysql) IN clauses are relatively well optimized.</p>\n\n<p>Also there is no product() aggregate function in sql, so of course you can just do sum(log(x)) or exp(sum(log(x))) to get the equivalent of product(x).</p>\n\n<p>So, if I get a new article in, split it up into distinct words and put those words into a big IN() clause and a potential entity id to test, how can I get the naive bayesian probability that the article falls into that entity id's class in sql?</p>\n\n<p>EDIT:</p>\n\n<p>Try #1:</p>\n\n<pre><code>set @entity_id = 1;\n\nselect @entity_doc_count = doc_count from each_entity_sum where entity_id=@entity_id;\n\nselect @total_doc_count = doc_count from total_entity_sum;\n\nselect \n            exp(\n\n                log(@entity_doc_count / @total_doc_count) + \n\n                (\n                    sum(log((ifnull(ew.word_count,0) + 1) / @entity_doc_count)) / \n                    sum(log(((aew.word_count + 1) - ifnull(ew.word_count, 0)) / (@total_doc_count - @entity_doc_count)))\n                )\n\n            ) as likelihood,\n        from total_entity_word aew \n        left outer join each_entity_word ew on ew.word=aew.word and ew.entity_id=@entity_id\n\n        where aew.word in ('I', 'want', 'to', 'use'...);\n</code></pre>\n",
    "score": 9,
    "creation_date": 1239636074,
    "view_count": 7124,
    "answer_count": 5,
    "tags": "sql;mysql;machine-learning;nlp;bayesian"
  },
  {
    "question_id": 65227103,
    "title": "Clause extraction / long sentence segmentation in python",
    "body": "<p>I'm currently working on a project involving sentence vectors (from a RoBERTa pretrained model). These vectors are lower quality when sentences are long, and my corpus contains many long sentences with subclauses.</p>\n<p>I've been looking for methods for clause extraction / long sentence segmentation, but I was surprised to see that none of the major NLP packages (e.g., spacy or stanza) offer this out of the box.</p>\n<p>I suppose this could be done by using either spacy or stanza's dependency parsing, but it would probably be quite complicated to handle all kinds of convoluted sentences and edge cases properly.</p>\n<p>I've come across <a href=\"https://github.com/mmxgn/spacy-clausie\" rel=\"noreferrer\">this implementation</a> of the the ClausIE information extraction system with spacy that does something similar, but it hasn't been updated and doesn't work on my machine.</p>\n<p>I've also come across <a href=\"https://github.com/freyamehta99/Sentence-Simplification\" rel=\"noreferrer\">this repo</a> for sentence simplification, but I get an annotation error from Stanford coreNLP when I run it locally.</p>\n<p>Is there any obvious package/method that I've overlooked? If not, is there a simple way to implement this with stanza or spacy?</p>\n",
    "score": 9,
    "creation_date": 1607562283,
    "view_count": 7540,
    "answer_count": 1,
    "tags": "python;nlp;stanford-nlp;spacy;bert-language-model"
  },
  {
    "question_id": 64013808,
    "title": "Why BERT model have to keep 10% MASK token unchanged?",
    "body": "<p>I am reading BERT model paper. In Masked Language Model task during pre-training BERT model, the paper said the model will choose 15% token ramdomly. In the chose token (Ti), 80% it will be replaced with [MASK] token, 10% Ti is unchanged and 10% Ti replaced with another word. I think the model just need to replace with [MASK] or another word is enough. Why does the model have to choose randomly a word and keep it unchanged? Does pre-training process predict only [MASK] token or it predict 15% a whole random token?</p>\n",
    "score": 9,
    "creation_date": 1600791630,
    "view_count": 4475,
    "answer_count": 1,
    "tags": "deep-learning;nlp;bert-language-model"
  },
  {
    "question_id": 63030692,
    "title": "How do I use BertForMaskedLM or BertModel to calculate perplexity of a sentence?",
    "body": "<p>I want to use BertForMaskedLM or BertModel to calculate perplexity of a sentence, so I write code like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertForMaskedLM\n# Load pre-trained model (weights)\nwith torch.no_grad():\n    model = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm-ext')\n    model.eval()\n    # Load pre-trained model tokenizer (vocabulary)\n    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')\n    sentence = &quot;我不会忘记和你一起奋斗的时光。&quot;\n    tokenize_input = tokenizer.tokenize(sentence)\n    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n    sen_len = len(tokenize_input)\n    sentence_loss = 0.\n\n    for i, word in enumerate(tokenize_input):\n        # add mask to i-th character of the sentence\n        tokenize_input[i] = '[MASK]'\n        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n\n        output = model(mask_input)\n\n        prediction_scores = output[0]\n        softmax = nn.Softmax(dim=0)\n        ps = softmax(prediction_scores[0, i]).log()\n        word_loss = ps[tensor_input[0, i]]\n        sentence_loss += word_loss.item()\n\n        tokenize_input[i] = word\n    ppl = np.exp(-sentence_loss/sen_len)\n    print(ppl)\n</code></pre>\n<p>I think this code is right, but I also notice BertForMaskedLM's paramaters <code>masked_lm_labels</code>, so could I use this paramaters to calculate PPL of a sentence easiler?\nI know the input_ids argument is the masked input, the masked_lm_labels argument is the desired output. But I couldn't understand the actual meaning of its output loss, its code like this:</p>\n<pre><code>if masked_lm_labels is not None:\n    loss_fct = CrossEntropyLoss()  # -100 index = padding token\n    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), \n    masked_lm_labels.view(-1))\n    outputs = (masked_lm_loss,) + outputs\n</code></pre>\n",
    "score": 9,
    "creation_date": 1595408820,
    "view_count": 8625,
    "answer_count": 1,
    "tags": "nlp;pytorch;transformer-model;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 49750112,
    "title": "Gensim: how to load precomputed word vectors from text file",
    "body": "<p>I have a text file with my precomputed word vectors in the following format (example):</p>\n\n<p><code>word -0.0762464299711 0.0128308048976 ... 0.0712385589283\\n’</code></p>\n\n<p>on each line for every word (with 297 extra floats in place of the <code>...</code>). I am trying to load these with Gensim as KeyedVectors, because I ultimately would like to compute the cosine similarity, find most similar words, etc. Unfortunately I have not worked with Gensim before and from the documentation it's not quite clear to me how to do this. I have tried the following which I found <a href=\"https://radimrehurek.com/gensim/models/keyedvectors.html\" rel=\"noreferrer\">here</a>:</p>\n\n<p><code>word_vectors = KeyedVectors.load_word2vec_format('/embeddings/word.vectors', binary=False)</code></p>\n\n<p>However this gives the following error:</p>\n\n<p><code>ValueError: invalid literal for int() with base 10: 'the'</code></p>\n\n<p>'the' is the first word in the text file, so I suspect that the loading function is expecting something to be there that is not. But I can't find any information on what should be there. I would highly appreciate a pointer to such information or any other solution to my problem. Thanks!</p>\n",
    "score": 9,
    "creation_date": 1523352619,
    "view_count": 10498,
    "answer_count": 1,
    "tags": "python;python-3.x;nlp;gensim"
  },
  {
    "question_id": 2425614,
    "title": "Natural Language Processing - Word Alignment",
    "body": "<p>I am looking for word alignment tools and algorithms.<br>\nI am dealing with bilingual English - Hindi text, and currently working on </p>\n\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Dynamic_time_warping\" rel=\"noreferrer\"><strong>DTW</strong></a> (Dynamic Time Warping) algorithm</li>\n<li><strong>CLA</strong> (Competitive Linking Algorithm)</li>\n<li><a href=\"http://linguateca.di.uminho.pt/natools/\" rel=\"noreferrer\"><strong>NATools</strong></a></li>\n<li><a href=\"http://www.fjoch.com/GIZA++.html\" rel=\"noreferrer\"><strong>Giza++</strong></a></li>\n</ul>\n\n<p>Could you please suggest any other algorithm/tool which is language independent and which could achieve <strong>Statistical word alignment for parallel English Hindi Corpora and its evaluation</strong>.<br>\nSome tools are best for certain languages; could you please tell me how true that is and, if so, could you please provide an example of what would be better suited  for Asian languages like Hindi.  Counter-examples of what one shouldn't I use for such languages is also welcome.</p>\n\n<p>I have heard a bit about <a href=\"http://stp.lingfil.uu.se/~joerg/uplug/\" rel=\"noreferrer\"><strong>Uplug word aligner</strong></a>... Could someone tell me if this tool is useful for my purpose.</p>\n\n<p>Thank you.. :)</p>\n",
    "score": 9,
    "creation_date": 1268317106,
    "view_count": 4320,
    "answer_count": 4,
    "tags": "alignment;nlp;linguistics"
  },
  {
    "question_id": 57395165,
    "title": "Extracting a person&#39;s age from unstructured text in Python",
    "body": "<p>I have a dataset of administrative filings that include short biographies. I am trying to extract people's ages by using python and some pattern matching. Some example of sentences are:</p>\n\n<ul>\n<li>\"Mr Bond, 67, is an engineer in the UK\"</li>\n<li>\"Amanda B. Bynes, 34, is an actress\"</li>\n<li>\"Peter Parker (45) will be our next administrator\"</li>\n<li>\"Mr. Dylan is 46 years old.\"</li>\n<li>\"Steve Jones, Age: 32,\"</li>\n</ul>\n\n<p>These are some of the patterns I have identified in the dataset. I want to add that there are other patterns, but I have not run into them yet, and not sure how I could get to that. I wrote the following code that works pretty well, but is pretty inefficient so will take too much time to run on the whole dataset. </p>\n\n<pre><code>#Create a search list of expressions that might come right before an age instance\nage_search_list = [\" \" + last_name.lower().strip() + \", age \",\n\" \" + clean_sec_last_name.lower().strip() + \" age \",\nlast_name.lower().strip() + \" age \",\nfull_name.lower().strip() + \", age \",\nfull_name.lower().strip() + \", \",\n\" \" + last_name.lower() + \", \",\n\" \" + last_name.lower().strip()  + \" \\(\",\n\" \" + last_name.lower().strip()  + \" is \"]\n\n#for each element in our search list\nfor element in age_search_list:\n    print(\"Searching: \",element)\n\n    # retrieve all the instances where we might have an age\n    for age_biography_instance in re.finditer(element,souptext.lower()):\n\n        #extract the next four characters\n        age_biography_start = int(age_biography_instance.start())\n        age_instance_start = age_biography_start + len(element)\n        age_instance_end = age_instance_start + 4\n        age_string = souptext[age_instance_start:age_instance_end]\n\n        #extract what should be the age\n        potential_age = age_string[:-2]\n\n        #extract the next two characters as a security check (i.e. age should be followed by comma, or dot, etc.)\n        age_security_check = age_string[-2:]\n        age_security_check_list = [\", \",\". \",\") \",\" y\"]\n\n        if age_security_check in age_security_check_list:\n            print(\"Potential age instance found for \",full_name,\": \",potential_age)\n\n            #check that what we extracted is an age, convert it to birth year\n            try:\n                potential_age = int(potential_age)\n                print(\"Potential age detected: \",potential_age)\n                if 18 &lt; int(potential_age) &lt; 100:\n                    sec_birth_year = int(filing_year) - int(potential_age)\n                    print(\"Filing year was: \",filing_year)\n                    print(\"Estimated birth year for \",clean_sec_full_name,\": \",sec_birth_year)\n                    #Now, we save it in the main dataframe\n                    new_sec_parser = pd.DataFrame([[clean_sec_full_name,\"0\",\"0\",sec_birth_year,\"\"]],columns = ['Name','Male','Female','Birth','Suffix'])\n                    df_sec_parser = pd.concat([df_sec_parser,new_sec_parser])\n\n            except ValueError:\n                print(\"Problem with extracted age \",potential_age)\n</code></pre>\n\n<p>I have a few questions:</p>\n\n<ul>\n<li>Is there a more efficient way to extract this information?</li>\n<li>Should I use a regex instead?</li>\n<li>My text documents are very long and I have lots of them. Can I do one search for all the items at once?</li>\n<li>What would be a strategy to detect other patterns in the dataset?</li>\n</ul>\n\n<p>Some sentences extracted from the dataset:</p>\n\n<ul>\n<li>\"Equity awards granted to Mr. Love in 2010 represented 48% of his total compensation\"</li>\n<li>\"George F. Rubin(14)(15) Age 68 Trustee since: 1997.\"</li>\n<li>\"INDRA K. NOOYI, 56, has been PepsiCos Chief Executive Officer (CEO) since 2006\"</li>\n<li>\"Mr. Lovallo, 47, was appointed Treasurer in 2011.\"</li>\n<li>\"Mr. Charles Baker, 79, is a business advisor to biotechnology companies.\"</li>\n<li>\"Mr. Botein, age 43, has been a member of our Board since our formation.\"</li>\n</ul>\n",
    "score": 9,
    "creation_date": 1565183029,
    "view_count": 4827,
    "answer_count": 6,
    "tags": "python;nlp;pattern-matching;text-mining"
  },
  {
    "question_id": 47998685,
    "title": "pyLDAvis: Validation error on trying to visualize topics",
    "body": "<p>I tried generating topics using gensim for 300000 records. On trying to visualize the topics, I get a validation error. I can print the topics after model training, but it fails on using pyLDAvis</p>\n\n<pre><code># Running and Training LDA model on the document term matrix.\nldamodel1 = Lda(doc_term_matrix1, num_topics=10, id2word = dictionary1, passes=50, workers = 4)\n\n(ldamodel1.print_topics(num_topics=10, num_words = 10))\n #pyLDAvis\nd = gensim.corpora.Dictionary.load('dictionary1.dict')\nc = gensim.corpora.MmCorpus('corpus.mm')\nlda = gensim.models.LdaModel.load('topic.model')\n\n#error on executing this line\ndata = pyLDAvis.gensim.prepare(lda, c, d)\n</code></pre>\n\n<p>I got the below error on trying to after running above pyLDAvis </p>\n\n<pre><code>---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n&lt;ipython-input-53-33fd88b65056&gt; in &lt;module&gt;()\n----&gt; 1 data = pyLDAvis.gensim.prepare(lda, c, d)\n      2 data\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py in prepare(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\n    110     \"\"\"\n    111     opts = fp.merge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n--&gt; 112     return vis_prepare(**opts)\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py in prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\n    372    doc_lengths      = _series_with_name(doc_lengths, 'doc_length')\n    373    vocab            = _series_with_name(vocab, 'vocab')\n--&gt; 374    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\n    375    R = min(R, len(vocab))\n    376 \n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py in _input_validate(*args)\n     63    res = _input_check(*args)\n     64    if res:\n---&gt; 65       raise ValidationError('\\n' + '\\n'.join([' * ' + s for s in res]))\n     66 \n     67 \n\nValidationError: \n * Not all rows (distributions) in topic_term_dists sum to 1.\n</code></pre>\n",
    "score": 9,
    "creation_date": 1514409025,
    "view_count": 8586,
    "answer_count": 4,
    "tags": "python;nlp;lda;topic-modeling"
  },
  {
    "question_id": 37199332,
    "title": "NLP of Legal Texts?",
    "body": "<p>I have a corpus of a few 100-thousand legal documents (mostly from the European Union) – laws, commentary, court documents etc. I am trying to algorithmically make some sense of them.</p>\n\n<p>I have modeled the known relationships (temporal, this-changes-that, etc). But on the single-document level, I wish I had better tools to allow fast comprehension. I am open for ideas, but here's a more specific question:</p>\n\n<p>For example: are there NLP methods to determine the relevant/controversial parts of documents as opposed to boilerplate? The recently leaked TTIP papers are thousands of pages with data tables, but one sentence somewhere in there may destroy an industry.</p>\n\n<p>I played around with google's new <code>Parsey McParface</code>, and other NLP solutions in the past, but while they work impressively well, I am not sure how good they are at isolating meaning.</p>\n",
    "score": 9,
    "creation_date": 1463098173,
    "view_count": 1770,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp;parsey-mcparseface;syntaxnet"
  },
  {
    "question_id": 30201476,
    "title": "getting hypernyms from wordnet through nltk python",
    "body": "<p>I am using this code to find all hypernyms of a word </p>\n\n<pre><code>from nltk import wordnet as wn\nfor synset in wn.wordnet.synsets('green'):\n   for hypernym in synset.hypernyms():\n       print synset, hypernym\n</code></pre>\n\n<p>but this gives error as:</p>\n\n<pre><code>   File\"&lt;stdin&gt;\",line 1 in &lt;module&gt;\n   File \"/usr/local/lib/python 2.7/dist-packages/nltk/corpus/reader/wordnet.py\"line 1219 in synset\n   lema,pos,synset_index_str=name.lower().rsplit('.',2)\n   ValueError: need more than 1 value to unpack\n</code></pre>\n\n<p>Can you suggest something</p>\n",
    "score": 9,
    "creation_date": 1431465019,
    "view_count": 10704,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 15551195,
    "title": "How to get the wordnet sense frequency of a synset in NLTK?",
    "body": "<p>According to the documentation i can load a sense tagged corpus in nltk as such:</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet_ic\n&gt;&gt;&gt; brown_ic = wordnet_ic.ic('ic-brown.dat')\n&gt;&gt;&gt; semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n</code></pre>\n\n<p>I can also get the <code>definition</code>, <code>pos</code>, <code>offset</code>, <code>examples</code> as such:</p>\n\n<pre><code>&gt;&gt;&gt; wn.synset('dog.n.01').examples\n&gt;&gt;&gt; wn.synset('dog.n.01').definition\n</code></pre>\n\n<p><strong>But how can get the frequency of a synset from a corpus?</strong> To break down the question:</p>\n\n<ol>\n<li>first how to count many times did a synset occurs a sense-tagged corpus?</li>\n<li>then the next step is to divide by the the count by the total number of counts for all synsets occurrences given the particular lemma.</li>\n</ol>\n",
    "score": 9,
    "creation_date": 1363878387,
    "view_count": 9050,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet;wsd"
  },
  {
    "question_id": 7853295,
    "title": "What do the abbreviations in POS tagging etc mean?",
    "body": "<p>Say I have the following Penn Tree:</p>\n\n<pre><code>(S (NP-SBJ the steel strike)\n (VP lasted\n     (ADVP-TMP (ADVP much longer)\n               (SBAR than\n                     (S (NP-SBJ he)\n                        (VP anticipated\n                            (SBAR *?*))))))\n .)\n</code></pre>\n\n<p>What do abbrevations like <code>VP</code> and <code>SBAR</code> etc mean? Where can I find these definitions? What are these abbreviations called?</p>\n",
    "score": 9,
    "creation_date": 1319219563,
    "view_count": 4371,
    "answer_count": 3,
    "tags": "language-agnostic;nlp;stanford-nlp"
  },
  {
    "question_id": 6162186,
    "title": "ARFF for natural language processing",
    "body": "<p>I'm trying to take a set of reviews, and convert them into the ARFF format for use with WEKA. Unfortunately either I completely misunderstand how the format works, or I'll have to have an attribute for ALL possible words, then a presence indicator. Does anyone know a better way, or ideally have a sample ARFF file?</p>\n",
    "score": 9,
    "creation_date": 1306592353,
    "view_count": 1858,
    "answer_count": 2,
    "tags": "nlp;machine-learning;weka;arff"
  },
  {
    "question_id": 341455,
    "title": "End user tool for generating a regular expression",
    "body": "<p>We have a SaaS application requirement to allow a user responsible for building a CMS site to define up to 10 custom fields in a form. \nAs part of this field definition we want to add a field validation option which we will store (and apply at runtime) as a reg-ex.</p>\n\n<p>Are there any tools, code samples or similar that offer a wizard style front end for building a reg-ex. We are looking to embed a control or code into our .NET site that will generate the reg-ex from (pseudo) user friendly terms (close to natural language if possible).</p>\n\n<p>e.g.\nField 1 = (5 alphanumerics) followed-by (1 to 3 numerics) followed by \"-\" followed by 1 alpha</p>\n",
    "score": 9,
    "creation_date": 1228412131,
    "view_count": 570,
    "answer_count": 3,
    "tags": ".net;regex;nlp"
  },
  {
    "question_id": 45590278,
    "title": "How to inverse lemmatization process given a lemma and a token?",
    "body": "<p>Generally, in natural language processing, we want to get the lemma of a token. </p>\n\n<p>For example, we can map 'eaten' to 'eat' using wordnet lemmatization.</p>\n\n<p><strong>Is there any tools in python that can inverse lemma to a certain form?</strong></p>\n\n<p>For example, we map 'go' to 'gone' given target form 'eaten'.</p>\n\n<p>PS: Someone mentions we have to store such mappings.\n<a href=\"https://stackoverflow.com/questions/30266502/how-to-un-stem-a-word-in-python\">How to un-stem a word in Python?</a></p>\n",
    "score": 9,
    "creation_date": 1502280501,
    "view_count": 3294,
    "answer_count": 1,
    "tags": "python;nlp;nltk;lemmatization"
  },
  {
    "question_id": 32400289,
    "title": "Best method to confirm an entity",
    "body": "<p>I would like to understand the best approach to the following problem.</p>\n\n<p>I have documents really similar to resume/cv and I have to extract entities (Name, Surname, Birthday, Cities, zipcode etc).</p>\n\n<p>To extract those entities I am combining different finders (Regex, Dictionary etc)</p>\n\n<p>There are no problems with those finders, but, I am looking for a method / algorithm or something like that to confirm the entities.</p>\n\n<p>With \"confirm\" I mean that I have to find specific term (or entities) in proximities (closer to the entities I have found).</p>\n\n<p>Example:</p>\n\n<pre><code>My name is &lt;name&gt;\nName: &lt;name&gt;\nName and Surname: &lt;name&gt;\n</code></pre>\n\n<p>I can confirm the entity <code>&lt;name&gt;</code> because it is closer to specific term that let me understand the \"context\". If i have \"name\" or \"surname\" words near the entity  so i can say that i have found the <code>&lt;name&gt;</code> with a good probability.</p>\n\n<p>So the goal is write those kind of rules to confirm entities. Another example should be:</p>\n\n<blockquote>\n  <p>My address is ......, 00143 Rome</p>\n</blockquote>\n\n<p>Italian zipcodes are 5 digits long (numeric only), it is easy to find a 5 digits number inside my document (i use regex as i wrote above), and i also check it by querying a database to understand if the number exists. The problem here is that i need one more check to confirm (definitely) it.</p>\n\n<p>I must see if that number is near the entity <code>&lt;city&gt;</code>, if yes, ok... I have good probabilities.</p>\n\n<p>I also tried to train a model but i do not really have a \"context\" (sentences).\nTraining the model with:</p>\n\n<pre><code>My name is: &lt;name&gt;John&lt;/name&gt;\nName: &lt;name&gt;John&lt;/name&gt;\nName/Surname: &lt;name&gt;John&lt;/name&gt;\n&lt;name&gt;John&lt;/name&gt; is my name\n</code></pre>\n\n<p>does not sound good to me because:</p>\n\n<ol>\n<li>I have read we need many sentences to train a good model</li>\n<li>Those are not \"sentences\" i do not have a \"context\" (remember where I said the document is similar to resume/cv)</li>\n<li>Maybe those phrases are too short</li>\n</ol>\n\n<p>I do not know how many different ways i could find to say the exact thing, but surely I can not find 15000 ways :)</p>\n\n<p>What method should I use to try to confirm my entities?</p>\n\n<p>Thank you so much!</p>\n",
    "score": 9,
    "creation_date": 1441376165,
    "view_count": 791,
    "answer_count": 2,
    "tags": "java;nlp;text-mining;opennlp;named-entity-recognition"
  },
  {
    "question_id": 8468874,
    "title": "NLP framework for .NET",
    "body": "<p>I have found references to SharpNLP (a port of the Java-based OpenNLP), and Antelope by Proxem.  I'm looking to create a full parse tree of a sentence (part-of-speech tagging), along with name-finding for dates/times and locations.</p>\n\n<p>The SharpNLP library appears to be inactive since 2007, and it appears that the Antelope library was last updated in 2009.  The former is LGPL, the latter appears to require a commercial license but the installer self-describes the license as \"underspecified\".  </p>\n\n<p>Is there a modern NLP library for .NET?  Is there a better platform choice for NLP?  (it's more important to me to have a great NLP implementation than to stick with a platform choice).</p>\n",
    "score": 9,
    "creation_date": 1323652021,
    "view_count": 4865,
    "answer_count": 1,
    "tags": ".net;nlp"
  },
  {
    "question_id": 223032,
    "title": "tf-idf and previously unseen terms",
    "body": "<p><a href=\"http://en.wikipedia.org/wiki/Tf-idf\" rel=\"noreferrer\">TF-IDF (term frequency - inverse document frequency)</a> is a staple of information retrieval.  It's not a proper model though, and it seems to break down when new terms are introduced into the corpus.  How do people handle it when queries or new documents have new terms, especially if they are high frequency.  Under traditional cosine matching, those would have no impact on the total match.  </p>\n",
    "score": 9,
    "creation_date": 1224615215,
    "view_count": 3800,
    "answer_count": 2,
    "tags": "nlp;statistics;tf-idf;oov"
  },
  {
    "question_id": 63514464,
    "title": "Graph to connect sentences",
    "body": "<p>I have a list of sentences of a few topics (two) like the below:</p>\n<pre><code>Sentences\nTrump says that it is useful to win the next presidential election. \nThe Prime Minister suggests the name of the winner of the next presidential election.\nIn yesterday's conference, the Prime Minister said that it is very important to win the next presidential election. \nThe Chinese Minister is in London to discuss about climate change.\nThe president Donald Trump states that he wants to win the presidential election. This will require a strong media engagement.\nThe president Donald Trump states that he wants to win the presidential election. The UK has proposed collaboration. \nThe president Donald Trump states that he wants to win the presidential election. He has the support of his electors. \n</code></pre>\n<p>As you can see there is similarity in sentences.</p>\n<p><a href=\"https://i.sstatic.net/TVJ8f.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/TVJ8f.png\" alt=\"enter image description here\" /></a></p>\n<p>I am trying to relate multiple sentences and visualise the characteristics of them by using a graph (directed). The graph is built from a similarity matrix, by applying row ordering of sentences as shown above.\nI created a new column, Time, to show the order of sentences, so first row (Trump says that....) is at time 1; second row (The Prime Minister suggests...) is at time 2, and so on.\nSomething like this</p>\n<pre><code>Time    Sentences\n1           Trump said that it is useful to win the next presidential election. \n2           The Prime Minister suggests the name of the winner of the next presidential election.\n\n3           In today's conference, the Prime Minister said that it is very important to win the next presidential election. \n\n...\n</code></pre>\n<p>I would like then to find the relationships in order to have a clear overview of the topic.\nMultiple paths for a sentence would show that there are multiple information associated with it.\nTo determine similarity between two sentences, I tried to extract nouns and verbs as follows:</p>\n<pre><code>noun=[]\nverb=[]\nfor  index, row in df.iterrows():\n      nouns.append([word for word,pos in pos_tag(row[0]) if pos == 'NN'])\n      verb.append([word for word,pos in pos_tag(row[0]) if pos == 'VB'])\n</code></pre>\n<p>as they are keywords in whatever sentence.\nSo when a keyword (noun or verb) appears in sentence x but not in the other sentences, it represents a difference between these two sentences.\nI think a better approach, however, could be using word2vec or gensim (WMD).</p>\n<p>This similarity has to be calculated for each sentence.\nI would like to build a graph which shows the content of the sentence in my example above.\nSince there are two topics (Trump and Chinese Minister), for each of them I need to look for sub-topics. Trump has sub-topic presidential election, for example. A node in my graph should represent a sentence. Words in each node represent differences for the sentences, showing new info in the sentence. For example, the word <code>states</code> in sentence at time 5 is in adjacent sentences at time 6 and 7.\nI would like just to find a way to have similar results as shown in picture below. I have tried using mainly nouns and verbs extraction, but probably it is not the right way to proceed.\nWhat I tried to do has been to consider sentence at time 1 and compare it with other sentences, assigning a similarity score (with noun and verbs extraction but also with word2vec), and repeat it for all the other sentences.\nBut my problem is now on how to extract difference to create a graph that can make sense.</p>\n<p>For the part of the graph, I would consider to use networkx (DiGraph):</p>\n<pre><code>G = nx.DiGraph()\nN = Network(directed=True) \n</code></pre>\n<p>to show direction of relationships.</p>\n<p>I provided a different example to make it be clearer (but if you worked with the previous example, it would be fine as well. Apologies for the inconvenience, but since my first question was not so clear, I had to provide also a better, probably easier, example).</p>\n",
    "score": 9,
    "creation_date": 1597966132,
    "view_count": 1460,
    "answer_count": 2,
    "tags": "python;nlp;nltk;networkx;word2vec"
  },
  {
    "question_id": 59344316,
    "title": "detect dates in spacy",
    "body": "<p>Is there a way to write a rule based system to catch things like start/end dates from a contract text. Here are a few real examples. I am bolding the date entities which I want spacy to automatically detect. If you have other ideas different than spacy that is also OK!</p>\n\n<ol>\n<li><p>The initial term of this Lease shall be for a period of Five (5) years commencing on\n<code>February 1, 2012</code>, (the “Lease Commencement Date”) and expiring on <code>January 31, 2017</code>\n(the “Initial Lease Term”).</p></li>\n<li><p>Term: One (1) year commencing <code>January 1, 2007</code> (\"Commencement Date\") and ending\n<code>December 31, 2007</code> (\"Expiration Date\"). </p></li>\n<li><p>This Lease Agreement is entered into for term of 15 years, beginning <code>January 1, 2014</code> and ending on <code>December 31, 2028</code>.</p></li>\n</ol>\n",
    "score": 9,
    "creation_date": 1576416319,
    "view_count": 18599,
    "answer_count": 3,
    "tags": "python;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 45923279,
    "title": "FastText - Cannot load model.bin due to C++ extension failed to allocate the memory",
    "body": "<p>I'm trying to use the FastText Python API <a href=\"https://pypi.python.org/pypi/fasttext\" rel=\"noreferrer\">https://pypi.python.org/pypi/fasttext</a> Although, from what I've read, this API can't load the newer .bin model files at <a href=\"https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\" rel=\"noreferrer\">https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md</a> as suggested in <a href=\"https://github.com/salestock/fastText.py/issues/115\" rel=\"noreferrer\">https://github.com/salestock/fastText.py/issues/115</a></p>\n\n<p>I've tried everything that is suggested at that issue, and furthermore <a href=\"https://github.com/Kyubyong/wordvectors\" rel=\"noreferrer\">https://github.com/Kyubyong/wordvectors</a> doesn't have the .bin for English, otherwise the problem would be solved. Does anyone know of a work-around for this?</p>\n",
    "score": 9,
    "creation_date": 1503936826,
    "view_count": 5688,
    "answer_count": 5,
    "tags": "python;nlp;embedding;fasttext"
  },
  {
    "question_id": 42634503,
    "title": "Importing external treebank-style BLLIP corpus using NLTK",
    "body": "<p>I have downloaded the <a href=\"https://catalog.ldc.upenn.edu/LDC2000T43\" rel=\"nofollow noreferrer\">BLLIP</a> corpus and would like to import it to NLTK. One way that I have found for doing this is described in the answer of the question\n<a href=\"https://stackoverflow.com/questions/30600975/how-to-read-corpus-of-parsed-sentences-using-nltk-in-python\">How to read corpus of parsed sentences using NLTK in python?</a>. In that answer they are doing it for one data file. I want to do it for a collection of them.</p>\n\n<p>The BLLIP corpus comes as a collection of a few million files, each of which containing a couple of parsed sentences or so. The main folder that contains the data is named <code>bllip_87_89_wsj</code> and it contains 3 subfolders, <code>1987</code>, <code>1988</code>, <code>1989</code> (one for each year). In subfolder <code>1987</code> you have sub-subfolders each containing a number of files corresponding to parsed sentences. A sub-subfolder is named something like <code>w7_001</code> (for folder <code>1987</code>) and the file names are <code>w7_001.000</code>, <code>w7_001.001</code> and so on and so forth.</p>\n\n<p>With all this at hand, my task is the following: <strong>Read all files sequentially using NLTK parsers. Then, convert the corpus to a list of lists, where each sublist is a sentence.</strong> </p>\n\n<p>The second part is easy, its done with the command <code>corpus_name.sents()</code>. It is the first part of the task that I don't know how to approach.</p>\n\n<p>All suggestions are welcome. I would also especially welcome suggestions that propose alternative, more efficient, approaches to the one I have in mind.</p>\n\n<p><strong>UPDATE</strong>:</p>\n\n<p>The parsed sentences of the BLLIP corpus are of the following form:</p>\n\n<pre><code>(S (NP (DT the) (JJ little) (NN dog)) (VP (VBD barked)))\n</code></pre>\n\n<p>In a number of sentences there is a syntactic category of the form <code>(-NONE- *-0)</code> so when I read the corpus <code>*-0</code> is considered a word. Is there a way to ignore the syntactic category <code>-NONE-</code>. For example, if I had the sentence</p>\n\n<pre><code>(S (NP-SBJ (-NONE- *-0))\n  (VP (TO to)\n   (VP (VB sell)\n    (NP (NP (PRP$#0 its) (NN TV) (NN station))\n     (NN advertising)\n     (NN representation)\n     (NN operation)\n     (CC and)\n     (NN program)\n     (NN production)\n     (NN unit))\n</code></pre>\n\n<p>I would like it to become:</p>\n\n<p><code>to sell its TV station advertising representation operation and program production unit</code></p>\n\n<p>and NOT</p>\n\n<p><code>*-0 to sell its TV station advertising representation operation and program production unit</code></p>\n\n<p>which it is currently.</p>\n",
    "score": 9,
    "creation_date": 1488830420,
    "view_count": 358,
    "answer_count": 1,
    "tags": "python;parsing;nlp;nltk;corpus"
  },
  {
    "question_id": 35281004,
    "title": "How to make api.ai agent learn something dynamically?",
    "body": "<p>I am currently using api.ai , to create agent to perform specific tasks, but one question i don't have answer to is , can i make it learn something while chatting , mean that i speak <strong>my name is 'John Cena'</strong> and she should store it and then whenever i ask her again bot should answer me that. i know there is a way to do it by logging into <strong>api.ai</strong> web and manually add entries , but it will not help, is there any work around programmatically or automatically ?  the file i've been using to practice is given in <a href=\"https://github.com/sitepoint-editors/Api-AI-Personal-Assistant-Demo/blob/master/index.html\">github</a> . and here is working <a href=\"http://patcat.com/demos/barry/\">DEMO</a></p>\n",
    "score": 9,
    "creation_date": 1454972473,
    "view_count": 2042,
    "answer_count": 3,
    "tags": "android;json;nlp;artificial-intelligence;dialogflow-es"
  },
  {
    "question_id": 2853384,
    "title": "How to identify ideas and concepts in a given text",
    "body": "<p>I'm working on a project at the moment where it would be really useful to be able to detect when a certain topic/idea is mentioned in a body of text. For instance, if the text contained:</p>\n\n<blockquote>\n  <p><em>Maybe if you tell me a little more about who Mr Jones is, that would help. It would also be useful if I could have a description of his appearance, or even better a photograph?</em></p>\n</blockquote>\n\n<p>It'd be great to be able to detect that the person has asked for a photograph of Mr Jones. I could take a really naïve approach and just look for the word \"photo\" or \"photograph\", but this would obviously be no good if they wrote something like:</p>\n\n<blockquote>\n  <p><em>Please, never send me a photo of Mr Jones.</em></p>\n</blockquote>\n\n<p>Does anyone know where to start with this? Is it even possible?</p>\n\n<p>I've looked into things like nltk, but I've yet to find an example of someone doing something similar and am still not entirely sure what this kind of analysis is called. Any help that can get me off the ground would be great.</p>\n\n<p>Thanks!</p>\n",
    "score": 9,
    "creation_date": 1274135891,
    "view_count": 2350,
    "answer_count": 4,
    "tags": "artificial-intelligence;nlp;nltk;text-mining"
  },
  {
    "question_id": 76758299,
    "title": "What is the difference between HuggingFace&#39;s TextGeneration and Text2TextGeneration pipelines",
    "body": "<p>I'm confused about the technical difference between the two huggingface pipelines <a href=\"https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.TextGenerationPipeline\" rel=\"noreferrer\">TextGeneration</a> and <a href=\"https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.Text2TextGenerationPipeline\" rel=\"noreferrer\">Text2TextGeneration</a>.</p>\n<p>In the TextGeneration it is stated that:</p>\n<blockquote>\n<p>Language generation pipeline using any ModelWithLMHead. This pipeline\npredicts the words that will follow a specified text prompt.</p>\n</blockquote>\n<p>But isn't any language model doing that? &quot;predicting the next words&quot;? So how is this pipeline different than Text2TextGeneration? Isn't Text2TextGeneration going to predict the next probable words?</p>\n<p>I also tried some models using &quot;Text2TextGeneration&quot; pipeline, and despite of HuggingFace's warning &quot;The model is not supported for text2text-generation&quot; it actually worked and generated some outputs.</p>\n<p>If someone can explain the technical difference it will be appreciated.</p>\n",
    "score": 9,
    "creation_date": 1690236452,
    "view_count": 4812,
    "answer_count": 2,
    "tags": "nlp;huggingface-transformers;huggingface"
  },
  {
    "question_id": 70048302,
    "title": "difference between Tokenization and Segmentation",
    "body": "<p>What is the difference between Tokenization and Segmentation in NLP. I searched about them but I didn't really find any differences\n.</p>\n",
    "score": 9,
    "creation_date": 1637429528,
    "view_count": 2489,
    "answer_count": 1,
    "tags": "machine-learning;nlp;artificial-intelligence;terminology;text-segmentation"
  },
  {
    "question_id": 32222121,
    "title": "Natural Language Processing - Converting unstructured bibliography to structured metadata",
    "body": "<p>Currently working on a natural language processing project in which I need to convert unstructured bibliography section (which is at the end of research article) to structured metadata like \"Year\", \"Author\", \"Journal\",  \"Volume ID\", \"Page Number\", \"Title\", etc.</p>\n\n<hr>\n\n<p>For example: Input  </p>\n\n<pre class=\"lang-none prettyprint-override\"><code>McCallum, A.; Nigam, K.; and Ungar, L. H. (2000). Efficient clustering of high-dimensional data sets with application to reference matching. In Knowledge Discovery and Data Mining, 169–178\n</code></pre>\n\n<hr>\n\n<p>Expected output: </p>\n\n<pre class=\"lang-none prettyprint-override\"><code>&lt;Author&gt; McCallum, A.&lt;/Author&gt; &lt;Author&gt;Nigam, K.&lt;/Author&gt; &lt;Author&gt;Ungar, L. H.&lt;/Author&gt;\n&lt;Year&gt; 2000 &lt;/Year&gt;\n&lt;Title&gt;Efficient clustering of high-dimensional data sets with application to reference matching &lt;Title&gt; and so on\n</code></pre>\n\n<p>Tool used: <a href=\"http://www.chokkan.org/software/crfsuite/\">CRFsuite</a> </p>\n\n<hr>\n\n<p>Data-set: This contains 12000 references</p>\n\n<ol>\n<li>Contains Journal title,  </li>\n<li>Contains article title's words, </li>\n<li>Contains location names,</li>\n</ol>\n\n<hr>\n\n<p>Each word in given line considered as token and for each token I derive following features</p>\n\n<ol>\n<li>BOR at the start of line, </li>\n<li>EOR for end </li>\n<li>digitFeature : if token is digit </li>\n<li>Year: if token is in year format like 19** and 20** </li>\n<li>available in current data-set,</li>\n</ol>\n\n<hr>\n\n<p>From above tool and data-set I got only 63.7% accuracy. Accuracy is very less for \"Title\" and good for \"Year\" and \"Volume\".</p>\n\n<p>Questions:</p>\n\n<ol>\n<li>Can I draw any additional features?</li>\n<li>Can I use any other tool?</li>\n</ol>\n",
    "score": 9,
    "creation_date": 1440579359,
    "view_count": 269,
    "answer_count": 2,
    "tags": "java;nlp;crf++"
  },
  {
    "question_id": 14518961,
    "title": "How to install the brat annotation tool on a Linux machine with SELinux enabled",
    "body": "<p>This is a self-answered question that describes how to resolve problems that occur when installing the <a href=\"http://brat.nlplab.org/\" rel=\"noreferrer\">brat annotation tool</a>, which is used to create annotated corpora for use in NLP, on an ordinary Linux machine that has SELinux enabled. This is based on version 1.3 of the tool.</p>\n\n<p>The installation procedure as <a href=\"http://brat.nlplab.org/installation.html\" rel=\"noreferrer\">documented</a> includes the steps below:</p>\n\n<ol>\n<li>Unpack the .tar.gz file in an (Apache) web server directory, typically <code>/var/www/html</code> or <code>$HOME/public_html</code></li>\n<li>Possibly rename the unpacked directory from <code>brat-v1.3_Crunchy_Frog</code> to something simple such as <code>brat</code></li>\n<li>Enter the directory and run <code>sudo ./install.sh</code></li>\n<li>Start the web server (<code>sudo service httpd start</code>) if it wasn't running already</li>\n</ol>\n\n<p><strong>Problem:</strong> When following this procedure, any attempt to use brat in a browser (by directing it to <code>http://localhost/brat/index.xhtml</code> fails with the following error messages displayed on screen:</p>\n\n<pre><code>Error: ActiongetCollectionInformation failed on error Internal Server Error\nError: Actionwhoami failed on error Internal Server Error\nError: ActionloadConf failed on error Internal Server Error\n</code></pre>\n\n<p>The Apache error log (typically found in <code>/var/log/httpd/error_log</code>) also shows errors:</p>\n\n<pre><code>(13)Permission denied: exec of '/var/www/html/new/ajax.cgi' failed, referer: http://localhost/new/index.xhtml\nPremature end of script headers: ajax.cgi, referer: http://localhost/new/index.xhtml\n</code></pre>\n\n<p>How to solve this problem?</p>\n",
    "score": 9,
    "creation_date": 1359106435,
    "view_count": 4276,
    "answer_count": 2,
    "tags": "nlp;brat"
  },
  {
    "question_id": 9854025,
    "title": "Which phrase extraction tool is the state of art now?",
    "body": "<p>I know of the following open source tools, but I haven't found any comparisons of how good they are respectively.\nTools with ready to use phrase extraction:</p>\n\n<ul>\n<li>KEA</li>\n<li>MAUI (http://code.google.com/p/maui-indexer/)</li>\n<li>Dragon, xTract (http://dragon.ischool.drexel.edu/xtract.asp)</li>\n<li>Lingpipe (http://alias-i.com/lingpipe/demos/tutorial/interestingPhrases/read-me.html)</li>\n<li>Mahout (https://cwiki.apache.org/MAHOUT/collocations.html)</li>\n<li>Anything else</li>\n</ul>\n\n<p>Did anyone ever see such a comparison?</p>\n",
    "score": 9,
    "creation_date": 1332611331,
    "view_count": 2323,
    "answer_count": 2,
    "tags": "nlp;information-extraction"
  },
  {
    "question_id": 8198923,
    "title": "Natural Language parser for parsing sports play-by-play data",
    "body": "<p>I'm trying to come up with a parser for football plays. I use the term \"natural language\" here very loosely so please bear with me as I know little to nothing about this field.</p>\n\n<p>Here are some examples of what I'm working with \n(Format: TIME|DOWN&amp;DIST|OFF_TEAM|DESCRIPTION):</p>\n\n<pre><code>04:39|4th and 20@NYJ46|Dal|Mat McBriar punts for 32 yards to NYJ14. Jeremy Kerley - no return. FUMBLE, recovered by NYJ.|\n04:31|1st and 10@NYJ16|NYJ|Shonn Greene rush up the middle for 5 yards to the NYJ21. Tackled by Keith Brooking.|\n03:53|2nd and 5@NYJ21|NYJ|Mark Sanchez rush to the right for 3 yards to the NYJ24. Tackled by Anthony Spencer. FUMBLE, recovered by NYJ (Matthew Mulligan).|\n03:20|1st and 10@NYJ33|NYJ|Shonn Greene rush to the left for 4 yards to the NYJ37. Tackled by Jason Hatcher.|\n02:43|2nd and 6@NYJ37|NYJ|Mark Sanchez pass to the left to Shonn Greene for 7 yards to the NYJ44. Tackled by Mike Jenkins.|\n02:02|1st and 10@NYJ44|NYJ|Shonn Greene rush to the right for 1 yard to the NYJ45. Tackled by Anthony Spencer.|\n01:23|2nd and 9@NYJ45|NYJ|Mark Sanchez pass to the left to LaDainian Tomlinson for 5 yards to the 50. Tackled by Sean Lee.|\n</code></pre>\n\n<p>As of now, I've written a dumb parser that handles all the easy stuff (playID, quarter, time, down&amp;distance, offensive team) along with some scripts that goes and gets this data and sanitizes it into the format seen above. A single line gets turned into a \"Play\" object to be stored into a database.</p>\n\n<p>The tough part here (for me at least) is parsing the description of the play.  Here is some information that I would like to extract from that string:</p>\n\n<p>Example string:</p>\n\n<pre><code>\"Mark Sanchez pass to the left to Shonn Greene for 7 yards to the NYJ44. Tackled by Mike Jenkins.\"\n</code></pre>\n\n<p>Result:</p>\n\n<pre><code>turnover = False\ninterception = False\nfumble = False\nto_on_downs = False\npassing = True\nrushing = False\ndirection = 'left'\nloss = False\npenalty = False\nscored = False\nTD = False\nPA = False\nFG = False\nTPC = False\nSFTY = False\npunt = False\nkickoff = False\nret_yardage = 0\nyardage_diff = 7\nplaymakers = ['Mark Sanchez', 'Shonn Greene', 'Mike Jenkins']\n</code></pre>\n\n<p>The logic that I had for my initial parser went something like this:</p>\n\n<pre><code># pass, rush or kick\n# gain or loss of yards\n# scoring play\n    # Who scored? off or def?\n    # TD, PA, FG, TPC, SFTY?\n# first down gained\n# punt?\n# kick?\n    # return yards?\n# penalty?\n    # def or off?\n# turnover?\n    # INT, fumble, to on downs?\n# off play makers\n# def play makers\n</code></pre>\n\n<p>The descriptions can get pretty hairy (multiple fumbles &amp; recoveries with penalties, etc) and I was wondering if I could take advantage of some NLP modules out there. Chances are I'm going to spend a few days on a dumb/static state-machine like parser instead but if anyone has suggestions on how to approach it using NLP techniques I'd like to hear about them.</p>\n",
    "score": 9,
    "creation_date": 1321754757,
    "view_count": 905,
    "answer_count": 2,
    "tags": "python;parsing;nlp"
  },
  {
    "question_id": 51184134,
    "title": "In a chatbot conversation using dialogflow, Is there a way to make the bot speak first?",
    "body": "<p>Is it possible to format a conversation so that the bot initiates conversation using dialogflow in a web demo integration? </p>\n\n<p>The objective is to say something like “Hi, I’m a bot, I can do x” to establish that it’s a chatbot rather than a human.</p>\n\n<p>Can anyone suggest any idea for this?</p>\n",
    "score": 9,
    "creation_date": 1530769685,
    "view_count": 2821,
    "answer_count": 2,
    "tags": "nlp;artificial-intelligence;chatbot;dialogflow-es"
  },
  {
    "question_id": 45196312,
    "title": "spaCy and scikit-learn vectorizer",
    "body": "<p>I wrote a lemma tokenizer using spaCy for scikit-learn based on their <a href=\"http://scikit-learn.org/stable/modules/feature_extraction.html\" rel=\"noreferrer\">example</a>, it works OK standalone:</p>\n\n<pre><code>import spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.spacynlp = spacy.load('en')\n    def __call__(self, doc):\n        nlpdoc = self.spacynlp(doc)\n        nlpdoc = [token.lemma_ for token in nlpdoc if (len(token.lemma_) &gt; 1) or (token.lemma_.isalnum()) ]\n        return nlpdoc\n\nvect = TfidfVectorizer(tokenizer=LemmaTokenizer())\nvect.fit(['Apples and oranges are tasty.'])\nprint(vect.vocabulary_)\n### prints {'apple': 1, 'and': 0, 'tasty': 4, 'be': 2, 'orange': 3}\n</code></pre>\n\n<p>However, using it in <code>GridSearchCV</code> gives errors, a self contained example is below:</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\n\nwordvect = TfidfVectorizer(analyzer='word', strip_accents='ascii', tokenizer=LemmaTokenizer())\nclassifier = OneVsRestClassifier(SVC(kernel='linear'))\npipeline = Pipeline([('vect', wordvect), ('classifier', classifier)])\nparameters = {'vect__min_df': [1, 2], 'vect__max_df': [0.7, 0.8], 'classifier__estimator__C': [0.1, 1, 10]}\ngs_clf = GridSearchCV(pipeline, parameters, n_jobs=7, verbose=1)\n\nfrom sklearn.datasets import fetch_20newsgroups\ncategories = ['comp.graphics', 'rec.sport.baseball']\nnewsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), shuffle=True, categories=categories)\nX = newsgroups.data\ny = newsgroups.target\ngs_clf = gs_clf.fit(X, y)\n\n### AttributeError: 'spacy.tokenizer.Tokenizer' object has no attribute '_prefix_re'\n</code></pre>\n\n<p>The error does not appear when I load spacy outside of constructor of the tokenizer, then the <code>GridSearchCV</code> runs:</p>\n\n<pre><code>spacynlp = spacy.load('en')\n    class LemmaTokenizer(object):\n        def __call__(self, doc):\n            nlpdoc = spacynlp(doc)\n            nlpdoc = [token.lemma_ for token in nlpdoc if (len(token.lemma_) &gt; 1) or (token.lemma_.isalnum()) ]\n            return nlpdoc\n</code></pre>\n\n<p>But this means that every of my <code>n_jobs</code> from the <code>GridSearchCV</code> will access and call the same spacynlp object, it is shared among these jobs, which leaves the questions:</p>\n\n<ol>\n<li>Is the spacynlp object from <code>spacy.load('en')</code> safe to be used by multiple jobs in GridSearchCV?</li>\n<li>Is this the correct way to implement calls to spacy inside a tokenizer for scikit-learn?</li>\n</ol>\n",
    "score": 9,
    "creation_date": 1500482305,
    "view_count": 8897,
    "answer_count": 2,
    "tags": "python;scikit-learn;nlp;spacy"
  },
  {
    "question_id": 5857979,
    "title": "Max edit distance and suggestion based on word frequency",
    "body": "<p>I need a spell checker with the following specification:  </p>\n\n<ul>\n<li>Very scalable.</li>\n<li>To be able to set a maximum edit distance for the suggested words.</li>\n<li>To get suggestion based on provided words frequencies (most common word first).</li>\n</ul>\n\n<hr>\n\n<p>I took a look at Hunspell:<br>\nI found the parameter MAXDIFF in the man but doesn't seem to work as expected. Maybe I'm using it the wrong way</p>\n\n<p>file <strong>t.aff</strong>:  </p>\n\n<pre><code>MAXDIFF 1 \n</code></pre>\n\n<p>file <strong>dico.dic</strong>:  </p>\n\n<pre><code>5  \nrouge  \nvert  \nbleu  \nbleue  \norange  \n</code></pre>\n\n<p>-  </p>\n\n<pre><code>NHunspell.Hunspell h = new NHunspell.Hunspell(\"t.aff\", \"dico.dic\");\nList&lt;string&gt; s = h.Suggest(\"bleuue\");\n</code></pre>\n\n<p>returns the same thing <code>t.aff</code> being empty or not:</p>\n\n<pre><code>bleue\nbleu\n</code></pre>\n",
    "score": 9,
    "creation_date": 1304344293,
    "view_count": 988,
    "answer_count": 2,
    "tags": "c#;c++;nlp;spell-checking;hunspell"
  },
  {
    "question_id": 62395559,
    "title": "Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2",
    "body": "<p>I am trying to create an image captioning model. Could you please help with this error? input1 is the image vector, input2 is the caption sequence. 32 is the caption length. I want to concatenate the image vector with the embedding of the sequence and then feed it to the decoder model.</p>\n\n<pre><code>\n    def define_model(vocab_size, max_length):\n      input1 = Input(shape=(512,))\n      input1 = tf.keras.layers.RepeatVector(32)(input1)\n      print(input1.shape)\n\n      input2 = Input(shape=(max_length,))\n      e1 = Embedding(vocab_size, 512, mask_zero=True)(input2)\n      print(e1.shape)\n\n      dec1 = tf.concat([input1,e1], axis=2)\n      print(dec1.shape)\n\n      dec2 = LSTM(512)(dec1)\n      dec3 = LSTM(256)(dec2)\n      dec4 = Dropout(0.2)(dec3)\n      dec5 = Dense(256, activation=\"relu\")(dec4)\n      output = Dense(vocab_size, activation=\"softmax\")(dec5)\n      model = tf.keras.Model(inputs=[input1, input2], outputs=output)\n      model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n      print(model.summary())\n      return model\n\n</code></pre>\n\n<pre><code>ValueError: Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 512]\n</code></pre>\n",
    "score": 9,
    "creation_date": 1592249320,
    "view_count": 5011,
    "answer_count": 1,
    "tags": "python;tensorflow;keras;nlp;lstm"
  },
  {
    "question_id": 44328663,
    "title": "Perform sentence segmentation on paragraphs without punctuation?",
    "body": "<p>I have a bunch of badly formatted text with lots of missing punctuation. I want to know if there was any method to segment text into sentences when periods, semi-colons, capitalization, etc. are missing. <br><br>\nFor example, consider the paragraph: <strong>\"the lion is called the king of the forest it has a majestic appearance it eats flesh it can run very fast the roar of the lion is very famous\"</strong>.<br><br>\nThis text should be segmented as separate sentences:</p>\n\n<ul>\n<li><strong>the lion is called the king of the forest</strong> </li>\n<li><strong>it has a majestic appearance</strong></li>\n<li><strong>it eats flesh</strong></li>\n<li><strong>it can run very fast</strong></li>\n<li><strong>the roar of the lion is very famous</strong></li>\n</ul>\n\n<p>Can this be done or is it impossible? Any suggestion is much appreciated!</p>\n",
    "score": 9,
    "creation_date": 1496405535,
    "view_count": 2376,
    "answer_count": 1,
    "tags": "algorithm;text;nlp;stanford-nlp;opennlp"
  },
  {
    "question_id": 10416077,
    "title": "Clean text coming from PDFs",
    "body": "<p>this is more of an algorithmic question rather than a specific language question, so I am happy to receive an answer in any language - even pseudocode, even just an idea.</p>\n\n<p>Here is my problem: I need to work on large dataset of papers that come from articles in PDF and that were brutally copied/pasted into .txt. I only have the result of this abomination, which is around 16k papers, for 3.5 GB or text (the corpus I am using is the ACL Antology Network, <a href=\"http://clair.si.umich.edu/clair/aan/DatasetContents.html\">http://clair.si.umich.edu/clair/aan/DatasetContents.html</a> ).</p>\n\n<p>The \"junk\" comes from things like formulae, images, tables, and so on. It just pops in the middle of the running text, so I can't use regular expressions to clean it, and I can't think of any way to use machine learning for it either. I already spent a week on it, and then I decided to move on with a quick&amp;dirty fix. I don't care about cleaning it completely anymore, I don't care about false negatives and positives as long as the majority of this areas of text is removed.</p>\n\n<p>Some examples of the text: note that formulae contain junk characters, but tables and caption don't (but they still make my sentence very long, and thus unparsable). Junk in bold.</p>\n\n<p>Easy one:</p>\n\n<blockquote>\n  <p>The experiments were repeated while inhibiting specialization of first the scheme with the most expansions, and then the two most expanded schemata.\n  Measures of coverage and speedup are important 1 As long as we are interested in preserving the f-structure assigned to sentences, this notion of coverage is stricter than necessary.\n  The same f-structure can in fact be assigned by more than one parse, so that in some cases a sentence is considered out of coverage even if the specialized grammar assigns to it the correct f-structure.\n  <strong>2'VPv' and 'VPverb[main]' cover VPs headed by a main verb.\n  'NPadj' covers NPs with adjectives attached.\n  205 The original rule: l/Pperfp --+ ADVP* SE (t ADJUNCT) ($ ADV_TYPE) = t,padv ~/r { @M_Head_Perfp I@M_Head_Passp } @( Anaph_Ctrl $) { AD VP+ SE ('~ ADJUNCT) ($ ADV_TYPE) = vpadv is replaced by the following: ADVP,[.E (~ ADJUNCT) (.l.\n  ADV_TYPE) = vpadv l/'Pperfp --+ @PPadjunct @PPcase_obl {@M.Head_Pevfp [@M..Head_Passp} @( Anaph_Ctrl ~ ) V { @M_Head_Perfp I@M_Head_Passp } @( Anaph_Ctrl ~) Figure 1: The pruning of a rule from the actual French grammar.</strong>\n  The \"*\" and the \"+\" signs have the usual interpretation as in regular expressions.\n  A sub-expression enclosed in parenthesis is optional.\n  Alternative sub-expressions are enclosed in curly brackets and separated by the \"[\" sign.\n  An \"@\" followed by an identifier is a macro expansion operator, and is eventually replaced by further functional descriptions.\n  <strong>Corpus --..\n  ,, 0.1[ Disambiguated Treebank treebank Human expert Grammar specialization Specialized grammar Figure 2: The setting for our experiments on grammar specialization.\n  indicators of what can be achieved with this form of grammar pruning.</strong>\n  However, they could potentially be misleading, since failure times for uncovered sentences might be considerably lower than their sentences times, had they not been out of coverage.</p>\n</blockquote>\n\n<p>Hard one:</p>\n\n<blockquote>\n  <p>Table 4 summarizes the precision results for both English and Romanian coreference.\n  The results indicate that the English coreference is more indicate than the Romanian coreference, but SNIZZLE improves coreference resolution in both languages.\n  There were 64% cases when the English coreference was resolved by a heuristic with higher priority than the corresponding heuristic for the Romanian counterpart.\n  This result explains why there is better precision enhancement for \n  <strong>English Romanian SWIZZLE on English SWIZZLE on Romanian Nominal Pronominal 73% 89% 66% 78% 76% 93% 71°/o 82% Table 4: Coreference precision Total 84% 72% 87% 76% English Romanian SWIZZLE on English SWIZZLE on Romanian Nominal 69% 63% 66% 61% Pronominal Total 89% 78% 83% 72% 87% 77% 80% 70% Table 5: Coreference recall</strong> the English coreference. Table 5 also illustrates the recall results.\n  The advantage of the data-driven coreference resolution over other methods is based on its better recall performance.\n  This is explained by the fact that this method captures a larger variety of coreference patterns.\n  Even though other coreference resolution systems perform better for some specific forms of systems, their recall results are surpassed by the systems approach.\n  Multilingual coreference in turn improves more the precision than the recall of the monolingual data-driven coreference systems.\n  In addition, Table 5 shows that the English coref- erence results in better recall than Romanian coref- erence.\n  However, the recall shows a decrease for both languages for SNIZZLE because imprecise coreference links are deleted.\n  As is usually the case, deleting data lowers the recall.\n  All results were obtained by using the automatic scorer program developed for the MUC evaluations.</p>\n</blockquote>\n\n<p>Note how the table does not contain strange characters and goes right in the middle of the sentence: \"This result explains why there is better precision enhancement for -TABLE HERE- the English coreference.\" I can't know where the table will be in regard to the running text. It may occur before a sentence, after it or within it like in this case. Also note that the table shit does not end with a full stop (most captions in papers don't...) so I can't rely on punctuation to spot it. I am happy with non-accurate boundaries of course, but I still need to do something with these tables. Some of them contain words rather than numbers, and I don't have enough information in those cases: no junky characters, nothing. It is obvious to only humans :S</p>\n",
    "score": 9,
    "creation_date": 1335969751,
    "view_count": 763,
    "answer_count": 1,
    "tags": "language-agnostic;nlp;stanford-nlp"
  },
  {
    "question_id": 12511701,
    "title": "Classify or keyword match a natural language string or phrase",
    "body": "<p>This is my first post on StackOverflow, so apologies if it's lacking the right information.</p>\n<p>Scenario.</p>\n<p>I'm in the process of moving away from the Google Weather API to BOM (Australia) weather service. I've managed to get the weather data from BOM just fine using streamreaders etc, but what I'm stuck on is the image icon that matches the daily forecast.</p>\n<p>What I did with the old Google Weather API was quite brutal yet did the trick. The Google Weather API only gave off a couple of different type of forecasts that I could jam together into a string that i could in turn use in an imageURL.</p>\n<p>Example of what I did with the Google Weather API...</p>\n<p><strong>imageDay1.ImageUrl = &quot;images/weather/&quot; + lbWeatherDay1Cond.Text.Replace(&quot; &quot;, string.Empty) + &quot;.png&quot;;</strong></p>\n<blockquote>\n<p>&quot;Mostly sunny&quot; = mostlysunny.png</p>\n<p>&quot;Sunny&quot; = sunny.png</p>\n<p>&quot;Chance of Rain&quot; = chanceofrain.png</p>\n<p>&quot;Showers&quot; = showers.png</p>\n<p>&quot;Partly cloudy&quot; = partlycloudy.png</p>\n</blockquote>\n<p>There was on say 15 different possible options for the daily forecast.</p>\n<p>The problems I have now and with BOM (Australia Weather Service) is this...</p>\n<blockquote>\n<p>Possible morning shower</p>\n<p>Shower or two, clearing later</p>\n<p>So many thousands more.... there is no standard.</p>\n</blockquote>\n<p>What I'm hoping is that it is possible is some of the great minds on here to create a string from a keyword within this string? Something like &quot;Showers&quot; for &quot;Showers.png&quot; or something a little more complex to recognise &quot;Chance of Showers&quot; as &quot;Chanceshowers.jpg&quot; while keeping &quot;Shower or two&quot; as &quot;Showers.png&quot;.</p>\n<p>I'm easy to any ideas or solutions (hopefully in c#). As long as it's very lightweight (the process has to be repeated for the 5 day forecast) and can capture almost any scenario...</p>\n<p>At this point of time, I'm carrying on with String.Replace, after String.Replace, after String.Replace option.... It will do for now, but I can't roll it into production like this.</p>\n<p>Cheers all!</p>\n<p>Trent</p>\n",
    "score": 9,
    "creation_date": 1348140620,
    "view_count": 1623,
    "answer_count": 3,
    "tags": "c#;machine-learning;nlp;artificial-intelligence;match"
  },
  {
    "question_id": 59008046,
    "title": "Extracting English imperative mood from verb tags with spaCy",
    "body": "<p>I would like to detect the imperative mood of verbs in English sentences. From <a href=\"https://stackoverflow.com/questions/53755559/how-to-extract-tag-attributes-using-spacy\">this question</a> I'm aware that spaCy has access to extended morphological features, but I don't see them when I use it, using spaCy version 2.1.8, Python 3.7.1:</p>\n\n<pre><code>&gt;&gt;&gt; import spacy\n&gt;&gt;&gt; nlp = spacy.load('en_core_web_sm')\n&gt;&gt;&gt; doc = nlp(\"Show me the money!\")\n&gt;&gt;&gt; token = doc[0]\n&gt;&gt;&gt; print(token, token.tag_, nlp.vocab.morphology.tag_map[token.tag_], token.pos_, token.dep_)\nShow VB {74: 100, 'VerbForm': 'inf'} VERB ROOT\n</code></pre>\n\n<p>Looks like spaCy correctly inferred that \"show\" is a verb, but it appears that it thinks it's an infinitive, and I see no mood information from the <code>tag_map</code>. I realize that this extended information is available in my linked question because Italian was being parsed there and determining the mood may be simpler for Romance-inflected verbs. </p>\n\n<p>In any case, does any extended information about mood exist for verbs in English sentences?</p>\n",
    "score": 9,
    "creation_date": 1574514957,
    "view_count": 1274,
    "answer_count": 1,
    "tags": "python-3.x;nlp;spacy"
  },
  {
    "question_id": 50643196,
    "title": "Gensim FastText compute Training Loss",
    "body": "<p>I am training a <code>fastText</code> model using <a href=\"https://radimrehurek.com/gensim/models/fasttext.html\" rel=\"noreferrer\"><code>gensim.models.fasttext</code></a>. However, I can't seem to find a method to compute the loss of the iteration for logging purposes. If I look at <a href=\"https://radimrehurek.com/gensim/models/word2vec.html\" rel=\"noreferrer\"><code>gensim.models.word2vec</code></a>, it has the <code>get_latest_training_loss</code> method which allows you to print the training loss. Are there any alternatives or it is simply impossible?</p>\n",
    "score": 9,
    "creation_date": 1527855188,
    "view_count": 1209,
    "answer_count": 0,
    "tags": "python;nlp;word2vec;gensim;fasttext"
  },
  {
    "question_id": 12011068,
    "title": "API for Natural Language Processing in Android",
    "body": "<p>I am trying to make an Android application similar to the one on <a href=\"http://cleverbot.com/\" rel=\"noreferrer\">this website</a>. </p>\n\n<p>The thing is that I am pretty new to the field of Natural Language Processing. I do not wish to achieve much, just provide the user some interaction with the application, give him a feeling that he is indeed chatting with someone.</p>\n\n<p>Basically, I would just capture the text entered by the user and send it over to the API and display the result retrieved from the API.\nI came across <a href=\"http://opennlp.apache.org/\" rel=\"noreferrer\">http://opennlp.apache.org/</a> and <a href=\"http://gate.ac.uk/\" rel=\"noreferrer\">http://gate.ac.uk/</a> but have NO idea how to implement them in my Android application.</p>\n\n<p><strong>ANY</strong> ideas/ suggestions/ help would indeed be great.</p>\n\n<p>Thanks!</p>\n",
    "score": 9,
    "creation_date": 1345227271,
    "view_count": 12851,
    "answer_count": 0,
    "tags": "android;nlp"
  },
  {
    "question_id": 9879276,
    "title": "How do I evaluate a text summarization tool?",
    "body": "<p>I have written a system that summarizes a long document containing thousands of words. Are there any norms on how such a system should be evaluated in the context of a user survey?</p>\n\n<p>In short, is there a metric for evaluating the time that my tool has saved a human? Currently, I was thinking of using the (Time taken to read the original document/Time taken to read the summary) as a way of determining the time saved, but are there better metrics?</p>\n\n<p>Currently, I am asking the user subjective questions about the accuracy of the summary.</p>\n",
    "score": 8,
    "creation_date": 1332793564,
    "view_count": 25077,
    "answer_count": 10,
    "tags": "language-agnostic;nlp;information-retrieval;evaluation"
  },
  {
    "question_id": 52677634,
    "title": "PyCharm can&#39;t find Spacy Model &#39;en&#39;",
    "body": "<p>I am trying to load a NLP model 'en' from SpaCy in my PyCharm and I am using Python 2.7 .<br>\nMy code to load the 'en' model is \n<code>nlp = spacy.load('en', disable=['parser', 'ner'])</code><br>\nHowever, I received the following error<br>\n<code>IOError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.</code><br>\nI then realised that I didn't download the model, so I used the terminal provided in PyCharm to download the model, I used <code>python -m spacy download en</code>  </p>\n\n<p>This was the following output:  </p>\n\n<blockquote>\n  <p>Requirement already satisfied: en_core_web_sm==2.0.0 from <a href=\"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0\" rel=\"noreferrer\">https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0</a>.\n  tar.gz#egg=en_core_web_sm==2.0.0 in c:\\python27\\lib\\site-packages<br>\n  You are using pip version 9.0.1, however version 18.0 is available.<br>\n  You should consider upgrading via the 'python -m pip install --upgrade pip' command.<br>\n  You do not have sufficient privilege to perform this operation.</p>\n  \n  <p>Linking successful\n      C:\\Python27\\lib\\site-packages\\en_core_web_sm -->\n      C:\\Python27\\lib\\site-packages\\spacy\\data\\en</p>\n  \n  <p>You can now load the model via spacy.load('en')  </p>\n</blockquote>\n\n<p>So I am quite confused... I presume that I was unable to download the 'en' model as I do not have enough privileges to do so, but how was the linking successful?<br>\nUpon seeing this message, I tried running my Python file again ( since the terminal stated that linking was successful) but the initial error popped out again.  </p>\n\n<p>Has anybody encountered this problem before, or knows how to solve this error? How am I able to 'escalate' my privileges in PyCharm terminal so that I will be able to download the model? </p>\n",
    "score": 8,
    "creation_date": 1538818257,
    "view_count": 16624,
    "answer_count": 7,
    "tags": "python;python-2.7;nlp;spacy"
  },
  {
    "question_id": 7917161,
    "title": "PHP and NLP: Nested parenthesis (parser output) to array?",
    "body": "<p>Would like to turn text with nested parenthesis to a nested array. Here is an example output from an NLP parser:</p>\n\n<pre><code>(TOP (S (NP (PRP I)) (VP (VBP love) (NP (NP (DT a) (JJ big) (NN bed)) (PP (IN of) (NP (NNS roses))))) (. .)))\n</code></pre>\n\n<p>(orig: I love a big bed of roses.)</p>\n\n<p>Would like to turn this into a nested array so it will look sg like this</p>\n\n<pre><code>TOP\n S\n  NP\n   PRP I\n  VP \n   VBP Love\n</code></pre>\n\n<p>etc.</p>\n\n<p>Found this <a href=\"https://stackoverflow.com/questions/2650414/php-curly-braces-into-array\">php curly braces into array</a> but that is not a nested array</p>\n",
    "score": 8,
    "creation_date": 1319724656,
    "view_count": 3460,
    "answer_count": 2,
    "tags": "php;multidimensional-array;nlp;parentheses"
  },
  {
    "question_id": 47144311,
    "title": "Removing punctuation using spaCy; AttributeError",
    "body": "<p>Currently I'm using the following code to lemmatize and calculate TF-IDF values for some text data using spaCy:</p>\n\n<pre><code>lemma = []\n\nfor doc in nlp.pipe(df['col'].astype('unicode').values, batch_size=9844,\n                        n_threads=3):\n    if doc.is_parsed:\n        lemma.append([n.lemma_ for n in doc if not n.lemma_.is_punct | n.lemma_ != \"-PRON-\"])\n    else:\n        lemma.append(None)\n\ndf['lemma_col'] = lemma\n\nvect = sklearn.feature_extraction.text.TfidfVectorizer()\nlemmas = df['lemma_col'].apply(lambda x: ' '.join(x))\nvect = sklearn.feature_extraction.text.TfidfVectorizer()\nfeatures = vect.fit_transform(lemmas)\n\nfeature_names = vect.get_feature_names()\ndense = features.todense()\ndenselist = dense.tolist()\n\ndf = pd.DataFrame(denselist, columns=feature_names)\ndf = pd.DataFrame(denselist, columns=feature_names)\nlemmas = pd.concat([lemmas, df])\ndf= pd.concat([df, lemmas])\n</code></pre>\n\n<p>I need to strip out proper nouns, punctuation, and stop words but am having some trouble doing that within my current code.  I've read some <a href=\"https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\" rel=\"nofollow noreferrer\">documentation</a> and <a href=\"https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/\" rel=\"nofollow noreferrer\">other resources</a>, but am now running into an error:</p>\n\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-21-e924639f7822&gt; in &lt;module&gt;()\n      7     if doc.is_parsed:\n      8         tokens.append([n.text for n in doc])\n----&gt; 9         lemma.append([n.lemma_ for n in doc if not n.lemma_.is_punct or n.lemma_ != \"-PRON-\"])\n     10         pos.append([n.pos_ for n in doc])\n     11     else:\n\n&lt;ipython-input-21-e924639f7822&gt; in &lt;listcomp&gt;(.0)\n      7     if doc.is_parsed:\n      8         tokens.append([n.text for n in doc])\n----&gt; 9         lemma.append([n.lemma_ for n in doc if not n.lemma_.is_punct or n.lemma_ != \"-PRON-\"])\n     10         pos.append([n.pos_ for n in doc])\n     11     else:\n\nAttributeError: 'str' object has no attribute 'is_punct'\n</code></pre>\n\n<p>Is there an easier way to strip this stuff out of the text, without having to drastically change my approach?</p>\n\n<p>Full code available <a href=\"https://github.com/LizMGagne/TIP_code/blob/master/TIP%20Stuff%20(8).ipynb\" rel=\"nofollow noreferrer\">here</a>.</p>\n",
    "score": 8,
    "creation_date": 1509996422,
    "view_count": 37116,
    "answer_count": 3,
    "tags": "python;python-3.x;nlp;spacy"
  },
  {
    "question_id": 19753945,
    "title": "TfidfVectorizer in sklearn how to specifically INCLUDE words",
    "body": "<p>I have some questions about the <code>TfidfVectorizer</code>.</p>\n\n<p>It is unclear to me how the words are selected. We can give a minimum support, but after that, what will decide which features will be selected (e.g. higher support more chance)? If we say <code>max_features = 10000</code>, do we always get the same? If we say <code>max_features = 12000</code>, will we get the same <code>10000</code> features, but an extra added <code>2000</code>? </p>\n\n<p>Also, is there a way to extend the, say, <code>max_features=20000</code> features? I fit it on some text, but I know of some words that should be included for sure, and also some emoticons \":-)\" etc. How to add these to the <code>TfidfVectorizer</code> object, so that it will be possible to use the object, use it to <code>fit</code> and <code>predict</code></p>\n\n<pre><code>to_include = [\":-)\", \":-P\"]\nmethod = TfidfVectorizer(max_features=20000, ngram_range=(1, 3),\n                      # I know stopwords, but how about include words?\n                      stop_words=test.stoplist[:100], \n                      # include words ??\n                      analyzer='word',\n                      min_df=5)\nmethod.fit(traindata)\n</code></pre>\n\n<h2>Sought result:</h2>\n\n<pre><code>X = method.transform(traindata)\nX\n&lt;Nx20002 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n with 1135520 stored elements in Compressed Sparse Row format&gt;], \n where N is sample size\n</code></pre>\n",
    "score": 8,
    "creation_date": 1383488386,
    "view_count": 8352,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;scikit-learn"
  },
  {
    "question_id": 27473957,
    "title": "Understanding DictVectorizer in scikit-learn?",
    "body": "<p>I'm exploring the different feature extraction classes that <code>scikit-learn</code> provides. Reading the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\" rel=\"noreferrer\">documentation</a> I did not understand very well what <code>DictVectorizer</code> can be used for? Other questions come to mind. For example, how can <code>DictVectorizer</code> be used for text classification?, i.e. how does this class help handle labelled textual data? Could anybody provide a short example apart from the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\" rel=\"noreferrer\">example</a> that I already read at the documentation web page?</p>\n",
    "score": 8,
    "creation_date": 1418590336,
    "view_count": 16842,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;scikit-learn"
  },
  {
    "question_id": 46860197,
    "title": "Doc2vec and word2vec with negative sampling",
    "body": "<p>My current doc2vec code is as follows.</p>\n\n<pre><code># Train doc2vec model\nmodel = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4, iter = 20)\n</code></pre>\n\n<p>I also have a word2vec code as below.</p>\n\n<pre><code> # Train word2vec model\nmodel = word2vec.Word2Vec(sentences, size=300, sample = 1e-3, sg=1, iter = 20)\n</code></pre>\n\n<p>I am interested in using both DM and DBOW in <strong>doc2vec</strong> AND both Skip-gram and CBOW in <strong>word2vec</strong>.</p>\n\n<p>In Gensim I found the below mentioned sentence:\n<strong>\"Produce word vectors with deep learning via word2vec’s “skip-gram and CBOW models”, using either hierarchical softmax or negative sampling\"</strong></p>\n\n<p>Thus, I am confused either to use hierarchical softmax or negative sampling. Please let me know what are the <strong>differences</strong> in these two methods.</p>\n\n<p>Also, I am interested in knowing <strong>what are the parameters that need to be changed</strong> to use <strong>hierarchical softmax</strong> AND/OR <strong>negative sampling</strong> with respect to <strong>dm, DBOW, Skip-gram and CBOW</strong>?</p>\n\n<p>P.s. my application is a recommendation system :)</p>\n",
    "score": 8,
    "creation_date": 1508561902,
    "view_count": 5132,
    "answer_count": 1,
    "tags": "python;nlp;word2vec;gensim;doc2vec"
  },
  {
    "question_id": 11424157,
    "title": "What approch for simple text processing in Haskell?",
    "body": "<p>I am trying to do some simple text processing in Haskell, and I am wondering what might me the best way to go about this in an FP language. I looked at the parsec module, but this seems much more sophisticated than I am looking for as a new Haskeller. What would be the best way to strip all the punctuation from a corpus of text? My naive approach was to make a function like this:</p>\n\n<pre><code>removePunc str = [c | c &lt;- str, c /= '.',\n                                 c /= '?',\n                                 c /= '.',\n                                 c /= '!',\n                                 c /= '-',\n                                 c /= ';',\n                                 c /= '\\'',\n                                 c /= '\\\"',]\n</code></pre>\n",
    "score": 8,
    "creation_date": 1341968783,
    "view_count": 1341,
    "answer_count": 3,
    "tags": "haskell;nlp"
  },
  {
    "question_id": 571932,
    "title": "Building or Finding a &quot;relevant terms&quot; suggestion feature",
    "body": "<p>Given a few words of input, I want to have a utility that will return a diverse set of relevant terms, phrases, or concepts.  A caveat is that it would need to have a large graph of terms to begin with, or else the feature would not be very useful.</p>\n\n<p>For example, submitting \"baseball\" would return </p>\n\n<pre><code>[\"shortstop\", \"Babe Ruth\", \"foul ball\", \"steroids\", ... ]\n</code></pre>\n\n<p><a href=\"http://labs.google.com/sets\" rel=\"noreferrer\">Google Sets</a> is the best example I can find of this kind of feature, but I can't use it since they have no public API (and I wont go against their TOS).  Also, single-word input doesn't garner a very diverse set of results.  I'm looking for a solution that goes off on tangents.</p>\n\n<p>The closest I've experimented with is using <a href=\"http://en.wikipedia.org/w/api.php\" rel=\"noreferrer\">WikiPedia's API</a> to search Categories and Backlinks, but there's no way to directly sort those results by <em>\"relevance\"</em> or <em>\"popularity\"</em>. Without that, the suggestion list is massive and all over the place, which is not immediately useful and very hard to whittle down.</p>\n\n<p>Using A Thesaurus could also work minimally, but that would leave out any proper nouns or tangentially relevant terms (like any of the results listed above).</p>\n\n<hr>\n\n<p>I would <em>happily</em> reuse an open service, if one exists, but I haven't found anything sufficient.  </p>\n\n<p>I'm looking for either a way to implement this either <em>in-house with a <strong>decently-populated</strong> starting set</em>, or <em>reuse a <strong>free service</em></strong> that offers this.</p>\n\n<p><strong><em>Have a solution?</strong>  Thanks ahead of time!</em></p>\n\n<hr>\n\n<p><strong>UPDATE:</strong>  Thank you for the incredibly dense &amp; informative answers.  I'll choose a winning answer in 6 to 12 months, when I'll hopefully understand what you've all suggested =)</p>\n",
    "score": 8,
    "creation_date": 1235181395,
    "view_count": 1186,
    "answer_count": 3,
    "tags": "algorithm;web-services;nlp;graph-theory;semantics"
  },
  {
    "question_id": 70464428,
    "title": "How to calculate perplexity of a sentence using huggingface masked language models?",
    "body": "<p>I have several masked language models (mainly Bert, Roberta, Albert, Electra). I also have a dataset of sentences. How can I get the perplexity of each sentence?</p>\n<p>From the huggingface documentation <a href=\"https://huggingface.co/docs/transformers/perplexity\" rel=\"noreferrer\">here</a> they mentioned that perplexity &quot;is not well defined for masked language models like BERT&quot;, though I still see people somehow calculate it.</p>\n<p>For example in this <a href=\"https://stackoverflow.com/questions/63030692/how-do-i-use-bertformaskedlm-or-bertmodel-to-calculate-perplexity-of-a-sentence\">SO</a> question they calculated it using the function</p>\n<pre><code>def score(model, tokenizer, sentence,  mask_token_id=103):\n  tensor_input = tokenizer.encode(sentence, return_tensors='pt')\n  repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)\n  mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]\n  masked_input = repeat_input.masked_fill(mask == 1, 103)\n  labels = repeat_input.masked_fill( masked_input != 103, -100)\n  loss,_ = model(masked_input, masked_lm_labels=labels)\n  result = np.exp(loss.item())\n  return result\n\nscore(model, tokenizer, '我爱你') # returns 45.63794545581973\n</code></pre>\n<p>However, when I try to use the code I get <code>TypeError: forward() got an unexpected keyword argument 'masked_lm_labels'</code>.</p>\n<p>I tried it with a couple of my models:</p>\n<pre><code>from transformers import pipeline, BertForMaskedLM, BertForMaskedLM, AutoTokenizer, RobertaForMaskedLM, AlbertForMaskedLM, ElectraForMaskedLM\nimport torch\n\n1)\ntokenizer = AutoTokenizer.from_pretrained(&quot;bioformers/bioformer-cased-v1.0&quot;)\nmodel = BertForMaskedLM.from_pretrained(&quot;bioformers/bioformer-cased-v1.0&quot;)\n2)\ntokenizer = AutoTokenizer.from_pretrained(&quot;sultan/BioM-ELECTRA-Large-Generator&quot;)\nmodel = ElectraForMaskedLM.from_pretrained(&quot;sultan/BioM-ELECTRA-Large-Generator&quot;)\n</code></pre>\n<p><a href=\"https://stackoverflow.com/questions/61470768/how-does-masked-lm-labels-argument-work-in-bertformaskedlm\">This</a> SO question also used the <code>masked_lm_labels</code> as an input and it seemed to work somehow.</p>\n",
    "score": 8,
    "creation_date": 1640274606,
    "view_count": 11044,
    "answer_count": 1,
    "tags": "nlp;pytorch;huggingface-transformers;bert-language-model;transformer-model"
  },
  {
    "question_id": 57857240,
    "title": "Ho to do lemmatization on German text?",
    "body": "<p>I have a German text that I want to apply lemmatization to. If lemmatization is not possible, then I can live with stemming too.</p>\n<p><strong>Data:</strong> This is my German text:</p>\n<pre><code>mails=['Hallo. Ich spielte am frühen Morgen und ging dann zu einem Freund. Auf Wiedersehen', 'Guten Tag Ich mochte Bälle und will etwas kaufen. Tschüss']\n</code></pre>\n<p><strong>Goal:</strong> After applying lemmatization it should look similar to this:</p>\n<pre><code>mails_lemma=['Hallo. Ich spielen am früh Morgen und gehen dann zu einer Freund. Auf Wiedersehen', 'Guten Tag Ich mögen Ball und wollen etwas kaufen Tschüss']\n</code></pre>\n<p>I tried using spacy</p>\n<blockquote>\n<p>conda install -c conda-forge spacy</p>\n<p>python -m spacy download de_core_news_md</p>\n</blockquote>\n<pre><code>import spacy\nfrom spacy.lemmatizer import Lemmatizer\nlemmatizer = Lemmatizer()\n[lemmatizer.lookup(word) for word in mails]\n</code></pre>\n<p>I see following problems.</p>\n<ol>\n<li><p>My data is structured in sentences and not single words</p>\n</li>\n<li><p>In my case spacy lemmatization doesn't seem to work even for single words.</p>\n</li>\n</ol>\n<p>Can you please tell me how this works?</p>\n",
    "score": 8,
    "creation_date": 1568043833,
    "view_count": 15722,
    "answer_count": 2,
    "tags": "nlp;spacy;lemmatization"
  },
  {
    "question_id": 24517722,
    "title": "How to stop NLTK stemmer from removing the trailing &quot;e&quot;?",
    "body": "<p>I'm using NLTK stemmer to remove grammatical variations of a stem word.\nHowever, the Port or Snowball stemmers remove the trailing \"e\" of the original form of a noun or verb, e.g., Profile becomes Profil.</p>\n\n<p>How can I prevent this from happening? I know I can use a conditional to guard against this. But obviously it will fail on different cases.</p>\n\n<p>Is there an option or another API for what I want?</p>\n",
    "score": 8,
    "creation_date": 1404242312,
    "view_count": 5105,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 1693257,
    "title": "Is there a fairly simple way for a script to tell (from context) whether &quot;her&quot; is a possessive pronoun?",
    "body": "<p>I am writing a script to reverse all genders in a piece of text, so all gendered words are swapped - \"man\" is swapped with \"woman\", \"she\" is swapped with \"he\", etc. But there is an ambiguity as to whether \"her\" should be replaced with \"him\" or \"his\".</p>\n",
    "score": 8,
    "creation_date": 1257604902,
    "view_count": 1072,
    "answer_count": 9,
    "tags": "regex;nlp;linguistics"
  },
  {
    "question_id": 47406555,
    "title": "Error faced while using TM package&#39;s VCorpus in R",
    "body": "<p>I am facing the below error while working on the TM package with R. </p>\n\n<pre><code>library(\"tm\")\nLoading required package: NLP\nWarning messages:\n1: package ‘tm’ was built under R version 3.4.2 \n2: package ‘NLP’ was built under R version 3.4.1 \n</code></pre>\n\n<p><code>corpus &lt;- VCorpus(DataframeSource(data))</code></p>\n\n<blockquote>\n  <p>Error: all(!is.na(match(c(\"doc_id\", \"text\"), names(x)))) is not TRUE</p>\n</blockquote>\n\n<p>Have tried various ways like reinstalling the package, updating with new version of R but the error still persists. For the same data file the same code runs on another system with the same version of R.</p>\n",
    "score": 8,
    "creation_date": 1511245648,
    "view_count": 10698,
    "answer_count": 2,
    "tags": "r;text-mining;tm;text-analysis"
  },
  {
    "question_id": 39552088,
    "title": "Select between skip-gram and CBOW model for training word2Vec in gensim",
    "body": "<p>Is it possible to choose between the <code>Skip-gram</code> and the <code>CBOW</code> model in <em>Gensim</em> when training a <em>Word2Vec</em> model?</p>\n",
    "score": 8,
    "creation_date": 1474149266,
    "view_count": 7647,
    "answer_count": 1,
    "tags": "nlp;gensim;word2vec"
  },
  {
    "question_id": 47519987,
    "title": "How to use pos_tag in NLTK?",
    "body": "<p>So I was trying to tag a bunch of words in a list (POS tagging to be exact) like so:</p>\n\n<pre><code>pos = [nltk.pos_tag(i,tagset='universal') for i in lw]\n</code></pre>\n\n<p>where <code>lw</code> is a list of words (it's really long or I would have posted it but it's like <code>[['hello'],['world']]</code> (aka a list of lists which each list containing one word) but when I try and run it I get:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"&lt;pyshell#183&gt;\", line 1, in &lt;module&gt;\n    pos = [nltk.pos_tag(i,tagset='universal') for i in lw]\n  File \"&lt;pyshell#183&gt;\", line 1, in &lt;listcomp&gt;\n    pos = [nltk.pos_tag(i,tagset='universal') for i in lw]\n  File \"C:\\Users\\my system\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\nltk\\tag\\__init__.py\", line 134, in pos_tag\n    return _pos_tag(tokens, tagset, tagger)\n  File \"C:\\Users\\my system\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\nltk\\tag\\__init__.py\", line 102, in _pos_tag\n    tagged_tokens = tagger.tag(tokens)\n  File \"C:\\Users\\my system\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\nltk\\tag\\perceptron.py\", line 152, in tag\n    context = self.START + [self.normalize(w) for w in tokens] + self.END\n  File \"C:\\Users\\my system\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\nltk\\tag\\perceptron.py\", line 152, in &lt;listcomp&gt;\n    context = self.START + [self.normalize(w) for w in tokens] + self.END\n  File \"C:\\Users\\my system\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\nltk\\tag\\perceptron.py\", line 240, in normalize\n    elif word[0].isdigit():\nIndexError: string index out of range\n</code></pre>\n\n<p>Can someone tell me why and how I get this error and how to fix it? Many thanks.</p>\n",
    "score": 8,
    "creation_date": 1511817662,
    "view_count": 36219,
    "answer_count": 3,
    "tags": "python;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 35242155,
    "title": "Kneser-Ney smoothing of trigrams using Python NLTK",
    "body": "<p>I'm trying to smooth a set of n-gram probabilities with Kneser-Ney smoothing using the Python NLTK.\nUnfortunately, the whole documentation is rather sparse.</p>\n\n<p>What I'm trying to do is this: I parse a text into a list of tri-gram tuples. From this list I create a FreqDist and then use that FreqDist to calculate a KN-smoothed distribution.</p>\n\n<p>I'm pretty sure though, that the result is totally wrong. When I sum up the individual probabilities I get something way beyond 1. Take this code example:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import nltk\n\nngrams = nltk.trigrams(\"What a piece of work is man! how noble in reason! how infinite in faculty! in \\\nform and moving how express and admirable! in action how like an angel! in apprehension how like a god! \\\nthe beauty of the world, the paragon of animals!\")\n\nfreq_dist = nltk.FreqDist(ngrams)\nkneser_ney = nltk.KneserNeyProbDist(freq_dist)\nprob_sum = 0\nfor i in kneser_ney.samples():\n    prob_sum += kneser_ney.prob(i)\nprint(prob_sum)\n</code></pre>\n\n<p>The output is \"41.51696428571428\". Depending on the corpus size, this value grows infinitely large. That makes whatever prob() returns anything but a probability distribution.</p>\n\n<p>Looking at the NLTK code I would say that the implementation is questionable. Maybe I just don't understand how the code is supposed to be used. In that case, could you give me a hint please? In any other case: do you know any working Python implementation? I don't really want to implement it myself.</p>\n",
    "score": 8,
    "creation_date": 1454768968,
    "view_count": 12336,
    "answer_count": 3,
    "tags": "python;nlp;nltk;smoothing"
  },
  {
    "question_id": 825846,
    "title": "Appropriate article (a/an) in String.Format",
    "body": "<p>I'm looking for a culturally-sensitive way to properly insert a noun into a sentence while using the appropriate article (a/an). It could use String.Format, or possibly something else if the appropriate way to do this exists elsewhere.</p>\n\n<p>For example:</p>\n\n<p>Base Sentence: \"You are looking at a/an {0}\"</p>\n\n<p>This should format to: \"You are looking at a carrot\" or \"You are looking at an egg.\"</p>\n\n<p>I'm currently doing this by manually checking the first character of the word to be inserted and then manually inserting \"a\" or \"an.\" But I'm concerned that this might limit me when the application is localized to other languages.</p>\n\n<p>Is there a best practice for approaching this problem?</p>\n\n<p>RESOLUTION: It appears that the problem is complicated to the point that there does not exist a utility or framework to solve this problem in the way I originally phrased. It appears that the best solution (in my situation) is to store the article in the database along with the noun so that the translators can have the level of control they need. Thanks for all of the suggestions!</p>\n",
    "score": 8,
    "creation_date": 1241542041,
    "view_count": 661,
    "answer_count": 6,
    "tags": "language-agnostic;localization;grammar;nlp"
  },
  {
    "question_id": 59209086,
    "title": "calculate perplexity in pytorch",
    "body": "<p>I've just trained an LSTM language model using pytorch. The main body of the class is this:  </p>\n\n<pre><code>class LM(nn.Module):\n    def __init__(self, n_vocab, \n                       seq_size, \n                       embedding_size, \n                       lstm_size, \n                       pretrained_embed):\n\n        super(LM, self).__init__()\n        self.seq_size = seq_size\n        self.lstm_size = lstm_size\n        self.embedding = nn.Embedding.from_pretrained(pretrained_embed, freeze = True)\n        self.lstm = nn.LSTM(embedding_size,\n                            lstm_size,\n                            batch_first=True)\n        self.fc = nn.Linear(lstm_size, n_vocab)\n\n    def forward(self, x, prev_state):\n        embed = self.embedding(x)\n        output, state = self.lstm(embed, prev_state)\n        logits = self.fc(output)\n\n        return logits, state\n</code></pre>\n\n<p>Now I want to write a function which calculates how good a sentence is, based on the trained language model <em>(some score like perplexity, etc.)</em>. </p>\n\n<p>I'm a bit confused and I don't know how should I calculate this. <br>A similar sample would be of greate use.</p>\n",
    "score": 8,
    "creation_date": 1575619093,
    "view_count": 19941,
    "answer_count": 1,
    "tags": "python;nlp;pytorch;language-model"
  },
  {
    "question_id": 33676526,
    "title": "POS-Tagger is incredibly slow",
    "body": "<p>I am using <code>nltk</code> to generate n-grams from sentences by first removing given stop words. However, <code>nltk.pos_tag()</code> is extremely slow taking up to 0.6 sec on my CPU (Intel i7).</p>\n\n<p>The output:</p>\n\n<pre><code>['The first time I went, and was completely taken by the live jazz band and atmosphere, I ordered the Lobster Cobb Salad.']\n0.620481014252\n[\"It's simply the best meal in NYC.\"]\n0.640982151031\n['You cannot go wrong at the Red Eye Grill.']\n0.644664049149\n</code></pre>\n\n<p>The code:</p>\n\n<pre><code>for sentence in source:\n\n    nltk_ngrams = None\n\n    if stop_words is not None:   \n        start = time.time()\n        sentence_pos = nltk.pos_tag(word_tokenize(sentence))\n        print time.time() - start\n\n        filtered_words = [word for (word, pos) in sentence_pos if pos not in stop_words]\n    else:\n        filtered_words = ngrams(sentence.split(), n)\n</code></pre>\n\n<p>Is this really that slow or am I doing something wrong here?</p>\n",
    "score": 8,
    "creation_date": 1447345939,
    "view_count": 4390,
    "answer_count": 4,
    "tags": "python;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 33183618,
    "title": "NLTK data out of date - Python 3.4",
    "body": "<p>I'm trying to install NLTK for Python 3.4. The actual NLTK module appears to have installed fine. I then ran</p>\n\n<pre><code>import nltk\n\nnltk.download()\n</code></pre>\n\n<p>and chose to download everything. However, after it was done, the window simply says 'out of date'. I tried refreshing and downloading, yet it stays 'out of date' as shown here:<a href=\"https://i.sstatic.net/Bo1fo.png\" rel=\"noreferrer\">NLTK Window 1</a></p>\n\n<p>I looked online and tried various fixes, but I haven't found any that helped my case yet.</p>\n\n<p>I also tried to manually find the missing parts, which turned out to be 'Open Multilingual Wordnet' and 'Wordnet'. Here's how I found which parts were missing: <a href=\"https://i.sstatic.net/IO7oC.png\" rel=\"noreferrer\">Open Multilingual Wordnet</a>.</p>\n\n<p>What should I do? Should I uninstall and reinstall NLTK? I haven't really found a way to delete the packages (except for manually deleting it).</p>\n\n<p>EDIT: Regarding Solution 2 and Solution 3:\nFor more clarification on the Solution 2 issue:</p>\n\n<p>If something has sucessfully downloaded, this is the output:</p>\n\n<pre><code>&gt;&gt;&gt; nltk.download('subjectivity')\n[nltk_data] Downloading package subjectivity to\n[nltk_data]     C:\\Users\\Shane\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package subjectivity is already up-to-date!\nTrue\n</code></pre>\n\n<p>However, for 'wordnet' and 'omw', this is what happens when I redownload:</p>\n\n<pre><code>&gt;&gt;&gt; nltk.download('omw')\n[nltk_data] Downloading package omw to\n[nltk_data]     C:\\Users\\Shane\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping corpora\\omw.zip.\nTrue\n</code></pre>\n",
    "score": 8,
    "creation_date": 1445063674,
    "view_count": 11762,
    "answer_count": 2,
    "tags": "python;download;nlp;nltk;wordnet"
  },
  {
    "question_id": 24204735,
    "title": "Want Regex to stop at first occurrence of &quot;.&quot; and &quot;;&quot;",
    "body": "<p>I am trying to extract sentence to from a paragraph, with pattern like</p>\n\n<pre><code> Current. time is six thirty at Scotland. Past. time was five thirty at India; Current. time is five thirty at Scotland. Past. time was five thirty at Scotland. Current. time is five ten at Scotland.\n</code></pre>\n\n<p>When I Use Regex as</p>\n\n<pre><code>/current\\..*scotland\\./i\n</code></pre>\n\n<p>This matches to all string </p>\n\n<pre><code>Current. time is six thirty at Scotland. Past. time was six thirty at India; Current. time is five thirty at Scotland. Past. time was five thirty at Scotland. Current. time is five ten at Scotland.\n</code></pre>\n\n<p>Instead I want to stop at first occurrence of \".\" to all capture groups like</p>\n\n<pre><code> Current. time is six thirty at Scotland.\n Current. time is five ten at Scotland. \n</code></pre>\n\n<p>Similarly for text like</p>\n\n<pre><code> Past. time was five thirty at India; Current. time is six thirty at Scotland. Past. time was five thirty at Scotland. Past. time was five ten at India;    \n</code></pre>\n\n<p>When I Use Regex Like</p>\n\n<pre><code> /past\\..*india\\;/i\n</code></pre>\n\n<p>This matches will whole string </p>\n\n<pre><code> Past. time was five thirty at India; Current. time is six thirty at Scotland. Past. time was five thirty at Scotland. Past. time was five ten at India; \n</code></pre>\n\n<p>Here I want to capture all groups or first group like following, and How to stop at first occurrence of \";\"</p>\n\n<pre><code>Past. time was five thirty at India; \nPast. time was five ten at India; \n</code></pre>\n\n<p>How I can make regular expression to stop at \",\" or \";\" with above examples?</p>\n",
    "score": 8,
    "creation_date": 1402660565,
    "view_count": 21238,
    "answer_count": 3,
    "tags": "ruby;regex;ruby-on-rails-3;nlp"
  },
  {
    "question_id": 5404243,
    "title": "extracting English verbs from a given text",
    "body": "<p>I need to extract all English verbs from a given text and I was wondering how I could do it...\nAt first glance, my idea is to use regular expressions because all English verb tenses follow patterns but maybe there is another way to do it. What I've thought is simply:</p>\n\n<ol>\n<li>Create a pattern for every verb tense. I have to distinguish between regular verbs (http://en.wikipedia.org/wiki/English_verbs) and irregular verbs (http://www.chompchomp.com/rules/irregularrules01.htm) in some way.</li>\n<li>Iterate over these patterns and split the text using them (the last word of each substring is supposed to be the verb that gives complete meaning to the sentence, which I need for other purposes -> nominalization)</li>\n</ol>\n\n<p>What do you think? I guess this isn't an efficient way to do it but I can't imagine another one.</p>\n\n<p>Thank you in advance!</p>\n\n<p>PS: </p>\n\n<ol>\n<li>I have two dictionaries, one for all English Verbs and the other one for all English nouns</li>\n<li>The main problem of all this is that the project consists on verb nominalization (is just a uni project), so all the \"effort\" is supposed to be focused in this part, nominalization. In concrete, I follow this model: acl.ldc.upenn.edu/P/P00/P00-1037.pdf). The project consists on given a text, find all the verbs in that text and propose multiple nominalizations for each verb. So the first step (finding verbs), should be as simple as possible... but I can't use any parser, it's not allowed</li>\n</ol>\n",
    "score": 8,
    "creation_date": 1300878259,
    "view_count": 15520,
    "answer_count": 4,
    "tags": "java;regex;nlp"
  },
  {
    "question_id": 4605062,
    "title": "Detecting whether or not text is English (in bulk)",
    "body": "<p>I'm looking for a simple way to detect whether a short excerpt of text, a few sentences, is English or not. Seems to me that this problem is much easier than trying to detect an arbitrary language. Is there any software out there that can do this? I'm writing in python, and would prefer a python library, but something else would be fine too. I've tried google, but then realized the TOS didn't allow automated queries.</p>\n",
    "score": 8,
    "creation_date": 1294237218,
    "view_count": 7978,
    "answer_count": 5,
    "tags": "python;nlp;language-detection"
  },
  {
    "question_id": 68444252,
    "title": "Multiple training with huggingface transformers will give exactly the same result except for the first time",
    "body": "<p>I have a function that will load a pre-trained model from huggingface and fine-tune it for sentiment analysis then calculates the F1 score and returns the result.\nThe problem is when I call this function multiple times with the exact same arguments, it will give the exact same metric score which is expected, except for the first time which is different, how is that possible?</p>\n<p>This is my function which is written based on <a href=\"https://huggingface.co/course/chapter3/3?fw=pt\" rel=\"noreferrer\">this tutorial</a> in huggingface:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import uuid\n\nimport numpy as np\n\nfrom datasets import (\n    load_dataset,\n    load_metric,\n    DatasetDict,\n    concatenate_datasets\n)\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n)\n\nCHECKPOINT = &quot;distilbert-base-uncased&quot;\nSAVING_FOLDER = &quot;sst2&quot;\ndef custom_train(datasets, checkpoint=CHECKPOINT, saving_folder=SAVING_FOLDER):\n\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n    \n    def tokenize_function(example):\n        return tokenizer(example[&quot;sentence&quot;], truncation=True)\n\n    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    saving_folder = f&quot;{SAVING_FOLDER}_{str(uuid.uuid1())}&quot;\n    training_args = TrainingArguments(saving_folder)\n\n    trainer = Trainer(\n        model,\n        training_args,\n        train_dataset=tokenized_datasets[&quot;train&quot;],\n        eval_dataset=tokenized_datasets[&quot;validation&quot;],\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )\n    \n    trainer.train()\n    \n    predictions = trainer.predict(tokenized_datasets[&quot;test&quot;])\n    print(predictions.predictions.shape, predictions.label_ids.shape)\n    preds = np.argmax(predictions.predictions, axis=-1)\n    \n    metric_fun = load_metric(&quot;f1&quot;)\n    metric_result = metric_fun.compute(predictions=preds, references=predictions.label_ids)\n    \n    return metric_result\n</code></pre>\n<p>And then I will run this function several times with the same datasets, and append the result of the returned F1 score each time:</p>\n<pre class=\"lang-py prettyprint-override\"><code>raw_datasets = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)\n\nsmall_datasets = DatasetDict({\n    &quot;train&quot;: raw_datasets[&quot;train&quot;].select(range(100)).flatten_indices(),\n    &quot;validation&quot;: raw_datasets[&quot;validation&quot;].select(range(100)).flatten_indices(),\n    &quot;test&quot;: raw_datasets[&quot;validation&quot;].select(range(100, 200)).flatten_indices(),\n})\n\nresults = []\nfor i in range(4):\n    result = custom_train(small_datasets)\n    results.append(result)\n</code></pre>\n<p>And then when I check the results list:</p>\n<pre><code>[{'f1': 0.7755102040816325}, {'f1': 0.5797101449275361}, {'f1': 0.5797101449275361}, {'f1': 0.5797101449275361}]\n</code></pre>\n<p>Something that may come to mind is that when I load a pre-trained model, the head will be initialized with random weights and that is why the results are different, if that is the case, why only the first one is different and the others are exactly the same?</p>\n",
    "score": 8,
    "creation_date": 1626713380,
    "view_count": 6433,
    "answer_count": 1,
    "tags": "python;machine-learning;deep-learning;nlp;huggingface-transformers"
  },
  {
    "question_id": 46724680,
    "title": "Why are word embedding actually vectors?",
    "body": "<p>I am sorry for my naivety, but I don't understand why word embeddings that are the result of NN training process (word2vec) are actually vectors.</p>\n\n<p>Embedding is the process of dimension reduction, during the training process NN reduces the 1/0 arrays of words into smaller size arrays, the process does nothing that applies vector arithmetic.</p>\n\n<p>So as result we got just arrays and not the vectors. Why should I think of these arrays as vectors?</p>\n\n<p>Even though, we got vectors, why does everyone depict them as vectors coming from the origin (0,0)?</p>\n\n<p>Again, I am sorry if my question looks stupid.</p>\n",
    "score": 8,
    "creation_date": 1507879675,
    "view_count": 4614,
    "answer_count": 4,
    "tags": "machine-learning;neural-network;nlp;word2vec;embedding"
  },
  {
    "question_id": 11370247,
    "title": "Methods for automated synonym detection",
    "body": "<p>I am currently working on a neural network based approach to short document classification, and since the corpuses I am working with are usually around ten words, the standard statistical document classification methods are of limited use. Due to this fact I am attempting to implement some form of automated synonym detection for the matches provided in the training. My question more specifically is about resolving a situation as follows:</p>\n\n<p>Say I have classifications of \"Involving Food\", and one of \"Involving Spheres\" and a data set as follows:</p>\n\n<pre><code>\"Eating Apples\"(Food);\"Eating Marbles\"(Spheres); \"Eating Oranges\"(Food, Spheres);\n\"Throwing Baseballs(Spheres)\";\"Throwing Apples(Food)\";\"Throwing Balls(Spheres)\";\n\"Spinning Apples\"(Food);\"Spinning Baseballs\";\n</code></pre>\n\n<p>I am looking for an incremental method that would move towards the following linkages:</p>\n\n<pre><code>Eating --&gt; Food\nApples --&gt; Food\nMarbles --&gt; Spheres\nOranges --&gt; Food, Spheres\nThrowing --&gt; Spheres\nBaseballs --&gt; Spheres\nBalls --&gt; Spheres\nSpinning --&gt; Neutral\nInvolving --&gt; Neutral\n</code></pre>\n\n<p>I do realize that in this specific case these might be slightly suspect matches, but it illustrates the problems I am having. My general thoughts were that if I incremented a word for appearing opposite the words in a category, but in that case I would end up incidentally linking everything to the word \"Involving\", I then thought that I would simply decrement a word for appearing in conjunction with multiple synonyms, or with non-synonyms, but then I would lose the link between \"Eating\" and \"Food\". Does anyone have any clue as to how I would put together an algorithm that would move me in the directions indicated above? </p>\n",
    "score": 8,
    "creation_date": 1341612183,
    "view_count": 7376,
    "answer_count": 4,
    "tags": "language-agnostic;machine-learning;nlp;artificial-intelligence;neural-network"
  },
  {
    "question_id": 4570751,
    "title": "What tag set is used in OpenNLP&#39;s german maxent model?",
    "body": "<p>currently I am using the OpenNLP tools to PoS-tag german sentences, with the maxent model listed on their <a href=\"http://opennlp.sourceforge.net/models-1.5/\">download-site</a>:</p>\n\n<pre>\nde      POS Tagger      Maxent model trained on tiger corpus.   de-pos-maxent.bin\n</pre>\n\n<p>This works very well and I got results as: </p>\n\n<pre>\nDiese, Community, bietet, Teilnehmern, der, Veranstaltungen, die, Möglichkeit ...\nPDAT, FM, VVFIN, NN, ART, NN, ART, NN ...\n</pre>\n\n<p>With the tagged sentences I want to do some further processing where I have to know the meaning of the single tags.  Unforunately searching the <a href=\"http://sourceforge.net/apps/mediawiki/opennlp/index.php?title=POS_Tagger\">OpenNLP-Wiki</a> for the tag sets isn't very helpful as it says:</p>\n\n<pre>\nTODO: Add more tag sets, also for non-english languages\n</pre>\n\n<p>Does anyone know where can I find the tag set used in the german maxent model?</p>\n",
    "score": 8,
    "creation_date": 1293807627,
    "view_count": 2995,
    "answer_count": 3,
    "tags": "tags;nlp;opennlp"
  },
  {
    "question_id": 75886674,
    "title": "How to compute sentence level perplexity from hugging face language models?",
    "body": "<p>I have a large collection of documents each consisting of ~ 10 sentences. For each document, I wish to find the sentence that maximises perplexity, or equivalently the loss from a fine-tuned causal LM. I have decided to use Hugging Face and the <code>distilgpt2</code> model for this purpose. I have 2 problems when trying to do in an efficient (vectorized) fashion:</p>\n<ol>\n<li><p>The tokenizer required padding to work in batch mode, but when computing the loss on padded <code>input_ids</code> those pad tokens are contributing to the loss. So the loss of a given sentence depends on the length of the longest sentence in the batch which is clearly wrong.</p>\n</li>\n<li><p>When I pass a batch of input IDs to the model and compute the loss, I get a scalar as it (mean?) pools across the batch. I instead need the loss per item, not the pooled one.</p>\n</li>\n</ol>\n<p>I made a version that operates on a sentence by sentence basis and while correct, it is extremely slow (I want to process ~ 25m sentences total). Any advice?</p>\n<p>Minimal example below:</p>\n<pre><code># Init\ntokenizer = AutoTokenizer.from_pretrained(&quot;distilgpt2&quot;)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(&quot;clm-gpu/checkpoint-138000&quot;)\nsegmenter = spacy.load('en_core_web_sm')\n\n# That's the part I need to vectorise, surely within a document (bsize ~ 10)\n# and ideally across documents (bsize as big as my GPU can handle)\ndef select_sentence(sentences):\n    &quot;&quot;&quot;We pick the sentence that maximizes perplexity&quot;&quot;&quot;\n    max_loss, best_index = 0, 0\n    for i, sentence in enumerate(sentences):\n        encodings = tokenizer(sentence, return_tensors=&quot;pt&quot;)\n        input_ids = encodings.input_ids\n        loss = lm(input_ids, labels=input_ids).loss.item()\n        if loss &gt; max_loss:\n            max_loss = loss\n            best_index = i\n\n    return sentences[best_index]\n\nfor document in documents:\n    sentences = [sentence.text.strip() for sentence in segmenter(document).sents]\n    best_sentence = select_sentence(sentences)\n    write(best_sentence)\n\n</code></pre>\n",
    "score": 8,
    "creation_date": 1680169994,
    "view_count": 12828,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers;large-language-model;huggingface-evaluate"
  },
  {
    "question_id": 62399475,
    "title": "Where I can find the complete list of SpaCy Dependency Parsing labels or annotations?",
    "body": "<p>I have try to refer to spaCy official website <a href=\"https://spacy.io/api/annotation#dependency-parsing\" rel=\"noreferrer\">https://spacy.io/api/annotation#dependency-parsing</a>\nbut I only got list of universal dependency relation which also on <a href=\"https://universaldependencies.org/u/dep/\" rel=\"noreferrer\">https://universaldependencies.org/u/dep/</a></p>\n\n<p>while, when I try to parse some sentences, I also got labels or annotations that not have been listed. Such as: <em>prep, dative,</em> and <em>dobj</em>, even though that those labels can be associated with preposition for <em>prep</em>, direct object for <em>dobj</em>, and ??? for <em>dative</em>.</p>\n\n<p><a href=\"https://i.sstatic.net/yC8TB.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/yC8TB.png\" alt=\"enter image description here\"></a>  </p>\n\n<p>Is there any reference for me to find the complete list of SpaCy Dependency Parsing labels or annotations?</p>\n",
    "score": 8,
    "creation_date": 1592270064,
    "view_count": 7820,
    "answer_count": 2,
    "tags": "nlp;dependencies;spacy;text-parsing"
  },
  {
    "question_id": 57231616,
    "title": "ValueError: [E088] Text of length 1027203 exceeds maximum of 1000000. spacy",
    "body": "<p>I'm trying to create a corpus of words by a text. I use spacy. So there is my code:</p>\n\n<pre><code>import spacy\nnlp = spacy.load('fr_core_news_md')\nf = open(\"text.txt\")\ndoc = nlp(''.join(ch for ch in f.read() if ch.isalnum() or ch == \" \"))\nf.close()\ndel f\nwords = []\nfor token in doc:\n    if token.lemma_ not in words:\n        words.append(token.lemma_)\n\nf = open(\"corpus.txt\", 'w')\nf.write(\"Number of words:\" + str(len(words)) + \"\\n\" + ''.join([i + \"\\n\" for i in sorted(words)]))\nf.close()\n</code></pre>\n\n<p>But it returns this exception:</p>\n\n<pre><code>ValueError: [E088] Text of length 1027203 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n</code></pre>\n\n<p>I tried somthing like this:</p>\n\n<pre><code>import spacy\nnlp = spacy.load('fr_core_news_md')\nnlp.max_length = 1027203\nf = open(\"text.txt\")\ndoc = nlp(''.join(ch for ch in f.read() if ch.isalnum() or ch == \" \"))\nf.close()\ndel f\nwords = []\nfor token in doc:\n    if token.lemma_ not in words:\n        words.append(token.lemma_)\n\nf = open(\"corpus.txt\", 'w')\nf.write(\"Number of words:\" + str(len(words)) + \"\\n\" + ''.join([i + \"\\n\" for i in sorted(words)]))\nf.close()\n</code></pre>\n\n<p>But got the same error:</p>\n\n<pre><code>ValueError: [E088] Text of length 1027203 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n</code></pre>\n\n<p>How to fix it?</p>\n",
    "score": 8,
    "creation_date": 1564226501,
    "view_count": 12825,
    "answer_count": 3,
    "tags": "python;python-3.x;nlp;spacy"
  },
  {
    "question_id": 44661200,
    "title": "Spacy to extract specific noun phrase",
    "body": "<p>Can I use spacy in python to find NP with specific neighbors? I want Noun phrases from my text that has verb before and after it.  </p>\n",
    "score": 8,
    "creation_date": 1497985709,
    "view_count": 8506,
    "answer_count": 3,
    "tags": "python;nlp;nltk;spacy"
  },
  {
    "question_id": 14765632,
    "title": "What is a relatively simple way to determine the probability that a sentence is in English?",
    "body": "<p>I have a number of strings (collections of characters) that represent sentences in different languages, say:</p>\n\n<blockquote>\n  <blockquote>\n    <p>Hello, my name is George.</p>\n    \n    <p>Das brot ist gut.</p>\n    \n    <p>... etc.</p>\n  </blockquote>\n</blockquote>\n\n<p>I want to assign each of them scores (from 0 .. 1) indicating the likelihood that they are English sentences. Is there an accepted algorithm (or Python library) from which to do this?</p>\n\n<p>Note: I don't care if the grammar of the English sentence is perfect.</p>\n",
    "score": 8,
    "creation_date": 1360297431,
    "view_count": 2070,
    "answer_count": 3,
    "tags": "python;string;nlp;bayesian"
  },
  {
    "question_id": 14510618,
    "title": "How is stemming useful?",
    "body": "<p>Simple question: When do we stem or lemmatize the words? Is stemming helpful for all nlp processes or are there applications where using full form of words might result in better accuracy or precision?</p>\n",
    "score": 8,
    "creation_date": 1359061057,
    "view_count": 6161,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 9286597,
    "title": "Stanford Parser multithread usage",
    "body": "<p>Stanford Parser is now 'thread-safe' as of <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml#History\" rel=\"noreferrer\">version 2.0</a> (02.03.2012).  I am currently running the command line tools and cannot figure out how to make use of my multiple cores by threading  the program.</p>\n\n<p>In the past, this question has been answered with \"Stanford Parser is not thread-safe\", as the FAQ still says.  I am hoping to find someone who has had success threading the latest version.</p>\n\n<p>I have tried using -t flag (-t10 and -tLLP) since that was all I could find in my searches, but both throw errors.</p>\n\n<p>An example of a command I issue is:</p>\n\n<pre><code>java -cp stanford-parser.jar edu.stanford.nlp.parser.lexparser.LexicalizedParser \\\n-outputFormat \"oneline\" ./grammar/englishPCFG.ser.gz ./corpus &gt; corpus.lex\n</code></pre>\n",
    "score": 8,
    "creation_date": 1329268240,
    "view_count": 3473,
    "answer_count": 1,
    "tags": "multithreading;nlp;multiprocessing;stanford-nlp"
  },
  {
    "question_id": 1271918,
    "title": "Ruby, Count syllables",
    "body": "<p>I am using ruby to calculate the Gunning Fog Index of some content that I have, I can successfully implement the algorithm described here: </p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Gunning_fog_index\" rel=\"nofollow noreferrer\">Gunning Fog Index</a></p>\n\n<p>I am using the below method to count the number of syllables in each word:</p>\n\n<pre><code>Tokenizer = /([aeiouy]{1,3})/\n\ndef count_syllables(word)\n\n  len = 0\n\n  if word[-3..-1] == 'ing' then\n    len += 1\n    word = word[0...-3]\n  end\n\n  got = word.scan(Tokenizer)\n  len += got.size()\n\n  if got.size() &gt; 1 and got[-1] == ['e'] and\n      word[-1].chr() == 'e' and\n      word[-2].chr() != 'l' then\n    len -= 1\n  end\n\n  return len\n\nend\n</code></pre>\n\n<p>It sometimes picks up words with only 2 syllables as having 3 syllables.  Can anyone give any advice or is aware of a better method?</p>\n\n<pre><code>text = \"The word logorrhoea is often used pejoratively to describe prose that is highly abstract and contains little concrete language. Since abstract writing is hard to visualize, it often seems as though it makes no sense and all the words are excessive. Writers in academic fields that concern themselves mostly with the abstract, such as philosophy and especially postmodernism, often fail to include extensive concrete examples of their ideas, and so a superficial examination of their work might lead one to believe that it is all nonsense.\"\n\n# used to get rid of any puncuation\ntext = text.gsub!(/\\W+/, ' ')\n\nword_array = text.split(' ')\n\nword_array.each do |word|\n    puts word if count_syllables(word) &gt; 2\nend\n</code></pre>\n\n<p>\"themselves\" is being counted as 3 but it's only 2</p>\n",
    "score": 8,
    "creation_date": 1250169821,
    "view_count": 5672,
    "answer_count": 4,
    "tags": "ruby;nlp"
  },
  {
    "question_id": 73290491,
    "title": "The model did not return a loss from the inputs - LabSE error",
    "body": "<p>I want to fine tune LabSE for Question answering using squad dataset. and i got this error:\n<code>ValueError: The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.</code></p>\n<p>I am trying to fine tune the model using pytorch. I tried to use smaller batch size and i took just 10% of training dataset because i had problems with memory allocation.\nIf memory allocation problems are gone this error happens.\nTo be honest i'm stuck with it. Do you have any hints?</p>\n<p>I'm trying to use huggingface tutorial, but i want to use other evaluation (i want to do it myself ) so i skipped using evaluation part of dataset.</p>\n<pre><code>from datasets import load_dataset\nraw_datasets = load_dataset(&quot;squad&quot;, split='train')\n\n\nfrom transformers import BertTokenizerFast, BertModel\nfrom transformers import AutoTokenizer\n\n\nmodel_checkpoint = &quot;setu4993/LaBSE&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = BertModel.from_pretrained(model_checkpoint)\n\n\n\nmax_length = 384\nstride = 128\n\n\ndef preprocess_training_examples(examples):\n    questions = [q.strip() for q in examples[&quot;question&quot;]]\n    inputs = tokenizer(\n        questions,\n        examples[&quot;context&quot;],\n        max_length=max_length,\n        truncation=&quot;only_second&quot;,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=&quot;max_length&quot;,\n    )\n\n    offset_mapping = inputs.pop(&quot;offset_mapping&quot;)\n    sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;)\n    answers = examples[&quot;answers&quot;]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer[&quot;answer_start&quot;][0]\n        end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        if offset[context_start][0] &gt; start_char or offset[context_end][1] &lt; end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[&quot;start_positions&quot;] = start_positions\n    inputs[&quot;end_positions&quot;] = end_positions\n    return inputs\n\n\ntrain_dataset = raw_datasets.map(\n    preprocess_training_examples,\n    batched=True,\n    remove_columns=raw_datasets.column_names,\n)\nlen(raw_datasets), len(train_dataset)\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    &quot;bert-finetuned-squad&quot;,\n    save_strategy=&quot;epoch&quot;,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n)\ntrainer.train()\n</code></pre>\n",
    "score": 8,
    "creation_date": 1660041805,
    "view_count": 17961,
    "answer_count": 4,
    "tags": "nlp;pytorch;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 66845379,
    "title": "How to set the label names when using the Huggingface TextClassificationPipeline?",
    "body": "<p>I am using a fine-tuned Huggingface model (on my company data) with the <a href=\"https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TextClassificationPipeline\" rel=\"noreferrer\">TextClassificationPipeline</a> to make class predictions. Now the labels that this <code>Pipeline</code> predicts defaults to <code>LABEL_0</code>, <code>LABEL_1</code> and so on. Is there a way to supply the label mappings to the <code>TextClassificationPipeline</code> object so that the output may reflect the same?</p>\n<blockquote>\n<p>Env:</p>\n<ul>\n<li>tensorflow==2.3.1</li>\n<li>transformers==4.3.2</li>\n</ul>\n</blockquote>\n<p>Sample Code:</p>\n<pre><code>import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n\nfrom transformers import TextClassificationPipeline, TFAutoModelForSequenceClassification, AutoTokenizer\n\nMODEL_DIR = &quot;path\\to\\my\\fine-tuned\\model&quot;\n\n# Feature extraction pipeline\nmodel = TFAutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\npipeline = TextClassificationPipeline(model=model,\n                                      tokenizer=tokenizer,\n                                      framework='tf',\n                                      device=0)\n\nresult = pipeline(&quot;It was a good watch. But a little boring.&quot;)[0]\n</code></pre>\n<p>Output:</p>\n<pre><code>In [2]: result\nOut[2]: {'label': 'LABEL_1', 'score': 0.8864616751670837}\n</code></pre>\n",
    "score": 8,
    "creation_date": 1616959670,
    "view_count": 8401,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers"
  },
  {
    "question_id": 60352003,
    "title": "How to Download webpage as .mhtml",
    "body": "<p>I am able to successfully open a URL and save the resultant page as a .html file. However, I am unable to determine how to download and save a .mhtml (Web Page, Single File).</p>\n\n<p>My code is: </p>\n\n<pre><code>import urllib.parse, time\nfrom urllib.parse import urlparse\nimport urllib.request\n\nurl = ('https://www.example.com')\n\nencoded_url = urllib.parse.quote(url, safe='')\n\nprint(encoded_url)\n\nbase_url = (\"https://translate.google.co.uk/translate?sl=auto&amp;tl=en&amp;u=\")\n\ntranslation_url = base_url+encoded_url\n\nprint(translation_url)\n\nreq = urllib.request.Request(translation_url, headers={'User-Agent': 'Mozilla/6.0'})\n\nprint(req)\n\nresponse = urllib.request.urlopen(req)\n\ntime.sleep(15)\n\nprint(response)\n\nwebContent = response.read()\n\nprint(webContent)\n\nf = open('GoogleTranslated.html', 'wb')\n\nf.write(webContent)\n\nprint(f)\n\nf.close\n</code></pre>\n\n<p>I have tried to use wget using the details captured in this question:\n<a href=\"https://stackoverflow.com/questions/42966245/how-to-download-a-webpage-mhtml-format-using-wget-in-python\">How to download a webpage (mhtml format) using wget in python</a> but the details are incomplete (or I am simply unabl eto understand).</p>\n\n<p>Any suggestions would be helpful at this stage.</p>\n",
    "score": 8,
    "creation_date": 1582373291,
    "view_count": 8335,
    "answer_count": 5,
    "tags": "python-3.x;nlp;arabic"
  },
  {
    "question_id": 58961983,
    "title": "How do you save a model, dictionary and corpus to disk in Gensim, and then load them again?",
    "body": "<p>In Gensim's <a href=\"https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html\" rel=\"noreferrer\">documentation</a>, it says:</p>\n\n<blockquote>\n  <p>You can save trained models to disk and later load them back, either to continue training on new training documents or to transform new documents.</p>\n</blockquote>\n\n<p>I would like to do this with a dictionary, corpus and tf.idf model. However, the documentation seems to say that it is possible, without explaining how to save these things and load them back up again.</p>\n\n<p>How do you do this?</p>\n\n<hr>\n\n<p>I've been using Pickle, but don't know if this is right...</p>\n\n<pre><code>import pickle\npickle.dump(tfidf, open(\"tfidf.p\", \"wb\"))\ntfidf_reloaded = pickle.load(open(\"tfidf.p\", \"rb\"))\n</code></pre>\n",
    "score": 8,
    "creation_date": 1574278220,
    "view_count": 7952,
    "answer_count": 3,
    "tags": "python;nlp;gensim"
  },
  {
    "question_id": 55921104,
    "title": "Spacy similarity warning : &quot;Evaluating Doc.similarity based on empty vectors.&quot;",
    "body": "<p>I'm trying to do data enhancement with a FAQ dataset. I change words, specifically nouns, by most similar words with <code>Wordnet</code> checking the similarity with Spacy. I use multiple for loop to go through my dataset.</p>\n\n<pre><code>import spacy\nimport nltk\nfrom nltk.corpus import wordnet as wn\nimport pandas as pd\n\nnlp = spacy.load('en_core_web_md')\nnltk.download('wordnet')\nquestions = pd.read_csv(\"FAQ.csv\")\n\nlist_questions = []\nfor question in questions.values:\n    list_questions.append(nlp(question[0]))\n\nfor question in list_questions: \n    for token in question:\n        treshold = 0.5\n        if token.pos_ == 'NOUN':\n            wordnet_syn = wn.synsets(str(token), pos=wn.NOUN)  \n            for syn in wordnet_syn:\n                for lemma in syn.lemmas():\n                    similar_word = nlp(lemma.name())\n                    if similar_word.similarity(token) != 1. and similar_word.similarity(token) &gt; treshold:\n                        good_word = similar_word\n                        treshold = token.similarity(similar_word)\n</code></pre>\n\n<p>However, the following warning is printed several times and I don't understand why : </p>\n\n<blockquote>\n  <p>UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.</p>\n</blockquote>\n\n<p>It is my <code>similar_word.similarity(token)</code> which creates the problem but I don't understand why. \nThe form of my list_questions is :</p>\n\n<p><code>list_questions = [Do you have a paper or other written explanation to introduce your model's details?, Where is the BERT code come from?, How large is a sentence vector?]</code></p>\n\n<p>I need to check token but also the <code>similar_word</code> in the loop, for example, I still get the error here :</p>\n\n<pre><code>tokens = nlp(u'dog cat unknownword')\nsimilar_word = nlp(u'rabbit')\n\nif(similar_word):\n    for token in tokens:\n        if (token):\n            print(token.text, similar_word.similarity(token))\n</code></pre>\n",
    "score": 8,
    "creation_date": 1556627707,
    "view_count": 12371,
    "answer_count": 2,
    "tags": "python-3.x;nlp;pytorch;spacy;wordnet"
  },
  {
    "question_id": 29311279,
    "title": "Linguistic tagger incorrectly tagging as &#39;OtherWord&#39;",
    "body": "<p>I've been using <code>NSLinguisticTagger</code> with sentences and have been encountering a strange issue with sentences such as 'I am hungry' or 'I am drunk'. Whilst one would expect 'I' to be tagged as a pronoun, 'am' as a verb and 'hungry' as an adjective, they are not. Rather they are all tagged as <code>OtherWord</code>.</p>\n\n<p>Is there something I'm doing incorrectly?</p>\n\n<pre><code>NSString *input = @\"I am hungry\";\nNSLinguisticTaggerOptions options = NSLinguisticTaggerOmitWhitespace;\nNSLinguisticTagger *tagger = [[NSLinguisticTagger alloc] initWithTagSchemes:[NSLinguisticTagger availableTagSchemesForLanguage:@\"en\"] options:options];\ntagger.string = input;\n\n[tagger enumerateTagsInRange:NSMakeRange(0, input.length) scheme:NSLinguisticTagSchemeNameTypeOrLexicalClass options:options usingBlock:^(NSString *tag, NSRange tokenRange, NSRange sentenceRange, BOOL *stop) {\n    NSString *token = [input substringWithRange:tokenRange];\n    NSString *lemma = [tagger tagAtIndex:tokenRange.location\n                                  scheme:NSLinguisticTagSchemeLemma\n                              tokenRange: NULL\n                           sentenceRange:NULL];\n    NSLog(@\"%@ (%@) : %@\\n\", token, lemma, tag);\n}];\n</code></pre>\n\n<p>And the output is:</p>\n\n<pre><code>I ((null)) : OtherWord\nam ((null)) : OtherWord\nhungry ((null)) : OtherWord\n</code></pre>\n",
    "score": 8,
    "creation_date": 1427495639,
    "view_count": 1475,
    "answer_count": 2,
    "tags": "ios;objective-c;cocoa;nlp;linguistics"
  },
  {
    "question_id": 23989168,
    "title": "Uses/Applications of Part-of-speech-tagging (POS Tagging)",
    "body": "<p>I understand the implicit value of part-of-speech tagging and have seen mentions about its use in parsing, text-to-speech conversion, etc.</p>\n\n<p>Could you tell me how is the output of a PoS tagger formated ?\nAlso, could you explain how is such an output used by other tasks/parts of an NLP system?</p>\n",
    "score": 8,
    "creation_date": 1401693575,
    "view_count": 4748,
    "answer_count": 2,
    "tags": "nlp;part-of-speech"
  },
  {
    "question_id": 57548180,
    "title": "Filling torch tensor with zeros after certain index",
    "body": "<p>Given a 3d tenzor, say:\n<code>batch x sentence length x embedding dim</code></p>\n\n<pre><code>a = torch.rand((10, 1000, 96)) \n</code></pre>\n\n<p>and an array(or tensor) of actual lengths for each sentence</p>\n\n<pre><code>lengths =  torch .randint(1000,(10,))\n</code></pre>\n\n<p><code>outputs tensor([ 370., 502., 652., 859., 545., 964., 566., 576.,1000., 803.])</code></p>\n\n<p>How to fill tensor ‘a’ with zeros after certain index along dimension 1 (sentence length) according to tensor ‘lengths’ ?</p>\n\n<p>I want smth like that :</p>\n\n<pre><code>a[ : , lengths : , : ]  = 0\n</code></pre>\n\n<p>One way of doing it (slow if batch size is big enough):</p>\n\n<pre><code>for i_batch in range(10):\n    a[ i_batch  , lengths[i_batch ] : , : ]  = 0\n</code></pre>\n",
    "score": 8,
    "creation_date": 1566160405,
    "view_count": 9464,
    "answer_count": 1,
    "tags": "python;nlp;pytorch"
  },
  {
    "question_id": 56554380,
    "title": "Why can&#39;t I import functions in bert after pip install bert",
    "body": "<p>I am a beginner for bert, and I am trying to use files of bert given on the GitHub:<a href=\"https://github.com/google-research/bert\" rel=\"noreferrer\">https://github.com/google-research/bert</a></p>\n\n<p>However I cannot import files(such as run_classifier, optimisation and so on) from bert after using <code>pip install bert</code> to install bert in terminal. I tried to run following codes in jupiter notebook:</p>\n\n<pre><code>import bert\nfrom bert import run_classifier\n</code></pre>\n\n<p>And the error is:</p>\n\n<pre><code>ImportError: cannot import name 'run_classifier'\n</code></pre>\n\n<p>Then I found the file named 'bert' in <code>\\anaconda3\\lib\\python3.6\\site-packages</code>, and there were no python files named 'run_classifier', 'optimization' etc inside it. So I downloaded those files from GitHub and put them into file 'bert' by myself. After doing this I could import run_classifier.</p>\n\n<p>However, another problem occurred. I couldn't use the functions inside the files although I could import them.\nFor example, there's a function <code>convert_to_unicode</code> in tokenization.py:</p>\n\n<pre><code>Help on module bert.tokenization in bert:\n\nNAME\n\n    bert.tokenization - Tokenization classes.    \nFUNCTIONS\n\n    convert_to_unicode(text)\n    Converts `text` to Unicode (if it's not already), assuming utf-8 input.\n</code></pre>\n\n<p>Then I tried this:</p>\n\n<pre><code>import tokenization from bert\nconvert_to_unicode('input.txt')\n</code></pre>\n\n<p>And the error is:</p>\n\n<pre><code>NameError: name 'convert_to_unicode' is not defined\n</code></pre>\n\n<p>Then I tried:</p>\n\n<pre><code>from tokenization import convert_to_unicode\n</code></pre>\n\n<p>And the error is:</p>\n\n<pre><code>ModuleNotFoundError: No module named 'tokenization'\n</code></pre>\n\n<p>I am really confused about this. </p>\n",
    "score": 8,
    "creation_date": 1560311006,
    "view_count": 20001,
    "answer_count": 3,
    "tags": "python;nlp;bert-language-model"
  },
  {
    "question_id": 55515637,
    "title": "How can I untokenize a spacy.tokens.token.Token?",
    "body": "<p>how can I untokenize the output of this code?</p>\n\n<p>class Core:</p>\n\n<pre><code>def __init__(self, user_input):\n    pos = pop(user_input)\n    subject = \"\"\n    for token in pos:\n        if token.dep == nsubj:\n            subject = untokenize.untokenize(token)\n    subject = S(subject)\n</code></pre>\n\n<p>I tried:\n<a href=\"https://pypi.org/project/untokenize/\" rel=\"noreferrer\">https://pypi.org/project/untokenize/</a></p>\n\n<p>MosesDetokenizer</p>\n\n<p>.join()</p>\n\n<p>But I have this error for my last code (from this post):</p>\n\n<pre><code>TypeError: 'spacy.tokens.token.Token' object is not iterable\n</code></pre>\n\n<p>This error for .join():</p>\n\n<pre><code>AttributeError: 'spacy.tokens.token.Token' object has no attribute 'join'\n</code></pre>\n\n<p>And for MosesDetokenizer:\n    text = u\" {} \".format(\" \".join(tokens))\nTypeError: can only join an iterable</p>\n",
    "score": 8,
    "creation_date": 1554379618,
    "view_count": 7026,
    "answer_count": 3,
    "tags": "python;nlp;nltk;token;spacy"
  },
  {
    "question_id": 48199353,
    "title": "How to use spacy in large dataset with short sentences efficiently?",
    "body": "<p>I choose spacy to process kinds of text because of the performance of it's lemmatation compared with nltk. But When I process millions short text, it always consumed all of my memory(32G) and crashed. Without it just a few minutes and less than 10G mem is consumed. </p>\n\n<p>Is something wrong with the usage of this method? is there any better solution to improve the performance? Thanks!</p>\n\n<pre><code>def tokenizer(text):\n    try:\n        tokens = [ word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n        tokens = list(filter(lambda t: t.lower() not in stop_words, tokens))\n        tokens = list(filter(lambda t: t not in punctuation, tokens))\n        tokens = list(filter(lambda t: len(t) &gt; 4, tokens))\n        filtered_tokens = []\n        for token in tokens:\n            if re.search('[a-zA-Z]', token):\n                filtered_tokens.append(token)\n\n        spacy_parsed = nlp(' '.join(filtered_tokens))\n        filtered_tokens = [token.lemma_ for token in spacy_parsed]\n        return filtered_tokens\n    except Exception as e:\n        raise e\n</code></pre>\n\n<p>Dask parrallel computing</p>\n\n<pre><code>ddata = dd.from_pandas(res, npartitions=50)\ndef dask_tokenizer(df):\n    df['text_token'] = df['text'].map(tokenizer)\n    return df\n%time res_final = ddata.map_partitions(dask_tokenizer).compute(get=get)\n</code></pre>\n\n<p>Info about spaCy</p>\n\n<pre><code>spaCy version      2.0.5          \nLocation           /opt/conda/lib/python3.6/site-packages/spacy\nPlatform           Linux-4.4.0-103-generic-x86_64-with-debian-stretch-sid\nPython version     3.6.3          \nModels             en, en_default \n</code></pre>\n",
    "score": 8,
    "creation_date": 1515641296,
    "view_count": 8833,
    "answer_count": 3,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 45026607,
    "title": "Best Algorithm to make correction typos in text",
    "body": "<p>I have a list of word library and a text in which there are a spell error (typos), and I want to correct the word spell error to be correct according to list of library</p>\n\n<p>for example </p>\n\n<p>in list of word :</p>\n\n<p><code>listOfWord = [...,\"halo\",\"saya\",\"sedangkan\",\"semangat\",\"cemooh\"..];</code></p>\n\n<p>this is my string :</p>\n\n<p><code>string = \"haaallllllooo ssya sdngkan ceemoooh , smngat semoga menyenangkan\"</code></p>\n\n<p>I want change the spellerror to be correct like :</p>\n\n<p><code>string = \"halo saya sedangkan cemooh, semangat semoga menyenangkan\"</code></p>\n\n<p>what is the best algorithm to check each word in list, because I have millions of words in the list and have many possibilities</p>\n",
    "score": 8,
    "creation_date": 1499753196,
    "view_count": 10316,
    "answer_count": 6,
    "tags": "python;string;algorithm;nlp;pattern-matching"
  },
  {
    "question_id": 33972717,
    "title": "convert plural nouns to singular NLP",
    "body": "<p>I have a list of plural nouns. For example, apples, oranges and etc. I would like to convert all of them to singular nouns. Is there any tools for this purpose? Prefer it to be Java or Python.</p>\n",
    "score": 8,
    "creation_date": 1448722694,
    "view_count": 8219,
    "answer_count": 2,
    "tags": "java;python;nlp"
  },
  {
    "question_id": 26478445,
    "title": "How to determine if a string is English sentence or code?",
    "body": "<p>Consider the following two strings, the first one is code, the second one is English sentence (phrase to be precise). How can I detect that the first one is code and the second is not. </p>\n\n<pre><code>1. for (int i = 0; i &lt; b.size(); i++) {\n2. do something in English (not necessary to be a sentence).\n</code></pre>\n\n<p>I'm thinking about counting special characters (such as \"=\", \";\", \"++\", etc ), and set if to some threshold.  Are there any better ways to do this? Any Java libraries? </p>\n\n<p>Note that the code may not parsable, because it is not a complete method/statement/expression.</p>\n\n<p>My assumption is that English sentences are pretty regular, it most likely contains only \",\", \".\", \"_\", \"(\", \")\", etc. They do not contains something like this: <code>write(\"the whole lot of text\");</code></p>\n",
    "score": 8,
    "creation_date": 1413862387,
    "view_count": 4360,
    "answer_count": 7,
    "tags": "java;string;nlp"
  },
  {
    "question_id": 15710292,
    "title": "How to compute letter frequency similarity?",
    "body": "<p>Given this data (relative letter frequency from both languages):</p>\n\n<pre><code>spanish =&gt; 'e' =&gt; 13.72, 'a' =&gt; 11.72, 'o' =&gt; 8.44, 's' =&gt; 7.20, 'n' =&gt; 6.83,\nenglish =&gt; 'e' =&gt; 12.60, 't' =&gt; 9.37, 'a' =&gt; 8.34, 'o' =&gt; 7.70, 'n' =&gt; 6.80,\n</code></pre>\n\n<p>And then computing the letter frequency for the string \"this is a test\" gives me:</p>\n\n<pre><code>\"t\"=&gt;21.43, \"s\"=&gt;14.29, \"i\"=&gt;7.14, \"r\"=&gt;7.14, \"y\"=&gt;7.14, \"'\"=&gt;7.14, \"h\"=&gt;7.14, \"e\"=&gt;7.14, \"l\"=&gt;7.14\n</code></pre>\n\n<p>So, what would be a good approach for matching the given string letter frequency with a language (and try to detect the language)? I've seen (and have tested) some examples using levenshtein distance, and it seems to work fine until you add more languages.</p>\n\n<pre><code>\"this is a test\" gives (shortest distance:) [:english, 13] ...\n\"esto es una prueba\" gives (shortest distance:) [:spanish, 13] ...\n</code></pre>\n",
    "score": 8,
    "creation_date": 1364585931,
    "view_count": 2436,
    "answer_count": 3,
    "tags": "text;nlp;levenshtein-distance;letter"
  },
  {
    "question_id": 12918606,
    "title": "What is the default nltk part of speech tagset?",
    "body": "<p>While experimenting with NLTK part of speech tagging, I noticed a lot of <code>VBP</code> tags in the output of my calls to <code>nltk.pos_tag</code>.  I noticed this tag is not in the Brown Corpus part of speech tagset.  It is however a part of the UPenn tagset.</p>\n\n<p>What tagset does nltk use by default?  I can't find this in the official documentation or the apidocs.</p>\n",
    "score": 8,
    "creation_date": 1350402688,
    "view_count": 4362,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 2061881,
    "title": "Natural Language Parsing tools: what is out there and what is not?",
    "body": "<p>I'm looking for various NLP tools for a project I'm working on and right now I've found most useful the Stanford NLP projects.  </p>\n\n<p>Does anyone know if there are other tools that are out there that would be useful for a language understander? </p>\n\n<p>And more importantly, are there tools that are NOT out there?</p>\n\n<p>Most specifically, I'm looking for an api for morphophoneme analysis etc.</p>\n\n<p>EDIT: I am an academic (a student working on a research project) and am mainly looking for open source or, at least, open api projects.</p>\n",
    "score": 8,
    "creation_date": 1263437853,
    "view_count": 2053,
    "answer_count": 4,
    "tags": "api;nlp"
  },
  {
    "question_id": 548951,
    "title": "NLP classify sentences/paragraph as funny",
    "body": "<p>Is there a way to classify a particular sentence/paragraph as funny. There are very few pointers as to where one should go further on this.</p>\n",
    "score": 8,
    "creation_date": 1234610041,
    "view_count": 2894,
    "answer_count": 5,
    "tags": "nlp;classification"
  },
  {
    "question_id": 62945590,
    "title": "How can I transform verbs from present tense to past tense with using NLP library?",
    "body": "<h1>What I would like to do</h1>\n<p>I would like to transform verbs from  present tense to past tense with using NLP library like below.</p>\n<pre><code>As she leaves the kitchen, his voice follows her.\n\n#output\nAs she left the kitchen, his voice followed her.\n</code></pre>\n<h1>Problem</h1>\n<p>There is no way to transform from present tense to past tense.</p>\n<p>I've checked the following similar question, but they only introduced the way to transform from\npast tense to present tense.</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/3753021/using-nltk-and-wordnet-how-do-i-convert-simple-tense-verb-into-its-present-pas\">Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?</a></li>\n</ul>\n<h1>What I tried to do</h1>\n<p>I was able to transform verbs from past tense to present tense using <a href=\"https://spacy.io/usage/linguistic-features\" rel=\"nofollow noreferrer\">spaCy</a>.\nHowever, there is no way to do the same thing from present tense to past tense.</p>\n<pre class=\"lang-py prettyprint-override\"><code>text = &quot;As she left the kitchen, his voice followed her.&quot;\ndoc_dep = nlp(text)\nfor i in range(len(doc_dep)):\n    token = doc_dep[i]\n    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_) \n    if token.pos_== 'VERB':\n        print(token.text)\n        print(token.lemma_)\n        text = text.replace(token.text, token.lemma_)\nprint(text)\n\n#output\n'As she leave the kitchen, his voice follow her.'\n</code></pre>\n<h1>Development Environment</h1>\n<p>Python 3.7.0</p>\n<p>spaCy version 2.3.1</p>\n",
    "score": 8,
    "creation_date": 1594947771,
    "view_count": 5926,
    "answer_count": 3,
    "tags": "python;python-3.x;nlp;stanford-nlp;spacy"
  },
  {
    "question_id": 56251267,
    "title": "BucketIterator throws &#39;Field&#39; object has no attribute &#39;vocab&#39;",
    "body": "<p>It's not a new question, references I found without any solution working for me <a href=\"https://stackoverflow.com/questions/53300155/attributeerror-field-object-has-no-attribute-vocab\">first</a> and <a href=\"https://stackoverflow.com/questions/51231852/iterating-over-torchtext-data-bucketiterator-object-throws-attributeerror-field\">second</a>.\nI'm a newbie to PyTorch, facing <code>AttributeError: 'Field' object has no attribute 'vocab'</code> while creating batches of the text data in <code>PyTorch</code> using <code>torchtext</code>.</p>\n\n<p>Following up the book <code>Deep Learning with PyTorch</code> I wrote the same example as explained in the book.</p>\n\n<p>Here's the snippet:</p>\n\n<pre><code>from torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import GloVe\n\nTEXT = data.Field(lower=True, batch_first=True, fix_length=20)\nLABEL = data.Field(sequential=False)\ntrain, test = datasets.IMDB.splits(TEXT, LABEL)\n\nprint(\"train.fields:\", train.fields)\nprint()\nprint(vars(train[0]))  # prints the object\n\n\n\nTEXT.build_vocab(train, vectors=GloVe(name=\"6B\", dim=300),\n                 max_size=10000, min_freq=10)\n\n# VOCABULARY\n# print(TEXT.vocab.freqs)  # freq\n# print(TEXT.vocab.vectors)  # vectors\n# print(TEXT.vocab.stoi)  # Index\n\ntrain_iter, test_iter = data.BucketIterator.splits(\n    (train, test), batch_size=128, device=-1, shuffle=True, repeat=False)  # -1 for cpu, None for gpu\n\n# Not working (FROM BOOK)\n# batch = next(iter(train_iter))\n\n# print(batch.text)\n# print()\n# print(batch.label)\n\n# This also not working (FROM Second solution)\nfor i in train_iter:\n    print (i.text)\n    print (i.label)\n</code></pre>\n\n<p>Here's the stacktrace:</p>\n\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-33-433ec3a2ca3c&gt; in &lt;module&gt;()\n      7 \n      8 \n----&gt; 9 for i in train_iter:\n     10     print (i.text)\n     11     print (i.label)\n\n/anaconda3/lib/python3.6/site-packages/torchtext/data/iterator.py in __iter__(self)\n    155                     else:\n    156                         minibatch.sort(key=self.sort_key, reverse=True)\n--&gt; 157                 yield Batch(minibatch, self.dataset, self.device)\n    158             if not self.repeat:\n    159                 return\n\n/anaconda3/lib/python3.6/site-packages/torchtext/data/batch.py in __init__(self, data, dataset, device)\n     32                 if field is not None:\n     33                     batch = [getattr(x, name) for x in data]\n---&gt; 34                     setattr(self, name, field.process(batch, device=device))\n     35 \n     36     @classmethod\n\n/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py in process(self, batch, device)\n    199         \"\"\"\n    200         padded = self.pad(batch)\n--&gt; 201         tensor = self.numericalize(padded, device=device)\n    202         return tensor\n    203 \n\n/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py in numericalize(self, arr, device)\n    300                 arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\n    301             else:\n--&gt; 302                 arr = [self.vocab.stoi[x] for x in arr]\n    303 \n    304             if self.postprocessing is not None:\n\n/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py in &lt;listcomp&gt;(.0)\n    300                 arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\n    301             else:\n--&gt; 302                 arr = [self.vocab.stoi[x] for x in arr]\n    303 \n    304             if self.postprocessing is not None:\n\nAttributeError: 'Field' object has no attribute 'vocab'\n</code></pre>\n\n<blockquote>\n  <p>If not using BucketIterator, what else I can use to get a similar\n  output?</p>\n</blockquote>\n",
    "score": 8,
    "creation_date": 1558509383,
    "view_count": 9991,
    "answer_count": 1,
    "tags": "python;iterator;nlp;pytorch;torchtext"
  },
  {
    "question_id": 50344228,
    "title": "Remove Spacy downloaded model",
    "body": "<p>After downloading and linking a spacy model (en large) by:</p>\n\n<pre><code>python -m spacy download en_core_web_lg\n</code></pre>\n\n<p>which is around 850 Mb of data.</p>\n\n<p>How can it find and delete the data (downloaded model) on my mac to free some space?</p>\n\n<pre><code>Spacy: 2.0.18  \nPython: 3.6.9  \nen_core_web_lg: 2.0.0\n</code></pre>\n",
    "score": 8,
    "creation_date": 1526368148,
    "view_count": 8294,
    "answer_count": 1,
    "tags": "python;pip;nlp;spacy"
  },
  {
    "question_id": 47205762,
    "title": "Embedding 3D data in Pytorch",
    "body": "<p>I want to implement character-level embedding. </p>\n\n<p>This is usual word embedding.</p>\n\n<p><strong>Word Embedding</strong></p>\n\n<pre><code>Input: [ [‘who’, ‘is’, ‘this’] ] \n-&gt; [ [3, 8, 2] ]     # (batch_size, sentence_len)\n-&gt; // Embedding(Input)\n # (batch_size, seq_len, embedding_dim)\n</code></pre>\n\n<p>This is what i want to do.</p>\n\n<p><strong>Character Embedding</strong></p>\n\n<pre><code>Input: [ [ [‘w’, ‘h’, ‘o’, 0], [‘i’, ‘s’, 0, 0], [‘t’, ‘h’, ‘i’, ‘s’] ] ]\n-&gt; [ [ [2, 3, 9, 0], [ 11, 4, 0, 0], [21, 10, 8, 9] ] ]      # (batch_size, sentence_len, word_len)\n-&gt; // Embedding(Input) # (batch_size, sentence_len, word_len, embedding_dim)\n-&gt; // sum each character embeddings  # (batch_size, sentence_len, embedding_dim)\nThe final output shape is same as Word embedding. Because I want to concat them later.\n</code></pre>\n\n<p>Although I tried it, I am not sure how to implement 3-D embedding. Do you know how to implement such a data?</p>\n\n<pre><code>def forward(self, x):\n    print('x', x.size()) # (N, seq_len, word_len)\n    bs = x.size(0)\n    seq_len = x.size(1)\n    word_len = x.size(2)\n    embd_list = []\n    for i, elm in enumerate(x):\n        tmp = torch.zeros(1, word_len, self.embd_size)\n        for chars in elm:\n            tmp = torch.add(tmp, 1.0, self.embedding(chars.unsqueeze(0)))\n</code></pre>\n\n<p>Above code got an error because output of <code>self.embedding</code> is <code>Variable</code>.</p>\n\n<pre><code>TypeError: torch.add received an invalid combination of arguments - got (torch.FloatTensor, float, Variable), but expected one of:\n * (torch.FloatTensor source, float value)\n * (torch.FloatTensor source, torch.FloatTensor other)\n * (torch.FloatTensor source, torch.SparseFloatTensor other)\n * (torch.FloatTensor source, float value, torch.FloatTensor other)\n      didn't match because some of the arguments have invalid types: (torch.FloatTensor, float, Variable)\n * (torch.FloatTensor source, float value, torch.SparseFloatTensor other)\n      didn't match because some of the arguments have invalid types: (torch.FloatTensor, float, Variable)\n</code></pre>\n\n<h2>Update</h2>\n\n<p>I could do this. But <code>for</code> is not effective for batch. Do you guys know more efficient way?</p>\n\n<pre><code>def forward(self, x):\n    print('x', x.size()) # (N, seq_len, word_len)\n    bs = x.size(0)\n    seq_len = x.size(1)\n    word_len = x.size(2)\n    embd = Variable(torch.zeros(bs, seq_len, self.embd_size))\n    for i, elm in enumerate(x): # every sample\n        for j, chars in enumerate(elm): # every sentence. [ [‘w’, ‘h’, ‘o’, 0], [‘i’, ‘s’, 0, 0], [‘t’, ‘h’, ‘i’, ‘s’] ]\n            chars_embd = self.embedding(chars.unsqueeze(0)) # (N, word_len, embd_size) [‘w’,‘h’,‘o’,0]\n            chars_embd = torch.sum(chars_embd, 1) # (N, embd_size). sum each char's embedding\n            embd[i,j] = chars_embd[0] # set char_embd as word-like embedding\n\n    x = embd # (N, seq_len, embd_dim)\n</code></pre>\n\n<h2>Update2</h2>\n\n<p>This is my final code. Thank you, Wasi Ahmad!</p>\n\n<pre><code>def forward(self, x):\n    # x: (N, seq_len, word_len)\n    input_shape = x.size()\n    bs = x.size(0)\n    seq_len = x.size(1)\n    word_len = x.size(2)\n    x = x.view(-1, word_len) # (N*seq_len, word_len)\n    x = self.embedding(x) # (N*seq_len, word_len, embd_size)\n    x = x.view(*input_shape, -1) # (N, seq_len, word_len, embd_size)\n    x = x.sum(2) # (N, seq_len, embd_size)\n\n    return x\n</code></pre>\n",
    "score": 8,
    "creation_date": 1510241594,
    "view_count": 4130,
    "answer_count": 2,
    "tags": "nlp;pytorch"
  },
  {
    "question_id": 36435207,
    "title": "pycorenlp: &quot;CoreNLP request timed out. Your document may be too long&quot;",
    "body": "<p>I'm trying to run <a href=\"https://github.com/smilli/py-corenlp\" rel=\"noreferrer\">pycorenlp</a> on a long text and get an <code>CoreNLP request timed out. Your document may be too long</code> error message. How to fix it? Is there any way to increase <a href=\"http://stanfordnlp.github.io/CoreNLP/\" rel=\"noreferrer\">Stanford CoreNLP</a>'s timed out?</p>\n\n<p>I don't want to segment the text into smaller texts.</p>\n\n<p>Here is the code I use:</p>\n\n<pre><code>'''\nFrom https://github.com/smilli/py-corenlp/blob/master/example.py\n'''\nfrom pycorenlp import StanfordCoreNLP\nimport pprint\n\nif __name__ == '__main__':\n    nlp = StanfordCoreNLP('http://localhost:9000')\n    fp = open(\"long_text.txt\")\n    text = fp.read()\n    output = nlp.annotate(text, properties={\n        'annotators': 'tokenize,ssplit,pos,depparse,parse',\n        'outputFormat': 'json'\n    })\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(output)\n</code></pre>\n\n<p>The Stanford Core NLP Server was launched using:</p>\n\n<pre><code>java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer 9000\n</code></pre>\n",
    "score": 8,
    "creation_date": 1459883995,
    "view_count": 5215,
    "answer_count": 1,
    "tags": "python;timeout;nlp;stanford-nlp"
  },
  {
    "question_id": 29419379,
    "title": "NLTK - TypeError: tagged_words() got an unexpected keyword argument &#39;simplify_tags&#39;",
    "body": "<p>I was just following the NLTK book chapter 5, and 'simplify_tags' argument in tagged_words() seems to be unexpected. I use Python 3.4, PyCharm, and standard NLTK package.</p>\n\n<pre><code>In[4]: nltk.corpus.brown.tagged_words()\nOut[4]: [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\nIn[5]: nltk.corpus.brown.tagged_words(simplify_tags = True)\nTraceback (most recent call last):\n  File \"C:\\Python34\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2883, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-5-c4f914e3e846&gt;\", line 1, in &lt;module&gt;\n    nltk.corpus.brown.tagged_words(simplify_tags = True)\nTypeError: tagged_words() got an unexpected keyword argument 'simplify_tags'\n</code></pre>\n\n<p>There is no problem with running this function without simplify_tags. I appreciate any suggestion or input. Thank you!</p>\n",
    "score": 8,
    "creation_date": 1427997356,
    "view_count": 3658,
    "answer_count": 2,
    "tags": "python-3.x;nlp;nltk;corpus;tagged-corpus"
  },
  {
    "question_id": 29301952,
    "title": "Testing the NLTK classifier on specific file",
    "body": "<p>The following code run <strong>Naive Bayes movie review classifier</strong>. \nThe code generate a list of the most informative features. </p>\n\n<p><em>Note:</em> <code>**movie review**</code> folder is in the <code>nltk</code>.</p>\n\n<pre><code>from itertools import chain\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import movie_reviews\nstop = stopwords.words('english')\n\ndocuments = [([w for w in movie_reviews.words(i) if w.lower() not in stop and w.lower() not in string.punctuation], i.split('/')[0]) for i in movie_reviews.fileids()]\n\n\nword_features = FreqDist(chain(*[i for i,j in documents]))\nword_features = word_features.keys()[:100]\n\nnumtrain = int(len(documents) * 90 / 100)\ntrain_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag in documents[:numtrain]]\ntest_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag  in documents[numtrain:]]\n\nclassifier = NaiveBayesClassifier.train(train_set)\nprint nltk.classify.accuracy(classifier, test_set)\nclassifier.show_most_informative_features(5)\n</code></pre>\n\n<p><a href=\"https://stackoverflow.com/questions/21107075/classification-using-movie-review-corpus-in-nltk-python\">link of code</a> from <a href=\"https://stackoverflow.com/users/610569/alvas\">alvas</a></p>\n\n<p>how can I <strong>test</strong> the classifier on <strong>specific file</strong>?</p>\n\n<p>Please let me know if my question is ambiguous or wrong.</p>\n",
    "score": 8,
    "creation_date": 1427463283,
    "view_count": 3284,
    "answer_count": 2,
    "tags": "python-2.7;nlp;classification;nltk;text-classification"
  },
  {
    "question_id": 27260799,
    "title": "Using counts and tfidf as features with scikit learn",
    "body": "<p>I'm trying to use both counts and <code>tfidf</code> as features for a multinomial Naive Bayes model. Here's my code:</p>\n<pre><code>text = [&quot;this is spam&quot;, &quot;this isn't spam&quot;]\nlabels = [0,1]\ncount_vectorizer = CountVectorizer(stop_words=&quot;english&quot;, min_df=3)\n\ntf_transformer = TfidfTransformer(use_idf=True)\ncombined_features = FeatureUnion([(&quot;counts&quot;, self.count_vectorizer), (&quot;tfidf&quot;, tf_transformer)]).fit(self.text)\n\nclassifier = MultinomialNB()\nclassifier.fit(combined_features, labels)\n</code></pre>\n<p>But I'm getting an error with <code>FeatureUnion</code> and <code>tfidf</code>:</p>\n<pre><code>TypeError: no supported conversion for types: (dtype('S18413'),)\n</code></pre>\n<p>Any idea why this could be happening? Is it not possible to have both counts and tfidf as features?</p>\n",
    "score": 8,
    "creation_date": 1417561980,
    "view_count": 2282,
    "answer_count": 1,
    "tags": "python;numpy;scikit-learn;nlp"
  },
  {
    "question_id": 21656861,
    "title": "Bytes vs Characters vs Words - which granularity for n-grams?",
    "body": "<p>At least 3 types of n-grams can be considered for representing text documents: </p>\n\n<ul>\n<li>byte-level n-grams</li>\n<li>character-level n-grams</li>\n<li>word-level n-grams</li>\n</ul>\n\n<p>It's unclear to me which one should be used for a given task (clustering, classification, etc). I read somewhere that character-level n-grams are preferred to word-level n-grams when the text contains typos, so that \"Mary loves dogs\" remains similar to \"Mary lpves dogs\".</p>\n\n<p>Are there other criteria to consider for choosing the \"right\" representation?</p>\n",
    "score": 8,
    "creation_date": 1391933919,
    "view_count": 3776,
    "answer_count": 3,
    "tags": "nlp;data-mining;text-mining;n-gram"
  },
  {
    "question_id": 19431754,
    "title": "Using Stanford Parser(CoreNLP) to find phrase heads",
    "body": "<p>I am going to use Stanford Corenlp 2013 to find phrase heads. I saw <a href=\"https://stackoverflow.com/questions/10768038/is-there-any-phrase-head-finder\">this thread</a>.</p>\n\n<p>But, the answer was not clear to me and I couldn't add any comment to continue that thread. So, I'm sorry for duplication.</p>\n\n<p>What I have at the moment is the parse tree of a sentence (using Stanford Corenlp) (I also tried with CONLL format which is created by Stanford Corenlp). And what I need is exactly the head of noun phrases.</p>\n\n<p>I don't know how I can use dependencies and the parse tree to extract heads of nounphrases.\nWhat I know is that if I have <code>nsubj (x, y)</code>, y is the head of the subject. If I have <code>dobj(x,y)</code>, y is the head of the direct object. f I have <code>iobj(x,y)</code>, y is the head of the indirect object.</p>\n\n<p>However, I am not sure if this way is the correct way to find all phrase heads. If it is, which rules I should add to get all heads of noun phrases?</p>\n\n<p>Maybe, it is worth saying that I need the heads of noun phrases in a java code.</p>\n",
    "score": 8,
    "creation_date": 1382025913,
    "view_count": 5346,
    "answer_count": 2,
    "tags": "java;nlp;stanford-nlp;phrase"
  },
  {
    "question_id": 18717536,
    "title": "In python, how can I distinguish between a human readable word and a random string?",
    "body": "<p>Examples of words:</p>\n\n<ol>\n<li>ball</li>\n<li>encyclopedia</li>\n<li>tableau</li>\n</ol>\n\n<p>Examples of random strings:</p>\n\n<ol>\n<li>qxbogsac</li>\n<li>jgaynj</li>\n<li>rnnfdwpm</li>\n</ol>\n\n<p>Of course it may happen that a random string will actually be a word in some language or look like one. But basically a human being is able to say it something looks 'random' or not, basically just by checking if you are able to pronounce it or not.</p>\n\n<p>I was trying to calculate entropy to distinguish those two but it's far from perfect. Do you have any other ideas, algorithms that works?</p>\n\n<p>There is one important requirement though, I can't use heavy-weight libraries like <code>nltk</code> or use dictionaries. Basically what I need is some simple and quick heuristic that works in most cases.</p>\n",
    "score": 8,
    "creation_date": 1378811943,
    "view_count": 7289,
    "answer_count": 6,
    "tags": "python;string;random;nlp;heuristics"
  },
  {
    "question_id": 18707401,
    "title": "Natural Language Processing for Smart Homes",
    "body": "<p>I'm writing up a Smart Home software for my bachelor's degree, that will only simulate the actual house, but I'm stuck at the NLP part of the project. The idea is to have the client listen to voice inputs (already done), transform it into text (done) and send it to the server, which does all the heavy lifting / decision making.</p>\n<p>So all my inputs will be fairly short (like &quot;please turn on the porch light&quot;). Based on this, I want to take the decision on which object to act, and how to act. So I came up with a few things to do, in order to write up something somewhat efficient.</p>\n<ol>\n<li>Get rid of unnecessary words (in the previous example &quot;please&quot; and &quot;the&quot; are words that don't change the meaning of what needs to be done; but if I say &quot;turn off <em>my</em> lights&quot;, &quot;my&quot; does have a fairly important meaning).</li>\n<li>Deal with synonyms (&quot;turn on lights&quot; should do the same as &quot;enable lights&quot; -- I know it's a stupid example). I'm guessing the only option is to have some kind of a dictionary (XML maybe), and just have a list of possible words for one particular object in the house.</li>\n<li>Detecting the verb and subject. &quot;turn on&quot; is the verb, and &quot;lights&quot; is the subject. I need a good way to detect this.</li>\n<li>General implementation. How are these things usually developed in terms of algorithms? I only managed to find one article about NLP in Smart Homes, which was very vague (and had bad English). Any links welcome.</li>\n</ol>\n",
    "score": 8,
    "creation_date": 1378762501,
    "view_count": 1055,
    "answer_count": 3,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 7805897,
    "title": "Simple spell checking algorithm",
    "body": "<p>I've been tasked with creating a simple spell checker for an assignment but have given next to no guidance so was wondering if anyone could help me out. I'm not after someone to do the assignment for me, but any direction or help with the algorithm would be awesome! If what I'm asking is not within the guildlines of the site then I'm sorry and I'll look elsewhere. :)</p>\n\n<p>The project loads correctly spelled lower case words and then needs to make spelling suggestions based on two criteria:</p>\n\n<ul>\n<li><p>One letter difference (either added or subtracted to get the word the same as a word in the dictionary). For example 'stack' would be a suggestion for 'staick' and 'cool' would be a suggestion for 'coo'.</p></li>\n<li><p>One letter substitution. So for example, 'bad' would be a suggestion for 'bod'.</p></li>\n</ul>\n\n<p>So, just to make sure I've explained properly.. I might load in the words [hello, goodbye, fantastic, good, god] and then the suggestions for the (incorrectly spelled) word 'godd' would be [good, god]. </p>\n\n<p><strong>Speed</strong> is my main consideration here so while I think I know a way to get this work, I'm really not too sure about how efficient it'll be. The way I'm thinking of doing it is to create a <code>map&lt;string, vector&lt;string&gt;&gt;</code> and then for each correctly spelled word that's loaded in, add the correctly spelled work in as a key in the map and the populate the vector to be all the possible 'wrong' permutations of that word. </p>\n\n<p>Then, when I want to look up a word, I'll look through every vector in the map to see if that word is a permutation of one of the correctly spelled word. If it is, I'll add the key as a spelling suggestion.</p>\n\n<p>This seems like it would take up HEAPS of memory though, cause there would surely be thousands of permutations for each word? It also seems like it'd be very very slow if my initial dictionary of correctly spelled words was large?</p>\n\n<p>I was thinking that maybe I could cut down time a bit by only looking in the keys that are similar to the word I'm looking at. But then again, if they're similar in some way then it probably means that the key will be a suggestion meaning I don't need all those permutations!</p>\n\n<p>So yeah, I'm a bit stumped about which direction I should look in. I'd really appreciate any help as I really am not sure how to estimate the speed of the different ways of doing things (we haven't been taught this at all in class). </p>\n",
    "score": 8,
    "creation_date": 1318934175,
    "view_count": 13072,
    "answer_count": 4,
    "tags": "c++;algorithm;language-agnostic;nlp;spell-checking"
  },
  {
    "question_id": 3926891,
    "title": "Trying to use HPSG PET Parser",
    "body": "<p>I'm trying to use the <a href=\"http://wiki.delph-in.net/moin/PetTop\" rel=\"nofollow noreferrer\">PET</a> Parser, but the given documentation for usage is insufficient. Can anyone point me to a good article or tutorial on using PET? Does it support UTF-8?</p>\n",
    "score": 8,
    "creation_date": 1286995250,
    "view_count": 1403,
    "answer_count": 2,
    "tags": "parsing;utf-8;nlp;pos-tagger"
  },
  {
    "question_id": 1003330,
    "title": "Is there a natural language parser for dates/times in ColdFusion?",
    "body": "<p>Is there a <a href=\"http://en.wikipedia.org/wiki/Natural_language_processing\" rel=\"noreferrer\">natural language parser</a> for date/times in ColdFusion?</p>\n",
    "score": 8,
    "creation_date": 1245178554,
    "view_count": 1327,
    "answer_count": 3,
    "tags": "datetime;coldfusion;nlp"
  },
  {
    "question_id": 60359628,
    "title": "Generating dictionaries to categorize tweets into pre-defined categories using NLTK",
    "body": "<p>I have a list of twitter users (screen_names) and I need to categorise them into 7 pre-defined categories -  Education, Art, Sports, Business, Politics, Automobiles, Technology based on thier interest area.\nI have extracted last 100 tweets of the users in Python and created a corpus for each user after cleaning the tweets.</p>\n\n<p>As mentioned here <a href=\"https://stackoverflow.com/questions/36875780/tweet-classification-into-multiple-categories-on-unsupervised-data-tweets\">Tweet classification into multiple categories on (Unsupervised data/tweets)</a> :<br>\nI am trying to generate dictionaries of common words under each category so that I can use it for classification. </p>\n\n<p>Is there a method to generate these dictionaries for a custom set of words automatically?</p>\n\n<p>Then I can use these for classifying the twitter data using a tf-idf classifier and get the degree of correspondence of the tweet to each of the categories. The highest value will give us the most probable category of the tweet. </p>\n\n<p>But since the categorisation is based on these pre-generated dictionaries, I am looking for a way to generate them automatically for a custom list of categories.</p>\n\n<p>Sample dictionaries :</p>\n\n<pre><code>Education - ['book','teacher','student'....]\n\nAutomobiles - ['car','auto','expo',....]\n</code></pre>\n\n<p>Example I/O:</p>\n\n<pre><code>**Input :** \nUserA - \"students visited share learning experience eye opening \narticle important preserve linaugural workshop students teachers \nothers know coding like know alphabets vision driving codeindia office \ninitiative get students tagging wrong people apologies apologies real \npeople work...\"\n.\n.\nUserN - &lt;another corpus of cleaned tweets&gt;\n\n\n**Expected output** : \nUserA - Education (61%)\nUserN - Automobiles (43%)\n</code></pre>\n",
    "score": 8,
    "creation_date": 1582437901,
    "view_count": 1330,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;nltk;text-classification"
  },
  {
    "question_id": 49811479,
    "title": "Fasttext algorithm use only word and subword? or sentences too?",
    "body": "<p>I read the paper and googled as well if there is any good example of the learning method(or more likely learning procedure)</p>\n\n<p>For word2vec, suppose there is corpus sentence</p>\n\n<blockquote>\n  <p>I go to school with lunch box that my mother wrapped every morning</p>\n</blockquote>\n\n<p>Then with window size 2, it will try to obtain the vector for 'school' by using surrounding words </p>\n\n<blockquote>\n  <p>['go', 'to', 'with', 'lunch']</p>\n</blockquote>\n\n<p>Now, FastText says that it uses the subword to obtain the vector, so it is definitely use n gram subword, for example with n=3,</p>\n\n<blockquote>\n  <p>['sc', 'sch', 'cho', 'hoo', 'ool', 'school']</p>\n</blockquote>\n\n<p>Up to here, I understood.\nBut it is not clear that if the other words are being used for learning for 'school'. I can only guess that other surrounding words are used as well like the word2vec, since the paper mentions</p>\n\n<p>=> the terms <em>Wc</em> and <em>Wt</em> are both used in functions</p>\n\n<p>where Wc is context word and Wt is word at sequence t.</p>\n\n<p>However, it is not clear that how FastText learns the vectors for word.</p>\n\n<p>.</p>\n\n<p>.</p>\n\n<p>Please clearly explain how FastText learning process goes in procedure?</p>\n\n<p>.</p>\n\n<p>.</p>\n\n<p>More precisely I want to know that if FastText also follows the same procedure as Word2Vec while it learns the n-gram characterized subword <strong><em>in addition</em></strong>. Or only n-gram characterized subword with word being used?</p>\n\n<p>How does it vectorize the subword at initial? etc</p>\n",
    "score": 8,
    "creation_date": 1523604170,
    "view_count": 4422,
    "answer_count": 2,
    "tags": "nlp;vectorization;word2vec;word-embedding;fasttext"
  },
  {
    "question_id": 39320015,
    "title": "How to split an NLP parse tree to clauses (independent and subordinate)?",
    "body": "<p>Given an NLP parse tree like </p>\n\n<pre><code>(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))\n</code></pre>\n\n<p>Original sentence is \"You could say that they regularly catch a shower, which adds to their exhilaration and joie de vivre.\"</p>\n\n<p>How could the clauses be extracted and reverse engineered?\nWe would be splitting at S and SBAR (to preserve the type of clause, eg subordinated)</p>\n\n<pre><code> - (S (NP (PRP You)) (VP (MD could) (VP (VB say) \n - (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower))\n - (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to)\n   (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW\n   de) (FW vivre))))))))))))) (. .)))\n</code></pre>\n\n<p>to arrive at</p>\n\n<pre><code> - You could say\n - that they regularly catch a shower \n - , which adds to their exhilaration and joie de vivre.\n</code></pre>\n\n<p>Splitting at S and SBAR seems very easy. The problem seems to be stripping away all the POS tags and chunks from the fragments.</p>\n",
    "score": 8,
    "creation_date": 1473012651,
    "view_count": 8498,
    "answer_count": 2,
    "tags": "nlp;nltk;grammar;stanford-nlp;clause"
  },
  {
    "question_id": 39100652,
    "title": "Python: Chunking others than noun phrases (e.g. prepositional) using Spacy, etc",
    "body": "<p>Since I was told Spacy was such a powerful Python module for natural speech processing, I am now desperately looking for a way to group words together to more than noun phrases, most importantly, prepositional phrases.\nI doubt there is a Spacy function for this but that would be the easiest way I guess (SpacySpaCy import is already implemented in my project).\nNevertheless, I'm open for any possibility of phrase recognition/ chunking. </p>\n",
    "score": 8,
    "creation_date": 1471953792,
    "view_count": 4388,
    "answer_count": 1,
    "tags": "python;nlp;chunking;phrases;spacy"
  },
  {
    "question_id": 10189685,
    "title": "Realtime tracking of top 100 twitter words per min/hour/day",
    "body": "<p>I recently came across this interview question:</p>\n\n<pre><code>Given a continuous twitter feed, design an algorithm to return the 100 most\nfrequent words used at this minute, this hour and this day.\n</code></pre>\n\n<p>I was thinking of a system with a hash map of <code>word -&gt; count</code> linked to 3 min-heaps for the current min, hour and day.</p>\n\n<p>Every incoming message is tokenized, sanitized and the word counts updated in the hash map (and  increase-key in the heaps if the word already exists in it)</p>\n\n<p>If any of the words don't exist in the heap (and heap size == 100) check if their <code>frequency &gt; min value</code> in the heap and if so then extract-min and insert into the heap.</p>\n\n<p>Are there better ways of doing this?</p>\n",
    "score": 8,
    "creation_date": 1334659401,
    "view_count": 2886,
    "answer_count": 2,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 8471472,
    "title": "Sentence detection using NLP",
    "body": "<p>I am trying to parse out sentences from a huge amount of text. using java I started off with NLP tools like OpenNLP and Stanford's Parser.</p>\n\n<p>But here is where i get stuck. though both these parsers are pretty great they fail when it comes to a non uniform text.</p>\n\n<p>For example in my text most sentences are delimited by a period, but in some cases like bullet points they aren't. Here both the parses fail miserably.</p>\n\n<p>I even tried setting the option in the stanford parses for multiple sentence terminators but the output was not much better!</p>\n\n<p>Any ideas??</p>\n\n<p><strong>Edit</strong> :To make it simpler I am looking to parse text where the delimiter is either a new line (\"\\n\") or a period(\".\") ...</p>\n",
    "score": 8,
    "creation_date": 1323677610,
    "view_count": 5515,
    "answer_count": 5,
    "tags": "java;nlp;opennlp;text-segmentation"
  },
  {
    "question_id": 6541141,
    "title": "Is POS tagging deterministic?",
    "body": "<p>I have been trying to wrap my head around why this is happening but am hoping someone can shed some light on this. I am trying to tag the following text:</p>\n\n<pre><code>ae0.475      X  mod \nae0.842      X  mod\nae0.842      X  mod \nae0.775      X  mod \n</code></pre>\n\n<p>using the following code:</p>\n\n<pre><code>import nltk\n\nfile = open(\"test\", \"r\")\n\nfor line in file:\n        words = line.strip().split(' ')\n        words = [word.strip() for word in words if word != '']\n        tags = nltk.pos_tag(words)\n        pos = [tags[x][1] for x in range(len(tags))]\n        key = ' '.join(pos)\n        print words, \" : \", key\n</code></pre>\n\n<p>and am getting the following result:</p>\n\n<pre><code>['ae0.475', 'X', 'mod']  :  NN NNP NN\n['ae0.842', 'X', 'mod']  :  -NONE- NNP NN\n['ae0.842', 'X', 'mod']  :  -NONE- NNP NN\n['ae0.775', 'X', 'mod']  :  NN NNP NN\n</code></pre>\n\n<p>And I don't get it. Does anyone know what is the reason for this inconsistency? I am not very particular about the accuracy about the pos tagging because I am attempting to extract some templates but it seems to be using different tags at different instances for a word that looks \"almost\" the same.</p>\n\n<p>As a solution, I replaced all numbers with 1 and solved the problem:</p>\n\n<pre><code>['ae1.111', 'X', 'mod']  :  NN NNP NN\n['ae1.111', 'X', 'mod']  :  NN NNP NN\n['ae1.111', 'X', 'mod']  :  NN NNP NN\n['ae1.111', 'X', 'mod']  :  NN NNP NN\n</code></pre>\n\n<p>but am curious why it tagged the instance with different tags in my first case. Any suggestions?</p>\n",
    "score": 8,
    "creation_date": 1309468263,
    "view_count": 467,
    "answer_count": 2,
    "tags": "python;nlp;machine-learning;nltk"
  },
  {
    "question_id": 6485748,
    "title": "Algorithm to understand meaning",
    "body": "<p>I want to know if is there any specific algorithm that can be followed to understand the meaning of a word/sentence/paragraph. Basically, I want to write a program that will take text/paragraph as input and try to find out what its meaning is. And thereby highlight the emotions within the text.</p>\n\n<p>Also, if there is an algorithm to understand things, can the same algorithm be applied to itself? It reduces the quest further to a point where we become interested in knowing meaning of meaning OR rather definition of definition.</p>\n",
    "score": 8,
    "creation_date": 1309111502,
    "view_count": 4285,
    "answer_count": 1,
    "tags": "algorithm;artificial-intelligence;nlp;semantics"
  },
  {
    "question_id": 69374258,
    "title": "Sentence similarity models not capturing opposite sentences",
    "body": "<p>I have tried different approaches to <strong>sentence similarity</strong>, namely:</p>\n<ul>\n<li><p><strong>spaCy models</strong>: <a href=\"https://spacy.io/models/en#en_core_web_md\" rel=\"noreferrer\"><code>en_core_web_md</code></a> and <a href=\"https://spacy.io/models/en#en_core_web_lg\" rel=\"noreferrer\"><code>en_core_web_lg</code></a>.</p>\n</li>\n<li><p><strong>Transformers</strong>: using the packages <a href=\"https://pypi.org/project/sentence-similarity/\" rel=\"noreferrer\"><code>sentence-similarity</code></a> and <a href=\"https://www.sbert.net/index.html\" rel=\"noreferrer\"><code>sentence-transformers</code></a>, I've tried models such as <a href=\"https://huggingface.co/distilbert-base-uncased\" rel=\"noreferrer\"><code>distilbert-base-uncased</code></a>, <a href=\"https://huggingface.co/bert-base-uncased\" rel=\"noreferrer\"><code>bert-base-uncased</code></a> or <a href=\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2\" rel=\"noreferrer\"><code>sentence-transformers/all-mpnet-base-v2</code></a>.</p>\n</li>\n<li><p><strong>Universal Sentence Encoding</strong>: using the package <a href=\"https://github.com/MartinoMensio/spacy-universal-sentence-encoder\" rel=\"noreferrer\"><code>spacy-universal-sentence-encoder</code></a>, with the models <a href=\"https://tfhub.dev/google/universal-sentence-encoder\" rel=\"noreferrer\"><code>en_use_md</code></a> and <a href=\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-large\" rel=\"noreferrer\"><code>en_use_cmlm_lg</code></a>.</p>\n</li>\n</ul>\n<p>However, while these models generally correctly detect similarity for equivalent sentences, they all fail when inputting negated sentences. E.g., these <strong>opposite sentences</strong>:</p>\n<ul>\n<li>&quot;I like rainy days because they make me feel relaxed.&quot;</li>\n<li>&quot;I don't like rainy days because they don't make me feel relaxed.&quot;</li>\n</ul>\n<p>return a <strong>similarity of 0.931</strong> with the model <code>en_use_md</code>.</p>\n<p>However, sentences that could be considered <strong>very similar</strong>:</p>\n<ul>\n<li>&quot;I like rainy days because they make me feel relaxed.&quot;</li>\n<li>&quot;I enjoy rainy days because they make me feel calm.&quot;</li>\n</ul>\n<p>return a <strong>smaller similarity: 0.914</strong>.</p>\n<p><strong>My question is</strong>: Is there any way around this? Are there any other models/approaches that take into account the affirmative/negative nature of sentences when calculating similarity?</p>\n",
    "score": 8,
    "creation_date": 1632909795,
    "view_count": 2812,
    "answer_count": 4,
    "tags": "python;nlp;spacy;huggingface-transformers;sentence-similarity"
  },
  {
    "question_id": 67427823,
    "title": "What is the difference between Transformer encoder vs Transformer decoder vs Transformer encoder-decoder?",
    "body": "<p>I know that GPT uses Transformer decoder, BERT uses Transformer encoder, and T5 uses Transformer encoder-decoder. But can someone help me understand why GPT only uses the decoder, BERT only uses encoder, and T5 uses both?</p>\n<p>What can you do with just the encoder without decoder, decoder without encoder, and both encoder and decoder?</p>\n<p>I'm new to NLP so any help would be nice :D\nThanks!</p>\n",
    "score": 8,
    "creation_date": 1620350481,
    "view_count": 2261,
    "answer_count": 2,
    "tags": "nlp;bert-language-model;generative-pretrained-transformer"
  },
  {
    "question_id": 51490620,
    "title": "Extracting names from a text file using Spacy",
    "body": "<p>I have a text file which contains lines as shown below:</p>\n<pre><code>Electronically signed : Wes Scott, M.D.; Jun 26 2010 11:10AM CST\n\nThe patient was referred by Dr. Jacob Austin.  \n\nElectronically signed by Robert Clowson, M.D.; Janury 15 2015 11:13AM CST\n\nElectronically signed by Dr. John Douglas, M.D.; Jun 16 2017 11:13AM CST\n\nThe patient was referred by\nDr. Jayden Green Olivia.  \n</code></pre>\n<p>I want to extract all names using Spacy. I am using Spacy's part of speech tagging and entity recognition but not able to get success.\nMay I please know on how it could done? Any help would be appreciable</p>\n<p>I am using some code in this way:</p>\n<pre><code>import spacy\nnlp = spacy.load('en')\ndocument_string= &quot;&quot;&quot; Electronically signed by stupid: Dr. John Douglas, M.D.; \n    Jun 13 2018 11:13AM CST&quot;&quot;&quot;\ndoc = nlp(document_string)\nfor sentence in doc.ents:\n    print(sentence, sentence.label_) \n</code></pre>\n",
    "score": 8,
    "creation_date": 1532407567,
    "view_count": 17739,
    "answer_count": 3,
    "tags": "python;nlp;nltk;spacy;named-entity-recognition"
  },
  {
    "question_id": 33473107,
    "title": "Visualize Parse Tree Structure",
    "body": "<p>I would like to display the parsing (POS tagging) from <strong>openNLP</strong> as a tree structure visualization.  Below I provide the parse tree from <strong>openNLP</strong> but I can not plot as a visual tree common to <a href=\"http://www.nltk.org/book/ch08.html#ubiquitous-ambiguity\" rel=\"nofollow noreferrer\">Python's parsing</a>. </p>\n\n<pre><code>install.packages(\n    \"http://datacube.wu.ac.at/src/contrib/openNLPmodels.en_1.5-1.tar.gz\",  \n    repos=NULL, \n    type=\"source\"\n)\n\nlibrary(NLP)\nlibrary(openNLP)\n\nx &lt;- 'Scroll bar does not work the best either.'\ns &lt;- as.String(x)\n\n## Annotators\nsent_token_annotator &lt;- Maxent_Sent_Token_Annotator()\nword_token_annotator &lt;- Maxent_Word_Token_Annotator()\nparse_annotator &lt;- Parse_Annotator()\n\na2 &lt;- annotate(s, list(sent_token_annotator, word_token_annotator))\np &lt;- parse_annotator(s, a2)\nptext &lt;- sapply(p$features, `[[`, \"parse\")\nptext\nTree_parse(ptext)\n\n## &gt; ptext\n## [1] \"(TOP (S (NP (NNP Scroll) (NN bar)) (VP (VBZ does) (RB not) (VP (VB work) (NP (DT the) (JJS best)) (ADVP (RB either))))(. .)))\"\n## &gt; Tree_parse(ptext)\n## (TOP\n##   (S\n##     (NP (NNP Scroll) (NN bar))\n##     (VP (VBZ does) (RB not) (VP (VB work) (NP (DT the) (JJS best)) (ADVP (RB either))))\n##     (. .)))\n</code></pre>\n\n<p>The tree structure should look similar to this:</p>\n\n<p><a href=\"https://i.sstatic.net/Njv6T.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Njv6T.png\" alt=\"enter image description here\"></a></p>\n\n<p>Is there a way to display this tree visualization? </p>\n\n<p>I found <a href=\"https://stackoverflow.com/a/33499140/1000343\">this related tree viz</a> question for plotting numeric expressions that may be of use but that I could not generalize to sentence parse visualization.</p>\n",
    "score": 8,
    "creation_date": 1446451542,
    "view_count": 6307,
    "answer_count": 1,
    "tags": "r;nlp;visualization;igraph;opennlp"
  },
  {
    "question_id": 25903007,
    "title": "Algorithms or data structures for dealing with ambiguity",
    "body": "<p>I'm looking for algorithms or data structures specifically for dealing with ambiguities.</p>\n\n<p>In my particular current field of interest I'm looking into ambiguous parses of natural languages, but I assume there must be many fields in computing where ambiguity plays a part.</p>\n\n<p>I can find a lot out there on trying to avoid ambiguity but very little on how to embrace ambiguity and analyse ambiguous data.</p>\n\n<p>Say a parser generates these alternative token streams or interpretations:</p>\n\n<ul>\n<li><code>A B1 C</code></li>\n<li><code>A B2 C</code></li>\n<li><code>A B3 B4 C</code></li>\n</ul>\n\n<p>It can be seen that some parts of the stream are shared between interpretations (<code>A</code> ... <code>B</code>) and other parts branch into alternative interpretations and often meet back with the main stream.</p>\n\n<p>Of course there may be many more interpretations, nesting of alternatives, and interpretations which have no main stream.</p>\n\n<p>This is obviously some kind of graph with nodes. I don't know if it has an established name.</p>\n\n<p>Are there extant algorithms or data structures I can study that are intended to deal with just this kind of ambiguous graph?</p>\n",
    "score": 8,
    "creation_date": 1411005262,
    "view_count": 2488,
    "answer_count": 3,
    "tags": "algorithm;data-structures;nlp;ambiguity;ambiguous-grammar"
  },
  {
    "question_id": 9014313,
    "title": "how to find similar sentences / phrases in R?",
    "body": "<p>Example, I have billions of short phrases, and I want to clusters of them that are similar. </p>\n\n<pre><code>&gt; strings.to.cluster &lt;- c(\"Best Toyota dealer in bay area. Drive out with a new car today\",\n                        \"Largest Selection of Furniture. Stock updated everyday\" , \n                        \" Unique selection of Handcrafted Jewelry\",\n                        \"Free Shipping for orders above $60. Offer Expires soon\",\n                        \"XXXX is where smart men buy anniversary gifts\",\n                        \"2012 Camrys on Sale. 0% APR for select customers\",\n                        \"Closing Sale on office desks. All Items must go\" \n                         )\n</code></pre>\n\n<p>assume that this vector is hundreds of thousands of rows. Is there a package in R to cluster these phrases by meaning? \nor could someone suggest a way to rank \"similar\" phrases by meaning to a given phrase. </p>\n",
    "score": 8,
    "creation_date": 1327556143,
    "view_count": 6592,
    "answer_count": 2,
    "tags": "r;statistics;nlp"
  },
  {
    "question_id": 7636959,
    "title": "Why getting different results with MALLET topic inference for single and batch of documents?",
    "body": "<p>I'm trying to perform LDA topic modeling with Mallet 2.0.7.  I can train a LDA model and get good results, judging by the output from the training session.  Also, I can use the inferencer built in that process and get similar results when re-processing my training file.  However, if I take an individual file from the larger training set, and process it with the inferencer I get very different results, which are not good.</p>\n\n<p>My understanding is that the inferencer should be using a fixed model, and only features local to that document, so I do not understand why I would get any different results while processing 1 file or the 1k from my training set.  I am not doing frequency cutoffs which would seem to be a global operation that would have this type of an effect.  You can see other parameters I'm using in the commands below, but they're mostly default. Changing # of iterations to 0 or 100 didn't help.</p>\n\n<p>Import data:</p>\n\n<pre><code>bin/mallet import-dir \\\n  --input trainingDataDir \\\n  --output train.data \\\n  --remove-stopwords TRUE \\\n  --keep-sequence TRUE \\\n  --gram-sizes 1,2 \\\n  --keep-sequence-bigrams TRUE\n</code></pre>\n\n<p>Train:</p>\n\n<pre><code>time ../bin/mallet train-topics\n  --input ../train.data \\\n  --inferencer-filename lda-inferencer-model.mallet \\\n  --num-top-words 50 \\\n  --num-topics 100 \\\n  --num-threads 3 \\\n  --num-iterations 100 \\\n  --doc-topics-threshold 0.1 \\\n  --output-topic-keys topic-keys.txt \\\n  --output-doc-topics doc-topics.txt\n</code></pre>\n\n<p>Topics assigned during training to one file in particular, #14 is about wine which is correct:</p>\n\n<pre><code>998 file:/.../29708933509685249 14  0.31684981684981683 \n&gt; grep \"^14\\t\" topic-keys.txt \n14  0.5 wine spray cooking car climate top wines place live honey sticking ice prevent collection market hole climate_change winery tasting california moldova vegas horses converted paper key weather farmers_market farmers displayed wd freezing winter trouble mexico morning spring earth round mici torrey_pines barbara kinda nonstick grass slide tree exciting lots \n</code></pre>\n\n<p>Run inference on entire train batch:</p>\n\n<pre><code>../bin/mallet infer-topics \\\n  --input ../train.data \\\n  --inferencer lda-inferencer-model.mallet \\\n  --output-doc-topics inf-train.1 \\\n  --num-iterations 100\n</code></pre>\n\n<p>Inference score on train -- very similar:</p>\n\n<pre><code>998 /.../29708933509685249 14 0.37505087505087503 \n</code></pre>\n\n<p>Run inference on another training data file comprised of only that 1 txt file:</p>\n\n<pre><code>../bin/mallet infer-topics \\\n  --input ../one.data \\\n  --inferencer lda-inferencer-model.mallet \\\n  --output-doc-topics inf-one.2 \\\n  --num-iterations 100\n</code></pre>\n\n<p>Inference on one document produces topic 80 and 36, which are very different (14 is given near 0 score):</p>\n\n<pre><code>0 /.../29708933509685249 80 0.3184778184778185 36 0.19067969067969068\n&gt; grep \"^80\\t\" topic-keys.txt \n80  0.5 tips dog care pet safety items read policy safe offer pay avoid stay important privacy services ebay selling terms person meeting warning poster message agree sellers animals public agree_terms follow pets payment fraud made privacy_policy send description puppy emailed clicking safety_tips read_safety safe_read stay_safe services_stay payment_services transaction_payment offer_transaction classifieds_offer \n</code></pre>\n",
    "score": 8,
    "creation_date": 1317654905,
    "view_count": 5139,
    "answer_count": 1,
    "tags": "nlp;machine-learning;mallet;topic-modeling"
  },
  {
    "question_id": 75725818,
    "title": "Loading Hugging face model is taking too much memory",
    "body": "<p>I am trying to load a large Hugging face model with code like below:</p>\n<pre><code>model_from_disc = AutoModelForCausalLM.from_pretrained(path_to_model)\ntokenizer_from_disc = AutoTokenizer.from_pretrained(path_to_model)\ngenerator = pipeline(&quot;text-generation&quot;, model=model_from_disc, tokenizer=tokenizer_from_disc)\n</code></pre>\n<p>The program is quickly crashing <strong>after the first line</strong> because it is running out of memory. Is there a way to chunk the model as I am loading it, so that the program doesn't crash?</p>\n<hr>\n<p><strong>EDIT</strong>\n<br>\nSee cronoik's answer for accepted solution, but here are the relevant pages on Hugging Face's documentation:</p>\n<p><strong>Sharded Checkpoints:</strong> <a href=\"https://huggingface.co/docs/transformers/big_models#sharded-checkpoints:%7E:text=in%20the%20future.-,Sharded%20checkpoints,-Since%20version%204.18.0\" rel=\"noreferrer\">https://huggingface.co/docs/transformers/big_models#sharded-checkpoints:~:text=in%20the%20future.-,Sharded%20checkpoints,-Since%20version%204.18.0</a>\n<br>\n<strong>Large Model Loading:</strong> <a href=\"https://huggingface.co/docs/transformers/main_classes/model#:%7E:text=the%20weights%20instead.-,Large%20model%20loading,-In%20Transformers%204.20.0\" rel=\"noreferrer\">https://huggingface.co/docs/transformers/main_classes/model#:~:text=the%20weights%20instead.-,Large%20model%20loading,-In%20Transformers%204.20.0</a></p>\n",
    "score": 8,
    "creation_date": 1678733217,
    "view_count": 15287,
    "answer_count": 1,
    "tags": "python;pytorch;nlp;huggingface-transformers;huggingface"
  },
  {
    "question_id": 48697595,
    "title": "What is the best way to handle missing words when using word embeddings?",
    "body": "<p>I have a set of pre-trained word2vec word vectors and a corpus. I want to use the word vectors to represent words in the corpus. The corpus has some words in it that I don't have trained word vectors for. What's the best way to handle those words for which there is no pre-trained vector?</p>\n\n<p>I've heard several suggestions. </p>\n\n<ol>\n<li><p>use a vector of zeros for every missing word</p></li>\n<li><p>use a vector of random numbers for every missing word (with a bunch of suggestions on how to bound those randoms)</p></li>\n<li><p>an idea I had: take a vector whose values are the mean of all values in that position from all pre-trained vectors</p></li>\n</ol>\n\n<p>Anyone with experience with the problem have thoughts on how to handle this?</p>\n",
    "score": 8,
    "creation_date": 1518141093,
    "view_count": 4775,
    "answer_count": 2,
    "tags": "machine-learning;nlp;deep-learning;word2vec;word-embedding"
  },
  {
    "question_id": 42479575,
    "title": "Is there a way to programmatically combine Korean unicode into one?",
    "body": "<p>Using a Korean Input Method Editor (IME), it's possible to type <code>버리</code> + <code>어</code> and it will automatically become <code>버려</code>.</p>\n<p>Is there a way to programmatically do that in Python?</p>\n<pre><code>&gt;&gt;&gt; x, y = '버리', '어'\n&gt;&gt;&gt; z = '버려'\n&gt;&gt;&gt; ord(z[-1])\n47140\n&gt;&gt;&gt; ord(x[-1]), ord(y)\n(47532, 50612)\n</code></pre>\n<p>Is there a way to compute that 47532 + 50612 -&gt; 47140?</p>\n<p>Here's some more examples:</p>\n<blockquote>\n<p>가보 + 아 -&gt; 가봐</p>\n<p>끝나 + ㄹ -&gt; 끝날</p>\n</blockquote>\n",
    "score": 8,
    "creation_date": 1488178763,
    "view_count": 2229,
    "answer_count": 2,
    "tags": "python;unicode;nlp;ime;korean-nlp"
  },
  {
    "question_id": 41881605,
    "title": "What should be the word vectors of token &lt;pad&gt;, &lt;unknown&gt;, &lt;go&gt;, &lt;EOS&gt; before sent into RNN?",
    "body": "<p>In word embedding, what should be a good vector representation for the start_tokens _PAD, _UNKNOWN, _GO, _EOS? </p>\n",
    "score": 8,
    "creation_date": 1485459624,
    "view_count": 6719,
    "answer_count": 2,
    "tags": "nlp;deep-learning;word2vec;word-embedding"
  },
  {
    "question_id": 39262183,
    "title": "How to extract Predicate and subject from a sentence using NLP Libraries?",
    "body": "<p>I want to find predicate and subject from a sentence using <code>Natural Language Processing Libraries</code>. Is this technique have any name in the world of <code>NLP</code> or Is there any way to do that? </p>\n\n<blockquote>\n  <p>Example : He likes child. Result: (He, likes child)</p>\n</blockquote>\n",
    "score": 8,
    "creation_date": 1472699919,
    "view_count": 3899,
    "answer_count": 2,
    "tags": "nlp;nltk;stanford-nlp;opennlp"
  },
  {
    "question_id": 22087407,
    "title": "Is there any best practice to prepare features for text-based classification?",
    "body": "<p>We have many feedback and issue reports from customers. And they are plain texts. We are trying to build a auto classifier for these docs so <strong>future</strong> feedback/issues could be auto routed to the correct support team. Besides the text itself, I think we should include things like customer profile, case submit region, etc into the classifier. I think this could provide more clues for classifier to make better predictions.</p>\n<p>Currently, all the features selected for training are based on the text content. How to include the above mentioned meta-features?</p>\n<h2>ADD 1</h2>\n<p>My current approach is to first do some typical pre-processing to the raw text (including title and body), such as remove the stop words, POS-tagging and extract significant words. Then I convert the title and body into a list of words and store them in some sparse format as below:</p>\n<blockquote>\n<p>instance 1:   word1:word1 count,  word2: word2 count, ....</p>\n<p>instance 2:   wordX:word1 count,  wordY: word2 count, ....</p>\n</blockquote>\n<p>And for the other non-text features, I am planning to add them as new columns after the word columns. So a final instance will look like:</p>\n<blockquote>\n<p>instance 1: word1:word1 count, ... , feature X:value, feature Y:value</p>\n</blockquote>\n",
    "score": 8,
    "creation_date": 1393567041,
    "view_count": 964,
    "answer_count": 2,
    "tags": "machine-learning;nlp;text-mining"
  },
  {
    "question_id": 21990004,
    "title": "I wish to create a system where I give a sentence and the system spits out sentences similar in meaning to the input sentence I gave",
    "body": "<p>This is an NLP problem and I was wondering how I should proceed.  </p>\n\n<p>How difficult is the problem?\nCould I replace the word with synonyms and check that the grammar is correct? </p>\n",
    "score": 8,
    "creation_date": 1393250586,
    "view_count": 2732,
    "answer_count": 3,
    "tags": "nlp;grammar;similarity;sentence;word-sense-disambiguation"
  },
  {
    "question_id": 20819262,
    "title": "Natural Language Date Parser for JS that supports arbitrary text surrounding e.g. - &quot;Follow up next week&quot;",
    "body": "<h2>PROBLEM</h2>\n\n<p>Libraries like <a href=\"http://sugarjs.com/dates\" rel=\"nofollow noreferrer\">sugar.js</a> can convert natural language date strings such as:\n<BR>\n\"next week\" but <em>cannot</em> handle strings such as: \"Blah blah blah... Follow up <strong>next week</strong>\"</p>\n\n<p>In my application, I need to process a paragraph of notes and detect action items in it.  Siri and Google Calendar are able to do this.</p>\n\n<h2>Potential Solution</h2>\n\n<p>Option 1: Maintain a list of \"Action Verbs\" for each language such as \"Follow Up\", \"Call back\", \"Remind me\" and then grab the natural language date portion after it and pipe it into Sugar.js to get a date back.</p>\n\n<p>I'm not sure if every language will work in this way though... like in all languages will there be  ? or in some languages is the sentence structure be completely different... </p>\n\n<p>Option2: I might be able to get back various supported prefixes from sugar.js locale specific grammars and by semi brute force pass in strings until I find a valid date.</p>\n\n<h2>QUESTION</h2>\n\n<p>Is there a library i've over looked that </p>\n\n<ol>\n<li>Works in <em>Javascript</em></li>\n<li>Supports multiple languages</li>\n<li>Can handle arbitrary text surrounding the date grammar.</li>\n</ol>\n\n<h2>Related Posts</h2>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/1003326/is-there-a-natural-language-parser-for-date-times-in-javascript?rq=1\">Is there a natural language parser for date/times in javascript?</a></li>\n<li>JAVA: <a href=\"http://ocpsoft.org/prettytime/\" rel=\"nofollow noreferrer\">http://ocpsoft.org/prettytime/</a> - based on description it'd probably work... text to date only english</li>\n<li>JAVA: <a href=\"http://nlp.stanford.edu/software/sutime.shtml\" rel=\"nofollow noreferrer\">http://nlp.stanford.edu/software/sutime.shtml</a> - Too complex, java based.  (<a href=\"https://stackoverflow.com/questions/13367066/date-extraction-from-text\">Date Extraction from Text</a>)\n*</li>\n</ul>\n",
    "score": 8,
    "creation_date": 1388263970,
    "view_count": 1414,
    "answer_count": 1,
    "tags": "javascript;nlp;date-parsing"
  },
  {
    "question_id": 9721173,
    "title": "Basic NLP in CoffeeScript or JavaScript -- Punkt tokenizaton, simple trained Bayes models -- where to start?",
    "body": "<p>My current web-app project calls for a little NLP:</p>\n\n<ul>\n<li>Tokenizing text into sentences, via Punkt and similar;</li>\n<li>Breaking down the longer sentences by subordinate clause (often it’s on commas except when it’s not)</li>\n<li>A Bayesian model fit for chunking paragraphs with an even feel, no orphans or widows and minimal awkward splits (maybe)</li>\n</ul>\n\n<p>... which much of that is a childishly easy task if you’ve got <a href=\"http://www.nltk.org/\" rel=\"nofollow noreferrer\">NLTK</a> — which I do, sort of: the app backend is Django on Tornado; you’d think doing these things would be a non-issue. </p>\n\n<p>However, I’ve got to interactively provide the user feedback for which the tokenizers are necessitated, so I need to do tokenize the data clientside.</p>\n\n<p>Right now I actually <em>am</em> using NLTK, via a REST API call to a Tornado process that wraps the NLTK function and little else. At the moment, things like latency and concurrency are obviously suboptimal w/r/t this ad-hoc service, to put it politely. What I should be doing, I think, is getting my hands on Coffee/Java versions of this function if not reimplementing it myself.</p>\n\n<p>And but so then from what I've seen, JavaScript hasn’t been considered cool long enough to have accumulated the not-just-web-specific, general-purpose library schmorgasbörd one can find in C or Python (or even Erlang). NLTK of course is a standout project by anyones’ measure but I only need a few percent of what it is packing.</p>\n\n<p>But so now I am at a crossroads — I have to double down on either:</p>\n\n<ul>\n<li>The “learning scientific JavaScript technique fit for reimplementing algorithms I am Facebook friends with at best” plan, or:</li>\n<li>The less interesting but more deterministically doable “settle for tokenizing over the wire, but overcompensate for the dearth of speed and programming interestingness — ensure a beachball-free UX by elevating a function call into a robustly performant paragon of web-scale service architecture, making Facebook look like Google+” option.</li>\n</ul>\n\n<p>Or something else entirely. What should I do? Like to start things off. This is my question. I’m open to solutions involving an atypical approach — as long as your recommendation is not distasteful (e.g. “use Silverlight”) and/or a time vortex (e.g. “get a computational linguistics PhD you troglodyte”) I am game. Thank you in advance.</p>\n",
    "score": 8,
    "creation_date": 1331819692,
    "view_count": 2092,
    "answer_count": 4,
    "tags": "javascript;nlp;coffeescript;user-experience;tokenize"
  },
  {
    "question_id": 7803561,
    "title": "How to parse a list of words according to a simplified grammar?",
    "body": "<p>Just to clarify, this isn't homework. I've been asked for help on this and am unable to do it, so it turned into a personal quest to solve it.</p>\n\n<p>Imagine you have a grammar for an English sentence like this:</p>\n\n<pre><code>S =&gt; NP VP | VP\nNP =&gt; N | Det N | Det Adj N\nVB =&gt; V | V NP\nN =&gt; i you bus cake bear\nV =&gt; hug love destroy am\nDet =&gt; a the\nAdj =&gt; pink stylish\n</code></pre>\n\n<p>I've searched for several hours and really am out of ideas. \nI found articles talking about shallow parsing, depth-first backtracking and related things, and while I'm familiar with most of them, I still can't apply them to this problem. I tagged Lisp and Haskell because those are the languages I plan to implement this in, but I don't mind if you use other languages in your replies.</p>\n\n<p>I'd appreciate hints, good articles and everything in general.</p>\n",
    "score": 8,
    "creation_date": 1318921328,
    "view_count": 972,
    "answer_count": 5,
    "tags": "algorithm;haskell;lisp;nlp"
  },
  {
    "question_id": 3162450,
    "title": "Unstructured Text to Structured Data",
    "body": "<p>I am looking for references (tutorials, books, academic literature) concerning structuring unstructured text in a manner similar to the google calendar quick add button.</p>\n\n<p>I understand this may come under the NLP category, but I am interested only in the process of going from something like \"Levi jeans size 32 A0b293\"</p>\n\n<p>to: Brand: Levi, Size: 32, Category: Jeans, code: A0b293</p>\n\n<p>I imagine it would be some combination of lexical parsing and machine learning techniques.</p>\n\n<p>I am rather language agnostic but if pushed would prefer python, Matlab or C++ references</p>\n\n<p>Thanks</p>\n",
    "score": 8,
    "creation_date": 1278028134,
    "view_count": 7809,
    "answer_count": 4,
    "tags": "python;nlp;structured-data"
  },
  {
    "question_id": 54323427,
    "title": "How to fill in the blank using bidirectional RNN and pytorch?",
    "body": "<p>I am trying to fill in the blank using a bidirectional RNN and pytorch. </p>\n\n<p>The input will be like: <code>The dog is _____, but we are happy he is okay.</code></p>\n\n<p>The output will be like: </p>\n\n<pre><code>1. hyper (Perplexity score here) \n2. sad (Perplexity score here) \n3. scared (Perplexity score here)\n</code></pre>\n\n<p>I discovered this idea here: <a href=\"https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27\" rel=\"noreferrer\">https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27</a></p>\n\n<pre><code>import torch, torch.nn as nn\nfrom torch.autograd import Variable\n\ntext = ['BOS', 'How', 'are', 'you', 'EOS']\nseq_len = len(text)\nbatch_size = 1\nembedding_size = 1\nhidden_size = 1\noutput_size = 1\n\nrandom_input = Variable(\n    torch.FloatTensor(seq_len, batch_size, embedding_size).normal_(), requires_grad=False)\n\nbi_rnn = torch.nn.RNN(\n    input_size=embedding_size, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n\nbi_output, bi_hidden = bi_rnn(random_input)\n\n# stagger\nforward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\nstaggered_output = torch.cat((forward_output, backward_output), dim=-1)\n\nlinear = nn.Linear(hidden_size * 2, output_size)\n\n# only predict on words\nlabels = random_input[1:-1]\n\n# for language models, use cross-entropy :)\nloss = nn.MSELoss()\noutput = loss(linear(staggered_output), labels)\n</code></pre>\n\n<p>I am trying to reimplement the code above found at the bottom of the blog post. I am new to pytorch and nlp, and can't understand what the input and output to the code is.</p>\n\n<p>Question about the input: I am guessing the input are the few words that are given. Why does one need beginning of sentence and end of sentence tags in this case? Why don't I see the input being a corpus on which the model is trained like other classic NLP problems? I would like to use the Enron email corpus to train the RNN.</p>\n\n<p>Question about the output: I see the output is a tensor. My understanding is the tensor is a vector, so maybe a word vector in this case. How can you use the tensor to output the words themselves? </p>\n",
    "score": 8,
    "creation_date": 1548234041,
    "view_count": 2381,
    "answer_count": 1,
    "tags": "python;nlp;pytorch"
  },
  {
    "question_id": 51426107,
    "title": "How to build a gensim dictionary that includes bigrams?",
    "body": "<p>I'm trying to build a Tf-Idf model that can score bigrams as well as unigrams using <a href=\"https://radimrehurek.com/gensim/index.html\" rel=\"noreferrer\">gensim</a>. To do this, I build a gensim dictionary and then use that dictionary to create bag-of-word representations of the corpus that I use to build the model. </p>\n\n<p>The step to build the dictionary looks like this:</p>\n\n<pre><code>dict = gensim.corpora.Dictionary(tokens)\n</code></pre>\n\n<p>where <code>token</code> is a list of unigrams and bigrams like this:</p>\n\n<pre><code>[('restore',),\n ('diversification',),\n ('made',),\n ('transport',),\n ('The',),\n ('grass',),\n ('But',),\n ('distinguished', 'newspaper'),\n ('came', 'well'),\n ('produced',),\n ('car',),\n ('decided',),\n ('sudden', 'movement'),\n ('looking', 'glasses'),\n ('shapes', 'replaced'),\n ('beauties',),\n ('put',),\n ('college', 'days'),\n ('January',),\n ('sometimes', 'gives')]\n</code></pre>\n\n<p>However, when I provide a list such as this to <code>gensim.corpora.Dictionary()</code>, the algorithm reduces all tokens to bigrams, e.g.:</p>\n\n<pre><code>test = gensim.corpora.Dictionary([(('happy', 'dog'))])\n[test[id] for id in test]\n=&gt; ['dog', 'happy']\n</code></pre>\n\n<p>Is there a way to generate a dictionary with gensim that includes bigrams? </p>\n",
    "score": 8,
    "creation_date": 1532012875,
    "view_count": 7809,
    "answer_count": 2,
    "tags": "python;nlp;gensim"
  },
  {
    "question_id": 46084574,
    "title": "What is the difference between mteval-v13a.pl and NLTK BLEU?",
    "body": "<p>There is an implementation of BLEU score in Python NLTK,\n <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py\" rel=\"noreferrer\"><code>nltk.translate.bleu_score.corpus_bleu</code></a> </p>\n\n<p>But I am not sure if it is the same as the <a href=\"https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\" rel=\"noreferrer\">mtevalv13a.pl script</a>.</p>\n\n<p><strong>What is the difference between them?</strong></p>\n",
    "score": 8,
    "creation_date": 1504733175,
    "view_count": 1644,
    "answer_count": 1,
    "tags": "machine-learning;nlp;nltk;machine-translation;bleu"
  },
  {
    "question_id": 45696028,
    "title": "SnowballStemmer for Russian words list",
    "body": "<p>I do know how to perform SnowballStemmer on a single word (in my case, on russian one). Doing the next things:</p>\n\n<pre><code>from nltk.stem.snowball import SnowballStemmer \n\nstemmer = SnowballStemmer(\"russian\") \nstemmer.stem(\"Василий\")\n'Васил'\n</code></pre>\n\n<p>How can I do the following if I have a list of words like ['Василий', 'Геннадий', 'Виталий']?</p>\n\n<p>My approach using for loop seems to be not working :( </p>\n\n<pre><code>l=[stemmer.stem(word) for word in l]\n</code></pre>\n",
    "score": 8,
    "creation_date": 1502810563,
    "view_count": 17548,
    "answer_count": 1,
    "tags": "python;nlp;nltk;stemming;snowball"
  },
  {
    "question_id": 42001875,
    "title": "Named entity recognition (NER) features",
    "body": "<p>I'm new to Named Entity Recognition and I'm having some trouble understanding what/how features are used for this task. </p>\n\n<p>Some papers I've read so far mention features used, but don't really explain them, for example in \n<a href=\"https://arxiv.org/pdf/cs/0306050.pdf\" rel=\"noreferrer\">Introduction to the CoNLL-2003 Shared Task:Language-Independent Named Entity Recognition</a>, the following features are mentioned: </p>\n\n<blockquote>\n  <p>Main features used by the the sixteen systems that participated in the\n  CoNLL-2003 shared task sorted by performance on the English test data.\n  Aff: affix information (n-grams); bag: bag of words; cas: global case\n  information; chu: chunk tags; doc: global document information; gaz:\n  gazetteers; lex: lexical features; ort: orthographic information; pat:\n  orthographic patterns (like Aa0); pos: part-of-speech tags; pre:\n  previously predicted NE tags; quo: flag signing that the word is\n  between quotes; tri: trigger words.</p>\n</blockquote>\n\n<p>I'm a bit confused by some of these, however. For example:</p>\n\n<ul>\n<li>isn't bag of words supposed to be a method to generate features (one for each word)? How can BOW itself be <em>a feature</em>? Or does this simply mean we have a feature for each word as in BOW, besides all the other features mentioned?</li>\n<li>how can a gazetteer be a feature?</li>\n<li>how can POS tags exactly be used as features ? Don't we have a POS tag for each word? Isn't each object/instance a \"text\"?</li>\n<li>what is global document information?</li>\n<li>what is the feature trigger words?</li>\n</ul>\n\n<p>I think all I need here is to just to look at an example table with each of these features as columns and see their values to understand how they really work, but so far I've failed to find an easy to read dataset. </p>\n\n<p>Could someone please clarify or point me to some explanation or example of these features being used?</p>\n",
    "score": 8,
    "creation_date": 1486037289,
    "view_count": 7044,
    "answer_count": 3,
    "tags": "machine-learning;nlp;classification;feature-selection;named-entity-recognition"
  },
  {
    "question_id": 41865465,
    "title": "How can I prevent spacy&#39;s tokenizer from splitting a specific substring when tokenizing a string?",
    "body": "<p>How can I prevent spacy's tokenizer from splitting a specific substring when tokenizing a string?</p>\n\n<p>More specifically, I have this sentence:</p>\n\n<blockquote>\n  <p>Once unregistered, the folder went away from the shell.</p>\n</blockquote>\n\n<p>which gets tokenized as [Once/unregistered/,/the/folder/went/away/from/the/<strong>she/ll</strong>/.] by scapy 1.6.0. I don't want the substring <code>shell</code> to be cut into two different tokens <code>she</code> and <code>ll</code>.</p>\n\n<hr>\n\n<p>Here is the code I use:</p>\n\n<pre><code># To install spacy:\n# sudo pip install spacy\n# sudo python -m spacy.en.download parser # will take 0.5 GB\n\nimport spacy\nnlp = spacy.load('en')\n\n# https://spacy.io/docs/usage/processing-text\ndocument = nlp(u'Once unregistered, the folder went away from the shell.')\n\nfor token in document:\n    print('token.i: {2}\\ttoken.idx: {0}\\ttoken.pos: {3:10}token.text: {1}'.\n      format(token.idx, token.text,token.i,token.pos_))\n</code></pre>\n\n<p>which outputs:</p>\n\n<pre><code>token.i: 0      token.idx: 0    token.pos: ADV       token.text: Once\ntoken.i: 1      token.idx: 5    token.pos: ADJ       token.text: unregistered\ntoken.i: 2      token.idx: 17   token.pos: PUNCT     token.text: ,\ntoken.i: 3      token.idx: 19   token.pos: DET       token.text: the\ntoken.i: 4      token.idx: 23   token.pos: NOUN      token.text: folder\ntoken.i: 5      token.idx: 30   token.pos: VERB      token.text: went\ntoken.i: 6      token.idx: 35   token.pos: ADV       token.text: away\ntoken.i: 7      token.idx: 40   token.pos: ADP       token.text: from\ntoken.i: 8      token.idx: 45   token.pos: DET       token.text: the\ntoken.i: 9      token.idx: 49   token.pos: PRON      token.text: she\ntoken.i: 10     token.idx: 52   token.pos: VERB      token.text: ll\ntoken.i: 11     token.idx: 54   token.pos: PUNCT     token.text: .\n</code></pre>\n",
    "score": 8,
    "creation_date": 1485401199,
    "view_count": 4584,
    "answer_count": 1,
    "tags": "python;nlp;tokenize;spacy"
  },
  {
    "question_id": 25109001,
    "title": "Phrase extraction algorithm for statistical machine translation",
    "body": "<p>I have written the following code with the phrase extraction algorithm for SMT.</p>\n\n<p><a href=\"https://github.com/alvations/nltk/blob/develop/nltk/align/phrase_based.py\" rel=\"noreferrer\">GitHub</a></p>\n\n<pre><code># -*- coding: utf-8 -*-\n\ndef phrase_extraction(srctext, trgtext, alignment):\n    \"\"\"\n    Phrase extraction algorithm. \n    \"\"\"\n    def extract(f_start, f_end, e_start, e_end):\n        phrases = set()\n        # return { } if f end == 0\n        if f_end == 0:\n            return\n        # for all (e,f) ∈ A do\n        for e,f in alignment:\n            # return { } if e &lt; e start or e &gt; e end\n            if e &lt; e_start or e &gt; e_end:        \n                return\n\n        fs = f_start\n        # repeat-\n        while True:\n            fe = f_end\n            # repeat-\n            while True:\n                # add phrase pair ( e start .. e end , f s .. f e ) to set E\n                trg_phrase = \" \".join(trgtext[i] for i in range(fs,fe))\n                src_phrase = \" \".join(srctext[i] for i in range(e_start,e_end))\n                phrases.add(\"\\t\".join([src_phrase, trg_phrase]))\n                fe+=1 # fe++\n                # -until fe aligned\n                if fe in f_aligned or fe &gt; trglen:\n                    break\n            fs-=1 # fe--\n            # -until fs aligned\n            if fs in f_aligned or fs &lt; 0:\n                break\n        return phrases\n\n    # Calculate no. of tokens in source and target texts.\n    srctext = srctext.split()\n    trgtext = trgtext.split()\n    srclen = len(srctext)\n    trglen = len(trgtext)\n    # Keeps an index of which source/target words are aligned.\n    e_aligned = [i for i,_ in alignment]\n    f_aligned = [j for _,j in alignment] \n\n    bp = set() # set of phrase pairs BP\n    # for e start = 1 ... length(e) do\n    for e_start in range(srclen):\n        # for e end = e start ... length(e) do       \n        for e_end in range(e_start, srclen):\n            # // find the minimally matching foreign phrase\n            # (f start , f end ) = ( length(f), 0 )\n            f_start, f_end = trglen, 0\n            # for all (e,f) ∈ A do\n            for e,f in alignment:\n                # if e start ≤ e ≤ e end then\n                if e_start &lt;= e &lt;= e_end:\n                    f_start = min(f, f_start)\n                    f_end = max(f, f_end)\n            # add extract (f start , f end , e start , e end ) to set BP\n            phrases = extract(f_start, f_end, e_start, e_end)\n            if phrases:\n                bp.update(phrases)\n    return bp\n\nsrctext = \"michael assumes that he will stay in the house\"\ntrgtext = \"michael geht davon aus , dass er im haus bleibt\"\nalignment = [(0,0), (1,1), (1,2), (1,3), (2,5), (3,6), (4,9), (5,9), (6,7), (7,7), (8,8)]\n\nphrases = phrase_extraction(srctext, trgtext, alignment)\n\nfor i in phrases:\n    print i\n</code></pre>\n\n<p>The phrase extraction algorithm from Philip Koehn's <em>Statistical Machine Translation</em> book, page 133 is as such:</p>\n\n<p><img src=\"https://i.sstatic.net/0j556.png\" alt=\"enter image description here\"></p>\n\n<p>And the desired output should be:</p>\n\n<p><img src=\"https://i.sstatic.net/qfR2m.png\" alt=\"enter image description here\"></p>\n\n<p>However with my code, I am only able to get these output:</p>\n\n<blockquote>\n  <p>michael assumes that he will stay in the - michael geht davon aus ,\n  dass er im haus</p>\n  \n  <p>michael assumes that he will stay in the - michael geht davon aus ,\n  dass er im haus bleibt</p>\n</blockquote>\n\n<p><strong>Does anyone spot what is wrong with my implementation?</strong></p>\n\n<p>The code does extract phrases but it's not the complete desired output as shown with the translation table above:</p>\n\n<p><img src=\"https://i.sstatic.net/E21Xq.png\" alt=\"enter image description here\"></p>\n",
    "score": 8,
    "creation_date": 1407098692,
    "view_count": 3702,
    "answer_count": 1,
    "tags": "python;algorithm;machine-learning;nlp;machine-translation"
  },
  {
    "question_id": 15727144,
    "title": "Increase performance of Stanford-tagger based program",
    "body": "<p>I just implemented a program that uses the Stanford POS tagger in Java.</p>\n\n<p>I used an input file of a few KB in size, consisting of a few hundred words. I even set the heap size to 600 MB.</p>\n\n<p>But it is still slow and sometimes runs out of heap memory. How can I increase its execution speed and memory performance? I would like to be able to use a few MB as input.</p>\n\n<pre><code>  public static void postag(String args) throws ClassNotFoundException\n\n  {\n\n     try\n\n     {\n\n     File filein=new File(\"c://input.txt\");\n\n     String content = FileUtils.readFileToString(filein);\n\n     MaxentTagger tagger = new MaxentTagger(\"postagging/wsj-0-18-bidirectional-distsim.tagger\");\n\n     String tagged = tagger.tagString(content);\n\n        try \n        {\n            File file = new File(\"c://output.txt\");\n            if (!file.exists()) \n            {\n                file.createNewFile();\n            } \n\n            FileWriter fw = new FileWriter(file.getAbsoluteFile());\n            BufferedWriter bw = new BufferedWriter(fw);\n            bw.write(\"\\n\"+tagged);\n            bw.close();\n\n            }\n              catch (IOException e) \n              {\n                    e.printStackTrace();\n               }\n\n     } catch (IOException e1)\n     {\n         e1.printStackTrace();\n     }\n\n }\n</code></pre>\n",
    "score": 8,
    "creation_date": 1364712821,
    "view_count": 694,
    "answer_count": 1,
    "tags": "java;nlp;pos-tagger;stanford-nlp"
  },
  {
    "question_id": 11005529,
    "title": "General synonym and part of speech processing using nltk",
    "body": "<p>I'm trying to create a general synonym identifier for the words in a sentence which are significant (i.e. not \"a\" or \"the\"), and I am using the natural language toolkit(nltk) in python for it. The problem I am having is that the synonym finder in nltk requires a part of speech argument in order to be linked to its synonyms. My attempted fix for this was to use the simplified part of speech tagger present in nltk, and then reduce the first letter in order to pass this argument into the synonym finder, however this is not working.</p>\n\n<pre><code>def synonyms(Sentence):\n    Keywords = []\n    Equivalence = WordNetLemmatizer()\n    Stemmer = stem.SnowballStemmer('english')\n    for word in Sentence:\n        word = Equivalence.lemmatize(word)\n    words = nltk.word_tokenize(Sentence.lower())\n    text = nltk.Text(words)\n    tags = nltk.pos_tag(text)\n    simplified_tags = [(word, simplify_wsj_tag(tag)) for word, tag in tags]\n    for tag in simplified_tags:\n        print tag\n        grammar_letter = tag[1][0].lower()\n        if grammar_letter != 'd':\n            Call = tag[0].strip() + \".\" + grammar_letter.strip() + \".01\"\n            print Call\n            Word_Set = wordnet.synset(Call)\n            paths = Word_Set.lemma_names\n            for path in paths:\n                Keywords.append(Stemmer.stem(path))\n    return Keywords\n</code></pre>\n\n<p>This is the code I am currently working from, and as you can see I am first lemmatizing the input to reduce the number of matches I will have in the long run (I plan on running this on tens of thousands of sentences), and in theory I would be stemming the word after this to further this effect and reduce the number of redundant words I generate, however this method almost invariably returns errors in the form of the one below: </p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:\\Python27\\test.py\", line 45, in &lt;module&gt;\n    synonyms('spray reddish attack force')\n  File \"C:\\Python27\\test.py\", line 39, in synonyms\n    Word_Set = wordnet.synset(Call)\n  File \"C:\\Python27\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\", line 1016, in synset\n    raise WordNetError(message % (lemma, pos))\nWordNetError: no lemma 'reddish' with part of speech 'n'\n</code></pre>\n\n<p>I don't have much control over the data this will be running over, and so simply cleaning my corpus is not really an option. Any ideas on how to solve this one?</p>\n\n<p>I did some more research and I have a promising lead, but I'm still not sure how I could implement it. In the case of a not found, or incorrectly assigned word I would like to use a similarity metric(Leacock Chodorow, Wu-Palmer etc.) to link the word to the closest correctly categorized other keyword. Perhaps in conjunction with an edit distance measure, but again I haven't been able to find any kind of documentation on this.</p>\n",
    "score": 8,
    "creation_date": 1339538462,
    "view_count": 4220,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;nltk;wordnet"
  },
  {
    "question_id": 10518329,
    "title": "How to use custom classifiers in ensemble classifiers in sklearn?",
    "body": "<p>I read that the builtin ensemble methods in sklearn use decision trees as the base classifiers. Is it possible to use custom classifiers instead?</p>\n",
    "score": 8,
    "creation_date": 1336574347,
    "view_count": 2343,
    "answer_count": 3,
    "tags": "machine-learning;nlp;scikits;scikit-learn"
  },
  {
    "question_id": 4600612,
    "title": "Extracting noun+noun or (adj|noun)+noun from Text",
    "body": "<p>Is it possible to extract <code>noun+noun</code> or <code>(adj|noun)+noun</code> using the R package <code>openNLP</code>? That is, I would like to use linguistic filtering to extract candidate noun phrases. Could you direct me how to do?\nMany thanks.</p>\n<hr />\n<p>Thanks for the responses.\nhere is the code:</p>\n<pre><code>library(&quot;openNLP&quot;)\n\nacq &lt;- &quot;Gulf Applied Technologies Inc said it sold its subsidiaries engaged in\n        pipeline and terminal operations for 12.2 mln dlrs. The company said \n        the sale is subject to certain post closing adjustments, \n        which it did not explain. Reuter.&quot; \n\nacqTag &lt;- tagPOS(acq)    \nacqTagSplit = strsplit(acqTag,&quot; &quot;)\nacqTagSplit\n\nqq = 0\ntag = 0\n\nfor (i in 1:length(acqTagSplit[[1]])){\n    qq[i] &lt;-strsplit(acqTagSplit[[1]][i],'/')\n    tag[i] = qq[i][[1]][2]\n}\n\nindex = 0\n\nk = 0\n\nfor (i in 1:(length(acqTagSplit[[1]])-1)) {\n    \n    if ((tag[i] == &quot;NN&quot; &amp;&amp; tag[i+1] == &quot;NN&quot;) | \n        (tag[i] == &quot;NNS&quot; &amp;&amp; tag[i+1] == &quot;NNS&quot;) | \n        (tag[i] == &quot;NNS&quot; &amp;&amp; tag[i+1] == &quot;NN&quot;) | \n        (tag[i] == &quot;NN&quot; &amp;&amp; tag[i+1] == &quot;NNS&quot;) | \n        (tag[i] == &quot;JJ&quot; &amp;&amp; tag[i+1] == &quot;NN&quot;) | \n        (tag[i] == &quot;JJ&quot; &amp;&amp; tag[i+1] == &quot;NNS&quot;))\n    {      \n            k = k +1\n            index[k] = i\n    }\n\n}\n\nindex\n</code></pre>\n<hr />\n<p>Reader can refer <strong>index</strong> on <strong>acqTagSplit</strong> to do <code>noun+noun</code> or <code>(adj|noun)+noun</code> extraction. (The code is not optimal, but it works. If you have any idea, please let me know.)</p>\n<p>I have an additional problem:</p>\n<p>Justeson and Katz (1995) proposed another linguistic filtering to extract candidate noun phrases:</p>\n<pre class=\"lang-none prettyprint-override\"><code>((Adj|Noun)+|((Adj|Noun)*(Noun-Prep)?)(Adj|Noun)*)Noun\n</code></pre>\n<p>I cannot understand its meaning well. Could you do me a favor and explain it? Or show how to code the filtering rule in the R language?\nMany thanks.</p>\n",
    "score": 8,
    "creation_date": 1294198455,
    "view_count": 5615,
    "answer_count": 2,
    "tags": "r;nlp;opennlp;pos-tagger"
  },
  {
    "question_id": 3703905,
    "title": "Regexp for Tokenizing English Text",
    "body": "<p>What would be the best regular expression for tokenizing an English text?</p>\n\n<p>By an English token, I mean an atom consisting of maximum number of characters that can be meaningfully used for NLP purposes. An analogy is a \"token\" in any programming language (e.g. in C, '{', '[', 'hello', '&amp;', etc. can be tokens). There is one restriction: Though English punctuation characters can be \"meaningful\", let's ignore them for the sake of simplicity when they do not appear in the middle of \\w+. So, \"Hello, world.\" yields 'hello' and 'world'; similarly, \"You are good-looking.\" may yield either [you, are, good-looking] or [you, are, good, looking].</p>\n",
    "score": 8,
    "creation_date": 1284407816,
    "view_count": 4473,
    "answer_count": 4,
    "tags": "regex;text;nlp"
  },
  {
    "question_id": 1490061,
    "title": "Classifying Text Based on Groups of Keywords?",
    "body": "<p>I have a list of requirements for a software project, assembled from the remains of its predecessor. Each requirement should map to one or more categories. Each of the categories consists of a group of keywords. What I'm trying to do is find an algorithm that would give me a score ranking which of the categories each requirement is likely to fall into. The results would be use as a starting point to further categorize the requirements.</p>\n\n<p>As an example, suppose I have the requirement:</p>\n\n<blockquote>\n  <p>The system shall apply deposits to a customer's specified account.</p>\n</blockquote>\n\n<p>And categories/keywords:</p>\n\n<ol>\n<li>Customer Transactions: deposits, deposit, customer, account, accounts</li>\n<li>Balance Accounts: account, accounts, debits, credits</li>\n<li>Other Category: foo, bar</li>\n</ol>\n\n<p>I would want the algorithm to score the requirement highest in category 1, lower in category 2, and not at all in category 3. The scoring mechanism is mostly irrelevant to me, but needs to convey how much more likely category 1 applies than category 2.</p>\n\n<p>I'm new to NLP, so I'm kind of at a loss. I've been reading <em>Natural Language Processing in Python</em> and was hoping to apply some of the concepts, but haven't seen anything that quite fits. I don't think a simple frequency distribution would work, since the text I'm processing is so small (a single sentence.)</p>\n",
    "score": 8,
    "creation_date": 1254185654,
    "view_count": 6186,
    "answer_count": 3,
    "tags": "algorithm;nlp;text-processing"
  },
  {
    "question_id": 54938815,
    "title": "Data Preprocessing for NLP Pre-training Models (e.g. ELMo, Bert)",
    "body": "<p>I plan to train ELMo or Bert model from scratch based on data(notes typed by people) on hand. The data I have now is all typed by different people. There are problems with spelling, formatting, and inconsistencies in sentences. After read the ELMo and Bert papers, I know that both models use a lot of sentences like from Wikipedia. I haven't been able to find any processed training samples or any preprocessing tutorial for Emlo or Bert model. My question is:</p>\n\n<ul>\n<li>Does the Bert and ELMo models have standard data preprocessing steps or standard processed data formats?</li>\n<li>Based on my existing dirty data, is there any way to preprocess this data so that the resulting word representation is more accurate?</li>\n</ul>\n",
    "score": 8,
    "creation_date": 1551420204,
    "view_count": 10668,
    "answer_count": 1,
    "tags": "machine-learning;pre-trained-model;transfer-learning;nlp"
  },
  {
    "question_id": 49542787,
    "title": "How can I get the noun clause that is the object of a certain verb?",
    "body": "<p>I am working with data from pharmaceutical labels. The text is always structured using the verb phrase 'indicated for'.</p>\n\n<p>For example:</p>\n\n<pre><code>sentence = \"Meloxicam tablet is indicated for relief of the signs and symptoms of osteoarthritis and rheumatoid arthritis\"\n</code></pre>\n\n<p>I have already used SpaCy to filter down to only sentences that contain the phrase 'indicated for'. </p>\n\n<p>I now need a function that will take in the sentence, and return the phrase that is the object of 'indicated for'. So for this example, the function, which I have called <code>extract()</code>, would operate like this:</p>\n\n<pre><code>extract(sentence)\n&gt;&gt; 'relief of the signs and symptoms of osteoarthritis and rheumatoid arthritis'\n</code></pre>\n\n<p>Is there functionality to do this using spacy?</p>\n\n<p>EDIT:\nSimply splitting after 'indicated for' won't work for complicated examples.</p>\n\n<p>Here's some examples:</p>\n\n<p>'''buprenorphine and naloxone sublingual tablets are indicated for the <strong>maintenance treatment of opioid dependence</strong> and should be used as part of a complete treatment plan to include counseling and psychosocial support buprenorphine and naloxone sublingual tablets contain buprenorphine a partial opioid agonist and naloxone an opioid antagonist and is indicated for the <strong>maintenance treatment of opioid dependence</strong>'''</p>\n\n<p>'''ofloxacin ophthalmic solution is indicated for <strong>the treatment of infections caused by susceptible strains of the following bacteria</strong> in the conditions listed below conjunctivitis gram positive bacteria gram negative bacteria staphylococcus aureus staphylococcus epidermidis streptococcus pneumoniae enterobacter cloacae haemophilus influenzae proteus mirabilis pseudomonas aeruginosa corneal ulcers gram positive bacteria gram negative bacteria staphylococcus aureus staphylococcus epidermidis streptococcus pneumoniae pseudomonas aeruginosa serratia marcescens'''</p>\n\n<p>where I just want the bold parts.</p>\n",
    "score": 8,
    "creation_date": 1522265768,
    "view_count": 2277,
    "answer_count": 4,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 48064378,
    "title": "How does Pyspark Calculate Doc2Vec from word2vec word embeddings?",
    "body": "<p>I have a pyspark dataframe with a corpus of ~300k unique rows each with a \"doc\" that contains a few sentences of text in each.</p>\n\n<p>After processing, I have a 200 dimension vectorized representation of each row/doc. My NLP Process: </p>\n\n<ol>\n<li>Remove Punctuation with regex udf </li>\n<li>Word Stemming with nltk snowball udf)</li>\n<li>Pyspark Tokenizer</li>\n<li>Word2Vec (ml.feature.Word2Vec, vectorSize=200, windowSize=5)</li>\n</ol>\n\n<p>I understand how this implementation uses the skipgram model to create embeddings for each word based on the full corpus used. My question is: <strong>How does this implementation go from a vector for each word in the corpus to a vector for each document/row?</strong></p>\n\n<p>Is it the same processes as in the gensim doc2vec implementation where it simply concatenates the word vectors in each doc together?: <a href=\"https://stackoverflow.com/questions/40413866/how-does-gensim-calculate-doc2vec-paragraph-vectors\">How does gensim calculate doc2vec paragraph vectors</a>. If so, how does it cut the vector down to the specified size of 200 (Does it use just the first 200 words? Average?)? </p>\n\n<p>I was unable to find the information from the sourcecode: <a href=\"https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/ml/feature.html#Word2Vec\" rel=\"noreferrer\">https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/ml/feature.html#Word2Vec</a> </p>\n\n<p>Any help or reference material to look at is super appreciated!</p>\n",
    "score": 8,
    "creation_date": 1514910045,
    "view_count": 6174,
    "answer_count": 2,
    "tags": "apache-spark;nlp;pyspark;word2vec;doc2vec"
  },
  {
    "question_id": 43286346,
    "title": "Phone number and Date of Birth from human speech",
    "body": "<p>Is there an effective Natural Language Processor that can fetch the phone number and date of birth from human speech. Each user has a different way of specifying the phone number and date of birth. Hence, converting speech to text and then parsing the text for phone number is not helpful.</p>\n",
    "score": 8,
    "creation_date": 1491595747,
    "view_count": 1524,
    "answer_count": 3,
    "tags": "nlp;speech-recognition;speech-to-text"
  },
  {
    "question_id": 41958115,
    "title": "Max over time pooling in Keras",
    "body": "<p>I'm using CNNs in <code>Keras</code> for an NLP task and instead of max pooling, I'm trying to achieve max over time pooling.</p>\n\n<p>Any ideas/hacks on how to achieve this?</p>\n\n<p>What I mean by max over time pooling is to pool the highest value, no matter where they are in the vector</p>\n",
    "score": 8,
    "creation_date": 1485867139,
    "view_count": 2588,
    "answer_count": 1,
    "tags": "machine-learning;neural-network;nlp;keras;conv-neural-network"
  },
  {
    "question_id": 34439208,
    "title": "nltk StanfordNERTagger : How to get proper nouns without capitalization",
    "body": "<p>I am trying to use the StanfordNERTagger and nltk to extract keywords from a piece of text. </p>\n\n<pre><code>docText=\"John Donk works for POI. Brian Jones wants to meet with Xyz Corp. for measuring POI's Short Term performance Metrics.\"\n\nwords = re.split(\"\\W+\",docText) \n\nstops = set(stopwords.words(\"english\"))\n\n    #remove stop words from the list\nwords = [w for w in words if w not in stops and len(w) &gt; 2]\n\nstr = \" \".join(words)\nprint str\nstn = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \nstp = StanfordPOSTagger('english-bidirectional-distsim.tagger') \nstanfordPosTagList=[word for word,pos in stp.tag(str.split()) if pos == 'NNP']\n\nprint \"Stanford POS Tagged\"\nprint stanfordPosTagList\ntagged = stn.tag(stanfordPosTagList)\nprint tagged\n</code></pre>\n\n<p>this gives me </p>\n\n<pre><code>John Donk works POI Brian Jones wants meet Xyz Corp measuring POI Short Term performance Metrics\nStanford POS Tagged\n[u'John', u'Donk', u'POI', u'Brian', u'Jones', u'Xyz', u'Corp', u'POI', u'Short', u'Term']\n[(u'John', u'PERSON'), (u'Donk', u'PERSON'), (u'POI', u'ORGANIZATION'), (u'Brian', u'ORGANIZATION'), (u'Jones', u'ORGANIZATION'), (u'Xyz', u'ORGANIZATION'), (u'Corp', u'ORGANIZATION'), (u'POI', u'O'), (u'Short', u'O'), (u'Term', u'O')]\n</code></pre>\n\n<p>so clearly, things like <code>Short</code> and <code>Term</code> were tagged as <code>NNP</code>. The data that i have contains many such instances where <strong>non <code>NNP</code> words are capitalized</strong>. This might be due to typos or maybe they are headers. I dont have much control over that. </p>\n\n<p>How can i parse or clean up the data so that i can detect a non <code>NNP</code> term even though it may be capitalized? <strong>I dont want terms like <code>Short</code> and <code>Term</code> to be categorized as <code>NNP</code></strong></p>\n\n<p>Also, not sure why <code>John Donk</code> was captured as a person but <code>Brian Jones</code> was not. Could it be due to the other capitalized non <code>NNP</code>s in my data? Could that be having an effect on how the <code>StanfordNERTagger</code> treats everything else?</p>\n\n<p><strong>Update, one possible solution</strong></p>\n\n<p>Here is what i plan to do</p>\n\n<ol>\n<li>Take each word and convert to lower case</li>\n<li>Tag the lowercase word</li>\n<li>If the tag is <code>NNP</code> then we know that the original word must also be an <code>NNP</code></li>\n<li>If not, then  the original word was mis-capitalized</li>\n</ol>\n\n<p>Here is what i tried to do</p>\n\n<pre><code>str = \" \".join(words)\nprint str\nstp = StanfordPOSTagger('english-bidirectional-distsim.tagger') \nfor word in str.split():\n    wl = word.lower()\n    print wl\n    w,pos = stp.tag(wl)\n    print pos\n    if pos==\"NNP\":\n        print \"Got NNP\"\n        print w\n</code></pre>\n\n<p>but this gives me error</p>\n\n<pre><code>John Donk works POI Jones wants meet Xyz Corp measuring POI short term performance metrics\njohn\nTraceback (most recent call last):\n  File \"X:\\crp.py\", line 37, in &lt;module&gt;\n    w,pos = stp.tag(wl)\nValueError: too many values to unpack\n</code></pre>\n\n<p>i have tried multiple approaches but some error always shows up. <strong>How can i Tag a single word?</strong></p>\n\n<p>I dont want to convert the whole string to lower case and then Tag. If i do that, the <code>StanfordPOSTagger</code> returns an empty string</p>\n",
    "score": 8,
    "creation_date": 1450885622,
    "view_count": 7000,
    "answer_count": 2,
    "tags": "python;nlp;nltk;stanford-nlp;pos-tagger"
  },
  {
    "question_id": 33282094,
    "title": "Differences between lexical features and orthographic features in NLP?",
    "body": "<p>Features are used for model training and testing. What are the differences between lexical features and orthographic features in Natural Language Processing? Examples preferred. </p>\n",
    "score": 8,
    "creation_date": 1445520182,
    "view_count": 10801,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 22452713,
    "title": "search similar meaning phrases with nltk",
    "body": "<p>I have a bunch of unrelated paragraphs, and I need to traverse them to find similar occurrences such as that, given a search where I look for <code>object</code> <code>falls</code>, I find a boolean <code>True</code> for text containing:</p>\n\n<ul>\n<li>Box fell from shelf</li>\n<li>Bulb shattered on the ground</li>\n<li>A piece of plaster fell from the ceiling</li>\n</ul>\n\n<p>And <code>False</code> for:</p>\n\n<ul>\n<li>The blame fell on Sarah</li>\n<li>The temperature fell abruptly</li>\n</ul>\n\n<p>I am able to use <strong>nltk</strong> to <code>tokenise</code>, <code>tag</code> and get <strong>Wordnet</strong> <code>synsets</code>, but I am finding it hard to figure out how to fit <strong>nltk</strong>'s moving parts together to achieve the desired result. Should I <code>chunk</code> before looking for synsets? Should I write a <code>context-free grammar</code>? Is there a best practice when translating from <strong>treebank</strong> tags to Wordnet grammar tags? None of this is explained in the <a href=\"http://www.nltk.org/book/\" rel=\"noreferrer\">nltk book</a>, and I couldn't find it on the <a href=\"http://www.packtpub.com/python-text-processing-nltk-20-cookbook/book\" rel=\"noreferrer\">nltk cookbook</a> yet.</p>\n\n<p>Bonus points for answers that include <a href=\"http://pandas.pydata.org\" rel=\"noreferrer\">pandas</a> in the answer.</p>\n\n<hr>\n\n<p><strong>[ EDIT ]:</strong> </p>\n\n<p><strong>Some code to get things started</strong></p>\n\n<pre><code>In [1]:\n\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\nfrom pandas import Series\n\ndef tag(x):\n    return pos_tag(word_tokenize(x))\n\nphrases = ['Box fell from shelf',\n           'Bulb shattered on the ground',\n           'A piece of plaster fell from the ceiling',\n           'The blame fell on Sarah',\n           'Berlin fell on May',\n           'The temperature fell abruptly']\n\nser = Series(phrases)\nser.map(tag)\n\nOut[1]:\n\n0    [(Box, NNP), (fell, VBD), (from, IN), (shelf, ...\n1    [(Bulb, NNP), (shattered, VBD), (on, IN), (the...\n2    [(A, DT), (piece, NN), (of, IN), (plaster, NN)...\n3    [(The, DT), (blame, NN), (fell, VBD), (on, IN)...\n4    [(Berlin, NNP), (fell, VBD), (on, IN), (May, N...\n5    [(The, DT), (temperature, NN), (fell, VBD), (a...\ndtype: object\n</code></pre>\n",
    "score": 8,
    "creation_date": 1395054403,
    "view_count": 4658,
    "answer_count": 2,
    "tags": "python;search;nlp;nltk"
  },
  {
    "question_id": 11293149,
    "title": "NLTK named entity recognition in dutch",
    "body": "<p>I am trying to extract named entities from dutch text. I used <a href=\"https://github.com/japerk/nltk-trainer/\" rel=\"noreferrer\">nltk-trainer</a> to train a tagger and a chunker on the conll2002 dutch corpus. However, the parse method from the chunker is not detecting any named entities. Here is my code:</p>\n\n<pre><code>str = 'Christiane heeft een lam.'\n\ntagger = nltk.data.load('taggers/dutch.pickle')\nchunker = nltk.data.load('chunkers/dutch.pickle')\n\nstr_tags = tagger.tag(nltk.word_tokenize(str))\nprint str_tags\n\nstr_chunks = chunker.parse(str_tags)\nprint str_chunks\n</code></pre>\n\n<p>And the output of this program:</p>\n\n<pre><code>[('Christiane', u'N'), ('heeft', u'V'), ('een', u'Art'), ('lam', u'Adj'), ('.', u'Punc')]\n(S Christiane/N heeft/V een/Art lam/Adj ./Punc)\n</code></pre>\n\n<p>I was expecting Christiane to be detected as a named entity.\nAny help?</p>\n",
    "score": 8,
    "creation_date": 1341230050,
    "view_count": 5982,
    "answer_count": 1,
    "tags": "python;nlp;nltk;named-entity-recognition"
  },
  {
    "question_id": 7248372,
    "title": "NLP software for classification of large datasets",
    "body": "<p>For years I've been using my own Bayesian-like methods to categorize new items from external sources based on a large and continually updated training dataset.</p>\n<p>There are three types of categorization done for each item:</p>\n<ol>\n<li>30 categories, where each item must belong to one category, and at most two categories.</li>\n<li>10 other categories, where each item is only associated with a category if there is a strong match, and each item can belong to as many categories as match.</li>\n<li>4 other categories, where each item must belong to only one category, and if there isn't a strong match the item is assigned to a default category.</li>\n</ol>\n<p>Each item consists of English text of around 2,000 characters.  In my training dataset there are about 265,000 items, which contain a rough estimate of 10,000,000 features (unique three word phrases).</p>\n<p>My homebrew methods have been fairly successful, but definitely have room for improvement.  I've read the NLTK book's chapter &quot;Learning to Classify Text&quot;, which was great and gave me a good overview of NLP classification techniques.  I'd like to be able to experiment with different methods and parameters until I get the best classification results possible for my data.</p>\n<h2>The Question</h2>\n<p>What off-the-shelf NLP tools are available that can efficiently classify such a large dataset?</p>\n<p>Those I've tried so far:</p>\n<ul>\n<li>NLTK</li>\n<li>TIMBL</li>\n</ul>\n<p>I tried to train them with a dataset that consisted of less than 1% of the available training data: 1,700 items, 375,000 features.  For NLTK I used a sparse binary format, and a similarly compact format for TIMBL.</p>\n<p>Both seemed to rely on doing everything in memory, and quickly consumed all system memory.  I can get them to work with tiny datasets, but nothing large.  I suspect that if I tried incrementally adding the training data the same problem would occur either then or when doing the actual classification.</p>\n<p>I've looked at Google's Prediction API, which seem to do much of what I'm looking for but not everything.  I'd also like to avoid relying on an external service if possible.</p>\n<p>About the choice of features: in testing with my homebrew methods over the years, three word phrases produced by far the best results.  Although I could reduce the number of features by using words or two word phrases, that would most likely produce inferior results and would still be a large number of features.</p>\n",
    "score": 8,
    "creation_date": 1314730810,
    "view_count": 2382,
    "answer_count": 4,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 6482152,
    "title": "Extracting &#39;useful&#39; information out of sentences?",
    "body": "<p>I am currently trying to understand sentences of this form: </p>\n\n<p><code>The problem was more with the set-top box than the television. Restarting the set-top box solved the problem.</code></p>\n\n<p>I am totally new to Natural Language Processing and started using Python's NLTK package to get my hands dirty. However, I am wondering if someone could give me an overview of the high-level steps involved in achieving this.</p>\n\n<p>What I am trying to do is to identify what the problem was so in this case, <code>set-top box</code> and whether the action that was taken resolved the problem so in this case, <code>yes</code> because restarting fixed the problem. So if all the sentences were of this form, my life would have been easier but because it is natural language, the sentences could also be of the following form:</p>\n\n<p><code>I took a look at the car and found nothing wrong with it. However, I suspect there is something wrong with the engine</code></p>\n\n<p>So in this case, the problem was with the <code>car</code>. The action taken did not resolve the problem because of the presence of the word <code>suspect</code>. And the potential problem could be with the <code>engine</code>.</p>\n\n<p>I am not looking for an absolute answer as I suspect this is very complex. What I am looking for is more rather a high-level overview that will point me in the right direction. If there is an easier/alternate way to do this, that is welcome as well.</p>\n",
    "score": 8,
    "creation_date": 1309062830,
    "view_count": 2456,
    "answer_count": 2,
    "tags": "language-agnostic;nlp;machine-learning;nltk"
  },
  {
    "question_id": 5264492,
    "title": "How to calculate tag-wise precision and recall for POS tagger?",
    "body": "<p>I am using some rule-based and statistical POS taggers to tag a corpus(of around <strong>5000 sentences</strong>) with Parts of Speech(POS). Following is a snippet of my test corpus where each word is seperated by its respective POS tag by '/'.</p>\n\n<pre><code>No/RB ,/, it/PRP was/VBD n't/RB Black/NNP Monday/NNP ./.\nBut/CC while/IN the/DT New/NNP York/NNP Stock/NNP Exchange/NNP did/VBD n't/RB fall/VB apart/RB Friday/NNP as/IN the/DT Dow/NNP Jones/NNP Industrial/NNP Average/NNP plunged/VBD 190.58/CD points/NNS --/: most/JJS of/IN it/PRP in/IN the/DT final/JJ hour/NN --/: it/PRP barely/RB managed/VBD *-2/-NONE- to/TO stay/VB this/DT side/NN of/IN chaos/NN ./.\nSome/DT ``/`` circuit/NN breakers/NNS ''/'' installed/VBN */-NONE- after/IN the/DT October/NNP 1987/CD crash/NN failed/VBD their/PRP$ first/JJ test/NN ,/, traders/NNS say/VBP 0/-NONE- *T*-1/-NONE- ,/, *-2/-NONE- unable/JJ *-3/-NONE- to/TO cool/VB the/DT selling/NN panic/NN in/IN both/DT stocks/NNS and/CC futures/NNS ./.\n</code></pre>\n\n<p>After tagging the corpus, it looks like this:</p>\n\n<pre><code>No/DT ,/, it/PRP was/VBD n't/RB Black/NNP Monday/NNP ./. \nBut/CC while/IN the/DT New/NNP York/NNP Stock/NNP Exchange/NNP did/VBD n't/RB fall/VB apart/RB Friday/VB as/IN the/DT Dow/NNP Jones/NNP Industrial/NNP Average/JJ plunged/VBN 190.58/CD points/NNS --/: most/RBS of/IN it/PRP in/IN the/DT final/JJ hour/NN --/: it/PRP barely/RB managed/VBD *-2/-NONE- to/TO stay/VB this/DT side/NN of/IN chaos/NNS ./. \nSome/DT ``/`` circuit/NN breakers/NNS ''/'' installed/VBN */-NONE- after/IN the/DT October/NNP 1987/CD crash/NN failed/VBD their/PRP$ first/JJ test/NN ,/, traders/NNS say/VB 0/-NONE- *T*-1/-NONE- ,/, *-2/-NONE- unable/JJ *-3/-NONE- to/TO cool/VB the/DT selling/VBG panic/NN in/IN both/DT stocks/NNS and/CC futures/NNS ./. \n</code></pre>\n\n<p>I need to calculate the tagging accuracy(<strong>Tag wise- Recall &amp; Precision</strong>), therefore need to find an error(if any) in tagging for each word-tag pair. </p>\n\n<p>The approach I am thinking of is to loop through these 2 text files and store them in a list and later compare the 'two' lists element by element. </p>\n\n<p>The approach seems really crude to me, so would like you guys to suggest some better solution to the above problem.</p>\n\n<p>From the <a href=\"http://en.wikipedia.org/wiki/Precision_and_recall\" rel=\"noreferrer\">wikipedia</a> page:</p>\n\n<blockquote>\n  <p>In a classification task, the\n  <strong>precision</strong> for a class is the number of\n  true positives (i.e. the number of\n  items correctly labeled as belonging\n  to the positive class) divided by the\n  total number of elements labeled as\n  belonging to the positive class (i.e.\n  the sum of true positives and false\n  positives, which are items incorrectly\n  labeled as belonging to the class).\n  <strong>Recall</strong> in this context is defined as\n  the number of true positives divided\n  by the total number of elements that\n  actually belong to the positive class\n  (i.e. the sum of true positives and\n  false negatives, which are items which\n  were not labeled as belonging to the\n  positive class but should have been).</p>\n</blockquote>\n",
    "score": 8,
    "creation_date": 1299783843,
    "view_count": 5111,
    "answer_count": 1,
    "tags": "python;shell;nlp;machine-learning;text-processing"
  },
  {
    "question_id": 77068488,
    "title": "How to Efficiently Convert a Markdown Table to a DataFrame in Python?",
    "body": "<p>I need to convert a markdown table into a pandas DataFrame. I've managed to do this using the <code>pd.read_csv</code> function with '|' as the separator, but it seems like there's some additional cleanup required. Specifically, I need to remove the row containing '-----', which is used for table separation, and I also want to get rid of the last column.</p>\n<p>Here's a simplified example of what I'm doing:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom io import StringIO\n\n# The text containing the table\ntext = &quot;&quot;&quot;\n| Some Title | Some Description             | Some Number |\n|------------|------------------------------|-------------|\n| Dark Souls | This is a fun game           | 5           |\n| Bloodborne | This one is even better      | 2           |\n| Sekiro     | This one is also pretty good | 110101      |\n&quot;&quot;&quot;\n\n# Use StringIO to create a file-like object from the text\ntext_file = StringIO(text)\n\n# Read the table using pandas read_csv with '|' as the separator\ndf = pd.read_csv(text_file, sep='|', skipinitialspace=True)\n\n# Remove leading/trailing whitespace from column names\ndf.columns = df.columns.str.strip()\n\n# Remove the index column\ndf = df.iloc[:, 1:]\n</code></pre>\n<p>Is there a more elegant and efficient way to convert a markdown table into a DataFrame without needing to perform these additional cleanup steps? I'd appreciate any suggestions or insights on improving this process.</p>\n",
    "score": 8,
    "creation_date": 1694190084,
    "view_count": 5758,
    "answer_count": 2,
    "tags": "python;nlp;markdown"
  },
  {
    "question_id": 59313461,
    "title": "Removing named entities from a document using spacy",
    "body": "<p>I have tried to remove words from a document that are considered to be named entities by spacy, so basically removing \"Sweden\" and \"Nokia\" from the string example. I could not find a way to work around the problem that entities are stored as a span. So when comparing them with single tokens from a spacy doc, it prompts an error.</p>\n\n<p>In a later step, this process is supposed to be a function applied to several text documents stored in a pandas data frame.</p>\n\n<p>I would appreciate any kind of help and advice on how to maybe better post questions as this is my first one here.</p>\n\n<pre><code>\nnlp = spacy.load('en')\n\ntext_data = u'This is a text document that speaks about entities like Sweden and Nokia'\n\ndocument = nlp(text_data)\n\ntext_no_namedentities = []\n\nfor word in document:\n    if word not in document.ents:\n        text_no_namedentities.append(word)\n\nreturn \" \".join(text_no_namedentities)\n\n</code></pre>\n\n<p>It creates the following error:</p>\n\n<blockquote>\n  <p>TypeError: Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got spacy.tokens.span.Span)</p>\n</blockquote>\n",
    "score": 8,
    "creation_date": 1576188862,
    "view_count": 10059,
    "answer_count": 4,
    "tags": "python;text;nlp;spacy"
  },
  {
    "question_id": 49087762,
    "title": "Parsing Index page in a PDF text book with Python",
    "body": "<p>I have to extract text from PDF pages as it is with the indentation into a CSV file.</p>\n\n<p>Index page from PDF text book:\n<img src=\"https://i.sstatic.net/WNdtb.jpg\" alt=\"\"></p>\n\n<p>I should split the text into class and subclass type hierarchy along with the page numbers. For example in the image,\n<strong><em>Application server</em></strong> is the class and <strong><em>Apache Tomcat</em></strong> is the subclass in the page number <strong>275</strong></p>\n\n<p>This is the  expected output of the CSV:\n<img src=\"https://i.sstatic.net/3HGnL.jpg\" alt=\"\"></p>\n\n<p>I have used Tika parser to parse the PDF, but the indentation is not maintained properly (not unique) in the parsed content for splitting the text into class and subclasses.</p>\n\n<p>This is how the parsed text looks like:</p>\n\n<p><img src=\"https://i.sstatic.net/tsNV9.jpg\" alt=\"\"></p>\n\n<p>Can anyone suggest me the right approach for this requirement?</p>\n",
    "score": 8,
    "creation_date": 1520102119,
    "view_count": 4178,
    "answer_count": 2,
    "tags": "python;pdfminer;pdftotext;named-entity-recognition;nlp"
  },
  {
    "question_id": 42698919,
    "title": "Classify words with the same meaning",
    "body": "<p>I have 50.000 subject lines from emails and i want to classify the words in them based on synonyms or words that can be used instead of others.</p>\n\n<p>For example:</p>\n\n<p>Top sales!</p>\n\n<p>Best sales</p>\n\n<p>I want them to be in the same group.</p>\n\n<p>I build the following function with nltk's wordnet but it doesn't work well.</p>\n\n<pre><code>def synonyms(w,group,guide):\n    try:\n         # Check if the words is similar\n        w1 = wordnet.synset(w +'.'+guide+'.01')\n        w2 = wordnet.synset(group +'.'+guide+'.01')\n\n        if w1.wup_similarity(w2)&gt;=0.7:\n             return True\n\n        elif w1.wup_similarity(w2)&lt;0.7:\n            return False\n\n    except:\n         return False\n</code></pre>\n\n<p>Any ideas or tools to accomplish this?</p>\n",
    "score": 8,
    "creation_date": 1489072464,
    "view_count": 5156,
    "answer_count": 3,
    "tags": "python-3.x;machine-learning;nlp;nltk;text-processing"
  },
  {
    "question_id": 39258476,
    "title": "Collocations with spaCy",
    "body": "<p>I've been using NLTK for finding collocations, or n-grams, and have recently discovered the spaCy module for NLP. I've only just begun familiarizing myself with it and have, thus far, seen little mention for supported collocation functions.</p>\n\n<p>Can spaCy be used to find collocations directly?</p>\n\n<p>I have read through the <a href=\"https://spacy.io/docs/\" rel=\"noreferrer\">documentation</a>, but haven't seen mention.</p>\n",
    "score": 8,
    "creation_date": 1472675356,
    "view_count": 3412,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 29603056,
    "title": "Using StanfordParser to get typed dependencies from a parsed sentence",
    "body": "<p>Using NLTK's StanfordParser, I can parse a sentence like this:</p>\n\n<pre><code>os.environ['STANFORD_PARSER'] = 'C:\\jars' \nos.environ['STANFORD_MODELS'] = 'C:\\jars'  \nos.environ['JAVAHOME'] ='C:\\ProgramData\\Oracle\\Java\\javapath' \nparser = stanford.StanfordParser(model_path=\"C:\\jars\\englishPCFG.ser.gz\")\nsentences = parser.parse((\"bring me a red ball\",)) \nfor sentence in sentences:\n    sentence    \n</code></pre>\n\n<p>The result is:</p>\n\n<pre><code>Tree('ROOT', [Tree('S', [Tree('VP', [Tree('VB', ['Bring']),\nTree('NP', [Tree('DT', ['a']), Tree('NN', ['red'])]), Tree('NP',\n[Tree('NN', ['ball'])])]), Tree('.', ['.'])])])\n</code></pre>\n\n<p>How can I use the Stanford parser to get typed dependencies in addition to the above graph? Something like:</p>\n\n<ol>\n<li>root(ROOT-0, bring-1) </li>\n<li>iobj(bring-1, me-2)</li>\n<li>det(ball-5, a-3) </li>\n<li>amod(ball-5, red-4) </li>\n<li>dobj(bring-1, ball-5)</li>\n</ol>\n",
    "score": 8,
    "creation_date": 1428920663,
    "view_count": 2064,
    "answer_count": 1,
    "tags": "python;parsing;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 17931053,
    "title": "Conduit: Multiple Stream Consumers",
    "body": "<p>I write a program which counts the frequencies of NGrams in a corpus. I already have a function that consumes a stream of tokens and produces NGrams of one single order:</p>\n\n<pre><code>ngram :: Monad m =&gt; Int -&gt; Conduit t m [t]\ntrigrams = ngram 3\ncountFreq :: (Ord t, Monad m) =&gt; Consumer [t] m (Map [t] Int)\n</code></pre>\n\n<p>At the moment i just can connect one stream consumer to a stream source:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>tokens --- trigrams --- countFreq\n</code></pre>\n\n<p>How do I connect multiple stream consumers to the same stream source?\nI would like to have something like this:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>           .--- unigrams --- countFreq\n           |--- bigrams  --- countFreq\ntokens ----|--- trigrams --- countFreq\n           '--- ...      --- countFreq\n</code></pre>\n\n<p>A plus would be to run each consumer in parallel</p>\n\n<p><strong>EDIT:</strong>\nThanks to Petr I came up with this solution</p>\n\n<pre><code>spawnMultiple orders = do\n    chan &lt;- atomically newBroadcastTMChan\n\n    results &lt;- forM orders $ \\_ -&gt; newEmptyMVar\n    threads &lt;- forM (zip results orders) $\n                        forkIO . uncurry (sink chan)\n\n    forkIO . runResourceT $ sourceFile \"test.txt\"\n                         $$ javascriptTokenizer\n                         =$ sinkTMChan chan\n\n    forM results readMVar\n\n    where\n        sink chan result n = do\n            chan' &lt;- atomically $ dupTMChan chan\n            freqs &lt;- runResourceT $ sourceTMChan chan'\n                                 $$ ngram n\n                                 =$ frequencies\n            putMVar result freqs\n</code></pre>\n",
    "score": 8,
    "creation_date": 1375120989,
    "view_count": 705,
    "answer_count": 1,
    "tags": "haskell;nlp;conduit"
  },
  {
    "question_id": 16523067,
    "title": "How to use Stanford parser",
    "body": "<p>I downloaded the Stanford parser 2.0.5 and use Demo2.java source code that is in the package, but After I compile and run the program it has many errors. \nA part of my program is:</p>\n\n<pre><code>public class testStanfordParser {\n/** Usage: ParserDemo2 [[grammar] textFile] */\n  public static void main(String[] args) throws IOException {\n    String grammar = args.length &gt; 0 ? args[0] : \"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\";\n    String[] options = { \"-maxLength\", \"80\", \"-retainTmpSubcategories\" };\n    LexicalizedParser lp = LexicalizedParser.loadModel(grammar, options);\n    TreebankLanguagePack tlp = new PennTreebankLanguagePack();\n    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();\n ...\n</code></pre>\n\n<p>the errors are:</p>\n\n<pre><code>Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz java.io.IOException: Unable to resolve edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\" as either class path, filename or URL\nat edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:408)\nat edu.stanford.nlp.io.IOUtils.readStreamFromString(IOUtils.java:356)\nat edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile(LexicalizedParser.java:594)\nat edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:389)\nat edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:157)\nat edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:143)\nat testStanfordParser.main(testStanfordParser.java:19).                                             Loading parser from text file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz Exception in thread \"main\" java.lang.NoSuchMethodError: edu.stanford.nlp.io.IOUtils.readerFromString(Ljava/lang/String;)Ljava/io/BufferedReader;\nat edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTextFile(LexicalizedParser.java:528)\nat edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:391)\nat edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:157)\nat edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:143)\nat testStanfordParser.main(testStanfordParser.java:19)\n</code></pre>\n\n<p>please help me to solve it.\nThanks</p>\n",
    "score": 8,
    "creation_date": 1368451075,
    "view_count": 11512,
    "answer_count": 2,
    "tags": "java;eclipse;parsing;nlp;stanford-nlp"
  },
  {
    "question_id": 12097565,
    "title": "Context Free Grammar (CFG) Parser in Go",
    "body": "<p>I am looking for a Go library providing CFG parsing (preferably not in Chomsky Normal Form). Has anybody heard of anything, or should I write it ? :)</p>\n",
    "score": 8,
    "creation_date": 1345744991,
    "view_count": 3686,
    "answer_count": 2,
    "tags": "parsing;nlp;go"
  },
  {
    "question_id": 3463338,
    "title": "Can I use NLTK to determine if a comment is a positive one or a negative one?",
    "body": "<p>Can you show me a simple example using <a href=\"http://www.nltk.org/code\" rel=\"noreferrer\">http://www.nltk.org/code</a> to determine if a string about a happy or upset mood?</p>\n",
    "score": 8,
    "creation_date": 1281565515,
    "view_count": 5804,
    "answer_count": 4,
    "tags": "nlp;nltk"
  },
  {
    "question_id": 2207168,
    "title": "Elegant command-parsing in an OOP-based text game",
    "body": "<p>I'm playing with writing a MUD/text adventure (please don't laugh) in Ruby.  Can anyone give me any pointers towards   an elegant, oop-based solution to parsing input text?  </p>\n\n<p>We're talking about nothing more complex than \"put wand on table\", here.  But everything needs to be soft; I want to extend the command set painlessly, later.  </p>\n\n<p>My current thoughts, slightly simplified:  </p>\n\n<ol>\n<li><p>Each item class (box, table, room, player) knows how to recognise a command that  'belongs' to it.</p></li>\n<li><p>The game class understands a sort of a domain-specific language involving actions such as \"move object X inside object Y\", \"show description of object X\", etc.  </p></li>\n<li><p>The game class asks each item in the room if it recognises the input command.  First to say yes wins.  </p></li>\n<li><p>It then passes control to a method in the item class that handles the command.  This method rephrases the command in the DSL, passes it back to the game object to make it happen.</p></li>\n</ol>\n\n<p>There must be well-worn, elegant ways of doing this stuff.  Can't seem to google anything, though.</p>\n",
    "score": 8,
    "creation_date": 1265373438,
    "view_count": 2289,
    "answer_count": 5,
    "tags": "ruby;language-agnostic;oop;nlp"
  },
  {
    "question_id": 63822152,
    "title": "PyTorch RNN is more efficient with `batch_first=False`?",
    "body": "<p>In machine translation, we always need to slice out the first timestep (the SOS token) in the annotation and prediction.</p>\n<p>When using <code>batch_first=False</code>, slicing out the first timestep still keeps the tensor contiguous.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nbatch_size = 128\nseq_len = 12\nembedding = 50\n\n# Making a dummy output that is `batch_first=False`\nbatch_not_first = torch.randn((seq_len,batch_size,embedding))\nbatch_not_first = batch_first[1:].view(-1, embedding) # slicing out the first time step\n</code></pre>\n<p>However, if we use <code>batch_first=True</code>, after slicing, the tensor is no longer contiguous. We need to make it contiguous before we can do different operations such as <code>view</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>batch_first = torch.randn((batch_size,seq_len,embedding))\nbatch_first[:,1:].view(-1, embedding) # slicing out the first time step\n\noutput&gt;&gt;&gt;\n&quot;&quot;&quot;\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-8-a9bd590a1679&gt; in &lt;module&gt;\n----&gt; 1 batch_first[:,1:].view(-1, embedding) # slicing out the first time step\n\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n&quot;&quot;&quot;\n</code></pre>\n<p>Does that mean <code>batch_first=False</code> is better, at least, in the context of machine translation? Since it saves us from doing the <code>contiguous()</code> step. Is there any cases that <code>batch_first=True</code> works better?</p>\n",
    "score": 8,
    "creation_date": 1599706446,
    "view_count": 6007,
    "answer_count": 1,
    "tags": "python;nlp;pytorch"
  },
  {
    "question_id": 59327637,
    "title": "How do I train gpt 2 from scratch?",
    "body": "<p>I want to train gpt 2 from scratch but there is only fine-tuning approach based on pretrained models in articles I found.\nI've used this <a href=\"https://github.com/nshepperd/gpt-2\" rel=\"noreferrer\">https://github.com/nshepperd/gpt-2</a> for train with existing model. Should I edit these Python scripts to train from scratch?</p>\n",
    "score": 8,
    "creation_date": 1576259866,
    "view_count": 4503,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;nlg"
  },
  {
    "question_id": 58516766,
    "title": "Sentence split using spacy sentenizer",
    "body": "<p>I am using spaCy's sentencizer to split the sentences. </p>\n\n<pre><code>from spacy.lang.en import English\nnlp = English()\nsbd = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sbd)\n\ntext=\"Please read the analysis. (You'll be amazed.)\"\ndoc = nlp(text)\n\nsents_list = []\nfor sent in doc.sents:\n   sents_list.append(sent.text)\n\nprint(sents_list)\nprint([token.text for token in doc])\n</code></pre>\n\n<p>OUTPUT</p>\n\n<pre><code>['Please read the analysis. (', \n\"You'll be amazed.)\"]\n\n['Please', 'read', 'the', 'analysis', '.', '(', 'You', \"'ll\", 'be', \n'amazed', '.', ')']\n</code></pre>\n\n<p>Tokenization is done correctly but I am not sure it's not splitting the 2nd sentence along with ( and taking this as an end in the first sentence.</p>\n",
    "score": 8,
    "creation_date": 1571812396,
    "view_count": 20208,
    "answer_count": 2,
    "tags": "python-3.x;nlp;spacy"
  },
  {
    "question_id": 58212589,
    "title": "How to check if a sentence is a question with spacy?",
    "body": "<p>I am using spacy library to build a chat bot. How do I check if a document is a question with a certain confidence? I know how to do relevance, but not sure how to filter statements from questions.</p>\n\n<p>I am looking for something like below:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>spacy.load('en_core_web_lg')('Is this a question?').is_question\n</code></pre>\n",
    "score": 8,
    "creation_date": 1570078880,
    "view_count": 3465,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 42706207,
    "title": "Train NER model in NLTK with custom corpus",
    "body": "<p>I have an annotated corpus in the conll2002 format, namely a tab separated file with a token, pos-tag, and IOB tag followed by entity tag. Example:</p>\n\n<blockquote>\n  <p>John NNP B-PERSON</p>\n</blockquote>\n\n<p>I want to train a <strong>portuguese</strong> NER model in NLTK, preferably the MaxEnt model. I do <strong>not</strong> want to use the \"built-in\" Stanford NER in NLTK since I was already able to use the stand-alone Stanford NER. I want to use the MaxEnt model to use as comparison to the Stanford NER.</p>\n\n<p>I found <a href=\"http://nltk-trainer.readthedocs.io/en/latest/train_chunker.html\" rel=\"noreferrer\">NLTK-trainer</a> but I wasn't able to use it.</p>\n\n<p>How can I achieve this?</p>\n",
    "score": 8,
    "creation_date": 1489096556,
    "view_count": 3530,
    "answer_count": 1,
    "tags": "python;nlp;nltk;named-entity-recognition"
  },
  {
    "question_id": 41801762,
    "title": "Paragraph Segmentation using Machine Learning",
    "body": "<p>I have a large repository of documents in PDF format. The documents come from different sources, and have no one single style. I use <a href=\"https://tika.apache.org/\" rel=\"noreferrer\">Tika</a> to extract the text from the documents, and now I'd like to segment the text into paragraphs.</p>\n\n<p>I can't use regexes, because the documents have no single style:</p>\n\n<ul>\n<li>The number of <code>\\nl</code> between paragraphs vary between 2 and 4.</li>\n<li>In some documents the lines within a single paragraph are separated by 2 <code>\\nl</code>, some with single <code>\\nl</code>.</li>\n</ul>\n\n<p>So I turn to machine learning. In the (great) Python NLTK book there's an excellent use of classification for <a href=\"http://www.nltk.org/book/ch06.html#sentence-segmentation\" rel=\"noreferrer\">segmentation of sentences</a> using attributes like characters before and after a '.' with a Bayesian network, but no paragraph segmentation.</p>\n\n<p>So my questions are:</p>\n\n<ul>\n<li>Is there another way for paragraph segmentation?</li>\n<li>If I go with machine learning, is there tagged data of segmented paragraphs I can use for training?</li>\n</ul>\n",
    "score": 8,
    "creation_date": 1485159384,
    "view_count": 3663,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;apache-tika;text-segmentation"
  },
  {
    "question_id": 35564360,
    "title": "How can I use the MIST library to de-identify a text?",
    "body": "<p>I wonder how I use can the <a href=\"http://mist-deid.sourceforge.net/\" rel=\"noreferrer\">MIST library</a> to de-identify a text, e.g., transforming</p>\n\n<pre><code>Patient ID: P89474\n\nMary Phillips is a 45-year-old woman with a history of diabetes.\nShe arrived at New Hope Medical Center on August 5 complaining\nof abdominal pain. Dr. Gertrude Philippoussis diagnosed her\nwith appendicitis and admitted her at 10 PM.\n</code></pre>\n\n<p>to</p>\n\n<pre><code>Patient ID: [ID]\n\n[NAME] is a [AGE]-year-old woman with a history of diabetes.\nShe arrived at [HOSPITAL] on [DATE] complaining\nof abdominal pain. Dr. [PHYSICIAN] diagnosed her\nwith appendicitis and admitted her at 10 PM.\n</code></pre>\n\n<p>I've wandered through the documentation but no luck so far.</p>\n",
    "score": 8,
    "creation_date": 1456176916,
    "view_count": 1694,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 30081982,
    "title": "Get noun from verb Wordnet",
    "body": "<p>I'm trying to get the noun from a verb with Wordnet in python.\nI want to be able to get :<br>\nfrom the verb 'created' the noun 'creator',</p>\n\n<pre><code>'funded' =&gt; 'funder'\nVerb X =&gt; Noun Y\n</code></pre>\n\n<p><code>Y</code> is referring to a person</p>\n\n<p>I've been able to do it the other side : <code>Noun Y =&gt; Verb X</code></p>\n\n<pre><code>import nltk as nltk\nfrom nltk.corpus import wordnet as wn\n\nlem = wn.lemmas('creation')\nprint lem\n\nrelated_forms = lem[0].derivationally_related_forms()\nprint related_forms\n</code></pre>\n\n<p>Here is the output given</p>\n\n<pre><code>[Lemma('creation.n.01.creation'), Lemma('creation.n.02.creation'), Lemma('creation.n.03.creation'), Lemma('initiation.n.02.creation'), Lemma('creation.n.05.Creation'), Lemma('universe.n.01.creation')]\n\n[Lemma('create.v.02.create'), Lemma('produce.v.02.create'), Lemma('create.v.03.create')]\n</code></pre>\n\n<p>But, I'm trying to do the opposite.\nHere is a link that looks like what I want to do, but the code is not working and doesn't answer my request:<br>\n<a href=\"https://stackoverflow.com/questions/14489309/convert-words-between-verb-noun-adjective-forms?rq=1\">Convert words between verb/noun/adjective forms</a></p>\n",
    "score": 8,
    "creation_date": 1430928354,
    "view_count": 3157,
    "answer_count": 1,
    "tags": "python;nlp;wordnet"
  },
  {
    "question_id": 23586591,
    "title": "Good way to add terms to python pattern singularize",
    "body": "<p>I am using python pattern to get the singular form of English nouns. </p>\n\n<pre><code>    In [1]: from pattern.en import singularize\n    In [2]: singularize('patterns')\n    Out[2]: 'pattern'\n    In [3]: singularize('gases')\n    Out[3]: 'gase'\n</code></pre>\n\n<p>I am solving the problem in the second example by defining</p>\n\n<pre><code>    def my_singularize(strn):\n        '''\n        Return the singular of a noun. Add special cases to correct pattern generic rules.\n        '''\n        exceptionDict = {'gases':'gas','spectra':'spectrum','cross':'cross','nuclei':'nucleus'}\n        try:\n            return exceptionDict[strn]\n        except:\n            return singularize(strn)\n</code></pre>\n\n<p>Is there a better way to do this, e.g. add to the rules of pattern, or make the <code>exceptionDict</code> somehow internal to pattern?</p>\n",
    "score": 8,
    "creation_date": 1399757874,
    "view_count": 5626,
    "answer_count": 1,
    "tags": "python;nlp"
  },
  {
    "question_id": 21871374,
    "title": "Is there any Part-Of-Speech tagger in C#?",
    "body": "<p>My data pre-processing for data clustering needs <strong>part of speech (POS)</strong> tagging. I am wondering if there's some library in C# ready for this.</p>\n",
    "score": 8,
    "creation_date": 1392785619,
    "view_count": 4409,
    "answer_count": 1,
    "tags": "c#;nlp;text-mining;part-of-speech"
  },
  {
    "question_id": 11870210,
    "title": "TF-IDF Simple Use - NLTK/Scikit Learn",
    "body": "<p>Okay so I am a little confused. This should be a simple straightforward question however. </p>\n\n<p>After calculating the TF-IDF Matrix of the Document against the entire corpus, I get a result very similar to this:</p>\n\n<pre><code>array([[ 0.85...,  0.  ...,  0.52...],\n       [ 1.  ...,  0.  ...,  0.  ...],\n       [ 1.  ...,  0.  ...,  0.  ...],\n       [ 1.  ...,  0.  ...,  0.  ...],\n       [ 0.55...,  0.83...,  0.  ...],\n       [ 0.63...,  0.  ...,  0.77...]])\n</code></pre>\n\n<p>How do I use this result to get the most similar document against the search query? Basically I am trying to re-create a search bar for Wikipedia. Based on a search query I want to return the most relevant articles from Wikipedia. In this scenario, there are 6 articles (rows) and the search query contains 3 words (columns).</p>\n\n<p>Do I add up all the results in the columns or add up all the rows? Is the greater value the most relevant or is the lowest value the most relevant?</p>\n",
    "score": 8,
    "creation_date": 1344448052,
    "view_count": 4572,
    "answer_count": 1,
    "tags": "python;nlp;nltk;scikit-learn;tf-idf"
  },
  {
    "question_id": 5878528,
    "title": "Is there a way to convert a natural language date NSString to an NSDate",
    "body": "<p>Say I have the NSString <code>@\"tomorrow\"</code></p>\n\n<p>Is there any library that takes strings such as this and converts them into NSDates? I'm imagining/hoping for something like this:</p>\n\n<pre><code>NSString* humanDate = @\"tomorrow at 4:15\";\nNSDateFormatter *dateFormatter = [[NSDateFormatter alloc] init];\n[dateFormatter setDateFormat:@\"x at HH:MM\"];\nNSDate* date = [dateFormatter dateFromString:humanDate];\n</code></pre>\n\n<p>I would also want to do things like \"Next monday\", etc. but it doesn't have to be super sophisticated. I can enforce rules on input, but I'd like a little bit of natural language available.</p>\n\n<p>My alternative is to take the string, break it up into pieces, and format it manually. But I was hoping someone has already done this.</p>\n",
    "score": 8,
    "creation_date": 1304483143,
    "view_count": 2082,
    "answer_count": 1,
    "tags": "objective-c;ios;parsing;nlp;nsdate"
  },
  {
    "question_id": 1805099,
    "title": "c/c++ NLP library",
    "body": "<p>I am looking for an open source Natural Language Processing library for c/c++ and especially i am interested in Part of speech tagging. </p>\n",
    "score": 8,
    "creation_date": 1259260194,
    "view_count": 10175,
    "answer_count": 1,
    "tags": "c++;c;nlp;open-source;pos-tagger"
  },
  {
    "question_id": 65218606,
    "title": "Is it possible to find uncertainties of spaCy POS tags?",
    "body": "<p>I am trying to build a non-English spell checker that relies on classification of sentences by spaCy, which allows my algorithm to then use the POS tags and the grammatical dependencies of the individual tokens to determine incorrect spelling (in my case more specifically: incorrect splits in Dutch compound words).</p>\n<p>However, spaCy appears to classify sentences incorrectly if they contain grammatical errors, for example classifying a noun as a verb, even though the classified word doesn't even look like a verb.</p>\n<p>Because of this I'm wondering if it is possible to obtain the uncertainties of spaCy's classification, to make it possible to tell if spaCy is struggling with a sentence. After all, if spaCy is struggling with a classification, that would provide my spell checker with more confidence that the sentence contains errors.</p>\n<p>Is there any way to know whether spaCy thinks a sentence is grammatically correct (without having to specify patterns of all correct sentence structures in my language), or to obtain classification certainties?</p>\n<hr />\n<p>Edit, based on suggestions in the comments by @Sergey Bushmanov:</p>\n<p>I found <a href=\"https://spacy.io/api/tagger#predict\" rel=\"noreferrer\">https://spacy.io/api/tagger#predict</a>, which might be useful to get the probabilities for the tags. However, I'm not really sure what I am looking at, and I'm not really following what the docs mean about the output. I'm using the following code:</p>\n<pre><code>import spacy\n\nnlp = spacy.load('en_core_web_sm')\ntext = &quot;This is an example sentence for the Spacy tagger.&quot;\ndoc = nlp(text)\n\ndocs = nlp(text, disable=['tagger'])\nscores, tensors = nlp.tagger.predict([docs])\n\nprint(scores)\nprobs = tensors[0]\nfor p in probs:\n    print(p, max(p), p.tolist().index(max(p)))\n</code></pre>\n<p>This prints what I am guessing is some integer representations of the predictions (considering that 'integer' and 'representation' get the same scores), and then an array of 96 floats for every word in the sentence. It also lists the highest score and the position of that highest score, but it seems like for most words, there are multiple items in the <code>p</code> array that get a similar value. Now I'm wondering what these arrays mean, and how to extract probabilities for each classification from it.</p>\n<hr />\n<p>The question is: How can I interpret this output to get the specific probabilities for specific tags found by spaCy's tagger? Or another way to put this same question is: What does the output generated by the above code mean?</p>\n",
    "score": 8,
    "creation_date": 1607523619,
    "view_count": 918,
    "answer_count": 1,
    "tags": "python;nlp;spacy;spell-checking"
  },
  {
    "question_id": 63433168,
    "title": "NLTK is called and got error of &quot;punkt&quot; not found on databricks pyspark",
    "body": "<p>I would like to call NLTK to do some NLP on databricks by pyspark.\nI have installed NLTK from the library tab of databricks. It should be accessible from all nodes.</p>\n<p>My py3 code :</p>\n<pre><code> import pyspark.sql.functions as F\n from pyspark.sql.types import StringType\n import nltk\n nltk.download('punkt')\n \n\n def get_keywords1(col):\n     sentences = []\n     sentence = nltk.sent_tokenize(col)\n      \n\n get_keywords_udf = F.udf(get_keywords1, StringType())\n</code></pre>\n<p>I run the above code and got:</p>\n<pre><code> [nltk_data] Downloading package punkt to /root/nltk_data...\n [nltk_data]   Package punkt is already up-to-date!\n</code></pre>\n<p>When I run the following code:</p>\n<pre><code> t = spark.createDataFrame(\n [(2010, 1, 'rdc', 'a book'), (2010, 1, 'rdc','a car'),\n  (2007, 6, 'utw', 'a house'), (2007, 6, 'utw','a hotel')\n ], \n (&quot;year&quot;, &quot;month&quot;, &quot;u_id&quot;, &quot;objects&quot;))\n \n t1 = t.withColumn('keywords', get_keywords_udf('objects'))\n t1.show() # error here !\n</code></pre>\n<p>I got error:</p>\n<pre><code> &lt;span class=&quot;ansi-red-fg&quot;&gt;&amp;gt;&amp;gt;&amp;gt; import nltk\n\n PythonException: \n  An exception was thrown from the Python worker. Please see the stack trace below.\n Traceback (most recent call last):\n  \n LookupError: \n **********************************************************************\n Resource punkt not found.\n Please use the NLTK Downloader to obtain the resource:\n\n &gt;&gt;&gt; import nltk\n &gt;&gt;&gt; nltk.download('punkt')\n\n For more information see: https://www.nltk.org/data.html\n\nAttempted to load tokenizers/punkt/PY3/english.pickle\n\nSearched in:\n- '/root/nltk_data'\n- '/databricks/python/nltk_data'\n- '/databricks/python/share/nltk_data'\n- '/databricks/python/lib/nltk_data'\n- '/usr/share/nltk_data'\n- '/usr/local/share/nltk_data'\n- '/usr/lib/nltk_data'\n- '/usr/local/lib/nltk_data'\n- ''\n</code></pre>\n<p>I have downloaded 'punkt'. It is located at</p>\n<pre><code>/root/nltk_data/tokenizers\n</code></pre>\n<p>I have updated the PATH in spark environment with the folder location.</p>\n<p>Why it cannot be found ?</p>\n<p>The solution at <a href=\"https://stackoverflow.com/questions/55297145/nltk-punkt-not-found\">NLTK. Punkt not found</a> and this <a href=\"https://stackoverflow.com/questions/3522372/how-to-config-nltk-data-directory-from-code/22987374#22987374\">How to config nltk data directory from code?</a>\nbut none of them  work for me.</p>\n<p>I have tried to updated</p>\n<pre><code> nltk.data.path.append('/root/nltk_data/tokenizers/')\n</code></pre>\n<p>it does not work.\nIt seems that nltk cannot see the new added path !</p>\n<p>I also copied punkz to the path where nltk will search for.</p>\n<p>cp -r /root/nltk_data/tokenizers/punkt /root/nltk_data</p>\n<p>but, nltk still cannot see it.</p>\n<p>thanks</p>\n",
    "score": 8,
    "creation_date": 1597552340,
    "view_count": 7299,
    "answer_count": 5,
    "tags": "python-3.x;pyspark;nlp;nltk"
  },
  {
    "question_id": 58069777,
    "title": "Use tf-idf with FastText vectors",
    "body": "<p>I'm interested in using tf-idf with FastText library, but have found a logical way to handle the ngrams. I have used tf-idf with SpaCy vectors already for what I have found several examples like these ones: </p>\n\n<ul>\n<li><p><a href=\"http://dsgeek.com/2018/02/19/tfidf_vectors.html\" rel=\"noreferrer\">http://dsgeek.com/2018/02/19/tfidf_vectors.html</a></p></li>\n<li><p><a href=\"https://www.aclweb.org/anthology/P16-1089\" rel=\"noreferrer\">https://www.aclweb.org/anthology/P16-1089</a></p></li>\n<li><a href=\"http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\" rel=\"noreferrer\">http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/</a></li>\n</ul>\n\n<p>But for FastText library is not that clear to me, since it has a granularity that isn't that intuitive, E.G.</p>\n\n<p>For a general word2vec aproach I will have one vector for each word, I can count the term frequency of that vector and divide its value accordingly. </p>\n\n<p>But for fastText same word will have several n-grams, </p>\n\n<p>\"Listen to the latest news summary\" will have n-grams generated by a sliding windows like: </p>\n\n<p>lis ist ste ten tot het... </p>\n\n<p>These n-grams are handled internally by the model so when I try: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model[\"Listen to the latest news summary\"] \n</code></pre>\n\n<p>I get the final vector directly, hence what I have though is to split the  text into n-grams before feeding the model like:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model['lis']\nmodel['ist']\nmodel['ten']\n</code></pre>\n\n<p>And make the tf-idf from there, but that seems like an inefficient approach both, is there a standar way to apply tf-idf to vector n-grams like these. </p>\n",
    "score": 8,
    "creation_date": 1569270512,
    "view_count": 4181,
    "answer_count": 3,
    "tags": "python;nlp;fasttext"
  },
  {
    "question_id": 53750468,
    "title": "spaCy coreference resolution - named entity recognition (NER) to return unique entity ID&#39;s?",
    "body": "<p>Perhaps I've skipped over a part of the docs, but what I am trying to determine is a unique ID for each entity in the standard NER toolset. For example:</p>\n\n<pre><code>import spacy\nfrom spacy import displacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\ntext = \"This is a text about Apple Inc based in San Fransisco. \"\\\n        \"And here is some text about Samsung Corp. \"\\\n        \"Now, here is some more text about Apple and its products for customers in Norway\"\n\ndoc = nlp(text)\n\nfor ent in doc.ents:\n    print('ID:{}\\t{}\\t\"{}\"\\t'.format(ent.label,ent.label_,ent.text,))\n\n\ndisplacy.render(doc, jupyter=True, style='ent')\n</code></pre>\n\n<p>returns:</p>\n\n<blockquote>\n<pre><code>ID:381    ORG \"Apple Inc\" \nID:382    GPE \"San Fransisco\" \nID:381    ORG \"Samsung Corp.\" \nID:381    ORG \"Apple\" \nID:382    GPE \"Norway\"\n</code></pre>\n</blockquote>\n\n<p>I have been looking at <code>ent.ent_id</code> and <code>ent.ent_id_</code> but these are inactive according to the <a href=\"https://spacy.io/api/token\" rel=\"nofollow noreferrer\">docs</a>. I couldn't find anything in <code>ent.root</code> either. </p>\n\n<p>For example, in <a href=\"https://cloud.google.com/natural-language/\" rel=\"nofollow noreferrer\">GCP NLP</a> each entity is returned with an ⟨entity⟩number that enables you to identify multiple instances of the same entity within a text.</p>\n\n<blockquote>\n  <p>This is a ⟨text⟩2 about ⟨Apple Inc⟩1 based in ⟨San Fransisco⟩4. And\n  here is some ⟨text⟩3 about ⟨Samsung Corp⟩6. Now, here is some more\n  ⟨text⟩8 about ⟨Apple⟩1 and its ⟨products⟩5 for ⟨customers⟩7 in\n  ⟨Norway⟩9\"</p>\n</blockquote>\n\n<p>Does spaCy support something similar? Or is there a way using NLTK or Stanford?</p>\n",
    "score": 8,
    "creation_date": 1544644632,
    "view_count": 3286,
    "answer_count": 1,
    "tags": "python;nlp;spacy;information-extraction;named-entity-recognition"
  },
  {
    "question_id": 45397168,
    "title": "Extract list in api.ai from user input",
    "body": "<p>I have a query of the following nature in API.ai\n\"btc, ltc, xrp to usd, inr\" How can I extract the query as \nsource = [btc, ltc, xrp]\ndestination = [usd, inr]</p>\n\n<p>The number of elements in the source can be variable and the number of elements in the destination can also be variable. I am aware of the list entity and I tried it with the query. It picks up only btc in one list and puts the rest in another rest. Any suggestions</p>\n",
    "score": 8,
    "creation_date": 1501397352,
    "view_count": 190,
    "answer_count": 1,
    "tags": "nlp;dialogflow-es"
  },
  {
    "question_id": 41584496,
    "title": "QA generation on sub sentences - NLP",
    "body": "<p>My dataset is structured like this:</p>\n\n<p>Product1 - Sentence1</p>\n\n<p>Product2 - Sentence2</p>\n\n<p>Product3 - Sentence3</p>\n\n<p>.\n.</p>\n\n<p>etc</p>\n\n<p>The sentences look like this:</p>\n\n<p>Product1 - \"We suggest that you wear this stylish piece with gold-toned drop earrings, churidar leggings and flats to complete an understated look.\"</p>\n\n<p>A possible question to this can be - \"Do we suggest that you wear this stylish piece with gold-toned drop earrings, churidar leggings and flats to complete an understated look?\" - This is what i get using <a href=\"http://www.cs.cmu.edu/~ark/mheilman/questions/\" rel=\"noreferrer\">http://www.cs.cmu.edu/~ark/mheilman/questions/</a></p>\n\n<h2>But, I want questions/answers like this:</h2>\n\n<p>Q: How can I get an understated look?</p>\n\n<h2>A: You can try Product1 for an understated look.</h2>\n\n<p>Q: What can I wear with gold-toned drop earrings?</p>\n\n<h2>A: You can wear Product1 with gold-toned drop earrings.</h2>\n\n<p>Q: What will give me a complete look with churidar leggings?</p>\n\n<h2>A: Product1 will.</h2>\n\n<p>So, what i feel I want to do is - to create sub-question on the nouns/proNouns and adjectives in an semantically correct putting product in context.</p>\n",
    "score": 8,
    "creation_date": 1484117575,
    "view_count": 229,
    "answer_count": 2,
    "tags": "java;nlp;stanford-nlp;heuristics"
  },
  {
    "question_id": 12264593,
    "title": "How to extract relationship from text in NLTK",
    "body": "<p>Hi I'm trying to extract relationships from a string of text based on the second last example here: <a href=\"https://web.archive.org/web/20120907184244/http://nltk.googlecode.com/svn/trunk/doc/howto/relextract.html\" rel=\"nofollow\">https://web.archive.org/web/20120907184244/http://nltk.googlecode.com/svn/trunk/doc/howto/relextract.html</a></p>\n\n<p>From a string such as \"Michael James editor of Publishers Weekly\" my desired result is to have an output such as:</p>\n\n<blockquote>\n  <p>[PER: 'Michael James'] ', editor of' [ORG: 'Publishers Weekly']</p>\n</blockquote>\n\n<p>What is the best way to do do this? What format does extract_rels expect and how do I format my input to meet that requirement?</p>\n\n<hr>\n\n<p>Tried to do it myself but it didn't work.\nHere is the code I've adapted from the book. I'm not getting any results printed. What am I doing wrong?</p>\n\n<pre><code>class doc():\n pass\n\ndoc.headline = ['this is expected by nltk.sem.extract_rels but not used in this script']\n\ndef findrelations(text):\nroles = \"\"\"\n(.*(                   \nanalyst|\neditor|\nlibrarian).*)|\nresearcher|\nspokes(wo)?man|\nwriter|\n,\\sof\\sthe?\\s*  # \"X, of (the) Y\"\n\"\"\"\nROLES = re.compile(roles, re.VERBOSE)\ntokenizedsentences = nltk.sent_tokenize(text)\nfor sentence in tokenizedsentences:\n    taggedwords  = nltk.pos_tag(nltk.word_tokenize(sentence))\n    doc.text = nltk.batch_ne_chunk(taggedwords)\n    print doc.text\n    for rel in relextract.extract_rels('PER', 'ORG', doc, corpus='ieer', pattern=ROLES):\n        print relextract.show_raw_rtuple(rel) # doctest: +ELLIPSIS\n</code></pre>\n\n<blockquote>\n  <p>text =\"Michael James editor of Publishers Weekly\"</p>\n  \n  <p>findrelations(text)</p>\n</blockquote>\n",
    "score": 8,
    "creation_date": 1346765513,
    "view_count": 4074,
    "answer_count": 1,
    "tags": "nlp;nltk"
  },
  {
    "question_id": 61567599,
    "title": "HuggingFace BERT `inputs_embeds` giving unexpected result",
    "body": "<p>The <a href=\"https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel\" rel=\"noreferrer\">HuggingFace BERT TensorFlow implementation</a> allows us to feed in a precomputed embedding in place of the embedding lookup that is native to BERT. This is done using the model's <code>call</code> method's optional parameter <code>inputs_embeds</code> (in place of <code>input_ids</code>). To test this out, I wanted to make sure that if I <em>did</em> feed in BERT's embedding lookup, I would get the same result as having fed in the <code>input_ids</code> themselves.</p>\n\n<p>The result of BERT's embedding lookup can be obtained by setting the BERT configuration parameter <code>output_hidden_states</code> to <code>True</code> and extracting the first tensor from the last output of the <code>call</code> method. (The remaining 12 outputs correspond to each of the 12 hidden layers.)</p>\n\n<p>Thus, I wrote the following code to test my hypothesis:</p>\n\n<pre><code>import tensorflow as tf\nfrom transformers import BertConfig, BertTokenizer, TFBertModel\n\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ninput_ids = tf.constant(bert_tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[None, :]\nattention_mask = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])\ntoken_type_ids = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])\n\nconfig = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\nbert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n\nresult = bert_model(inputs={'input_ids': input_ids, \n                            'attention_mask': attention_mask, \n                             'token_type_ids': token_type_ids})\ninputs_embeds = result[-1][0]\nresult2 = bert_model(inputs={'inputs_embeds': inputs_embeds, \n                            'attention_mask': attention_mask, \n                             'token_type_ids': token_type_ids})\n\nprint(tf.reduce_sum(tf.abs(result[0] - result2[0])))  # 458.2522, should be 0\n</code></pre>\n\n<p>Again, the output of the <code>call</code> method is a tuple. The first element of this tuple is the output of the last layer of BERT. Thus, I expected <code>result[0]</code> and <code>result2[0]</code> to match. <strong>Why is this not the case?</strong></p>\n\n<p>I am using Python 3.6.10 with <code>tensorflow</code> version 2.1.0 and <code>transformers</code> version 2.5.1.</p>\n\n<p><strong>EDIT</strong>: Looking at some of the <a href=\"https://huggingface.co/transformers/_modules/transformers/modeling_bert.html\" rel=\"noreferrer\">HuggingFace code</a>, it seems that the raw embeddings that are looked up when <code>input_ids</code> is given or assigned when <code>inputs_embeds</code> is given are added to the positional embeddings and token type embeddings before being fed into subsequent layers. If this is the case, then it <em>may</em> be possible that what I'm getting from <code>result[-1][0]</code> is the raw embedding plus the positional and token type embeddings. This would mean that they are erroneously getting added in again when I feed <code>result[-1][0]</code> as <code>inputs_embeds</code> in order to calculate <code>result2</code>.</p>\n\n<p><strong>Could someone please tell me if this is the case and if so, please explain how to get the positional and token type embeddings, so I can subtract them out?</strong> Below is what I came up with for positional embeddings based on the equations given <a href=\"https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3\" rel=\"noreferrer\">here</a> (but according to the <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"noreferrer\">BERT paper</a>, the positional embeddings may actually be learned, so I'm not sure if these are valid):</p>\n\n<pre><code>import numpy as np\n\npositional_embeddings = np.stack([np.zeros(shape=(len(sent),768)) for sent in input_ids])\nfor s in range(len(positional_embeddings)):\n    for i in range(len(positional_embeddings[s])):\n        for j in range(len(positional_embeddings[s][i])):\n            if j % 2 == 0:\n                positional_embeddings[s][i][j] = np.sin(i/np.power(10000., j/768.))\n            else:\n                positional_embeddings[s][i][j] = np.cos(i/np.power(10000., (j-1.)/768.))\npositional_embeddings = tf.constant(positional_embeddings)\ninputs_embeds += positional_embeddings\n</code></pre>\n",
    "score": 8,
    "creation_date": 1588461497,
    "view_count": 3587,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 42212423,
    "title": "Python tf-idf: fast way to update the tf-idf matrix",
    "body": "<p>I have a dataset of several thousand rows of text, my target is to calculate the tfidf score and then cosine similarity between documents, this is what I did using gensim in Python followed the tutorial:</p>\n\n<pre><code>dictionary = corpora.Dictionary(dat)\ncorpus = [dictionary.doc2bow(text) for text in dat]\n\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\nindex = similarities.MatrixSimilarity(corpus_tfidf)\n</code></pre>\n\n<p>Let's say we have the tfidf matrix and similarity built, when we have a new document come in, I want to query for its most similar document in our existing dataset.</p>\n\n<p>Question: is there any way we can update the tf-idf matrix so that I don't have to append the new text doc to the original dataset and recalculate the whole thing again? </p>\n",
    "score": 8,
    "creation_date": 1487015673,
    "view_count": 5998,
    "answer_count": 2,
    "tags": "python;nlp;tf-idf;gensim;cosine-similarity"
  },
  {
    "question_id": 32333312,
    "title": "How to extract chunks from BIO chunked sentences? - python",
    "body": "<p>Give an input sentence, that has <a href=\"https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk\">BIO chunk tags</a>:</p>\n\n<blockquote>\n  <p>[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed',\n  'I-NP'), ('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'),\n  ('swallow', 'I-NP'), ('?', 'O')]</p>\n</blockquote>\n\n<p>I would need to extract the relevant phrases out, e.g. if I want to extract <code>'NP'</code>, I would need to extract the fragments of tuples that contains <code>B-NP</code> and <code>I-NP</code>.</p>\n\n<p>[out]:</p>\n\n<pre><code>[('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]\n</code></pre>\n\n<p>(Note: the numbers in the extract tuples represent the token index.)</p>\n\n<p>I have tried extracting it using the following code:</p>\n\n<pre><code>def extract_chunks(tagged_sent, chunk_type):\n    current_chunk = []\n    current_chunk_position = []\n    for idx, word_pos in enumerate(tagged_sent):\n        word, pos = word_pos\n        if '-'+chunk_type in pos: # Append the word to the current_chunk.\n            current_chunk.append((word))\n            current_chunk_position.append((idx))\n        else:\n            if current_chunk: # Flush the full chunk when out of an NP.\n                _chunk_str = ' '.join(current_chunk) \n                _chunk_pos_str = '-'.join(map(str, current_chunk_position))\n                yield _chunk_str, _chunk_pos_str \n                current_chunk = []\n                current_chunk_position = []\n    if current_chunk: # Flush the last chunk.\n        yield ' '.join(current_chunk), '-'.join(current_chunk_position)\n\n\ntagged_sent = [('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'), ('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'), ('?', 'O')]\nprint (list(extract_chunks(tagged_sent, chunk_type='NP')))\n</code></pre>\n\n<p>But when I have adjacent chunk of the same type:</p>\n\n<pre><code>tagged_sent = [('The', 'B-NP'), ('Mitsubishi', 'I-NP'),  ('Electric', 'I-NP'), ('Company', 'I-NP'), ('Managing', 'B-NP'), ('Director', 'I-NP'), ('ate', 'B-VP'), ('ramen', 'B-NP')]\n\nprint (list(extract_chunks(tagged_sent, chunk_type='NP')))\n</code></pre>\n\n<p>It outputs this:</p>\n\n<pre><code>[('The Mitsubishi Electric Company Managing Director', '0-1-2-3-4-5'), ('ramen', '7')]\n</code></pre>\n\n<p>Instead of the desired:</p>\n\n<pre><code>[('The Mitsubishi Electric Company', '0-1-2-3'), ('Managing Director', '4-5'), ('ramen', '7')]\n</code></pre>\n\n<p><strong>How can this be resolved from the above code?</strong></p>\n\n<p><strong>Other than how it's done from the code above, is there a better solution to extract the desired chunks of a specific <code>chunk_type</code>?</strong></p>\n",
    "score": 8,
    "creation_date": 1441115107,
    "view_count": 1537,
    "answer_count": 3,
    "tags": "python;list;nlp;text-parsing;text-chunking"
  },
  {
    "question_id": 16791716,
    "title": "Obtain multiple taggings with Stanford POS Tagger",
    "body": "<p>I'm performing POS tagging with the <a href=\"http://nlp.stanford.edu/software/tagger.shtml\">Stanford POS Tagger</a>. The tagger only returns one possible tagging for the input sentence. For instance, when provided with the input sentence \"The clown weeps.\", the POS tagger produces the (erroneous) \"The_DT clown_NN weeps_NNS ._.\".</p>\n\n<p>However, my application will try to parse the result, and may reject a POS tagging because there is no way to parse it. Hence, in this example, it would reject \"The_DT clown_NN weeps_NNS ._.\" but would accept \"The_DT clown_NN weeps_VBZ ._.\" which I assume is a lower-confidence tagging for the parser.</p>\n\n<p>I would therefore like the POS tagger to provide multiple hypotheses for the tagging of each word, annotated by some kind of confidence value. In this way, my application could choose the POS tagging with highest confidence that achieves a valid parsing for its purposes.</p>\n\n<p>I have found no way to ask the Stanford POS Tagger to produce multiple (n-best) tagging hypotheses for each word (or even for the whole sentence). Is there a way to do this? (Alternatively, I am also OK with using another POS tagger with comparable performance that would have support for this.)</p>\n",
    "score": 8,
    "creation_date": 1369742994,
    "view_count": 991,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp;pos-tagger"
  },
  {
    "question_id": 9010817,
    "title": "Processing malformed text data with machine learning or NLP",
    "body": "<p>I'm trying to extract data from a few large textfiles containing entries about people. The problem is, though, I cannot control the way the data comes to me.</p>\n\n<p>It is usually in a format like this:</p>\n\n<blockquote>\n  <p>LASTNAME, Firstname Middlename (Maybe a Nickname)Why is this text hereJanuary, 25, 2012</p>\n  \n  <p>Firstname Lastname 2001 Some text that I don't care about</p>\n  \n  <p>Lastname, Firstname blah blah ... January 25, 2012 ...</p>\n</blockquote>\n\n<p>Currently, I am using a <em>huge</em> regex that splits all <code>kindaCamelcase</code> words, all words that have a month name tacked onto the end, and a lot of special cases for names. Then I use more regex to extract a lot of combinations for the name and date.</p>\n\n<p>This seems sub-optimal.</p>\n\n<p>Are there any machine-learning libraries for Python that can parse malformed data that is somewhat structured?</p>\n\n<p>I've tried NLTK, but it could not handle my dirty data. I'm tinkering with Orange right now and I like it's OOP style, but I'm not sure if I'm wasting my time.</p>\n\n<p>Ideally, I'd like to do something like this to train a parser (with many input/output pairs):</p>\n\n<pre><code>training_data = (\n  'LASTNAME, Firstname Middlename (Maybe a Nickname)FooBarJanuary 25, 2012',\n   ['LASTNAME', 'Firstname', 'Middlename', 'Maybe a Nickname', 'January 25, 2012']\n)\n</code></pre>\n\n<p>Is something like this possible or am I overestimating machine learning? Any suggestions will be appreciated, as I'd like to learn more about this topic.</p>\n",
    "score": 8,
    "creation_date": 1327528151,
    "view_count": 2097,
    "answer_count": 5,
    "tags": "python;parsing;nlp;machine-learning"
  },
  {
    "question_id": 2883012,
    "title": "Indexing and Searching Over Word Level Annotation Layers in Lucene",
    "body": "<p>I have a data set with multiple layers of annotation over the underlying text, such as <a href=\"http://en.wikipedia.org/wiki/Part-of-speech_tagging\" rel=\"noreferrer\">part-of-tags</a>, <a href=\"http://www.cnts.ua.ac.be/conll2000/chunking/\" rel=\"noreferrer\">chunks from a shallow parser</a>, <a href=\"http://en.wikipedia.org/wiki/Named_entity_recognition\" rel=\"noreferrer\">name entities</a>, and others from various  <a href=\"http://en.wikipedia.org/wiki/Natural_language_processing\" rel=\"noreferrer\">natural language processing</a> (NLP) tools. For a sentence like <code>The man went to the store</code>, the annotations might look like:</p>\n\n<pre>\n\nWord  POS  Chunk       NER\n====  ===  =====  ========\nThe    DT     NP    Person     \nman    NN     NP    Person\nwent  VBD     VP         -\nto     TO     PP         - \nthe    DT     NP  Location\nstore  NN     NP  Location\n</pre>\n\n<p>I'd like to index a bunch of documents with annotations like these using Lucene and then perform searches across the different layers. An example of a simple query would be to retrieve all documents where <strong>Washington</strong> is tagged as a <strong>person</strong>. While I'm not absolutely committed to the notation, syntactically end-users might enter the query as follows:</p>\n\n<p><strong>Query</strong>: <code>Word=Washington,NER=Person</code> </p>\n\n<p>I'd also like to do more complex queries involving the <strong>sequential order of annotations</strong> across different layers, e.g. find all the documents where there's a word tagged <strong>person</strong> followed by the words <strong><code>arrived at</code></strong> followed by a word tagged <strong>location</strong>. Such a query might look like:</p>\n\n<p><strong>Query</strong>: <code>\"NER=Person Word=arrived Word=at NER=Location\"</code></p>\n\n<p>What's a good way to go about approaching this with Lucene? Is there anyway to index and search over document fields that contain structured tokens?</p>\n\n<p><strong>Payloads</strong></p>\n\n<p>One suggestion was to try to use Lucene <a href=\"http://lucene.apache.org/java/2_9_2/api/all/org/apache/lucene/search/payloads/package-summary.html\" rel=\"noreferrer\">payloads</a>. But, I thought payloads could only be used to adjust the rankings of documents, and that they aren't used to select what documents are returned. </p>\n\n<p>The latter is important since, for some use-cases, the <strong>number of documents</strong> that contain a pattern is really what I want.</p>\n\n<p>Also, only the payloads on terms that match the query are examined. This means that <strong>payloads could only even help with the rankings of the first example query</strong>, <code>Word=Washington,NER=Person</code>, whereby we just want to make sure the term <strong><code>Washingonton</code></strong> is tagged as a <strong><code>Person</code></strong>. However, for the second example query,  <code>\"NER=Person Word=arrived Word=at NER=Location\"</code>, I need to check the tags on unspecified, and thus non-matching, terms.   </p>\n",
    "score": 8,
    "creation_date": 1274452652,
    "view_count": 1385,
    "answer_count": 3,
    "tags": "java;lucene;nlp;data-mining;text-mining"
  },
  {
    "question_id": 2325210,
    "title": "Looking for a database of n-grams taken from wikipedia",
    "body": "<p>I am effectively trying to solve the same problem as this question:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/610399/finding-related-words-specifically-physical-objects-to-a-specific-word\">Finding related words (specifically physical objects) to a specific word</a></p>\n\n<p>minus the requirement that words represent physical objects.  The answers and edited question seem to indicate that a good start is building a list of frequency of n-grams using wikipedia text as a corpus.  Before I start downloading the mammoth wikipedia dump, does anyone know if such a list already exists? </p>\n\n<p>PS if the original poster of the previous question sees this, I would love to know how you went about solving the problem, as your results seem excellent :-)</p>\n",
    "score": 8,
    "creation_date": 1267007246,
    "view_count": 2723,
    "answer_count": 2,
    "tags": "nlp;semantics;wikipedia"
  },
  {
    "question_id": 52429869,
    "title": "Batch running spaCy nlp() pipeline for large documents",
    "body": "<p>I am trying to run the nlp() pipeline on a series of transcripts amounting to 20,211,676 characters. I am running on a machine with 8gb RAM. I'm very new at both Python and spaCy, but the corpus comparison tools and sentence chunking features are perfect for the paper I'm working on now. </p>\n\n<p><strong>What I've tried</strong></p>\n\n<p>I've begun by importing the English pipeline and removing 'ner' for faster speeds</p>\n\n<pre><code>nlp = spacy.load('en_core_web_lg', disable = ['ner'])\n</code></pre>\n\n<p>Then I break up the corpus into pieces of 800,000 characters since spaCy recommends 100,000 characters per gb</p>\n\n<pre><code>split_text = [text[i:i+800000] for i in range(0, len(text), 800000)]\n</code></pre>\n\n<p>Loop the pieces through the pipeline and create a list of nlp objects</p>\n\n<pre><code>nlp_text = []\nfor piece in split_text:\n    piece = nlp(piece)\n    nlp_text.append(piece)\n</code></pre>\n\n<p>Which works after a long wait period. <strong>note:</strong> I have tried upping the threshold via 'nlp.max_length' but anything above 1,200,000 breaks my python session.</p>\n\n<p>Now that I have everything piped through I need to concatenate everything back since I will eventually need to compare the <strong>whole</strong> document to another (of roughly equal size). Also I would be interested in finding the most frequent noun-phrases in the document as a whole, not just in artificial 800,000 character pieces.</p>\n\n<pre><code>nlp_text = ''.join(nlp_text)\n</code></pre>\n\n<p>However I get the error message:</p>\n\n<blockquote>\n  <p>TypeError: sequence item 0: expected str instance,\n  spacy.tokens.doc.Doc found</p>\n</blockquote>\n\n<p>I realize that I could turn to string and concatenate, but that would defeat the purpose of having \"token\" objects to works with.</p>\n\n<p><strong>What I need</strong></p>\n\n<p>Is there anything I can do (apart from using AWS expensive CPU time) to split my documents, run the nlp() pipeline, then join the tokens to reconstruct my complete document as an object of study? Am I running the pipeline wrong for a big document? Am I doomed to getting 64gb RAM somewhere? </p>\n\n<p><strong>Edit 1: Response to Ongenz</strong></p>\n\n<p>(1) Here is the error message I receive</p>\n\n<blockquote>\n  <p>ValueError: [E088] Text of length 1071747 exceeds maximum of 1000000.\n  The v2.x parser and NER models require roughly 1GB of temporary memory\n  per 100,000 characters in the input. This means long texts may cause\n  memory allocation errors. If you're not using the parser or NER, it's\n  probably safe to increase the nlp.max_length limit. The limit is in\n  number of characters, so you can check whether your inputs are too\n  long by checking len(text).</p>\n</blockquote>\n\n<p>I could not find a part of the documentation that refers to this directly.</p>\n\n<p>(2) My goal is to do a series of measures including (but not limited to if need arises): word frequency, tfidf count, sentence count, count most frequent noun-chunks, comparing two corpus using w2v or d2v strategies.\nMy understanding is that I need every part of the spaCy pipeline apart from Ner for this.</p>\n\n<p>(3) You are completely right about cutting the document, in a perfect world I would cut on a line break instead. But as you mentioned I cannot use join to regroup my broken-apart corpus, so it might not be relevant anyways.</p>\n",
    "score": 8,
    "creation_date": 1537462463,
    "view_count": 4666,
    "answer_count": 1,
    "tags": "python-3.x;nlp;spacy"
  },
  {
    "question_id": 40092477,
    "title": "combining multiple sentences into one",
    "body": "<p>Given following sentences, for example:</p>\n\n<pre><code>A cat is on a bed.\nThe color of the cat is brown.\nThe bed is small.\n</code></pre>\n\n<p>Is there a way to combine/summarize the sentences into a single sentence such as:</p>\n\n<pre><code>A brown cat is on a small bed.\n</code></pre>\n\n<p>I have no knowledge of summary extraction/generation, but can it hopefully be exploited for this purpose?</p>\n",
    "score": 8,
    "creation_date": 1476725891,
    "view_count": 2551,
    "answer_count": 1,
    "tags": "nlp;nltk;stanford-nlp;summary"
  },
  {
    "question_id": 57164764,
    "title": "What is the significance of the magnitude/norm of BERT word embeddings?",
    "body": "<p>We generally compare similarity between word embeddings with cosine similarity, but this only takes into account the angle between the vectors, not the norm. With word2vec, the norm of the vector decreases as the word is used in more varied contexts. So, stopwords are close to 0 and very unique, high meaning words tend to be large vectors. BERT is context sensitive, so this explanation doesn't entirely cover BERT embeddings. Does anyone have any idea what the significance of vector magnitude could be with BERT?</p>\n",
    "score": 8,
    "creation_date": 1563886856,
    "view_count": 2732,
    "answer_count": 1,
    "tags": "nlp;bert-language-model"
  },
  {
    "question_id": 35542740,
    "title": "Implementing trigram markov model",
    "body": "<p>Given : </p>\n\n<p><a href=\"https://i.sstatic.net/N8U6I.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/N8U6I.png\" alt=\"enter image description here\"></a></p>\n\n<p>and the following : </p>\n\n<p><a href=\"https://i.sstatic.net/ly09L.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ly09L.png\" alt=\"enter image description here\"></a></p>\n\n<p>For : </p>\n\n<pre><code>q(runs | the, dog) = 0.5\n</code></pre>\n\n<p>Should this not be <code>1</code> as for \n<code>q(runs | the, dog)</code> : xi=runs , xi-2=the , xi-1=dog</p>\n\n<p>Probability is (wi has been swapped for xi): </p>\n\n<p><a href=\"https://i.sstatic.net/cuvuL.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/cuvuL.png\" alt=\"enter image description here\"></a></p>\n\n<p>therefore : </p>\n\n<pre><code>count(the dog runs) / count(the dog) = 1 / 1 = 1\n</code></pre>\n\n<p>But in above example the value is 0.5 . How is 0.5 arrived at ?</p>\n\n<p>Based on <a href=\"http://files.asimihsan.com/courses/nlp-coursera-2013/notes/nlp.html#markov-processes-part-1\" rel=\"noreferrer\">http://files.asimihsan.com/courses/nlp-coursera-2013/notes/nlp.html#markov-processes-part-1</a></p>\n",
    "score": 8,
    "creation_date": 1456092393,
    "view_count": 1144,
    "answer_count": 1,
    "tags": "nlp;markov;trigram"
  },
  {
    "question_id": 15468178,
    "title": "Code example for Sentiment Analysis for Asian languages - Python NLTK",
    "body": "<p>There is a demo on <code>sentiment analysis</code> with <code>NLTK</code> (python) here <a href=\"http://text-processing.com/demo/sentiment/\" rel=\"noreferrer\">http://text-processing.com/demo/sentiment/</a>.</p>\n\n<p>And also the tutorials on the parts of sentiment analysis</p>\n\n<ul>\n<li><a href=\"http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/\" rel=\"noreferrer\">http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/</a> </li>\n<li><a href=\"http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\" rel=\"noreferrer\">http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/</a></li>\n<li><a href=\"http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html\" rel=\"noreferrer\">http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html</a></li>\n</ul>\n\n<p><strong>Is there any full code example or working projects with python NLTK on sentiment analysis for <em>Asian languages</em>?</strong> (especially for Chinese, Japanese, Korean or Arabic, Hebrew and Persian languages)</p>\n",
    "score": 8,
    "creation_date": 1363567896,
    "view_count": 3490,
    "answer_count": 2,
    "tags": "python;nlp;nltk;sentiment-analysis;asianfonts"
  },
  {
    "question_id": 69938317,
    "title": "How to add index to python FAISS incrementally",
    "body": "<p>I am using Faiss to index my huge dataset embeddings, embedding generated from bert model. I want to add the embeddings incrementally, it is working fine if I only add it with faiss.IndexFlatL2 , but the problem is while saving it the size of it is too large.\nSo I tried with faiss.IndexIVFPQ, but it needs to train embeddings before I add the data, so I can not add it incrementally, I have to compute all embeddings first and then train and add it, it is having issue because all the data should be kept in RAM till I write it. Is there any way to do this incrementally.\nHere is my code:</p>\n<pre><code>    # It is working fine when using with IndexFlatL2\n    def __init__(self, sentences, model):\n        self.sentences = sentences\n        self.model = model\n        self.index = faiss.IndexFlatL2(768)\n\n    def process_sentences(self):\n        result = self.model(self.sentences)\n        self.sentence_ids = []\n        self.token_ids = []\n        self.all_tokens = []\n        for i, (toks, embs) in enumerate(tqdm(result)):\n            # initialize all_embeddings for every new sentence (INCREMENTALLY)\n            all_embeddings = []\n            for j, (tok, emb) in enumerate(zip(toks, embs)):\n                self.sentence_ids.append(i)\n                self.token_ids.append(j)\n                self.all_tokens.append(tok)\n                all_embeddings.append(emb)\n\n            all_embeddings = np.stack(all_embeddings) # Add embeddings after every sentence\n            self.index.add(all_embeddings)\n\n        faiss.write_index(self.index, &quot;faiss_Model&quot;)\n</code></pre>\n<p>ANd when using with IndexIVFPQ:</p>\n<pre><code>   def __init__(self, sentences, model):\n       self.sentences = sentences\n       self.model = model\n       self.quantizer = faiss.IndexFlatL2(768)\n       self.index = faiss.IndexIVFPQ(self.quantizer, 768, 1000, 16, 8)\n\n   def process_sentences(self):\n       result = self.model(self.sentences)\n       self.sentence_ids = []\n       self.token_ids = []\n       self.all_tokens = []\n       all_embeddings = []\n       for i, (toks, embs) in enumerate(tqdm(result)):\n           for j, (tok, emb) in enumerate(zip(toks, embs)):\n               self.sentence_ids.append(i)\n               self.token_ids.append(j)\n               self.all_tokens.append(tok)\n               all_embeddings.append(emb)\n\n       all_embeddings = np.stack(all_embeddings)\n       self.index.train(all_embeddings) # Train\n       self.index.add(all_embeddings) # Add to index\n       faiss.write_index(self.index, &quot;faiss_Model_mini&quot;)\n</code></pre>\n",
    "score": 8,
    "creation_date": 1636694588,
    "view_count": 8712,
    "answer_count": 1,
    "tags": "python;python-3.x;nlp;bert-language-model;faiss"
  },
  {
    "question_id": 69230690,
    "title": "How to increase row height in Pandas?",
    "body": "<p>There are good answers on how to change column width in pandas dataframes, for example <a href=\"https://stackoverflow.com/questions/39680147/can-i-set-variable-column-widths-in-pandas\">here</a> but I couldn't find anything describing how we can change row height?</p>\n<p>I'm trying to view two long texts side by side to compare them and I'd like to increase the row height to make it easier. Maybe some to do with <code>expand_frame_repr</code>?</p>\n",
    "score": 8,
    "creation_date": 1631925292,
    "view_count": 1246,
    "answer_count": 2,
    "tags": "python;pandas;nlp"
  },
  {
    "question_id": 46441377,
    "title": "Extract actions on objects from a sentence in R",
    "body": "<p>I want to extract actions done on objects from a list of sentences in R. To give a small overview.</p>\n\n<pre><code>S = “The boy opened the box. He took the chocolates. He ate the chocolates. \n     He went to school”\n</code></pre>\n\n<p>I am looking for combinations as follows:</p>\n\n<pre><code>Opened box\nTook chocolates\nAte chocolates\nWent school\n</code></pre>\n\n<p>I have been able to get the verbs and the nouns extracted individually. But can’t figure out a way to combine them to get such insights.</p>\n\n<pre><code>library(openNLP)\nlibrary(openNLPmodels.en)\nlibrary(NLP)\n\ns = as.String(\"The boy opened the box. He took the chocolates. He ate the \n               chocolates. He went to school\")\n\ntagPOS&lt;-  function(x, ...) {\ns &lt;- as.String(x)\nword_token_annotator&lt;- Maxent_Word_Token_Annotator()\na2 &lt;- Annotation(1L, \"sentence\", 1L, nchar(s))\na2 &lt;- annotate(s, word_token_annotator, a2)\na3 &lt;- annotate(s, Maxent_POS_Tag_Annotator(), a2)\na3w &lt;- a3[a3$type == \"word\"]\nPOStags&lt;- unlist(lapply(a3w$features, `[[`, \"POS\"))\nPOStagged&lt;- paste(sprintf(\"%s/%s\", s[a3w], POStags), collapse = \",\")\nlist(POStagged = POStagged, POStags = POStags)\n}\n\nnouns = c(\"/NN\", \"/NNS\",\"/NNP\",\"/NNPS\")\nverbs = c(\"/VB\",\"/VBD\",\"/VBG\",\"/VBN\",\"/VBP\",\"/VBZ\")\n\ns = tolower(s)\ns = gsub(\"\\n\",\"\",s)\ns = gsub('\"',\"\",s)\n\ntags = tagPOS(s)\ntags = tags$POStagged\ntags = unlist(strsplit(tags, split=\",\"))\n\nnouns_present = tags[grepl(paste(nouns, collapse = \"|\"), tags)]\nnouns_present = unique(nouns_present)\nverbs_present = tags[grepl(paste(verbs, collapse = \"|\"), tags)]\nverbs_present = unique(verbs_present)\nnouns_present&lt;- gsub(\"^(.*?)/.*\", \"\\\\1\", nouns_present)\nverbs_present = gsub(\"^(.*?)/.*\", \"\\\\1\", verbs_present)\nnouns_present = \npaste(\"'\",as.character(nouns_present),\"'\",collapse=\",\",sep=\"\")\nverbs_present = \npaste(\"'\",as.character(verbs_present),\"'\",collapse=\",\",sep=\"\")\n</code></pre>\n\n<p>The idea is to build a network graph where on clicking on an verb node, all the objects attached to it will come up and vice versa.\nAny help on this would be great.</p>\n",
    "score": 8,
    "creation_date": 1506495501,
    "view_count": 609,
    "answer_count": 1,
    "tags": "r;nlp;opennlp"
  },
  {
    "question_id": 5301655,
    "title": "What is the most accurate open-source tool for sentence splitting?",
    "body": "<p>I need to split text into sentences. I'm currently playing around with OpenNLP's sentence detector tool. I've also heard of NLTK and Stanford CoreNLP tools. What is the most accurate English sentence detection tools out there? I don't need too many NLP features--only a good tool for sentence splitting/detection.</p>\n\n<p>I've also heard about Lucene...but that may be too much. But if it has a kick-ass sentence detection module, then I'll use it.</p>\n",
    "score": 8,
    "creation_date": 1300121296,
    "view_count": 2369,
    "answer_count": 3,
    "tags": "parsing;nlp;tokenize"
  },
  {
    "question_id": 5390397,
    "title": "BLEU score implementation for sentence similarity detection",
    "body": "<p>I need to calculate BLEU score for identifying whether two sentences are similar or not.I have read some articles which are mostly about BLEU score for Measuring Machine translation accuracy.But i'm in need of a BLEU score to find out similarity between sentences in a same language[English].(i.e)(Both the sentences are in English).Thanks in anticipation.</p>\n",
    "score": 7,
    "creation_date": 1300792971,
    "view_count": 14243,
    "answer_count": 6,
    "tags": "java;algorithm;nlp;text-processing;machine-translation"
  },
  {
    "question_id": 815292,
    "title": "Programming tips with Japanese Language/Characters",
    "body": "<p>I have an idea for a few web apps to write to help me, and maybe others, learn Japanese better since I am studying the language.</p>\n\n<p>My problem is the site will be in mostly english, so it needs to mix fluently Japanese Characters, usually hirigana and katakana, but later kanji. I am getting closer to accomplishing this; I have figured out that the pages and source files need to be unicode and utf-8 content types.</p>\n\n<p>However, my problem comes in the actual coding. What I need is to manipulate strings of text that are kana.  One example is:</p>\n\n<p>けす I need to take that verb and convert it to the te-form けして. I would prefer to do this in javascript as it will help down the road to do more manipulation, but if I have to will just do DB calls and hold everything in a DB.</p>\n\n<p>My question is not only how to do it in javascript, but what are some tips and strategies to doing these kinds of things in other languages, too.  I am hoping to get more into doing language learning apps, but am lost when it comes to this.</p>\n",
    "score": 7,
    "creation_date": 1241287220,
    "view_count": 4122,
    "answer_count": 7,
    "tags": "javascript;language-agnostic;unicode;nlp;cjk"
  },
  {
    "question_id": 30822131,
    "title": "NLTK: Package Errors? punkt and pickle?",
    "body": "<p><img src=\"https://i.sstatic.net/Gs346.png\" alt=\"Errors on Command Prompt\" /></p>\n<p>Basically, I have no idea why I'm getting this error.</p>\n<p>Just to have more than an image, here is a similar message in code format. As it is more recent, the answer of this thread has already been mentioned in the message:</p>\n<pre><code>Preprocessing raw texts ...\n\n---------------------------------------------------------------------------\n\nLookupError                               Traceback (most recent call last)\n\n&lt;ipython-input-38-263240bbee7e&gt; in &lt;module&gt;()\n----&gt; 1 main()\n\n7 frames\n\n&lt;ipython-input-32-62fa346501e8&gt; in main()\n     32     data = data.fillna('')  # only the comments has NaN's\n     33     rws = data.abstract\n---&gt; 34     sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n     35     # Define the topic model object\n     36     #tm = Topic_Model(k = 10), method = TFIDF)\n\n&lt;ipython-input-31-f75213289788&gt; in preprocess(docs, samp_size)\n     25     for i, idx in enumerate(samp):\n     26         sentence = preprocess_sent(docs[idx])\n---&gt; 27         token_list = preprocess_word(sentence)\n     28         if token_list:\n     29             idx_in.append(idx)\n\n&lt;ipython-input-29-eddacbfa6443&gt; in preprocess_word(s)\n    179     if not s:\n    180         return None\n--&gt; 181     w_list = word_tokenize(s)\n    182     w_list = f_punct(w_list)\n    183     w_list = f_noun(w_list)\n\n/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py in word_tokenize(text, language, preserve_line)\n    126     :type preserver_line: bool\n    127     &quot;&quot;&quot;\n--&gt; 128     sentences = [text] if preserve_line else sent_tokenize(text, language)\n    129     return [token for sent in sentences\n    130             for token in _treebank_word_tokenizer.tokenize(sent)]\n\n/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py in sent_tokenize(text, language)\n     92     :param language: the model name in the Punkt corpus\n     93     &quot;&quot;&quot;\n---&gt; 94     tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n     95     return tokenizer.tokenize(text)\n     96 \n\n/usr/local/lib/python3.7/dist-packages/nltk/data.py in load(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\n    832 \n    833     # Load the resource.\n--&gt; 834     opened_resource = _open(resource_url)\n    835 \n    836     if format == 'raw':\n\n/usr/local/lib/python3.7/dist-packages/nltk/data.py in _open(resource_url)\n    950 \n    951     if protocol is None or protocol.lower() == 'nltk':\n--&gt; 952         return find(path_, path + ['']).open()\n    953     elif protocol.lower() == 'file':\n    954         # urllib might not use mode='rb', so handle this one ourselves:\n\n/usr/local/lib/python3.7/dist-packages/nltk/data.py in find(resource_name, paths)\n    671     sep = '*' * 70\n    672     resource_not_found = '\\n%s\\n%s\\n%s\\n' % (sep, msg, sep)\n--&gt; 673     raise LookupError(resource_not_found)\n    674 \n    675 \n\nLookupError: \n**********************************************************************\n  Resource punkt not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  &gt;&gt;&gt; import nltk\n  &gt;&gt;&gt; nltk.download('punkt')\n  \n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n</code></pre>\n",
    "score": 7,
    "creation_date": 1434220047,
    "view_count": 42389,
    "answer_count": 3,
    "tags": "python;command-line;package;nlp;nltk"
  },
  {
    "question_id": 2037832,
    "title": "semantic similarity between sentences",
    "body": "<p>I'm doing a project. I need any opensource tool or technique to find the semantic similarity of two sentences, where I give two sentences as an input, and receive score (i.e.,semantic similarity) as an output. Any help?</p>\n",
    "score": 7,
    "creation_date": 1263144584,
    "view_count": 13985,
    "answer_count": 3,
    "tags": "java;machine-learning;nlp"
  },
  {
    "question_id": 885097,
    "title": "Natural Programming Language.... what would you like to see?",
    "body": "<p>I am looking at writing a compiler and after I complete something in a \"C\" style I am looking at adapting it to other models.  What are some syntactical constructs you would expect to see in a \"natural\" programming language?  </p>\n\n<p>The target platform for this compiler will be the CLR and I am currently using Oslo+MGrammar for the lexer/parser (as you can probably tell this is really just an excuse to play)</p>\n\n<p>One of the goals of my project would be to allow programming to feel more like a conversation than structured syntax and demands.</p>\n\n<p>Guess I should extend this out a little.  One of the ideas I am working with is having a class declaration read like a paragraph. </p>\n\n<pre><code>    A Dog is a mammal.  It may Bark and Run.  To Run it\nuses its feet to move forward. It does Lay.\n</code></pre>\n\n<p>...would translate too...    </p>\n\n<pre><code>public class Dog : Mammal{\n\n    public Feet Feet { get; set;}\n\n    public virtual void Bark() {}\n    public virtual void Run() {\n        this.Feet.MoveForward();\n    }\n    public void Lay(){}\n}\n</code></pre>\n",
    "score": 7,
    "creation_date": 1242768262,
    "view_count": 3260,
    "answer_count": 9,
    "tags": "compiler-construction;syntax;language-design;nlp"
  },
  {
    "question_id": 2287962,
    "title": "C++ - How to read Unicode characters( Hindi Script for e.g. ) using C++ or is there a better Way through some other programming language?",
    "body": "<p>I have a hindi script file like this:</p>\n<pre><code>3.  भारत का इतिहास काफी समृद्ध एवं विस्तृत है।\n</code></pre>\n<p>I have to write a program which adds a position to each and every word in each sentence.\nThus the numbering for every line for a particular word position should start off with 1 in parentheses. The output should be something like this.</p>\n<pre><code>3.  भारत(1) का(2) इतिहास(3) काफी(4) समृद्ध(5) एवं(6) विस्तृत(7) है(8) ।(9)\n</code></pre>\n<p>The meaning of the above sentence is:</p>\n<pre><code>3.  India has a long and rich history.\n</code></pre>\n<p>If you observe the '।'( which is a full stop in hindi equivalent to a '.' in English ) also has a word position and similarly other special symbols would also have as I am trying to go about English-Hindi Word alignment( a part of Natural Language Processing ( NLP ) ) so the full stop in english '.' should map to '।'  in Hindi. Serial nos remain as it is untouched.\nI thought reading character by character could be a solution. How can I do this?</p>\n<p>The thing is I am able to get word positions for my English text using C++ as I was able to read character by character using ASCII values in C++ but I don't have a clue to how to go about the same for the hindi text.</p>\n<p>The final aim of all this is to see which word position of the English text maps to which postion in Hindi. This way I can achieve bidirectional alignment.</p>\n<p>Thank you for your time...:)</p>\n",
    "score": 7,
    "creation_date": 1266490292,
    "view_count": 10923,
    "answer_count": 7,
    "tags": "c++;utf-8;nlp"
  },
  {
    "question_id": 573620,
    "title": "How to get started on Information Extraction?",
    "body": "<p>Could you recommend a training path to start and become very good in Information Extraction. I started reading about it to do one of my hobby project and soon realized that I would have to be good at math (Algebra, Stats, Prob). I have read some of the introductory books on different math topics (and its so much fun). Looking for some guidance. Please help.</p>\n\n<p>Update: Just to answer one of the comment. I am more interested in Text Information Extraction.</p>\n",
    "score": 7,
    "creation_date": 1235245222,
    "view_count": 3594,
    "answer_count": 8,
    "tags": "math;machine-learning;nlp;information-extraction"
  },
  {
    "question_id": 348958,
    "title": "Algorithms recognizing physical address on a webpage",
    "body": "<p>What are the best algorithms for recognizing structured data on an HTML page?</p>\n\n<p>For example Google will recognize the address of home/company in an email, and offers a map to this address.</p>\n",
    "score": 7,
    "creation_date": 1228727196,
    "view_count": 4561,
    "answer_count": 9,
    "tags": "algorithm;screen-scraping;nlp;pattern-matching;named-entity-recognition"
  },
  {
    "question_id": 54512264,
    "title": "Use Spacy to find Lemma of Russian (Those langs which don&#39;t have model)",
    "body": "<p>I have downloaded Spacy English model and finding lemma using this code.</p>\n\n<pre><code>import spacy\nnlp = spacy.load('en')\ndoc = nlp(u'Two apples')\nfor token in doc:\n    print(token, token.lemma, token.lemma_)\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>Two 11711838292424000352 two\napples 8566208034543834098 apple\n</code></pre>\n\n<p>Now I wanted to do same thing for <strong>Russian</strong> language. But Spacy don't have models for Russian language. But I am seeing their <a href=\"https://github.com/explosion/spaCy/tree/master/spacy/lang/ru\" rel=\"noreferrer\">GitHub code for Russian language</a> and I think that code could be used to find lemma.</p>\n\n<p>I am new to Spacy. Will needed a starting point for those languages which don't have models. Also I have noted that for some languages let say for URDU they have provided a <a href=\"https://github.com/explosion/spaCy/blob/master/spacy/lang/ur/lemmatizer.py\" rel=\"noreferrer\">look up dictionary</a> for lemmatization.</p>\n\n<p>I want to expand this thing to all those languages which don't have models.</p>\n\n<p>Note: In above code I believe that it could be further improved as in my case I needed lemma only so what are the things which I can turn off and how?</p>\n",
    "score": 7,
    "creation_date": 1549268126,
    "view_count": 8915,
    "answer_count": 4,
    "tags": "nlp;spacy;lemmatization"
  },
  {
    "question_id": 8629737,
    "title": "Coreference Resolution using OpenNLP",
    "body": "<p>I want to do <em>\"coreference resolution\"</em> using OpenNLP. Documentation from Apache (<a href=\"http://incubator.apache.org/opennlp/documentation/manual/opennlp.html#tools.coref\" rel=\"noreferrer\">Coreference Resolution</a>) doesn't cover how to do <em>\"coreference resolution\"</em>. Does anybody have any docs/tutorial how to do this?</p>\n",
    "score": 7,
    "creation_date": 1324818683,
    "view_count": 5980,
    "answer_count": 2,
    "tags": "nlp;opennlp"
  },
  {
    "question_id": 29397708,
    "title": "Tagging a single word with the nltk pos tagger tags each letter instead of the word",
    "body": "<p>I'm try to tag a single word with the nltk pos tagger:</p>\n\n<pre><code>word = \"going\"\npos = nltk.pos_tag(word)\nprint pos\n</code></pre>\n\n<p>But the output is this:</p>\n\n<pre><code>[('g', 'NN'), ('o', 'VBD'), ('i', 'PRP'), ('n', 'VBP'), ('g', 'JJ')]\n</code></pre>\n\n<p>It's tagging each letter rather than just the one word.</p>\n\n<p>What can I do to make it tag the word?</p>\n",
    "score": 7,
    "creation_date": 1427911458,
    "view_count": 12773,
    "answer_count": 4,
    "tags": "python;python-2.7;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 23317458,
    "title": "How to remove punctuation?",
    "body": "<p>I am using the tokenizer from <strong>NLTK in Python</strong>.</p>\n\n<p>There are whole bunch of answers for removing punctuations on the forum already. However, none of them address all of the following issues together:</p>\n\n<ol>\n<li><em>More than one symbol in a row</em>. For example, the sentence: He said,\"that's it.\" Because there's a comma followed by quotation mark, the tokenizer won't remove .\" in the sentence. The tokenizer will give ['He', 'said', ',\"', 'that', 's', 'it.'] instead of ['He','said', 'that', 's', 'it']. Some other examples include '...', '--', '!?', ',\"', and so on.</li>\n<li><em>Remove symbol at the end of the sentence</em>. i.e. the sentence: Hello World. The tokenizer will give ['Hello', 'World.'] instead of ['Hello', 'World']. Notice the period at the end of the word 'World'. Some other examples include '--',',' in the beginning, middle, or end of any character.</li>\n<li><em>Remove characters with symbols in front and after</em>. i.e. <code>'*u*', '''','\"\"'</code></li>\n</ol>\n\n<p>Is there an elegant way of solving both problems?</p>\n",
    "score": 7,
    "creation_date": 1398553711,
    "view_count": 31910,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 9962721,
    "title": "Detect a pronoun and its noun?",
    "body": "<p>Wondering if there is any tool that can help me to detect a pronoun's name in a text.</p>\n\n<p>Example</p>\n\n<pre><code>Jone is Spanish. He can speak German.\n</code></pre>\n\n<p>How can I tag <code>He</code> to <code>Jone</code>?</p>\n",
    "score": 7,
    "creation_date": 1333265949,
    "view_count": 2482,
    "answer_count": 3,
    "tags": "java;nlp;weka;stanford-nlp"
  },
  {
    "question_id": 53867351,
    "title": "How to visualize attention weights?",
    "body": "<p><a href=\"https://github.com/keras-team/keras/issues/4962\" rel=\"noreferrer\">Using this implementation</a>\nI have included attention to my RNN (which classify the input sequences into two classes) as follows.</p>\n\n<pre><code>visible = Input(shape=(250,))\n\nembed=Embedding(vocab_size,100)(visible)\n\nactivations= keras.layers.GRU(250, return_sequences=True)(embed)\n\nattention = TimeDistributed(Dense(1, activation='tanh'))(activations) \nattention = Flatten()(attention)\nattention = Activation('softmax')(attention)\nattention = RepeatVector(250)(attention)\nattention = Permute([2, 1])(attention) \n\nsent_representation = keras.layers.multiply([activations, attention])\nsent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\npredictions=Dense(1, activation='sigmoid')(sent_representation)\n\nmodel = Model(inputs=visible, outputs=predictions)\n</code></pre>\n\n<p>I have trained the model and saved the weights into <code>weights.best.hdf5</code> file.</p>\n\n<p>I am dealing with binary classification problem and the input to my model is the one hot vectors (character based).</p>\n\n<p>How can I visualize the attention weights for certain specific test case in the current implementation?</p>\n",
    "score": 7,
    "creation_date": 1545303613,
    "view_count": 17884,
    "answer_count": 2,
    "tags": "keras;deep-learning;nlp;recurrent-neural-network;attention-model"
  },
  {
    "question_id": 37237594,
    "title": "How can I split at word boundaries with regexes?",
    "body": "<p>I'm trying to do this:</p>\n\n<pre><code>import re\nsentence = \"How are you?\"\nprint(re.split(r'\\b', sentence))\n</code></pre>\n\n<p>The result being</p>\n\n<pre><code>[u'How are you?']\n</code></pre>\n\n<p>I want something like <code>[u'How', u'are', u'you', u'?']</code>. How can this be achieved?</p>\n",
    "score": 7,
    "creation_date": 1463311073,
    "view_count": 6667,
    "answer_count": 3,
    "tags": "python;regex;nlp"
  },
  {
    "question_id": 15921417,
    "title": "Creating own POS Tagger",
    "body": "<p>I have found the <a href=\"http://nlp.stanford.edu/software/tagger.shtml\" rel=\"noreferrer\">Stanford POS Tagger</a> pretty good, but somehow I found myself in need of creating my own POS tagger.</p>\n\n<p>For the last two weeks, I am rambling here and there, on whether to start from parsing tree, or once we have a pos tagger than we can parse tree, using ugly CFGs and NFAs so that they can help me in creating a POS tagger and what not.</p>\n\n<p>I am ending the question here, asking seniors, where to begin POS tagging.\n(language of choice is Python, but C and JAVA won't hurt).</p>\n",
    "score": 7,
    "creation_date": 1365584510,
    "view_count": 5718,
    "answer_count": 1,
    "tags": "java;python;c;nlp;stanford-nlp"
  },
  {
    "question_id": 2941029,
    "title": "PyParsing: Is this correct use of setParseAction()?",
    "body": "<p>I have strings like this:</p>\n\n<pre><code>\"MSE 2110, 3030, 4102\"\n</code></pre>\n\n<p>I would like to output:</p>\n\n<pre><code>[(\"MSE\", 2110), (\"MSE\", 3030), (\"MSE\", 4102)]\n</code></pre>\n\n<p>This is my way of going about it, although I haven't quite gotten it yet:</p>\n\n<pre><code>def makeCourseList(str, location, tokens):\n    print \"before: %s\" % tokens\n\n    for index, course_number in enumerate(tokens[1:]):\n        tokens[index + 1] = (tokens[0][0], course_number)\n\n    print \"after: %s\" % tokens\n\ncourse = Group(DEPT_CODE + COURSE_NUMBER) # .setResultsName(\"Course\")\n\ncourse_data = (course + ZeroOrMore(Suppress(',') + COURSE_NUMBER)).setParseAction(makeCourseList)\n</code></pre>\n\n<p>This outputs:</p>\n\n<pre><code>&gt;&gt;&gt; course.parseString(\"CS 2110\")\n([(['CS', 2110], {})], {})\n&gt;&gt;&gt; course_data.parseString(\"CS 2110, 4301, 2123, 1110\")\nbefore: [['CS', 2110], 4301, 2123, 1110]\nafter: [['CS', 2110], ('CS', 4301), ('CS', 2123), ('CS', 1110)]\n([(['CS', 2110], {}), ('CS', 4301), ('CS', 2123), ('CS', 1110)], {})\n</code></pre>\n\n<p>Is this the right way to do it, or am I totally off?</p>\n\n<p>Also, the output of isn't quite correct - I want <code>course_data</code> to emit a list of <code>course</code> symbols that are in the same format as each other. Right now, the first course is different from the others. (It has a <code>{}</code>, whereas the others don't.)</p>\n",
    "score": 7,
    "creation_date": 1275270930,
    "view_count": 6780,
    "answer_count": 4,
    "tags": "python;parsing;nlp;pyparsing"
  },
  {
    "question_id": 1082789,
    "title": "Simple Sentiment Analysis",
    "body": "<p>It appears that the simplest, naivest way to do basic sentiment analysis is with a Bayesian classifier (confirmed by what I'm finding here on SO). Any counter-arguments or other suggestions?</p>\n",
    "score": 7,
    "creation_date": 1246733895,
    "view_count": 3671,
    "answer_count": 3,
    "tags": "nlp;bayesian"
  },
  {
    "question_id": 62405155,
    "title": "BertWordPieceTokenizer vs BertTokenizer from HuggingFace",
    "body": "<p>I have the following pieces of code and trying to understand the difference between BertWordPieceTokenizer and BertTokenizer.</p>\n<h1><strong>BertWordPieceTokenizer (Rust based)</strong></h1>\n<pre><code>from tokenizers import BertWordPieceTokenizer\n\nsequence = &quot;Hello, y'all! How are you Tokenizer 😁 ?&quot;\ntokenizer = BertWordPieceTokenizer(&quot;bert-base-uncased-vocab.txt&quot;)\ntokenized_sequence = tokenizer.encode(sequence)\nprint(tokenized_sequence)\n&gt;&gt;&gt;Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n\nprint(tokenized_sequence.tokens)\n&gt;&gt;&gt;['[CLS]', 'hello', ',', 'y', &quot;'&quot;, 'all', '!', 'how', 'are', 'you', 'token', '##izer', '[UNK]', '?', '[SEP]']\n</code></pre>\n<h1>BertTokenizer</h1>\n<pre><code>from transformers import BertTokenizer\ntokenizer = BertTokenizer(&quot;bert-base-cased-vocab.txt&quot;)\ntokenized_sequence = tokenizer.encode(sequence)\nprint(tokenized_sequence)\n#Output: [19082, 117, 194, 112, 1155, 106, 1293, 1132, 1128, 22559, 17260, 100, 136]\n</code></pre>\n<ol>\n<li>Why is encode working differently in both ? In BertWordPieceTokenizer it gives Encoding object while in BertTokenizer it gives the ids of the vocab.</li>\n<li>What is the Difference between BertWordPieceTokenizer and BertTokenizer fundamentally, because as I understand BertTokenizer also uses WordPiece under the hood.</li>\n</ol>\n<p>Thanks</p>\n",
    "score": 7,
    "creation_date": 1592299179,
    "view_count": 12429,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;bert-language-model;huggingface-tokenizers"
  },
  {
    "question_id": 50752266,
    "title": "Spacy - Tokenize quoted string",
    "body": "<p>I am using spacy 2.0 and using a quoted string as input.  </p>\n\n<p>Example string</p>\n\n<pre><code>\"The quoted text 'AA XX' should be tokenized\"\n</code></pre>\n\n<p>and expecting to extract </p>\n\n<pre><code>[The, quoted, text, 'AA XX', should, be, tokenized]\n</code></pre>\n\n<p>I however get some strange results while experimenting.  Noun chunks and ents looses one of the quote.  </p>\n\n<pre><code>import spacy\nnlp = spacy.load('en')\ns = \"The quoted text 'AA XX' should be tokenized\"\ndoc = nlp(s)\nprint([t for t in doc])\nprint([t for t in doc.noun_chunks])\nprint([t for t in doc.ents])\n</code></pre>\n\n<p>Result</p>\n\n<pre><code>[The, quoted, text, ', AA, XX, ', should, be, tokenized]\n[The quoted text 'AA XX]\n[AA XX']\n</code></pre>\n\n<p>What is the best way to address what I need</p>\n",
    "score": 7,
    "creation_date": 1528422489,
    "view_count": 5042,
    "answer_count": 1,
    "tags": "python-3.x;nlp;spacy"
  },
  {
    "question_id": 37939341,
    "title": "How do I get started with a project on Text Summarization using NLP?",
    "body": "<p>My final year engineering project requires me to build an application using Java or Python which summarizes a text document using Natural Language Processing. How do I even begin with the programming of such an application? </p>\n\n<p>Based on some research, I've just noted down that extraction-based summarization will be the best bet for me since it isn't so complex as abstraction based algorithms. Even then, it'd be really helpful if someone would guide me in the right direction to go about this.  </p>\n",
    "score": 7,
    "creation_date": 1466498041,
    "view_count": 7744,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 33139531,
    "title": "Preserve empty lines with NLTK&#39;s Punkt Tokenizer",
    "body": "<p>I'm using the NLTK's PUNKT sentence tokenizer to split a file into a list of sentences, and would like to preserve the empty lines within the file:</p>\n\n<pre><code>from nltk import data\ntokenizer = data.load('tokenizers/punkt/english.pickle')\ns = \"That was a very loud beep.\\n\\n I don't even know\\n if this is working. Mark?\\n\\n Mark are you there?\\n\\n\\n\"\nsentences = tokenizer.tokenize(s)\nprint sentences\n</code></pre>\n\n<p>I would like this to print:</p>\n\n<pre><code>['That was a very loud beep.\\n\\n', \"I don't even know\\n if this is working.\", 'Mark?\\n\\n', 'Mark are you there?\\n\\n\\n']\n</code></pre>\n\n<p>But the content that's actually printed shows that the trailing empty lines have been removed from the first and third sentences:</p>\n\n<pre><code>['That was a very loud beep.', \"I don't even know\\n if this is working.\", 'Mark?', 'Mark are you there?\\n\\n\\n']\n</code></pre>\n\n<p><a href=\"http://www.nltk.org/_modules/nltk/tokenize/simple.html#LineTokenizer.tokenize\" rel=\"noreferrer\">Other tokenizers</a> in NLTK have a <code>blanklines='keep'</code> parameter, but I don't see any such option in the case of the Punkt tokenizer. It's very possible I'm missing something simple. Is there a way to retrain these trailing empty lines using the Punkt sentence tokenizer? I'd be grateful for any insights others can offer!</p>\n",
    "score": 7,
    "creation_date": 1444880769,
    "view_count": 4027,
    "answer_count": 4,
    "tags": "python;nlp;newline;nltk;line-breaks"
  },
  {
    "question_id": 18672082,
    "title": "How to get n-gram collocations and association in python nltk?",
    "body": "<p>In <a href=\"http://nltk.googlecode.com/svn/trunk/doc/howto/collocations.html\" rel=\"noreferrer\">this documentation</a>, there is example using <code>nltk.collocations.BigramAssocMeasures()</code>, <code>BigramCollocationFinder</code>,<code>nltk.collocations.TrigramAssocMeasures()</code>, and <code>TrigramCollocationFinder</code>.</p>\n\n<p>There is example method find nbest based on pmi for bigram and trigram.\nexample: </p>\n\n<pre><code>finder = BigramCollocationFinder.from_words(\n...     nltk.corpus.genesis.words('english-web.txt'))\n&gt;&gt;&gt; finder.nbest(bigram_measures.pmi, 10)\n</code></pre>\n\n<p>I know that <code>BigramCollocationFinder</code> and <code>TrigramCollocationFinder</code> inherit from <code>AbstractCollocationFinder.</code> While <code>BigramAssocMeasures()</code> and <code>TrigramAssocMeasures()</code> inherit from <code>NgramAssocMeasures.</code></p>\n\n<p>How can I use the methods(e.g. <code>nbest()</code>) in <code>AbstractCollocationFinder</code> and <code>NgramAssocMeasures</code> for 4-gram, 5-gram, 6-gram, ...., n-gram (like using bigram and trigram easily)?</p>\n\n<p>Should I create class which inherit <code>AbstractCollocationFinder</code>?</p>\n\n<p>Thanks.</p>\n",
    "score": 7,
    "creation_date": 1378547928,
    "view_count": 9626,
    "answer_count": 2,
    "tags": "python;nlp;nltk;n-gram;collocation"
  },
  {
    "question_id": 12946373,
    "title": "How do I do use non-integer string labels with SVM from scikit-learn? Python",
    "body": "<p>Scikit-learn has fairly user-friendly python modules for machine learning.</p>\n\n<p>I am trying to train an SVM tagger for Natural Language Processing (NLP) where my labels and input data are words and annotation. E.g. Part-Of-Speech tagging, rather than using double/integer data as input tuples <code>[[1,2], [2,0]]</code>, my tuples will look like this <code>[['word','NOUN'], ['young', 'adjective']]</code></p>\n\n<p>Can anyone give an example of how i can use the SVM with string tuples? the tutorial/documentation given here are for integer/double inputs. <a href=\"http://scikit-learn.org/stable/modules/svm.html\">http://scikit-learn.org/stable/modules/svm.html</a></p>\n",
    "score": 7,
    "creation_date": 1350528811,
    "view_count": 9329,
    "answer_count": 2,
    "tags": "python;nlp;svm;scikit-learn;pos-tagger"
  },
  {
    "question_id": 8017432,
    "title": "Most efficient way to index words in a document?",
    "body": "<p>This came up in another question but I figured it is best to ask this as a separate question. Give a large list of sentences (order of 100 thousands):</p>\n\n<pre><code>[\n\"This is sentence 1 as an example\",\n\"This is sentence 1 as another example\",\n\"This is sentence 2\",\n\"This is sentence 3 as another example \",\n\"This is sentence 4\"\n]\n</code></pre>\n\n<p>what is the best way to code the following function?</p>\n\n<pre><code>def GetSentences(word1, word2, position):\n    return \"\"\n</code></pre>\n\n<p>where given two words, <code>word1</code>, <code>word2</code> and a position <code>position</code>, the function should return the list of all sentences satisfying that constraint. For example:</p>\n\n<pre><code>GetSentences(\"sentence\", \"another\", 3)\n</code></pre>\n\n<p>should return sentences <code>1</code> and <code>3</code> as the index of the sentences. My current approach was using a dictionary like this:</p>\n\n<pre><code>Index = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: [])))\n\nfor sentenceIndex, sentence in enumerate(sentences):\n    words = sentence.split()\n    for index, word in enumerate(words):\n        for i, word2 in enumerate(words[index:):\n            Index[word][word2][i+1].append(sentenceIndex)\n</code></pre>\n\n<p>But this quickly blows everything out of proportion on a dataset that is about 130 MB in size as my 48GB RAM is exhausted in less than 5 minutes. I somehow get a feeling this is a common problem but can't find any references on how to solve this efficiently. Any suggestions on how to approach this?</p>\n",
    "score": 7,
    "creation_date": 1320455395,
    "view_count": 3785,
    "answer_count": 2,
    "tags": "python;text;nlp"
  },
  {
    "question_id": 2705888,
    "title": "RDF representation of sentences",
    "body": "<p>I need to represent sentences in RDF format.  </p>\n\n<p>In other words \"John likes coke\" would be automatically represented as:</p>\n\n<pre><code>Subject : John\nPredicate : Likes\nObject : Coke\n</code></pre>\n\n<p>Does anyone know where I should start? Are there any programs which can do this automatically or would I need to do everything from scratch?</p>\n",
    "score": 7,
    "creation_date": 1272138344,
    "view_count": 3698,
    "answer_count": 2,
    "tags": "nlp;artificial-intelligence;rdf"
  },
  {
    "question_id": 75042153,
    "title": "Can&#39;t load from AutoTokenizer.from_pretrained - TypeError: duplicate file name (sentencepiece_model.proto)",
    "body": "<p>I'm trying to load tokenizer and seq2seq model from pretrained models.</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;ozcangundes/mt5-small-turkish-summarization&quot;)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(&quot;ozcangundes/mt5-small-turkish-summarization&quot;)\n</code></pre>\n<p>But I got this error.</p>\n<pre><code>File ~/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py:1028, in FileDescriptor.__new__(cls, name, package, options, serialized_options, serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)\n   1026     raise RuntimeError('Please link in cpp generated lib for %s' % (name))\n   1027 elif serialized_pb:\n-&gt; 1028   return _message.default_pool.AddSerializedFile(serialized_pb)\n   1029 else:\n   1030   return super(FileDescriptor, cls).__new__(cls)\n\n    TypeError: Couldn't build proto file into descriptor pool: duplicate file name (sentencepiece_model.proto)\n</code></pre>\n<p>I tried updating or downgrading the protobuf version. But I couldn't fix</p>\n",
    "score": 7,
    "creation_date": 1673110584,
    "view_count": 5198,
    "answer_count": 2,
    "tags": "python;nlp;protocol-buffers;huggingface"
  },
  {
    "question_id": 69240815,
    "title": "I am trying to import:from torchtext.legacy.data import Field, BucketIterator,Iterator,data, but get error &#39;No module named &#39;torchtext.legacy&#39;",
    "body": "<p>I am trying to execute the following code for a nlp proj</p>\n<pre><code>import torchtext\nfrom torchtext.legacy.data import Field, BucketIterator, Iterator\nfrom torchtext.legacy import data\n\n\n----&gt; 6 from torchtext.legacy.data import Field, BucketIterator, Iterator\n      7 from torchtext.legacy import data\n      8 \n\nModuleNotFoundError: No module named 'torchtext.legacy'.\n</code></pre>\n<p>I have tried it on both kaggle notebook and jupyter notebook and found the same error in both.\ni even tried to install !pip install -qqq deepmatcher==0.1.1 in kaggle to solve the issue but it still gives the same error.\nis there any solution to this?</p>\n",
    "score": 7,
    "creation_date": 1632032414,
    "view_count": 39261,
    "answer_count": 3,
    "tags": "python;nlp;pytorch"
  },
  {
    "question_id": 50264487,
    "title": "What&#39;s a good measure for classifying text documents?",
    "body": "<p>I have written an application that measures text importance. It takes a text article, splits it into words, drops stopwords, performs stemming, and counts word-frequency and document-frequency. Word-frequency is a measure that counts how many times the given word appeared in all documents, and document-frequency is a measure that counts how many documents the given word appeared.</p>\n\n<p>Here's an example with two text articles:</p>\n\n<ul>\n<li>Article I) \"A fox jumps over another fox.\"</li>\n<li>Article II) \"A hunter saw a fox.\"</li>\n</ul>\n\n<p>Article I gets split into words (afters stemming and dropping stopwords): </p>\n\n<ul>\n<li>[\"fox\", \"jump\", \"another\", \"fox\"].</li>\n</ul>\n\n<p>Article II gets split into words:</p>\n\n<ul>\n<li>[\"hunter\", \"see\", \"fox\"].</li>\n</ul>\n\n<p>These two articles produce the following word-frequency and document-frequency counters:</p>\n\n<ul>\n<li><code>fox</code> (word-frequency: 3, document-frequency: 2)</li>\n<li><code>jump</code> (word-frequency: 1, document-frequency: 1) </li>\n<li><code>another</code> (word-frequency: 1, document-frequency: 1) </li>\n<li><code>hunter</code> (word-frequency: 1, document-frequency: 1) </li>\n<li><code>see</code> (word-frequency: 1, document-frequency: 1) </li>\n</ul>\n\n<p>Given a new text article, how do I measure how similar this article is to previous articles?</p>\n\n<p>I've read about df-idf measure but it doesn't apply here as I'm dropping stopwords, so words like \"a\" and \"the\" don't appear in the counters.</p>\n\n<p>For example, I have a new text article that says \"hunters love foxes\", how do I come up with a measure that says this article is pretty similar to ones previously seen?</p>\n\n<p>Another example, I have a new text article that says \"deer are funny\", then this one is a totally new article and similarity should be 0.</p>\n\n<p>I imagine I somehow need to sum word-frequency and document-frequency counter values but what's a good formula to use?</p>\n",
    "score": 7,
    "creation_date": 1525918310,
    "view_count": 756,
    "answer_count": 7,
    "tags": "text;nlp;similarity;words;measure"
  },
  {
    "question_id": 7143723,
    "title": "Applying SVD throws a Memory Error instantaneously?",
    "body": "<p>I am trying to apply SVD on my matrix (3241 x 12596) that was obtained after some text processing (with the ultimate goal of performing Latent Semantic Analysis) and I am unable to understand why this is happening as my 64-bit machine has 16GB RAM. The moment <code>svd(self.A)</code> is called, it throws an error. The precise error is given below:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \".\\SVD.py\", line 985, in &lt;module&gt;\n    _svd.calc()\n  File \".\\SVD.py\", line 534, in calc\n    self.U, self.S, self.Vt = svd(self.A)\n  File \"C:\\Python26\\lib\\site-packages\\scipy\\linalg\\decomp_svd.py\", line 81, in svd\n    overwrite_a = overwrite_a)\nMemoryError\n</code></pre>\n\n<p>So I tried using</p>\n\n<pre><code>self.U, self.S, self.Vt = svd(self.A, full_matrices= False)\n</code></pre>\n\n<p>and this time, it throws the following error:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \".\\SVD.py\", line 985, in &lt;module&gt;\n    _svd.calc()\n  File \".\\SVD.py\", line 534, in calc\n    self.U, self.S, self.Vt = svd(self.A, full_matrices= False)\n  File \"C:\\Python26\\lib\\site-packages\\scipy\\linalg\\decomp_svd.py\", line 71, in svd\n    return numpy.linalg.svd(a, full_matrices=0, compute_uv=compute_uv)\n  File \"C:\\Python26\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 1317, in svd\n    work = zeros((lwork,), t)\nMemoryError\n</code></pre>\n\n<p>Is this supposed to be such a large matrix that Numpy cannot handle and is there something that I can do at this stage without changing the methodology itself?</p>\n",
    "score": 7,
    "creation_date": 1313993326,
    "view_count": 7662,
    "answer_count": 2,
    "tags": "python;memory;numpy;nlp;scipy"
  },
  {
    "question_id": 6252236,
    "title": "using python nltk to find similarity between two web pages?",
    "body": "<p>I want to find whether two web pages are similar or not. Can someone suggest if python nltk with wordnet similarity functions helpful and how? What is the best similarity function to be used in this case?</p>\n",
    "score": 7,
    "creation_date": 1307364446,
    "view_count": 6944,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 1104429,
    "title": "How can I create relative/approximate dates in Perl?",
    "body": "<p>I'd like to know if there are any libraries (preferably <a href=\"http://search.cpan.org/~drolsky/DateTime-0.50/lib/DateTime.pm\" rel=\"nofollow noreferrer\">DateTime</a>-esque) that can take a normal date time and create an appropriate relative human readable date.\nEssentially the exact opposite of the more common question: <a href=\"https://stackoverflow.com/questions/296738/how-can-i-parse-relative-dates-with-perl\">How can I parse relative dates with Perl?</a>.</p>\n\n<p>Obviously, the exact wording/interpretation is up to the actual implementation, but I'm looking to provide a consistent way to specify dates in the future. Knowing an apporximation like \"<code>due in 2 weeks</code>\" is (to me) more helpful in getting a grasp of how much time I have remaining than something \"<code>due on 2009-07-30</code>\".</p>\n\n<p>Examples:</p>\n\n<pre><code>2009-07-06        =&gt; \"in 1 year\"\n2009-07-30        =&gt; \"in 2 weeks\"\n2009-07-09        =&gt; \"tomorrow\"\n2009-07-09 12:32  =&gt; \"tomorrow at 12:32\"\n2009-07-12 05:43  =&gt; \"monday morning\"\n2009-07-03 05:74  =&gt; \"6 days ago\"\n</code></pre>\n",
    "score": 7,
    "creation_date": 1247151524,
    "view_count": 970,
    "answer_count": 3,
    "tags": "perl;datetime;nlp;relative-date"
  },
  {
    "question_id": 60852962,
    "title": "Training time of gensim word2vec",
    "body": "<p>I'm training word2vec from scratch on 34 GB pre-processed MS_MARCO corpus(of 22 GB). (Preprocessed corpus is sentnecepiece tokenized and so its size is more) I'm training my word2vec model using following code : </p>\n\n<pre><code>from gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\n\nclass Corpus():\n    \"\"\"Iterate over sentences from the corpus.\"\"\"\n    def __init__(self):\n        self.files = [\n            \"sp_cor1.txt\",\n            \"sp_cor2.txt\",\n            \"sp_cor3.txt\",\n            \"sp_cor4.txt\",\n            \"sp_cor5.txt\",\n            \"sp_cor6.txt\",\n            \"sp_cor7.txt\",\n            \"sp_cor8.txt\"\n        ]\n\n    def __iter__(self):\n        for fname in self.files:\n            for line in open(fname):\n                words = line.split()\n                yield words\n\nsentences = Corpus()\n\nmodel = Word2Vec(sentences, size=300, window=5, min_count=1, workers=8, sg=1, hs=1, negative=10)\nmodel.save(\"word2vec.model\")\n\n</code></pre>\n\n<p>My model is running now for about more than 30 hours now. This is doubtful since on my i5 laptop with 8 cores, I'm using all the 8 cores at 100% for every moment of time. Plus, my program seems to have read more than 100 GB of data from the disk now. I don't know if there is anything wrong here, but the main reason after my doubt on the training is because of this 100 GB of read from the disk. The whole corpus is of 34 GB, then why my code has read 100 GB of data from the disk? Does anyone know how much time should it take to train word2vec on 34 GB of text, with 8 cores of i5 CPU running all in parallel? Thank you. For more information, I'm also attaching the photo of my process from system monitor. </p>\n\n<p><a href=\"https://i.sstatic.net/QrJRM.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/QrJRM.png\" alt=\"enter image description here\"></a></p>\n\n<p>I want to know why my model has read 112 GB from memory, even when my corpus is of 34 GB in total? Will my training ever get finished? Also I'm bit worried about health of my laptop, since it is running constantly at its peak capacity since last 30 hours. It is really hot now. \nShould I add any additional parameter in <code>Word2Vec</code> for quicker training without much performance loss?</p>\n",
    "score": 7,
    "creation_date": 1585153358,
    "view_count": 5894,
    "answer_count": 1,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 53743729,
    "title": "When to use GlobalAveragePooling1D and when to use GlobalMaxPooling1D while using Keras for an LSTM model?",
    "body": "<p>I have to make LSTM classification model for some text and I am confused between GlobalAveragePooling1D and GlobalMaxPooling1D in the pooling layer while using keras. Which one should I use and what are the things to consider while deciding a particular choice.</p>\n",
    "score": 7,
    "creation_date": 1544620038,
    "view_count": 5079,
    "answer_count": 1,
    "tags": "keras;nlp;lstm"
  },
  {
    "question_id": 40207422,
    "title": "Binary numbers instead of one hot vectors",
    "body": "<p>While doing logistic regression, it is common practice to use one hot vectors as desired result. So, <code>no of classes = no of nodes in output layer</code>. We don't use index of word in vocabulary(or a class number in general) because that may falsely indicate closeness of two classes. But why can't we use binary numbers instead of one-hot vectors?</p>\n\n<p>i.e if there are 4 classes, we can represent each class as 00,01,10,11 resulting in <code>log(no of classes)</code> nodes in output layer.</p>\n",
    "score": 7,
    "creation_date": 1477253999,
    "view_count": 4557,
    "answer_count": 2,
    "tags": "machine-learning;nlp;computer-vision;neural-network"
  },
  {
    "question_id": 17245123,
    "title": "Getting adjective from an adverb in nltk or other NLP library",
    "body": "<p>Is there a way to get an adjective corresponding to a given adverb in NLTK or other python library.\nFor example, for the adverb \"<strong>terribly</strong>\", I need to get \"<strong>terrible</strong>\".\nThanks.</p>\n",
    "score": 7,
    "creation_date": 1371852952,
    "view_count": 4366,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 76675018,
    "title": "How does one use accelerate with the hugging face (HF) trainer?",
    "body": "<p>What are the code changes one has to do to run accelerate with a trianer?\nI keep seeing:</p>\n<pre><code>from accelerate import Accelerator\n\naccelerator = Accelerator()\n\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(\n    model, optimizer, training_dataloader, scheduler\n)\n\nfor batch in training_dataloader:\n    optimizer.zero_grad()\n    inputs, targets = batch\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    accelerator.backward(loss)\n    optimizer.step()\n    scheduler.step()\n</code></pre>\n<p>but when I tried the analogous thing it didn't work:</p>\n<pre><code>!pip\ninstall\naccelerate\n!pip\ninstall\ndatasets\n!pip\ninstall\ntransformers\n\n# %%\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast, TrainingArguments, Trainer\n\n# Initialize accelerator\naccelerator = Accelerator()\n\n# Specify dataset\ndataset = load_dataset('imdb')\n\n# Specify tokenizer and model\ntokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.to(accelerator.device)\n\n\n# Tokenize and format dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[&quot;text&quot;], truncation=True, max_length=512)\n\n\ntokenized_datasets = dataset.map(\n    tokenize_function,\n    batched=True,\n    num_proc=accelerator.num_processes,\n    remove_columns=[&quot;text&quot;]\n)\n\n# Training configuration\ntraining_args = TrainingArguments(\n    output_dir=&quot;output&quot;,\n    overwrite_output_dir=True,\n    # num_train_epochs=3,\n    max_steps=10,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=2,\n    save_steps=10_000,\n    save_total_limit=2,\n    prediction_loss_only=True,\n    fp16=False,  # Set to True for mixed precision training (FP16)\n    fp16_full_eval=False,  # Set to True for mixed precision evaluation (FP16)\n    dataloader_num_workers=accelerator.num_processes,  # Use multiple processes for data loading\n)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[&quot;train&quot;],\n    eval_dataset=tokenized_datasets[&quot;test&quot;],\n    tokenizer=tokenizer,\n)\n\n# Train model\ntrainer.train()\n\n</code></pre>\n<p>why?</p>\n<p>related:</p>\n<ul>\n<li><a href=\"https://discuss.huggingface.co/t/trainer-and-accelerate/26382/5\" rel=\"noreferrer\">https://discuss.huggingface.co/t/trainer-and-accelerate/26382/5</a></li>\n</ul>\n",
    "score": 7,
    "creation_date": 1689204347,
    "view_count": 12927,
    "answer_count": 3,
    "tags": "pytorch;nlp;huggingface-transformers;huggingface;accelerate"
  },
  {
    "question_id": 76198051,
    "title": "How to add new tokens to an existing Huggingface tokenizer?",
    "body": "<h1>How to add new tokens to an existing Huggingface AutoTokenizer?</h1>\n<p>Canonically, there's this tutorial from Huggingface <a href=\"https://huggingface.co/learn/nlp-course/chapter6/2\" rel=\"noreferrer\">https://huggingface.co/learn/nlp-course/chapter6/2</a> but it ends on the note of &quot;quirks when using existing tokenizers&quot;. And then it points to the <code>train_new_from_iterator()</code> function in Chapter 7 but I can't seem to find reference to how to use it to extend the tokenizer without re-training it.</p>\n<p><strong>I've tried</strong> the solution from <a href=\"https://stackoverflow.com/questions/71974438/training-new-autotokenizer-hugging-face\">Training New AutoTokenizer Hugging Face</a> that uses <code>train_new_from_iterator()</code> but that will re-train a tokenizer, but it is not extending it, the solution would replace the existing token indices. <a href=\"https://stackoverflow.com/questions/71974438/training-new-autotokenizer-hugging-face\">Training New AutoTokenizer Hugging Face</a></p>\n<pre><code>import pandas as pd\n\ndef batch_iterator(batch_size=3, size=8):\n        df = pd.DataFrame({&quot;note_text&quot;: ['foobar', 'helloworld']})\n        for x in range(0, size, batch_size):\n            yield df['note_text'].to_list()\n\nold_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\ntraining_corpus = batch_iterator()\nnew_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 32000)\n\nprint(len(old_tokenizer))\nprint(old_tokenizer( ['foobarzz', 'helloworld'] ))\nprint(new_tokenizer( ['foobarzz', 'hello world'] ))\n</code></pre>\n<p>[out]:</p>\n<pre><code>50265\n{'input_ids': [[0, 21466, 22468, 7399, 2], [0, 20030, 1722, 39949, 2]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}\n{'input_ids': [[0, 275, 2], [0, 276, 2]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}\n</code></pre>\n<p><strong>Note:</strong> The reason why the new tokens starts from 275 and 276 is because there are reserved tokens from ids 0-274.</p>\n<p>The expected behavior of <code>new_tokenizer( ['foo bar', 'hello word'] )</code> is to have IDs beyond the tokenizer vocab size (i.e. 50265 for the <code>roberta-base</code> model) and it should look like this:</p>\n<pre><code>{'input_ids': [[0, 50265, 2], [0, 50266, 2]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}\n</code></pre>\n\n",
    "score": 7,
    "creation_date": 1683528092,
    "view_count": 12003,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers;huggingface-tokenizers;large-language-model"
  },
  {
    "question_id": 49483971,
    "title": "Train Spacy NER on Indian Names",
    "body": "<p>I am trying to customize Spacy's NER to identify Indian names.\nFollowing this guide <a href=\"https://spacy.io/usage/training\" rel=\"noreferrer\">https://spacy.io/usage/training</a> and this is the dataset I am using <a href=\"https://gist.githubusercontent.com/mbejda/9b93c7545c9dd93060bd/raw/b582593330765df3ccaae6f641f8cddc16f1e879/Indian-Female-Names.csv\" rel=\"noreferrer\">https://gist.githubusercontent.com/mbejda/9b93c7545c9dd93060bd/raw/b582593330765df3ccaae6f641f8cddc16f1e879/Indian-Female-Names.csv</a></p>\n\n<p>As per the code , I am supposed to provide training data in following format:</p>\n\n<pre><code>TRAIN_DATA = [\n    ('Shivani', {\n        'entities': [(0, 6, 'PERSON')]\n    }),\n    ('Isha ', {\n        'entities': [(0,3 , 'PERSON')]\n    })\n]\n</code></pre>\n\n<p>How do I provide training data to Spacy for ~12000 names as manually specifying each entity will be a chore? Is there any other tool available to tag all the names ?</p>\n",
    "score": 7,
    "creation_date": 1522038348,
    "view_count": 6545,
    "answer_count": 4,
    "tags": "python;python-3.x;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 38986235,
    "title": "Spacy Pipeline?",
    "body": "<p>So lately I've been playing around with a WikiDump.\nI preprocessed it and trained it on Word2Vec + Gensim</p>\n\n<p>Does anyone know if there is only one script within Spacy that would generate\ntokenization, sentence recognition, part of speech tagging, lemmatization, dependency parsing, and named entity recognition all at once</p>\n\n<p>I have not been able to find clear documentation\nThank you </p>\n",
    "score": 7,
    "creation_date": 1471392155,
    "view_count": 2067,
    "answer_count": 3,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 38982423,
    "title": "OpenNLP lemmatization example",
    "body": "<p>Does anyone know where I can find an example of how to use the <code>SimpleLemmatizer</code> class in the OpenNLP library, and where I can find a sample english dictionary? It appears to be missing from the documentation.</p>\n",
    "score": 7,
    "creation_date": 1471373485,
    "view_count": 7235,
    "answer_count": 2,
    "tags": "nlp;opennlp"
  },
  {
    "question_id": 32545180,
    "title": "From of list of strings, identify which are human names and which are not",
    "body": "<p>I have a vector like the one below and would like to determine which elements in the list are human names and which are not. I found the humaniformat package, which formats names but unfortunately does not determine if a string is in fact a name. I also found a few packages for entity extraction, but they seem to require actual text for part-of-speech tagging, rather than a single name. </p>\n\n<p><strong>Example</strong></p>\n\n<pre><code>pkd.names.quotes &lt;- c(\"Mr. Rick Deckard\", # Name\n                      \"Do Androids Dream of Electric Sheep\", # Not a name\n                      \"Roy Batty\", # Name \n                      \"How much is an electric ostrich?\", # Not a name\n                      \"My schedule for today lists a six-hour self-accusatory depression.\", # Not a name\n                      \"Upon him the contempt of three planets descended.\", # Not a name\n                      \"J.F. Sebastian\", # Name\n                      \"Harry Bryant\", # Name\n                      \"goat class\", # Not a name\n                      \"Holden, Dave\", # Name\n                      \"Leon Kowalski\", # Name\n                      \"Dr. Eldon Tyrell\") # Name\n</code></pre>\n",
    "score": 7,
    "creation_date": 1442104610,
    "view_count": 5814,
    "answer_count": 1,
    "tags": "r;text;nlp;classification"
  },
  {
    "question_id": 27561971,
    "title": "how to create word vector",
    "body": "<p>How to create word vector? I used one hot key to create word vector, but it is very huge and not generalized for similar semantic word. So I have heard about word vector using neural network that finds word similarity and word vector. So I wanted to know how to generate this vector (algorithm) or good material to start creating word vector ?.</p>\n",
    "score": 7,
    "creation_date": 1418976474,
    "view_count": 11809,
    "answer_count": 2,
    "tags": "nlp;neural-network;word2vec"
  },
  {
    "question_id": 16883919,
    "title": "What&#39;s the difference between Stanford Tagger, Parser and CoreNLP?",
    "body": "<p>I'm currently using different tools from Stanford NLP Group and trying to understand the differences between them. It seems to me that somehow they intersect each other, since I can use same features in different tools (e.g. tokenize, and POS-Tag a sentence can be done by Stanford POS-Tagger, Parser and CoreNLP).</p>\n\n<p>I'd like to know what's the actual difference between each tool and in which situations I should use each of them.</p>\n",
    "score": 7,
    "creation_date": 1370184894,
    "view_count": 1991,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 10034881,
    "title": "n-gram name analysis in non-english languages (CJK, etc)",
    "body": "<p>I'm working on deduping a database of people. For a first pass, I'm following a basic 2-step process to avoid an O(n^2) operation over the whole database, as described <a href=\"http://nike.psu.edu/publications/jcdl05.pdf\" rel=\"nofollow noreferrer\">in the literature</a>. First, I \"block\"- iterate over the whole dataset, and bin each record based on n-grams AND initials present in the name. Second, all the records per bin are compared using Jaro-Winkler to get a measure of the likelihood of their representing the same person.</p>\n\n<p>My problem- the names are Unicode. Some (though not many) of these names are in CJK (Chinese-Japanese-Korean) languages. I have no idea how to <a href=\"https://stackoverflow.com/questions/1738788/python-split-unicode-string-on-word-boundaries\">find word boundaries</a> for something like initials in these languages. I have no idea whether n-gram analysis is valid on names in languages where names can be 2 characters. I also don't know if string edit-distance or other similarity metrics are valid in this context.</p>\n\n<p>Any ideas from linguist programmers or native speakers?</p>\n",
    "score": 7,
    "creation_date": 1333654498,
    "view_count": 2186,
    "answer_count": 3,
    "tags": "python;nlp;similarity;n-gram;cjk"
  },
  {
    "question_id": 595110,
    "title": "Stemming - code examples or open source projects?",
    "body": "<p>Stemming is something that's needed in tagging systems.  I use delicious, and I don't have time to manage and prune my tags.  I'm a bit more careful with my blog, but it isn't perfect.  I write software for embedded systems that would be much more functional (helpful to the user) if they included stemming.</p>\n\n<p>For instance:<br>\nParse<br>\nParser<br>\nParsing  </p>\n\n<p>Should all mean the same thing to whatever system I'm putting them into.</p>\n\n<p>Ideally there's a BSD licensed stemmer somewhere, but if not, where do I look to learn the common algorithms and techniques for this?</p>\n\n<p>Aside from BSD stemmers, what other open source licensed stemmers are out there?</p>\n\n<p>-Adam</p>\n",
    "score": 7,
    "creation_date": 1235746806,
    "view_count": 8325,
    "answer_count": 4,
    "tags": "algorithm;tags;nlp;stemming"
  },
  {
    "question_id": 39302880,
    "title": "Getting the root word using the Wordnet Lemmatizer",
    "body": "<p>I need to find a common root word matched for all related words for a keyword extractor.</p>\n\n<p>How to convert words into the same root using the python nltk lemmatizer? </p>\n\n<ul>\n<li>Eg: \n\n<ol>\n<li>generalized, generalization -> general </li>\n<li>optimal, optimized -> optimize (maybe) </li>\n<li>configure, configuration, configured -> configure</li>\n</ol></li>\n</ul>\n\n<p>The python nltk lemmatizer gives 'generalize', for 'generalized' and 'generalizing' when part of speech(pos) tag parameter is used but not for 'generalization'.</p>\n\n<p>Is there a way to do this?</p>\n",
    "score": 7,
    "creation_date": 1472872245,
    "view_count": 14626,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet;lemmatization"
  },
  {
    "question_id": 33594721,
    "title": "Why NLTK lemmatization has wrong output even if verb.exc has added right value?",
    "body": "<p>When I open verb.exc, I can see</p>\n\n<pre><code>saw see\n</code></pre>\n\n<p>While I use lemmatization in code</p>\n\n<pre><code>&gt;&gt;&gt;print lmtzr.lemmatize('saw', 'v')\nsaw\n</code></pre>\n\n<p>How can this happen? Do I misunderstand in revising wordNet?</p>\n",
    "score": 7,
    "creation_date": 1446990951,
    "view_count": 2351,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet;lemmatization"
  },
  {
    "question_id": 30995232,
    "title": "How to use OpenNLP to get POS tags in R?",
    "body": "<p>Here is the R Code:</p>\n\n<pre><code>library(NLP) \nlibrary(openNLP)\ntagPOS &lt;-  function(x, ...) {\ns &lt;- as.String(x)\nword_token_annotator &lt;- Maxent_Word_Token_Annotator()\na2 &lt;- Annotation(1L, \"sentence\", 1L, nchar(s))\na2 &lt;- annotate(s, word_token_annotator, a2)\na3 &lt;- annotate(s, Maxent_POS_Tag_Annotator(), a2)\na3w &lt;- a3[a3$type == \"word\"]\nPOStags &lt;- unlist(lapply(a3w$features, `[[`, \"POS\"))\nPOStagged &lt;- paste(sprintf(\"%s/%s\", s[a3w], POStags), collapse = \" \")\nlist(POStagged = POStagged, POStags = POStags)}\nstr &lt;- \"this is a the first sentence.\"\ntagged_str &lt;-  tagPOS(str)\n</code></pre>\n\n<p>Output is :</p>\n\n<blockquote>\n  <p>tagged_str\n      $POStagged\n      [1]\"this/DT is/VBZ a/DT the/DT first/JJ sentence/NN ./.\"</p>\n</blockquote>\n\n<p>Now I want to extract only NN word i.e sentence from the above sentence and want to store it into a variable .Can anyone help me out with this .</p>\n",
    "score": 7,
    "creation_date": 1435040398,
    "view_count": 13351,
    "answer_count": 3,
    "tags": "r;nlp;text-mining;opennlp;pos-tagger"
  },
  {
    "question_id": 27441191,
    "title": "Splitting chinese document into sentences",
    "body": "<p>I have to split Chinese text into multiple sentences. I tried the Stanford DocumentPreProcessor. It worked quite well for English but not for Chinese.</p>\n\n<p>Please can you let me know any good sentence splitters for Chinese preferably in Java or Python.</p>\n",
    "score": 7,
    "creation_date": 1418378671,
    "view_count": 7063,
    "answer_count": 3,
    "tags": "nlp;tokenize;stanford-nlp;sentence"
  },
  {
    "question_id": 25833693,
    "title": "Python faster alternative to dictionary?",
    "body": "<p>I'm making a simple sentiment mining system using a <code>Naive Bayes classifier</code>. </p>\n\n<p>For training my classifier, I have a text file where each line contains a list of tokens (generated from a tweet), and the associated sentiment (0 for -ve, 4 for positive).</p>\n\n<p>For example:</p>\n\n<pre><code>0 @ switchfoot http : //twitpic.com/2y1zl - Awww , that 's a bummer . You shoulda got David Carr of Third Day to do it . ; D\n0 spring break in plain city ... it 's snowing\n0 @ alydesigns i was out most of the day so did n't get much done\n0 some1 hacked my account on aim now i have to make a new one\n0 really do n't feel like getting up today ... but got to study to for tomorrows practical exam ...\n</code></pre>\n\n<p>Now, what I'm trying to do is for each token, count how many times it occurs in a positive tweet, and how many times it occurs in a negative tweet. I then plan to use these counts for calculating probabilities. I'm using the built-in dictionary for storing these counts. The keys are the tokens and the values are integer arrays of size 2.</p>\n\n<p>The problem is that this code starts off pretty fast, but keeps getting slower and when it has processed around 200 thousand tweets, it gets really slow - around 1 tweet per second. Since my training set has 1.6 million tweets, this is too slow.\nThe code I have is this:</p>\n\n<pre><code>def compute_counts(infile):\n    f = open(infile)\n    counts = {}\n    i = 0\n    for line in f:\n        i = i + 1\n        print(i)\n        words = line.split(' ')\n        for word in words[1:]:\n            word = word.replace('\\n', '').replace('\\r', '')\n            if words[0] == '0':\n                if word in counts.keys():\n                    counts[word][0] += 1\n                else:\n                    counts[word] = [1, 0]\n            else:\n                if word in counts.keys():\n                    counts[word][1] += 1\n                else:\n                    counts[word] = [0, 1]\n    return counts\n</code></pre>\n\n<p>What can I do to make this process faster? A better data structure? </p>\n\n<p>Edit: Not a duplicate, the question is not about something faster than dict in the general case, but in this specific use case.</p>\n",
    "score": 7,
    "creation_date": 1410700919,
    "view_count": 16730,
    "answer_count": 1,
    "tags": "python;performance;dictionary;nlp"
  },
  {
    "question_id": 19036288,
    "title": "Stemming unstructured text in NLTK",
    "body": "<p>I tried the regex stemmer, but I get hundreds of unrelated tokens. I'm just interested in the \"play\" stem. Here is the code I'm working with:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import nltk\nfrom nltk.book import *\nf = open('tupac_original.txt', 'rU')\ntext = f.read()\ntext1 = text.split()\ntup = nltk.Text(text1)\nlowtup = [w.lower() for w in tup if w.isalpha()]\nimport sys, re\ntupclean = [w for w in lowtup if not w in nltk.corpus.stopwords.words('english')]\nfrom nltk import stem\ntupstem = stem.RegexpStemmer('az$|as$|a$')\n[tupstem.stem(i) for i in tupclean] \n</code></pre>\n\n<p>The result of the above is;</p>\n\n<pre><code>['like', 'ed', 'young', 'black', 'like'...]\n</code></pre>\n\n<p>I'm trying to clean up <code>.txt</code> files (all lowercase, remove stopwords, etc), normalize multiple spellings of a word into one and do a frequency dist/count. I know how to do <code>FreqDist</code>, but any suggestions as to where I'm going wrong with the stemming?</p>\n",
    "score": 7,
    "creation_date": 1380221379,
    "view_count": 7637,
    "answer_count": 1,
    "tags": "nltk;tokenize;text-analysis;lemmatization"
  },
  {
    "question_id": 17625385,
    "title": "Tokenize, remove stop words using Lucene with Java",
    "body": "<p>I am trying to tokenize and remove stop words from a txt file with Lucene. I have this:</p>\n\n<pre><code>public String removeStopWords(String string) throws IOException {\n\nSet&lt;String&gt; stopWords = new HashSet&lt;String&gt;();\n    stopWords.add(\"a\");\n    stopWords.add(\"an\");\n    stopWords.add(\"I\");\n    stopWords.add(\"the\");\n\n    TokenStream tokenStream = new StandardTokenizer(Version.LUCENE_43, new StringReader(string));\n    tokenStream = new StopFilter(Version.LUCENE_43, tokenStream, stopWords);\n\n    StringBuilder sb = new StringBuilder();\n\n    CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);\n    while (tokenStream.incrementToken()) {\n        if (sb.length() &gt; 0) {\n            sb.append(\" \");\n        }\n        sb.append(token.toString());\n    System.out.println(sb);    \n    }\n    return sb.toString();\n}}\n</code></pre>\n\n<p>My main looks like this:</p>\n\n<pre><code>    String file = \"..../datatest.txt\";\n\n    TestFileReader fr = new TestFileReader();\n    fr.imports(file);\n    System.out.println(fr.content);\n\n    String text = fr.content;\n\n    Stopwords stopwords = new Stopwords();\n    stopwords.removeStopWords(text);\n    System.out.println(stopwords.removeStopWords(text));\n</code></pre>\n\n<p>This is giving me an error but I can't figure out why.</p>\n",
    "score": 7,
    "creation_date": 1373671049,
    "view_count": 13220,
    "answer_count": 3,
    "tags": "java;lucene;nlp;tokenize;stop-words"
  },
  {
    "question_id": 11535483,
    "title": "Fuzzy Group By, Grouping Similar Words",
    "body": "<p>this question is asked here before</p>\n\n<p><a href=\"https://stackoverflow.com/questions/6579263/what-is-a-good-strategy-to-group-similar-words\">What is a good strategy to group similar words?</a></p>\n\n<p>but no clear answer is given on how to \"group\" items. The solution based on difflib is basically search, for given item, difflib can return the most similar word out of a list. But how can this be used for grouping? </p>\n\n<p>I would like to reduce </p>\n\n<pre><code>['ape', 'appel', 'apple', 'peach', 'puppy']\n</code></pre>\n\n<p>to </p>\n\n<pre><code>['ape', 'appel', 'peach', 'puppy']\n</code></pre>\n\n<p>or</p>\n\n<pre><code>['ape', 'apple', 'peach', 'puppy']\n</code></pre>\n\n<p>One idea I tried was, for each item, iterate through the list, if get_close_matches returns more than one match, use it, if not keep the word as is. This partly worked, but it can suggest apple for appel, then appel for apple, these words would simply switch places and nothing would change. </p>\n\n<p>I would appreciate any pointers, names of libraries, etc.</p>\n\n<p>Note: also in terms of performance, we have a list of 300,000 items, and get_close_matches seems a bit slow. Does anyone know of a C/++ based solution out there? </p>\n\n<p>Thanks, </p>\n\n<p>Note: Further investigation revealed kmedoid is the right algorithm (as well as hierarchical clustering), since kmedoid does not require \"centers\", it takes / uses data points themselves as centers (these points are called medoids, hence the name). In word grouping case, the medoid would be the representative element of that group / cluster.</p>\n",
    "score": 7,
    "creation_date": 1342593024,
    "view_count": 10084,
    "answer_count": 5,
    "tags": "python;algorithm;machine-learning;nlp;cluster-analysis"
  },
  {
    "question_id": 10731645,
    "title": "Spell check and/or spell correction in Java",
    "body": "<p>How can I do spell checking and/or spell correction in a Java application?</p>\n",
    "score": 7,
    "creation_date": 1337837380,
    "view_count": 19121,
    "answer_count": 6,
    "tags": "java;nlp;spell-checking;languagetool"
  },
  {
    "question_id": 7942284,
    "title": "Standard C++ library for large scale data processing",
    "body": "<p>Could you please let me know some of the standard library of C++ useful for processing large scale data for example Natural Language Processing with huge data set,data set of protein protein interactions etc.</p>\n\n<p>Best,\nThetna</p>\n",
    "score": 7,
    "creation_date": 1319930173,
    "view_count": 1181,
    "answer_count": 2,
    "tags": "c++;nlp"
  },
  {
    "question_id": 7822922,
    "title": "noun countability",
    "body": "<p>Are there any recourses on determining the countability of nouns? Either some way to work it out or a dictionary that records whether a noun is likely to countable or not countable? </p>\n\n<p>I'm not interested in whether the noun can be countable but more is it likely to be countable. for instance rice can go to rices which means it can be countable but in most cases it wont be.</p>\n",
    "score": 7,
    "creation_date": 1319034721,
    "view_count": 1736,
    "answer_count": 4,
    "tags": "nlp"
  },
  {
    "question_id": 4583689,
    "title": "understanding semcor corpus structure h",
    "body": "<p>I'm learning NLP.  I currently playing with Word Sense Disambiguation.  I'm planning to use the semcor corpus as training data but I have trouble understanding the xml structure.  I tried googling but did not get any resource describing the content structure of semcor.</p>\n\n<pre><code>&lt;s snum=\"1\"&gt;\n&lt;wf cmd=\"ignore\" pos=\"DT\"&gt;The&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"group\" lexsn=\"1:03:00::\" pn=\"group\" pos=\"NNP\" rdf=\"group\" wnsn=\"1\"&gt;Fulton_County_Grand_Jury&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"say\" lexsn=\"2:32:00::\" pos=\"VB\" wnsn=\"1\"&gt;said&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"friday\" lexsn=\"1:28:00::\" pos=\"NN\" wnsn=\"1\"&gt;Friday&lt;/wf&gt;\n&lt;wf cmd=\"ignore\" pos=\"DT\"&gt;an&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"investigation\" lexsn=\"1:09:00::\" pos=\"NN\" wnsn=\"1\"&gt;investigation&lt;/wf&gt;\n&lt;wf cmd=\"ignore\" pos=\"IN\"&gt;of&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"atlanta\" lexsn=\"1:15:00::\" pos=\"NN\" wnsn=\"1\"&gt;Atlanta&lt;/wf&gt;\n&lt;wf cmd=\"ignore\" pos=\"POS\"&gt;'s&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"recent\" lexsn=\"5:00:00:past:00\" pos=\"JJ\" wnsn=\"2\"&gt;recent&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"primary_election\" lexsn=\"1:04:00::\" pos=\"NN\" wnsn=\"1\"&gt;primary_election&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"produce\" lexsn=\"2:39:01::\" pos=\"VB\" wnsn=\"4\"&gt;produced&lt;/wf&gt;\n&lt;punc&gt;``&lt;/punc&gt;\n&lt;wf cmd=\"ignore\" pos=\"DT\"&gt;no&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"evidence\" lexsn=\"1:09:00::\" pos=\"NN\" wnsn=\"1\"&gt;evidence&lt;/wf&gt;\n&lt;punc&gt;''&lt;/punc&gt;\n&lt;wf cmd=\"ignore\" pos=\"IN\"&gt;that&lt;/wf&gt;\n&lt;wf cmd=\"ignore\" pos=\"DT\"&gt;any&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"irregularity\" lexsn=\"1:04:00::\" pos=\"NN\" wnsn=\"1\"&gt;irregularities&lt;/wf&gt;\n&lt;wf cmd=\"done\" lemma=\"take_place\" lexsn=\"2:30:00::\" pos=\"VB\" wnsn=\"1\"&gt;took_place&lt;/wf&gt;\n&lt;punc&gt;.&lt;/punc&gt;\n&lt;/s&gt;\n</code></pre>\n\n<ul>\n<li>I'm assuming wnsn is 'word sense'.  Is it correct?</li>\n<li>What does the attribute lexsn mean? How does it map to wordnet?</li>\n<li>What does the attribute pn refer to? (third line)</li>\n<li>How is the rdf attribute assigned? (again third line)</li>\n<li>In general, what are the possible attributes?</li>\n</ul>\n",
    "score": 7,
    "creation_date": 1294050420,
    "view_count": 1920,
    "answer_count": 1,
    "tags": "linguistics;corpus;nlp"
  },
  {
    "question_id": 61988776,
    "title": "How to calculate perplexity for a language model using Pytorch",
    "body": "<p>I'm fine-tuning GPT-2 model for language generation task using huggingface Transformers library-pytorch, and I need to calculate an evaluation score(perplexity) for the fine-tuned model. But I'm not sure how that can be done using loss. I would like to know how perplexity can be calculated for the model with sum_loss or mean loss or any other suggestions are also welcome. Any help is appriciated.</p>\n\n<p><strong>Edit:</strong></p>\n\n<pre><code>outputs = model(article_tens, labels=article_tens)\n\n        loss, prediction_scores = outputs[:2]                        \n        loss.backward()\n        sum_loss = sum_loss + loss.detach().data\n</code></pre>\n\n<p>given above is how I calculate loss for each batch of data for the fine-tuning task.</p>\n\n<pre><code>sum loss 1529.43408203125\nloss 4.632936000823975\nprediction_scores tensor([[[-11.2291,  -9.2614, -11.8575,  ..., -18.1927, -17.7286, -11.9215],\n         [-67.2786, -63.5928, -70.7110,  ..., -75.3516, -73.8672, -67.6637],\n         [-81.1397, -80.0295, -82.9357,  ..., -83.7913, -85.7201, -78.9877],\n         ...,\n         [-85.3213, -82.5135, -86.5459,  ..., -90.9160, -90.4393, -82.3141],\n         [-44.2260, -43.1702, -49.2296,  ..., -58.9839, -55.2058, -42.3312],\n         [-63.2842, -59.7334, -61.8444,  ..., -75.0798, -75.7507, -54.0160]]],\n       device='cuda:0', grad_fn=&lt;UnsafeViewBackward&gt;)\n</code></pre>\n\n<p>The above given is when loss is printed for a batch only</p>\n",
    "score": 7,
    "creation_date": 1590336898,
    "view_count": 13164,
    "answer_count": 1,
    "tags": "nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 54819075,
    "title": "What are some of the ways to convert NLP to SQL?",
    "body": "<p>Recently, have started working on the idea of conversational chatbot and have been thinking of different ways to convert Natural Language query to SQL. \nThese are some of the libraries I have shortlisted to evaluate before writing from scratch. Any other ideas or suggestions?</p>\n\n<ul>\n<li><a href=\"https://github.com/FerreroJeremy/ln2sql\" rel=\"noreferrer\">https://github.com/FerreroJeremy/ln2sql</a></li>\n<li><a href=\"https://github.com/dadashkarimi/seq2sql\" rel=\"noreferrer\">https://github.com/dadashkarimi/seq2sql</a></li>\n<li><a href=\"https://github.com/xiaojunxu/SQLNet\" rel=\"noreferrer\">https://github.com/xiaojunxu/SQLNet</a></li>\n<li><a href=\"http://www.ling.helsinki.fi/kit/2009s/clt231/NLTK/book/ch10-AnalyzingTheMeaningOfSentences.html#querying-a-database\" rel=\"noreferrer\">http://www.ling.helsinki.fi/kit/2009s/clt231/NLTK/book/ch10-AnalyzingTheMeaningOfSentences.html#querying-a-database</a></li>\n</ul>\n",
    "score": 7,
    "creation_date": 1550801444,
    "view_count": 19972,
    "answer_count": 3,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 23703530,
    "title": "What features do NLP practitioners use to pick out English names?",
    "body": "<p>I am trying named entity recognition for the first time. I'm looking for features that will pick out English names. I am using the methods outlined in the <a href=\"https://class.coursera.org/nlp/lecture/preview\">coursera nlp course</a> (week three) and the <a href=\"http://www.nltk.org/book/ch06.html\">nltk book</a>. In other words: I am defining features, identifying features of words and then running those words/features through a classifier that I train on labeled data. </p>\n\n<p>What features are used to pick out English names?</p>\n\n<p>I can imagine that you'd look for two capital words in a row, or a capital word and then an initial and then a capital word. (ex. John Smith or James P. Smith).</p>\n\n<p>But what other features are used for NER?</p>\n",
    "score": 7,
    "creation_date": 1400269959,
    "view_count": 819,
    "answer_count": 2,
    "tags": "nlp;nltk"
  },
  {
    "question_id": 21321044,
    "title": "Extracting Important words from a sentence using Node",
    "body": "<p>I admit that I havent searched extensively in the SO database. I tried reading the natural npm package but doesnt seem to provide the feature. I would like to know if the below requirement is somewhat possible ? </p>\n\n<p>I have a database that has list of all cities of a country. I also have rating of these cities (best place to live, worst place to live, best rated city, worsrt rated city etc..). Now from the User interface, I would like to enable the user to enter free text and from there I should be able to search my database.</p>\n\n<p>For e.g Best place to live in California \nor places near California \nor places in California </p>\n\n<p>From the above sentence, I want to extract the nouns only (may be ) as this will be name of the city or country that I can search for. </p>\n\n<p>Then extract 'best' means I can sort is a particular order etc...</p>\n\n<p>Any suggestions or directions to look for? </p>\n\n<p>I risk a chance that the question will be marked as 'debatable'. But the reason I posted is to get some direction to proceed. </p>\n",
    "score": 7,
    "creation_date": 1390517675,
    "view_count": 9289,
    "answer_count": 3,
    "tags": "node.js;nlp"
  },
  {
    "question_id": 19145948,
    "title": "Converting an English Statement into a Questi0n",
    "body": "<p>(Apologies for the title. Stack overflow doesn't allow the word \"Question\" in titles.)</p>\n\n<p>How would one go about writing an algorithm to convert an english statement into a question? Where would one even begin? For example:</p>\n\n<p>\"<em>The ingredients for an omelette are eggs, bacon, cheese, and onions</em>\" would become \"<em>What are the ingredients for an omelette?</em>\" or \"<em>The ingredients for an omelette are what?</em>\"</p>\n\n<p>I can imagine parsing a sentence into it's components, and then re-arranging these while adding and removing words to form a grammatically correct sentence, but I'd have no idea where to start. I know this is by no means a trivial task, and I think the most helpful thing right now would be pointers to literature or similar problems.</p>\n",
    "score": 7,
    "creation_date": 1380744646,
    "view_count": 6882,
    "answer_count": 3,
    "tags": "algorithm;nlp;artificial-intelligence"
  },
  {
    "question_id": 17386394,
    "title": "Determining geo location by arbitrary body of text",
    "body": "<p>I am working on a project that I am not exactly sure how to approach. The problem can be summarized as following:</p>\n\n<ul>\n<li>Given an arbitrary body of text(kind of like a report), determine what geographic location that each part of the report is referring to.</li>\n</ul>\n\n<p>Geographic locations range from states to counties(all within US), so their number is limited, but each report generally contains references to multiple locations. For example, first 5 paragraphs of report might be about a state as a whole, and then then next 5 would be about individual counties within that state, or something like that.</p>\n\n<p>I am curious what would be the best way of approaching a problem like that, perhaps with a specific recommendation in terms of NLP or ML frameworks(Python or Java)?</p>\n",
    "score": 7,
    "creation_date": 1372557152,
    "view_count": 3479,
    "answer_count": 4,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 6585728,
    "title": "Which classifier to choose in NLTK",
    "body": "<p>I want to classify text messages into several categories like, \"relation building\", \"coordination\", \"information sharing\", \"knowledge sharing\" &amp; \"conflict resolution\". I am using NLTK library to process these data. I would like to know which classifier, in nltk, is better for this particular multi-class classification problem. </p>\n\n<p>I am planning to use Naive Bayes Classification, is it advisable? </p>\n",
    "score": 7,
    "creation_date": 1309882479,
    "view_count": 5241,
    "answer_count": 2,
    "tags": "nlp;classification;nltk"
  },
  {
    "question_id": 5449098,
    "title": "Stanford Parser: how to extract dependencies?",
    "body": "<p>My work consists in finding a query (can be <code>noun+verb</code>) in a sentence, then extract the object.</p>\n\n<p>exemple: <code>\"coding is sometimes a tough work.\"</code> My query would be: <code>\"coding is\"</code>.</p>\n\n<p>the typed dependencies i get are:</p>\n\n<pre><code>nsubj(work-6, coding-1)   \ncop(work-6, is-2)    \nadvmod(work-6, sometimes-3)\ndet(work-6, a-4)\namod(work-6, tough-5)\n</code></pre>\n\n<p>My program should extract the nsubj dependency, identify <code>\"coding\"</code> as the query and save <code>\"work\"</code>.  </p>\n\n<p>May be this seems simple, but until now, i didn't find a method able to extract a specific typed dependency, and I really need this to finish my work.</p>\n\n<p>Any help is welcome,</p>\n",
    "score": 7,
    "creation_date": 1301227599,
    "view_count": 7400,
    "answer_count": 2,
    "tags": "parsing;nlp;stanford-nlp"
  },
  {
    "question_id": 2065862,
    "title": "How to choose a Feature Selection Algorithm? - advice",
    "body": "<p>Is there a research paper/book that I can read which can tell me for the problem at hand what sort of feature selection algorithm would work best. </p>\n\n<p>I am trying to simply identify twitter messages as pos/neg (to begin with). I started out with Frequency based feature selection (having started with NLTK book) but soon realised that for a similar problem various individuals have choosen different algorithms</p>\n\n<p>Although I can try Frequency based, mutual information, information gain and various other algorithms the list seems endless.. and was wondering if there an efficient way then trial and error. </p>\n\n<p>any advice</p>\n",
    "score": 7,
    "creation_date": 1263487906,
    "view_count": 1631,
    "answer_count": 3,
    "tags": "nlp;nltk;semantic-analysis"
  },
  {
    "question_id": 66150469,
    "title": "SpaCy 3 Transformer Vector Token Alignment",
    "body": "<p>I'm using the SpaCy <code>3.0.1</code> together with the transformer model (<code>en_core_web_trf</code>).<br>\nWhen I previously used SpaCy transformers it was possible to get the transformer vectors from a <code>Token</code> or <code>Span</code>.\nIn SpaCy 3 however it seems like you can only access the transformer vectors via the <code>Doc</code> (<code>doc._.trf_data</code>) without a proper alignment to the SpaCy tokens.</p>\n<p>How can I get the alignment between SpaCy Tokens and Vectors/Wordpieces? <br>\nOr alternatively; is there some function that allows you to directly get the vectors for a <code>Token</code> or <code>Span</code>?</p>\n",
    "score": 7,
    "creation_date": 1613028686,
    "view_count": 1165,
    "answer_count": 1,
    "tags": "python;nlp;spacy;spacy-3"
  },
  {
    "question_id": 64881478,
    "title": "Passing multiple sentences to BERT?",
    "body": "<p>I have a dataset with paragraphs that I need to classify into two classes. These paragraphs are usually 3-5 sentences long. The overwhelming majority of them are less than 500 words long. I would like to make use of BERT to tackle this problem.</p>\n<p>I am wondering how I should use BERT to generate vector representations of these paragraphs and especially, whether it is fine to just pass the whole paragraph into BERT?</p>\n<p>There have been informative discussions of related problems <a href=\"https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification/63413589#63413589\">here</a> and <a href=\"https://stackoverflow.com/questions/63671085/how-to-use-bert-for-long-sentences\">here</a>. These discussions focus on how to use BERT for representing whole documents. In my case the paragraphs are not that long, and indeed could be passed to BERT without exceeding its maximum length of 512. However, BERT was trained on sentences. Sentences are relatively self-contained units of meaning. I wonder if feeding multiple sentences into BERT doesn't conflict fundamentally with what the model was designed to do (although this appears to be done regularly).</p>\n",
    "score": 7,
    "creation_date": 1605639015,
    "view_count": 7082,
    "answer_count": 1,
    "tags": "nlp;text-classification;bert-language-model;huggingface-transformers"
  },
  {
    "question_id": 60967134,
    "title": "Named Entity Recognition in aspect-opinion extraction using dependency rule matching",
    "body": "<p>Using Spacy, I extract aspect-opinion pairs from a text, based on the grammar rules that I defined. Rules are based on POS tags and dependency tags, which is obtained by <code>token.pos_</code> and <code>token.dep_</code>. Below is an example of one of the grammar rules. If I pass the sentence <code>Japan is cool,</code> it returns <code>[('Japan', 'cool', 0.3182)]</code>, where the value represents the polarity of <code>cool</code>.</p>\n\n<p>However I don't know how I can make it recognise the Named Entities. For example, if I pass <code>Air France is cool</code>, I want to get <code>[('Air France', 'cool', 0.3182)]</code> but what I currently get is <code>[('France', 'cool', 0.3182)]</code>. </p>\n\n<p>I checked Spacy online documentation and I know how to extract NE(<code>doc.ents</code>). But I want to know what the possible workaround is to make my extractor work. Please note that I don't want a forced measure such as concatenating strings <code>AirFrance</code>, <code>Air_France</code> etc.</p>\n\n<p>Thank you!</p>\n\n<pre><code>import spacy\n\nnlp = spacy.load(\"en_core_web_lg-2.2.5\")\nreview_body = \"Air France is cool.\"\ndoc=nlp(review_body)\n\nrule3_pairs = []\n\nfor token in doc:\n\n    children = token.children\n    A = \"999999\"\n    M = \"999999\"\n    add_neg_pfx = False\n\n    for child in children :\n        if(child.dep_ == \"nsubj\" and not child.is_stop): # nsubj is nominal subject\n            A = child.text\n\n        if(child.dep_ == \"acomp\" and not child.is_stop): # acomp is adjectival complement\n            M = child.text\n\n        # example - 'this could have been better' -&gt; (this, not better)\n        if(child.dep_ == \"aux\" and child.tag_ == \"MD\"): # MD is modal auxiliary\n            neg_prefix = \"not\"\n            add_neg_pfx = True\n\n        if(child.dep_ == \"neg\"): # neg is negation\n            neg_prefix = child.text\n            add_neg_pfx = True\n\n    if (add_neg_pfx and M != \"999999\"):\n        M = neg_prefix + \" \" + M\n\n    if(A != \"999999\" and M != \"999999\"):\n        rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))\n</code></pre>\n\n<p>Result</p>\n\n<pre><code>rule3_pairs\n&gt;&gt;&gt; [('France', 'cool', 0.3182)]\n</code></pre>\n\n<p>Desired output</p>\n\n<pre><code>rule3_pairs\n&gt;&gt;&gt; [('Air France', 'cool', 0.3182)]\n</code></pre>\n",
    "score": 7,
    "creation_date": 1585731563,
    "view_count": 1217,
    "answer_count": 1,
    "tags": "python;nlp;spacy;named-entity-recognition;dependency-parsing"
  },
  {
    "question_id": 52734146,
    "title": "Word2vec Gensim Accuracy Analysis",
    "body": "<p>I'm working on a NLP application, where I have a corpus of text files. I would like to create word vectors using the <strong>Gensim word2vec algorithm</strong>. </p>\n\n<p>I did a 90% training and 10% testing split. I trained the model on the appropriate set, but I would like to assess the accuracy of the model on the testing set.</p>\n\n<p>I have surfed the internet for any documentation on accuracy assessment, but I could not find any methods that allowed me to do so. Does anyone know of a function that does accuracy analysis?</p>\n\n<p>The way I processed my test data was that I extracted all the sentences from the text files in the test folder, and I turned it into a giant list of sentences. After that, I used a function that I though was the right one (turns out it wasn't as it gave me this error: <strong>TypeError: don't know how to handle uri</strong>). Here is how I went about doing this:</p>\n\n<pre><code>test_filenames = glob.glob('./testing/*.txt')\n\nprint(\"Found corpus of %s safety/incident reports:\" %len(test_filenames))\n\ntest_corpus_raw = u\"\"\nfor text_file in test_filenames:\n    txt_file = open(text_file, 'r')\n    test_corpus_raw += unicode(txt_file.readlines())\nprint(\"Test Corpus is now {0} characters long\".format(len(test_corpus_raw)))\n\ntest_raw_sentences = tokenizer.tokenize(test_corpus_raw)\n\ndef sentence_to_wordlist(raw):\n    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n    words = clean.split()\n    return words\n\ntest_sentences = []\nfor raw_sentence in test_raw_sentences:\n    if len(raw_sentence) &gt; 0:\n        test_sentences.append(sentence_to_wordlist(raw_sentence))\n\ntest_token_count = sum([len(sentence) for sentence in test_sentences])\nprint(\"The test corpus contains {0:,} tokens\".format(test_token_count))\n\n\n####### THIS LAST LINE PRODUCES AN ERROR: TypeError: don't know how to handle uri \ntexts2vec.wv.accuracy(test_sentences, case_insensitive=True)\n</code></pre>\n\n<p>I have no idea how to fix this last part. Please help. Thanks in advance!</p>\n",
    "score": 7,
    "creation_date": 1539153792,
    "view_count": 6901,
    "answer_count": 2,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 45855160,
    "title": "NLP - When to lowercase text during preprocessing",
    "body": "<p>I want to build a model for language modelling, which should predict the next words in a sentence, given the previous word(s) and/or the previous sentence.</p>\n\n<p><strong>Use case:</strong> I want to automate writing reports. So the model should automatically complete the sentence I am writing. Therefore, it is important that nouns and the words at the beginning of a sentence are capitalized. </p>\n\n<p><strong>Data</strong>: The data is in German and contains a lot of technical jargon.</p>\n\n<p>My text corpus is in <strong>German</strong> and I am currently working on the preprocessing. Because my model should predict gramatically correct sentences I have decided to use/not use the following preprocessing steps:</p>\n\n<ul>\n<li>no stopword removal</li>\n<li><p>no lemmatization</p></li>\n<li><p>replace all expressions with numbers by NUMBER</p></li>\n<li>normalisation of synonyms and abbreviations </li>\n<li>replace rare words with RARE</li>\n</ul>\n\n<p>However, I am not sure whether to convert the corpus to lowercase. When searching the web I found different opinions. Although lower-casing is quite common it will cause my model to wrongly predict the capitalization of nouns, sentence beginnings etc.</p>\n\n<p>I also found the idea to convert only the words at the beginning of a sentence to lower-case on the following <a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/capitalizationcase-folding-1.html\" rel=\"noreferrer\">Stanford page</a>.</p>\n\n<p>What is the best strategy for this use-case? Should I convert the text to lower-case and change the words to the correct case after prediction? Should I leave the capitalization as it is? Should I only lowercase words at the beginning of a sentence?</p>\n\n<p>Thanks a lot for any suggestions and experiences!</p>\n",
    "score": 7,
    "creation_date": 1503558661,
    "view_count": 14013,
    "answer_count": 3,
    "tags": "python;machine-learning;nlp;nltk"
  },
  {
    "question_id": 41682781,
    "title": "Why does Naive Bayes fail to solve XOR",
    "body": "<p>I have recently started understanding algorithms related to natural language processing, and have come across various sites which indicate that Naive Bayes cannot capture the XOR concept. Firstly I do not understand what exactly is the XOR problem. Can someone please explain, what the XOR problem is with a simple classification example if possible.</p>\n",
    "score": 7,
    "creation_date": 1484591138,
    "view_count": 3004,
    "answer_count": 1,
    "tags": "nlp;naivebayes"
  },
  {
    "question_id": 41400920,
    "title": "Search for job titles in an article using Spacy or NLTK",
    "body": "<p>I'm new to NLP and recently been playing with NTLK and Spacy. However, I could not find a way to search for job titles (ex: product manager, chief marketing officer, etc) in an article.</p>\n\n<p>Example, I have 1000 articles and I want to get all the articles that have job titles that I am interested in.</p>\n\n<p>Also, what entity type does job titles fall in? I check <a href=\"https://spacy.io/docs/usage/entity-recognition\" rel=\"noreferrer\">https://spacy.io/docs/usage/entity-recognition</a> and did not see it in there. I there a plan to add it?</p>\n\n<p>Thanks.</p>\n",
    "score": 7,
    "creation_date": 1483122426,
    "view_count": 6370,
    "answer_count": 2,
    "tags": "nlp;named-entity-recognition;spacy"
  },
  {
    "question_id": 34831167,
    "title": "WordNet - What does n and the number represent?",
    "body": "<p>My question is related to <a href=\"http://www.nltk.org/howto/wordnet.html\" rel=\"noreferrer\">WordNet Interface</a>.</p>\n\n<pre><code>   &gt;&gt;&gt; wn.synsets('cat')\n       [Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'),\n        Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), \n        Synset('caterpillar.n.02'), Synset('big_cat.n.01'), \n        Synset('computerized_tomography.n.01'), Synset('cat.v.01'), \n        Synset('vomit.v.01')]\n    &gt;&gt;&gt; \n</code></pre>\n\n<p>I could not find the answer to what is the purpose of <code>n</code> and the following <code>number</code> in <code>cat.n.01</code> or <code>caterpillar.n.02</code>.</p>\n",
    "score": 7,
    "creation_date": 1452972502,
    "view_count": 3219,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet;part-of-speech"
  },
  {
    "question_id": 33612296,
    "title": "Load Custom Dataset (which is like 20 news group set) in Scikit for Classification of text documents",
    "body": "<p>I'm trying to run <a href=\"http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\" rel=\"nofollow noreferrer\">this scikit example code</a> for my custom dataset of Ted Talks. \nEach Directory is a Topic under which are text files which contain the description for each Ted Talk.</p>\n\n<p>This is how my datasets tree structure is. As you see, each directory is a topic and below it are text files which carry description. </p>\n\n<pre><code>Topics/\n|-- Activism\n|   |-- 1149.txt\n|   |-- 1444.txt\n|   |-- 157.txt\n|   |-- 1616.txt\n|   |-- 1706.txt\n|   |-- 1718.txt\n|-- Adventure\n|   |-- 1036.txt\n|   |-- 1777.txt\n|   |-- 2930.txt\n|   |-- 2968.txt\n|   |-- 3027.txt\n|   |-- 3290.txt\n|-- Advertising\n|   |-- 3673.txt\n|   |-- 3685.txt\n|   |-- 6567.txt\n|   `-- 6925.txt\n|-- Africa\n|   |-- 1045.txt\n|   |-- 1072.txt\n|   |-- 1103.txt\n|   |-- 1112.txt\n|-- Aging\n|   |-- 1848.txt\n|   |-- 2495.txt\n|   |-- 2782.txt\n|-- Agriculture\n|   |-- 3469.txt\n|   |-- 4140.txt\n|   |-- 4733.txt\n|   |-- 4939.txt\n</code></pre>\n\n<p>I have made my dataset in such form to resemble the 20news group whose tree structure is such:</p>\n\n<pre><code>20news-18828/\n|-- alt.atheism\n|   |-- 49960\n|   |-- 51060\n|   |-- 51119\n\n|-- comp.graphics\n|   |-- 37261\n|   |-- 37913\n|   |-- 37914\n|   |-- 37915\n|   |-- 37916\n|   |-- 37917\n|   |-- 37918\n|-- comp.os.ms-windows.misc\n|   |-- 10000\n|   |-- 10001\n|   |-- 10002\n|   |-- 10003\n|   |-- 10004\n|   |-- 10005 \n</code></pre>\n\n<p>In the <a href=\"http://pastebin.com/wx3pcArQ\" rel=\"nofollow noreferrer\">original code</a> (98-124), This is how training and testing data is loaded directly from scikit. </p>\n\n<pre><code>print(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories if categories else \"all\")\n\ndata_train = fetch_20newsgroups(subset='train', categories=categories,\n                                shuffle=True, random_state=42,\n                                remove=remove)\n\ndata_test = fetch_20newsgroups(subset='test', categories=categories,\n                               shuffle=True, random_state=42,\n                               remove=remove)\nprint('data loaded')\n\ncategories = data_train.target_names    # for case categories == None\ndef size_mb(docs):\n    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n\ndata_train_size_mb = size_mb(data_train.data)\ndata_test_size_mb = size_mb(data_test.data)\n\nprint(\"%d documents - %0.3fMB (training set)\" % (\n    len(data_train.data), data_train_size_mb))\nprint(\"%d documents - %0.3fMB (test set)\" % (\n    len(data_test.data), data_test_size_mb))\nprint(\"%d categories\" % len(categories))\nprint()\n\n# split a training set and a test set\ny_train, y_test = data_train.target, data_test.target\n</code></pre>\n\n<p>Since this dataset was available with Scikit, Its labels etc were all built in. \nFor my case, I know how to load the dataset <a href=\"http://pastebin.com/1MGeL3D4\" rel=\"nofollow noreferrer\">(Line 84)</a>: </p>\n\n<pre><code>dataset = load_files('./TED_dataset/Topics/')\n</code></pre>\n\n<p>I have no idea what I should do after that. I want to know how I should split this data in training and testing and generate these labels from my dataset:  </p>\n\n<pre><code>data_train.data,  data_test.data \n</code></pre>\n\n<p>All in all, I just want to load my dataset, run it on this code error free. I have <a href=\"https://drive.google.com/file/d/0B0byAC8kZZPhYVJNVTdJa003U2M/view?usp=sharing\" rel=\"nofollow noreferrer\">uploaded the dataset here</a> for those who might want to see it. </p>\n\n<p>I have referred to <a href=\"https://stackoverflow.com/questions/27761803/problems-loading-textual-data-with-scikit-learn\">this question</a> which speaks briefly about test-train loading. I also want to know how data_train.target_names should be fetched from my dataset.</p>\n\n<p>Edit: </p>\n\n<p>I tried to get the train and test which returns error:</p>\n\n<pre><code>dataset = load_files('./TED_dataset/Topics/')\ntrain, test = train_test_split(dataset, train_size = 0.8)\n</code></pre>\n\n<p>Updated code is <a href=\"http://pastebin.com/DcPCwaVS\" rel=\"nofollow noreferrer\">here</a>. </p>\n",
    "score": 7,
    "creation_date": 1447082304,
    "view_count": 6278,
    "answer_count": 2,
    "tags": "python;machine-learning;dataset;nlp;scikit-learn"
  },
  {
    "question_id": 25424787,
    "title": "SPARQL queries with relational operator",
    "body": "<p>I want to use <strong>relational AND/OR operator</strong> onto <strong>SPARQL</strong> queries.</p>\n\n<p>Here query : </p>\n\n<pre><code>SELECT DISTINCT ?dbpedia_link str(?name) as ?label str(?label1) as ?label1 ?freebase_link WHERE {\n            ?dbpedia_link rdfs:label ?label1 . \n            ?dbpedia_link foaf:name ?name .\n            {\n                { ?dbpedia_link rdf:type dbpedia-owl:Film .}\n                UNION\n                { ?dbpedia_link rdf:type dbpedia-owl:Person .}\n            }\n            ?dbpedia_link owl:sameAs ?freebase_link .\n            FILTER regex(?freebase_link, \"^http://rdf.freebase.com\") .\n            FILTER (lang(?label1) = 'en'). \n            ?name bif:contains \"Akshay_Kumar\" . \n            ?dbpedia_link dcterms:subject ?sub \n        }\n</code></pre>\n\n<p>In this query, I have used <strong>Akshay_Kumar</strong> which is single name. Now I want that, how can I <strong>use place multiple names at once using relational AND/OR operator</strong>. In short, how can we use <strong>relational operators in sparql</strong>.</p>\n\n<p>Executing sparql query URL : <a href=\"http://dbpedia.org/sparql\" rel=\"noreferrer\">http://dbpedia.org/sparql</a></p>\n",
    "score": 7,
    "creation_date": 1408619237,
    "view_count": 6825,
    "answer_count": 1,
    "tags": "python;nlp;sparql;dbpedia"
  },
  {
    "question_id": 5318076,
    "title": "Extracting Country Name from Author Affiliations",
    "body": "<p>I am currently exploring the possibility of extracting country name from Author Affiliations (PubMed Articles) my sample data looks like:</p>\n\n<p><code>Mechanical and Production Engineering Department, National University of Singapore.</code></p>\n\n<p><code>Cancer Research Campaign Mammalian Cell DNA Repair Group, Department of Zoology, Cambridge, U.K.</code></p>\n\n<p><code>Cancer Research Campaign Mammalian Cell DNA Repair Group, Department of Zoology, Cambridge, UK.</code></p>\n\n<p><code>Lilly Research Laboratories, Eli Lilly and Company, Indianapolis, IN 46285.</code></p>\n\n<p>Initially I tried to remove punctuations and split the vector into words and then compared it with a list of country names from Wikipedia but I am not successful at this. </p>\n\n<p>Can anyone please suggest me a better way of doing it? I would prefer the solution in <code>R</code> as I have to do further analysis and generate graphics in <code>R</code>.  </p>\n",
    "score": 7,
    "creation_date": 1300223192,
    "view_count": 3183,
    "answer_count": 3,
    "tags": "r;text;nlp"
  },
  {
    "question_id": 4234356,
    "title": "Dictionary words for download",
    "body": "<p>Can someone offer a suggestion on where to find a dictionary word list with frequency information?</p>\n\n<p>Ideally, the source would be English words of the North American variety.</p>\n",
    "score": 7,
    "creation_date": 1290278812,
    "view_count": 7445,
    "answer_count": 4,
    "tags": "nlp;document-classification"
  },
  {
    "question_id": 70067608,
    "title": "How padding in huggingface tokenizer works?",
    "body": "<p>I tried following tokenization example:</p>\n<pre><code>tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)\nsent = &quot;I hate this. Not that.&quot;,        \n_tokenized = tokenizer(sent, padding=True, max_length=20, truncation=True)\nprint(_tknzr.decode(_tokenized['input_ids'][0]))\nprint(len(_tokenized['input_ids'][0]))\n</code></pre>\n<p>The output was:</p>\n<pre><code>[CLS] i hate this. not that. [SEP]\n9\n</code></pre>\n<p>Notice the parameter to <code>tokenizer</code>: <code>max_length=20</code>. How can I make Bert tokenizer to append 11 <code>[PAD]</code> tokens to this sentence to make it total <code>20</code>?</p>\n",
    "score": 7,
    "creation_date": 1637592234,
    "view_count": 17767,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;bert-language-model;transformer-model;huggingface-tokenizers"
  },
  {
    "question_id": 66302371,
    "title": "How to specify the loss function when finetuning a model using the Huggingface TFTrainer Class?",
    "body": "<p>I have followed the basic example as given below, from: <a href=\"https://huggingface.co/transformers/training.html\" rel=\"noreferrer\">https://huggingface.co/transformers/training.html</a></p>\n<pre><code>from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments\n\nmodel = TFBertForSequenceClassification.from_pretrained(&quot;bert-large-uncased&quot;)\n\ntraining_args = TFTrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total # of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n)\n\ntrainer = TFTrainer(\n    model=model,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=tfds_train_dataset,    # tensorflow_datasets training dataset\n    eval_dataset=tfds_test_dataset       # tensorflow_datasets evaluation dataset\n)\ntrainer.train()\n</code></pre>\n<p>But there seems to be no way to specify the loss function for the classifier. For-ex if I finetune on a binary classification problem, I would use</p>\n<pre><code>tf.keras.losses.BinaryCrossentropy(from_logits=True)\n</code></pre>\n<p>else I would use</p>\n<pre><code>tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n</code></pre>\n<p>My set up is as follows:</p>\n<pre><code>transformers==4.3.2\ntensorflow==2.3.1\npython==3.6.12\n</code></pre>\n",
    "score": 7,
    "creation_date": 1613911055,
    "view_count": 6435,
    "answer_count": 2,
    "tags": "python-3.x;tensorflow;nlp;huggingface-transformers"
  },
  {
    "question_id": 66193575,
    "title": "Why is the vocab size of Byte level BPE smaller than Unicode&#39;s vocab size?",
    "body": "<p>I recently read GPT2 and the paper says:</p>\n<blockquote>\n<p>This would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added.  This is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with BPE. In contrast, a byte-level version of BPE only requires a base vocabulary of size 256.</p>\n</blockquote>\n<p>I really don't understand the words. The number of characters that Unicode represents is 130K but how can this be reduced to 256? Where's the rest of approximately 129K characters? What am I missing? Does byte-level BPE allow duplicating of representation between different characters?</p>\n<p>I don't understand the logic. Below are my questions:</p>\n<ul>\n<li>Why the size of vocab is reduced? (from 130K to 256)</li>\n<li>What's the logic of the BBPE (Byte-level BPE)?</li>\n</ul>\n<hr />\n<h1>Detail question</h1>\n<p>Thank you for your answer but I really don't get it. Let's say we have 130K unique characters. What we want (and BBPE do) is to reduce this basic (unique) vocabulary. Each Unicode character can be converted 1 to 4 bytes by utilizing UTF-8 encoding. The original paper of BBPE says (Neural Machine Translation with Byte-Level Subwords):</p>\n<blockquote>\n<p>Representing text at the level of bytes and <strong>using the 256 bytes</strong> set as vocabulary is a potential solution to this issue.</p>\n</blockquote>\n<p>Each byte can represent 256 characters (bits, 2^8), we only need 2^17 (131072) bits for representing the unique Unicode characters. In this case, where did the <strong>256 bytes in the original paper</strong> come from? I don't know both the logic and how to derive this result.</p>\n<p>I arrange my questions again, more detail:</p>\n<ul>\n<li>How does BBPE work?</li>\n<li>Why the size of vocab is reduced? (from 130K to 256 bytes)\n<ul>\n<li>Anyway, we always need 130K space for a vocab. What's the difference between representing unique characters as Unicode and Bytes?</li>\n</ul>\n</li>\n</ul>\n<p>Since I have little knowledge of computer architecture and programming, please let me know if there's something I missed.</p>\n<p>Sincerely, thank you.</p>\n",
    "score": 7,
    "creation_date": 1613290654,
    "view_count": 4671,
    "answer_count": 4,
    "tags": "unicode;utf-8;nlp"
  },
  {
    "question_id": 65433239,
    "title": "Calculate cosine similarity for between all cases in a dataframe fast",
    "body": "<p>I'm working on an NLP project  where I have to compare the similarity between many sentences\nE.G. from this dataframe:</p>\n<p><a href=\"https://i.sstatic.net/lCJDb.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/lCJDb.png\" alt=\"enter image description here\" /></a></p>\n<p>The first thing I tried was to make a join of the dataframe with itself to get the bellow format and compare row by row:</p>\n<p><a href=\"https://i.sstatic.net/KMz2H.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/KMz2H.png\" alt=\"enter image description here\" /></a></p>\n<p>The problem with this that I get out of memory quickly for big medium/big datasets,\ne.g. for a 10k rows join I will get 100MM rows which I can not fit in ram</p>\n<p>My current aproach is to iterate over the dataframe with as:</p>\n<pre><code>final = pd.DataFrame()\n\n### for each row \nfor i in range(len(df_sample)):\n\n    ### select the corresponding vector to compare with \n    v =  df_sample[df_sample.index.isin([i])][&quot;use_vector&quot;].values\n    ### compare all cases agains the selected vector\n    df_sample.apply(lambda x:  cosine_similarity_numba(x.use_vector,v[0])  ,axis=1)\n\n    ### kept the cases with a similarity over a given th, in this case 0.6\n    temp = df_sample[df_sample.apply(lambda x:  cosine_similarity_numba(x.use_vector,v[0])  ,axis=1) &gt; 0.6]  \n    ###  filter out the base case \n    temp = temp[~temp.index.isin([i])]\n    temp[&quot;original_question&quot;] = copy.copy(df_sample[df_sample.index.isin([i])][&quot;questions&quot;].values[0])\n    ### append the result     \n    final = pd.concat([final,temp])\n</code></pre>\n<p>But this aproach is not fast either.\nHow can I improve the performance of this process?</p>\n",
    "score": 7,
    "creation_date": 1608776712,
    "view_count": 3299,
    "answer_count": 2,
    "tags": "python;pandas;numpy;nlp;linear-algebra"
  },
  {
    "question_id": 64712375,
    "title": "Fine-tune Bert for specific domain (unsupervised)",
    "body": "<p>I want to fine-tune BERT on texts that are related to a specific domain (in my case related to engineering). The training should be unsupervised since I don't have any labels or anything. Is this possible?</p>\n",
    "score": 7,
    "creation_date": 1604656480,
    "view_count": 2184,
    "answer_count": 1,
    "tags": "python;deep-learning;neural-network;nlp;bert-language-model"
  },
  {
    "question_id": 62082938,
    "title": "How to stop BERT from breaking apart specific words into word-piece",
    "body": "<p>I am using a pre-trained BERT model to tokenize a text into meaningful tokens. However, the text has many specific words and I don't want BERT model to break them into word-pieces. Is there any solution to it?\nFor example:</p>\n\n<pre><code>tokenizer = BertTokenizer('bert-base-uncased-vocab.txt')\ntokens = tokenizer.tokenize(\"metastasis\")\n</code></pre>\n\n<p>Create tokens like this:</p>\n\n<pre><code>['meta', '##sta', '##sis']\n</code></pre>\n\n<p>However, I want to keep the whole words as one token, like this:</p>\n\n<pre><code>['metastasis']\n</code></pre>\n",
    "score": 7,
    "creation_date": 1590745057,
    "view_count": 5639,
    "answer_count": 3,
    "tags": "python;text;nlp;tokenize;bert-language-model"
  },
  {
    "question_id": 59636002,
    "title": "Spacy lemmatization of a single word",
    "body": "<p>I am trying to get the lemmatized version of a single word.  Is there a way using \"spacy\" (fantastic python NLP library) to do this.</p>\n\n<p>Below is the code I have tried but this does not work):</p>\n\n<pre><code>from spacy.lemmatizer import Lemmatizer\nfrom spacy.lookups import Lookups\nlookups = Lookups()\nlemmatizer = Lemmatizer(lookups)\nword = \"ducks\"\nlemmas = lemmatizer.lookup(word)\nprint(lemmas)\n</code></pre>\n\n<p>The result I was hoping for was that the word \"ducks\" (plural) would result in \"duck\" (singular).  Unfortunately, \"ducks\" (plural) is returned.</p>\n\n<p>Is there a way of doing this?</p>\n\n<p>NOTE: I realize that I could process an entire string of words from a document (nlp(document)) and then find the required token and then get its lemma (token.lemma_), but the word(s) I need to lemmatize are somewhat dynamic and are not able to be processed as a large document.</p>\n",
    "score": 7,
    "creation_date": 1578430835,
    "view_count": 8087,
    "answer_count": 4,
    "tags": "nlp;spacy"
  },
  {
    "question_id": 58186670,
    "title": "Gensim Word2Vec model getting worse by increasing the number of epochs",
    "body": "<p>I'm building a Word2Vec model for a category-recommendation on a dataset consisting of ~35.000 sentences for a total of ~500.000 words but only ~3.000 distinct ones.\nI build the model basically like this :</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def train_w2v_model(df, epochs):\n    w2v_model = Word2Vec(min_count=5,\n                                 window=100,\n                                 size=230,\n                                 sample=0,\n                                 workers=cores-1,\n                                 batch_words=100)\n    vocab = df['sentences'].apply(list)\n    w2v_model.build_vocab(vocab)\n    w2v_model.train(vocab, total_examples=w2v_model.corpus_count, total_words=w2v_model.corpus_total_words, epochs=epochs, compute_loss=True)\n    return w2v_model.get_latest_training_loss()\n</code></pre>\n\n<p>I tried to find the right number of epochs for such a model like this :</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>print(train_w2v_model(1))\n=&gt;&gt; 86898.2109375\nprint(train_w2v_model(100))\n=&gt;&gt; 5025273.0\n</code></pre>\n\n<p>I find the results very counterintuitive.\nI do not understand how increasing the number of epochs could lead to lower the performance.\nIt seems not to be a misunderstanding from the function <code>get_latest_training_loss</code> since I observe the results with the function <code>most_similar</code> way better with only 1 epoch :</p>\n\n<p>100 epochs :</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>w2v_model.wv.most_similar(['machine_learning'])\n=&gt;&gt; [('salesforce', 0.3464601933956146),\n ('marketing_relationnel', 0.3125850558280945),\n ('batiment', 0.30903393030166626),\n ('go', 0.29414454102516174),\n ('simulation', 0.2930642068386078),\n ('data_management', 0.28968319296836853),\n ('scraping', 0.28260597586631775),\n ('virtualisation', 0.27560457587242126),\n ('dataviz', 0.26913416385650635),\n ('pandas', 0.2685554623603821)]\n</code></pre>\n\n<p>1 epoch :</p>\n\n<pre><code>w2v_model.wv.most_similar(['machine_learning'])\n=&gt;&gt; [('data_science', 0.9953729510307312),\n ('data_mining', 0.9930223822593689),\n ('big_data', 0.9894922375679016),\n ('spark', 0.9881765842437744),\n ('nlp', 0.9879133701324463),\n ('hadoop', 0.9834049344062805),\n ('deep_learning', 0.9831978678703308),\n ('r', 0.9827396273612976),\n ('data_visualisation', 0.9805369973182678),\n ('nltk', 0.9800992012023926)]\n</code></pre>\n\n<p>Any insight on why it behaves like this ? I would have think that increasing the number of epochs would have for sure a positive effect on the <strong>training</strong> loss.</p>\n",
    "score": 7,
    "creation_date": 1569939147,
    "view_count": 4683,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;gensim;word2vec"
  },
  {
    "question_id": 53846331,
    "title": "TypeError: can only concatenate str (not &quot;numpy.int64&quot;) to str",
    "body": "<p>I want to print the variable based on the index number based on the following dataset:</p>\n\n<p><a href=\"https://i.sstatic.net/ajrcx.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ajrcx.png\" alt=\"enter image description here\"></a></p>\n\n<p>Here I used the following code:</p>\n\n<pre><code>import pandas as pd\n\nairline = pd.read_csv(\"AIR-LINE.csv\")\n\npnr = input(\"Enter the PNR Number \")\nindex = airline.PNRNum[airline.PNRNum==pnr].index.tolist()\nzzz = int(index[0])\nprint( \"The flight number is \" + airline.FlightNo[zzz]  )\n</code></pre>\n\n<p>I get the following error:</p>\n\n<blockquote>\n  <p>TypeError: can only concatenate str (not \"numpy.int64\") to str</p>\n</blockquote>\n\n<p>I know that the error is because the <code>FlightNo</code> variable contains <code>int</code> value. But I don't know how to solve it. Any idea?</p>\n",
    "score": 7,
    "creation_date": 1545203945,
    "view_count": 30987,
    "answer_count": 2,
    "tags": "python;nlp"
  },
  {
    "question_id": 53588518,
    "title": "&#39;string&#39; has incorrect type (expected str, got spacy.tokens.doc.Doc)",
    "body": "<p>I have a dataframe:</p>\n\n<pre><code>train_review = train['review']\ntrain_review\n</code></pre>\n\n<p>It looks like:</p>\n\n<pre><code>0      With all this stuff going down at the moment w...\n1      \\The Classic War of the Worlds\\\" by Timothy Hi...\n2      The film starts with a manager (Nicholas Bell)...\n3      It must be assumed that those who praised this...\n4      Superbly trashy and wondrously unpretentious 8...\n</code></pre>\n\n<p>I add the tokens into a string:</p>\n\n<pre><code>train_review = train['review']\ntrain_token = ''\nfor i in train['review']:\n   train_token +=i\n</code></pre>\n\n<p>What I want is to tokenize the reviews using Spacy.\nHere is what I tried, but I get the following error: </p>\n\n<blockquote>\n  <p>Argument 'string' has incorrect type (expected str, got\n  spacy.tokens.doc.Doc)</p>\n</blockquote>\n\n<p>How can I solve that? Thanks in advance!</p>\n",
    "score": 7,
    "creation_date": 1543818340,
    "view_count": 21252,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 52224666,
    "title": "use tf–idf in keras Tokenizer",
    "body": "<p>I have a dataframe where the column Title of the first row contains this text:</p>\n\n<pre><code>Use of hydrocolloids as cryoprotectant for frozen foods\n</code></pre>\n\n<p>Using this code:</p>\n\n<pre><code>vocabulary_size = 1000\ntokenizer = Tokenizer(num_words=vocabulary_size)\ntokenizer.fit_on_texts(df['Title'])\nsequences = tokenizer.texts_to_sequences(df['Title'])\nprint(sequences[0])\n</code></pre>\n\n<p>I am getting this sequence:</p>\n\n<pre><code>[57, 1, 21, 7]\n</code></pre>\n\n<p>Using this:</p>\n\n<pre><code>index_word = {v: k for k, v in tokenizer.word_index.items()}\nprint(index_word[57])\nprint(index_word[1])\nprint(index_word[21])\nprint(index_word[7])\n</code></pre>\n\n<p>I obtain:</p>\n\n<pre><code>use\nof\nas\nfor\n</code></pre>\n\n<p>It makes sense, as these are the more frequent words. Is it also possible to use the Tokenizer to base the tokenisation on <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" rel=\"noreferrer\">tf–idf</a>? </p>\n\n<p>Increasing the vocabulary_size also tokenises less frequent words like:</p>\n\n<pre><code>hydrocolloids\n</code></pre>\n\n<p>I intend to use glove downstream for a classification task. Does it make sense to keep frequent and thus potentially less discrimitative words like:</p>\n\n<pre><code>use\n</code></pre>\n\n<p>in? Maybe yes, as glove also looks at context, which is in contrast to bag of word approaches I used in the past. Here tf–idf makes sense.</p>\n",
    "score": 7,
    "creation_date": 1536330514,
    "view_count": 4216,
    "answer_count": 2,
    "tags": "python;python-3.x;keras;nlp"
  },
  {
    "question_id": 49354665,
    "title": "Should I perform both lemmatization and stemming?",
    "body": "<p>I'm writing a text classification system in Python. This is what I'm doing to canonicalize each token:</p>\n\n<pre><code>lem, stem = WordNetLemmatizer(), PorterStemmer()\nfor doc in corpus:\n    for word in doc:\n        lemma = stem.stem(lem.lemmatize(word))\n</code></pre>\n\n<p>The reason I don't want to just lemmatize is because I noticed that <code>WordNetLemmatizer</code> wasn't handling some common inflections. In the case of adverbs, for example, <code>lem.lemmatize('walking')</code> returns <code>walking</code>.</p>\n\n<p>Is it wise to perform both stemming and lemmatization? Or is it redundant? Do researchers typically do one or the other, and not both?</p>\n",
    "score": 7,
    "creation_date": 1521423861,
    "view_count": 13297,
    "answer_count": 3,
    "tags": "python;machine-learning;nlp;nltk;stemming"
  },
  {
    "question_id": 43040525,
    "title": "Text classification using Keras: How to add custom features?",
    "body": "<p>I'm writing a program to classify texts into a few classes. Right now, the program loads the train and test samples of word indices, applies an embedding layer and a convolutional layer, and classifies them into the classes. I'm trying to add handcrafted features for experimentation, as in the following code. The <code>features</code> is a list of two elements, where the first element consists of features for the training data, and the second consists of features for the test data. Each training/test sample will have a corresponding feature vector (i.e. the features are not word features).</p>\n\n<pre><code>model = Sequential()\nmodel.add(Embedding(params.nb_words,\n                    params.embedding_dims,\n                    weights=[embedding_matrix],\n                    input_length=params.maxlen,\n                    trainable=params.trainable))\nmodel.add(Convolution1D(nb_filter=params.nb_filter,\n                        filter_length=params.filter_length,\n                        border_mode='valid',\n                        activation='relu'))\nmodel.add(Dropout(params.dropout_rate))\nmodel.add(GlobalMaxPooling1D())\n\n# Adding hand-picked features\nmodel_features = Sequential()\nnb_features = len(features[0][0])\n\nmodel_features.add(Dense(1,\n                         input_shape=(nb_features,),\n                         init='uniform',\n                         activation='relu'))\n\nmodel_final = Sequential()\nmodel_final.add(Merge([model, model_features], mode='concat'))\n\nmodel_final.add(Dense(len(citfunc.funcs), activation='softmax'))\nmodel_final.compile(loss='categorical_crossentropy',\n                    optimizer='adam',\n                    metrics=['accuracy'])\n\nprint model_final.summary()\nmodel_final.fit([x_train, features[0]], y_train,\n                nb_epoch=params.nb_epoch,\n                batch_size=params.batch_size,\n                class_weight=data.get_class_weights(x_train, y_train))\n\ny_pred = model_final.predict([x_test, features[1]])\n</code></pre>\n\n<p>My question is, is this code correct? Is there any conventional way of adding features to each of the text sequences?</p>\n",
    "score": 7,
    "creation_date": 1490600455,
    "view_count": 2318,
    "answer_count": 1,
    "tags": "machine-learning;neural-network;nlp;deep-learning;keras"
  },
  {
    "question_id": 32333996,
    "title": "Python NLTK WUP Similarity Score not unity for exact same word",
    "body": "<p>Simple code like follows gives out similarity score of 0.75 for both cases. As you can see both the words are the exact same. To avoid any confusion I also compared a word with itself. The score refuses to bulge from 0.75. What is going on here?</p>\n\n<pre><code>from nltk.corpus import wordnet as wn\nactual=wn.synsets('orange')[0]\npredicted=wn.synsets('orange')[0]\nsimilarity=actual.wup_similarity(predicted)\nprint similarity\nsimilarity=actual.wup_similarity(actual)\nprint similarity\n</code></pre>\n",
    "score": 7,
    "creation_date": 1441117019,
    "view_count": 5247,
    "answer_count": 1,
    "tags": "python;nlp;nltk;similarity"
  },
  {
    "question_id": 31360092,
    "title": "Using natural language processing to extract an address from a tweet",
    "body": "<p>I'm building a twitter bot that will listen for tweets like the following:</p>\n\n<pre><code>Hey @twitterbot, I'm looking for restaurants around 123 Main Street, New York\n</code></pre>\n\n<p>or, another example:</p>\n\n<pre><code>@twitterbot, what's near Yonge &amp; Dundas, Toronto? I'm hungry!\n</code></pre>\n\n<p>It'll then reply with the kind of data you'd expect these questions to return. I've got most of the problem solved, but I'm stuck on something that shouldn't be so hard; extracting the address from the tweet.</p>\n\n<p>I'll be forwarding the address to a geocoding service to get lat/lng, so I don't need to format or prepare the address in any way; I just need to isolate it from unrelated text like \"I'm looking for restaurants around\" or \"I'm hungry!\".</p>\n\n<p>Are there any NLP tools that will perform this address-identification within a block of text? Any suggestions for another way to go about it? Because Google's geocoder handles such a wide array of address formats (even a point of interest like 'The eaton centre, Toronto' counts as an address), I can't use regex to pluck the address out.</p>\n\n<p>Phrased another way, I just want to remove any text that is not part of an address.</p>\n\n<p>The addresses I'm looking for need to work for US/Canada.</p>\n\n<p>There are some similar questions on StackOverflow but none that tackle this exact problem that I could find. Because Google's geocoder is so forgiving, the solution doesn't have to be perfect, it just needs to get rid of enough of the fuzz so that Google knows what I'm trying to say.</p>\n\n<p>I'm very new to NLP so I'd appreciate any guidance on the subject.</p>\n",
    "score": 7,
    "creation_date": 1436637163,
    "view_count": 6904,
    "answer_count": 2,
    "tags": "google-maps;machine-learning;nlp;street-address"
  },
  {
    "question_id": 18366071,
    "title": "Looking for a database or text file of english words with their different forms",
    "body": "<p>I am working on a project and I need to get the root of a given word (stemming). As you know, the stemming algorithms that don't use a dictionary are not accurate. Also I tried the WordNet but it is not good for my project. I found phpmorphy project but it doesn't include API in Java. </p>\n\n<p>At this time I am looking for a database or a text file of english words with their different forms. for example:</p>\n\n<p>run running ran ...\ninclude including included ...\n...</p>\n\n<p>Thank you for your help or advise. </p>\n",
    "score": 7,
    "creation_date": 1377113466,
    "view_count": 2407,
    "answer_count": 1,
    "tags": "nlp;stemming;lemmatization"
  },
  {
    "question_id": 15261004,
    "title": "Detect if text in English with python",
    "body": "<p>Well, i knew this question being asked multiple of times but i still couldn't fix it with the \"available\" solution. Hope to got any further ideas or concepts of how to detect my sentences is english in python. The available solution:</p>\n\n<ul>\n<li>Language Detector (in ruby not in python :/)</li>\n<li>Google Translate API v2 (No longer free, have to pay 20 bucks a month while i'm doing this project for academic purposes. Courtesy limit: 0 characters/day ) </li>\n<li>Language identification for python (source code not found, link at below. <a href=\"http://cogscicoder.blogspot.com/2009/03/automatic-language-identification-using.html\" rel=\"nofollow noreferrer\">automatic-language-identification</a>)</li>\n<li><a href=\"http://pythonhosted.org/pyenchant/\" rel=\"nofollow noreferrer\">Enchant</a> (it's not for python 2.7? I'm new to python, any guide? I bet this would be the one i need)</li>\n<li>Wordnet from NLTK (i got no idea why \"wordnet.synsets\" is missing and only \"wordnet.Synset\" is available. the sample code in solution is not working for me as well T_T, probably versioning issue again?)</li>\n<li>Store english words into list and compare if the word exist (yea, it's kinda bad approach while the sentences are from twitter and.. you knew that :P)</li>\n</ul>\n\n<p><strong>WORKING SOLUTION</strong></p>\n\n<p>Finally after a series of trying, the following is the working solution (alternative to the above list)</p>\n\n<ul>\n<li>Wiktionary API (Using Urllib2, and simplejson to parse it. then find if the key is -1 means the word doesn't exist. else it's english. of course, for use in twitter have to preprocess your word into no special character like @#,?!. For how to find the key would referencing here. <a href=\"https://stackoverflow.com/questions/15261465/simplejson-and-random-key-value\">Simplejson and random key value</a>)</li>\n<li>Answer from Dogukan Tufekci (Ticked)(Weakness: Let say if the sentence shorter than 20 characters long have to install PyEnchant or it will return UNKNOWN. While PyEnchant is not supporting Python 2.7, means couldn't install and not working to less than 20 character sentence)</li>\n</ul>\n\n<p><strong><em>References</em></strong></p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/4605062/detecting-whether-or-not-text-is-english-in-bulk\">Detecting whether or not text is English (in bulk)</a></li>\n<li><a href=\"https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\">How to check if a word is an English word with Python?</a></li>\n<li><a href=\"https://stackoverflow.com/questions/2770547/is-there-an-api-available-to-retrieve-raw-wiktionary-data\">How to retrieve Wiktionary word content?</a></li>\n</ul>\n",
    "score": 7,
    "creation_date": 1362616691,
    "view_count": 8198,
    "answer_count": 2,
    "tags": "python;api;python-2.7;nlp"
  },
  {
    "question_id": 8967544,
    "title": "Using Stanford CoreNLP",
    "body": "<p>I am trying to get around using the Stanford CoreNLP. I used some code from the web to understand what is going on with the coreference tool. I tried running the project in Eclipse but keep encountering an out of memory exception. I tried increasing the heap size but there isnt any difference.</p>\n<p>Why this keeps happening? Is this a code specific problem?</p>\n<p>Here is my code:</p>\n<pre><code>import edu.stanford.nlp.dcoref.CorefChain;\nimport edu.stanford.nlp.dcoref.CorefCoreAnnotations;\nimport edu.stanford.nlp.pipeline.Annotation;\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP;\n\n\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Properties;\n\n\npublic class testmain {\n\n    public static void main(String[] args) {\n\n        String text = &quot;Viki is a smart boy. He knows a lot of things.&quot;;\n        Annotation document = new Annotation(text);\n        Properties props = new Properties();\n        props.put(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, parse, dcoref&quot;);\n        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n        pipeline.annotate(document);\n\n\n        Map&lt;Integer, CorefChain&gt; graph = document.get(CorefCoreAnnotations.CorefChainAnnotation.class);\n\n\n\n        Iterator&lt;Integer&gt; itr = graph.keySet().iterator();\n    \n        while (itr.hasNext()) {\n        \n             String key = itr.next().toString();\n        \n             String value = graph.get(key).toString();\n        \n             System.out.println(key + &quot; &quot; + value);      \n        }\n\n   }\n}\n</code></pre>\n",
    "score": 7,
    "creation_date": 1327296584,
    "view_count": 9436,
    "answer_count": 3,
    "tags": "java;eclipse;nlp;stanford-nlp"
  },
  {
    "question_id": 8434205,
    "title": "Regular expression to match object dimensions",
    "body": "<p>I'll put it right out there: I'm terrible with regular expressions. I've tried to come up with one to solve my problem but I really don't know much about them. . .</p>\n\n<p>Imagine some sentences along the following lines:</p>\n\n<blockquote>\n  <ul>\n  <li>Hello blah blah. It's around 11 1/2\" x 32\".</li>\n  <li>The dimensions are 8 x 10-3/5!</li>\n  <li>Probably somewhere in the region of 22\" x 17\".</li>\n  <li>The roll is quite large: 42 1/2\" x 60 yd.</li>\n  <li>They are all 5.76 by 8 frames.</li>\n  <li>Yeah, maybe it's around 84cm long.</li>\n  <li>I think about 13/19\".</li>\n  <li>No, it's probably 86 cm actually.</li>\n  </ul>\n</blockquote>\n\n<p>I want to, as cleanly as possible, extract item dimensions from within these sentences. In a perfect world the regular expression would output the following:</p>\n\n<blockquote>\n  <ul>\n  <li>11 1/2\" x 32\"</li>\n  <li>8 x 10-3/5</li>\n  <li>22\" x 17\"</li>\n  <li>42 1/2\" x 60 yd</li>\n  <li>5.76 by 8</li>\n  <li>84cm</li>\n  <li>13/19\"</li>\n  <li>86 cm</li>\n  </ul>\n</blockquote>\n\n<p>I imagine a world where the following rules apply:</p>\n\n<ul>\n<li>The following are valid units: <code>{cm, mm, yd, yards, \", ', feet}</code>, though I'd prefer a solution that considers an arbitrary set of units rather than an explicit solution for the above units.</li>\n<li>A dimension is always described numerically, may or may not have units following it and may or may not have a fractional or decimal part. Being made up of a fractional part on it's own is allowed, e.g., <code>4/5\"</code>.</li>\n<li>Fractional parts always have a <code>/</code> separating the numerator / denominator, and one can assume there is no space between the parts (though if someone takes that in to account that's great!).</li>\n<li>Dimensions may be one-dimensional or two-dimensional, in which case one can assume the following are acceptable for separating two dimensions: <code>{x, by}</code>. If a dimension is only one-dimensional it <strong>must</strong> have units from the set above, i.e., <code>22 cm</code> is OK, <code>.333</code> is not, nor is <code>4.33 oz</code>.</li>\n</ul>\n\n<p>To show you how useless I am with regular expressions (and to show I at least tried!), I got this far. . .</p>\n\n<pre><code>[1-9]+[/ ][x1-9]\n</code></pre>\n\n<p><strong>Update (2)</strong></p>\n\n<p>You guys are very fast and efficient! I'm going to add an extra few of test cases that haven't been covered by the regular expressions below:</p>\n\n<blockquote>\n  <ul>\n  <li>The last but one test case is 12 yd x.</li>\n  <li>The last test case is 99 cm by.</li>\n  <li>This sentence doesn't have dimensions in it: 342 / 5553 / 222.</li>\n  <li>Three dimensions? 22\" x 17\" x 12 cm</li>\n  <li>This is a product code: c720 with another number 83 x better.  </li>\n  <li>A number on its own 21.</li>\n  <li>A volume shouldn't match 0.332 oz.</li>\n  </ul>\n</blockquote>\n\n<p>These should result in the following (# indicates nothing should match):</p>\n\n<blockquote>\n  <ul>\n  <li>12 yd</li>\n  <li>99 cm</li>\n  <li>#</li>\n  <li>22\" x 17\" x 12 cm</li>\n  <li>#</li>\n  <li>#</li>\n  <li>#</li>\n  </ul>\n</blockquote>\n\n<p>I've adapted <a href=\"https://stackoverflow.com/a/8434591/646300\">M42's</a> answer below, to:</p>\n\n<pre><code>\\d+(?:\\.\\d+)?[\\s-]*(?:\\d+)?(?:\\/\\d+)?(?:cm|mm|yd|\"|'|feet)(?:\\s*x\\s*|\\s*by\\s*)?(?:\\d+(?:\\.\\d+)?[\\s*-]*(?:\\d+(?:\\/\\d+)?)?(?:cm|mm|yd|\"|'|feet)?)?\n</code></pre>\n\n<p>But while that resolves some new test cases it now fails to match the following others. It reports:</p>\n\n<blockquote>\n  <ul>\n  <li>11 1/2\" x 32\" PASS</li>\n  <li>(nothing) FAIL</li>\n  <li>22\" x 17\" PASS</li>\n  <li>42 1/2\" x 60 yd PASS</li>\n  <li>(nothing) FAIL</li>\n  <li>84cm PASS</li>\n  <li>13/19\" PASS</li>\n  <li>86 cm PASS</li>\n  <li>22\" PASS</li>\n  <li>(nothing) FAIL</li>\n  <li><p>(nothing) FAIL</p></li>\n  <li><p>12 yd x FAIL</p></li>\n  <li>99 cm by FAIL</li>\n  <li>22\" x 17\" [and also, but separately '12 cm'] FAIL</li>\n  <li><h1>PASS</h1></li>\n  <li><h1>PASS</h1></li>\n  </ul>\n</blockquote>\n",
    "score": 7,
    "creation_date": 1323361575,
    "view_count": 2795,
    "answer_count": 3,
    "tags": "regex;parsing;text;nlp;text-extraction"
  },
  {
    "question_id": 7979292,
    "title": "PHP library for word clustering/NLP?",
    "body": "<p>What I am trying to implement is a rather trivial \"take search results (as in title &amp; short description), cluster them into meaningful named groups\" program in PHP.</p>\n\n<p>After hours of googling and countless searches on SO (yielding interesting results as always, albeit nothing really useful) I'm still unable to find any PHP library that would help me handle clustering.</p>\n\n<ul>\n<li>Is there such a PHP library out there that I might have missed?</li>\n<li>If not, is there any FOSS that handles clustering and has a decent API?</li>\n</ul>\n",
    "score": 7,
    "creation_date": 1320231903,
    "view_count": 3085,
    "answer_count": 6,
    "tags": "php;nlp;cluster-analysis;information-retrieval"
  },
  {
    "question_id": 7742894,
    "title": "NLTK/NLP buliding a many-to-many/multi-label subject classifier",
    "body": "<p>I have a human tagged corpus of over 5000 subject indexed documents in XML. They vary in size from a few hundred kilobytes to a few hundred megabytes. Being short articles to manuscripts. They have all been subjected indexed as deep as the paragraph level. I am lucky to have such a corpus available, and I am trying to teach myself some NLP concepts. Admittedly, I've only begun. Thus far reading only the freely available NLTK book, <a href=\"http://streamhacker.com\" rel=\"nofollow\">streamhacker</a>, and skimming jacobs(?) NLTK cookbook. I like to experiment with some ideas.</p>\n\n<p>It was suggested to me, that perhaps, I could take bi-grams and use naive Bayes classification to tag new documents. I feel as if this is the wrong approach. a Naive Bayes is proficient at a true/false sort of relationship, but to use it on my hierarchical tag set I would need to build a new classifier for each tag. Nearly a 1000 of them. I have the memory and processor power to undertake such a task, but am skeptical of the results. However, I will be trying this approach first, to appease someones request. I should likely have this accomplished in the next day or two, but I predict the accuracy to be low.</p>\n\n<p>So my question is a bit open ended. Laregly becuase of the nature of the discipline and the general unfamilirity with my data it will likely be hard to give an exact answer. </p>\n\n<ol>\n<li><p>What sort of classifier would be appropriate for this task. Was I wrong can a Bayes be used for more than a true/false sort of operation.</p></li>\n<li><p>what feature extraction should I pursue for such a task. I am not expecting much with the bigrams. </p></li>\n</ol>\n\n<p>Each document also contains some citational information including, author/s, an authors gender of m,f,mix(m&amp;f),and other (Gov't inst et al.), document type, published date(16th cent. to current), human analyst, and a few other general elements. I'd also appreciate some useful descriptive tasks to help investigate this data better for gender bias, analyst bias, etc. But realize that is a bit beyond the scope of this question. </p>\n",
    "score": 7,
    "creation_date": 1318434770,
    "view_count": 2577,
    "answer_count": 2,
    "tags": "python;statistics;nlp;machine-learning;nltk"
  },
  {
    "question_id": 6767770,
    "title": "NLTK - when to normalize the text?",
    "body": "<p>I've finished gathering my data I plan to use for my corpus, but I'm a bit confused about whether I should normalize the text. I plan to tag &amp; chunk the corpus in the future. Some of NLTK's corpora are all lower case and others aren't.</p>\n\n<p>Can anyone shed some light on this subject, please?</p>\n",
    "score": 7,
    "creation_date": 1311192111,
    "view_count": 3163,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 695347,
    "title": "Algorithm to classify a list of products?",
    "body": "<p>I have a list representing products which are more or less the same. For instance, in the list below, they are all Seagate hard drives. </p>\n\n<ol>\n<li>Seagate Hard Drive 500Go</li>\n<li>Seagate Hard Drive 120Go for laptop</li>\n<li>Seagate Barracuda 7200.12 ST3500418AS 500GB 7200 RPM SATA 3.0Gb/s Hard Drive</li>\n<li>New and shinny 500Go hard drive from Seagate</li>\n<li>Seagate Barracuda 7200.12</li>\n<li>Seagate FreeAgent Desk 500GB External Hard Drive Silver 7200RPM USB2.0 Retail</li>\n</ol>\n\n<p>For a human being, the hard drives 3 and 5 are the same. We could go a little bit further and suppose that the products 1, 3, 4 and 5 are the same and put in other categories the product 2 and 6. </p>\n\n<p>We have a huge list of products that I would like to classify. <strong>Does anybody have an idea of what would be the best algorithm to do such thing.</strong> Any suggestions?</p>\n\n<p>I though of a <strong>Bayesian classifier</strong> but I am not sure if it is the best choice. Any help would be appreciated!</p>\n\n<p>Thanks.</p>\n",
    "score": 7,
    "creation_date": 1238359445,
    "view_count": 1839,
    "answer_count": 7,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 68086929,
    "title": "How to get the size of a Hugging Face pretrained model?",
    "body": "<p>I keep getting a <code>CUDA out of memory</code> error when trying to fine-tune a Hugging Face pretrained XML Roberta model. So, the first thing I want to find out is the size of the pretrained model.</p>\n<pre><code>model = XLMRobertaForCausalLM.from_pretrained('xlm-roberta-base', config=config)\ndevice = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)\n\nmodel.to(device)\n</code></pre>\n<p>I have tried to get the size of the model with</p>\n<pre><code>sys.getsizeof(model)\n</code></pre>\n<p>and, unsurprisingly, I get an incorrect result.  I get 56 as a result, which is the size of the python object.</p>\n<p>But then, I tried <code>model. element_size()</code>, and I get the error</p>\n<pre><code>ModuleAttributeError: 'XLMRobertaForCausalLM' object has no attribute 'element_size'\n</code></pre>\n<p>I have searched in the Hugging Face documentation, but I have not found how to do it. Does anyone here know how to do it?</p>\n",
    "score": 7,
    "creation_date": 1624377374,
    "view_count": 10328,
    "answer_count": 4,
    "tags": "python;deep-learning;nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 59877735,
    "title": "How to get probability of prediction per entity from Spacy NER model?",
    "body": "<p>I used this <a href=\"https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py\" rel=\"noreferrer\">official example code</a> to train a NER model from scratch using my own training samples. </p>\n\n<p>When I predict using this model on new text, I want to get the probability of prediction of each entity.</p>\n\n<blockquote>\n<pre><code>    # test the saved model\n    print(\"Loading from\", output_dir)\n    nlp2 = spacy.load(output_dir)\n    for text, _ in TRAIN_DATA:\n        doc = nlp2(text)\n        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n</code></pre>\n</blockquote>\n\n<p>I am unable to find a method in Spacy to get the probability of prediction of each entity.</p>\n\n<p>How do I get this probability from Spacy? I need it to apply a cutoff on it.</p>\n",
    "score": 7,
    "creation_date": 1579779617,
    "view_count": 6490,
    "answer_count": 3,
    "tags": "python;deep-learning;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 51079986,
    "title": "Generate misspelled words (typos)",
    "body": "<p>I have implemented a fuzzy matching algorithm and I would like to evaluate its recall using some sample queries with test data. </p>\n\n<p>Let's say I have a document containing the text:</p>\n\n<pre><code>{\"text\": \"The quick brown fox jumps over the lazy dog\"}\n</code></pre>\n\n<p>I want to see if I can retrieve it by testing queries such as \"sox\" or \"hazy drog\" instead of \"fox\" and \"lazy dog\". </p>\n\n<p>In other words, I want to add noise to strings to generate  misspelled words (typos). </p>\n\n<p><strong>What would be a way of automatically generating words with typos</strong> for evaluating fuzzy search?</p>\n",
    "score": 7,
    "creation_date": 1530179972,
    "view_count": 7768,
    "answer_count": 2,
    "tags": "python;nlp;fuzzy-search"
  },
  {
    "question_id": 47865034,
    "title": "Recurrent NNs: what&#39;s the point of parameter sharing? Doesn&#39;t padding do the trick anyway?",
    "body": "<p>The following is how I understand the point of parameter sharing in RNNs:</p>\n<p>In regular feed-forward neural networks, every input unit is assigned an individual parameter, which means that the number of input units (features) corresponds to the number of parameters to learn. In processing e.g. image data, the number of input units is the same over all training examples (usually constant pixel size * pixel size * rgb frames).</p>\n<p>However, sequential input data like sentences can come in highly varying lengths, which means that the number of parameters will not be the same depending on which example sentence is processed. That is why parameter sharing is necessary for efficiently processing sequential data: it makes sure that the model always has the same input size regardless of the sequence length, as it is specified in terms of transition from one state to another. It is thus possible to use the same transition function with the same weights (input to hidden weights, hidden to output weights, hidden to hidden weights) at every time step. The big advantage is that it allows generalization to sequence lengths that did not appear in the training set.</p>\n<p>My questions are:</p>\n<ol>\n<li>Is my understanding of RNNs, as summarized above, correct?</li>\n<li>In the actual code example in Keras I looked at for LSTMs, they padded the sentences to equal lengths before all. By doing so, doesn't this wash away the whole purpose of parameter sharing in RNNs?</li>\n</ol>\n",
    "score": 7,
    "creation_date": 1513587775,
    "view_count": 5774,
    "answer_count": 1,
    "tags": "deep-learning;nlp;lstm;recurrent-neural-network"
  },
  {
    "question_id": 44621452,
    "title": "hierarchical classification in sklearn",
    "body": "<p>I would like to know if there is an implementation of hierarchical classification in the scikit-learn package or in any other python package. </p>\n\n<p>Thank you so much in advance.</p>\n",
    "score": 7,
    "creation_date": 1497839185,
    "view_count": 5333,
    "answer_count": 2,
    "tags": "machine-learning;scikit-learn;nlp;data-science"
  },
  {
    "question_id": 36509825,
    "title": "Multi-Threaded NLP with Spacy pipe",
    "body": "<p>I'm trying to apply Spacy NLP (Natural Language Processing) pipline to a big text file like Wikipedia Dump. Here is my code based on Spacy's <a href=\"https://spacy.io/docs\" rel=\"noreferrer\">documentation</a> example:</p>\n\n<pre><code>from spacy.en import English\n\ninput = open(\"big_file.txt\")\nbig_text= input.read()\ninput.close()\n\nnlp= English()    \n\nout = nlp.pipe([unicode(big_text, errors='ignore')], n_threads=-1)\ndoc = out.next() \n</code></pre>\n\n<p>Spacy applies all nlp operations like POS tagging, Lemmatizing and etc all at once. It is like a pipeline for NLP that takes care of everything you need in one step. Applying pipe method tho is supposed to make the process a lot faster by multithreading the expensive parts of the pipeline. But I don't see big improvement in speed and my CPU usage is around 25% (only one of 4 cores working). I also tried to read the file in multiple chuncks and increase the batch of input texts:</p>\n\n<pre><code>out = nlp.pipe([part1, part2, ..., part4], n_threads=-1)\n</code></pre>\n\n<p>but still the same performance. Is there anyway to speed up the process? I suspect that OpenMP feature should be enabled compiling Spacy to utilize multi-threading feature. But there is no instructions on how to do it on Windows. </p>\n",
    "score": 7,
    "creation_date": 1460151896,
    "view_count": 8072,
    "answer_count": 1,
    "tags": "python;multithreading;nlp;pipeline;spacy"
  },
  {
    "question_id": 34252170,
    "title": "How can Stanford CoreNLP Named Entity Recognition capture measurements like 5 inches, 5&quot;, 5 in., 5 in",
    "body": "<p>I'm looking to capture measurements using <a href=\"http://stanfordnlp.github.io/CoreNLP/ner.html\" rel=\"noreferrer\">Stanford CoreNLP</a>. (If you can suggest a different extractor, that is fine too.)</p>\n\n<p>For example, I want to find <strong>15kg</strong>, <strong>15 kg</strong>, <strong>15.0 kg</strong>, <strong>15 kilogram</strong>, <strong>15 lbs</strong>, <strong>15 pounds</strong>, etc. But among CoreNLPs extraction rules, I don't see one for measurements.</p>\n\n<p>Of course, I can do this with pure regexes, but toolkits can run more quickly, and they offer the opportunity to chunk at a higher level, e.g.  to treat <strong>gb</strong> and <strong>gigabytes</strong> together, and <strong>RAM</strong> and <strong>memory</strong> as building blocks--even without full syntactic parsing--as they build  bigger units  like <strong>128 gb RAM</strong> and <strong>8 gigabytes memory</strong>.</p>\n\n<p>I want an  extractor for this that is  rule-based, not machine-learning-based), but don't see one as part of <a href=\"http://nlp.stanford.edu/software/regexner/\" rel=\"noreferrer\">RegexNer</a> or elsewhere. How do I go about this?</p>\n\n<p><a href=\"https://www-01.ibm.com/support/knowledgecenter/SSPT3X_2.1.1/com.ibm.swg.im.infosphere.biginsights.text.doc/doc/ana_txtan_NamedEntities.html\" rel=\"noreferrer\">IBM Named Entity Extraction</a> can do this. The regexes are run in an efficient way rather than passing the text through each one. And the regexes are bundled to express meaningful entities, as for example one that unites all the measurement units into a single concept.</p>\n",
    "score": 7,
    "creation_date": 1450017017,
    "view_count": 1432,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp;named-entity-recognition;named-entity-extraction"
  },
  {
    "question_id": 29958531,
    "title": "Is there a stop word list for twitter?",
    "body": "<p>I want to do some mining on tweets. Is there any more specific stop word list for tweets such as removing \"lol\" and other twitter smiley?</p>\n",
    "score": 7,
    "creation_date": 1430364521,
    "view_count": 5612,
    "answer_count": 3,
    "tags": "twitter;nlp;data-mining"
  },
  {
    "question_id": 29473169,
    "title": "Approach for identifying whether a sentence includes an imperative within it",
    "body": "<p>Looking to find out whether a sentence includes an imperative within it (e.g. categorize \"click below\" as an imperative, whereas \"here is some information\" as not).</p>\n\n<p>Is this possible with e.g. the Stanford Parser? For reference, the main site (<a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"noreferrer\">http://nlp.stanford.edu/software/lex-parser.shtml</a>) indicates 'Improved recognition of imperatives', however the dependency manual does not indicate a filed for them  <a href=\"http://nlp.stanford.edu/software/dependencies_manual.pdf\" rel=\"noreferrer\">http://nlp.stanford.edu/software/dependencies_manual.pdf</a> )</p>\n\n<p>Alternatively, is there another approach which would work?</p>\n",
    "score": 7,
    "creation_date": 1428330143,
    "view_count": 2719,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 23859892,
    "title": "Independent clause boundary disambiguation, and independent clause segmentation – any tools to do this?",
    "body": "<p>I remember skimming the sentence segmentation section from the NLTK site a long time ago. </p>\n\n<p>I use a crude text replacement of “period” “space” with “period” “manual line break” to achieve sentence segmentation, such as with a Microsoft Word replacement (<code>.</code> -> <code>.^p</code>) or a Chrome extension:</p>\n\n<p><a href=\"https://github.com/AhmadHassanAwan/Sentence-Segmentation\" rel=\"nofollow noreferrer\">https://github.com/AhmadHassanAwan/Sentence-Segmentation</a></p>\n\n<p><a href=\"https://chrome.google.com/webstore/detail/sentence-segmenter/jfbhkblbhhigbgdnijncccdndhbflcha\" rel=\"nofollow noreferrer\">https://chrome.google.com/webstore/detail/sentence-segmenter/jfbhkblbhhigbgdnijncccdndhbflcha</a></p>\n\n<p>This is instead of an NLP method like the Punkt tokenizer of NLTK. </p>\n\n<p>I segment to help me more easily locate and reread sentences, which can sometimes help with reading comprehension. </p>\n\n<p>What about independent clause boundary disambiguation, and independent clause segmentation? Are there any tools that attempt to do this?</p>\n\n<p>Below is some example text. If an independent clause can be identified within a sentence, there’s a split. Starting from the end of a sentence, it moves left, and greedily splits:</p>\n\n<p>E.g.</p>\n\n<blockquote>\n  <p><strong>Sentence</strong> boundary disambiguation\n  (SBD), also known as sentence\n  breaking, is the problem in natural\n  language processing of deciding where </p>\n  \n  <p>sentences begin and end. </p>\n  \n  <p><strong>Often</strong>, natural language processing\n  tools </p>\n  \n  <p>require their input to be divided into\n  sentences for a number of reasons. </p>\n  \n  <p><strong>However</strong>, sentence boundary\n  identification is challenging because punctuation </p>\n  \n  <p>marks are often ambiguous.</p>\n  \n  <p><strong>For</strong> example, a period may </p>\n  \n  <p>denote an abbreviation, decimal point,\n  an ellipsis, or an email address - not\n  the end of a sentence. </p>\n  \n  <p><strong>About</strong> 47% of the periods in the Wall\n  Street Journal corpus </p>\n  \n  <p>denote abbreviations.[1] </p>\n  \n  <p><strong>As</strong> well, question marks and\n  exclamation marks may </p>\n  \n  <p>appear in embedded quotations,\n  emoticons, computer code, and slang.</p>\n  \n  <p><strong>Another</strong> approach is to automatically </p>\n  \n  <p>learn a set of rules from a set of\n  documents where the sentence</p>\n  \n  <p>breaks are pre-marked. </p>\n  \n  <p><strong>Languages</strong> like Japanese and Chinese </p>\n  \n  <p>have unambiguous sentence-ending\n  markers.</p>\n  \n  <p><strong>The</strong> standard 'vanilla' approach to </p>\n  \n  <p>locate the end of a sentence:</p>\n  \n  <p>(a) <strong>If</strong> </p>\n  \n  <p>it's a period,  </p>\n  \n  <p>it ends a sentence.</p>\n  \n  <p>(b) <strong>If</strong> the preceding  </p>\n  \n  <p>token is on my hand-compiled list of\n  abbreviations, then  </p>\n  \n  <p>it doesn't end a sentence.</p>\n  \n  <p>(c) <strong>If</strong> the next  </p>\n  \n  <p>token is capitalized, then  </p>\n  \n  <p>it ends a sentence.</p>\n  \n  <p><strong>This</strong> </p>\n  \n  <p>strategy gets about 95% of sentences\n  correct.[2]</p>\n  \n  <p><strong>Solutions</strong> have been based on a maximum\n  entropy model.[3] </p>\n  \n  <p><strong>The</strong> SATZ architecture uses a neural\n  network to </p>\n  \n  <p>disambiguate sentence boundaries and\n  achieves 98.5% accuracy.</p>\n</blockquote>\n\n<p>(I’m not sure if I split it properly.)</p>\n\n<p>If there are no means to segment independent clauses, are there any search terms that I can use to further explore this topic?</p>\n\n<p>Thanks.</p>\n",
    "score": 7,
    "creation_date": 1401051438,
    "view_count": 4632,
    "answer_count": 5,
    "tags": "nlp;text-segmentation"
  },
  {
    "question_id": 10156448,
    "title": "Parsing words into (prefix, root, suffix) in Python",
    "body": "<p>I'm trying to create a simple parser for some text data. (The text is in a language that NLTK doesn't have any parsers for.) </p>\n\n<p>Basically, I have a limited number of prefixes, which can be either one or two letters; a word can have more than one prefix. I also have a limited number of suffixes of one or two letters. Whatever's in between them should be the \"root\" of the word. Many words will have more the one possible parsing, so I want to input a word and get back a list of possible parses in the form of a tuple (prefix,root,suffix).</p>\n\n<p>I can't figure out how to structure the code though. I pasted an example of one way I tried (using some dummy English data to make it more understandable), but it's clearly not right. For one thing it's really ugly and redundant, so I'm sure there's a better way to do it. For another, it doesn't work with words that have more than one prefix or suffix, or both prefix(es) and suffix(es).</p>\n\n<p>Any thoughts?</p>\n\n<pre><code>prefixes = ['de','con']\nsuffixes = ['er','s']\n\ndef parser(word):\n    poss_parses = []\n    if word[0:2] in prefixes:\n        poss_parses.append((word[0:2],word[2:],''))\n    if word[0:3] in prefixes:\n        poss_parses.append((word[0:3],word[3:],''))\n    if word[-2:-1] in prefixes:\n        poss_parses.append(('',word[:-2],word[-2:-1]))\n    if word[-3:-1] in prefixes:\n        poss_parses.append(('',word[:-3],word[-3:-1]))\n    if word[0:2] in prefixes and word[-2:-1] in suffixes and len(word[2:-2])&gt;2:\n        poss_parses.append((word[0:2],word[2:-2],word[-2:-1]))\n    if word[0:2] in prefixes and word[-3:-1] in suffixes and len(word[2:-3])&gt;2:\n        poss_parses.append((word[0:2],word[2:-2],word[-3:-1]))\n    if word[0:3] in prefixes and word[-2:-1] in suffixes and len(word[3:-2])&gt;2:\n        poss_parses.append((word[0:2],word[2:-2],word[-2:-1]))\n    if word[0:3] in prefixes and word[-3:-1] in suffixes and len(word[3:-3])&gt;2:\n        poss_parses.append((word[0:3],word[3:-2],word[-3:-1]))\n    return poss_parses\n\n\n\n&gt;&gt;&gt; wordlist = ['construct','destructer','constructs','deconstructs']\n&gt;&gt;&gt; for w in wordlist:\n...   parses = parser(w)\n...   print w\n...   for p in parses:\n...     print p\n... \nconstruct\n('con', 'struct', '')\ndestructer\n('de', 'structer', '')\nconstructs\n('con', 'structs', '')\ndeconstructs\n('de', 'constructs', '')\n</code></pre>\n",
    "score": 7,
    "creation_date": 1334430194,
    "view_count": 8640,
    "answer_count": 3,
    "tags": "python;parsing;nlp"
  },
  {
    "question_id": 3851723,
    "title": "Justadistraction: tokenizing English without whitespaces. Murakami SheepMan",
    "body": "<p>I wondered how <strong>you</strong> would go about tokenizing strings in English (or other western languages) if whitespaces were removed?</p>\n\n<p>The inspiration for the question is the Sheep Man character in the Murakami novel '<a href=\"http://en.wikipedia.org/wiki/Dance_Dance_Dance\" rel=\"noreferrer\">Dance Dance Dance</a>'</p>\n\n<p>In the novel, the Sheep Man is translated as saying things like:</p>\n\n<blockquote>\n  <p>\"likewesaid, we'lldowhatwecan. Trytoreconnectyou, towhatyouwant,\" said the Sheep Man. \"Butwecan'tdoit-alone. Yougottaworktoo.\"</p>\n</blockquote>\n\n<p>So, some punctuation is kept, but not all. Enough for a human to read, but somewhat arbitrary.</p>\n\n<p>What would be your strategy for building a parser for this? Common combinations of letters, syllable counts, conditional grammars, look-ahead/behind regexps etc.?</p>\n\n<p>Specifically, python-wise, how would you structure a (forgiving) translation flow? Not asking for a completed answer, just more how your thought process would go about breaking the problem down.</p>\n\n<p>I ask this in a frivolous manner, but I think it's a question that might get some interesting (nlp/crypto/frequency/social) answers.\nThanks!</p>\n",
    "score": 7,
    "creation_date": 1286142226,
    "view_count": 383,
    "answer_count": 4,
    "tags": "python;linguistics;nlp"
  },
  {
    "question_id": 2233355,
    "title": "Python - letter frequency count and translation",
    "body": "<p>I am using Python 3.1, but I can downgrade if needed.</p>\n\n<p>I have an ASCII file containing a short story written in one of the languages the alphabet of which can be represented with upper and or lower ASCII. I wish to:</p>\n\n<p>1) Detect an encoding to the best of my abilities, get some sort of confidence metric (would vary depending on the length of the file, right?)</p>\n\n<p>2) Automatically translate the whole thing using some free online service or a library.</p>\n\n<p>Additional question: What if the text is written in a language where it takes 2 or more bytes to represent one letter and the byte order mark is not there to help me?</p>\n\n<p>Finally, how do I deal with punctuation and misc characters such as space? It will occur more frequently than some letters, right? How about the fact that punctuation and characters can be sometimes mixed - there might be two representations of a comma, two representations for what looks like an \"a\", etc.?</p>\n\n<p>Yes, I have read <a href=\"http://www.joelonsoftware.com/articles/Unicode.html\" rel=\"nofollow noreferrer\">the article by Joel Spolsky on Unicode</a>. Please help me with at least some of these items.</p>\n\n<p>Thank you!</p>\n\n<p>P.S. This is not a homework, but it is for self-educational purposes. I prefer using a letter frequency library that is open-source and readable as opposed to the one that is closed, efficient, but gets the job done well.</p>\n",
    "score": 7,
    "creation_date": 1265758665,
    "view_count": 3579,
    "answer_count": 4,
    "tags": "python;character-encoding;translation;nlp"
  },
  {
    "question_id": 1913143,
    "title": "How to Convert English to Cron?",
    "body": "<p>I did some searching but haven't landed anything that looks useful yet but I am wondering if anyone knows of something (tool,lib etc) that can parse English phrases and translate them into a cron string.</p>\n\n<p>For example: <code>Every Tuesday at 15:00</code> converts to <code>0 15 * * 2</code></p>\n\n<p>It seems like something that would have lots of gotchas and it would be preferable to benefit from someone elses work. You see it in a few nice sites/apps that can work out what you mean from a simple phrase rather than having some hideous user interface.</p>\n\n<p>Thanks in advance.</p>\n",
    "score": 7,
    "creation_date": 1260951934,
    "view_count": 4179,
    "answer_count": 3,
    "tags": "parsing;nlp"
  },
  {
    "question_id": 67976977,
    "title": "Use BERT under spaCy to get sentence embeddings",
    "body": "<p>I am trying to use BERT to get sentence embeddings. Here is how I am doing it:</p>\n<pre><code>import spacy\nnlp = spacy.load(&quot;en_core_web_trf&quot;)\nnlp(&quot;The quick brown fox jumps over the lazy dog&quot;).vector \n</code></pre>\n<p>This outputs an empty vector!!</p>\n<pre><code>array([], dtype=float32)\n</code></pre>\n<p>Am I missing something?</p>\n",
    "score": 7,
    "creation_date": 1623703357,
    "view_count": 2614,
    "answer_count": 1,
    "tags": "python;nlp;spacy;bert-language-model"
  },
  {
    "question_id": 63201036,
    "title": "Add additional layers to the Huggingface transformers",
    "body": "<p>I want to add additional <code>Dense</code> layer after pretrained <code>TFDistilBertModel</code>, <code>TFXLNetModel</code> and <code>TFRobertaModel</code> Huggingface models. I have already seen how I can do this with the <code>TFBertModel</code>, e.g. <a href=\"https://www.kaggle.com/dhruv1234/huggingface-tfbertmodel\" rel=\"noreferrer\">in this notebook</a>:</p>\n<pre><code>output = bert_model([input_ids,attention_masks])\noutput = output[1]\noutput = tf.keras.layers.Dense(32,activation='relu')(output)\n</code></pre>\n<p>So, here I need to use the second item(i.e. item with index <code>1</code>) of the <code>BERT</code> output tuple. According to the <a href=\"https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel\" rel=\"noreferrer\">docs</a> <code>TFBertModel</code> has <code>pooler_output</code> at this tuple index. But the other three models don't have <code>pooler_output</code>.</p>\n<p>So, how can I add additional layers to the other three model outputs?</p>\n",
    "score": 7,
    "creation_date": 1596246868,
    "view_count": 9286,
    "answer_count": 1,
    "tags": "python;tensorflow;keras;nlp;huggingface-transformers"
  },
  {
    "question_id": 51390568,
    "title": "Creating relations in sentence using chunk tags (not NER) with NLTK | NLP",
    "body": "<p>I am trying to create custom chunk tags and to extract relations from them. Following is the code that takes me to the cascaded chunk tree.</p>\n\n<pre><code>grammar = r\"\"\"\n  NPH: {&lt;DT|JJ|NN.*&gt;+}          # Chunk sequences of DT, JJ, NN\n  PPH: {&lt;IN&gt;&lt;NP&gt;}               # Chunk prepositions followed by NP\n  VPH: {&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+$} # Chunk verbs and their arguments\n  CLAUSE: {&lt;NP&gt;&lt;VP&gt;}           # Chunk NP, VP\n  \"\"\"\ncp = nltk.RegexpParser(grammar)\nsentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n\n\nchunked = cp.parse(sentence)\n</code></pre>\n\n<p><strong>Output -</strong> </p>\n\n<p>(S\n  (NPH Mary/NN)\n  saw/VBD\n  (NPH the/DT cat/NN)\n  sit/VB\n  on/IN\n  (NPH the/DT mat/NN))</p>\n\n<p>Now I am trying to extract relations between the NPH tag values with the text in between using the nltk.sem.extract_rels function, BUT it seems to work ONLY on named entities generated with the ne_chunk function. </p>\n\n<pre><code>IN = re.compile(r'.*\\bon\\b')\nfor rel in nltk.sem.extract_rels('NPH', 'NPH', chunked,corpus='ieer',pattern = IN):\n        print(nltk.sem.rtuple(rel))\n</code></pre>\n\n<p>This gives the following error - </p>\n\n<p><strong>ValueError: your value for the subject type has not been recognized: NPH</strong></p>\n\n<p>Is there an easy way to use only chunk tags to create relations as I don't really want to retrain the NER model to detect my chunk tags as respective named entities</p>\n\n<p>Thank you!</p>\n",
    "score": 7,
    "creation_date": 1531863538,
    "view_count": 925,
    "answer_count": 1,
    "tags": "python;nlp;nltk;named-entity-recognition;chunking"
  },
  {
    "question_id": 47734900,
    "title": "Detect abbreviations in the text in python",
    "body": "<p>I want to find abbreviations in the text and remove it. What I am currently doing is identifying consecutive capital letters and remove them.</p>\n\n<p>But I see that it does not remove abbreviations such as <code>MOOCs</code>, <code>M.O.O.C</code>, <code>M.O.O.Cs</code>. Is there an easy way of doing this in python? Or are there any libraries that I can use instead?</p>\n",
    "score": 7,
    "creation_date": 1512868096,
    "view_count": 15548,
    "answer_count": 2,
    "tags": "python;nlp"
  },
  {
    "question_id": 26716622,
    "title": "how to automatically detect acronym meaning / extension",
    "body": "<p>How can you detect / find out the meaning (the extension) of an acronym using NLP / Information Extraction (IE) methods?</p>\n\n<p>We want to detect in free text if a word or it's acronym is used and map it to the same entity / token.</p>\n\n<p>Most papers available online are about medical acronyms and they do not provide a library for acomplish this task.</p>\n\n<p>Any ideas?</p>\n",
    "score": 7,
    "creation_date": 1415026090,
    "view_count": 6244,
    "answer_count": 2,
    "tags": "nlp;information-extraction;acronym"
  },
  {
    "question_id": 24394196,
    "title": "What does the dependency-parse output of TurboParser mean?",
    "body": "<p>I have been trying to use the dependency parse trees generated by <a href=\"http://www.ark.cs.cmu.edu/TurboParser/\" rel=\"noreferrer\">CMU's TurboParser</a>. It works flawlessly. The problem, however, is that there is very little documentation. I need to precisely understand the output of their parser. For example, the sentence \"<em>I solved the problem with statistics.</em>\" generates the following output:</p>\n\n<pre><code>1   I           _   PRP PRP _   2   SUB\n2   solved      _   VBD VBD _   0   ROOT\n3   the         _   DT  DT  _   4   NMOD\n4   problem     _   NN  NN  _   2   OBJ\n5   with        _   IN  IN  _   2   VMOD\n6   statistics  _   NNS NNS _   5   PMOD\n7   .           _   .   .   _   2   P\n</code></pre>\n\n<p>I haven't found any documentation that can help understand what the various columns stand for, and how the indices in the second-last column (2, 0, 4, 2, ... ) are created. Also, I have no idea why there are two columns devoted to part-of-speech tags. Any help (or link to external documentation) will be of great help.</p>\n\n<p>P.S. If you want to try out their parser, <a href=\"http://demo.ark.cs.cmu.edu/parse\" rel=\"noreferrer\">here is their online demo</a>.</p>\n\n<p>P.P.S. Please do not suggest using Stanford's dependency parse output. I am interested in linear programming algorithms, which is not what Stanford's NLP system does.</p>\n",
    "score": 7,
    "creation_date": 1403636062,
    "view_count": 1962,
    "answer_count": 2,
    "tags": "nlp;parse-tree"
  },
  {
    "question_id": 11471376,
    "title": "Finding topics of an unseen document via Gensim",
    "body": "<p>I am using Gensim to do some large-scale topic modeling. I am having difficulty understanding how to determine predicted topics for an unseen (non-indexed) document. For example: I have 25 million documents which I have converted to vectors in LSA (and LDA) space. I now want to figure out the topics of a new document, lets call it x.</p>\n\n<p>According to the Gensim documentation, I can use:</p>\n\n<pre><code>topics = lsi[doc(x)]\n</code></pre>\n\n<p>where doc(x) is a function that converts x into a vector.</p>\n\n<p>The problem is, however, that the above variable, topics, returns a vector. The vector is useful if I am comparing x to additional documents because it allows me to find the cosine similarity between them, but I am unable to actually return specific words that are associated with x itself.</p>\n\n<p>Am I missing something, or does Gensim not have this capability?</p>\n\n<p>Thank you,</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Larsmans has the answer.</p>\n\n<p>I was able to show the topics by using:</p>\n\n<pre><code>for t in topics:\n    print lsi.show_topics(t[0])\n</code></pre>\n",
    "score": 7,
    "creation_date": 1342185730,
    "view_count": 11480,
    "answer_count": 2,
    "tags": "python;nlp;latent-semantic-indexing;gensim"
  },
  {
    "question_id": 10112500,
    "title": "Implementing alternative forms of LDA",
    "body": "<p>I am using Latent Dirichlet Allocation with a corpus of news data from six different sources. I am interested in topic evolution, emergence, and want to compare how the sources are alike and different from each other over time. I know that there are a number of modified LDA algorithms such as the Author-Topic model, Topics Over Time, and so on.</p>\n\n<p>My issue is that very few of these alternate model specifications are implemented in any standard format. A few are available in Java, but most exist as conference papers only. What is the best way to go about implementing some of these algorithms on my own? I am fairly proficient in R and jags, and can stumble around in Python when given long enough. I am willing to write the code, but I don't really know where to start and I don't know C or Java. Can I build a model in JAGS or Python just having the formulas from the manuscript? If so, can someone point me at an example of doing this? Thanks.</p>\n",
    "score": 7,
    "creation_date": 1334172025,
    "view_count": 2826,
    "answer_count": 2,
    "tags": "python;r;nlp;text-mining;lda"
  },
  {
    "question_id": 9665501,
    "title": "Word splitting statistical approach",
    "body": "<p>I want to solve word splitting problem (parse words from long string with no spaces).\nFor examle we want extract words from <code>somelongword</code> to <code>[some, long, word]</code>.</p>\n\n<p>We can achieve this by some dynamic approach with dictionary, but another issue we encounter is parsing ambiguity. I.e. <code>orcore</code> => <code>or core</code> or <code>orc ore</code> (We don't take into account phrase meaning or part of speech). So i think about usage of some statistical or ML approach.</p>\n\n<p>I found that Naive Bayes and Viterbi algorithm with train set can be used for solving this. Can you point me some information about application of these algorithms to word splitting problem?</p>\n\n<p>UPD: I've implemented this method on Clojure, using some advices from Peter Norvig's <a href=\"http://norvig.com/ngrams/ngrams.py\" rel=\"nofollow\">code</a></p>\n",
    "score": 7,
    "creation_date": 1331548632,
    "view_count": 503,
    "answer_count": 2,
    "tags": "algorithm;nlp;text-segmentation"
  },
  {
    "question_id": 6970359,
    "title": "Find an efficient way to integrate different language libraries into one project using Python as the &quot;glue&quot;",
    "body": "<p>I am about to get involved in a NLP-related project and I need to use various libraries. Some are in java, others in C/C++ (for tasks that require more speed) and finally some are in Python. I was thinking of using Python as the \"glue\" and create wrapper-classes for every task that I want to do that relies on a different language. In order to do that, the wrapper class, for example, would execute the java program and communicate with it using pipes. \nMy questions are:</p>\n\n<ol>\n<li><p>Do you think that would work for cpu-demanding and highly repetitive tasks? Or would the overhead added by the pipe-communication be too heavy?</p></li>\n<li><p>Is there any other (preferably simple) architecture that you would suggest?</p></li>\n</ol>\n",
    "score": 7,
    "creation_date": 1312684510,
    "view_count": 404,
    "answer_count": 5,
    "tags": "java;c++;python;nlp;wrapper"
  },
  {
    "question_id": 5733243,
    "title": "How to get sentence number from input?",
    "body": "<p>It seems hard to detect a sentence boundary in a text. Quotation marks like .!? may be used to delimite sentences but not so accurate as there may be ambiguous words and quotations such as U.S.A or Prof. or Dr.  I am studying Tperlregex library and Regular Expression Cookbook by <a href=\"http://www.regular-expressions.info/delphi.html\" rel=\"nofollow\">Jan Goyvaerts</a> but I do not know how to write the expression that detects sentence?</p>\n\n<p>What may be comparatively accurate expression using Tperlregex in delphi?</p>\n\n<p>Thanks</p>\n",
    "score": 7,
    "creation_date": 1303315183,
    "view_count": 299,
    "answer_count": 3,
    "tags": "regex;delphi;nlp;text-segmentation"
  },
  {
    "question_id": 3986974,
    "title": "Grammar production class implementation in C#",
    "body": "<p>Grammar by definition contains productions, example of very simple grammar:</p>\n\n<pre><code>E -&gt; E + E\nE -&gt; n\n</code></pre>\n\n<p>I want to implement Grammar class in c#, but I'm not sure how to store productions, for example how to make difference between terminal and non-terminal symbol.\ni was thinking about:</p>\n\n<pre><code>struct Production\n{\n   String Left;       // for example E\n   String Right;      // for example +\n}\n</code></pre>\n\n<p>Left will always be non-terminal symbol (it's about context-free grammars)\nBut right side of production can contain terminal &amp; non-terminal symbols</p>\n\n<p>So now I'm thinkig about 2 ways of implementation:</p>\n\n<ol>\n<li><p>Non-terminal symbols will be written using brackets, for example:</p>\n\n<p>E+E will be represented as string \"[E]+[E]\"</p></li>\n<li><p>Create additional data structure NonTerminal</p>\n\n<p>struct NonTerminal\n{\n  String Symbol;\n}</p></li>\n</ol>\n\n<p>and E+E will be represented as array/list:</p>\n\n<pre><code>[new NonTerminal(\"E\"), \"+\", new NonTerminal(\"E\")]\n</code></pre>\n\n<p>but think that there are better ideas, it would be helpfull to hear some response</p>\n",
    "score": 7,
    "creation_date": 1287660980,
    "view_count": 3639,
    "answer_count": 3,
    "tags": "c#;grammar;nlp;context-free-grammar"
  },
  {
    "question_id": 3581951,
    "title": "algorithm to calculate similarity between texts",
    "body": "<p>I am trying to score similarity between posts from social networks, but didn't find any good algorithms for that, thoughts?</p>\n\n<p>I just tried Levenshtein, JaroWinkler, and others, but those one are more used to compare texts without sentiments. In posts we can get one text saying \"I really love dogs\" and an other saying \"I really hate dogs\", we need to classify this case as totally different.</p>\n\n<p>Thanks</p>\n",
    "score": 7,
    "creation_date": 1282891782,
    "view_count": 2713,
    "answer_count": 3,
    "tags": "java;text;artificial-intelligence;nlp;mining"
  },
  {
    "question_id": 895893,
    "title": "Which NLP toolkit to use in JAVA?",
    "body": "<p>i'm working on a project that consists of a website that connects to the NCBI(National Center for Biotechnology Information) and searches for articles there.  Thing is that I have to do some text mining on all the results. \nI'm using the JAVA language for textmining and AJAX with ICEFACES for the development of the website.\n What do I have :\nA list of articles returned from a search.\nEach article has an ID and an abstract.\nThe idea is to get keywords from each abstract text.\nAnd then compare all the keywords from all abstracts and find the ones that are the most repeated. So then show in the website the related words for the search.\nAny ideas ? \nI searched a lot in the web, and I know there is Named Entity Recognition,Part Of Speech tagging, there is teh GENIA thesaurus for NER on genes and proteins, I already tried stemming ... Stop words lists, etc...\nI just need to know the best aproahc to resolve this problem.\nThanks a lot.</p>\n",
    "score": 7,
    "creation_date": 1242950964,
    "view_count": 9956,
    "answer_count": 4,
    "tags": "java;nlp;text-mining"
  },
  {
    "question_id": 483348,
    "title": "Java library that finds sentence boundaries",
    "body": "<p>Does anyone know of a Java library that handles finding sentence boundaries? I'm thinking that it would be a smart StringTokenizer implementation that knows about all of the sentence terminators that languages can use.</p>\n\n<p>Here's my experience with BreakIterator:</p>\n\n<p>Using the example <a href=\"http://java.sun.com/docs/books/tutorial/i18n/text/examples/BreakIteratorDemo.java\" rel=\"nofollow noreferrer\">here</a>:\nI have the following Japanese:</p>\n\n<pre><code>今日はパソコンを買った。高性能のマックは早い！とても快適です。\n</code></pre>\n\n<p>In ascii, it looks like this:</p>\n\n<pre><code>\\ufeff\\u4eca\\u65e5\\u306f\\u30d1\\u30bd\\u30b3\\u30f3\\u3092\\u8cb7\\u3063\\u305f\\u3002\\u9ad8\\u6027\\u80fd\\u306e\\u30de\\u30c3\\u30af\\u306f\\u65e9\\u3044\\uff01\\u3068\\u3066\\u3082\\u5feb\\u9069\\u3067\\u3059\\u3002\n</code></pre>\n\n<p>Here's the part of that sample that I changed:\n  static void sentenceExamples() {</p>\n\n<pre><code>  Locale currentLocale = new Locale (\"ja\",\"JP\");\n  BreakIterator sentenceIterator = \n     BreakIterator.getSentenceInstance(currentLocale);\n  String someText = \"今日はパソコンを買った。高性能のマックは早い！とても快適です。\";\n</code></pre>\n\n<p>When I look at the Boundary indices, I see this:</p>\n\n<pre><code>0|13|24|32\n</code></pre>\n\n<p>But those indices don't correspond to any sentence terminators.</p>\n",
    "score": 7,
    "creation_date": 1233062009,
    "view_count": 2682,
    "answer_count": 2,
    "tags": "java;string;nlp;text-segmentation"
  },
  {
    "question_id": 71573565,
    "title": "Google Cloud Vertex AI with .Net",
    "body": "<p>I am new to google cloud service VERTEX AI.</p>\n<p>I am looking to Create, train, and deploy an AutoML text classification model through .Net application. I did not find anything for .Net with Vertex AI. If someone can please guide my to the location or any .Net code samples, will be really helpful.</p>\n",
    "score": 7,
    "creation_date": 1647958547,
    "view_count": 2193,
    "answer_count": 2,
    "tags": ".net;machine-learning;google-cloud-platform;nlp;google-cloud-vertex-ai"
  },
  {
    "question_id": 71461092,
    "title": "Is there any NER model that recognizes first and last names instead of just PERSON?",
    "body": "<p>Given a set of strings like:</p>\n<pre><code>&quot;John Doe&quot;\n&quot;Doe John&quot;\n&quot;Albert Green&quot;\n&quot;Greenshpan David&quot;\n\n...\n</code></pre>\n<p>I would like to run a NER model that will recognize the first name and last name.\nAll English models I use (in Spacy, NLTK etc.) gives me PERSON entity.</p>\n<p>Please advise if there is a model that already trained?</p>\n<p>Desired output:</p>\n<pre><code>{&quot;John&quot;: &quot;First Name&quot;, &quot;Doe&quot;: &quot;Last Name&quot;}\n{&quot;Doe&quot;: &quot;Last Name&quot;, &quot;John&quot;: &quot;First Name&quot;}\n{&quot;Albert&quot;: &quot;First Name&quot;, &quot;Green&quot;: &quot;Last Name&quot;}\n{&quot;Greenshpan&quot;: &quot;Last Name&quot;, &quot;David&quot;: &quot;First Name&quot;}\n</code></pre>\n",
    "score": 7,
    "creation_date": 1647207791,
    "view_count": 3289,
    "answer_count": 1,
    "tags": "python-3.x;nlp;nltk;spacy;named-entity-recognition"
  },
  {
    "question_id": 67193312,
    "title": "Huggingface Transformers returning &#39;ValueError: too many values to unpack (expected 2)&#39;, upon training a Bert binary classification model",
    "body": "<p>I am learning how to use the Huggingface Transformers library, building a binary classification BERT model, on the Kaggle Twitter Disaster Dataset.</p>\n<p>Upon entering the training loop, I get the following error, during the forward() function execution:</p>\n<pre><code>Epoch 1/50\n----------\nAici incepe train_epoch\n\n/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n\n----Checkpoint train_epoch 2----\n----Checkpoint train_epoch 2----\n----forward checkpoint 1----\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n&lt;ipython-input-175-fd9f98819b6f&gt; in &lt;module&gt;()\n     23     device,\n     24     scheduler,\n---&gt; 25     df_train.shape[0]\n     26   )\n     27     print(f'Train loss {train_loss} Accuracy:{train_acc}')\n\n4 frames\n\n&lt;ipython-input-173-bfbecd87c5ec&gt; in train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\n     21         targets = d['targets'].to(device)\n     22         print('----Checkpoint train_epoch 2----')\n---&gt; 23         outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n     24         print('----Checkpoint train_epoch 3----')\n     25         _,preds = torch.max(outputs,dim=1)\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    887             result = self._slow_forward(*input, **kwargs)\n    888         else:\n--&gt; 889             result = self.forward(*input, **kwargs)\n    890         for hook in itertools.chain(\n    891                 _global_forward_hooks.values(),\n\n&lt;ipython-input-171-e754ea3edc36&gt; in forward(self, input_ids, attention_mask)\n     16                 input_ids=input_ids,\n     17                 attention_mask=attention_mask,\n---&gt; 18                 return_dict=False)\n     19 \n     20         print('----forward checkpoint 2-----')\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    887             result = self._slow_forward(*input, **kwargs)\n    888         else:\n--&gt; 889             result = self.forward(*input, **kwargs)\n    890         for hook in itertools.chain(\n    891                 _global_forward_hooks.values(),\n\n/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\n    923         elif input_ids is not None:\n    924             input_shape = input_ids.size()\n--&gt; 925             batch_size, seq_length = input_shape\n    926         elif inputs_embeds is not None:\n    927             input_shape = inputs_embeds.size()[:-1]\n\nValueError: too many values to unpack (expected 2)\n</code></pre>\n<p>At first, I thought it was related to the return_dict=False change that they added, but I was wrong.\nThe code for the classifier and training loop is down below</p>\n<p>Classifier:</p>\n<pre><code>class DisasterClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(DisasterClassifier,self).__init__()\n        self.bert=BertModel.from_pretrained(PRE_TRAINED_MODEL,return_dict=False)\n        self.drop=nn.Dropout(p=0.3) # in timpul antrenarii, valori aleatorii sunt inlocuite cu 0, cu probabilitate p -&gt; regularization and preventing the co-adaptation of neurons\n        self.out = nn.Linear(self.bert.config.hidden_size,n_classes)\n        \n    def forward(self,input_ids,attention_mask):\n        print('----forward checkpoint 1----')\n        bertOutput = self.bert(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=False)\n        \n        print('----forward checkpoint 2-----')\n        output = self.drop(bertOutput['pooler_output'])\n        return self.out(output)`\n</code></pre>\n<p>Training epoch:</p>\n<pre><code>optimizer = AdamW(model.parameters(),lr = 2e-5,correct_bias=False)\ntotal_steps = len(train_data_loader)*EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n                                            optimizer,\n                                            num_warmup_steps=0,\n                                            num_training_steps=total_steps)\nloss_fn = nn.CrossEntropyLoss().to(device)\n\ndef train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n print('Aici incepe train_epoch') \n model = model.train()\n losses =[]\n correct_predictions = 0\n    \n for d in data_loader:\n        print('----Checkpoint train_epoch 2----')\n        input_ids = d['input_ids'].to(device)\n        attention_mask=d['attention_mask'].to(device)\n        targets = d['targets'].to(device)\n        print('----Checkpoint train_epoch 2----')\n        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n        print('----Checkpoint train_epoch 3----')\n        _,preds = torch.max(outputs,dim=1)\n        loss = loss_fn(outputs, targets)\n    \n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n\n        #backpropagation steps\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters,max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n   \n return (correct_predictions.double() / n_examples), np.mean(losses)\n</code></pre>\n<p>And the training loop:</p>\n<pre><code>history = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n    \n    # train_acc,train_loss = train_epoch(model,\n    #                                    train_data_loader,\n    #                                    loss_fn,\n    #                                    optimizer,\n    #                                    device,\n    #                                    scheduler,\n    #                                    len(df_train))\n    \n    train_acc, train_loss = train_epoch(\n    model,\n    train_data_loader,    \n    loss_fn, \n    optimizer, \n    device, \n    scheduler, \n    df_train.shape[0]\n  )\n    print(f'Train loss {train_loss} Accuracy:{train_acc}')\n    \n    val_acc, val_loss = eval_model(model,val_data_loader,loss_fn,device,len(df_val))\n    print(f'Validation loss {val_loss} Accuracy:{val_acc}')\n    print()\n        \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n\n    if val_acc &gt; best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc\n</code></pre>\n<p>Has anybody encountered a similar situation?</p>\n",
    "score": 7,
    "creation_date": 1618998474,
    "view_count": 13613,
    "answer_count": 1,
    "tags": "python;nlp;classification;huggingface-transformers"
  },
  {
    "question_id": 64253599,
    "title": "SPACY - Confusion about word vectors and tok2vec",
    "body": "<p>it would be really helpful for me if you would help me understand some underlying concepts about Spacy.</p>\n<p>I understand some spacy models have some predefined static vectors, for example, for the Spanish models these are the vectors generated by FastText.\nI also understand that there is a tok2vec layer that generates vectors from tokens, and this is used for example as the input of the NER components of the model.</p>\n<p>If the above is correct, then I have some questions:</p>\n<ul>\n<li>Does the NER component also use the static vectors?\n<ul>\n<li>If yes, then where does the tok2vec layer comes into play?</li>\n<li>If no, then is there any advantage on using the lg or md models if you only intend to use the model for e.g. the NER component?</li>\n</ul>\n</li>\n<li>Is the tok2vec layer already trained for pretrained downloaded models, e.g. Spanish?</li>\n<li>If I replace the NER component of a pretrained model, does it keep the tok2vec layer untouched i.e. with the learned weights?</li>\n<li>Is the tok2vec layer also trained when I train a NER model?</li>\n<li>Would the pretrain command help the tok2vec layer learn some domain-specific words that may be OOV?</li>\n</ul>\n<p>Thanks a lot!</p>\n",
    "score": 7,
    "creation_date": 1602112696,
    "view_count": 7273,
    "answer_count": 1,
    "tags": "python;nlp;spacy;fasttext"
  },
  {
    "question_id": 64109483,
    "title": "How to recognize if string is human name?",
    "body": "<p>So I have some text data that's been messily parsed, and due to that I get names mixed in with the actual data. Is there any kind of package/library that helps identify whether a word is a name or not? (In this case, I would be assuming US/western/euro-centric names)</p>\n<p>Otherwise, what would be a good way to flag this? Maybe train a model on a corpus of names and assign each word in the dataset a classification? Just not sure the best way to approach this problem/what kind of model would be suited, or if a solution already exists</p>\n",
    "score": 7,
    "creation_date": 1601326405,
    "view_count": 9473,
    "answer_count": 1,
    "tags": "python;nlp"
  },
  {
    "question_id": 63157909,
    "title": "How to determine if two sentences talk about similar topics?",
    "body": "<p>I would like to ask you a question. Is there any algorithm/tool which can allow me to do some association between words?\nFor example: I have the following group of sentences:</p>\n<pre><code>(1)\n    &quot;My phone is on the table&quot;\n    &quot;I cannot find the charger&quot;. # no reference on phone\n(2) \n    &quot;My phone is on the table&quot;\n    &quot;I cannot find the phone's charger&quot;. \n</code></pre>\n<p>What I would like to do is to find a connection, probably a semantic connection, which can allow me to say that the first two sentences are talking about a topic (phone) as two terms (phone and charger) are common within it (in general). Same for the second sentence.\nI should have something that can connect phone to charger, in the first sentence.\nI was thinking of using Word2vec, but I am not sure if this is something that I can do with it.\nDo you have any suggestions about algorithms that I can use to determine similarity of topics (i.e. sentence which are formulated in a different way, but having same topic)?</p>\n",
    "score": 7,
    "creation_date": 1596039109,
    "view_count": 3975,
    "answer_count": 2,
    "tags": "python;algorithm;nlp;sentence-similarity"
  },
  {
    "question_id": 60382793,
    "title": "What are the inputs to the transformer encoder and decoder in BERT?",
    "body": "<p>I was reading the <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"noreferrer\">BERT paper</a> and was not clear regarding the inputs to the <a href=\"https://arxiv.org/pdf/1706.03762.pdf\" rel=\"noreferrer\">transformer</a> encoder and decoder. </p>\n\n<p>For learning masked language model (Cloze task), the paper says that 15% of the tokens are masked and the network is trained to predict the masked tokens. Since this is the case, what are the inputs to the transformer encoder and decoder?</p>\n\n<p><a href=\"https://i.sstatic.net/jIIuo.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/jIIuo.png\" alt=\"BERT input representation (from the paper)\"></a></p>\n\n<p>Is the input to the transformer encoder this input representation (see image above). If so, what is the decoder input?</p>\n\n<p>Further, how is the output loss computed? Is it a softmax for only the masked locations? For this, the same linear layer is used for all masked tokens?</p>\n",
    "score": 7,
    "creation_date": 1582573021,
    "view_count": 4793,
    "answer_count": 1,
    "tags": "python;deep-learning;nlp;huggingface-transformers"
  },
  {
    "question_id": 58226482,
    "title": "What is the difference between spacy.lang.en and load(&#39;en&#39;)?",
    "body": "<p>In my studies on NLP, more specifically the spacy library, I was confused with that, \nwhat is the difference between <code>from spacy.lang.en import English()</code> and <code>spacy.load('en')</code> and how it works? Someone can help me explain this and if possible with some example of this difference? Thanks in advance.</p>\n",
    "score": 7,
    "creation_date": 1570134729,
    "view_count": 1822,
    "answer_count": 1,
    "tags": "python;nlp;jupyter-notebook;spacy"
  },
  {
    "question_id": 57057992,
    "title": "Wordpiece tokenization versus conventional lemmatization?",
    "body": "<p>I'm looking at NLP preprocessing. At some point I want to implement a context-sensitive word embedding, as a way of discerning word sense, and I was thinking about using the output from BERT to do so. I noticed BERT uses WordPiece tokenization (for example, \"playing\" -> \"play\" + \"##ing\").</p>\n\n<p>Right now, I have my text preprocessed using a standard tokenizer that splits on spaces / some punctuation, and then I have a lemmatizer (\"playing\" ->\"play\"). I'm wondering what the benefit of WordPiece tokenization is over a standard tokenization + lemmatization. I know WordPiece helps with out of vocabulary words, but is there anything else? That is, even if I don't end up using BERT, should I consider replacing my tokenizer + lemmatizer with wordpiece tokenization? In what situations would that be useful?</p>\n",
    "score": 7,
    "creation_date": 1563282457,
    "view_count": 1773,
    "answer_count": 1,
    "tags": "nlp;tokenize;lemmatization"
  },
  {
    "question_id": 54905774,
    "title": "why softmax get small gradient when the value is large in paper &#39;Attention is all you need&#39;",
    "body": "<p>This is the screen of the original paper: <a href=\"https://i.sstatic.net/J9thT.png\" rel=\"noreferrer\">the screen of the paper</a>. I understand the meaning of the paper is that when the value of dot-product is large, the gradient of softmax will get very small.\n<br/>However, I tried to calculate the gradient of softmax with the cross entropy loss and found that the gradient of softmax is not directly related to value passed to softmax. <br/>Even the single value is large, it still can get a large gradient when ather values are large. (sorry about that I don't know how to pose the calculation process here)</p>\n",
    "score": 7,
    "creation_date": 1551271334,
    "view_count": 2767,
    "answer_count": 3,
    "tags": "deep-learning;nlp;softmax;attention-model"
  },
  {
    "question_id": 50743734,
    "title": "NLP, spaCy: Strategy for improving document similarity",
    "body": "<p><strong>One sentence backdrop</strong>: I have text data from auto-transcribed talks, and I want to compare their similarity of their content (e.g. what they are talking about) to do clustering and recommendation. I am quite new to NLP.</p>\n<hr />\n<p><strong>Data</strong>: The data I am using is available <a href=\"https://github.com/TMorville/transcribed_data\" rel=\"nofollow noreferrer\">here</a>. For all the lazy ones</p>\n<p><code>clone https://github.com/TMorville/transcribed_data</code></p>\n<p>and here is a snippet of code to put it in a df:</p>\n<pre><code>import os, json\nimport pandas as pd\n\nfrom pandas.io.json import json_normalize \n\ndef td_to_df():\n    \n    path_to_json = '#FILL OUT PATH'\n    json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('td.json')]\n\n    tddata = pd.DataFrame(columns=['trans', 'confidence'])\n\n    for index, js in enumerate(json_files):\n        with open(os.path.join(path_to_json, js)) as json_file:\n            json_text = json_normalize(json.load(json_file))\n\n            tddata['trans'].loc[index] = str(json_text['trans'][0])\n            tddata['confidence'].loc[index] = str(json_text['confidence'][0])\n\n    return tddata\n</code></pre>\n<hr />\n<p><strong>Approach</strong>: So far, I have only used the spaCy package to do &quot;out of the box&quot; similarity. I simply apply the nlp model on the entire corpus of text, and compare it to all others.</p>\n<pre><code>def similarity_get():\n    \n    tddata = td_to_df()\n    \n    nlp = spacy.load('en_core_web_lg')\n    \n    baseline = nlp(tddata.trans[0])\n    \n    for text in tddata.trans:\n        print (baseline.similarity(nlp(text)))\n</code></pre>\n<hr />\n<p><strong>Problem</strong>: <em>Practically all similarities comes out as &gt; 0.95</em>. This is more or less independent of the baseline.  Now, this may not come a major surprise given the lack of preprocessing.</p>\n<hr />\n<p><strong>Solution strategy</strong>: Following the advice in <a href=\"https://stackoverflow.com/questions/49767270/document-similarity-in-spacy-vs-word2vec\">this post</a>, I would like to do the following (using spaCy where possible): 1) Remove stop words. 2) Remove most frequent words. 3) Merge word pairs. 4) Possibly use Doc2Vec outside of spaCy.</p>\n<hr />\n<p><strong>Questions</strong>: Does the above seem like a sound strategy? If no, what's missing? If yes, how much of this already happening under the hood by using the pre-trained model loaded in <code>nlp = spacy.load('en_core_web_lg')</code>?</p>\n<p>I can't seem find the documentation that demonstrates what exactly these models are doing, or how to configure it. A <a href=\"https://www.google.dk/search?ei=Xj8ZW8i2FKGB6QSx4qX4Cg&amp;q=spacy.load%20config%20api&amp;oq=spacy.load%20config%20api&amp;gs_l=psy-ab.3...8259.8938.0.9041.4.4.0.0.0.0.73.281.4.4.0....0...1c.1.64.psy-ab..0.3.218...33i160k1.0.0yzROYBDp-g\" rel=\"nofollow noreferrer\">quick google search yields nothing</a> and even the, very neat, <a href=\"https://spacy.io/api/\" rel=\"nofollow noreferrer\">api documentation</a> does not seem to help. Perhaps I am looking in the wrong place?</p>\n",
    "score": 7,
    "creation_date": 1528381524,
    "view_count": 3798,
    "answer_count": 1,
    "tags": "nlp;similarity;spacy"
  },
  {
    "question_id": 42363897,
    "title": "AttributeError: type object &#39;Word2Vec&#39; has no attribute &#39;load_word2vec_format&#39;",
    "body": "<p>I am trying to implement word2vec model and getting Attribute error </p>\n\n<blockquote>\n  <p>AttributeError: type object 'Word2Vec' has no attribute 'load_word2vec_format'</p>\n</blockquote>\n\n<p>Below is the code :</p>\n\n<pre><code>wv = Word2Vec.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\nwv.init_sims(replace=True)\n</code></pre>\n\n<p>Please let me know the issue ?</p>\n",
    "score": 7,
    "creation_date": 1487670573,
    "view_count": 16416,
    "answer_count": 2,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 40752242,
    "title": "Machine Learning - Information extraction from a document",
    "body": "<p>I'm trying to train a couple of neural networks (using tensorflow) to be able to extract semantic information from invoices. After a long list of reading I came up with this:</p>\n\n<ul>\n<li>Use <a href=\"https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html\" rel=\"nofollow noreferrer\">word2vec</a> to generate word embeddings (more on the corpus below).</li>\n<li>Feed the output of <code>word2vec</code> to a CNN since vectors that are close together share similar semantic meanings.</li>\n</ul>\n\n<p>So the very high level approach I described above seems quite alright to me. I would love for it to be corrected if anything looks wrong.</p>\n\n<p>A couple of concerns that I have:</p>\n\n<ol>\n<li>Corpus selection. Is it sufficient to use a generic corpus of, for instance, wikipedia? Or should I use a specialized corpus for invoices? If it's the latter, how can I generate this corpus? I do have a big dataset of invoices that I can utilize.</li>\n<li>Information extraction. Let's say all of the above work fine and I'm able to understand semantic information from a new unseen invoice. How do I go about extracting certain pieces of information? For instance, let's say we introduce a new invoice that has <code>order number: 12345</code>, assuming <code>order number</code> is understood to be the <strong>invoice number</strong> (or whatever vectors that lie in the same vicinity of <code>order number</code>), how do I extract the value <code>12345</code>? One area I was looking at is <a href=\"https://github.com/tensorflow/models/tree/master/syntaxnet\" rel=\"nofollow noreferrer\">SyntaxNet</a> that could help here.</li>\n</ol>\n\n<p>Any help/insight is appreciated.</p>\n\n<p><strong>Follow up to @wasi-ahmad's question</strong>:\nThe reason I'm trying to understand semantic information about an invoice is to ultimately be able to extract values out of it. So, for instance, if I present an unseen invoice to my neural network it would find the invoice's number (whatever its label is called) and extract its value.</p>\n",
    "score": 7,
    "creation_date": 1479851210,
    "view_count": 4532,
    "answer_count": 2,
    "tags": "machine-learning;nlp;tensorflow"
  },
  {
    "question_id": 29169732,
    "title": "Sentiment analysis of non-English texts",
    "body": "<p>I want to analyze sentiment of texts that are written in German. I found a lot of tutorials on how to do this with English, but I found none on how to apply it to different languages.</p>\n\n<p>I have an idea to use the <code>TextBlob</code> Python library to first translate the sentences into English and then to do sentiment analysis, but I am not sure whether or not it is the best way to solve this task.</p>\n\n<p>Or are there any other possible ways to solve this task?</p>\n",
    "score": 7,
    "creation_date": 1426864027,
    "view_count": 9168,
    "answer_count": 5,
    "tags": "python;machine-learning;nlp;sentiment-analysis;textblob"
  },
  {
    "question_id": 8818265,
    "title": "Using my own corpus for category classification in Python NLTK",
    "body": "<p>I'm a NTLK/Python beginner and managed to load my own corpus using CategorizedPlaintextCorpusReader but how do I actually train and use the data for classification of text?</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n&gt;&gt;&gt; reader = CategorizedPlaintextCorpusReader('/ebs/category', r'.*\\.txt', cat_pattern=r'(.*)\\.txt')\n&gt;&gt;&gt; len(reader.categories())\n234\n</code></pre>\n",
    "score": 7,
    "creation_date": 1326280403,
    "view_count": 3054,
    "answer_count": 1,
    "tags": "python;nlp;machine-learning;nltk;corpus"
  },
  {
    "question_id": 7302431,
    "title": "Decoding Permutated English Strings",
    "body": "<p>A coworker was recently asked this when trying to land a (different) research job:</p>\n\n<p>Given 10 128-character strings which have been permutated in exactly the same way, decode the strings. The original strings are English text with spaces, numbers, punctuation and other non-alpha characters removed.</p>\n\n<p>He was given a few days to think about it before an answer was expected. How would you do this? You can use any computer resource, including character/word level language models.</p>\n",
    "score": 7,
    "creation_date": 1315174018,
    "view_count": 457,
    "answer_count": 4,
    "tags": "algorithm;puzzle;nlp"
  },
  {
    "question_id": 51038266,
    "title": "Why my tensorflow model outputs become NaN after x epochs?",
    "body": "<p>After 85 epochs the loss (a cosine distance) of my model (a RNN with 3 LSTM layers) become NaN. Why does it happen and how can I fix it? Outputs of my model also become NaN.</p>\n\n<p>My model :</p>\n\n<pre><code>tf.reset_default_graph()\n\nseqlen = tf.placeholder(tf.int32, [None])\nx_id = tf.placeholder(tf.int32, [None, None])\ny_id = tf.placeholder(tf.int32, [None, None])\n\nembeddings_matrix = tf.placeholder(np.float32, [vocabulary_size, embedding_size])\nx_emb = tf.nn.embedding_lookup(embeddings_matrix, x_id)\ny_emb = tf.nn.embedding_lookup(embeddings_matrix, y_id)\n\ncells = [tf.contrib.rnn.LSTMCell(s, activation=a) for s, a in [(400, tf.nn.relu), (400, tf.nn.relu), (400, tf.nn.tanh)]]\ncell = tf.contrib.rnn.MultiRNNCell(cells)\n\noutputs, _ = tf.nn.dynamic_rnn(cell, x_emb, dtype=tf.float32, sequence_length=seqlen)\n\nloss = tf.losses.cosine_distance(tf.nn.l2_normalize(outputs, 2), tf.nn.l2_normalize(y_emb, 2), 1)\ntf.summary.scalar('loss', loss)\nopt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\nmerged = tf.summary.merge_all()\n</code></pre>\n\n<p>The output of the training :</p>\n\n<pre><code>Epoch 80/100\n    Time : 499 s    Loss : 0.972911523852701    Val Loss : 0.9729658\nEpoch 81/100\n    Time : 499 s    Loss : 0.9723407568655597   Val Loss : 0.9718646\nEpoch 82/100\n    Time : 499 s    Loss : 0.9718870568505438   Val Loss : 0.971976\nEpoch 83/100\n    Time : 499 s    Loss : 0.9913996352643445   Val Loss : 0.990693\nEpoch 84/100\n    Time : 499 s    Loss : 0.9901496524596137   Val Loss : 0.98957264\nEpoch 85/100\n    Time : 499 s    Loss : nan  Val Loss : nan\nEpoch 86/100\n    Time : 498 s    Loss : nan  Val Loss : nan\nEpoch 87/100\n    Time : 498 s    Loss : nan  Val Loss : nan\nEpoch 88/100\n    Time : 499 s    Loss : nan  Val Loss : nan\nEpoch 89/100\n    Time : 498 s    Loss : nan  Val Loss : nan\nEpoch 90/100\n    Time : 498 s    Loss : nan  Val Loss : nan\n</code></pre>\n\n<p>And here sis the curve of the loos during the entire training :\n<a href=\"https://i.sstatic.net/E6QGm.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/E6QGm.png\" alt=\"Loss\"></a></p>\n\n<p>The blue curve is the loss on training data and the orange one in the loss on validation data.</p>\n\n<p>The learning rate used for ADAM is 0.001.</p>\n\n<p>My x and y got the following shape : [batch size, maximum sequence length], they're both set to None, because the last batch of each epoch is smaller, and the maximal sequence length change at each batch.</p>\n\n<p>x and y go through an embedding lookup and become of shape [batch size, maximum sequence length, embedding size], the embedding for the padding word is a vector of 0.</p>\n\n<p>The dynamic rnn take the length of each sequence (seqlen in the code, with a shape of [batch size]) so it will only make predictions for the exact length of each sequence and the rest of the output will be padded with vectors of zero, as for y.</p>\n\n<p>My guess is the values of the output become so close of zero, that once they're squared to compute the cosine distance they become 0 so it leads to a division by zero.</p>\n\n<p>Cosine distance formula :<br>\n<img src=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=1%20-%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20O_%7Bi%7D%20Y_%7Bi%7D%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20O_%7Bi%7D%5E%7B2%7D%7D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20Y_%7Bi%7D%5E%7B2%7D%7D%7D\" alt=\"cosine distance\"></p>\n\n<p>I don't know if I'm right, neither how to prevent this.</p>\n\n<p><strong>EDIT:</strong><br>\nI just checked weights of every layers and they're all NaN</p>\n\n<p><strong>SOLVED:</strong><br>\nUsing a l2 regularization worked.</p>\n\n<pre><code>tf.reset_default_graph()\n\nseqlen = tf.placeholder(tf.int32, [None])\nx_id = tf.placeholder(tf.int32, [None, None])\ny_id = tf.placeholder(tf.int32, [None, None])\n\nembeddings_matrix = tf.placeholder(np.float32, [vocabulary_size, embedding_size])\nx_emb = tf.nn.embedding_lookup(embeddings_matrix, x_id)\ny_emb = tf.nn.embedding_lookup(embeddings_matrix, y_id)\n\ncells = [tf.contrib.rnn.LSTMCell(s, activation=a) for s, a in [(400, tf.nn.relu), (400, tf.nn.relu), (400, tf.nn.tanh)]]\ncell = tf.contrib.rnn.MultiRNNCell(cells)\n\noutputs, _ = tf.nn.dynamic_rnn(cell, x_emb, dtype=tf.float32, sequence_length=seqlen)\n\nregularizer = tf.reduce_sum([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\ncos_distance = tf.losses.cosine_distance(tf.nn.l2_normalize(outputs, 2), tf.nn.l2_normalize(y_emb, 2), 1)\nloss = cos_distance + beta * regularizer\n\nopt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\ntf.summary.scalar('loss', loss)\ntf.summary.scalar('regularizer', regularizer)\ntf.summary.scalar('cos_distance', cos_distance)\nmerged = tf.summary.merge_all()\n</code></pre>\n",
    "score": 7,
    "creation_date": 1530001582,
    "view_count": 3833,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;deep-learning"
  },
  {
    "question_id": 50454857,
    "title": "Determine if a text extract from spacy is a complete sentence",
    "body": "<p>We are working on sentences extracted from a PDF. The problem is that it includes the title, footers, table of contents, etc. Is there a way to determine if the sentence we get when pass the document to spacy is a complete sentence. Is there a way to filter parts of sentences like titles?</p>\n",
    "score": 7,
    "creation_date": 1526928231,
    "view_count": 5815,
    "answer_count": 3,
    "tags": "python;nlp"
  },
  {
    "question_id": 48074642,
    "title": "Fuzzy matching a word inside a pyspark dataframe string",
    "body": "<p>I have some data in which column 'X' contains strings. I am writing a function, using pyspark, where a search_word is passed and all rows which do not contain the substring search_word within the column 'X' string are filtered out. The function must also allow for misspellings of the word, i.e. fuzzy matching.\nI have loaded the data into a pyspark dataframe and written a function using the NLTK and fuzzywuzzy python libraries to return True or False if the string contains the search_word.</p>\n\n<p>My problem is that I cannot map the function to the dataframe correctly.\nAm I approaching this problem incorrectly? Should I be trying to do the fuzzy match through some kind of SQL query, or using an RDD perhaps?</p>\n\n<p>I am new to pyspark so I feel like this question must have been answered before but I cannot find the answer anywhere. I have never done any NLP with SQL and I have never heard of SQL being capable of fuzzy matching a substring.</p>\n\n<p><em>Update #1</em></p>\n\n<p>The function looks like:</p>\n\n<pre><code>wf = WordFinder(search_word='some_substring')\nresult1 = wf.find_word_in_string(string_to_search='string containing some_substring or misspelled some_sibstrung')\nresult2 = wf.find_word_in_string(string_to_search='string not containing the substring')\n</code></pre>\n\n<p>result1 is True</p>\n\n<p>result2 is False</p>\n",
    "score": 7,
    "creation_date": 1514971927,
    "view_count": 9134,
    "answer_count": 1,
    "tags": "python;nlp;pyspark;apache-spark-sql;fuzzy-search"
  },
  {
    "question_id": 44152302,
    "title": "Which database can be used to store processed data from NLP engine",
    "body": "<p>I am looking at taking unstructured data in the form of files, processing it and storing it in a <code>database</code> for retrieval.\nThe data will be in natural language and the queries to get information will also be in natural language.\nEx: the data could be <strong>\"Roses are red\"</strong> and the query could be <strong>\"What is the color of a rose?\"</strong></p>\n\n<p>I have looked at several <code>nlp</code> systems, focusing more on <code>open-source</code> information extraction and relation extraction system and the following seems apt and easy for quick start:\n<a href=\"https://www.npmjs.com/package/mitie\" rel=\"noreferrer\"><a href=\"https://www.npmjs.com/package/mitie\" rel=\"noreferrer\">https://www.npmjs.com/package/mitie</a></a></p>\n\n<p>This can give data in the form of (word,type) pairs. It also gives a relation as result of running the the processing (check the site example).</p>\n\n<p>I want to know if <code>sql</code> is good <code>database</code> to save this information. For retrieving the information, I will need to convert the natural language query also to some kind of (word, meaning) pairs\nand for using <code>sql</code> I will have to write a layer that converts natural language to <code>sql</code> queries.  </p>\n\n<p>Please suggest if there are any open source <code>database</code> that work well in this situation. I'm open to suggestions for databases that work with other <code>open-source</code> information extraction and relation extraction systems if not MITIE.</p>\n",
    "score": 7,
    "creation_date": 1495612531,
    "view_count": 3694,
    "answer_count": 1,
    "tags": "mysql;database;nlp;information-retrieval;information-extraction"
  },
  {
    "question_id": 37270999,
    "title": "SyntaxNet creating tree to root verb",
    "body": "<p>I am new to Python and the world of NLP.  The recent announcement of Google's Syntaxnet intrigued me. However I am having a lot of trouble understanding documentation around both syntaxnet and related tools (nltk, etc.)</p>\n\n<p><strong>My goal:</strong> given an input such as \"Wilbur kicked the ball\" I would like to extract the root verb (kicked) and the object it pertains to \"the ball\".</p>\n\n<p>I stumbled across \"spacy.io\" and <a href=\"https://spacy.io/demos/displacy?share=8251221623493000114\" rel=\"nofollow noreferrer\">this visualization</a> seems to encapsulate what I am trying to accomplish: POS tag a string, and load it into some sort of tree structure so that I can start at the root verb and traverse the sentence.</p>\n\n<p>I played around with the syntaxnet/demo.sh, and as suggested in <a href=\"https://stackoverflow.com/questions/37219598/how-to-get-dependency-parse-output-from-syntaxnet\">this thread</a> commented out the last couple lines to get conll output.</p>\n\n<p>I then loaded this input in a python script (kludged together myself, probably not correct):</p>\n\n<pre><code>import nltk\nfrom nltk.corpus import ConllCorpusReader\ncolumntypes = ['ignore', 'words', 'ignore', 'ignore', 'pos']\ncorp = ConllCorpusReader('/Users/dgourlay/development/nlp','input.conll', columntypes)\n</code></pre>\n\n<p>I see that I have access to corp.tagged_words(), but no relationship between the words. Now I am stuck!  How can I load this corpus into a tree type structure?  </p>\n\n<p>Any help is much appreciated!</p>\n",
    "score": 7,
    "creation_date": 1463473995,
    "view_count": 1788,
    "answer_count": 3,
    "tags": "python;nlp;syntaxnet"
  },
  {
    "question_id": 31959690,
    "title": "Choosing appropriate sense of a word from wordnet",
    "body": "<p>I am using Wordnet for finding synonyms of ontology concepts. How can i find choose the appropriate sense for my ontology concept. e.g there is an ontlogy concept \"conference\" it has following synsets in wordnet \nThe noun conference has 3 senses (first 3 from tagged texts)</p>\n\n<ol>\n<li>(12) conference -- (a prearranged meeting for consultation or exchange of information or discussion (especially one with a formal agenda))</li>\n<li>(2) league, conference -- (an association of sports teams that organizes matches for its members)</li>\n<li>(2) conference, group discussion -- (a discussion among participants who have an agreed (serious) topic)\nnow 1st and 3rd synsets have apprpriate sense for my ontology concept. How can i choose only these two from wordnet?</li>\n</ol>\n",
    "score": 7,
    "creation_date": 1439367458,
    "view_count": 2826,
    "answer_count": 3,
    "tags": "nlp;ontology;wordnet;word-sense-disambiguation"
  },
  {
    "question_id": 27814959,
    "title": "Extract Person Name from unstructure text",
    "body": "<p>I have a collection of bills and Invoices, so there is no context in the text (i mean they don't tell a story).\nI want to extract people names from those bills.\nI tried OpenNLP but the quality of trained model is not good because i don't have context.\nso the first question is: can I train model contains only people names without context? and if that possible can you give me good article for how i build that new model (most of the article that i read didn't explain the steps that i should made to build new model).</p>\n\n<p>I have database name with more than 100,000 person name (first name, last name), so if the NER systems don't work in my case (because there is no context), what is the best way to search for those candidates (I mean searching for every first name with all other last names?)</p>\n\n<p>thanks.</p>\n",
    "score": 7,
    "creation_date": 1420618232,
    "view_count": 1958,
    "answer_count": 2,
    "tags": "nlp;text-mining;opennlp;named-entity-recognition"
  },
  {
    "question_id": 22646060,
    "title": "Entity Recognition and Sentiment Analysis using NLP",
    "body": "<p>So, this question might be a little naive, but I thought asking the friendly people of Stackoverflow wouldn't hurt.</p>\n<p>My current company has been using a third party API for NLP for a while now. We basically URL encode a string and send it over, and they extract certain entities for us (we have a list of entities that we're looking for) and return a json mapping of entity : sentiment. We've recently decided to bring this project in house instead.</p>\n<p>I've been studying NLTK, Stanford NLP and lingpipe for the past 2 days now, and can't figure out if I'm basically reinventing the wheel doing this project.</p>\n<p>We already have massive tables containing the original unstructured text and another table containing the extracted entities from that text and their sentiment. The entities are single words. For example:</p>\n<blockquote>\n<p>Unstructured text : Now for the bed. It wasn't the best.</p>\n<p>Entity : Bed</p>\n<p>Sentiment : Negative</p>\n</blockquote>\n<p>I believe that implies we have training data (unstructured text) as well as entity and sentiments. Now how I can go about using this training data on one of the NLP frameworks and getting what we want? No clue. I've sort of got the steps, but not sure:</p>\n<ol>\n<li>Tokenize sentences</li>\n<li>Tokenize words</li>\n<li>Find the noun in the sentence (POS tagging)</li>\n<li>Find the sentiment of that sentence.</li>\n</ol>\n<p>But that should fail for the case I mentioned above since it talks about the bed in 2 different sentences?</p>\n<p>So the question - Does any one know what the best framework would be for accomplishing the above tasks, and any tutorials on the same (Note: I'm not asking for a solution). If you've done this stuff before, is this task too large to take on? I've looked up some commercial APIs but they're absurdly expensive to use (we're a tiny startup).</p>\n<p>Thanks stackoverflow!</p>\n",
    "score": 7,
    "creation_date": 1395781010,
    "view_count": 3177,
    "answer_count": 4,
    "tags": "nlp;nltk;stanford-nlp;sentiment-analysis;lingpipe"
  },
  {
    "question_id": 16929203,
    "title": "Python: using scikit-learn to predict, gives blank predictions",
    "body": "<p>I work in customer support, and I'm using scikit-learn to predict tags for our tickets, given a training set of tickets (approx. 40,000 tickets in the training set).</p>\n\n<p>I'm using the classification model based on <a href=\"https://stackoverflow.com/questions/10526579/use-scikit-learn-to-classify-into-multiple-categories?rq=1\">this one</a>. It's predicting just \"()\" as the tags for many of my test set of tickets, even though none of the tickets in the training set are without tags.</p>\n\n<p>My training data for tags is a list of lists, like: </p>\n\n<pre><code>tags_train = [['international_solved'], ['from_build_guidelines my_new_idea eligibility'], ['dropbox other submitted_faq submitted_help'], ['my_new_idea_solved'], ['decline macro_backer_paypal macro_prob_errored_pledge_check_credit_card_us loading_problems'], ['dropbox macro__turnaround_time other plq__turnaround_time submitted_help'], ['dropbox macro_creator__logo_style_guide outreach press submitted_help']]\n</code></pre>\n\n<p>While my training data for ticket descriptions is just a list of strings, e.g.:</p>\n\n<pre><code>descs_train = ['description of ticket one', 'description of ticket two', etc]\n</code></pre>\n\n<p>Here's the relevant part of my code to build the model:</p>\n\n<pre><code>import numpy as np\nimport scipy\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\n# We have lists called tags_train, descs_train, tags_test, descs_test with the test and train data\n\nX_train = np.array(descs_train)\ny_train = tags_train\nX_test = np.array(descs_test)  \n\nclassifier = Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', OneVsRestClassifier(LinearSVC(class_weight='auto')))])\n\nclassifier.fit(X_train, y_train)\npredicted = classifier.predict(X_test)\n</code></pre>\n\n<p>However, \"predicted\" gives a list that looks like:</p>\n\n<pre><code>predicted = [(), ('account_solved',), (), ('images_videos_solved',), ('my_new_idea_solved',), (), (), (), (), (), ('images_videos_solved', 'account_solved', 'macro_launched__edit_update other tips'), ('from_guidelines my_new_idea', 'from_guidelines my_new_idea macro__eligibility'), ()]\n</code></pre>\n\n<p>I don't understand why it's predicting blank () when there are none in the training set. Shouldn't it predict the closest tag? Can anyone recommend any improvements to the model I'm using?</p>\n\n<p>Thank you so much for your help in advance!</p>\n",
    "score": 7,
    "creation_date": 1370389051,
    "view_count": 4406,
    "answer_count": 2,
    "tags": "python;nlp;scipy;classification"
  },
  {
    "question_id": 8147751,
    "title": "Natural Language to Sparql",
    "body": "<p>I'm building a small prototype of a Movies semantic search engine based on the data of LinkedIMDB</p>\n\n<p>I've defined some Query Types as an example of use cases </p>\n\n<ul>\n<li>search by entity name search by </li>\n<li>entity type</li>\n<li>search common features between two entities ...etc</li>\n</ul>\n\n<p>So far I've developed a SPARQL engine that takes any type of those Queries and send the Query to the endpoint and preview the result.</p>\n\n<p>The problem here is that I want to make a natural language or semi natural language interface for it in order for users to invoke those sentences using Natural language search Queries. But I don't know from where to start.</p>\n\n<p>I've found some papers that are trying to extract triplets from the text but I don't feel that's the key to the solution.</p>\n\n<p>Also I've found some LSA techniques to interpret Natural language search Queries but I feel it's not applicable to semantic search domain.</p>\n\n<p>Any idea or resources to start reading from?\nIs there a best practice than the natural language interface?</p>\n",
    "score": 7,
    "creation_date": 1321425936,
    "view_count": 5438,
    "answer_count": 2,
    "tags": "nlp;semantic-web;sparql"
  },
  {
    "question_id": 5493565,
    "title": "Wordnet selectional restrictions in NLTK",
    "body": "<p>Is there a way to capture WordNet selectional restrictions (such as +animate, +human, etc.) from synsets through NLTK?\nOr is there any other way of providing semantic information about synset? The closest I could get to it were hypernym relations.</p>\n",
    "score": 7,
    "creation_date": 1301526636,
    "view_count": 880,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 5280572,
    "title": "Korean, Thai and Indonesian POS tagger",
    "body": "<p>Can someone recommend an open source POS tagger for Korean, Indonesian, Thai and Vietnamese?</p>\n\n<p>That I can use to tag the corpus data that I currently have. (e.g. <a href=\"http://nlp.stanford.edu/software/index.shtml\" rel=\"noreferrer\">the stanford-postagger</a>)</p>\n\n<p>If you are a dev and care to share and let me test out the POS tagger, I don't mind either.</p>\n\n<p>With some modifications of the output, I've POS tagged the Vietnamese data with <a href=\"http://sourceforge.net/projects/jvntextpro/\" rel=\"noreferrer\">jvntextpro</a></p>\n\n<p>But I'd still like more input on Korean, Indonesian and Thai POS tagging.</p>\n",
    "score": 7,
    "creation_date": 1299904286,
    "view_count": 3512,
    "answer_count": 2,
    "tags": "nlp;nltk;cjk;pos-tagger;thai"
  },
  {
    "question_id": 1994880,
    "title": "How to recognize words in text with non-word tokens?",
    "body": "<p>I am currently parsing a bunch of mails and want to get words and other interesting tokens out of mails (even with spelling errors or combination of characters and letters, like \"zebra21\" or \"customer242\"). But how can I know that \"0013lCnUieIquYjSuIA\" and \"anr5Brru2lLngOiEAVk1BTjN\" are not words and not relevant?  How to extract words and discard tokens that are encoding errors or parts of pgp signature or whatever else we get in mails and know that we will never be interested in those?</p>\n",
    "score": 7,
    "creation_date": 1262523238,
    "view_count": 1488,
    "answer_count": 3,
    "tags": "algorithm;nlp;lexical-analysis"
  },
  {
    "question_id": 1605493,
    "title": "Natural language command language",
    "body": "<p>I'm interested in developing a natural language command language for a domain with existing rules. I was very impressed when Terry Winograd's <a href=\"http://en.wikipedia.org/wiki/Shrdlu\" rel=\"noreferrer\">SHRDLU</a> showed the way (the conversation below is 40 years old! Astonishing). Can we do better now and if so where can I get examples?</p>\n\n<blockquote>\n  <p>Person: Pick up a big red block.<br>\n  Computer: OK.<br>\n  Person: Grasp the pyramid.<br>\n  Computer: I don't understand which pyramid you mean.<br>\n  Person (changing their mind): Find a block which is taller than the one you are holding and put it into the box.<br>\n  Computer: By \"it\", I assume you mean the block which is taller than the one I am holding.<br>\n  Computer: OK.<br>\n  Person: What does the box contain?<br>\n  Computer: The blue pyramid and the blue block.<br>\n  Person: What is the pyramid supported by?<br>\n  Computer: The box.<br>\n  Person: How many blocks are not in the box?<br>\n  Computer: Four of them.<br>\n  Person: Is at least one of them narrower than the one which I told you to pick up?<br>\n  Computer: Yes, the red cube.</p>\n</blockquote>\n\n<p>I have an ontology for the domain so can reason over common problems.</p>\n",
    "score": 7,
    "creation_date": 1256195174,
    "view_count": 1481,
    "answer_count": 3,
    "tags": "command-line;nlp"
  },
  {
    "question_id": 50347481,
    "title": "I cannot understand the skipgrams() function in keras",
    "body": "<p>I am trying to understand the <code>skipgrams()</code> function in keras by using the following code</p>\n\n<pre><code>from keras.preprocessing.text import *\nfrom keras.preprocessing.sequence import skipgrams\n\ntext = \"I love money\" #My test sentence\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([text])\nword2id = tokenizer.word_index\nwids = [word2id[w] for w in text_to_word_sequence(text)]\npairs, labels = skipgrams(wids, len(word2id),window_size=1)\n\nfor i in range(len(pairs)): #Visualizing the result\n    print(\"({:s} , {:s} ) -&gt; {:d}\".format(\n          id2word[pairs[i][0]], \n          id2word[pairs[i][1]], \n          labels[i]))\n</code></pre>\n\n<p>For the sentence \"I love money\", I would expect the following <code>(context, word)</code> pairs with the window size=1 as defined in keras:</p>\n\n<pre><code>([i, money], love)\n([love], i)\n([love], money)\n</code></pre>\n\n<p>From what I understand in Keras' documentation, it will output the label of 1 if <code>(word, word in the same window)</code> , and the label of 0 if <code>(word, random word from the vocabulary)</code>.</p>\n\n<p>Since I am using the windows size of 1, I would expect the label of 1 for the following pairs:</p>\n\n<pre><code>(love, i)\n(love, money)\n(i, love)\n(money, love)\n</code></pre>\n\n<p>And the label of 0 for the following pairs</p>\n\n<pre><code>(i, money)\n(money, i)\n</code></pre>\n\n<p>Yet, the code give me the result like this</p>\n\n<pre><code>(love , i ) -&gt; 1\n(love , money ) -&gt; 1\n(i , love ) -&gt; 1\n(money , love ) -&gt; 1    \n(i , i ) -&gt; 0\n(love , love ) -&gt; 0\n(love , i ) -&gt; 0\n(money , love ) -&gt; 0\n</code></pre>\n\n<p>How can the pair <code>(love , i )</code> and <code>(money , love )</code> be labelled as both 0 and 1?\nand also where is the <code>(i, money)</code> and <code>(money, i)</code> result?</p>\n\n<p>Am I understanding things wrongly that the labels of 0 are all out of my expectation? But it seems I understand the label of 1 quite well.</p>\n",
    "score": 7,
    "creation_date": 1526378762,
    "view_count": 2427,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;keras;text-processing"
  },
  {
    "question_id": 47549856,
    "title": "Tokenizing an HTML document",
    "body": "<p>I have an HTML document and I'd like to tokenize it using spaCy while keeping HTML tags as a single token.\nHere's my code:</p>\n\n<pre><code>import spacy\nfrom spacy.symbols import ORTH\nnlp = spacy.load('en', vectors=False, parser=False, entity=False)\n\nnlp.tokenizer.add_special_case(u'&lt;i&gt;', [{ORTH: u'&lt;i&gt;'}])\nnlp.tokenizer.add_special_case(u'&lt;/i&gt;', [{ORTH: u'&lt;/i&gt;'}])\n\ndoc = nlp('Hello, &lt;i&gt;world&lt;/i&gt; !')\n\nprint([e.text for e in doc])\n</code></pre>\n\n<p>The output is:</p>\n\n<pre><code>['Hello', ',', '&lt;', 'i', '&gt;', 'world&lt;/i', '&gt;', '!']\n</code></pre>\n\n<p>If I put spaces around the tags, like this:</p>\n\n<pre><code>doc = nlp('Hello, &lt;i&gt; world &lt;/i&gt; !')\n</code></pre>\n\n<p>The output is as I want it:</p>\n\n<pre><code>['Hello', ',', '&lt;i&gt;', 'world', '&lt;/i&gt;', '!']\n</code></pre>\n\n<p>but I'd like avoiding complicated pre-processing to the HTML.</p>\n\n<p>Any idea how can I approach this?</p>\n",
    "score": 7,
    "creation_date": 1511949496,
    "view_count": 5049,
    "answer_count": 2,
    "tags": "python;html;nlp;spacy"
  },
  {
    "question_id": 45941104,
    "title": "Tokenizing texts in both Chinese and English improperly splits English words into letters",
    "body": "<p>When tokenizing texts that contain both Chinese and English, the result will split English words into letters, which is not what I want. Consider the following code:</p>\n\n<pre><code>from nltk.tokenize.stanford_segmenter import StanfordSegmenter\nsegmenter = StanfordSegmenter()\nsegmenter.default_config('zh')\nprint(segmenter.segment('哈佛大学的Melissa Dell'))\n</code></pre>\n\n<p>The output will be <code>哈佛大学 的 M e l i s s a D e l l</code>. How do I modify this behavior?</p>\n",
    "score": 7,
    "creation_date": 1504015190,
    "view_count": 4462,
    "answer_count": 2,
    "tags": "python-3.x;nlp;nltk;stanford-nlp;tokenize"
  },
  {
    "question_id": 44626264,
    "title": "Split compound sentences into simple sentences",
    "body": "<p>I am looking for a sentence segmentor that can split compound sentences into simple sentences. </p>\n\n<p>Example:</p>\n\n<pre><code>Input: Andrea is beautiful but she is strict.\n(expected) Output: Andrea is beautiful. she is strict.\n\nInput: i am andrea and i work for google. \n(expected) Output: i am andrea. i work for google.\n\nInput: Italy is my favorite country; i plan to spend two weeks there next year.\n(expected) Output: Italy is my favorite country. i plan to spend two weeks there next year.\n</code></pre>\n\n<p>Any recommendations? I tried NLTK, spacy, segtok, nlp-compromise but they don't work on these complex examples (I understand this is a difficult problem, thus no easy solutions).</p>\n",
    "score": 7,
    "creation_date": 1497862894,
    "view_count": 4043,
    "answer_count": 2,
    "tags": "nlp;chatbot"
  },
  {
    "question_id": 43618145,
    "title": "Improving on the basic, existing GloVe model",
    "body": "<p>I am using GloVe as part of my research. I've downloaded the models from <a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">here</a>. I've been using GloVe for sentence classification. The sentences I'm classifying are specific to a particular domain, say some STEM subject. However, since the existing GloVe models are trained on a general corpus, they may not yield the best results for my particular task. </p>\n\n<p>So my question is, how would I go about loading the retrained model and just retraining it a little more on my own corpus to learn the semantics of my corpus as well? There would be merit in doing this were it possible. </p>\n",
    "score": 7,
    "creation_date": 1493144128,
    "view_count": 2343,
    "answer_count": 3,
    "tags": "nlp;text-classification;glove"
  },
  {
    "question_id": 43576136,
    "title": "How can I use Python NLTK to identify collocations among single characters?",
    "body": "<p>I want to use NLTK to identify collocations among particular kanji characters in Japanese and hanzi characters in Chinese. As with word collocations, some sequences of Chinese characters are far more likely than others. Example: Many words in Chinese and Japanese are two-character bigrams — character A and character B (e.g. 日本 = Japan, <em>ni-hon</em> in Japanese and <em>ri-ben</em> in Chinese). Given character A (日), it is much more likely that 本 will appear as character B. So the characters 日 and 本 are collocates.</p>\n\n<p>I want to use NLTK to find out answers to these questions: </p>\n\n<p>(1) Given character A, what characters are most likely to be character B? </p>\n\n<p>(2) Given character B, what characters are most likely to be character A? </p>\n\n<p>(3) How likely are character A and character B to appear together in a sentence, even if they do not appear side-by-side?</p>\n\n<p>Relatedly: if I a have a frequency list of kanji/hanzi, can I force the NLTK collocations module to examine only relations among the kanji/hanzi in my list, ignoring all other characters? This would filter out results in which single Roman letters (a, b, c, etc.) or punctuation marks are considered in the set of possible collocates.</p>\n\n<p>Unfortunately, the <a href=\"http://www.nltk.org/api/nltk.html?highlight=collocations#module-nltk.collocations\" rel=\"noreferrer\">documentation</a>, <a href=\"http://www.nltk.org/howto/collocations.html\" rel=\"noreferrer\">how-to</a>, and <a href=\"http://www.nltk.org/_modules/nltk/collocations.html\" rel=\"noreferrer\">source code</a> for nltk.collocations and the <a href=\"http://www.nltk.org/book/ch01.html#collocation_index_term\" rel=\"noreferrer\">NLTK Book</a> only discuss English NLP, and understandably do not address the question of single-character collocations. Functions in the nltk.collocations module seem to have a word tokenizer built in, so I think they ignore single characters by default.</p>\n\n<p>UPDATE: The following code seems to be on the right track:</p>\n\n<pre><code>def main():\n    scorer = nltk.collocations.BigramAssocMeasures.likelihood_ratio\n    with open('sample_jp_text.txt', mode='r') as infile:\n        sample_text = infile.read()\n    bigram_measures = nltk.collocations.BigramAssocMeasures()\n    finder = BigramCollocationFinder.from_words(sample_text,window_size = 13)\n    #corpus = make_corpus()\n    print('\\t', [' '.join(tup) for tup in finder.nbest(scorer, 15)])\n</code></pre>\n\n<p>Results:</p>\n\n<pre><code> ['リ ザ', 'フ ザ', 'フ リ', '0 0', '悟 空', 'リ ー', 'ー ザ', '億 0', '2 0', 'サ ヤ', '0 万', 'サ 人', '0 円', '復 活', '。 \\n']\n</code></pre>\n\n<p>So, for whatever reason, the <code>BigramCollocationFinder</code> seems to be treating the individual characters in my Japanese sample text as candidates for bigram collocations. I'm still not sure how to take the next step from this result to answering the questions posed above.</p>\n",
    "score": 7,
    "creation_date": 1492979280,
    "view_count": 1414,
    "answer_count": 1,
    "tags": "python;string;nlp;nltk;linguistics"
  },
  {
    "question_id": 35029952,
    "title": "How to predict correct country name for user provided country name?",
    "body": "<p>I am planning to do some data tuning on my data.</p>\n\n<p><strong>Situation</strong>-I have a data which has a field <code>country</code>. It contains user input country names( It might contain spelling mistakes or different country names for same country like US/U.S.A/United States for USA). I have a list of correct country names.</p>\n\n<p><strong>What I want</strong>- To predict which closest country it is referring to. For example- If <code>U.S.</code> is given then it will change to <code>USA</code>(correct country name in our list).</p>\n\n<p>Is there any way I can do it using Java or opennlp or any other method?</p>\n",
    "score": 7,
    "creation_date": 1453874801,
    "view_count": 890,
    "answer_count": 3,
    "tags": "java;nlp;string-matching;text-mining;opennlp"
  },
  {
    "question_id": 34554548,
    "title": "How to modify text that matches a particular regular expression in Python?",
    "body": "<p>I need to mark negative contexts in a sentence. The algorithm goes as follows:</p>\n\n<ol>\n<li>Detect a negator (not/never/ain't/don't/ etc)</li>\n<li>Detect a clause ending punctuation (.;:!?)</li>\n<li>Add _NEG to all the words in between this.</li>\n</ol>\n\n<p>Now, I have defined a regex to pick out all such occurences:</p>\n\n<pre><code>def replacenegation(text):\n    match=re.search(r\"((\\b(never|no|nothing|nowhere|noone|none|not|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint)\\b)|\\b\\w+n't\\b)((?![.:;!?]).)*[.:;!?\\b]\", text)\n    if match:\n        s=match.group()\n        print s\n        news=\"\"\n        wlist=re.split(r\"[.:;!? ]\" , s)\n        wlist=wlist[1:]\n        print wlist\n        for w in wlist:\n            if w:\n                news=news+\" \"+w+\"_NEG\"\n        print news\n</code></pre>\n\n<p>I can detect and replace the matched group. However, I don't know how to recreate the complete sentence after this operation. Also for multiple matches, match.groups() gives me wrong output.</p>\n\n<p>For example, if my input sentence is:</p>\n\n<pre><code>I don't like you at all; I should not let you know my happiest secret.\n</code></pre>\n\n<p>Output should be:</p>\n\n<pre><code>I don't like_NEG you_NEG at_NEG all_NEG ; I should not let_NEG you_NEG know_NEG my_NEG happiest_NEG secret_NEG .\n</code></pre>\n\n<p>How do I do this?</p>\n",
    "score": 7,
    "creation_date": 1451635648,
    "view_count": 1308,
    "answer_count": 1,
    "tags": "python;regex;python-2.7;nlp"
  },
  {
    "question_id": 20920416,
    "title": "In Latent Semantic Analysis, how do you recombine the decomposed matrices after truncating the singular values?",
    "body": "<p>I'm reading <strong><em><a href=\"http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf\" rel=\"nofollow\">Matrix decompositions and latent semantic indexing</a></em></strong> (Online edition ©\n2009 Cambridge UP)</p>\n\n<p>I'm trying to understand how you reduce the number of dimensions in a matrix. There's an example on page 13 which I'm trying to replicate using <a href=\"http://www.numpy.org/\" rel=\"nofollow\">Python's numpy</a>.</p>\n\n<p>Let's call the original occurrence matrix \"a\" and the three <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\" rel=\"nofollow\">SVD</a> (Singular Value Decomposition) decomposed matrices \"U\", \"S\" and \"V\".</p>\n\n<p>The trouble I'm having is that after I zero out the smaller singular values in \"S\", when I multiply together \"U\", \"S\" and \"V\" using numpy, the answer is not as it is given in the pdf. The bottom 3 rows are not all zeros. The funny thing is that when I just multiply \"S\" and \"V\" I get the right answer.</p>\n\n<p>This is sort of surprising but multiplying \"S\" and \"V\" is actually what Manning and Schutze's book Foundations of Statistical Natural Language Processing says you have to do. But this is not what the pdf says you have to do in page 10.</p>\n\n<p>So what's going on here?</p>\n",
    "score": 7,
    "creation_date": 1388694812,
    "view_count": 970,
    "answer_count": 1,
    "tags": "nlp;linear-algebra;svd;latent-semantic-analysis"
  },
  {
    "question_id": 18904498,
    "title": "Best way to classify labeled sentences from a set of documents",
    "body": "<p>I have a classification problem and I need to figure out the best approach to solve it.  I have a set of training documents, where some the sentences and/or paragraphs within the documents are labeled with some tags.  Not all sentences/paragraphs are labeled. A sentence or paragraph may have more than one tag/label.  What I want to do is make some model, where given a new documents, it will give me suggested labels for each of the sentences/paragraphs within the document.  Ideally, it would only give me high-probability suggestions.</p>\n\n<p>If I use something like nltk NaiveBayesClassifier, it gives poor results, I think because it does not take into account the \"unlabeled\" sentences from the training documents, which will contain many similar words and phrases as the labeled sentences.  The documents are legal/financial in nature and are filled with legal/financial jargon most of which should be discounted in the classification model.</p>\n\n<p>Is there some better classification algorithm that Naive Bayes, or is there some way to push the unlabelled data into naive bayes, in addition to the labelled data from the training set?</p>\n",
    "score": 7,
    "creation_date": 1379623153,
    "view_count": 3686,
    "answer_count": 2,
    "tags": "machine-learning;nlp;classification;nltk"
  },
  {
    "question_id": 14760902,
    "title": "Are there any off-the-shelf solutions for lexical analysis in Haskell that allow for a run-time dynamic lexicon?",
    "body": "<p>I'm working on a small Haskell project that needs to be able to lex a very small subset of strictly formed English in to tokens for semantic parsing.  It's a very naïve natural language interface to a system with many different end effectors than can be issued commands.  I'm currently using Alex for this, but Alex relies on its lexicon to be statically compiled.  The nature of the system is such that the number and even type of end effectors in the world can increase as well as decrease after compilation, and so I need to be able to add or remove viable tokens from the lexicon at runtime.</p>\n\n<p>I've tried looking around for dynamic lexing solutions, and the closest I could get was <a href=\"http://www.nondot.org/sabre/Projects/HaskellLexer/\">this</a> Dynamic Lexer Engine that doesn't look to have been updated since 2000.</p>\n\n<p>I've been considering some techniques like using a less-high level approach (Attoparsec, perhaps), or even wiring up a recompilation hook for Alex  and separating the lexer from the rest of the application.</p>\n\n<p>Are there any well-known solutions for this sort of lexical analysis?  I intend on working through <a href=\"http://nlpwp.org/book/index.xhtml\">Natural Language Processing for the Working Programmer</a> eventually so I can take a less simplified approach, but currently a basically lexer is what I need.</p>\n",
    "score": 7,
    "creation_date": 1360270905,
    "view_count": 296,
    "answer_count": 1,
    "tags": "haskell;nlp;lexical-analysis;alex"
  },
  {
    "question_id": 12869310,
    "title": "sentiment analysis - wordNet , sentiWordNet lexicon",
    "body": "<p>I need a list of positive and negative words with the <strong>weights</strong> assigned to words according to how strong and week they are. I have got :</p>\n\n<p>1.) WordNet - It gives a + or - score for every word.  </p>\n\n<p>2.) SentiWordNet - Giving positive and negative values in the range [0,1].</p>\n\n<p>I checked these on few words,</p>\n\n<p>love - wordNet is giving 0.0 for both noun and verb, I dont know why i think it should be positive by at least some factor.  </p>\n\n<p>repress - wordNet gives -9.93<br>\n        - SentiWordNet gives - 0.0 for both pos and neg. (should be negative)  </p>\n\n<p>repose  - wordNet - 2.488<br>\n        - SentiWordNet - { pos - 0.125, neg - 0.5 } (should be positive)  </p>\n\n<p>I need some help to decide which one to use. </p>\n\n<p>Thanks. </p>\n",
    "score": 7,
    "creation_date": 1350093733,
    "view_count": 13975,
    "answer_count": 3,
    "tags": "nlp;text-mining;wordnet;sentiment-analysis"
  },
  {
    "question_id": 10740767,
    "title": "Techniques for extracting regular expressions out of a labeled data set",
    "body": "<p>Let's suppose I have a data set of several hundred thousand strings (which happen to be natural language sentences, if it matters) which are each tagged with a certain \"label\". Each sentence is tagged with exactly one label, and there are about 10 labels, each with approximately 10% of the data set belonging to them. There is a high degree of similarity to the structure of sentences within a label.</p>\n\n<p>I know the above sounds like a classical example of a machine learning problem, but I want to ask a slightly different question. <strong>Are there any known techniques for programatically generating a set of regular expressions for each label, which can successfully classify the training data while still generalizing to future test data?</strong></p>\n\n<p>I would be very happy with references to the literature; I realize that this will not be a straightforward algorithm :)</p>\n\n<p><strong>PS:</strong> I know that the normal way to do classification is with machine learning techniques like an SVM or such. I am, however, explicitly looking for a way to generate <em>regular expressions</em>. (I would be happy with with machine learning techniques for generating the regular expressions, just not with machine learning techniques for doing the classification itself!)</p>\n",
    "score": 7,
    "creation_date": 1337874016,
    "view_count": 1192,
    "answer_count": 3,
    "tags": "regex;algorithm;nlp;machine-learning"
  },
  {
    "question_id": 6666537,
    "title": "Does anybody know an Implementation of yarowsky&#39;s algorithm?",
    "body": "<p>I want to find collocation in huge text using yarowsky's algorithm.\nI have read about this algorithm in these links:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Yarowsky_algorithm\" rel=\"noreferrer\">wikipedia and Yarowsky</a></p>\n\n<p><a href=\"http://books.google.com/books?id=cwNxbeSlIXoC&amp;pg=PA361&amp;dq=yarowsky%20%2b%20collocation&amp;hl=en&amp;ei=kK8VTt7lEort0gGYlNVf&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=10&amp;ved=0CFkQ6AEwCQ#v=onepage&amp;q=yarowsky%20%2b%20collocation&amp;f=false\" rel=\"noreferrer\">google book and yarowsky</a>\nI wanted to know if there is an implementation of the yarowsky's algorithm`? \nplease help me find some code for this algorithm.\nthanks</p>\n",
    "score": 7,
    "creation_date": 1310483682,
    "view_count": 1581,
    "answer_count": 1,
    "tags": "python;nlp;nltk;word-sense-disambiguation"
  },
  {
    "question_id": 6213968,
    "title": "nlg building a sentence",
    "body": "<p>I would like to generate a sentence having as input words.\nE.g.</p>\n\n<p>Input:</p>\n\n<pre><code>Mary\nchase\nthe monkey\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>Mary chases the monkey.\n</code></pre>\n\n<p>This could be done using a simpleNLG library: <a href=\"http://code.google.com/p/simplenlg/\" rel=\"noreferrer\">http://code.google.com/p/simplenlg/</a> in the following way:</p>\n\n<pre><code>String subject = \"Mary\";\nString verb = \"chase\";\nString object = \"the monkey\";\n\np.setSubject(subject);\np.setVerb(verb);\np.setObject(object);\n\nString output = realiser.realiseSentence(p);\nSystem.out.println(output);\n</code></pre>\n\n<p>This will generate the sentence Mary chases the monkey. But I would like to make it automated where I input words and the sentence gets generated. This would require some preprocessing that would specify which word is a subject which word is a verb and which is an object. I know there are POS (parts of speech) tagging libraries but they don't specify whether it is a subject or object. Any suggestions how this could be done? Also for make it work for bigger sentences with multiple objects, adverbs etc.</p>\n",
    "score": 7,
    "creation_date": 1307014420,
    "view_count": 4309,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 5564066,
    "title": "how to use the Gale-Church algorithm in Python-NLTK?",
    "body": "<p>The gale-church algorithm is available in the python-NLTK but can anyone show me an example of how to call the function within a python script? i'm clueless about how to do that.</p>\n\n<p><a href=\"http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/align/align.py?r=8552&amp;spec=svn8552\" rel=\"noreferrer\">http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/align/align.py?r=8552&amp;spec=svn8552</a></p>\n",
    "score": 7,
    "creation_date": 1302081508,
    "view_count": 2123,
    "answer_count": 1,
    "tags": "python;nlp;alignment;nltk;text-alignment"
  },
  {
    "question_id": 5529652,
    "title": "Analyze text (lemmatization, edit distance)",
    "body": "<p>I need to analyze the text to exist in it banned words. Suppose the black list is the word: \"Forbid\". The word has many forms. In the text the word can be, for example: \"forbidding\", \"forbidden\", \"forbad\". To bring the word to the initial form, I use a process lemmatization. Your suggestions?</p>\n\n<p><strong>What about typos?</strong><br>\nFor example: \"F0rb1d\".  I  think use damerau–Levenshtein or another. You suggestions?</p>\n\n<p><strong>And what if the text is written as follows</strong>:<br>\n\"ForbiddenInformation.Privatecorrespondenceofthecompany.\"  OR\n\"F0rb1dden1nformation.Privatecorresp0ndenceofthec0mpany.\" (yes, without whitespace)</p>\n\n<p>How to solve this problem?<br>\nPreferably fast algorithm, because text are processed in real time.<br>\nAnd maybe what some tips to improve performance (how to store, etc)?</p>\n",
    "score": 7,
    "creation_date": 1301834711,
    "view_count": 2611,
    "answer_count": 2,
    "tags": "c#;nlp;similarity;lemmatization"
  },
  {
    "question_id": 1996008,
    "title": "How does twitter&#39;s trending topics algorithm decide which words to extract from tweets?",
    "body": "<p>I saw <a href=\"https://stackoverflow.com/questions/787496/what-is-the-best-way-to-compute-trending-topics-or-tags\">this question</a>, which focuses on the \"Brittney Spears\" problem.  But I have a bit of a different question.  How does the algorithm determine which words or phrases need to be ranked?  For instance, if I send out a tweet that says \"Michael Jackson died\", how does it know to pull out \"Michael Jackson\" but not \"died\"?</p>\n\n<p>Or suppose that Alec Baldwin and Steven Baldwin were in the news that day and thus were both mentioned in a lot of tweets.  How would it know to treat both names differently instead of just pulling out \"Baldwin\"?</p>\n\n<p>Done naively, I could see this problem as being NP-complete (you'd have to compare all potential phrases in the tweet with all potential phrases in everyone else's tweets).</p>\n",
    "score": 7,
    "creation_date": 1262547164,
    "view_count": 2610,
    "answer_count": 2,
    "tags": "algorithm;twitter;nlp;ranking"
  },
  {
    "question_id": 68691450,
    "title": "How can I check a confusion_matrix after fine-tuning with custom datasets?",
    "body": "<p>This question is the same with <a href=\"https://datascience.stackexchange.com/questions/99815/how-can-i-check-a-confusion-matrix-after-fine-tuning-with-custom-datasets\">How can I check a confusion_matrix after fine-tuning with custom datasets?</a>, on Data Science Stack Exchange.</p>\n<h2>Background</h2>\n<p>I would like to check a confusion_matrix, including precision, recall, and f1-score like below after fine-tuning with custom datasets.</p>\n<p>Fine tuning process and the task are <a href=\"https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews\" rel=\"noreferrer\">Sequence Classification with IMDb Reviews</a> on the <a href=\"https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer\" rel=\"noreferrer\">Fine-tuning with custom datasets tutorial on Hugging face</a>.</p>\n<p>After finishing the fine-tune with Trainer, how can I check a confusion_matrix in this case?</p>\n<p>An image of confusion_matrix, including precision, recall, and f1-score <a href=\"http://www.renom.jp/notebooks/product/renom_dl/trainer/notebook.html\" rel=\"noreferrer\">original site</a>: just for example output image</p>\n<pre><code>predictions = np.argmax(trainer.test(test_x), axis=1)\n\n# Confusion matrix and classification report.\nprint(classification_report(test_y, predictions))\n\n            precision    recall  f1-score   support\n\n          0       0.75      0.79      0.77      1000\n          1       0.81      0.87      0.84      1000\n          2       0.63      0.61      0.62      1000\n          3       0.55      0.47      0.50      1000\n          4       0.66      0.66      0.66      1000\n          5       0.62      0.64      0.63      1000\n          6       0.74      0.83      0.78      1000\n          7       0.80      0.74      0.77      1000\n          8       0.85      0.81      0.83      1000\n          9       0.79      0.80      0.80      1000\n\navg / total       0.72      0.72      0.72     10000\n</code></pre>\n<h2>Code</h2>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10,\n)\n\nmodel = DistilBertForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset             # evaluation dataset\n)\n\ntrainer.train()\n</code></pre>\n<h2>What I did so far</h2>\n<p>Data set Preparation for <a href=\"https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews\" rel=\"noreferrer\">Sequence Classification with IMDb Reviews</a>, and I'm fine-tuning with Trainer.</p>\n<pre><code>from pathlib import Path\n\ndef read_imdb_split(split_dir):\n    split_dir = Path(split_dir)\n    texts = []\n    labels = []\n    for label_dir in [&quot;pos&quot;, &quot;neg&quot;]:\n        for text_file in (split_dir/label_dir).iterdir():\n            texts.append(text_file.read_text())\n            labels.append(0 if label_dir is &quot;neg&quot; else 1)\n\n    return texts, labels\n\ntrain_texts, train_labels = read_imdb_split('aclImdb/train')\ntest_texts, test_labels = read_imdb_split('aclImdb/test')\n\nfrom sklearn.model_selection import train_test_split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n\nfrom transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)\n\nimport torch\n\nclass IMDbDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = IMDbDataset(train_encodings, train_labels)\nval_dataset = IMDbDataset(val_encodings, val_labels)\ntest_dataset = IMDbDataset(test_encodings, test_labels)\n</code></pre>\n",
    "score": 7,
    "creation_date": 1628331593,
    "view_count": 6546,
    "answer_count": 1,
    "tags": "python-3.x;machine-learning;pytorch;nlp;huggingface-transformers"
  },
  {
    "question_id": 52123026,
    "title": "Sklearn Pipeline ValueError: could not convert string to float",
    "body": "<p>I'm playing around with sklearn and NLP for the first time, and thought I understood everything I was doing up until I didn't know how to fix this error. Here is the relevant code (largely adapted from <a href=\"http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\" rel=\"noreferrer\">http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html</a>):</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom sgboost import XGBClassifier\nfrom pandas import DataFrame\n\ndef read_files(path):\n    for article in os.listdir(path):\n        with open(os.path.join(path, doc)) as f:\n            text = f.read()\n        yield os.path.join(path, article), text\n\ndef build_data_frame(path, classification)\n    rows = []\n    index = []\n    for filename, text in read_files(path):\n        rows.append({'text': text, 'class': classification})\n        index.append(filename)\n    df = DataFrame(rows, index=index)\n    return df\n\ndata = DataFrame({'text': [], 'class': []})\nfor path, classification in SOURCES: # SOURCES is a list of tuples\n    data = data.append(build_data_frame(path, classification))\ndata = data.reindex(np.random.permutation(data.index))\n\nclassifier = Pipeline([\n    ('features', FeatureUnion([\n        ('text', Pipeline([\n            ('tfidf', TfidfVectorizer()),\n            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)\n            ])),\n        ('words', Pipeline([('wscaler', StandardScaler())])),\n    ])),\n    ('clf, XGBClassifier(silent=False)),\n])\nclassifier.fit(data['text'].values, data['class'].values)\n</code></pre>\n\n<p>The data loaded into the DataFrame is preprocessed text with all stopwords, punctuation, unicode, capitals, etc. taken care of. This is the error I'm getting once I call fit on the classifier where the ... represents one of the documents that should have been vecorized in the pipeline:</p>\n\n<pre><code>ValueError: could not convert string to float: ...\n</code></pre>\n\n<p>I first thought the TfidfVectorizer() is not working, causing an error on the SVD algorithm, but after I extracted each step out of the pipeline and implemented them sequentially, the same error only came up on XGBClassifer.fit().</p>\n\n<p>Even more confusing to me, I tried to piece this script apart step-by-step in the interpreter, but when I tried to import either read_files or build_data_frame, the same ValueError came up with one of my strings, but this was merely after:</p>\n\n<pre><code>from classifier import read_files\n</code></pre>\n\n<p>I have no idea how that could be happening, if anyone has any idea what my glaring errors may be, I'd really appreciate it. Trying to wrap my head around these concepts on my own but coming across a problem likes this leaves me feeling pretty incapacitated.</p>\n",
    "score": 7,
    "creation_date": 1535752772,
    "view_count": 4979,
    "answer_count": 2,
    "tags": "python;scikit-learn;nlp;text-classification"
  },
  {
    "question_id": 50781707,
    "title": "How to handle unbalanced label data using FastText?",
    "body": "<p>In FastText, I have unbalanced labels. What is the best way to handle it?</p>\n",
    "score": 7,
    "creation_date": 1528617725,
    "view_count": 2642,
    "answer_count": 2,
    "tags": "nlp;word2vec;fasttext"
  },
  {
    "question_id": 49638869,
    "title": "Online clustering of news articles",
    "body": "<p>Is there a common online algorithm to classify news dynamically? I have a huge data set of news classified by topics. I consider each of that topics a <em>cluster</em>. Now I need to classify breaking news. Probably, I will need to generate new topics, or new <em>clusters</em>, dynamically.</p>\n\n<p>The algorithm I'm using is the following:</p>\n\n<p>1) I go through a group of feeds from news sites and I recognize news links.</p>\n\n<p>2) For each new link, I extract the content using dragnet, and then I tokenize it.</p>\n\n<p>3) I find the vector representation of all the old news and the last one using TfidfVectorizer from sklearn.</p>\n\n<p>4) I find the nearest neighbor in my dataset computing euclidean distance from the last news vector representation and all the vector representations of the old news.</p>\n\n<p>5) If that distance is smaller than a threshold, I put it in the cluster that the neighbor belongs. Otherwise, I create a new <em>cluster</em>, with the breaking news.</p>\n\n<p>Each time a news arrive, I re-fit all the data using a TfidfVectorizer, because new dimensions can be founded. I can't wait to re-fit once per day, because <strong>I need to detect breaking events, which can be related to unknown topics</strong>. Is there a common approach more efficient than the one I am using?</p>\n",
    "score": 7,
    "creation_date": 1522788222,
    "view_count": 443,
    "answer_count": 1,
    "tags": "machine-learning;nlp;cluster-analysis;information-retrieval;unsupervised-learning"
  },
  {
    "question_id": 42417481,
    "title": "How good is GATE for NLP?",
    "body": "<p>I am trying to build a NLP app which essentially has to do Named Entity Recognition (NER). I came across <a href=\"https://gate.ac.uk/\" rel=\"noreferrer\">GATE</a>. From what i understand it is a framework to build NLP apps. I tested ANNIE, the IE system distributed with GATE but the NER results for my domain is not up-to the expectation. As a matter of fact any NER, like Stanford CoreNLP or NLTK, is not giving me required results. So i decide to tweak the existing systems to get desired result.</p>\n\n<p>Regarding GATE i liked few things:<br>\n1. The modularity of components: For example in ANNIE, components like Tokenizer, Gaztteer, Sentence splitter, POS tagger etc can be used independently of each other.<br>\n2. Its rule language called JAPE which has a very nice way of writing rules or patterns.</p>\n\n<p>But few things i want to know about GATE are:<br>\n1. What are the other major advantages of GATE particularly for NER?<br>\n2. How flexible is GATE for adding new components? For example some day if i want to use NLTK's POS tagger inside GATE?<br>\n3. If i want to use custom machine learning models with GATE?<br>\n4. I am aware that NLP group at University of Sheffield is involved in GATE, but i want to know how active is GATE's community and how active is the support for GATE?<br>\n5. Can GATE be used for commercial software?</p>\n\n<p>Keen to here suggestions from people who have actually used GATE </p>\n",
    "score": 7,
    "creation_date": 1487857395,
    "view_count": 2597,
    "answer_count": 1,
    "tags": "java;nlp;named-entity-recognition;gate"
  },
  {
    "question_id": 23571785,
    "title": "NLTK - Multi-labeled Classification",
    "body": "<p>I am using NLTK, to classify documents - having 1 label each, with there being 10 type of documents.</p>\n\n<p>For text extraction, I am cleaning text (punctuation removal, html tag removal, lowercasing), removing nltk.corpus.stopwords, as well as my own collection of stopwords.</p>\n\n<p>For my document feature I am looking across all 50k documents, and gathering the top 2k words, by frequency (frequency_words) then for each document identifying which words in the document that are also in the global frequency_words. </p>\n\n<p>I am then passing in each document as hashmap of <code>{word: boolean}</code> into the nltk.NaiveBayesClassifier(...) I have a 20:80 test-training ratio in regards to the total number of documents.</p>\n\n<p>The issues I am having:</p>\n\n<ol>\n<li>Is this classifier by NLTK, suitable to multi labelled data? - all examples I have seen are more about 2-class classification, such as whether something is declared as a <a href=\"http://stevenloria.com/how-to-build-a-text-classification-system-with-python-and-textblob/\">positive or negative.</a> </li>\n<li>The documents are such that they should have a set of key skills in - unfortunately I haven't got a corpus where these skills lie. So I have taken an approach with the understanding, a word count per document would not be a good document extractor - is this correct? Each document has been written by individuals, so I need to leave way for individual variation in the document. I am aware SkLearn <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">MBNaiveBayes</a> which deals with word count.</li>\n<li>Is there an alternative library I should be using, or variation of this algorithm?</li>\n</ol>\n\n<p>Thanks!</p>\n",
    "score": 7,
    "creation_date": 1399660797,
    "view_count": 2525,
    "answer_count": 1,
    "tags": "python;nlp;nltk;document-classification"
  },
  {
    "question_id": 21548504,
    "title": "Parse Location, Person name, Date from string by NLTK",
    "body": "<p>I have lots of strings like following,</p>\n\n<ol>\n<li><code>ISLAMABAD: Chief Justice Iftikhar Muhammad Chaudhry said that National Accountab</code></li>\n<li><code>KARACHI, July 24 -- Police claimed to have arrested several suspects in separate</code></li>\n<li><code>ALUM KULAM, Sri Lanka -- As gray-bellied clouds started to blot out the scorchin</code></li>\n</ol>\n\n<p>I am using NLTK to remove the dateline part and recognize the date, location and person name?</p>\n\n<p>Using pos tagging I can find the parts of speech. But I need to determine <em>location</em>, <em>date</em>, <em>person name</em>. How can I do that?</p>\n\n<p><strong>Update:</strong>  </p>\n\n<p>Note: I dont want to perform another http request. I need to parse it using my own code. If there is a library its okay to use it.</p>\n\n<p><strong>Update:</strong></p>\n\n<p>I use <code>ne_chunk</code>. But no luck.</p>\n\n<pre><code>import nltk\n\ndef pchunk(t):\n    w_tokens = nltk.word_tokenize(t)\n    pt = nltk.pos_tag(w_tokens)\n    ne = nltk.ne_chunk(pt)\n    print ne\n\n# txts is a list of those 3 sentences.\nfor t in txts:                                            \n    print t\n    pchunk(t)\n</code></pre>\n\n<p>Output is following,</p>\n\n<pre><code>ISLAMABAD: Chief Justice Iftikhar Muhammad Chaudhry said that National Accountab\n\n(S\n  ISLAMABAD/NNP\n  :/:\n  Chief/NNP\n  Justice/NNP\n  (PERSON Iftikhar/NNP Muhammad/NNP Chaudhry/NNP)\n  said/VBD\n  that/IN\n  (ORGANIZATION National/NNP Accountab/NNP))\n\nKARACHI, July 24 -- Police claimed to have arrested several suspects in separate\n\n(S\n  (GPE KARACHI/NNP)\n  ,/,\n  July/NNP\n  24/CD\n  --/:\n  Police/NNP\n  claimed/VBD\n  to/TO\n  have/VB\n  arrested/VBN\n  several/JJ\n  suspects/NNS\n  in/IN\n  separate/JJ)\n\nALUM KULAM, Sri Lanka -- As gray-bellied clouds started to blot out the scorchin\n\n(S\n  (GPE ALUM/NN)\n  (ORGANIZATION KULAM/NN)\n  ,/,\n  (PERSON Sri/NNP Lanka/NNP)\n  --/:\n  As/IN\n  gray-bellied/JJ\n  clouds/NNS\n  started/VBN\n  to/TO\n  blot/VB\n  out/RP\n  the/DT\n  scorchin/NN)\n</code></pre>\n\n<p>Check carefully. Even <em>KARACHI</em> is recognized very well, but <em>Sri Lanka</em> is recognized as Person and <em>ISLAMABAD</em> is recognized as NNP not GPE.</p>\n",
    "score": 7,
    "creation_date": 1391506190,
    "view_count": 4520,
    "answer_count": 2,
    "tags": "python;nlp;nltk;corpus"
  },
  {
    "question_id": 19057837,
    "title": "How to generate multiple parse trees for an ambiguous sentence in NLTK?",
    "body": "<p>I have the following code in Python.</p>\n\n<pre><code>sent = [(\"very\",\"ADJ\"),(\"colourful\",\"ADJ\"),(\"ice\",\"NN\"),(\"cream\",\"NN\"),(\"van\",\"NN\")] \npatterns= r\"\"\"\n  NP:{&lt;ADJ&gt;*&lt;NN&gt;+}  \n\n\"\"\"\nNPChunker=nltk.RegexpParser(patterns) # create chunk parser\nfor s in NPChunker.nbest_parse(sent):\n    print s.draw()\n</code></pre>\n\n<p>The output is:</p>\n\n<pre><code>(S (NP very/ADJ colourful/ADJ ice/NN cream/NN van/NN))\n</code></pre>\n\n<p>But the output should have another 2 parse trees.</p>\n\n<pre><code>(S (NP very/ADJ colourful/ADJ ice/NN) (NP cream/NN) (NP van/NN))\n(S (NP very/ADJ colourful/ADJ ice/NN cream/NN) van/NN)\n</code></pre>\n\n<p>The problem is that only the first regular expression is taken by the RegexpParser. How can I generate all possible parse trees at once?</p>\n",
    "score": 7,
    "creation_date": 1380307174,
    "view_count": 1519,
    "answer_count": 1,
    "tags": "python;regex;nlp;nltk"
  },
  {
    "question_id": 12578408,
    "title": "How to ignore certain characters while doing diff in google-diff-match-patch?",
    "body": "<p>I'm using <a href=\"https://code.google.com/p/google-diff-match-patch/\" rel=\"noreferrer\">google-diff-match-patch</a> to compare plain text in natural languages.</p>\n\n<p>How can I make google-diff-match-patch to ignore certain characters?\n(Some tiny differences which I don't care.)</p>\n\n<p>For example, given text1:</p>\n\n<pre><code>give me a cup of bean-milk. Thanks.\n</code></pre>\n\n<p>and text2:</p>\n\n<pre><code>please give mom a cup of bean milk!  Thank you.\n</code></pre>\n\n<p>(Note that there are two space characters before 'Thank you'.)</p>\n\n<p>google-diff-match-patch outputs something like this:</p>\n\n<pre><code>[please] give m(e)[om] a cup of bean(-)[ ]milk(.)[!] Thank(s)[ you].\n</code></pre>\n\n<p>It seems that google-diff-match-patch only ignores different numbers of white spaces.</p>\n\n<p>How can I tell google-diff-match-patch to also ignore characters like <code>[-.!]</code>?</p>\n\n<p>The expect result would be</p>\n\n<pre><code>[please] give m(e)[om] a cup of bean-milk. Thank(s)[ you].\n</code></pre>\n\n<p>Thanks.</p>\n",
    "score": 7,
    "creation_date": 1348559174,
    "view_count": 2262,
    "answer_count": 1,
    "tags": "nlp;diff;text-processing"
  },
  {
    "question_id": 8107896,
    "title": "How to extract keywords from a block of text in Haskell",
    "body": "<p>So I know this is a kind of a large topic, but I need to accept a chunk of text, and extract the most interesting keywords from it. The text comes from TV captions, so the subject can range from news to sports to pop culture references. It is possible to provide the type of show the text came from. </p>\n\n<p>I have an idea to match the text against a dictionary of terms I know to be interesting somehow. </p>\n\n<p>Which libraries for Haskell can help me with this? </p>\n\n<p>Assuming I do have a dictionary of interesting terms, and a database to store them in, is there a particular approach you'd recommend to matching keywords within the text? </p>\n\n<p>Is there an obvious approach I'm not thinking of?</p>\n",
    "score": 7,
    "creation_date": 1321135363,
    "view_count": 527,
    "answer_count": 2,
    "tags": "haskell;nlp"
  },
  {
    "question_id": 6967792,
    "title": "Reconstructing now-famous 17-year-old&#39;s Markov-chain-based information-retrieval algorithm &quot;Apodora&quot;",
    "body": "<p>While we were all twiddling our thumbs, a 17-year-old Canadian boy has apparently found an information retrieval algorithm that: </p>\n\n<p>a) performs with twice the precision of the current, and widely-used vector space model</p>\n\n<p>b) is 'fairly accurate' at identifying similar words. </p>\n\n<p>c) makes microsearch more accurate</p>\n\n<p>Here is a good <a href=\"http://www.theglobeandmail.com/news/technology/science-fair-gold-medalist-17-invents-better-way-to-search-internet/article2118962/\" rel=\"noreferrer\">interview</a>.</p>\n\n<p>Unfortunately, there's no published paper I can find yet, but, from the snatches I remember from the graphical models and machine learning classes I took a few years ago, I think we should be able to reconstruct it from his submision abstract, and what he says about it in interviews.</p>\n\n<p>From interview:</p>\n\n<blockquote>\n  <p>Some searches find words that appear in similar contexts. That’s\n  pretty good, but that’s following the relationships to the first\n  degree. My algorithm tries to follow connections further. Connections\n  that are close are deemed more valuable. In theory, it follows\n  connections to an infinite degree.</p>\n</blockquote>\n\n<p>And the abstract puts it in context:</p>\n\n<blockquote>\n  <p>A novel information retrieval algorithm called \"Apodora\" is introduced,\n  using limiting powers of Markov chain-like matrices to determine\n  models for the documents and making contextual statistical inferences\n  about the semantics of words. The system is implemented and compared\n  to the vector space model. Especially when the query is short, the\n  novel algorithm gives results with approximately twice the precision\n  and has interesting applications to microsearch.</p>\n</blockquote>\n\n<p>I feel like someone who knows about markov-chain-like matrices or information retrieval would immediately be able to realize what he's doing. </p>\n\n<p>So: what is he doing?</p>\n",
    "score": 7,
    "creation_date": 1312644282,
    "view_count": 712,
    "answer_count": 1,
    "tags": "nlp;machine-learning;information-retrieval;markov-chains"
  },
  {
    "question_id": 5319358,
    "title": "natural language processing fix for combined words",
    "body": "<p>I have some text that was generate by another system.  It combined some words together in what I assume was some sort of wordwrap by-product.  So something simple like 'the dog' is combine into 'thedog'. </p>\n\n<p>I checked the ascii and unicode string to see is there wasn't some unseen character in there, but there wasn't.  A confounding problem is that this is medical text and a corpus to check against aren't that available.  So, real example is '...test to rule out SARS versus pneumonia' ends up as '... versuspneumonia.'</p>\n\n<p>Anyone have a suggestion for finding and separating these?</p>\n",
    "score": 7,
    "creation_date": 1300232487,
    "view_count": 1131,
    "answer_count": 3,
    "tags": "regex;nlp"
  },
  {
    "question_id": 2959458,
    "title": "How do you think the &quot;Quick Add&quot; feature in Google Calendar works?",
    "body": "<p>Am thinking about a project which might use similar functionality to how \"Quick Add\" handles parsing natural language into something that can be understood with some level of semantics. I'm interested in understanding this better and wondered what your thoughts were on how this might be implemented.</p>\n\n<hr>\n\n<p>If you're unfamiliar with what \"Quick Add\" is, check out <a href=\"http://www.google.com/support/calendar/bin/answer.py?hl=en&amp;answer=36604#text\" rel=\"noreferrer\">Google's KB</a> about it.</p>\n\n<hr>\n\n<p><strong>6/4/10 Update</strong><br>\nAdditional research on \"Natural Language Parsing\" (NLP) yields results which are MUCH broader than what I feel is actually implemented in something like \"Quick Add\". Given that this feature expects specific types of input rather than the true free-form text, I'm thinking this is a much more narrow implementation of NLP. If anyone could suggest more narrow topic matter that I could research rather than the entire breadth of NLP, it would be greatly appreciated.</p>\n\n<p>That said, I've found a nice <a href=\"http://www.aaai.org/AITopics/pmwiki/pmwiki.php/AITopics/NaturalLanguage\" rel=\"noreferrer\">collection of resources about NLP</a> including this great <a href=\"http://www.faqs.org/faqs/natural-lang-processing-faq/\" rel=\"noreferrer\">FAQ</a>.</p>\n",
    "score": 7,
    "creation_date": 1275494875,
    "view_count": 1384,
    "answer_count": 2,
    "tags": "parsing;nlp;google-calendar-api"
  },
  {
    "question_id": 2066005,
    "title": "General frameworks for preparing training data?",
    "body": "<p>As a student of computational linguistics, I frequently do machine learning experiments where I have to prepare training data from all kinds of different resources like raw or annotated text corpora or syntactic tree banks. For every new task and every new experiment I write programs (normally in Python and sometimes Java) to extract the features and values I need and transform the data from one format to the other. This usually results in a very large number of very large files and a very large number of small programs which process them in order to get the input for some machine learning framework (like the arff files for Weka). </p>\n\n<p>One needs to be extremely well organised to deal with that and program with great care not to miss any important peculiarities, exceptions or errors in the tons of data. Many principles of good software design like design patterns or refactoring paradigms are no big use for these tasks because things like security, maintainability or sustainability are of no real importance - once the program successfully processed the data one doesn't need it any longer. This has gone so far that I even stopped bothering about using classes or functions at all in my Python code and program in a simple procedural way. The next experiment will require different data sets with unique characteristics and in a different format so that their preparation will likely have to be programmed from scratch anyway. My experience so far is that it's not unusual to spend 80-90% of a project's time on the task of preparing training data. Hours and days go by only on thinking about how to get from one data format to another. At times, this can become quite frustrating.</p>\n\n<p>Well, you probably guessed that I'm exaggerating a bit, on purpose even, but I'm positive you understand what I'm trying to say. My question, actually, is this:</p>\n\n<p>Are there any general frameworks, architectures, best practices for approaching these tasks?    How much of the code I write can I expect to be reusable given optimal design?</p>\n",
    "score": 7,
    "creation_date": 1263489063,
    "view_count": 425,
    "answer_count": 2,
    "tags": "machine-learning;nlp;code-reuse;training-data"
  },
  {
    "question_id": 66490221,
    "title": "Spacy 3 Confidence Score on Named-Entity recognition",
    "body": "<p>I need to get a confidence score for the tags predicted by NER 'de_core_news_lg' model. There was a well known solution to the problem in the Spacy 2:</p>\n<pre><code>nlp = spacy.load('de_core_news_lg')\ndoc = nlp('ich möchte mit frau Mustermann in der Musterbank sprechen')\ntext = content\ndoc = nlp.make_doc(text)\nbeams = nlp.entity.beam_parse([doc], beam_width=16, beam_density=0.0001)\nfor score, ents in nlp.entity.moves.get_beam_parses(beams[0]):\n    print (score, ents)\n    entity_scores = defaultdict(float)\n    for start, end, label in ents:\n        # print (&quot;here&quot;)\n        entity_scores[(start, end, label)] += score\n        print ('entity_scores', entity_scores)\n</code></pre>\n<p>However, in Spacy 3 I get the following error:</p>\n<pre><code>AttributeError: 'German' object has no attribute 'entity'\n</code></pre>\n<p>Obviously <code>language</code> object does not have <code>entity</code> attribute anymore.\nDoes anyone know how to get the confidence scores in Spacy 3?</p>\n",
    "score": 7,
    "creation_date": 1614937600,
    "view_count": 4560,
    "answer_count": 2,
    "tags": "python;nlp;named-entity-recognition;spacy-3"
  },
  {
    "question_id": 59240668,
    "title": "Text generation using huggingface&#39;s distilbert models",
    "body": "<p>I've been struggling with huggingface's DistilBERT model for some time now, since the documentation seems very unclear and their examples (e.g. <a href=\"https://github.com/huggingface/transformers/blob/master/notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/blob/master/notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb</a> and <a href=\"https://github.com/huggingface/transformers/tree/master/examples/distillation\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/tree/master/examples/distillation</a>) are extremely thick and the thing they are showcasing doesn't seem well documented.</p>\n\n<p>I'm wondering if anyone here has any experience and knows of some good code example for basic in-python usage of their models. Namely:</p>\n\n<ul>\n<li><p>How to properly decode the output of the model into actual text (no matter how I change its shape the tokenizer seems willing to decode it and always yields some sequence of <code>[UNK]</code> tokens)</p></li>\n<li><p>How to actually use their schedulers+optimizers to train a model for a simple text to text task.</p></li>\n</ul>\n",
    "score": 7,
    "creation_date": 1575845812,
    "view_count": 1347,
    "answer_count": 1,
    "tags": "machine-learning;nlp;pytorch;huggingface-transformers;distilbert"
  },
  {
    "question_id": 29572319,
    "title": "Extracting age-related info from text",
    "body": "<p>I am trying to find mention of age in a large dataset of messages posted by users on the internet (stored in a .csv)</p>\n\n<p>I am currently using regular expressions in python to extract age and save it in a list</p>\n\n<p>For example,\n    \"I am 20 years old\" would return 20 to the list\n    \"He is 30 now\" would return 30\n    \"She is in her fifties\" would return 50</p>\n\n<p>But the problem is, using RE is very slow for a huge dataset and if text is in a pattern not satisfied by my RE, then I cannot get the age... So, my question is: Is there a better way of doing this? Perhaps some NLP packages/tools in python?\nI tried researching if nltk has something for this, but it doesnt.</p>\n\n<p>ps:Sorry if the question is unclear, english is not my first language..\n   I have included some of the RE i used below..</p>\n\n<pre><code>m = re.search(r'.*(I|He|She) (is|am) ([0-9]{2}).*',s,re.IGNORECASE)\nn = re.search(r'.*(I|He|She) (is|am) in (my|his|her) (late|mid|early)? ?(tens|twenties|thirties|forties|fifties|sixties|seventies|eighties|nineties|hundreds).*',s,re.IGNORECASE)\no = re.search(r'.*(I|He|She) (is|am) (twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen) ?(one|two|three|four|five|six|seven|eight|nine)?.*',s,re.IGNORECASE)\np = re.search(r'.*(age|is|@|was) ([0-9]{2}).*',s,re.IGNORECASE)\nq = re.search(r'.*(age|is|@|was) (twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen) ?(one|two|three|four|five|six|seven|eight|nine)?.*',s,re.IGNORECASE)\nr = re.search(r'.*([0-9]{2}) (yrs|years).*',s,re.IGNORECASE)\ns = re.search(r'.*(twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen) ?(one|two|three|four|five|six|seven|eight|nine)? (yrs|years).*',s,re.IGNORECASE)\n</code></pre>\n",
    "score": 7,
    "creation_date": 1428709509,
    "view_count": 1980,
    "answer_count": 2,
    "tags": "python;regex;nlp"
  },
  {
    "question_id": 25449948,
    "title": "How to disambiguate words in Conceptnet",
    "body": "<p><a href=\"http://conceptnet5.media.mit.edu/\" rel=\"nofollow\">Conceptnet</a> contains two basic types of nodes, words (e.g. /c/en/cat) and senses (e.g. /c/en/cat/n/domestic_cat). Unfortunately, the vast majority of edges use word nodes. This makes inferring difficult, because I can't be sure which sense a word-to-word edge is referring to. </p>\n\n<p>For example, Conceptnet contains 9 senses that use the word \"cat\", most being proper nouns (/c/en/cat/n/musical, /c/en/cat/n/magazine, /c/en/cat/n/a_spiteful_woman_gossip, etc). If an edge says \"/c/en/cat /r/HasA /c/en/tail\", I know by using my own experience that that's probably referring to /c/en/cat/n/domestic_cat and no other senses. Whereas if I see an edge that says \"/c/en/cat /r/IsA /c/en/fun_to_watch\", I know it's probably referring to /c/en/cat/n/musical, but it also still might be referring to /c/en/cat/n/domestic_cat.</p>\n\n<p>How do I automate this process? How do I translate edges that only use word nodes so that they use sense nodes?</p>\n",
    "score": 7,
    "creation_date": 1408719260,
    "view_count": 765,
    "answer_count": 1,
    "tags": "nlp;wordnet;word-sense-disambiguation;conceptnet"
  },
  {
    "question_id": 22678496,
    "title": "Convert interrogative sentence to imperative sentence",
    "body": "<p>I'm trying to develop a Natural Language Interfaces to Database, and I'm just wondering if there is a library or an API (Java) that I can use to convert a question (interrogative sentence) to a command (imperative sentence).</p>\n\n<p>Ex : from \"Which employees were born before 1970?\" to \"Get employees born before 1970.\"</p>\n",
    "score": 7,
    "creation_date": 1395895484,
    "view_count": 2133,
    "answer_count": 1,
    "tags": "nlp;sentence"
  },
  {
    "question_id": 19079547,
    "title": "Why does the Penn Treebank POS tagset have a separate tag for the word &#39;to&#39;?",
    "body": "<p>The <a href=\"http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html\" rel=\"noreferrer\">Penn Treebank tagset</a> has a separate tag <code>TO</code> for the word 'to', irrespective of whether it's used in the preposition sense (such as <code>I went to school</code>) or the infinitive sense (such as <code>I want to eat</code>). What purpose does this serve from an overall NLP perspective? Just tagging the infinitival 'to' separately makes intuitive sense, but I don't see the logic behind combining an infinitive and a preposition in a single tag.</p>\n\n<p>Thanks, and apologies if this doesn't fit the stack overflow guidelines.</p>\n",
    "score": 7,
    "creation_date": 1380467135,
    "view_count": 782,
    "answer_count": 1,
    "tags": "nlp;pos-tagger"
  },
  {
    "question_id": 14339290,
    "title": "How to extract keywords (tags) from text",
    "body": "<p>i am currently trying to implement a tagging engine in Java and searched for solutions to extract keywords/tag from texts (articles). I have found some solutions on stackoverflow suggesting to use Pointwise Mutual Information.</p>\n\n<p><a href=\"https://stackoverflow.com/questions/2764116/tag-generation-from-a-small-text-content-such-as-tweets\">Solution 1</a></p>\n\n<p><a href=\"https://stackoverflow.com/questions/2661778/tag-generation-from-a-text-content/2664351#2664351\">Solution 2</a></p>\n\n<p>I cant use pyton and nltk so i have to implement it myself. But i dont know how to calculate the probabilities.\nThe equation looks like this:</p>\n\n<pre><code>PMI(term, doc) = log [ P(term, doc) / (P(term)*P(doc)) ]\n</code></pre>\n\n<p>What i want to know is how to calculate P(term, doc)</p>\n\n<p>I already have a lange text corpus and a collection of articles. The articles are not part of the corpus. The corpus is indexed with lucene. </p>\n\n<p>Please help me out.\nBest regards.</p>\n",
    "score": 7,
    "creation_date": 1358258132,
    "view_count": 10122,
    "answer_count": 1,
    "tags": "tags;nlp;keyword;nltk"
  },
  {
    "question_id": 8952760,
    "title": "How can I vary the sentence prefix &quot;I am working on [X]&quot; such that it has correct sentence structure for all X?",
    "body": "<p>I want the user to be able to enter a task and I will prefix it appropriately such that it has correct sentence structure.</p>\n\n<p>E.g.</p>\n\n<pre><code>I am working on [making the world a better place]\n</code></pre>\n\n<p>...sounds good.</p>\n\n<pre><code>I am working on [discuss draft proposal]\n</code></pre>\n\n<p>...doesn't sound good. In this case it would want the program to respond with something like:</p>\n\n<pre><code>I am discussing a draft proposal\n</code></pre>\n\n<p>Basically the way people write tasks or todos appears to be imperative (e.g. pick up milk, write essay, etc.) or simply a noun (e.g. assignment 1, client meeting, etc.). I want to convert these to <a href=\"http://www.englishpage.com/verbpage/presentcontinuous.html\">Present Progressive</a> tense.</p>\n\n<p>I am looking into the field of Natural Language Processing at the moment, but I was wondering if there was some sort of API available that would do what I need, or if someone has had experience with a similar problem.</p>\n",
    "score": 7,
    "creation_date": 1327146774,
    "view_count": 411,
    "answer_count": 1,
    "tags": "api;nlp"
  },
  {
    "question_id": 7677201,
    "title": "Identifying important words and phrases in text",
    "body": "<p>I have text stored in a python string.</p>\n\n<p><strong>What I Want</strong></p>\n\n<ol>\n<li>To identify key words in that text.</li>\n<li>to identify N-grams in that text (ideally more than just bi and tri grams).</li>\n</ol>\n\n<p>Keep in mind...</p>\n\n<ul>\n<li>The text might be small (e.g. tweet sized)</li>\n<li>The text might be middle (e.g. news article sized)</li>\n<li>The text might be large (e.g. book or chapter sized)</li>\n</ul>\n\n<p><strong>What I Have</strong></p>\n\n<p>I'm already using <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">nltk</a> to break the corpus into tokens and remove stopwords:</p>\n\n<pre><code>    # split across any non-word character\n    tokenizer = nltk.tokenize.RegexpTokenizer('[^\\w\\']+', gaps=True)\n\n    # tokenize\n    tokens = tokenizer.tokenize(text)\n\n    # remove stopwords\n    tokens = [w for w in tokens if not w in nltk.corpus.stopwords.words('english')]\n</code></pre>\n\n<p>I'm aware of the BigramCollocationFinder and TrigramCollectionFinder which does exaclty what I'm looking for for those two cases.</p>\n\n<p><strong>The Question</strong></p>\n\n<p>I need advice for n-grams of higher order, improving the kinds of results that come from BCF and TCF, and advice on the best way to identify the most unique individual key words.</p>\n\n<p>Many thanks!</p>\n",
    "score": 7,
    "creation_date": 1317917204,
    "view_count": 5631,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 5457993,
    "title": "Python: Clustering Search Engine Keywords",
    "body": "<p>Python: Clustering Search Engine Keywords </p>\n\n<p>Hi,\nI have a CSV, up to 20,000 rows (I have had 100,000+ for different websites), each row containing a referring keyword (i.e.  a keyword someone typed into a search engine to find the website in question), and a number of visits.</p>\n\n<p>What I'm looking to do is cluster these keywords into clusters of \"similar meaning\", and create a hierarchy of the clusters (structured in order of summed total number of searches per cluster).</p>\n\n<p>An example cluster - \"womens clothing\" - would ideally contain keywords along these lines: \nwomens clothing, 1000 \nladies wear, 300 \nwomens clothes, 50 \nladies clothing, 6 \nwomens wear, 2 </p>\n\n<p>I could look to use something like the Python Natural Language Toolkit: <a href=\"http://www.nltk.org/\" rel=\"nofollow noreferrer\">http://www.nltk.org/</a> and WordNet, but, I'm guessing that for some websites the referring keywords will be words/phrases that WordNet knows nothing about. For example, if the website is a celebrity website WordNet is unlikely to know anything about \"Lady Gaga\", worse situation if the website is a news website.</p>\n\n<p>So, I'm also guessing therefore that the solution has to be one that looks to use just the source data itself.</p>\n\n<p>My query is very similar to the one raised at <a href=\"https://stackoverflow.com/questions/4617023/how-to-cluster-search-engine-keywords\">How to cluster search engine keywords?</a>, only I'm looking for somewhere to start but using Python instead of Java.</p>\n\n<p>I did also wonder whether Google Predict and/or Google Refine might be of any use.</p>\n\n<p>Anyway, any thoughts/suggestions most welcome,</p>\n\n<p>Thanks,\nC</p>\n",
    "score": 7,
    "creation_date": 1301309928,
    "view_count": 6648,
    "answer_count": 2,
    "tags": "python;text;nlp;cluster-analysis;keyword"
  },
  {
    "question_id": 3048268,
    "title": "Online job-searching is tedious. Help me automate it",
    "body": "<p>Many job sites have broken searches that don't let you narrow down jobs by experience level. Even when they do, it's usually wrong. This requires you to wade through hundreds of postings that you can't apply for before finding a relevant one, quite tedious. Since I'd rather focus on writing cover letters etc., I want to write a program to look through a large number of postings, and save the URLs of just those jobs that don't require years of experience.</p>\n\n<p>I don't require help writing the scraper to get the html bodies of possibly relevant job posts. The issue is accurately detecting the level of experience required for the job. This should not be too difficult as job posts are usually very explicit about this (\"must have 5 years experience in...\"), but there may be some issues with overly simple solutions.</p>\n\n<p>In my case, I'm looking for entry-level positions. Often they don't say \"entry-level\", but inclusion of the words probably means the job should be saved.</p>\n\n<p>Next, I can safely exclude a job the says it requires \"5 years\" of experience in whatever, so a regex like /\\d\\syears/ seems reasonable to exclude jobs. But then, I realized some jobs say they'll take 0-2 years of experience, matches the exclusion regex but is clearly a job I want to take a look at. Hmmm, I can handle that with another regex. But some say \"less than 2 years\" or \"fewer than 2 years\". Can handle that too, but it makes me wonder what other patterns I'm not thinking of, and possibly excluding many jobs. That's what brings me here, to find a better way to do this than regexes, if there is one.</p>\n\n<p>I'd like to minimize the false negative rate and save all the jobs that seem like they might not require many years of experience. Does excluding anything that matches /[3-9]\\syears|1\\d\\syears/ seem reasonable? Or is there a better way? Training a bayesian filter maybe?</p>\n\n<p><strong>Edit:</strong> There's a similar, but harder problem, which would probably be more useful to solve. There are lots of jobs that just require an \"engineering degree\", as you just have to understand a few technical things. But searching for \"engineering\" gives you thousands of jobs, mostly irrelevant.</p>\n\n<p>How do I narrow this down to just those jobs that require any engineering degree, rather than particular degrees, without looking at each myself?</p>\n",
    "score": 7,
    "creation_date": 1276629265,
    "view_count": 618,
    "answer_count": 3,
    "tags": "python;ruby;regex;perl;nlp"
  },
  {
    "question_id": 77102881,
    "title": "OpenAI API: What would be a good strategy to handle 80+ function calling?",
    "body": "<p>My business handles a variety of entities (job, invoice, quote, resource, vehicle, contact, person, message, alert, etc.).</p>\n<p>My goal is to use OpenAI function calling to allow my users to ask &quot;anything&quot; about their data. If you roughly estimate 20 entities and 4 potential actions each, that's approximately 80 API calls I need to define in my prompt so GPT can select the one best matching the user's question.</p>\n<p>Not only would this consume a significant number of tokens, but it might also confuse the engine.</p>\n<p>What are some effective strategies to work around this issue?</p>\n<p>Initially, I considered presenting users with a route so they can first select which area of my system they want to inquire about, in order to narrow it down. However, this defeats the purpose of being able to &quot;ask for anything.&quot;</p>\n<p>I'm also considering making an initial call to the OpenAI API to identify the entity from a predefined list, followed by a second call with a list of my APIs matching that entity. However, if the user's language doesn't match my predefined list, I may direct the logic down the wrong path. For example, &quot;<em>What is Ryan's phone number?</em>&quot; Is Ryan a resource? A person? A web user? What if the context needs switching because the user asks a follow-up question?</p>\n<p><strong>-- Edit --</strong></p>\n<p>After Rok's answer I think I need to precise I have over 2,000 customers who will be be asking questions about their own data. Each of these customers could have 100k CRM contacts, 100k jobs completed for these contacts, 100k invoices raised for these jobs etc...</p>\n",
    "score": 7,
    "creation_date": 1694678717,
    "view_count": 3086,
    "answer_count": 2,
    "tags": "nlp;tokenize;openai-api;chatgpt-api;gpt-4"
  },
  {
    "question_id": 62565480,
    "title": "Differentially generate sentences with Huggingface Library for adversarial training (GANs)",
    "body": "<p>I have the following goal, which I have been trying to achieve with the Huggingface Library but I encountered some roadblocks.</p>\n<p><strong>The Problem:</strong></p>\n<p>I want to generate sentences in a differentiable way at training time. Why am I doing this? I want to apply a discriminator to this output to generate sentences with certain properties, which are &quot;enforced&quot; by the discriminator. These sentences will also be conditioned on a input sentence, so I need a Encoder Decoder Model.</p>\n<p>To get around the non differentiability of argmax, I simply take the softmax output of the decoder and multiply it with my embedding matrix. Then I am taking this embedded input and feed it into a transformer discriminator, which simply classifies the input as original/fake. Then I backpropagate through the encoder decoder. Just as one would do it with a normal GAN.</p>\n<p>So far I have tried to use the <code>EncoderDecoderModel</code> from Huggingface. This class has a method named generate, which generates sentences in a non differentiable way (greedy or beam-search). So I dug through the source code and tried to build my own differentiable generate method. I didn't get it to work though.</p>\n<p><strong>Questions:</strong></p>\n<ul>\n<li>Is there a reasonably easy way to do this with the Huggingface Library, as I really want to use the pretrained models and everything else that comes with it?</li>\n<li>Is there a way to invoke the forward method of the decoder and only generate one new token, not the whole sequence again?</li>\n</ul>\n<p>Thanks for your help, I would really appreciate it, I have been stuck on this for quiet a while now.</p>\n",
    "score": 7,
    "creation_date": 1593040605,
    "view_count": 401,
    "answer_count": 1,
    "tags": "nlp;pytorch;huggingface-transformers;generative-adversarial-network;discriminator"
  },
  {
    "question_id": 61969412,
    "title": "How to use multiple text features for NLP classifier?",
    "body": "<p>I am trying to build text classifier, Usually, we have one text column and ground truth. But I am working on a problem where dataset contains many text features. I am exploring different ways how to make use of different text features.</p>\n\n<p>For example, my dataset looks like this </p>\n\n<pre><code>Index_no                   domain  comment_by   comment       research_paper      books_name\n\n01                         Science  Professor   Thesis needs  Evolution of         MOIRCS \n                                                more work     Quiescent            Deep \n                                                              Galaxies as a        Survey\n                                                              Function of\n                                                              Stellar Mass       \n\n\n\n02                         Math    Professor   Doesn't follow  Evolution of   \n                                               Latex format   Quiescent           nonlinear \n                                                              Galaxies as a       dispersive\n                                                              Function of         equations\n                                                              Stellar Mass             \n</code></pre>\n\n<p>This is just a dummy dataset, Here my ground truth (Y) is domain and features are <code>comment_by</code>, <code>comment</code>, <code>research_paper</code>, <code>books_name</code></p>\n\n<p>If I am using any NLP model (RNN-LSTM, Transformers etc), those models usually take one 3 dim vectors, for that if I am using one text column that works but How to many text features for text classifier? </p>\n\n<p>What I've tried :</p>\n\n<blockquote>\n  <p>1) <strong>Joining all column and making a long string</strong></p>\n</blockquote>\n\n<p>Professor Thesis needs more work Evolution of Quiescent Galaxies as a Function of Stellar Mass MOIRCS Deep Survey  </p>\n\n<blockquote>\n  <p>2) <strong>Using a token between columns</strong></p>\n</blockquote>\n\n<pre><code>&lt;CB&gt; Professor &lt;C&gt; Thesis needs more work &lt;R&gt; Evolution of Quiescent Galaxies as a Function of Stellar Mass &lt;B&gt; MOIRCS Deep Survey \n</code></pre>\n\n<p>where <code>&lt;CB&gt;</code> comment_by , <code>&lt;C&gt;</code> comment, <code>&lt;R&gt;</code> research_paper, <code>&lt;B&gt;</code> books_name</p>\n\n<p>Should I use <code>&lt;CB&gt;</code> at the beginning or use like this?</p>\n\n<pre><code>Professor &lt;1&gt; Thesis needs more work &lt;2&gt; Evolution of Quiescent Galaxies as a Function of Stellar Mass &lt;3&gt; MOIRCS Deep Survey\n</code></pre>\n\n<blockquote>\n  <p>3) <strong>Using different dense layers (or embedding) for each column and\n  concatenate them.</strong></p>\n</blockquote>\n\n<p>I've tried all three approaches, Is there any other approach I can try to improve the model accuracy? or extract, combine, join the better features?</p>\n\n<p>Thanks in advance!</p>\n",
    "score": 7,
    "creation_date": 1590222775,
    "view_count": 2028,
    "answer_count": 1,
    "tags": "machine-learning;keras;deep-learning;neural-network;nlp"
  },
  {
    "question_id": 59155676,
    "title": "Unable to train my keras model : (Data cardinality is ambiguous:)",
    "body": "<p>I am using the bert-for-tf2 library to do a Multi-Class Classification problem. I created the model but training throws the following error:</p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-25-d9f382cba5d4&gt; in &lt;module&gt;()\n----&gt; 1 model.fit([INPUT_IDS,INPUT_MASKS,INPUT_SEGS], list(train.SECTION))\n\n5 frames\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/data_adapter.py in \n__init__(self, x, y, sample_weights, batch_size, epochs, steps, shuffle, **kwargs)\n243             label, \", \".join([str(i.shape[0]) for i in nest.flatten(data)]))\n244       msg += \"Please provide data which shares the same first dimension.\"\n--&gt; 245       raise ValueError(msg)\n246     num_samples = num_samples.pop()\n247 \n\nValueError: Data cardinality is ambiguous:\nx sizes: 3\ny sizes: 6102\nPlease provide data which shares the same first dimension.\n</code></pre>\n\n<p>I am referring the medium article called <a href=\"https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\" rel=\"noreferrer\">Simple BERT using TensorFlow 2.0</a> \nThe git repo for the library bert-for-tf2 can be found <a href=\"https://github.com/kpe/bert-for-tf2/tree/master/bert\" rel=\"noreferrer\">here</a>.</p>\n\n<p>Please find the entire code <a href=\"https://gist.github.com/AmalVijayan/d7d50e5ea4af39760dc9b55daa840dec\" rel=\"noreferrer\">here</a>.</p>\n\n<p><a href=\"https://colab.research.google.com/drive/1fIbPKxMayTgmRbFGQcC6tfY_6ZAXca-t\" rel=\"noreferrer\">Here</a> is a link to my colab notebook</p>\n\n<p>Really appreciate your help!</p>\n",
    "score": 7,
    "creation_date": 1575371099,
    "view_count": 20810,
    "answer_count": 1,
    "tags": "machine-learning;nlp;text-classification;tensorflow2.0;tf.keras"
  },
  {
    "question_id": 9136940,
    "title": ".NET dll for Natural language to SQL/SPARQL",
    "body": "<p>I'm trying to build an interface for my tool to query from Semantic/Relational DB using C#.NET</p>\n\n<p>I now need to have a layer above the query layer to convert NL input to SQL/SPARQL, I read through papers of NLIs, The process of making such a layer is such a load for my project besides, it's not the main target, it's an add-on.</p>\n\n<p>I don't care if the dll supports Guided input only or freely input text and handles unmatchings, I just need a dll to start from and add some code on it.</p>\n\n<p>The fact of whether it should support both SQL and SPARQL doesn't really matter, because I can manage to convert one to another in my project's domain (something local)</p>\n\n<p>any idea on available dlls ?</p>\n",
    "score": 7,
    "creation_date": 1328312314,
    "view_count": 1796,
    "answer_count": 2,
    "tags": "sql;nlp;sparql"
  },
  {
    "question_id": 73093272,
    "title": "Is there a way tell a trained transformer model (e.g. from hugging face) to cast to float?",
    "body": "<p>I am attempting to run the T5 transformer on an M1 Mac using MPS backend:</p>\n<pre><code>import torch\nimport json \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n#Make sure sentencepiece is installed\ndevice = torch.device('mps')\nmodel = T5ForConditionalGeneration.from_pretrained('t5-3b').to(&quot;mps&quot;)\ntokenizer = T5Tokenizer.from_pretrained('t5-3b')#, device = device)\n\npreprocess_text = full_text.strip().replace(&quot;\\n&quot;,&quot;.&quot;)\nt5_prepared_Text = &quot;summarize: &quot;+preprocess_text\nprint (&quot;original text preprocessed: \\n&quot;, preprocess_text)\ntokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=&quot;pt&quot;).to(device)\n\n# summmarize \nsummary_ids = model.generate(tokenized_text,\n                                    num_beams=6,\n                                    no_repeat_ngram_size=3,\n                                    min_length=30,\n                                    max_length=9000,\n                                    early_stopping=True)\n\noutput = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nprint (&quot;\\n\\nSummarized text: \\n&quot;,output)\n\n</code></pre>\n<p>where &quot;full_text&quot; is a string defined earlier. This code works with CPU but I'd like to speed it up by using MPS (as shown in the code above). Here, I get the error:</p>\n<pre><code>TypeError: Operation 'abs_out_mps()' does not support input type 'int64' in MPS backend.\n</code></pre>\n<p>So int64 isn't supported with MPS. Is there a way I can get the model to cast to float if this error occurs rather than just breaking the whole model?</p>\n",
    "score": 7,
    "creation_date": 1658601070,
    "view_count": 611,
    "answer_count": 0,
    "tags": "python-3.x;nlp;apple-m1;torch;huggingface-transformers"
  },
  {
    "question_id": 61322251,
    "title": "Token indices sequence length error when using encode_plus method",
    "body": "<p>I got a strange error when trying to encode question-answer pairs for BERT using the <code>encode_plus</code> method provided in the Transformers library.</p>\n<p>I am using data from <a href=\"https://www.kaggle.com/c/google-quest-challenge/data\" rel=\"noreferrer\">this Kaggle competition</a>. Given a question title, question body and answer, the model must predict 30 values (regression problem). My goal is to get the following encoding as input to BERT:</p>\n<p>[CLS] question_title question_body [SEP] answer [SEP]</p>\n<p>However, when I try to use</p>\n<pre><code>tokenizer = transformers.BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\n</code></pre>\n<p>and encode only the second input from train.csv as follows:</p>\n<pre><code>inputs = tokenizer.encode_plus(\n            df_train[&quot;question_title&quot;].values[1] + &quot; &quot; + df_train[&quot;question_body&quot;].values[1], # first sequence to be encoded\n            df_train[&quot;answer&quot;].values[1], # second sequence to be encoded\n            add_special_tokens=True, # [CLS] and 2x [SEP] \n            max_len = 512,\n            pad_to_max_length=True\n            )\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>Token indices sequence length is longer than the specified maximum sequence length for this model (46 &gt; 512). Running this sequence through the model will result in indexing errors\n</code></pre>\n<p>It says that the length of the token indices is longer than the specified maximum sequence length, but this is not true (as you can see, 46 is not &gt; 512).</p>\n<p>This happens for several of the rows in <code>df_train</code>. Am I doing something wrong here?</p>\n",
    "score": 7,
    "creation_date": 1587384771,
    "view_count": 5676,
    "answer_count": 1,
    "tags": "nlp;tokenize;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 58594017,
    "title": "Input Text Data Formatting for CNN in Flux, in Julia",
    "body": "<p>I am implementing Yoon Kim's CNN (<a href=\"https://arxiv.org/abs/1408.5882\" rel=\"noreferrer\">https://arxiv.org/abs/1408.5882</a>) for text classification in Julia, using Flux as the deep learning framework, with individual sentences as input datapoints. The model zoo (<a href=\"https://github.com/FluxML/model-zoo\" rel=\"noreferrer\">https://github.com/FluxML/model-zoo</a>) has proven useful to an extent, but it does not have an NLP example with CNNs. I'd like to check if my input data format is the correct one.  </p>\n\n<p>There is no explicit implementation in Flux of a 1D Conv, so I'm using Conv found in <a href=\"https://github.com/FluxML/Flux.jl/blob/master/src/layers/conv.jl\" rel=\"noreferrer\">https://github.com/FluxML/Flux.jl/blob/master/src/layers/conv.jl</a>\nHere is part of the docstring that explains the input data format:</p>\n\n<pre><code>Data should be stored in WHCN order (width, height, # channels, # batches).\nIn other words, a 100×100 RGB image would be a `100×100×3×1` array,\nand a batch of 50 would be a `100×100×3×50` array.\n</code></pre>\n\n<p>My format is as follows:</p>\n\n<pre><code>1. width: since text in a sentence is 1D, the width is always 1 \n2. height: this is the maximum number of tokens allowable in a sentence\n3. \\# of channels: this is the embedding size\n4. \\# of batches: the number of sentences in each batch\n</code></pre>\n\n<p>Following the MNIST example in the model zoo, I have </p>\n\n<pre><code>function make_minibatch(X, Y, idxs)\n    X_batch = zeros(1, num_sentences, emb_dims, MAX_LEN)\n\n    function get_sentence_matrix(sentence)\n        embeddings = Vector{Array{Float64, 1}}()\n        for word in sentence\n            embedding = get_embedding(word)\n            push!(embeddings, embedding)\n        end\n        embeddings = hcat(embeddings...)\n        return embeddings\n    end\n\n    for i in 1:length(idxs)\n        X_batch[1, i, :, :] = get_sentence_matrix(X[idxs[i]])\n    end\n    Y_batch = [Flux.onehot(label+1, 1:2) for label in Y[idxs]]\n    return (X_batch, Y_batch)\nend\n</code></pre>\n\n<p>where the X is an array of arrays of words and the get_embedding function returns an embedding as an array. </p>\n\n<p><code>X_batch</code> is then a <code>Array{Float64,4}</code>. Is this the correct approach?</p>\n",
    "score": 7,
    "creation_date": 1572277428,
    "view_count": 292,
    "answer_count": 0,
    "tags": "deep-learning;nlp;julia;conv-neural-network;flux.jl"
  },
  {
    "question_id": 30946906,
    "title": "Transforming a text from first-person to third-person",
    "body": "<p>I would like to transform a big string - written in a first-person perspective - to a third-person one. Such as:</p>\n\n<blockquote>\n  <p>I am very happy. I went to the swimming pool with my friend.</p>\n</blockquote>\n\n<p>to</p>\n\n<blockquote>\n  <p>He is very happy. He went to the swimming pool with his friend.</p>\n</blockquote>\n\n<p>I've searched for a library (any language - Python preferred) that could do that but could not find any.</p>\n\n<p>Does such libraries exist?</p>\n\n<p>If there is no such library, my idea was the following:</p>\n\n<ol>\n<li>Use a simple regular expression to change all the 'I' by 'He'</li>\n<li>Use NLTK to grammar check the text and correct the verbs so it can fit with the third-person</li>\n</ol>\n\n<p>Does it seems like a 'proper' solution?</p>\n",
    "score": 7,
    "creation_date": 1434746333,
    "view_count": 1946,
    "answer_count": 0,
    "tags": "python;nlp"
  },
  {
    "question_id": 29734260,
    "title": "Memory Efficient data structure for Trie Implementation",
    "body": "<p>I am implementing a Trie in python. Till now I have come across two different methods to implement it:</p>\n<h3>1) use a class Node (similar to struct Node in C++) with data members:</h3>\n<p><code>char</code> - to store character<br>\n<code>is_end</code> - to store end of word (true or false)<br>\n<code>prefix_count</code> - store number of words with current prefix<br></p>\n<p><code>child</code> - Node type dict (to store other nodes i.e. for 26 alphabets)</p>\n<pre><code>class Node(object):\n    def __init__(self):\n        self.char = ''\n        self.word = ''\n        self.is_end = False\n        self.prefix_count = 0\n        self.child = {}\n</code></pre>\n<h3>2) use a dictionary to store all the data:</h3>\n<p>e.g. for the input <code>words = {'foo', 'bar', 'baz', 'barz'}</code></p>\n<p>we derive this dictionary:</p>\n<pre><code>         {'b': {'a': {'r': {'_end_': '_end_', 'z': {'_end_': '_end_'}},\n          'z': {'_end_': '_end_'}}},\n          'f': {'o': {'o': {'_end_': '_end_'}}}}\n</code></pre>\n<p>Which is the efficient and standard data structure, which is both memory efficient and fast for traversal and other trie operations on big dataset of words?</p>\n",
    "score": 7,
    "creation_date": 1429469193,
    "view_count": 2626,
    "answer_count": 3,
    "tags": "python;data-structures;tree;nlp;trie"
  },
  {
    "question_id": 2750931,
    "title": "Is it better to use a &quot;natural&quot; language to write code?",
    "body": "<p>I recently saw a programming language called <a href=\"http://supernova.sourceforge.net/\" rel=\"nofollow noreferrer\">supernova</a> and they said in the web page :</p>\n\n<blockquote>\n  <p>The Supernova Programming language is\n  a modern scripting language and the </p>\n  \n  <p>First one presents the concept of\n  programming with direct Fiction\n  Description using </p>\n  \n  <p>Clear subset of pure Human Language.</p>\n</blockquote>\n\n<p>and you can write code like:</p>\n\n<pre><code>i want window and the window title is Hello World.\ni want button and button caption is Close.\nand button name is btn1.\n\nbtn1 mouse click. instructions are\n   you close window\nend of instructions\n</code></pre>\n\n<p>my question is not about the language itself but it is that are we need such languages and did they make writing code easier or not?</p>\n",
    "score": 6,
    "creation_date": 1272735677,
    "view_count": 1024,
    "answer_count": 10,
    "tags": "nlp"
  },
  {
    "question_id": 23861355,
    "title": "How to install and invoke Stanford NERTagger?",
    "body": "<p>I am trying to use NLTK interface for Stanford NER in the python enviornment, <a href=\"http://nltk.org/api/nltk.tag.html?highlight=stanford#nltk.tag.stanford.NERTagger\" rel=\"noreferrer\"><code>nltk.tag.stanford.NERTagger</code></a>.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from nltk.tag.stanford import NERTagger\nst = NERTagger('/usr/share/stanford-ner/classifiers/all.3class.distsim.crf.ser.gz',\n               '/usr/share/stanford-ner/stanford-ner.jar') \nst.tag('Rami Eid is studying at Stony Brook University in NY'.split()) \n</code></pre>\n\n<p>I am supposed to get the output:</p>\n\n<pre><code>[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),\n('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),\n('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]\n</code></pre>\n\n<p>I have installed NLTK according the procedure described in the <a href=\"http://www.nltk.org/install.html\" rel=\"noreferrer\">NLTK website</a>. However, I can not find /usr/share/stanford-ner at all. Where and how do I find the whole package and install it in my directory.</p>\n",
    "score": 6,
    "creation_date": 1401065502,
    "view_count": 19133,
    "answer_count": 5,
    "tags": "python;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 2699646,
    "title": "How to get logical parts of a sentence with java?",
    "body": "<p>Let's say there is a sentence:</p>\n\n<pre><code>On March 1, he was born.\n</code></pre>\n\n<p>Changing it to </p>\n\n<pre><code>He was born on March 1.\n</code></pre>\n\n<p>doesn't break the sense of the sentence and it is still valid. Shuffling words in any other way would produce weird to invalid sentences. So basically, I'm talking about parts of the sentence, which make the information more specific, but removing them doesn't break the whole sentence. Is there any NLP library in which identifying such parts is available?</p>\n",
    "score": 6,
    "creation_date": 1272035351,
    "view_count": 6918,
    "answer_count": 2,
    "tags": "java;artificial-intelligence;nlp;linguistics"
  },
  {
    "question_id": 47485687,
    "title": "NameError: name &#39;stopwords&#39; is not defined",
    "body": "<p>I'm getting the error <code>NameError: name 'stopwords' is not defined</code> for some reason, even though I have the package installed. I'm trying to do natural language processing on some feedback reviews. The <code>dataset</code> object is a table with two columns, <code>Reviews (a sentence of feedback)</code> and target variable <code>Liked (1 or 0)</code>.  Help appreciated, thanks!</p>\n\n<hr>\n\n<p>Block 1 </p>\n\n<pre><code>import re\nimport nltk\nnltk.download('stopwords')\n</code></pre>\n\n<p>Output 1 </p>\n\n<pre><code>   &gt; [nltk_data] Downloading package stopwords to\n\n   &gt; [nltk_data]     /Users/user/nltk_data...\n\n   &gt; [nltk_data]   Package stopwords is already up-to-date!\n\n   &gt; Out[14]: True\n</code></pre>\n\n<p>Block 2 </p>\n\n<pre><code>dataset['Review'][0]\nreview = re.sub('[^a-zA-Z]',' ' ,dataset['Review'][0])\nreview = review.lower()\nreview = review.split()\nreview = [word for word in review if not word in stopwords.words('english')] **ERROR ON THIS LINE**\n</code></pre>\n\n<p>Output 2 </p>\n\n<pre><code>&gt;NameError                                 Traceback (most recent call last)\n&lt;ipython-input-16-8d0ee1fd7c7f&gt; in &lt;module&gt;()\n      3 review = review.lower()\n      4 review = review.split()\n----&gt; 5 review = [word for word in review if not word in stopwords.words('english')]\n\n&gt;&lt;ipython-input-16-8d0ee1fd7c7f&gt; in &lt;listcomp&gt;(.0)\n      3 review = review.lower()\n      4 review = review.split()\n----&gt; 5 review = [word for word in review if not word in stopwords.words('english')]\n\n&gt;NameError: name 'stopwords' is not defined\n</code></pre>\n",
    "score": 6,
    "creation_date": 1511610981,
    "view_count": 45432,
    "answer_count": 1,
    "tags": "python;nlp;stop-words"
  },
  {
    "question_id": 1835605,
    "title": "Natural Language Processing: Find obscenities in English?",
    "body": "<p>Given a set of words tagged for part of speech, I want to find those that are obscenities in mainstream English. How might I do this? Should I just make a huge list, and check for the presence of anything in the list? Should I try to use a regex to capture a bunch of variations on a single root?</p>\n\n<p>If it makes it easier, I don't want to filter out, just to get a count. So if there are some false positives, it's not the end of the world, as long as there's a more or less uniformly over exaggerated rate.</p>\n",
    "score": 6,
    "creation_date": 1259786045,
    "view_count": 3354,
    "answer_count": 11,
    "tags": "java;nlp"
  },
  {
    "question_id": 42919076,
    "title": "python charmap codec can&#39;t decode byte X in position Y character maps to &lt;undefined&gt;",
    "body": "<p>I'm experimenting with python libraries for data analysis,the problem i'm facing is this exception</p>\n\n<blockquote>\n  <p>UnicodeDecodeError was unhandled by user code Message: 'charmap' codec\n  can't decode byte 0x81 in position 165: character maps to &lt; undefined></p>\n</blockquote>\n\n<p>I have looked into answers with similar issues and the OP seems to be either reading text with different encoding or printing it.</p>\n\n<p>In my code the error shows up at import statement,that's what confuses me.\n<a href=\"https://i.sstatic.net/4ZINb.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/4ZINb.png\" alt=\"enter image description here\"></a></p>\n\n<p>I'm using python 64 bit 3.3 on Visual Studio 2015\nand geotext is the library where it shows the error.</p>\n\n<p>Kindly point as to where to look to deal with this error.</p>\n",
    "score": 6,
    "creation_date": 1490074263,
    "view_count": 21565,
    "answer_count": 1,
    "tags": "python;python-3.x;unicode;nlp;python-unicode"
  },
  {
    "question_id": 58270129,
    "title": "Convert categorical data into numerical data in Python",
    "body": "<p>I have a data set. One of its columns - \"Keyword\" - contains categorical data. The machine learning algorithm that I am trying to use takes only numeric data. I want to convert \"Keyword\" column into numeric values - How can I do that? Using NLP? Bag of words?</p>\n\n<p>I tried the following but I got <code>ValueError: Expected 2D array, got 1D array instead</code>.</p>\n\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer()\ndataset['Keyword'] = count_vector.fit_transform(dataset['Keyword'])\nfrom sklearn.model_selection import train_test_split\ny=dataset['C']\nx=dataset(['Keyword','A','B'])\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(x_train,y_train)\n</code></pre>\n",
    "score": 6,
    "creation_date": 1570453371,
    "view_count": 37866,
    "answer_count": 1,
    "tags": "python;machine-learning;encoding;nlp;categorical-data"
  },
  {
    "question_id": 1695971,
    "title": "Does WordNet have &quot;levels&quot;? (NLP)",
    "body": "<p>For example...</p>\n\n<p>Chicken is an <strong>animal</strong>.<br>\nBurrito is a <strong>food</strong>.</p>\n\n<p>WordNet allows you to do \"is-a\"...the hiearchy feature.</p>\n\n<p>However, how do I know when to stop travelling up the tree? I want a LEVEL.<br>\nThat is consistent.</p>\n\n<p>For example, if presented with a bunch of words, I want wordNet to categorize all of them, but at a certain level, so it doesn't go too far up.  Categorizing \"burrito\" as a \"thing\" is too broad, yet \"mexican wrapped food\" is too specific.  I want to go up the hiearchy or down..until the right LEVEL.</p>\n",
    "score": 6,
    "creation_date": 1257676159,
    "view_count": 2819,
    "answer_count": 5,
    "tags": "python;text;nlp;words;wordnet"
  },
  {
    "question_id": 57693333,
    "title": "processing before or after train test split",
    "body": "<p>I am using this excellent article to learn Machine learning.</p>\n\n<p><a href=\"https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/\" rel=\"noreferrer\">https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/</a></p>\n\n<p>The author has tokenized the X and y data after splitting it up.</p>\n\n<pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42\n)\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nmaxlen = 200\n\nX_train = pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\nX_test = pad_sequences(X_test, padding=\"post\", maxlen=maxlen)\n</code></pre>\n\n<p>If I tokenize it before using train_test_split class, I can save a few lines of code.</p>\n\n<pre><code>tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X)\n\nX_t = tokenizer.texts_to_sequences(X)\nvocab_size = len(tokenizer.word_index) + 1\nmaxlen = 200\n\nX = pad_sequences(X_t, padding=\"post\", maxlen=maxlen)\n</code></pre>\n\n<p>I just want to confirm that my approach is correct and I do not expect any surprises later in the script.</p>\n",
    "score": 6,
    "creation_date": 1566998121,
    "view_count": 4830,
    "answer_count": 3,
    "tags": "keras;scikit-learn;nlp;tokenize;train-test-split"
  },
  {
    "question_id": 19495967,
    "title": "Getting additional information (Active/Passive, Tenses ...) from a Tagger",
    "body": "<p>I'm using the Stanford Tagger for determining the Parts of Speech. However, I want to get more information out of the text. Is there a possibility to get further information like the tense of the sentence or if it is in active/passive?</p>\n\n<p>So far, I'm using the very basic PoS-Tagging approach:</p>\n\n<pre><code>List&lt;List&lt;TaggedWord&gt;&gt; taggedUnits = new ArrayList&lt;List&lt;TaggedWord&gt;&gt;();\n\nString input = \"This sentence is going to be future. The door was opened.\";\nfor (List&lt;HasWord&gt; sentence : MaxentTagger.tokenizeText(new StringReader(input)))\n{\n     taggedUnits.add(tagger.tagSentence(sentence));\n}\n</code></pre>\n",
    "score": 6,
    "creation_date": 1382362317,
    "view_count": 4196,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;pos-tagger"
  },
  {
    "question_id": 15253798,
    "title": "R remove stopwords from a character vector using %in%",
    "body": "<p>I have a data frame with strings that I'd like to remove stop words from.  I'm trying to avoid using the <code>tm</code> package as it's a large data set and <code>tm</code> seems to run a bit slowly.  I am using the <code>tm</code> <code>stopword</code> dictionary.</p>\n\n<pre><code>library(plyr)\nlibrary(tm)\n\nstopWords &lt;- stopwords(\"en\")\nclass(stopWords)\n\ndf1 &lt;- data.frame(id = seq(1,5,1), string1 = NA)\nhead(df1)\ndf1$string1[1] &lt;- \"This string is a string.\"\ndf1$string1[2] &lt;- \"This string is a slightly longer string.\"\ndf1$string1[3] &lt;- \"This string is an even longer string.\"\ndf1$string1[4] &lt;- \"This string is a slightly shorter string.\"\ndf1$string1[5] &lt;- \"This string is the longest string of all the other strings.\"\n\nhead(df1)\ndf1$string1 &lt;- tolower(df1$string1)\nstr1 &lt;-  strsplit(df1$string1[5], \" \")\n\n&gt; !(str1 %in% stopWords)\n[1] TRUE\n</code></pre>\n\n<p>This is not the answer I'm looking for.  I'm trying to get a vector or string of the words NOT in the <code>stopWords</code> vector. </p>\n\n<p>What am I doing wrong?</p>\n",
    "score": 6,
    "creation_date": 1362590137,
    "view_count": 18549,
    "answer_count": 3,
    "tags": "r;nlp;subset;tm;stop-words"
  },
  {
    "question_id": 46341840,
    "title": "How do I solve the following error?Input must be a character vector of any length or a list of character vectors, each of which has a length of 1",
    "body": "<p>I am working on a R project. The data set I used is available at the following link\n<a href=\"https://www.kaggle.com/ranjitha1/hotel-reviews-city-chennai/data\" rel=\"nofollow noreferrer\">https://www.kaggle.com/ranjitha1/hotel-reviews-city-chennai/data</a></p>\n<p>The code I have used is.</p>\n<pre><code>df1 = read.csv(&quot;chennai.csv&quot;, header = TRUE)\nlibrary(tidytext)\ntidy_books &lt;- df1 %&gt;% unnest_tokens(word,Review_Text)\n</code></pre>\n<p>Here Review_Text is the text column. Yet, I get the following error.</p>\n<blockquote>\n<p>Error in check_input(x) :\nInput must be a character vector of any length or a list of character\nvectors, each of which has a length of 1.</p>\n</blockquote>\n",
    "score": 6,
    "creation_date": 1505990558,
    "view_count": 19169,
    "answer_count": 1,
    "tags": "r;nlp;sentiment-analysis"
  },
  {
    "question_id": 46202097,
    "title": "NLTK was unable to find the java file! for Stanford POS Tagger",
    "body": "<p>I have been stuck trying to get the Stanford POS Tagger to work for a while. From an <a href=\"https://stackoverflow.com/questions/34726200/nltk-was-unable-to-find-stanford-postagger-jar-set-the-classpath-environment-va\">old SO post</a> I found the following (slightly modified) code:</p>\n\n<pre><code>stanford_dir = 'C:/Users/.../stanford-postagger-2017-06-09/'\n\nfrom nltk.tag import StanfordPOSTagger\n#from nltk.tag.stanford import StanfordPOSTagger # I tried it both ways\nfrom nltk import word_tokenize\n\n# Add the jar and model via their path (instead of setting environment variables):\njar = stanford_dir + 'stanford-postagger.jar'\nmodel = stanford_dir + 'models/english-left3words-distsim.tagger'\n\npos_tagger = StanfordPOSTagger(model, jar, encoding='utf8')\n\ntext = pos_tagger.tag(word_tokenize(\"What's the airspeed of an unladen swallow ?\"))\nprint(text)\n</code></pre>\n\n<p>However, I get the following error:</p>\n\n<pre><code>LookupError: \n\n===========================================================================\nNLTK was unable to find the java file!\nUse software specific configuration paramaters or set the JAVAHOME environment variable.\n===========================================================================\n</code></pre>\n\n<p>I don't know what java file it is talking about. I'm sure it's finding the right files because if I change the path to something incorrect I get a different error:</p>\n\n<pre><code>LookupError: Could not find stanford-postagger.jar jar file at C:/Users/.../stanford-postagger-2017-06-09/sstanford-postagger.jar\n</code></pre>\n\n<p>What java file is missing? How do I get the Stanford POS tagger to work?</p>\n\n<p>EDIT:</p>\n\n<p>I went to this <a href=\"https://gist.github.com/alvations/0ed8641d7d2e1941b9f9\" rel=\"noreferrer\">link for Stanford NLP on Windows</a> and tried:</p>\n\n<p>(Second EDIT - adding the installation procedures):</p>\n\n<pre><code>import urllib.request\nimport zipfile\nurllib.request.urlretrieve(r'http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip', r'C:/Users/HMISYS/Downloads/stanford-postagger-full-2015-04-20.zip')\nzfile = zipfile.ZipFile(r'C:/Users/HMISYS/Downloads/stanford-postagger-full-2015-04-20.zip')\nzfile.extractall(r'C:/Users/HMISYS/Downloads/')\n# End second edit\n\nfrom nltk.tag.stanford import StanfordPOSTagger\n# Trying on an older version\n_model_filename = r'C:/Users/HMISYS/Downloads/stanford-postagger-full-2015-04-20/models/english-bidirectional-distsim.tagger'\n_path_to_jar = r'C:/Users/HMISYS/Downloads/stanford-postagger-full-2015-04-20/stanford-postagger.jar'\nst = StanfordPOSTagger(model_filename=_model_filename, path_to_jar=_path_to_jar)\ntext = st.tag(nltk.word_tokenize(\"What's the airspeed of an unladen swallow ?\"))\nprint(text)\n</code></pre>\n\n<p>but I got the same error. Based on <a href=\"https://gist.github.com/alvations/e1df0ba227e542955a8a\" rel=\"noreferrer\">this post</a> I set my path variables with the following:</p>\n\n<pre><code>set STANFORDTOOLSDIR=$HOME\nset CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar\nset export STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models\n</code></pre>\n\n<p>But I get this error:</p>\n\n<pre><code>NLTK was unable to find stanford-postagger.jar! Set the CLASSPATH environment variable.\n</code></pre>\n",
    "score": 6,
    "creation_date": 1505318405,
    "view_count": 12088,
    "answer_count": 1,
    "tags": "python;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 393248,
    "title": "Counting the number of occurrences of words in a textfile",
    "body": "<p>How could I go about keeping track of the number of times a word appears in a textfile? I would like to do this for every word.</p>\n\n<p>For example, if the input is something like:</p>\n\n<p>\"the man said hi to the boy.\"</p>\n\n<p>Each of \"man said hi to boy\" would have an occurrence of 1.</p>\n\n<p>\"the\" would have an occurence of 2.</p>\n\n<p>I was thinking of keeping a dictionary with word/occurrence pairs but I'm not sure how to implement this in C. A link to any similar or related problems with a solution would be great.</p>\n\n<hr>\n\n<p>EDIT: To avoid rolling out my own hash table I decided to learn how to use glib. Along the way I found an excellent tutorial which walks through a similar problem. <a href=\"http://bo.majewski.name/bluear/gnu/GLib/ch03s03.html\" rel=\"nofollow noreferrer\">http://bo.majewski.name/bluear/gnu/GLib/ch03s03.html</a></p>\n\n<p>I am awestruck by the number of different approaches, and in particular the simplicity and elegance of the Ruby implementation.</p>\n",
    "score": 6,
    "creation_date": 1230244491,
    "view_count": 17091,
    "answer_count": 8,
    "tags": "c;algorithm;nlp;counting"
  },
  {
    "question_id": 51491851,
    "title": "Automatically extracting strings with mismatched spellings from a column and replacing them in R",
    "body": "<p>I have a huge dataset which is similar to the columns posted below</p>\n\n<pre><code>NameofEmployee &lt;- c(x, y, z, a)\nRegion &lt;- c(\"Pune\", \"Orissa\", \"Orisa\", \"Poone\")\n</code></pre>\n\n<p>As you can see, in the <code>Region</code> column, the region \"Pune\" is spelled in two different ways- i.e \"Pune\" and \"Poona\".</p>\n\n<p>Similarly, \"Orissa\" is spelled as \"Orissa\" and \"Orisa\".</p>\n\n<p>I have multiple regions which are actually the same but are spelled in different ways. This will cause problems when I analyze the data.</p>\n\n<p>I want to automatically be able to obtain a list of these mismatched spellings with the help of R.<br>\nI would also like to replace the spellings with the correct spellings automatically.  </p>\n",
    "score": 6,
    "creation_date": 1532413732,
    "view_count": 305,
    "answer_count": 2,
    "tags": "r;string;text-analysis"
  },
  {
    "question_id": 27912176,
    "title": "Stanford parser java error",
    "body": "<p>I am working on a research about NLP, i woul to use Stanford parser to extract noun phrases from text, the parser version i used is 3.4.1\nthis is the sample code i used</p>\n<pre><code>package stanfordparser;\n\n\nimport java.util.Collection;\nimport java.util.List;\nimport java.io.StringReader;\n\nimport edu.stanford.nlp.process.Tokenizer;\nimport edu.stanford.nlp.process.TokenizerFactory;\nimport edu.stanford.nlp.process.CoreLabelTokenFactory;\nimport edu.stanford.nlp.process.DocumentPreprocessor;\nimport edu.stanford.nlp.process.PTBTokenizer;\nimport edu.stanford.nlp.ling.CoreLabel;\nimport edu.stanford.nlp.ling.HasWord;\nimport edu.stanford.nlp.ling.Sentence;\nimport edu.stanford.nlp.trees.*;\nimport edu.stanford.nlp.parser.lexparser.LexicalizedParser;\n\nclass ParserDemo {\n\n  public static void main(String[] args) {\n    LexicalizedParser lp = LexicalizedParser.loadModel(&quot;edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz&quot;);\n    if (args.length &gt; 0) {\n      demoDP(lp, args[0]);\n    } else {\n      demoAPI(lp);\n    }\n  }\n\n\n  public static void demoDP(LexicalizedParser lp, String filename) {\n\n    TreebankLanguagePack tlp = new PennTreebankLanguagePack();\n    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();\n\n    for (List&lt;HasWord&gt; sentence : new DocumentPreprocessor(filename)) {\n      Tree parse = lp.apply(sentence);\n      parse.pennPrint();\n      System.out.println();\n\n      GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);\n      Collection tdl = gs.typedDependenciesCCprocessed();\n      System.out.println(tdl);\n      System.out.println();\n    }\n  }\n\n  public static void demoAPI(LexicalizedParser lp) {\n    // This option shows parsing a list of correctly tokenized words\n    String[] sent = { &quot;This&quot;, &quot;is&quot;, &quot;an&quot;, &quot;easy&quot;, &quot;sentence&quot;, &quot;.&quot; };\n    List&lt;CoreLabel&gt; rawWords = Sentence.toCoreLabelList(sent);\n    Tree parse = lp.apply(rawWords);\n    parse.pennPrint();\n    System.out.println();\n\n    // This option shows loading and using an explicit tokenizer\n    String sent2 = &quot;This is another sentence.&quot;;\n    TokenizerFactory&lt;CoreLabel&gt; tokenizerFactory =\n        PTBTokenizer.factory(new CoreLabelTokenFactory(), &quot;&quot;);\n    Tokenizer&lt;CoreLabel&gt; tok =\n        tokenizerFactory.getTokenizer(new StringReader(sent2));\n    List&lt;CoreLabel&gt; rawWords2 = tok.tokenize();\n    parse = lp.apply(rawWords2);\n\n    TreebankLanguagePack tlp = new PennTreebankLanguagePack();\n    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();\n    GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);\n    List&lt;TypedDependency&gt; tdl = gs.typedDependenciesCCprocessed();\n    System.out.println(tdl);\n    System.out.println();\n\n    // You can also use a TreePrint object to print trees and dependencies\n    TreePrint tp = new TreePrint(&quot;penn,typedDependenciesCollapsed&quot;);\n    tp.printTree(parse);\n  }\n\n  private ParserDemo() {} // static methods only\n\n}\n</code></pre>\n<p>but when i run this code i get the following error</p>\n<pre><code>java.io.IOException: Unable to resolve &quot;edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz&quot; as either class path, filename or URL\n    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:446)\n    at edu.stanford.nlp.io.IOUtils.readStreamFromString(IOUtils.java:380)\n    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile(LexicalizedParser.java:628)\n    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:423)\n    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:182)\n    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:161)\n    at stanfordparser.ParserDemo.main(ParserDemo.java:29)\n</code></pre>\n<p>I think the problem in the loading of the model file,\nCould any one help me to solve the problem?\nThanks</p>\n<p>UPDATE:(1) I am already includes the cornlp model jar <br />\nUPDATE:(2) I am using Netbeans\n<img src=\"https://i.sstatic.net/nkiyd.png\" alt=\"enter image description here\" /></p>\n",
    "score": 6,
    "creation_date": 1421103409,
    "view_count": 6802,
    "answer_count": 3,
    "tags": "java;parsing;nlp;stanford-nlp"
  },
  {
    "question_id": 56642816,
    "title": "ValueError: [E024] Could not find an optimal move to supervise the parser",
    "body": "<p>I am getting the following error while training <code>spacy</code> NER model with my custom training data.</p>\n\n<pre><code>ValueError: [E024] Could not find an optimal move to supervise the parser. Usually, this means the GoldParse was not correct. For example, are all labels added to the model?\n</code></pre>\n\n<p>Can anyone help me with this?</p>\n",
    "score": 6,
    "creation_date": 1560838874,
    "view_count": 4960,
    "answer_count": 3,
    "tags": "python;python-3.x;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 45375488,
    "title": "How to filter tokens from spaCy document",
    "body": "<p>I would like to parse a document using spaCy and apply a token filter so that the final spaCy document does not include the filtered tokens. I know that I can take the sequence of tokens filtered, but I am insterested in having the actual <code>Doc</code> structure.  </p>\n\n<pre><code>text = u\"This document is only an example. \" \\\n    \"I would like to create a custom pipeline that will remove specific tokesn from the final document.\"\n\ndoc = nlp(text)\n\ndef keep_token(tok):\n    # This is only an example rule\n    return tok.pos_ not not in {'PUNCT', 'NUM', 'SYM'}\n\nfinal_tokens = list(filter(keep_token, doc))\n\n# How to get a spacy.Doc from final_tokens?\n</code></pre>\n\n<p>I tried to reconstruct a new spaCy <code>Doc</code> from the tokens lists but the API is not clear how to do it.</p>\n",
    "score": 6,
    "creation_date": 1501250558,
    "view_count": 14884,
    "answer_count": 2,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 33813405,
    "title": "concordance for a phrase using NLTK in Python",
    "body": "<p>Is it possible to get concordance for a phrase in NLTK?</p>\n\n<pre><code>import nltk\nfrom nltk.corpus import PlaintextCorpusReader\n\ncorpus_loc = \"c://temp//text//\"\nfiles = \".*\\.txt\"\nread_corpus = PlaintextCorpusReader(corpus_loc, files)\ncorpus  = nltk.Text(read_corpus.words())\ntest = nltk.TextCollection(corpus_loc)\n\ncorpus.concordance(\"claim\")\n</code></pre>\n\n<p>for example the above returns</p>\n\n<pre><code>on okay okay okay i can give you the claim number and my information and\n decide on the shop okay okay so the claim number is xxxx - xx - xxxx got\n</code></pre>\n\n<p>and now if I try <code>corpus.concordance(\"claim number\")</code> it does not work... I do have the code to do this with just by using <code>.partition()</code> method and some further coding on the same... but I'm wondering if it's possible to do the same using <code>concordance</code>.</p>\n",
    "score": 6,
    "creation_date": 1447963647,
    "view_count": 3090,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 31689621,
    "title": "How to Traverse an NLTK Tree object?",
    "body": "<p>Given a bracketed parse, I could convert it into a Tree object in NLTK as such:</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.tree import Tree\n&gt;&gt;&gt; s = '(ROOT (S (NP (NNP Europe)) (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends)))) (. .)))'\n&gt;&gt;&gt; Tree.fromstring(s)\nTree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Europe'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['same']), Tree('NNS', ['trends'])])])]), Tree('.', ['.'])])])\n</code></pre>\n\n<p>But when I try to traverse it, I can only access the top most Tree:</p>\n\n<pre><code>&gt;&gt;&gt; for i in Tree.fromstring(s):\n...     print i\n... \n(S\n  (NP (NNP Europe))\n  (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))\n  (. .))\n&gt;&gt;&gt; for i in Tree.fromstring(s):\n...     print i, i.label()\n... \n(S\n  (NP (NNP Europe))\n  (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))\n  (. .)) S\n&gt;&gt;&gt; \n</code></pre>\n\n<p>I could go one level deep as follows:</p>\n\n<pre><code>&gt;&gt;&gt; for i in Tree.fromstring(s):\n...     print i.subtrees()\n... \n&lt;generator object subtrees at 0x7f1eb1571410&gt;\n&gt;&gt;&gt; for i in Tree.fromstring(s):\n...     for j in i.subtrees():\n...             print j\n... \n(S\n  (NP (NNP Europe))\n  (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))\n  (. .))\n(NP (NNP Europe))\n(NNP Europe)\n(VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))\n(VBZ is)\n(PP (IN in) (NP (DT the) (JJ same) (NNS trends)))\n(IN in)\n(NP (DT the) (JJ same) (NNS trends))\n(DT the)\n(JJ same)\n(NNS trends)\n(. .)\n</code></pre>\n\n<p>But is there a way to traverse all subtrees depth wise?</p>\n\n<p><strong>How should one traverse a tree in NLTK?</strong></p>\n\n<p><strong>How to traverse all subtrees in NLTK?</strong></p>\n",
    "score": 6,
    "creation_date": 1438131963,
    "view_count": 11824,
    "answer_count": 1,
    "tags": "parsing;tree;nlp;nltk;depth-first-search"
  },
  {
    "question_id": 26474731,
    "title": "Missing Spanish wordnet from NLTK",
    "body": "<p>I am trying to use the Spanish Wordnet from the Open Multilingual Wordnet in NLTK 3.0, but it seems that it was not downloaded with the 'omw' package. For example, with a code like the following:</p>\n\n<pre><code>from nltk.corpus import wordnet as wn\n\nprint [el.lemma_names('spa') for el in wn.synsets('bank')]\n</code></pre>\n\n<p>I get the following error message:</p>\n\n<pre><code>IOError: No such file or directory: u'***/nltk_data/corpora/omw/spa/wn-data-spa.tab'\n</code></pre>\n\n<p>According to the <a href=\"http://www.nltk.org/howto/wordnet.html\" rel=\"nofollow\">documentation</a>, Spanish should be included, in the 'omw' package, but it was not downloaded with it. Do you know why this could happen?</p>\n",
    "score": 6,
    "creation_date": 1413838399,
    "view_count": 7776,
    "answer_count": 3,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 20983494,
    "title": "Python and NLTK: How to analyze sentence grammar?",
    "body": "<p>I have this code which should show the syntactic structure of the sentence according to defined grammar. However it is returning an empty []. What am I missing or doing wrong?</p>\n\n<pre><code>import nltk\n\ngrammar = nltk.parse_cfg(\"\"\"\nS -&gt; NP VP \nPP -&gt; P NP\nNP -&gt; Det N | Det N PP \nVP -&gt; V NP | VP PP\nN -&gt; 'Kim' | 'Dana' | 'everyone'\nV -&gt; 'arrived' | 'left' |'cheered'\nP -&gt; 'or' | 'and'\n\"\"\")\n\ndef main():\n    sent = \"Kim arrived or Dana left and everyone cheered\".split()\n    parser = nltk.ChartParser(grammar)\n    trees = parser.nbest_parse(sent)\n    for tree in trees:\n        print tree\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n",
    "score": 6,
    "creation_date": 1389134456,
    "view_count": 7977,
    "answer_count": 2,
    "tags": "python-2.7;tree;nlp;nltk"
  },
  {
    "question_id": 66625389,
    "title": "AttributeError: &#39;list&#39; object has no attribute &#39;size&#39; Hugging-Face transformers",
    "body": "<p>I am trying to use Huggingface to transform stuff from English to Hindi.\nThis is the code snippet</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-hi&quot;)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-hi&quot;)\ntext = &quot;Hello my friends! How are you doing today?&quot;\ntokenized_text = tokenizer.prepare_seq2seq_batch([text])\n\n# Perform translation and decode the output\ntranslation = model.generate(**tokenized_text)\ntranslated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\n\n# Print translated text\nprint(translated_text)\n</code></pre>\n<p>I am getting this error while trying to call the method generate on 'model'.</p>\n<blockquote>\n<p>AttributeError: 'list' object has no attribute 'size'.</p>\n</blockquote>\n<p>I am running on transformer version 4.3.3.</p>\n",
    "score": 6,
    "creation_date": 1615730428,
    "view_count": 13841,
    "answer_count": 1,
    "tags": "python-3.x;nlp;huggingface-transformers"
  },
  {
    "question_id": 34037094,
    "title": "Setting NLTK with Stanford NLP (both StanfordNERTagger and StanfordPOSTagger) for Spanish",
    "body": "<p>The <code>NLTK</code> documentation is rather poor in this integration. The steps I <a href=\"https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#stanford-tagger-ner-tokenizer-and-parser\" rel=\"noreferrer\">followed</a> were:</p>\n\n<ul>\n<li><p>Download <a href=\"http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\" rel=\"noreferrer\">http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip</a> to <code>/home/me/stanford</code></p></li>\n<li><p>Download <a href=\"http://nlp.stanford.edu/software/stanford-spanish-corenlp-2015-01-08-models.jar\" rel=\"noreferrer\">http://nlp.stanford.edu/software/stanford-spanish-corenlp-2015-01-08-models.jar</a> to <code>/home/me/stanford</code></p></li>\n</ul>\n\n<p>Then in a <code>ipython</code> console:</p>\n\n<p>In [11]: import nltk                                                                                                                                                                 </p>\n\n<pre><code>In [12]: nltk.__version__\nOut[12]: '3.1'\n\nIn [13]: from nltk.tag import StanfordNERTagger\n</code></pre>\n\n<p>Then</p>\n\n<pre><code>st = StanfordNERTagger('/home/me/stanford/stanford-postagger-full-2015-04-20.zip', '/home/me/stanford/stanford-spanish-corenlp-2015-01-08-models.jar')\n</code></pre>\n\n<p>But when I tried to run it:</p>\n\n<pre><code>st.tag('Adolfo se la pasa corriendo'.split())\nError: no se ha encontrado o cargado la clase principal edu.stanford.nlp.ie.crf.CRFClassifier\n\n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\n&lt;ipython-input-14-0c1a96b480a6&gt; in &lt;module&gt;()\n----&gt; 1 st.tag('Adolfo se la pasa corriendo'.split())\n\n/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/nltk/tag/stanford.py in tag(self, tokens)\n     64     def tag(self, tokens):\n     65         # This function should return list of tuple rather than list of list\n---&gt; 66         return sum(self.tag_sents([tokens]), [])\n     67 \n     68     def tag_sents(self, sentences):\n\n/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/nltk/tag/stanford.py in tag_sents(self, sentences)\n     87         # Run the tagger and get the output\n     88         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n---&gt; 89                                                        stdout=PIPE, stderr=PIPE)\n     90         stanpos_output = stanpos_output.decode(encoding)\n     91 \n\n/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/nltk/__init__.py in java(cmd, classpath, stdin, stdout, stderr, blocking)\n    132     if p.returncode != 0:\n    133         print(_decode_stdoutdata(stderr))\n--&gt; 134         raise OSError('Java command failed : ' + str(cmd))\n    135 \n    136     return (stdout, stderr)\n\nOSError: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', '/home/nanounanue/Descargas/stanford-spanish-corenlp-2015-01-08-models.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/home/nanounanue/Descargas/stanford-postagger-full-2015-04-20.zip', '-textFile', '/tmp/tmp6y169div', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '\"tokenizeNLs=false\"', '-encoding', 'utf8']\n</code></pre>\n\n<p>The same occur with the <code>StandfordPOSTagger</code></p>\n\n<p><strong>NOTE</strong>: I need that this will be the spanish version.\n<strong>NOTE</strong>: I  am running this in <code>python 3.4.3</code></p>\n",
    "score": 6,
    "creation_date": 1449041315,
    "view_count": 12662,
    "answer_count": 3,
    "tags": "python;python-3.x;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 31438822,
    "title": "How to implement Word2Vec in Java?",
    "body": "<p>I installed word2Vec <a href=\"http://textminingonline.com/getting-started-with-word2vec-and-glove\" rel=\"noreferrer\">using this tutorial</a> on by Ubuntu laptop. Is it completely necessary to install <a href=\"http://deeplearning4j.org\" rel=\"noreferrer\">DL4J</a> in order to implement word2Vec vectors in Java? I'm comfortable working in Eclipse and I'm not sure that I want all the other pre-requisites that DL4J wants me to install. </p>\n\n<p>Ideally there would be a really easy way for me to just use the Java code I've already written (in Eclipse) and change a few lines -- so that word look-ups that I am doing would retrieve a word2Vec vector instead of the current retrieval process I'm using.</p>\n\n<hr>\n\n<p>Also, I've looked into using GloVe, however, I do not have MatLab. Is it possible to use GloVe without MatLab? (I got an error while installing it because of this). If so, the same question as above goes... I have no idea how to implement it in Java.</p>\n",
    "score": 6,
    "creation_date": 1436986673,
    "view_count": 12911,
    "answer_count": 2,
    "tags": "java;machine-learning;nlp;artificial-intelligence;word2vec"
  },
  {
    "question_id": 32073018,
    "title": "NER model to recognize Indian names",
    "body": "<p>I am planning to use Named Entity Recognition (NER) technique to identify person names (most of which are Indian names) from a given text. I have already explored the CRF-based NER model from Stanford NLP, however it is not quite accurate in recognizing Indian names. Hence I decided to create my own custom NER model via supervised training. I have a fair idea of how to create own NER model using the Stanford NER CRF, but creating a large training corpus with manual annotation is something I would like to avoid, as it is a humongous effort for an individual and secondly obtaining diverse people names from different states of India is also a challenge. Could anybody suggest any automation/programmatic way to prepare a labelled training corpus with at least 100k Indian names?<br>\nI have already looked into Facebook and LinkedIn API, but did not find a way to extract 100k number of user's full name from a given location (e.g. India).</p>\n",
    "score": 6,
    "creation_date": 1439902430,
    "view_count": 11004,
    "answer_count": 3,
    "tags": "facebook-graph-api;nlp;stanford-nlp;named-entity-recognition;linkedin-api"
  },
  {
    "question_id": 69517460,
    "title": "BERT get sentence embedding",
    "body": "<p>I am replicating code from <a href=\"https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX\" rel=\"noreferrer\">this page</a>. I have downloaded the BERT model to my local system and getting sentence embedding.</p>\n<p>I have around 500,000 sentences for which I need sentence embedding and it is taking a lot of time.</p>\n<ol>\n<li>Is there a way to expedite the process?</li>\n<li>Would sending batches of sentences rather than one sentence at a time help?</li>\n</ol>\n<p>.</p>\n<pre><code>#!pip install transformers\nimport torch\nimport transformers\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased',\n                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n                                  )\n\n# Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation.\nmodel.eval()\n\ncorpa=[&quot;i am a boy&quot;,&quot;i live in a city&quot;]\n\n\n\nstorage=[]#list to store all embeddings\n\nfor text in corpa:\n    # Add the special tokens.\n    marked_text = &quot;[CLS] &quot; + text + &quot; [SEP]&quot;\n\n    # Split the sentence into tokens.\n    tokenized_text = tokenizer.tokenize(marked_text)\n\n    # Map the token strings to their vocabulary indeces.\n    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n\n    segments_ids = [1] * len(tokenized_text)\n\n    tokens_tensor = torch.tensor([indexed_tokens])\n    segments_tensors = torch.tensor([segments_ids])\n\n    # Run the text through BERT, and collect all of the hidden states produced\n    # from all 12 layers. \n    with torch.no_grad():\n\n        outputs = model(tokens_tensor, segments_tensors)\n\n        # Evaluating the model will return a different number of objects based on \n        # how it's  configured in the `from_pretrained` call earlier. In this case, \n        # becase we set `output_hidden_states = True`, the third item will be the \n        # hidden states from all layers. See the documentation for more details:\n        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n        hidden_states = outputs[2]\n\n\n    # `hidden_states` has shape [13 x 1 x 22 x 768]\n\n    # `token_vecs` is a tensor with shape [22 x 768]\n    token_vecs = hidden_states[-2][0]\n\n    # Calculate the average of all 22 token vectors.\n    sentence_embedding = torch.mean(token_vecs, dim=0)\n\n    storage.append((text,sentence_embedding))\n</code></pre>\n<p>######update 1</p>\n<p>I modified my code based upon the answer provided. It is not doing full batch processing</p>\n<pre><code>#!pip install transformers\nimport torch\nimport transformers\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased',\n                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n                                  )\n\n# Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation.\nmodel.eval()\n\nbatch_sentences = [&quot;Hello I'm a single sentence&quot;,\n                    &quot;And another sentence&quot;,\n                    &quot;And the very very last one&quot;]\nencoded_inputs = tokenizer(batch_sentences)\n\n\nstorage=[]#list to store all embeddings\nfor i,text in enumerate(encoded_inputs['input_ids']):\n    \n    tokens_tensor = torch.tensor([encoded_inputs['input_ids'][i]])\n    segments_tensors = torch.tensor([encoded_inputs['attention_mask'][i]])\n    print (tokens_tensor)\n    print (segments_tensors)\n\n    # Run the text through BERT, and collect all of the hidden states produced\n    # from all 12 layers. \n    with torch.no_grad():\n\n        outputs = model(tokens_tensor, segments_tensors)\n\n        # Evaluating the model will return a different number of objects based on \n        # how it's  configured in the `from_pretrained` call earlier. In this case, \n        # becase we set `output_hidden_states = True`, the third item will be the \n        # hidden states from all layers. See the documentation for more details:\n        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n        hidden_states = outputs[2]\n\n\n    # `hidden_states` has shape [13 x 1 x 22 x 768]\n\n    # `token_vecs` is a tensor with shape [22 x 768]\n    token_vecs = hidden_states[-2][0]\n\n    # Calculate the average of all 22 token vectors.\n    sentence_embedding = torch.mean(token_vecs, dim=0)\n    print (sentence_embedding[:10])\n    storage.append((text,sentence_embedding))\n</code></pre>\n<p>I could update first 2 lines from the for loop to below. But they work only if all sentences have same length after tokenization</p>\n<pre><code>tokens_tensor = torch.tensor([encoded_inputs['input_ids']])\nsegments_tensors = torch.tensor([encoded_inputs['attention_mask']])\n</code></pre>\n<p>moreover in that case <code>outputs = model(tokens_tensor, segments_tensors) </code> fails.</p>\n<p>How could I fully perform batch processing in such case?</p>\n",
    "score": 6,
    "creation_date": 1633887151,
    "view_count": 12560,
    "answer_count": 2,
    "tags": "python;nlp;huggingface-transformers;bert-language-model;huggingface-tokenizers"
  },
  {
    "question_id": 67573416,
    "title": "Unable to recreate Gensim docs for training FastText. TypeError: Either one of corpus_file or corpus_iterable value must be provided",
    "body": "<p>I am trying to make my own Fasttext embeddings so I went to official Gensim documentation and <a href=\"https://radimrehurek.com/gensim/models/fasttext.html\" rel=\"noreferrer\">implemented this exact code below</a> with exact <code>4.0</code> version.</p>\n<pre><code>from gensim.models import FastText\nfrom gensim.test.utils import common_texts\n\nmodel = FastText(vector_size=4, window=3, min_count=1)  # instantiate\nmodel.build_vocab(sentences=common_texts)\nmodel.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)\n</code></pre>\n<p>And to my surprise it is giving me errors as:</p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-4-6b2d1de02d90&gt; in &lt;module&gt;\n      1 model = FastText(vector_size=4, window=3, min_count=1)  # instantiate\n----&gt; 2 model.build_vocab(sentences=common_texts)\n      3 model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)\n\n~/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py in build_vocab(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\n    477 \n    478         &quot;&quot;&quot;\n--&gt; 479         self._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=1)\n    480         total_words, corpus_count = self.scan_vocab(\n    481             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\n~/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py in _check_corpus_sanity(self, corpus_iterable, corpus_file, passes)\n   1484         &quot;&quot;&quot;Checks whether the corpus parameters make sense.&quot;&quot;&quot;\n   1485         if corpus_file is None and corpus_iterable is None:\n-&gt; 1486             raise TypeError(&quot;Either one of corpus_file or corpus_iterable value must be provided&quot;)\n   1487         if corpus_file is not None and corpus_iterable is not None:\n   1488             raise TypeError(&quot;Both corpus_file and corpus_iterable must not be provided at the same time&quot;)\n\nTypeError: Either one of corpus_file or corpus_iterable value must be provided\n</code></pre>\n<p>Can someone please help what is happening here?</p>\n",
    "score": 6,
    "creation_date": 1621268010,
    "view_count": 2874,
    "answer_count": 1,
    "tags": "python;nlp;gensim;fasttext"
  },
  {
    "question_id": 4207057,
    "title": "How to include words as numerical feature in classification",
    "body": "<p>Whats the best method to use the words itself as the features in any machine learning algorithm ? </p>\n\n<p>The problem I have to extract word related feature from a particular paragraph. Should I use the index in the dictionary as the numerical feature ? If so, how will I normalize these ? </p>\n\n<p>In general, How are words itself used as features in NLP ? </p>\n",
    "score": 6,
    "creation_date": 1290013420,
    "view_count": 2483,
    "answer_count": 3,
    "tags": "machine-learning;nlp;classification;document-classification"
  },
  {
    "question_id": 605388,
    "title": "Natural language date parser for ruby/rails",
    "body": "<p>Does anybody know of something similar to <a href=\"http://www.datejs.com/\" rel=\"nofollow noreferrer\">Date.js</a> in Ruby? Something that would be able to return a date object from something like: \"two weeks from today\". The Remember the Milk webapp incorporates this feature into their system and it is incredibly easy to use.</p>\n\n<p>I would use the Date.js library itself but because it is on the client side it has its limitations. If the user doesn't have javascript enabled the functionality would be lost. This would affect mobile phone users who would, ideally, use our system via text message (sms).</p>\n\n<p>I would love to use a solution that's already out there but if not how hard would it be to port this code into Ruby? I really don't know much about natural language interpretation but it seems like it would take some time.</p>\n\n<p>Thanks.</p>\n",
    "score": 6,
    "creation_date": 1236063350,
    "view_count": 1833,
    "answer_count": 1,
    "tags": "ruby-on-rails;ruby;datetime;nlp"
  },
  {
    "question_id": 63144230,
    "title": "Proper way to add new vectors for OOV words",
    "body": "<p>I'm using some domain-specific language which have a lot of OOV words as well as some typos. I have noticed Spacy will just assign an all-zero vector for these OOV words, so I'm wondering what's the proper way to handle this. I appreciate clarification on all of these points if possible:</p>\n<ol>\n<li>What exactly does the pre-train command do? Honestly I cannot seem to parse correctly the explanation from the website:</li>\n</ol>\n<blockquote>\n<p>Pre-train the “token to vector” (tok2vec) layer of pipeline components, using an approximate language-modeling objective. Specifically, we load pretrained vectors, and train a component like a CNN, BiLSTM, etc to predict vectors which match the pretrained ones</p>\n</blockquote>\n<p>Isn't the tok2vec the part that generates the vectors? So shouldn't this command then change the produced vectors?\nWhat does it mean loading pretrained vectors and then train a component to predict these vectors? What's the purpose of doing this?</p>\n<p>What does the --use-vectors flag do?\nWhat does the --init-tok2vec flag do? Is this included by mistake in the documentation?</p>\n<ol start=\"2\">\n<li><p>It seems pretrain is not what I'm looking for, it doesn't change the vectors for a given word. What would be the easiest way to generate a new set of vectors which includes my OOV words but still contain the general knowledge of the lanaguage?</p>\n</li>\n<li><p>As far as I can see Spacy's pretrained models use fasttext vectors. Fasttext website mentions:</p>\n</li>\n</ol>\n<blockquote>\n<p>A nice feature is that you can also query for words that did not appear in your data! Indeed words are represented by the sum of its substrings. As long as the unknown word is made of known substrings, there is a representation of it!</p>\n</blockquote>\n<p>But it seems Spacy does not use this feature. Is there a way to still make use of this for OOV words?</p>\n<p>Thanks a lot</p>\n",
    "score": 6,
    "creation_date": 1595978928,
    "view_count": 3665,
    "answer_count": 1,
    "tags": "python;nlp;spacy;fasttext"
  },
  {
    "question_id": 59413960,
    "title": "How to compare two strings by meaning?",
    "body": "<p>I want the user of my node.js application to write down ideas, which then get stored in a database.\nSo far so good, but I don't want redundant entrys in that table, so I decided to check for similarity, using this one:\n<a href=\"https://www.npmjs.com/package/string-similarity-js\" rel=\"nofollow noreferrer\">https://www.npmjs.com/package/string-similarity-js</a></p>\n\n<p>Do you know a way, in which I can compare two strings by meaning? In like getting a high similarity score for \"using public transport\" vs \"driving by train\" which performs very poor in the above one.</p>\n",
    "score": 6,
    "creation_date": 1576774464,
    "view_count": 3717,
    "answer_count": 3,
    "tags": "javascript;node.js;nlp;ibm-watson;tensorflow.js"
  },
  {
    "question_id": 56527814,
    "title": "Stanford typed dependencies using coreNLP in python",
    "body": "<p>In <a href=\"https://nlp.stanford.edu/software/dependencies_manual.pdf\" rel=\"noreferrer\">Stanford Dependency Manual</a> they mention  \"Stanford typed dependencies\" and particularly the type \"neg\" - negation  modifier. It is also available when using Stanford enhanced++ parser using the website. for example, the sentence: </p>\n\n<blockquote>\n  <p>\"Barack Obama was not born in Hawaii\"</p>\n</blockquote>\n\n<p><a href=\"https://i.sstatic.net/dEo0q.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/dEo0q.png\" alt=\"enter image description here\"></a> </p>\n\n<p>The parser indeed find neg(born,not) </p>\n\n<p>but when I'm using the <code>stanfordnlp</code> python library, the only dependency parser I can get will parse the sentence as  follow:</p>\n\n<pre><code>('Barack', '5', 'nsubj:pass')\n\n('Obama', '1', 'flat')\n\n('was', '5', 'aux:pass')\n\n('not', '5', 'advmod')\n\n('born', '0', 'root')\n\n('in', '7', 'case')\n\n('Hawaii', '5', 'obl')\n</code></pre>\n\n<p>and the code that generates it: </p>\n\n<pre><code>import stanfordnlp\nstanfordnlp.download('en')  \nnlp = stanfordnlp.Pipeline()\ndoc = nlp(\"Barack Obama was not born in Hawaii\")\na  = doc.sentences[0]\na.print_dependencies()\n</code></pre>\n\n<p>Is there a way to get similar results to the enhanced dependency parser or any other Stanford parser that result in typed dependencies that will give me the negation modifier?</p>\n",
    "score": 6,
    "creation_date": 1560174895,
    "view_count": 3892,
    "answer_count": 5,
    "tags": "python;parsing;nlp;stanford-nlp"
  },
  {
    "question_id": 51658153,
    "title": "Lemmatize a doc with spacy?",
    "body": "<p>I have a spaCy <code>doc</code> that I would like to lemmatize.</p>\n\n<p>For example:</p>\n\n<pre><code>import spacy\nnlp = spacy.load('en_core_web_lg')\n\nmy_str = 'Python is the greatest language in the world'\ndoc = nlp(my_str)\n</code></pre>\n\n<p>How can I convert every token in the <code>doc</code> to its lemma?</p>\n",
    "score": 6,
    "creation_date": 1533226572,
    "view_count": 10812,
    "answer_count": 3,
    "tags": "python;nlp;spacy;lemmatization"
  },
  {
    "question_id": 41210384,
    "title": "What is Two-Level Morphology?",
    "body": "<p>In Natural Language Processing what are the two levels of this two-level Morphology framework ?</p>\n",
    "score": 6,
    "creation_date": 1482078228,
    "view_count": 2561,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 18705778,
    "title": "What is the use of Brown Corpus in measuring Semantic Similarity based on WordNet",
    "body": "<p>I came across several methods for measuring semantic similarity that use the structure and hierarchy of WordNet, e.g. Jiang and Conrath measure (JNC), Resnik measure(RES), Lin measure (LIN) etc.</p>\n\n<p>The way they are measured using NLTK is:</p>\n\n<pre><code>sim2=wn.jcn_similarity(entry1,entry2,brown_ic)\nsim3=entry1.res_similarity(entry2, brown_ic)\nsim4=entry1.lin_similarity(entry2,brown_ic)\n</code></pre>\n\n<p>If WordNet is the basis of calculating semantic similarity, what is the use of Brown Corpus here?</p>\n",
    "score": 6,
    "creation_date": 1378755957,
    "view_count": 3683,
    "answer_count": 2,
    "tags": "nlp;similarity;wordnet;corpus;semantic-analysis"
  },
  {
    "question_id": 76729793,
    "title": "how to specify similarity threshold in langchain faiss retriever?",
    "body": "<p>I would like to pass to the retriever a similarity threshold. So far I could only figure out how to pass a k value but this was not what I wanted. How can I pass a threshold instead?</p>\n<pre><code>from langchain.document_loaders import PyPDFLoader\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\ndef get_conversation_chain(vectorstore):\n    llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n    qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever(search_kwargs={'k': 2}), return_source_documents=True, verbose=True)\n    return qa\n\nloader = PyPDFLoader(&quot;sample.pdf&quot;)\n# get pdf raw text\npages = loader.load_and_split()\nfaiss_index = FAISS.from_documents(list_of_documents, OpenAIEmbeddings())\n# create conversation chain\nchat_history = []\nqa = get_conversation_chain(faiss_index)\nquery = &quot;What is a sunflower?&quot;\nresult = qa({&quot;question&quot;: query, &quot;chat_history&quot;: chat_history}) \n</code></pre>\n",
    "score": 6,
    "creation_date": 1689855502,
    "view_count": 13616,
    "answer_count": 2,
    "tags": "python;nlp;openai-api;langchain;large-language-model"
  },
  {
    "question_id": 63026282,
    "title": "Error &#39;power iteration failed to converge within 100 iterations&#39;) when I tried to summarize a text document using python networkx",
    "body": "<p>I got an PowerIterationFailedConvergence:(PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations') when I tried to summarize a text document using python networkx as shown in the code below. the error shown at the code &quot;scores = nx.pagerank(sentence_similarity_graph)&quot;</p>\n<pre><code>def read_article(file_name):\n    file = open(file_name, &quot;r&quot;,encoding=&quot;utf8&quot;)\n    filedata = file.readlines()\n    text=&quot;&quot;\n    for s in filedata:\n        text=text+s.replace(&quot;\\n&quot;,&quot;&quot;)\n        text=re.sub(' +', ' ', text) #remove space\n        text=re.sub('—',' ',text)\n    \n    article = text.split(&quot;. &quot;) \n    sentences = []\n    for sentence in article:\n#         print(sentence)\n        sentences.append(sentence.replace(&quot;[^a-zA-Z]&quot;, &quot;&quot;).split(&quot; &quot;))\n    sentences.pop()\n    new_sent=[]\n    for lst in sentences:\n        newlst=[]\n        for i in range(len(lst)):\n            if lst[i].lower()!=lst[i-1].lower():\n                newlst.append(lst[i])\n            else:\n                newlst=newlst\n        new_sent.append(newlst)\n    return new_sent\ndef sentence_similarity(sent1, sent2, stopwords=None):\n    if stopwords is None:\n        stopwords = []\n \n    sent1 = [w.lower() for w in sent1]\n    sent2 = [w.lower() for w in sent2]\n \n    all_words = list(set(sent1 + sent2))\n \n    vector1 = [0] * len(all_words)\n    vector2 = [0] * len(all_words)\n \n    # build the vector for the first sentence\n    for w in sent1:\n        if w in stopwords:\n            continue\n        vector1[all_words.index(w)] += 1\n \n    # build the vector for the second sentence\n    for w in sent2:\n        if w in stopwords:\n            continue\n        vector2[all_words.index(w)] += 1\n \n    return 1 - cosine_distance(vector1, vector2)\ndef build_similarity_matrix(sentences, stop_words):\n    # Create an empty similarity matrix\n    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n \n    for idx1 in range(len(sentences)):\n        for idx2 in range(len(sentences)):\n            if idx1 == idx2: #ignore if both are same sentences\n                continue \n            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n\n    return similarity_matrix\nstop_words = stopwords.words('english')\nsummarize_text = []\n\n    # Step 1 - Read text anc split it\nnew_sent =  read_article(&quot;C:\\\\Users\\\\Documents\\\\fedPressConference_0620.txt&quot;)\n\n    # Step 2 - Generate Similary Martix across sentences\nsentence_similarity_martix = build_similarity_matrix(new_sent1, stop_words)\n\n    # Step 3 - Rank sentences in similarity martix\nsentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\nscores = nx.pagerank(sentence_similarity_graph)\n\n    # Step 4 - Sort the rank and pick top sentences\nranked_sentence = sorted(((scores[i],s) for i,s in enumerate(new_sent1)), reverse=True)    \nprint(&quot;Indexes of top ranked_sentence order are &quot;, ranked_sentence)    \n\nfor i in range(10):\n    summarize_text.append(&quot; &quot;.join(ranked_sentence[i][1]))\n\n    # Step 5 - Offcourse, output the summarize texr\nprint(&quot;Summarize Text: \\n&quot;, &quot;. &quot;.join(summarize_text))\n\n</code></pre>\n",
    "score": 6,
    "creation_date": 1595387861,
    "view_count": 11077,
    "answer_count": 3,
    "tags": "python;nlp;networkx"
  },
  {
    "question_id": 58197863,
    "title": "How to get sentence number in spaCy?",
    "body": "<p>I get the tokens for a string as</p>\n\n<pre><code>doc = nlp(u\"This is the first sentence. This is the second sentence.\")\nfor token in doc:\n    print(token.i, token.text)\n</code></pre>\n\n<p>with the output</p>\n\n<pre><code>0 This\n1 is\n2 the\n3 first\n4 sentence\n5 .\n6 This\n7 is\n8 the\n9 second\n10 sentence\n11 .\n</code></pre>\n\n<p>How can I get the sentence number as <code>(SENTENCE_NUMBER, token.i, token.text)</code></p>\n\n<pre><code>0 0 This\n0 1 is\n0 2 the\n0 3 first\n0 4 sentence\n0 5 .\n1 0 This\n1 1 is\n1 2 the\n1 3 second\n1 4 sentence\n1 5 .\n</code></pre>\n\n<p>I can reset the token number in the loop, but how can I get the sentence number from <code>doc</code>?</p>\n",
    "score": 6,
    "creation_date": 1570004803,
    "view_count": 6836,
    "answer_count": 2,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 51934474,
    "title": "Find the similarity between two string columns of a DataFrame",
    "body": "<p>I am new to programming.I have a pandas data frame in which two string columns are present.</p>\n\n<p>Data frame is like below:</p>\n\n<pre><code>Col-1             Col-2\nUpdate            have a account\nAccount           account summary\nAccountDTH        Cancel\nBalance           Balance Summary\nCredit Card       Update credit card\n</code></pre>\n\n<p>Here i  need to check the similarity of Col-2 elements with each element of Col-1.\nIt Means i have to compare <code>have a account</code> with all the elements of <code>Col-1</code>.\nThen find the top 3 similar one. Suppose the similarity scores are :<code>Account(85),AccountDTH(80),Balance(60),Update(45),Credit Card(35)</code>.</p>\n\n<p>Expected Output is:</p>\n\n<pre><code>Col-2              Output\nhave a account     Account(85),AccountDTH(80),Balance(60)\n</code></pre>\n",
    "score": 6,
    "creation_date": 1534780671,
    "view_count": 6063,
    "answer_count": 1,
    "tags": "python;string;pandas;nlp;similarity"
  },
  {
    "question_id": 51214026,
    "title": "Person Name Detection using SpaCy in English Lang. Looking for Answer",
    "body": "<p>I am using Spacy and trying to detect names in the text. For example, text = 'Keras is a good package. Adam Smith uses a car of black colour. I hope Katrina is doing well in her job.'</p>\n\n<p>The answer should like this: Adam Smith and Katrina. </p>\n\n<p>Can anyone recommend </p>\n",
    "score": 6,
    "creation_date": 1530892753,
    "view_count": 25453,
    "answer_count": 2,
    "tags": "python;nlp;nltk;spacy;named-entity-recognition"
  },
  {
    "question_id": 26834576,
    "title": "Big Text Corpus breaks tm_map",
    "body": "<p>I have been breaking my head over this one over the last few days. I searched all the SO archives and tried the suggested solutions but just can't seem to get this to work. I have sets of txt documents in folders such as 2000 06, 1995 -99 etc, and want to run some basic text mining operations such as creating document term matrix and term document matrix and doing some operations based co-locations of words. My script works on a smaller corpus, however, when I try it with the bigger corpus, it fails me. I have pasted in the code for one such folder operation.</p>\n\n<pre><code>library(tm) # Framework for text mining.\nlibrary(SnowballC) # Provides wordStem() for stemming.\nlibrary(RColorBrewer) # Generate palette of colours for plots.\nlibrary(ggplot2) # Plot word frequencies.\nlibrary(magrittr)\nlibrary(Rgraphviz)\nlibrary(directlabels)\n\nsetwd(\"/ConvertedText\")\ntxt &lt;- file.path(\"2000 -06\")\n\ndocs&lt;-VCorpus(DirSource(txt, encoding = \"UTF-8\"),readerControl = list(language = \"UTF-8\"))\ndocs &lt;- tm_map(docs, content_transformer(tolower), mc.cores=1)\ndocs &lt;- tm_map(docs, removeNumbers, mc.cores=1)\ndocs &lt;- tm_map(docs, removePunctuation, mc.cores=1)\ndocs &lt;- tm_map(docs, stripWhitespace, mc.cores=1)\ndocs &lt;- tm_map(docs, removeWords, stopwords(\"SMART\"), mc.cores=1)\ndocs &lt;- tm_map(docs, removeWords, stopwords(\"en\"), mc.cores=1)\n#corpus creation complete\n\nsetwd(\"/ConvertedText/output\")\ndtm&lt;-DocumentTermMatrix(docs)\ntdm&lt;-TermDocumentMatrix(docs)\nm&lt;-as.matrix(dtm)\nwrite.csv(m, file=\"dtm.csv\")\ndtms&lt;-removeSparseTerms(dtm, 0.2)\nm1&lt;-as.matrix(dtms)\nwrite.csv(m1, file=\"dtms.csv\")\n# matrix creation/storage complete\n\nfreq &lt;- sort(colSums(as.matrix(dtm)), decreasing=TRUE)\nwf &lt;- data.frame(word=names(freq), freq=freq)\nfreq[1:50]\n#adjust freq score in next line\np &lt;- ggplot(subset(wf, freq&gt;100), aes(word, freq))+ geom_bar(stat=\"identity\")+ theme(axis.text.x=element_text(angle=45, hjust=1))\nggsave(\"frequency2000-06.png\", height=12,width=17, dpi=72)\n# frequency graph generated\n\n\nx&lt;-as.matrix(findFreqTerms(dtm, lowfreq=1000))\nwrite.csv(x, file=\"freqterms00-06.csv\")\npng(\"correlation2000-06.png\", width=12, height=12, units=\"in\", res=900)\ngraph.par(list(edges=list(col=\"lightblue\", lty=\"solid\", lwd=0.3)))\ngraph.par(list(nodes=list(col=\"darkgreen\", lty=\"dotted\", lwd=2, fontsize=50)))\nplot(dtm, terms=findFreqTerms(dtm, lowfreq=1000)[1:50],corThreshold=0.7)\ndev.off()\n</code></pre>\n\n<p>When I use the mc.cores=1 argument in tm_map, the operation continues indefinitely. However, if I use the lazy=TRUE argument in tm_map, it seemingly goes well, but subsequent operations give this error. </p>\n\n<pre><code>Error in UseMethod(\"meta\", x) : \n  no applicable method for 'meta' applied to an object of class \"try-error\"\nIn addition: Warning messages:\n1: In mclapply(x$content[i], function(d) tm_reduce(d, x$lazy$maps)) :\n  all scheduled cores encountered errors in user code\n2: In mclapply(unname(content(x)), termFreq, control) :\n  all scheduled cores encountered errors in user code\n</code></pre>\n\n<p>I have been looking all over for a solution but have failed consistently. Any help would be greatly appreciated!</p>\n\n<p>Best!\nk</p>\n",
    "score": 6,
    "creation_date": 1415575815,
    "view_count": 6807,
    "answer_count": 1,
    "tags": "r;text-mining;tm;text-analysis;term-document-matrix"
  },
  {
    "question_id": 23634759,
    "title": "How to create the negative of a sentence in nltk",
    "body": "<p>I am new to NLTK. I would like to create the negative of a sentence (which will usually be in the present tense). For example, is there a function to allow me to convert:\n 'I run' to 'I do not run'</p>\n\n<p>or</p>\n\n<p>'She runs' to 'She does not run'.</p>\n\n<p>I suppose I could use POS to detect the verb and its preceding pronoun but I just wondered if there was a simpler built in function</p>\n",
    "score": 6,
    "creation_date": 1399993007,
    "view_count": 2727,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 10223314,
    "title": "Using Sentiwordnet 3.0",
    "body": "<p>I plan on using Sentiwordnet 3.0 for Sentiment classification. Could someone clarify as to what the numbers associated with words in Sentiwordnet represent? For e.g. what does 5 in rank#5 mean? Also for POS what is the letter used to represent adverbs? Im assuming 'a' is adjectives. I could not find an explanation either on their site or on other sites.</p>\n",
    "score": 6,
    "creation_date": 1334819854,
    "view_count": 6936,
    "answer_count": 1,
    "tags": "machine-learning;nlp;wordnet;sentiment-analysis;senti-wordnet"
  },
  {
    "question_id": 9505714,
    "title": "Python: How to prepend the string &#39;ub&#39; to every pronounced vowel in a string?",
    "body": "<p><strong>Example</strong>: Speak -> Spubeak, <a href=\"http://en.wikipedia.org/wiki/Ubbi_dubbi\" rel=\"nofollow\">more info here</a></p>\n\n<p>Don't give me a solution, but point me in the right direction or tell which which python library I could use? I am thinking of regex since I have to find a vowel, but then which method could I use to insert 'ub' in front of a vowel?</p>\n",
    "score": 6,
    "creation_date": 1330545429,
    "view_count": 1525,
    "answer_count": 3,
    "tags": "python;regex;string;nlp"
  },
  {
    "question_id": 4693226,
    "title": "Full Text PDFs for PubMed Articles",
    "body": "<p>While working on a project I need to download and process full text articles for PubMed abstracts, is there any implemented code or tool that allows the user to input a set of PubMed ids and downloads the free full text articles for the same.  Any kind of help or tips is greatly appreciated. </p>\n",
    "score": 6,
    "creation_date": 1295022052,
    "view_count": 5977,
    "answer_count": 2,
    "tags": "pdf;nlp;text-mining;pubmed"
  },
  {
    "question_id": 3101179,
    "title": "how to create exclamations for a particular sentence",
    "body": "<p>I would like to create exclamations for a particular sentence using the java API?</p>\n\n<p>e.g. It's surprising == Isn't it surprising!<br>\ne.g. It's cold == Isn't it cold!  </p>\n\n<p>Are there any vendors or tools which help you generate exclamations, provided you give a sentence (i.e. the left hand side in the above example). Note: The sentences will be provided by the user and we should be able to get the correct sentence.</p>\n\n<p>I am not sure, if this needs to be tagged under other categories</p>\n\n<p><strong>EDIT1</strong></p>\n\n<p>Some more examples, I would like this to be as generic as possible</p>\n\n<p>e.g. They're late == Aren't they late!<br>\ne.g. He looks tired == Doesn't he look tired!<br>\ne.g. That child is dirty == Isn't that child dirty!<br>\ne.g. It's hot == Isn't it hot!  </p>\n",
    "score": 6,
    "creation_date": 1277292758,
    "view_count": 493,
    "answer_count": 6,
    "tags": "java;regex;nlp;text-manipulation"
  },
  {
    "question_id": 66096703,
    "title": "Running huggingface Bert tokenizer on GPU",
    "body": "<p>I'm dealing with a huge text dataset for content classification. I've implemented the distilbert model and distilberttokenizer.from_pretrained() tokenizer..\nThis tokenizer is taking incredibly long to tokenizer my text data roughly 7 mins for just 14k records and that's because it runs on my CPU.</p>\n<p>Is there any way to force the tokenizer to run on my GPU.</p>\n",
    "score": 6,
    "creation_date": 1612764992,
    "view_count": 16980,
    "answer_count": 1,
    "tags": "deep-learning;nlp;huggingface-transformers;huggingface-tokenizers"
  },
  {
    "question_id": 53503049,
    "title": "Measure similarity between two documents using Doc2Vec",
    "body": "<p>I have already trained gensim doc2Vec model, which is finding most similar documents to an unknown one.</p>\n\n<p>Now I need to find the similarity value between two unknown documents (which were not in the training data, so they can not be referenced by doc id)</p>\n\n<pre><code>d2v_model = doc2vec.Doc2Vec.load(model_file)\n\nstring1 = 'this is some random paragraph'\nstring2 = 'this is another random paragraph'\n\nvec1 = d2v_model.infer_vector(string1.split())\nvec2 = d2v_model.infer_vector(string2.split())\n</code></pre>\n\n<p>in the code above vec1 and vec2 are successfully initialized to some values and of size - 'vector_size'</p>\n\n<p>now looking through the gensim api and examples I could not find method that works for me, all of them are expecting TaggedDocument</p>\n\n<p>Can I compare the feature vectors value by value and if they are closer => the texts are more similar?</p>\n",
    "score": 6,
    "creation_date": 1543332885,
    "view_count": 10702,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;gensim;doc2vec"
  },
  {
    "question_id": 41793842,
    "title": "WordNet Python words similarity",
    "body": "<p>I'm trying to find a reliable way to measure the semantic similarity of 2 terms.\nThe first metric could be the path distance on a hyponym/hypernym graph (eventually a linear combination of 2-3 metrics could be better..).</p>\n\n<pre><code>from nltk.corpus import wordnet as wn\ndog = wn.synset('dog.n.01')\ncat = wn.synset('cat.n.01')\nprint(dog.path_similarity(cat))\n</code></pre>\n\n<ul>\n<li>I still don't get what <code>n.01</code> means and why it's necessary.</li>\n<li>there is a way to visually show the computed path between 2 terms?</li>\n<li>Which other nltk semantic metric could I use?</li>\n</ul>\n",
    "score": 6,
    "creation_date": 1485105286,
    "view_count": 11195,
    "answer_count": 1,
    "tags": "python;nlp;nltk;semantics"
  },
  {
    "question_id": 34603922,
    "title": "Difference between Python&#39;s collections.Counter and nltk.probability.FreqDist",
    "body": "<p>I want to calculate the term-frequencies of words in a text corpus. I've been using NLTK's word_tokenize followed by probability.FreqDist for some time to get this done. The word_tokenize returns a list, which is converted to a frequency distribution by FreqDist. However, I recently came across the Counter function in collections (collections.Counter), which seems to be doing the exact same thing. Both FreqDist and Counter have a most_common(n) function which return the n most common words. Does anyone know if there's a difference between these two? Is one faster than the other? Are there cases where one would work and the other wouldn't?</p>\n",
    "score": 6,
    "creation_date": 1451966306,
    "view_count": 4158,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 21267988,
    "title": "How to rank features by their importance in a Weka classifier?",
    "body": "<p>I use Weka to successfully build a classifier. I would now like to evaluate how effective or important my features are. Fot this I use AttributeSelection. But I don't know how to ouput the different features with their corresponding importance. I want simply list the features in decreasing order of their information gain scores! </p>\n",
    "score": 6,
    "creation_date": 1390334717,
    "view_count": 12641,
    "answer_count": 1,
    "tags": "machine-learning;nlp;weka;feature-selection;text-classification"
  },
  {
    "question_id": 18851864,
    "title": "why Wordnet dictionary doesn&#39;t contain the word &#39;she&#39;?",
    "body": "<p>anyone know why wordnet doesn't contain the word 'she'? thanks. </p>\n\n<p>see this <a href=\"http://wordnetweb.princeton.edu/perl/webwn?s=she&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=\">link</a></p>\n",
    "score": 6,
    "creation_date": 1379426049,
    "view_count": 773,
    "answer_count": 2,
    "tags": "nlp;wordnet"
  },
  {
    "question_id": 18369515,
    "title": "Natural Language Understanding API",
    "body": "<p>I am unaware if such an API or service exists currently so this is a vague question, my apologies.</p>\n\n<p>I have a PHP script that works with Freebase and I was wondering if I can enable it so a user can ask a question on my site which will be deconstructed using natural language processing, query the Freebase API and then return an answer.</p>\n\n<p>Does anyone know of an already existing tool like this that works with Freebase?</p>\n\n<p>If not, does anyone know of any great Natural Language Understanding APIs that would be able to strip down a question such as <code>\"how tall is mount everest?\"</code> and tell my script to query <code>\"height\"</code> on the mount everest article on Freebase?</p>\n",
    "score": 6,
    "creation_date": 1377129221,
    "view_count": 1756,
    "answer_count": 4,
    "tags": "php;nlp;artificial-intelligence;freebase;nlp-question-answering"
  },
  {
    "question_id": 4259044,
    "title": "1 million sentences to save in DB - removing non-relevant English words",
    "body": "<p>I am trying to train a Naive Bayes classifier with positive/negative words extracting from a sentiment. example:  </p>\n\n<p>I love this movie :))  </p>\n\n<p>I hate when it rains :(  </p>\n\n<p>The idea is I extract positive or negative sentences based on the emoctions used, but in order to train a classifier and persist it into database.  </p>\n\n<p>The problem is that I have more than 1 million such sentences, so if I train it word by word, the database will go for a toss. I want to remove all non-relevant word example 'I','this', 'when', 'it' so that number of times I have to make a database query is less.  </p>\n\n<p>Please help me in resolving this issue to suggest me better ways of doing it  </p>\n\n<p>Thank you</p>\n",
    "score": 6,
    "creation_date": 1290533972,
    "view_count": 943,
    "answer_count": 3,
    "tags": "database;hadoop;nlp;classification;sentiment-analysis"
  },
  {
    "question_id": 3854900,
    "title": "finding noun and verb in stanford parser",
    "body": "<p>I need to find whether a word is verb or noun or it is both</p>\n\n<p>For example, the word is \"search\" it can be both noun and a verb but stanford parser gives NN tag to  it..</p>\n\n<p>is there any way that stanford parser will give that \"search\" is both noun and verb?</p>\n\n<p>code that i use now</p>\n\n<pre><code>public static String Lemmatize(String word) {\n    WordTag w = new WordTag(word);\n    w.setTag(POSTagWord(word));\n    Morphology m = new Morphology();\n    WordLemmaTag wT = m.lemmatize(w);\n\n    return wT.lemma();\n}\n</code></pre>\n\n<p>or should i use any other software to do it? please suggest me \nthanks in advance</p>\n",
    "score": 6,
    "creation_date": 1286192673,
    "view_count": 10183,
    "answer_count": 3,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 67198877,
    "title": "Cannot import biluo_tags_from_offsets from spacy.gold",
    "body": "<p>I am trying to import biluo_tags_from_offsets from spacy.gold but cannot do it.\nGetting ModuleNotFoundError.</p>\n<pre><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-21-d17a54331c7a&gt; in &lt;module&gt;\n      3 \n      4 import spacy\n----&gt; 5 from spacy.gold import biluo_tags_from_offsets\n      6 nlp = spacy.load(&quot;en_core_web_lg&quot;)\n      7 \n\nModuleNotFoundError: No module named 'spacy.gold'\n</code></pre>\n<p>How can I use this spacy.gold module. I have already installed spacy.\nI am an amateur, sorry if this question does not make any sense.\nAny kind of help is appreciated.\nThanks!</p>\n",
    "score": 6,
    "creation_date": 1619018974,
    "view_count": 4394,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 61826824,
    "title": "Can you train a BERT model from scratch with task specific architecture?",
    "body": "<p>BERT pre-training of the base-model is done by a language modeling approach, where we mask certain percent of tokens in a sentence, and we make the model learn those missing mask. Then, I think in order to do downstream tasks, we add a newly initialized layer and we fine-tune the model.</p>\n<p>However, suppose we have a gigantic dataset for sentence classification. Theoretically, can we initialize the BERT base architecture from scratch, train both the additional downstream task specific layer + the base model weights form scratch with this sentence classification dataset only, and still achieve a good result?</p>\n",
    "score": 6,
    "creation_date": 1589570516,
    "view_count": 6788,
    "answer_count": 2,
    "tags": "machine-learning;nlp;bert-language-model"
  },
  {
    "question_id": 61124443,
    "title": "Using huggingface fill-mask pipeline to get more than 5 suggestions",
    "body": "<p>The below lets me get 5 suggestions for the masked token, but i'd like to get 10 suggestions - does anyone know if this is possible with hugging face?</p>\n\n<pre><code>!pip install -q transformers\nfrom __future__ import print_function\nimport ipywidgets as widgets\nfrom transformers import pipeline\n\nnlp_fill = pipeline('fill-mask')\nnlp_fill(\"I am going to guess &lt;mask&gt; in this sentence\")\n</code></pre>\n",
    "score": 6,
    "creation_date": 1586445360,
    "view_count": 6994,
    "answer_count": 2,
    "tags": "python;neural-network;nlp;huggingface-transformers"
  },
  {
    "question_id": 58694762,
    "title": "Using textblob or spacy for correction spelling in french",
    "body": "<p>I would like to correct the misspelled words of a text in french, it seems that spacy is the most accurate and faster package to do it, but it's to complex,\nI tried with textblob, but I didn't manage to do it with french words.</p>\n<p>It works perfectly in english, but when I try to do the same in french I get the same misspelled words:</p>\n<pre class=\"lang-py prettyprint-override\"><code>#english words \nfrom textblob import TextBlob\nmisspelled=[&quot;hapenning&quot;, &quot;mornin&quot;, &quot;windoow&quot;, &quot;jaket&quot;]\n[str(TextBlob(word).correct()) for word in misspelled]\n\n#french words\nmisspelled2=[&quot;resaissir&quot;, &quot;matinnée&quot;, &quot;plonbier&quot;, &quot;tecnicien&quot;]\n[str(TextBlob(word).correct()) for word in misspelled2]\n</code></pre>\n<p>I get this:</p>\n<pre><code>#english:\n['happening', 'morning', 'window', 'jacket']\n\n#french:\n['resaissir', 'matinnée', 'plonbier', 'tecnicien']\n</code></pre>\n",
    "score": 6,
    "creation_date": 1572875264,
    "view_count": 8683,
    "answer_count": 1,
    "tags": "python;nlp;spacy;textblob"
  },
  {
    "question_id": 50184280,
    "title": "How to conceptually think about relationship between tokenized words and word embeddings?",
    "body": "<p>I have been using JJ Allaire's guide to using word embeddings in neural network model for text processing (<a href=\"https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html\" rel=\"noreferrer\">https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html</a>). I am confused as to how the model relates the tokenized sequences of words (x_train) back to the word embeddings that are defined using the whole dataset (instead of just the training data). Is there a way to conceptualize how the word tokens are mapped to word embeddings? Otherwise, how does a word like 'king' map to the word embedding (obtained using Glove for example). I am speaking to the relation between these chunks of code:</p>\n\n<pre><code>#building model \nhistory &lt;- model %&gt;% fit(\n x_train, y_train,\n epochs = 20,\n batch_size = 32,\n validation_data = list(x_val, y_val)\n)\n\n#relating model to word embeddings\nmodel &lt;- keras_model_sequential() %&gt;% \nlayer_embedding(input_dim = max_words, output_dim = embedding_dim, \n              input_length = maxlen) %&gt;% \nlayer_flatten() %&gt;% \nlayer_dense(units = 32, activation = \"relu\") %&gt;% \nlayer_dense(units = 1, activation = \"sigmoid\")\n\nget_layer(model, index = 1) %&gt;% \n set_weights(list(embedding_matrix)) %&gt;% \n freeze_weights()\n</code></pre>\n\n<p>How is a tokenized word from the x_train linked back to a word in the embedding_matrix (especially if the embedding layer is trained on all data)?</p>\n",
    "score": 6,
    "creation_date": 1525476730,
    "view_count": 7188,
    "answer_count": 2,
    "tags": "r;nlp;keras;text-analysis"
  },
  {
    "question_id": 49021389,
    "title": "Doc2vec: Only 10 docvecs in gensim doc2vec model?",
    "body": "<p>I used gensim fit a doc2vec model, with tagged document (length>10) as training data. The target is to get doc vectors of all training docs, but only 10 vectors can be found in model.docvecs.</p>\n\n<p>The example of training data (length>10)</p>\n\n<pre><code>docs = ['This is a sentence', 'This is another sentence', ....]\n</code></pre>\n\n<p>with some pre-treatment</p>\n\n<pre><code>doc_=[d.strip().split(\" \") for d in doc]\ndoc_tagged = []\nfor i in range(len(doc_)):\n  tagd = TaggedDocument(doc_[i],str(i))\n  doc_tagged.append(tagd)\n</code></pre>\n\n<p>tagged docs</p>\n\n<pre><code>TaggedDocument(words=array(['a', 'b', 'c', ..., ],\n  dtype='&lt;U32'), tags='117')\n</code></pre>\n\n<p>fit a doc2vec model</p>\n\n<pre><code>model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)\nmodel.build_vocab(doc_tagged)\nmodel.train(doc_tagged, total_examples= model.corpus_count, epochs= model.iter)\n</code></pre>\n\n<p>then i get the final model</p>\n\n<pre><code>len(model.docvecs)\n</code></pre>\n\n<p>the result is 10...</p>\n\n<p>I tried other datasets (length>100, 1000) and got same result of <code>len(model.docvecs)</code>.\nSo, my question is:\nHow to use model.docvecs to get full vectors? (without using <code>model.infer_vector</code>)\nIs <code>model.docvecs</code> designed to provide all training docvecs?</p>\n",
    "score": 6,
    "creation_date": 1519787642,
    "view_count": 975,
    "answer_count": 1,
    "tags": "machine-learning;nlp;word2vec;gensim;doc2vec"
  },
  {
    "question_id": 32128802,
    "title": "How to use sklearn&#39;s CountVectorizerand() to get ngrams that include any punctuation as separate tokens?",
    "body": "<p>I use <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">sklearn.feature_extraction.text.CountVectorizer</a> to compute n-grams. Example:</p>\n\n<pre><code>import sklearn.feature_extraction.text # FYI http://scikit-learn.org/stable/install.html\nngram_size = 4\nstring = [\"I really like python, it's pretty awesome.\"]\nvect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size))\nvect.fit(string)\nprint('{1}-grams: {0}'.format(vect.get_feature_names(), ngram_size))\n</code></pre>\n\n<p>outputs:</p>\n\n<pre><code>4-grams: [u'like python it pretty', u'python it pretty awesome', u'really like python it']\n</code></pre>\n\n<p>The punctuation is removed: how to include them as separate tokens?</p>\n",
    "score": 6,
    "creation_date": 1440106536,
    "view_count": 9951,
    "answer_count": 1,
    "tags": "python;nlp;scikit-learn;tokenize;n-gram"
  },
  {
    "question_id": 19437631,
    "title": "How to Normalize Names",
    "body": "<p>I am using pandas dataframes and I have data where I have customers per company. However, the company titles vary slightly but ultimately affect the data.\nExample:</p>\n\n<pre><code>Company    Customers\nAAAB       1,000\nAAAB Inc.  900\nThe AAAB Inc.  20\nAAAB the INC   10\n</code></pre>\n\n<p>I want to get the total customers out of a data base of several different companies with the companies having non-standard names. Any idea where I should start?</p>\n",
    "score": 6,
    "creation_date": 1382045285,
    "view_count": 10450,
    "answer_count": 2,
    "tags": "python;pandas;nlp;normalize"
  },
  {
    "question_id": 18183810,
    "title": "Gensim Dictionary Implementation",
    "body": "<p>I was just curious about the gensim dictionary implementation. I have the following code:</p>\n\n<pre><code>    def build_dictionary(documents):\n        dictionary = corpora.Dictionary(documents)\n        dictionary.save('/tmp/deerwester.dict') # store the dictionary\n        return dictionary    \n</code></pre>\n\n<p>and I looked inside the file deerwester.dict and it looks like this:</p>\n\n<pre><code>8002 6367 656e 7369 6d2e 636f 7270 6f72\n612e 6469 6374 696f 6e61 7279 0a44 6963\n7469 6f6e 6172 790a 7101 2981 7102 7d71\n0328 5508 6e75 6d5f 646f 6373 7104 4b09\n5508 ...\n</code></pre>\n\n<p>the following code, however,</p>\n\n<pre><code>my_dict = dictionary.load('/tmp/deerwester.dict') \nprint my_dict.token2id #view dictionary\n</code></pre>\n\n<p>yields this:</p>\n\n<pre><code>{'minors': 30, 'generation': 22, 'testing': 16, 'iv': 29, 'engineering': 15, 'computer': 2, 'relation': 20, 'human': 3, 'measurement': 18, 'unordered': 25, 'binary': 21, 'abc': 0, 'ordering': 31, 'graph': 26, 'system': 10, 'machine': 6, 'quasi': 32, 'random': 23, 'paths': 28, 'error': 17, 'trees': 24, 'lab': 5, 'applications': 1, 'management': 14, 'user': 12, 'interface': 4, 'intersection': 27, 'response': 8, 'perceived': 19, 'widths': 34, 'well': 33, 'eps': 13, 'survey': 9, 'time': 11, 'opinion': 7}\n</code></pre>\n\n<p>So my question is, since I don't see the actual words inside the .dict file, what are all of the hexadecimal values stored there? Is this some kind of super compressed format? I'm curious because I feel like if it is, I should consider using it from now on.</p>\n",
    "score": 6,
    "creation_date": 1376300300,
    "view_count": 9777,
    "answer_count": 1,
    "tags": "python;nlp;topic-modeling;gensim"
  },
  {
    "question_id": 17969532,
    "title": "How to Normalize similarity measures from Wordnet",
    "body": "<p>I am trying to calculate semantic similarity between two words. I am using Wordnet-based similarity measures i.e Resnik measure(RES), Lin measure(LIN), Jiang and Conrath measure(JNC) and Banerjee and Pederson measure(BNP).</p>\n\n<p>To do that, I am using nltk and Wordnet 3.0. Next, I want to combine the similarity values obtained from different measure. To do that i need to normalize the similarity values as some measure give values between 0 and 1, while others give values greater than 1.</p>\n\n<p>So, my question is how do I normalize the similarity values obtained from different measures.</p>\n\n<p><strong>Extra detail</strong> on what I am actually trying to do: I have a set of words. I calculate pairwise similarity between the words. and remove the words that are not strongly correlated with other words in the set.</p>\n",
    "score": 6,
    "creation_date": 1375271296,
    "view_count": 2779,
    "answer_count": 1,
    "tags": "python;nlp;nltk;similarity;wordnet"
  },
  {
    "question_id": 13922895,
    "title": "Square brackets applied to &quot;self&quot; in Python",
    "body": "<p>I've come across some code where square brackets are used on \"self\". I'm not familiar with this notation and as I'm trying to get my head around source code not written by me, it makes it difficult to understand what sort of object is being dealt with here.</p>\n\n<p>The example I've come across is in the Natural Language Toolkit for Python <a href=\"http://nltk.org/_modules/nltk/model/ngram.html\" rel=\"noreferrer\">here</a>. You can find an example of what I mean if you ctrl-F <code>self[context]</code>.</p>\n\n<p>It may not be possible to tell exactly how it's being used without more context, but here's a snippet with an example:</p>\n\n<pre><code>context = tuple(context)\nif (context + (word,) in self._ngrams) or (self._n == 1):\n     return self[context].prob(word)\nelse:\n     return self._alpha(context) * self._backoff.prob(word, context[1:])\n</code></pre>\n",
    "score": 6,
    "creation_date": 1355781490,
    "view_count": 4676,
    "answer_count": 3,
    "tags": "python;nlp"
  },
  {
    "question_id": 12884411,
    "title": "Horizontal Markovization",
    "body": "<p>I have to implement horizontal markovization (NLP concept) and I'm having a little trouble understanding what the trees will look like. I've been reading the <a href=\"http://nlp.stanford.edu/~manning/papers/unlexicalized-parsing.pdf\">Klein and Manning paper</a>, but they don't explain what the trees with horizontal markovization of order 2 or order 3 will look like. Could someone shed some light on the algorithm and what the trees are SUPPOSED to look like? I'm relatively new to NLP.</p>\n",
    "score": 6,
    "creation_date": 1350233676,
    "view_count": 2763,
    "answer_count": 2,
    "tags": "parsing;tree;nlp;context-free-grammar"
  },
  {
    "question_id": 6534030,
    "title": "Algorithm to Detect and Compare Phrases",
    "body": "<p>I have a couple of non-English texts. I would like to perform stylistic comparisons on them.</p>\n\n<p>One method of comparing style is to look for similar phrases. If I find in one book \"fishing, skiing and hiking\" a couple of times and in another book \"fishing, hiking and skiing\" the similarity in style points to one author. I need to also be able to find \"fishing and even skiing or hiking\" though. Ideally I would also find \"angling, hiking and skiing\" but because they are non-English texts (Koine Greek), synonyms are harder to allow for and this aspect is not vital.</p>\n\n<p>What is the best way to (1) go about detecting these sorts of phrases and then (2) searching for them in a way that is not overly rigid in other texts (so as to find \"fishing and even skiing or hiking\")?</p>\n",
    "score": 6,
    "creation_date": 1309433428,
    "view_count": 1798,
    "answer_count": 3,
    "tags": "algorithm;language-agnostic;nlp;semantics"
  },
  {
    "question_id": 1840386,
    "title": "Calculating context-sensitive text correlation",
    "body": "<p>Suppose I want to match address records (or person names or whatever) against each other to merge records that are most likely referring to the same address. Basically, I guess I would like to calculate some kind of correlation between the text values and merge the records if this value is over a certain threshold.</p>\n\n<p>Example: \n\"West Lawnmower Drive 54 A\" is probably the same as \"W. Lawn Mower Dr. 54A\" but different from \"East Lawnmower Drive 54 A\".</p>\n\n<p>How would you approach this problem? Would it be necessary to have some kind of context-based dictionary that knows, in the address case, that \"W\", \"W.\" and \"West\" are the same? What about misspellings (\"mover\" instead of \"mower\" etc)?</p>\n\n<p>I think this is a tricky one - perhaps there are some well-known algorithms out there?</p>\n",
    "score": 6,
    "creation_date": 1259851991,
    "view_count": 3839,
    "answer_count": 5,
    "tags": "algorithm;string;text;nlp"
  },
  {
    "question_id": 1232167,
    "title": "How to use Wordnet in SQL",
    "body": "<p>How to use Wordnet in SQL database. Does it exists anywhere can someone give me step by step procedure</p>\n",
    "score": 6,
    "creation_date": 1249465114,
    "view_count": 2504,
    "answer_count": 1,
    "tags": "sql;nlp"
  },
  {
    "question_id": 466917,
    "title": "Natural language parser for dates (.NET)?",
    "body": "<p>I want to be able to let users enter dates (including recurring dates) using natural language (eg \"next friday\", \"every weekday\"). Much like the examples at <a href=\"http://todoist.com/Help/timeInsert\" rel=\"nofollow noreferrer\">http://todoist.com/Help/timeInsert</a></p>\n\n<p>I found <a href=\"https://stackoverflow.com/questions/23689/natural-language-date-time-parser-for-net\">this post</a>, but it's a bit old and offered only <a href=\"http://www.codeplex.com/DateTimeEnglishParse\" rel=\"nofollow noreferrer\">one solution</a> that I'm not entirely content with. I thought I'd resurrect this question and see: are there any other .NET libraries out there that do this kind of date parsing?</p>\n",
    "score": 6,
    "creation_date": 1232570882,
    "view_count": 1191,
    "answer_count": 6,
    "tags": ".net;datetime;ironpython;nlp"
  },
  {
    "question_id": 70043467,
    "title": "How to run huggingface Helsinki-NLP models",
    "body": "<p>I am trying to use the Helsinki-NLP models from <a href=\"https://huggingface.co/Helsinki-NLP/opus-mt-es-en\" rel=\"noreferrer\"><code>huggingface</code></a>, but\nI cannot find any instructions on how to do it.\nThe README files are computer generated and do not contain explanations.\nCan some one point me to a getting started guide, or show an example of how to run a model like opus-mt-en-es?</p>\n",
    "score": 6,
    "creation_date": 1637386026,
    "view_count": 8160,
    "answer_count": 3,
    "tags": "machine-learning;nlp;huggingface-transformers"
  },
  {
    "question_id": 52686159,
    "title": "How to extract the location name, country name, city name, tourist places by using nlp or spacy in python",
    "body": "<p>I am trying to extract the location name, country name, city name, tourist places from txt file by using nlp or scapy library in python. </p>\n\n<p>I have tried below: </p>\n\n<pre><code>import spacy\nen = spacy.load('en')\n\nsents = en(open('subtitle.txt').read())\nplace = [ee for ee in sents.ents]\n</code></pre>\n\n<p>Getting output:</p>\n\n<pre><code>[1, \n, three, London, \n, \n, \n, \n, first, \n, \n, 00:00:20,520, \n, \n, London, the\n\n4\n00:00:20,520, 00:00:26,130\n, Buckingham Palace, \n, \n</code></pre>\n\n<p>I just want only location name, country name, city name and any place within city. </p>\n\n<p>I also tried by using NLP:</p>\n\n<pre><code>import nltk\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\n\nwith open('subtitle.txt', 'r') as f:\n    sample = f.read()\n\n\nsentences = nltk.sent_tokenize(sample)\ntokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\ntagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\nchunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n\ndef extract_entity_names(t):\n    entity_names = []\n\n    if hasattr(t, 'label') and t.label:\n        if t.label() == 'NE':\n            entity_names.append(' '.join([child[0] for child in t]))\n        else:\n            for child in t:\n                entity_names.extend(extract_entity_names(child))\n\n    return entity_names\n\nentity_names = []\nfor tree in chunked_sentences:\n    # Print results per sentence\n    #print (extract_entity_names(tree))\n\n    entity_names.extend(extract_entity_names(tree))\n\n# Print all entity names\n#print (entity_names)\n\n# Print unique entity names\nprint (set(entity_names))\n</code></pre>\n\n<p>Output Getting: </p>\n\n<pre><code>{'Okay', 'Buckingham Palace', 'Darwin Brasserie', 'PDF', 'London', 'Local Guide', 'Big Ben'}\n</code></pre>\n\n<p>Here, also getting unwanted words like 'Okay', 'PDF', 'Local Guide' and some places are missing. </p>\n\n<p>Please suggest.</p>\n\n<p><strong>Edit-1</strong></p>\n\n<p><strong>Script</strong></p>\n\n<pre><code>import spacy\nnlp = spacy.load('en_core_web_lg')\n\ngpe = [] # countries, cities, states\nloc = [] # non gpe locations, mountain ranges, bodies of water\n\n\ndoc = nlp(open('subtitle.txt').read())\nfor ent in doc.ents:\n    if (ent.label_ == 'GPE'):\n        gpe.append(ent.text)\n    elif (ent.label_ == 'LOC'):\n        loc.append(ent.text)\n\ncities = []\ncountries = []\nother_places = []\nimport wikipedia\nfor text in gpe:\n    summary = str(wikipedia.summary(text))\n    if ('city' in summary):\n        cities.append(text)\n        print (cities)\n    elif ('country' in summary):\n        countries.append(text)\n        print (countries)\n    else:\n        other_places.append(text)\n        print (other_places)\n\nfor text in loc:\n    other_places.append(text)\n    print (other_places)\n</code></pre>\n\n<p>By using answered script: getting below output</p>\n\n<pre><code>['London', 'London']\n['London', 'London', 'London']\n['London', 'London', 'London', 'London']\n['London', 'London', 'London', 'London', 'London']\n['London', 'London', 'London', 'London', 'London', 'London']\n['London', 'London', 'London', 'London', 'London', 'London', 'London']\n['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']\n['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']\n['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']\n['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']\n['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']\n</code></pre>\n",
    "score": 6,
    "creation_date": 1538895166,
    "view_count": 10834,
    "answer_count": 1,
    "tags": "python-3.x;machine-learning;nlp;stanford-nlp;spacy"
  },
  {
    "question_id": 50999596,
    "title": "SMOTE, Oversampling on text classification in Python",
    "body": "<p>I am doing a text classification and I have very imbalanced data like </p>\n\n<pre><code>Category | Total Records\nCate1    | 950\nCate2    |  40\nCate3    |  10\n</code></pre>\n\n<p>Now I want to over sample Cate2 and Cate3 so it at least have 400-500 records, I prefer to use SMOTE over random sampling, Code </p>\n\n<pre><code>from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nX_train, X_test, y_train, y_test = train_test_split(fewRecords['text'],\n                                   fewRecords['category'])\n\nsm = SMOTE(random_state=12, ratio = 1.0)\nx_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n</code></pre>\n\n<p>It does not work as it can't generate the sample synthetic text, Now when I covert it into vector like </p>\n\n<pre><code>count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(fewRecords['category'])\n\n# transform the training and validation data using count vectorizer object\nxtrain_count =  count_vect.transform(X_train)\nytrain_train =  count_vect.transform(y_train)\n</code></pre>\n\n<p>I am not sure if it is right approach and how to convert vector to real text when  I want to predict real category after classification </p>\n",
    "score": 6,
    "creation_date": 1529744446,
    "view_count": 17693,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;text-classification;resampling"
  },
  {
    "question_id": 33326810,
    "title": "scikit weighted f1 score calculation and usage",
    "body": "<p>I have a question regarding <code>weighted</code> average in sklearn.metrics.f1_score</p>\n\n<pre><code>sklearn.metrics.f1_score(y_true, y_pred, labels=None, pos_label=1, average='weighted', sample_weight=None)\n\nCalculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n</code></pre>\n\n<p>First, if there is any reference that justifies the usage of weighted-F1, I am just curios in which cases I should use weighted-F1.</p>\n\n<p>Second, I heard that weighted-F1 is deprecated, is it true?</p>\n\n<p>Third, how actually weighted-F1 is being calculated, for example</p>\n\n<pre><code>{\n    \"0\": {\n        \"TP\": 2,\n        \"FP\": 1,\n        \"FN\": 0,\n        \"F1\": 0.8\n    },\n    \"1\": {\n        \"TP\": 0,\n        \"FP\": 2,\n        \"FN\": 2,\n        \"F1\": -1\n    },\n    \"2\": {\n        \"TP\": 1,\n        \"FP\": 1,\n        \"FN\": 2,\n        \"F1\": 0.4\n    }\n}\n</code></pre>\n\n<p>How to calculate weighted-F1 of the above example. I though it should be something like (0.8*2/3 + 0.4*1/3)/3, however I was wrong.</p>\n",
    "score": 6,
    "creation_date": 1445753853,
    "view_count": 11687,
    "answer_count": 1,
    "tags": "machine-learning;nlp;scikit-learn;precision-recall"
  },
  {
    "question_id": 29406247,
    "title": "How to remove english text from arabic string in python?",
    "body": "<p>I have an Arabic string with English text and punctuations. I need to filter Arabic text and I tried removing punctuations and English words using sting. However, I lost the spacing between Arabic words. Where am I wrong?   </p>\n\n<pre><code>import string\nexclude = set(string.punctuation)\n\nmain_text = \"وزارة الداخلية: لا تتوفر لدينا معلومات رسمية عن سعوديين موقوفين في ليبيا http://alriyadh.com/1031499\"\nmain_text = ''.join(ch for ch in main_text if ch not in exclude)\n[output after this step=\"وزارة الداخلية لا تتوفر لدينا معلومات رسمية عن سعوديين موقوفين في ليبيا httpalriyadhcom1031499]\"\nn = filter(lambda x: x not in string.printable, n)\nprint n\nوزارةالداخليةلاتتوفرلدينامعلوماترسميةعنسعوديينموقوفينفيليبيا\n</code></pre>\n\n<p>I am able to remove punctuations and english text but I lost the space between words. How can I retain each words?</p>\n",
    "score": 6,
    "creation_date": 1427955588,
    "view_count": 3001,
    "answer_count": 2,
    "tags": "python;lambda;nlp"
  },
  {
    "question_id": 15257674,
    "title": "scikit-learn, add features to a vectorized set of documents",
    "body": "<p>I am starting with scikit-learn and I am trying to transform a set of documents into a format on which I could apply clustering and classification. I have seen the details about the vectorization methods, and the tfidf transformations to load the files and index their vocabularies.</p>\n\n<p>However, I have extra metadata for each documents, such as the authors, the division that was responsible, list of topics, etc.</p>\n\n<p>How can I add features to each document vector generated by the vectorizing function?</p>\n",
    "score": 6,
    "creation_date": 1362602832,
    "view_count": 1772,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;scikit-learn"
  },
  {
    "question_id": 14168601,
    "title": "NLTK makes it easy to compute bigrams of words. What about letters?",
    "body": "<p>I've seen tons of documentation all over the web about how the python NLTK makes it easy to compute bigrams of words.</p>\n\n<p>What about letters?</p>\n\n<p>What I want to do is plug in a dictionary and have it tell me the relative frequencies of different letter pairs.</p>\n\n<p>Ultimately I'd like to make some kind of markov process to generate likely-looking (but fake) words.</p>\n",
    "score": 6,
    "creation_date": 1357360421,
    "view_count": 3294,
    "answer_count": 2,
    "tags": "python;nlp;nltk;n-gram"
  },
  {
    "question_id": 12578790,
    "title": "Disease named entity recognition",
    "body": "<p>I have a bunch of text documents that describe diseases. Those documents are in most cases quite short and often only contain a single sentence. An example is given here:</p>\n\n<blockquote>\n  <p>Primary pulmonary hypertension is a progressive disease in which widespread occlusion of the smallest pulmonary arteries leads to increased pulmonary vascular resistance, and subsequently right ventricular failure.</p>\n</blockquote>\n\n<p>What I need is a tool that finds all disease terms (e.g. \"pulmonary hypertension\" in this case) in the sentences and maps them to a controlled vocabulary like <a href=\"http://www.ncbi.nlm.nih.gov/mesh\" rel=\"noreferrer\">MeSH</a>.</p>\n\n<p>Thanks in advance for your answers!</p>\n",
    "score": 6,
    "creation_date": 1348560905,
    "view_count": 3212,
    "answer_count": 5,
    "tags": "machine-learning;nlp;medical;named-entity-recognition"
  },
  {
    "question_id": 10260736,
    "title": "How to detect if a event/action occurred from a text?",
    "body": "<p>I was wondering if there's a NLP/ML technique for this.</p>\n\n<p>Suppose given a set of sentences, </p>\n\n<ol>\n<li>I watched the movie.</li>\n<li>Heard the movie is great, have to watch it.</li>\n<li>Got the tickets for the movie. </li>\n<li>I am at the movie.</li>\n</ol>\n\n<p>If i have to assign a probability to each of these sentences, that they have \"actually\" watched the movie, i would assign it in decreasing order of 1,4,3,2.</p>\n\n<p>Is there a way to do this automatically, using some classifier or rules? Any paper/link would help.</p>\n",
    "score": 6,
    "creation_date": 1335024796,
    "view_count": 3061,
    "answer_count": 3,
    "tags": "nlp;machine-learning;data-mining;information-retrieval"
  },
  {
    "question_id": 10180730,
    "title": "Splitting string containing letters and numbers not separated by any particular delimiter in PHP",
    "body": "<p>Currently I am developing a web application to fetch Twitter stream and trying to create a natural language processing by my own. </p>\n\n<p>Since my data is from Twitter (limited by 140 characters) there are many words shortened, or on this case, <strong>omitted space</strong>.</p>\n\n<p>For example:</p>\n\n<pre><code>\"Hi, my name is Bob. I m 19yo and 170cm tall\"\n</code></pre>\n\n<p>Should be tokenized to:</p>\n\n<pre><code>- hi\n- my\n- name\n- bob\n- i\n- 19\n- yo\n- 170\n- cm\n- tall\n</code></pre>\n\n<p>Notice that <code>19</code> and <code>yo</code> in <code>19yo</code> have <strong>no space</strong> between them. I use it mostly for extracting numbers with their units. </p>\n\n<p>Simply, what I need is a way to 'explode' each tokens that has number in it by chunk of numbers or letters <strong>without</strong> delimiter. </p>\n\n<p><code>'123abc'</code> will be <code>['123', 'abc']</code></p>\n\n<p><code>'abc123'</code> will be <code>['abc', '123']</code></p>\n\n<p><code>'abc123xyz'</code> will be <code>['abc', '123', 'xyz']</code></p>\n\n<p>and so on.</p>\n\n<p>What is the best way to achieve it in PHP?</p>\n\n<hr>\n\n<p>I found something close to it, but it's C# and spesifically for day/month splitting. <a href=\"https://stackoverflow.com/q/2362153/670623\">How do I split a string in C# based on letters and numbers</a></p>\n",
    "score": 6,
    "creation_date": 1334606140,
    "view_count": 2518,
    "answer_count": 2,
    "tags": "php;regex;string;algorithm;nlp"
  },
  {
    "question_id": 2587658,
    "title": "are there any c# libraries for Named Entity Recognition?",
    "body": "<p>I am looking for any free libraries for Named Entity Recognition in c# or any other .net language.</p>\n",
    "score": 6,
    "creation_date": 1270581931,
    "view_count": 3751,
    "answer_count": 3,
    "tags": "c#;dll;nlp"
  },
  {
    "question_id": 1361030,
    "title": "which is better... GATE or RapidMiner",
    "body": "<p>I've started to write a simple sentiment analysis tool.</p>\n\n<p>Currently I am looking at <a href=\"http://gate.ac.uk\" rel=\"noreferrer\">GATE</a> and <a href=\"http://rapid-i.com/\" rel=\"noreferrer\">RapidMiner</a> but being a beginner not able to concentrate on both.</p>\n\n<p>Could someone please tell me which one will be better in terms of usage, learning curve, licensing etc?</p>\n",
    "score": 6,
    "creation_date": 1251790146,
    "view_count": 4054,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 1266319,
    "title": "Processing English Statements",
    "body": "<p>Any recommendations for languages/libraries to convert sentence like:</p>\n\n<blockquote>\n  <p>\"X bumped Y, who in turn kicked Z.\"</p>\n</blockquote>\n\n<p>to</p>\n\n<ol>\n<li>X: Bumped</li>\n<li>Y: Was bumped, kicked Z</li>\n</ol>\n",
    "score": 6,
    "creation_date": 1250085397,
    "view_count": 1201,
    "answer_count": 6,
    "tags": "nlp"
  },
  {
    "question_id": 76914119,
    "title": "Validation and Training Loss when using HuggingFace",
    "body": "<p>I do not seem to find an explanation on how the validation and training losses are calculated when we finetune a model using the huggingFace trainer. Does anyone know here to find this information?</p>\n",
    "score": 6,
    "creation_date": 1692192967,
    "view_count": 7482,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;huggingface;huggingface-trainer"
  },
  {
    "question_id": 61588381,
    "title": "Speed up embedding of 2M sentences with RoBERTa",
    "body": "<p>I have roughly 2 million sentences that I want to turn into vectors using Facebook AI's RoBERTa-large,fine-tuned on NLI and STSB for sentence similarity (using the awesome <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"noreferrer\">sentence-transformers</a> package).</p>\n\n<p>I already have a dataframe with two columns: \"utterance\" containing each sentence from the corpus, and \"report\" containing, for each sentence, the title of the document from which it is from.</p>\n\n<p>From there, my code is the following:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\nmodel = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n\nprint(\"Embedding sentences\")\n\ndata = pd.read_csv(\"data/sentences.csv\")\n\nsentences = data['utterance'].tolist()\n\nsentence_embeddings = []\n\nfor sent in tqdm(sentences):\n    embedding = model.encode([sent])\n    sentence_embeddings.append(embedding[0])\n\ndata['vector'] = sentence_embeddings\n</code></pre>\n\n<p>Right now, tqdm estimates that the whole process will take around 160 hours on my computer, which is more than I can spare.</p>\n\n<p>Is there any way I could speed this up by changing my code? Is creating a huge list in memory then appending it to the dataframe the best way to proceed here? (I suspect not).</p>\n\n<p>Many thanks in advance!</p>\n",
    "score": 6,
    "creation_date": 1588582215,
    "view_count": 2992,
    "answer_count": 2,
    "tags": "python;nlp;word-embedding;transformer-model"
  },
  {
    "question_id": 41601087,
    "title": "What string distance algorithm is best for measuring typing accuracy?",
    "body": "<p>I'm trying to write a function that detects how accurate the user typed a particular phrase/sentence/word/words. My objective is to build an app to train the user's typing accuracy of certain phrases.</p>\n\n<p>My initial instinct is to use the basic levenshtein distance algorithm (mostly because that's the only algo I knew off the top of my head).</p>\n\n<p>But after a bit more research, I saw that <a href=\"https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance\" rel=\"nofollow noreferrer\">Jaro-Winkler</a> is a slightly more interesting algorithm because of its consideration for transpositions.</p>\n\n<p>I even found a link that talks about the differences between these algorithms:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/25540581/difference-between-jaro-winkler-and-levenshtein-distance\">Difference between Jaro-Winkler and Levenshtein distance?</a></p>\n\n<p>Having read all that, in addition to the respective Wikipedia posts, I am still a little clueless as to which algorithm fits my objective the best.</p>\n",
    "score": 6,
    "creation_date": 1484170487,
    "view_count": 2769,
    "answer_count": 3,
    "tags": "algorithm;nlp;levenshtein-distance;jaro-winkler"
  },
  {
    "question_id": 41174566,
    "title": "How to normalize Persian texts with Hazm",
    "body": "<p>I have a folder containing some other <em>folders</em> and each contains a lot of text files. I have to extract <strong>5 words</strong> before and after a specific word and following code works fine.</p>\n\n<p>The problem is that because I did not normalize the texts, it just returns a few sentences while there is more. \nIn Persian there is a module called <strong>hazm</strong> for normalizing the texts. How can I use that in this code? </p>\n\n<p>For example of normalizing: <strong>\"ك\"</strong>  should change to <strong>\"ک\"</strong> or <strong>\"ؤ\"</strong> should change to \"و\". Because the first two ones are actually <em>Arabic</em> alphabets which were used in <em>Persian</em>. Without normalizing the code just returns the words that are written with the second form and it does not recognize the words which are in the first forms <em>Arabic</em>).</p>\n\n<pre><code>import os\nfrom hazm import Normalizer\n\n\ndef getRollingWindow(seq, w):\n    win = [next(seq) for _ in range(11)]\n    yield win\n    for e in seq:\n        win[:-1] = win[1:]\n        win[-1] = e\n        yield win\n\n\ndef extractSentences(rootDir, searchWord):\n    with open(\"پاکت\", \"w\", encoding=\"utf-8\") as outfile:\n        for root, _dirs, fnames in os.walk(rootDir):\n            for fname in fnames:\n                print(\"Looking in\", os.path.join(root, fname))\n                with open(os.path.join(root, fname), encoding = \"utf-8\") as infile:\n                    #normalizer = Normalizer()\n                    #fname = normalizer.normalize(fname)\n                    for window in getRollingWindow((word for line in infile for word in line(normalizer.normalize(line)).split()), 11):\n                        if window[5] != searchWord: continue\n                        outfile.write(' '.join(window)+ \"\\n\")\n</code></pre>\n",
    "score": 6,
    "creation_date": 1481841896,
    "view_count": 3227,
    "answer_count": 1,
    "tags": "python-3.x;nlp;persian"
  },
  {
    "question_id": 38156017,
    "title": "Defining vocabulary size in text classification",
    "body": "<p>I have a question regarding the defining of vocabulary set needed for feature extraction in text classification.\n In an experiment, there are two approaches I can think of:</p>\n\n<p>1.Define vocabulary size using both training data and test data, so that no word from the test data would be treated as being 'unknown' during the testing.</p>\n\n<p>2.Define vocabulary size according to data only from the training data, and treat every word in the testing data that does not also appear in the training data as 'unknown'.</p>\n\n<p>At first glance the more scientific way is the second one. However it is worth noticing that although there is no way we can know about the true size of vocabulary in a practical system, there seems to be no problem to set the vocabulary size a little bit larger than the size appeared in the training data in order to cover potentially larger problems. This is helpful in that it actually treats different unknown words as being different, instead of summing them up as 'unknown'. Is there any reason why this is not practical?</p>\n\n<p>New to machine learning. Help much appreciated. </p>\n",
    "score": 6,
    "creation_date": 1467427464,
    "view_count": 9304,
    "answer_count": 2,
    "tags": "machine-learning;nlp;text-classification"
  },
  {
    "question_id": 34603987,
    "title": "When to stop training neural networks?",
    "body": "<p>I'm trying to carry out a domain-specific classification research using RNN and have accumulated tens of millions of texts. Since it takes days and even months to run the whole dataset over, I only picked a small portion of it for testing, say 1M texts (80% for training, 20% for validation). I pre-trained the whole corpus with word vectorization and I also applied Dropout to the model to avoid over-fitting. When it trained 60000 text within 12 hrs, the loss had already dropped to a fairly low level with the accuracy 97%. Should I continue or not? Does it help continue with the training?</p>\n\n<p>It is still running the first epoch and I'm afraid if I stopped right now, the model wouldn't cover the whole...</p>\n",
    "score": 6,
    "creation_date": 1451966991,
    "view_count": 6040,
    "answer_count": 1,
    "tags": "machine-learning;nlp;neural-network;lstm;recurrent-neural-network"
  },
  {
    "question_id": 27513185,
    "title": "Simplifying the French POS Tag Set with NLTK",
    "body": "<p>How can one simplify the part of speech tags returned by Stanford's French POS tagger? It is fairly easy to read an English sentence into NLTK, find each word's part of speech, then use map_tag() to simplify the tag set:</p>\n\n<pre><code>#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nfrom nltk.tag.stanford import POSTagger\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import map_tag\n\n#set java_home path from within script. Run os.getenv(\"JAVA_HOME\") to test java_home\nos.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.7.0_25\\\\bin\"\n\nenglish = u\"the whole earth swarms with living beings, every plant, every grain and leaf, supports the life of thousands.\"\n\npath_to_english_model = \"C:\\\\Text\\\\Professional\\\\Digital Humanities\\\\Packages and Tools\\\\Stanford Packages\\\\stanford-postagger-full-2014-08-27\\\\stanford-postagger-full-2014-08-27\\\\models\\\\english-bidirectional-distsim.tagger\"\npath_to_jar = \"C:\\\\Text\\\\Professional\\\\Digital Humanities\\\\Packages and Tools\\\\Stanford Packages\\\\stanford-postagger-full-2014-08-27\\\\stanford-postagger-full-2014-08-27\\\\stanford-postagger.jar\"\n\n#define english and french taggers\nenglish_tagger = POSTagger(path_to_english_model, path_to_jar, encoding=\"utf-8\")\n\n#each tuple in list_of_english_pos_tuples = (word, pos)\nlist_of_english_pos_tuples = english_tagger.tag(word_tokenize(english))\n\nsimplified_pos_tags_english = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in list_of_english_pos_tuples]\n\nprint simplified_pos_tags_english\n\n#output = [(u'the', u'DET'), (u'whole', u'ADJ'), (u'earth', u'NOUN'), (u'swarms', u'NOUN'), (u'with', u'ADP'), (u'living', u'NOUN'), (u'beings', u'NOUN'), (u',', u'.'), (u'every', u'DET'), (u'plant', u'NOUN'), (u',', u'.'), (u'every', u'DET'), (u'grain', u'NOUN'), (u'and', u'CONJ'), (u'leaf', u'NOUN'), (u',', u'.'), (u'supports', u'VERB'), (u'the', u'DET'), (u'life', u'NOUN'), (u'of', u'ADP'), (u'thousands', u'NOUN'), (u'.', u'.')]\n</code></pre>\n\n<p>But I'm not sure how to map the French tags returned by the following code to the universal tagset:</p>\n\n<pre><code>#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nfrom nltk.tag.stanford import POSTagger\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import map_tag\n\n#set java_home path from within script. Run os.getenv(\"JAVA_HOME\") to test java_home\nos.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.7.0_25\\\\bin\"\n\nfrench = u\"Chaque plante, chaque graine, chaque particule de matière organique contient des milliers d'atomes animés.\"\n\npath_to_french_model = \"C:\\\\Text\\\\Professional\\\\Digital Humanities\\\\Packages and Tools\\\\Stanford Packages\\\\stanford-postagger-full-2014-08-27\\\\stanford-postagger-full-2014-08-27\\\\models\\\\french.tagger\"\npath_to_jar = \"C:\\\\Text\\\\Professional\\\\Digital Humanities\\\\Packages and Tools\\\\Stanford Packages\\\\stanford-postagger-full-2014-08-27\\\\stanford-postagger-full-2014-08-27\\\\stanford-postagger.jar\"\n\nfrench_tagger = POSTagger(path_to_french_model, path_to_jar, encoding=\"utf-8\")\n\nlist_of_french_pos_tuples = french_tagger.tag(word_tokenize(french))\n\n#up to this point all is well, but I'm not sure how to successfully create a simplified pos tagset with the French tuples\nsimplified_pos_tags_french = [(word, map_tag('SOME_ARGUMENT', 'universal', tag)) for word, tag in list_of_french_pos_tuples]\nprint simplified_pos_tags_french\n</code></pre>\n\n<p>Does anyone know how to simplify the default tag set used by the french model in the Stanford POS tagger? I would be grateful for any insights others can offer on this question.</p>\n",
    "score": 6,
    "creation_date": 1418761189,
    "view_count": 3867,
    "answer_count": 1,
    "tags": "python;syntax;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 21617540,
    "title": "How to &quot;update&quot; an existing Named Entity Recognition model - rather than creating from scratch?",
    "body": "<p>Please see the tutorial steps for OpenNLP - Named Entity Recognition : <a href=\"https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html\" rel=\"noreferrer\">Link to tutorial</a>\nI am using the \"en-ner-person.bin\" model found <a href=\"http://opennlp.sourceforge.net/models-1.5/\" rel=\"noreferrer\">here</a>\nIn the tutorial, there are instructions on Training and creating a new model. Is there any way to \"Update\" the existing \"en-ner-person.bin\" with additional training data? </p>\n\n<p>Say I have a list of 500 additional person names that are otherwise not recognized as persons - how do I generate a new model?</p>\n",
    "score": 6,
    "creation_date": 1391735853,
    "view_count": 3346,
    "answer_count": 2,
    "tags": "java;nlp;opennlp;corpus"
  },
  {
    "question_id": 20509678,
    "title": "OpenNLP: foreign names does not get recognized",
    "body": "<p>I just started using openNLP to recognize names. I am using the model (en-ner-person.bin) that comes with open NLP.  I noticed that while it recognizes us, uk, and european names, it fails to recognize Indian or Japanese names.  My questions are (1) is there already models available that I can use to recognize foreign names (2) If not, then I believe I will need to generate new models.  In that case, is there a copora available that I can use?</p>\n",
    "score": 6,
    "creation_date": 1386730383,
    "view_count": 4682,
    "answer_count": 1,
    "tags": "nlp;opennlp"
  },
  {
    "question_id": 14858267,
    "title": "How to calculate readabilty in R with the tm package",
    "body": "<p>Is there a pre-built function for this in the <code>tm</code> library, or one that plays nicely with it?  </p>\n\n<p>My current corpus is loaded into tm, something like as follows:</p>\n\n<pre><code>s1 &lt;- \"This is a long, informative document with real words and sentence structure:  introduction to teaching third-graders to read.  Vocabulary is key, as is a good book.  Excellent authors can be hard to find.\" \ns2 &lt;- \"This is a short jibberish lorem ipsum document.  Selling anything to strangers and get money!  Woody equal ask saw sir weeks aware decay. Entrance prospect removing we packages strictly is no smallest he. For hopes may chief get hours day rooms. Oh no turned behind polite piqued enough at. \"\nstuff &lt;- rbind(s1,s2) \nd &lt;- Corpus(VectorSource(stuff[,1]))\n</code></pre>\n\n<p>I tried using <a href=\"https://stackoverflow.com/questions/14835894/how-do-i-extract-contents-from-a-korpus-object-in-r\">koRpus</a>, but it seems silly to retokenize in a different package than the one I'm already using. I also had problems vectorizing its return object in a way that would allow me to reincorporate the results into <code>tm</code>.  (Namely, due to errors, it would often return more or fewer readability scores than the number of documents in my collection.)</p>\n\n<p>I understand I could do a naive calculation parsing vowels as syllables, but want a more thorough package that takes care of the edge cases already (address silent e's, etc.).  </p>\n\n<p>My readability scores of choice are Flesch-Kincaid or Fry.</p>\n\n<p>What I had tried originally where d is my corpus of 100 documents:</p>\n\n<pre><code>f &lt;- function(x) tokenize(x, format=\"obj\", lang='en')\ng &lt;- function(x) flesch.kincaid(x)\nx &lt;- foreach(i=1:length(d), .combine='c',.errorhandling='remove') %do% g(f(d[[i]]))\n</code></pre>\n\n<p>Unfortunately, x returns less than 100 documents, so I can't associate successes with the correct document.  (This is partly my misunderstanding of 'foreach' versus 'lapply' in R, but I found the structure of a text object sufficiently difficult that I could not appropriately tokenize, apply flesch.kincaid, and successfully check errors in a reasonable sequence of  apply statements.)</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Two other things I've tried, trying to apply the koRpus functions to the tm object...</p>\n\n<ol>\n<li><p>Pass arguments into the tm_map object, using the default tokenizer:\n<code>tm_map(d,flesch.kincaid,force.lang=\"en\",tagger=tokenize)</code></p></li>\n<li><p>Define a tokenizer, pass that in.</p>\n\n<pre><code> f &lt;- function(x) tokenize(x, format=\"obj\", lang='en')\n tm_map(d,flesch.kincaid,force.lang=\"en\",tagger=f)\n</code></pre></li>\n</ol>\n\n<p>Both of these returned:</p>\n\n<pre><code>   Error: Specified file cannot be found:\n</code></pre>\n\n<p>Then lists the full text of d[<a href=\"https://stackoverflow.com/questions/14835894/how-do-i-extract-contents-from-a-korpus-object-in-r\">1</a>].  Seems to have found it?  What should I do to pass the function correctly?</p>\n\n<p><strong>UPDATE 2</strong></p>\n\n<p>Here's the error I get when I try to map koRpus functions directly with lapply:</p>\n\n<pre><code>&gt; lapply(d,tokenize,lang=\"en\")\nError: Unable to locate\n Introduction to teaching third-graders to read.  Vocabulary is key, as is a good book.  Excellent authors can be hard to find. \n</code></pre>\n\n<p>This looks like a strange error---I almost don't think it means it can't locate the text, but that it can't locate some blank error code (such as, 'tokenizer'), before dumping the located text.</p>\n\n<p><strong>UPDATE 3</strong></p>\n\n<p>Another problem with retagging using <code>koRpus</code> was that retagging (versus the tm tagger) was extremely slow and output its tokenization progress to stdout.  Anyway, I've tried the following:</p>\n\n<pre><code>f &lt;- function(x) capture.output(tokenize(x, format=\"obj\", lang='en'),file=NULL)\ng &lt;- function(x) flesch.kincaid(x)\nx &lt;- foreach(i=1:length(d), .combine='c',.errorhandling='pass') %do% g(f(d[[i]]))\ny &lt;- unlist(sapply(x,slot,\"Flesch.Kincaid\")[\"age\",])\n</code></pre>\n\n<p>My intention here would be to rebind the <code>y</code> object above back to my <code>tm(d)</code> corpus as metadata, <code>meta(d, \"F-KScore\") &lt;- y</code>.</p>\n\n<p>Unfortunately, applied to my actual data set, I get the error message:</p>\n\n<pre><code>Error in FUN(X[[1L]], ...) : \n  cannot get a slot (\"Flesch.Kincaid\") from an object of type \"character\"\n</code></pre>\n\n<p>I think one element of my actual corpus must be an NA, or too long, something else prohibitive---and due to the nested functionalizing, I am having trouble tracking down exactly which it is.  </p>\n\n<p>So, currently, it looks like there is no pre-built function for reading scores that play nicely with the <code>tm</code> library.  Unless someone sees an easy error-catching solution I could sandwich into my function calls to deal with inability to tokenize some apparently erroneous, malformed documents?  </p>\n",
    "score": 6,
    "creation_date": 1360773111,
    "view_count": 4578,
    "answer_count": 3,
    "tags": "r;nlp;tm"
  },
  {
    "question_id": 5168342,
    "title": "Mallet topic modelling",
    "body": "<p>I have been using mallet for inferring topics for a text file  containing 100,000 lines(around 34 MB in mallet format). But now i need to run it for on a file containing a million lines(around 180MB) and I am getting an java.lang.outofmemory exception . Is there a way of splitting the file into smaller ones and build a model for the data present in all the files combined??\nthanks in advance  </p>\n",
    "score": 6,
    "creation_date": 1299073738,
    "view_count": 2409,
    "answer_count": 5,
    "tags": "java;nlp;machine-learning;mallet"
  },
  {
    "question_id": 2528284,
    "title": "Constructing human readable sentences based on a survey",
    "body": "<p>The following is a survey given to course attendees to assess an instructor at the end of the course.</p>\n\n<pre><code>Communication Skills\n1. The instructor communicated course material clearly and accurately.\nYes No\n2. The instructor explained course objectives and learning outcomes.\nYes No\n3. In the event of not understanding course materials the instructor was available outside of class.\nYes No\n4. Was instructor feedback and grading process clear and helpful?\nYes No\n5. Do you feel that your oral and written skills have improved while in this course?\nYes No\n</code></pre>\n\n<p>We would like to summarize each attendees selection based on the choices chosen by him.</p>\n\n<p>If the provided answers were [No, No, Yes, Yes, Yes]. Then we would summarize this as \"The instructor was not able to summarize course objectives and learning outcomes clearly, but was available and usually helpful outside of class. The instructor feedback and grading process was clear and helpful and I feel that my oral and written skills have improved because of this course.</p>\n\n<p>Based on the selections chosen by the attendee the summary would be quite different. This leads to many answers based on the choices selected and the number of such questions in the survey. The questions are usually provided by the training organization. How do you come up with a generic solution so that this can be effectively translated into a human readable form. I am looking for tools or libraries (java based), suggestions which will help me create such human readable output. I would like to hide the complexity from the end users as much as possible.</p>\n",
    "score": 6,
    "creation_date": 1269668783,
    "view_count": 434,
    "answer_count": 7,
    "tags": "java;parsing;nlp;semantics"
  },
  {
    "question_id": 70954157,
    "title": "ModuleNotFoundError: No module named &#39;milvus&#39;",
    "body": "<p>Goal: to run this Auto Labelling <a href=\"https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial16_Document_Classifier_at_Index_Time.ipynb\" rel=\"nofollow noreferrer\">Notebook</a> on AWS SageMaker Jupyter Labs.</p>\n<p>Kernels tried: <code>conda_pytorch_p36</code>, <code>conda_python3</code>, <code>conda_amazonei_mxnet_p27</code>.</p>\n<hr />\n<pre><code>! pip install farm-haystack -q\n# Install the latest master of Haystack\n!pip install grpcio-tools==1.34.1 -q\n!pip install git+https://github.com/deepset-ai/haystack.git -q\n!wget --no-check-certificate https://dl.xpdfreader.com/xpdf-tools-linux-4.03.tar.gz\n!tar -xvf xpdf-tools-linux-4.03.tar.gz &amp;&amp; sudo cp xpdf-tools-linux-4.03/bin64/pdftotext /usr/local/bin\n!pip install git+https://github.com/deepset-ai/haystack.git -q\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code># Here are the imports we need\nfrom haystack.document_stores.elasticsearch import ElasticsearchDocumentStore\nfrom haystack.nodes import PreProcessor, TransformersDocumentClassifier, FARMReader, ElasticsearchRetriever\nfrom haystack.schema import Document\nfrom haystack.utils import convert_files_to_dicts, fetch_archive_from_http, print_answers\n</code></pre>\n<p>Traceback:</p>\n<pre><code>02/02/2022 10:36:29 - INFO - faiss.loader -   Loading faiss with AVX2 support.\n02/02/2022 10:36:29 - INFO - faiss.loader -   Could not load library with AVX2 support due to:\nModuleNotFoundError(&quot;No module named 'faiss.swigfaiss_avx2'&quot;,)\n02/02/2022 10:36:29 - INFO - faiss.loader -   Loading faiss.\n02/02/2022 10:36:29 - INFO - faiss.loader -   Successfully loaded faiss.\n02/02/2022 10:36:33 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-4-6ff421127e9c&gt; in &lt;module&gt;\n      1 # Here are the imports we need\n----&gt; 2 from haystack.document_stores.elasticsearch import ElasticsearchDocumentStore\n      3 from haystack.nodes import PreProcessor, TransformersDocumentClassifier, FARMReader, ElasticsearchRetriever\n      4 from haystack.schema import Document\n      5 from haystack.utils import convert_files_to_dicts, fetch_archive_from_http, print_answers\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/__init__.py in &lt;module&gt;\n      3 import pandas as pd\n      4 from haystack.schema import Document, Label, MultiLabel, BaseComponent\n----&gt; 5 from haystack.finder import Finder\n      6 from haystack.pipeline import Pipeline\n      7 \n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/finder.py in &lt;module&gt;\n      6 from collections import defaultdict\n      7 \n----&gt; 8 from haystack.reader.base import BaseReader\n      9 from haystack.retriever.base import BaseRetriever\n     10 from haystack import MultiLabel\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/reader/__init__.py in &lt;module&gt;\n----&gt; 1 from haystack.reader.farm import FARMReader\n      2 from haystack.reader.transformers import TransformersReader\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/reader/farm.py in &lt;module&gt;\n     22 \n     23 from haystack import Document\n---&gt; 24 from haystack.document_store.base import BaseDocumentStore\n     25 from haystack.reader.base import BaseReader\n     26 \n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/document_store/__init__.py in &lt;module&gt;\n      2 from haystack.document_store.faiss import FAISSDocumentStore\n      3 from haystack.document_store.memory import InMemoryDocumentStore\n----&gt; 4 from haystack.document_store.milvus import MilvusDocumentStore\n      5 from haystack.document_store.sql import SQLDocumentStore\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/document_store/milvus.py in &lt;module&gt;\n      5 import numpy as np\n      6 \n----&gt; 7 from milvus import IndexType, MetricType, Milvus, Status\n      8 from scipy.special import expit\n      9 from tqdm import tqdm\n\nModuleNotFoundError: No module named 'milvus'\n</code></pre>\n<hr />\n<pre><code>pip install milvus\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>import milvus\n</code></pre>\n<p>Traceback:</p>\n<pre><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-3-91c33e248077&gt; in &lt;module&gt;\n----&gt; 1 import milvus\n\nModuleNotFoundError: No module named 'milvus'\n</code></pre>\n",
    "score": 6,
    "creation_date": 1643798438,
    "view_count": 8970,
    "answer_count": 2,
    "tags": "elasticsearch;nlp;document-classification;milvus;haystack"
  },
  {
    "question_id": 70932129,
    "title": "How to extract Bold text from pdf using python?",
    "body": "<p><strong>The list below provides examples of items and services that should not be billed separately. Please note that the list is not all inclusive.</strong></p>\n<p><strong>1. Surgical rooms and services</strong> – To include surgical suites, major and minor, treatment rooms,\nendoscopy labs, cardiac cath labs, X-ray.</p>\n<p><strong>2. Facility Basic Charges</strong> - pulmonary and cardiology procedural rooms. The hospital’s\ncharge for surgical suites and services shall include the entire above listed nursing personnel services, supplies, and equipment</p>\n<p>I want output like:</p>\n<ol>\n<li>Surgical rooms and services</li>\n<li>Facility Basic Charges</li>\n</ol>\n<p>there is first sentence also bold but we need to omit that sentence, we need to extract only those text which are represented with numbers</p>\n",
    "score": 6,
    "creation_date": 1643659202,
    "view_count": 9576,
    "answer_count": 2,
    "tags": "python;nlp;python-re"
  },
  {
    "question_id": 67851322,
    "title": "How to test masked language model after training it?",
    "body": "<p>I have followed this tutorial for masked language modelling from Hugging Face using BERT, but I am unsure how to actually deploy the model.</p>\n<p>Tutorial: <a href=\"https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb\" rel=\"noreferrer\">https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb</a></p>\n<p>I have trained the model using my own dataset, which has worked fine, but I don't know how to actually use the model, as the notebook does not include an example on how to do this, sadly.</p>\n<p><a href=\"https://i.sstatic.net/dLyex.png\" rel=\"noreferrer\">Example of what I want to do with my trained model</a></p>\n<p>On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my model:</p>\n<pre><code>&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='bert-base-uncased')\n&gt;&gt;&gt; unmasker(&quot;Hello I'm a [MASK] model.&quot;)\n\n[{'sequence': &quot;[CLS] hello i'm a fashion model. [SEP]&quot;,\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': &quot;[CLS] hello i'm a role model. [SEP]&quot;,\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': &quot;[CLS] hello i'm a new model. [SEP]&quot;,\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': &quot;[CLS] hello i'm a super model. [SEP]&quot;,\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': &quot;[CLS] hello i'm a fine model. [SEP]&quot;,\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}\n</code></pre>\n<p>Any help on how to do this would be great.</p>\n",
    "score": 6,
    "creation_date": 1622908188,
    "view_count": 3328,
    "answer_count": 2,
    "tags": "python;nlp;bert-language-model;huggingface-transformers"
  },
  {
    "question_id": 67192945,
    "title": "nltk.corpus - &#39;getset_descriptor&#39; object has no attribute &#39;setdefault&#39;",
    "body": "<p>I am using below code and importing stop words from nltk</p>\n<pre><code>   #from nltk.corpus import words as word_corp\n    from nltk.corpus import stopwords\n    nlp = spacy.load('en_core_web_sm')\n    phrase_matcher = PhraseMatcher(nlp.vocab)\n    en_words = nltk.corpus.words.words('en')\n    stop_words = stopwords.words('english')\n</code></pre>\n<p>But error is <code>AttributeError: 'getset_descriptor' object has no attribute 'setdefault' for ----&gt; 3 nlp = spacy.load('en_core_web_sm')</code> this line.</p>\n",
    "score": 6,
    "creation_date": 1618997102,
    "view_count": 2212,
    "answer_count": 1,
    "tags": "python-3.x;nlp;nltk;corpus"
  },
  {
    "question_id": 64356953,
    "title": "Batch-wise beam search in pytorch",
    "body": "<p>I'm trying to implement a beam search decoding strategy in a text generation model. This is the function that I am using to decode the output probabilities.</p>\n<pre><code>def beam_search_decoder(data, k):\n    sequences = [[list(), 0.0]]\n    # walk over each step in sequence\n    for row in data:\n        all_candidates = list()\n        for i in range(len(sequences)):\n            seq, score = sequences[i]\n            for j in range(len(row)):\n                candidate = [seq + [j], score - torch.log(row[j])]\n                all_candidates.append(candidate)\n        # sort candidates by score\n        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n        sequences = ordered[:k]\n    return sequences\n</code></pre>\n<p>Now you can see this function is implemented with batch_size 1 in mind. Adding another loop for batch size would make the algorithm <code>O(n^4)</code>. It is slow as it is now. Is there any way to improve the speed of this function. My model output is usually of the size <code>(32, 150, 9907)</code> which follows the format <code>(batch_size, max_len, vocab_size)</code></p>\n",
    "score": 6,
    "creation_date": 1602690178,
    "view_count": 9950,
    "answer_count": 3,
    "tags": "python;deep-learning;nlp;pytorch;beam-search"
  },
  {
    "question_id": 48443999,
    "title": "gensim - Word2vec continue training on existing model - AttributeError: &#39;Word2Vec&#39; object has no attribute &#39;compute_loss&#39;",
    "body": "<p>I am trying to continue training on an existing model,</p>\n\n<pre><code>model = gensim.models.Word2Vec.load('model/corpus.zhwiki.word.model')\nmore_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']]    \nmodel.build_vocab(more_sentences, update=True)\nmodel.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)\n</code></pre>\n\n<p>but I got an error with the last line:</p>\n\n<p>AttributeError: 'Word2Vec' object has no attribute 'compute_loss'</p>\n\n<p>Some posts said it's caused by using a earlier version of gensim, and I have tried to add this after loading the existing model and before train(). </p>\n\n<pre><code>model.compute_loss = False\n</code></pre>\n\n<p>After that, it didn't give me the AttributeError, but the output of model.train() is 0, and model didn't trained with new sentences.</p>\n\n<p><a href=\"https://i.sstatic.net/ECtw0.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/ECtw0.png\" alt=\"enter image description here\"></a></p>\n\n<p>How to solve this problem?</p>\n",
    "score": 6,
    "creation_date": 1516886886,
    "view_count": 5473,
    "answer_count": 2,
    "tags": "python;nlp;word2vec;gensim"
  },
  {
    "question_id": 43506531,
    "title": "Using British National Corpus in NLTK",
    "body": "<p>I am new to NLTK (<a href=\"http://www.nltk.org/\" rel=\"noreferrer\">http://www.nltk.org/</a>), and python for that matter.  I wish to use the NLTK python library, but use the BNC for the corpus.  I do not believe this corpus is distributed through the NLTK Data download.  Is there a way to import the BNC corpus to be used by NLTK.  If so, how?  I did find a function called BNCCorpusReader but have no idea how to use it.  Also, at the BNC site, I was able to download the corpus (<a href=\"http://ota.ox.ac.uk/desc/2554\" rel=\"noreferrer\">http://ota.ox.ac.uk/desc/2554</a>).</p>\n\n<p><a href=\"http://www.nltk.org/api/nltk.corpus.reader.html?highlight=bnc#nltk.corpus.reader.BNCCorpusReader.word\" rel=\"noreferrer\">http://www.nltk.org/api/nltk.corpus.reader.html?highlight=bnc#nltk.corpus.reader.BNCCorpusReader.word</a></p>\n\n<p><strong>Update</strong></p>\n\n<p>I have tried entrophy's suggestion, but get the following error:</p>\n\n<pre><code>raise IOError('No such file or directory: %r' % _path)\nOSError: No such file or directory: 'C:\\\\Users\\\\jason\\\\Documents\\\\NetBeansProjects\\\\DemoCollocations\\\\src\\\\Corpora\\\\bnc\\\\A\\\\A0\\\\A00.xml'\n</code></pre>\n\n<p>My code to read in the corpora:</p>\n\n<pre><code>bnc_reader = BNCCorpusReader(root=\"Corpora/bnc\", fileids=r'[A-K]/\\w*/\\w*\\.xml')\n</code></pre>\n\n<p>And by corpora is located in:\nC:\\Users\\jason\\Documents\\NetBeansProjects\\DemoCollocations\\src\\Corpora\\bnc\\</p>\n",
    "score": 6,
    "creation_date": 1492637867,
    "view_count": 2895,
    "answer_count": 1,
    "tags": "python-3.x;nlp;nltk"
  },
  {
    "question_id": 28674417,
    "title": "How to read constituency based parse tree",
    "body": "<p>I have a corpus of sentences that were preprocessed by Stanford's <a href=\"http://nlp.stanford.edu/software/corenlp.shtml\" rel=\"noreferrer\">CoreNLP</a> systems. One of the things it provides is the sentence's Parse Tree (Constituency-based). While I can understand a parse tree when it's drawn (like a tree), I'm not sure how to read it in this format:</p>\n\n<p>E.g.:</p>\n\n<pre><code>          (ROOT\n          (FRAG\n          (NP (NN sent28))\n          (: :)\n          (S\n          (NP (NNP Rome))\n          (VP (VBZ is)\n          (PP (IN in)\n          (NP\n          (NP (NNP Lazio) (NN province))\n          (CC and)\n          (NP\n          (NP (NNP Naples))\n          (PP (IN in)\n          (NP (NNP Campania))))))))\n          (. .)))\n</code></pre>\n\n<p>The original sentence is:</p>\n\n<pre><code>sent28: Rome is in Lazio province and Naples in Campania .\n</code></pre>\n\n<p>How am I supposed to read this tree, or alternatively, is there a code (in python) that does it properly?\nThanks.</p>\n",
    "score": 6,
    "creation_date": 1424696445,
    "view_count": 4609,
    "answer_count": 3,
    "tags": "python;parsing;nlp;parse-tree"
  },
  {
    "question_id": 25154231,
    "title": "Updating the feature names into scikit TFIdfVectorizer",
    "body": "<p>I am trying out this code</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ntrain_data = [\"football is the sport\",\"gravity is the movie\", \"education is imporatant\"]\nvectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n                                                 stop_words='english')\n\nprint \"Applying first train data\"\nX_train = vectorizer.fit_transform(train_data)\nprint vectorizer.get_feature_names()\n\nprint \"\\n\\nApplying second train data\"\ntrain_data = [\"cricket\", \"Transformers is a film\",\"AIMS is a college\"]\nX_train = vectorizer.transform(train_data)\nprint vectorizer.get_feature_names()\n\nprint \"\\n\\nApplying fit transform onto second train data\"\nX_train = vectorizer.fit_transform(train_data)\nprint vectorizer.get_feature_names()\n</code></pre>\n\n<p>The output for this one is</p>\n\n<pre><code>Applying first train data\n[u'education', u'football', u'gravity', u'imporatant', u'movie', u'sport']\n\n\nApplying second train data\n[u'education', u'football', u'gravity', u'imporatant', u'movie', u'sport']\n\n\n Applying fit transform onto second train data\n[u'aims', u'college', u'cricket', u'film', u'transformers']\n</code></pre>\n\n<p>I gave the first set of data using fit_transform to vectorizer so it gave me feature names like  <code>[u'education', u'football', u'gravity', u'imporatant', u'movie', u'sport']</code> after that i applied another train set to the same vectorizer but it gave me the same feature names as I didnt use fit or fit_transform. But I want to know how to update the features of a vectorizer without overwriting the previous oncs. If I use fit_transform again the previous features will get overwritten. So I want to update the feature list of the vectorizer. So i want something like <code>[u'education', u'football', u'gravity', u'imporatant', u'movie', u'sport',u'aims', u'college', u'cricket', u'film', u'transformers']</code> How can I get  that. </p>\n",
    "score": 6,
    "creation_date": 1407308866,
    "view_count": 3904,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;scikit-learn"
  },
  {
    "question_id": 22469506,
    "title": "Are there any efficient python libraries for Dynamic Topic Models, preferably extending Gensim?",
    "body": "<p>I'm trying to model twitter stream data with topic models. Gensim, being an easy to use solution, is impressive in it's simplicity. It has a truly online implementation for LSI, but not for LDA. For a changing content stream like twitter, Dynamic Topic Models are ideal. Is there any way, or even a hack - an implementation or even a strategy, using which I can utilize Gensim for this purpose?</p>\n\n<p>Are there any other python implementations which derive (preferably) from Gensim or independent? I am preferring python, since I want to get started asap, but if there is an optimum solution with some work, please mention it.</p>\n\n<p>Thanks.</p>\n",
    "score": 6,
    "creation_date": 1395111172,
    "view_count": 3972,
    "answer_count": 3,
    "tags": "python;lda;text-analysis;topic-modeling;gensim"
  },
  {
    "question_id": 20968237,
    "title": "How to download datasets for sklearn? - python",
    "body": "<p>In NLTK there is a <code>nltk.download()</code> function to download the datasets that are comes with the NLP suite.</p>\n\n<p>In sklearn, it talks about loading data sets (<a href=\"http://scikit-learn.org/stable/datasets/\" rel=\"noreferrer\">http://scikit-learn.org/stable/datasets/</a>) and fetching datas from <a href=\"http://mldata.org/\" rel=\"noreferrer\">http://mldata.org/</a> but for the rest of the datasets, the instructions were to download from the source.</p>\n\n<p><strong>Where should I save the data that I've downloaded from the source?</strong> Are there any other steps after I save the data into the correct directory before I can call from my python code?</p>\n\n<p><strong>Is there an example of how to download e.g. the <code>20newsgroups</code> dataset?</strong></p>\n\n<p>I've pip installed sklearn and tried this but I got an <code>IOError</code>. Most probably because I haven't downloaded the dataset from the source.</p>\n\n<pre><code>&gt;&gt;&gt; from sklearn.datasets import fetch_20newsgroups\n&gt;&gt;&gt; fetch_20newsgroups(subset='train')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/datasets/twenty_newsgroups.py\", line 207, in fetch_20newsgroups\n    cache_path=cache_path)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/datasets/twenty_newsgroups.py\", line 89, in download_20newsgroups\n    tarfile.open(archive_path, \"r:gz\").extractall(path=target_dir)\n  File \"/usr/lib/python2.7/tarfile.py\", line 1678, in open\n    return func(name, filemode, fileobj, **kwargs)\n  File \"/usr/lib/python2.7/tarfile.py\", line 1727, in gzopen\n    **kwargs)\n  File \"/usr/lib/python2.7/tarfile.py\", line 1705, in taropen\n    return cls(name, mode, fileobj, **kwargs)\n  File \"/usr/lib/python2.7/tarfile.py\", line 1574, in __init__\n    self.firstmember = self.next()\n  File \"/usr/lib/python2.7/tarfile.py\", line 2334, in next\n    raise ReadError(\"empty file\")\ntarfile.ReadError: empty file\n</code></pre>\n",
    "score": 6,
    "creation_date": 1389087525,
    "view_count": 7226,
    "answer_count": 1,
    "tags": "python;machine-learning;dataset;nlp;scikit-learn"
  },
  {
    "question_id": 17990673,
    "title": "Analyse the sentences and extract person name, organization and location with the help of NLP",
    "body": "<p>I need to solve the following using NLP, can you give me pointers on how to achieve this using OpenNLP API</p>\n\n<p>a. How to find out if a sentence implies a certain action in the past, present or future.</p>\n\n<pre><code>(e.g.) I was very sad last week - past\n       I feel like hitting my neighbor - present\n       I am planning to go to New York next week - future\n</code></pre>\n\n<p>b. How to find the word which corresponds to a person or company or country</p>\n\n<pre><code>(e.g.) John is planning to specialize in Electrical Engineering in UC Berkley and pursue a career with IBM).\n</code></pre>\n\n<p>Person = John</p>\n\n<p>Company = IBM</p>\n\n<p>Location = Berkley</p>\n\n<p>Thanks</p>\n",
    "score": 6,
    "creation_date": 1375350628,
    "view_count": 8635,
    "answer_count": 2,
    "tags": "java;nlp;stanford-nlp;opennlp"
  },
  {
    "question_id": 13853584,
    "title": "list of english verbs and their tenses, various forms, etc",
    "body": "<p>Is there a huge CSV/XML or whatever file somewhere that contains a list of english verbs and their variations (e.g sell -> sold, sale, selling, seller, sellee)?</p>\n\n<p>I imagine this will be useful for NLP systems, but there doesn't seem to be a listing anywhere, or it could be my terrible googling skills. Does anybody have a clue otherwise?</p>\n",
    "score": 6,
    "creation_date": 1355376786,
    "view_count": 4135,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 11406657,
    "title": "python nltk keyword extraction from sentence",
    "body": "<blockquote>\n  <p>\"First thing we do, let's kill all the lawyers.\" - <em>William Shakespeare</em></p>\n</blockquote>\n\n<p>Given the quote above, I would like to pull out <code>\"kill\"</code> and <code>\"lawyers\"</code> as the two prominent keywords to describe the overall meaning of the sentence. I have extracted the following noun/verb POS tags:</p>\n\n<pre><code>[[\"First\", \"NNP\"], [\"thing\", \"NN\"], [\"do\", \"VBP\"], [\"lets\", \"NNS\"], [\"kill\", \"VB\"], [\"lawyers\", \"NNS\"]]\n</code></pre>\n\n<p>The more general problem I am trying to solve is to distill a sentence to the \"most important\"* words/tags to summarise the overall \"meaning\"* of a sentence.</p>\n\n<p>*note the scare quotes. I acknowledge this is a very hard problem and there is most likely no perfect solution at this point in time. Nonetheless, I am interested to see attempts at solving the specific problem (extracting <code>\"kill\"</code> and <code>\"lawyers\"</code>) and the general problem (summarising the overall meaning of a sentence in keywords/tags)</p>\n",
    "score": 6,
    "creation_date": 1341894786,
    "view_count": 16872,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 10325197,
    "title": "Latent Semantic Analysis in Python discrepancy",
    "body": "<p>I'm trying to follow the <a href=\"http://en.wikipedia.org/wiki/Latent_semantic_analysis\" rel=\"nofollow\">Wikipedia Article on latent semantic indexing</a> in Python using the following code:</p>\n\n<pre><code>documentTermMatrix = array([[ 0.,  1.,  0.,  1.,  1.,  0.,  1.],\n                            [ 0.,  1.,  1.,  0.,  0.,  0.,  0.],\n                            [ 0.,  0.,  0.,  0.,  0.,  1.,  1.],\n                            [ 0.,  0.,  0.,  1.,  0.,  0.,  0.],\n                            [ 0.,  1.,  1.,  0.,  0.,  0.,  0.],\n                            [ 1.,  0.,  0.,  1.,  0.,  0.,  0.],\n                            [ 0.,  0.,  0.,  0.,  1.,  1.,  0.],\n                            [ 0.,  0.,  1.,  1.,  0.,  0.,  0.],\n                            [ 1.,  0.,  0.,  1.,  0.,  0.,  0.]])\nu,s,vt = linalg.svd(documentTermMatrix, full_matrices=False)\n\nsigma = diag(s)\n## remove extra dimensions...\nnumberOfDimensions = 4\nfor i in range(4, len(sigma) -1):\n    sigma[i][i] = 0\nqueryVector = array([[ 0.], # same as first column in documentTermMatrix\n                     [ 0.],\n                     [ 0.],\n                     [ 0.],\n                     [ 0.],\n                     [ 1.],\n                     [ 0.],\n                     [ 0.],\n                     [ 1.]])\n</code></pre>\n\n<p>How the math says it should work:</p>\n\n<pre><code>dtMatrixToQueryAgainst = dot(u, dot(s,vt))\nqueryVector = dot(inv(s), dot(transpose(u), queryVector))\nsimilarityToFirst = cosineDistance(queryVector, dtMatrixToQueryAgainst[:,0]\n# gives 'matrices are not aligned' error.  should be 1 because they're the same\n</code></pre>\n\n<p>What does work, with math that looks incorrect: ( from <a href=\"http://www.gototheboard.com/articles/An_Example_of_Latent_Semantic_Indexing\" rel=\"nofollow\">here</a>)</p>\n\n<pre><code>dtMatrixToQueryAgainst = dot(s, vt)\nqueryVector  = dot(transpose(u), queryVector)\nsimilarityToFirst = cosineDistance(queryVector, dtMatrixToQueryAgainsst[:,0]) \n# gives 1, which is correct\n</code></pre>\n\n<p>Why does route work, and the first not, when everything I can find about the math of LSA shows the first as correct?  I feel like I'm missing something obvious...</p>\n",
    "score": 6,
    "creation_date": 1335396286,
    "view_count": 1588,
    "answer_count": 1,
    "tags": "python;numpy;nlp;scipy;latent-semantic-indexing"
  },
  {
    "question_id": 6984264,
    "title": "Word Net - Word Synonyms &amp; related word constructs - Java or Python",
    "body": "<p>I am looking to use WordNet to look for a collection of like terms from a base set of terms. </p>\n\n<p>For example, the word <strong><em>'discouraged'</em></strong> - potential synonyms could be: <code>daunted, glum, deterred, pessimistic</code>. </p>\n\n<p>I also wanted to identify potential bi-grams such as; <code>beat down, put off, caved in</code> etc.</p>\n\n<p>How do I go about extracting this information using Java or Python? Are there any hosted WordNet databases/web interfaces which would allow such querying?</p>\n\n<p>Thanks!</p>\n",
    "score": 6,
    "creation_date": 1312816232,
    "view_count": 2633,
    "answer_count": 3,
    "tags": "java;python;nlp;text-mining;wordnet"
  },
  {
    "question_id": 5867129,
    "title": "Best path to get into natural language processing",
    "body": "<p>Currently I'm a PHP programmer and I would like to know the best way to learn about NLP, from theory to practice. Doesn't matter the language.</p>\n\n<p>For example:</p>\n\n<p><strong>Theory</strong> </p>\n\n<ol>\n<li>Firt learn the Basic grammar</li>\n<li>Then learn about first order logic, Description logic etc.</li>\n</ol>\n\n<p><strong>Technical</strong></p>\n\n<ol>\n<li>Learn PROLOG</li>\n<li>Learn about openCyc</li>\n<li>For web applications you can use prolog and python with the library pylog.</li>\n</ol>\n\n<p>Is it possible to create a topic based tutorial?</p>\n",
    "score": 6,
    "creation_date": 1304412407,
    "view_count": 2522,
    "answer_count": 2,
    "tags": "nlp"
  },
  {
    "question_id": 2178745,
    "title": "English verb inflector",
    "body": "<p>Does anybody know of an English verb inflector that I can use on a lexicon of verbs (in present-participle) that can give me other inflected forms of the verbs?</p>\n\n<p>For example:</p>\n\n<pre><code>I give it     I get\n=========     ======================================\nrun           ran, running, runs\nsing          sang, singing, sings\nplay          played, playing, plays\n</code></pre>\n",
    "score": 6,
    "creation_date": 1265045944,
    "view_count": 861,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 76921252,
    "title": "AttributeError: module &#39;chromadb&#39; has no attribute &#39;config&#39;",
    "body": "<p>so i recently started to work on chromabd and i am facing this error:\n&quot;module 'chromadb' has no attribute 'config'&quot;</p>\n<p>here is my code:</p>\n<pre><code>    from langchain.vectorstores import Chroma \n    from sentence_transformers import SentenceTransformer\n\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n\n    #Sentences are encoded by calling model.encode()\n    embeddings = [model.encode(text[i].page_content) for i in range(len(text))]\n\n    presist_directory = 'db'\n    vectordb =       Chroma.from_documents(documents=text,embedding=embeddings,persist_directory=presist_directory)\n</code></pre>\n<ol>\n<li>i have already tried down versioning chromadb</li>\n<li>tried this solution:</li>\n</ol>\n<pre><code>    from langchain.indexes import VectorstoreIndexCreator\n    from langchain.vectorstores import Chroma\n\n    presist_directory = 'db'\n    vectordb = VectorstoreIndexCreator().from_documents(\n        documents=text,\n        embedding=embeddings,\n        persist_directory=presist_directory,\n        vectorstore_cls=Chroma,\n    ) \n</code></pre>\n<ol start=\"3\">\n<li>and tried this solution too:</li>\n</ol>\n<pre><code>    import chromadb\n\n    # Get the version of ChromaDB\n    chroma_version = chromadb.__version__\n\n    # Check if the version is 0.4 or later\n    if float(chroma_version[:3]) &gt;= 0.4:\n        # Use the new configuration\n        _client_settings = chromadb.config.Settings(\n            chroma_db_impl=&quot;new_configuration&quot;,\n            persist_directory=persist_directory,\n        )\n    else:\n        # Use the old configuration\n        _client_settings = chromadb.config.Settings(\n            chroma_db_impl=&quot;duckdb+parquet&quot;,\n            persist_directory=persist_directory,\n        )\n\n</code></pre>\n",
    "score": 6,
    "creation_date": 1692273136,
    "view_count": 8187,
    "answer_count": 2,
    "tags": "python;nlp;chatbot;langchain;chromadb"
  },
  {
    "question_id": 66473977,
    "title": "Document Layout Analysis for text extraction",
    "body": "<p>I need to analyze the layout structure of different documents type like: <strong>pdf</strong>, <strong>doc</strong>, <strong>docx</strong>, <strong>odt</strong> etc.</p>\n<p>My task is:\nGiving a document, group the text in blocks finding the correct boundaries of each.</p>\n<p>I did some tests using Apache Tika, which is a good extractor, it is a very good tool but it often mess up the order of the block, let me explain a bit what i mean with ORDER.</p>\n<p>Apache Tika just extracts the text, so if my document has two columns, Tika extracts the entire text of the first column and then the text of the second column, which is ok...but sometimes the text on the first column is related to the text  on the second, like a table that has row relation.</p>\n<p>So i must take care of the positions of each block, so the problems are:</p>\n<ol>\n<li><p>Define the box boundaries, which is hard... i should understand if a sentence is starting a new block or not.</p>\n</li>\n<li><p>Define the <em>orientation</em>, for example, giving a table the &quot;sentence&quot; should be the row, NOT the column.</p>\n</li>\n</ol>\n<p>So basically here i have to deal with the <strong>layout structure</strong> to correcly understand the block boundaries.</p>\n<p>I give you a visual example:</p>\n<p><a href=\"https://i.sstatic.net/i6vHT.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/i6vHT.png\" alt=\"enter image description here\" /></a></p>\n<p>A classical extractor returns:</p>\n<pre><code>2019\n2018\n2017\n2016\n2015\n2014\nOregon Arts Commission Individual Artist Fellowship...\n</code></pre>\n<p>Which is <em><strong>wrong</strong></em> (in my case) because the dates are related to the texts on the right.</p>\n<p>This task is preparatory for other NLP analysis, so it is very important, because, for example doing, when i need to recognize the entities(NER) inside the text, and then identify their relations, <strong>working with the correct context is very important</strong>.</p>\n<p>How to extract the text from the document and assembly related pieces of text (understanding the layout structure of the document) under the same block?</p>\n",
    "score": 6,
    "creation_date": 1614856833,
    "view_count": 5784,
    "answer_count": 5,
    "tags": "python;machine-learning;nlp;artificial-intelligence;document-layout-analysis"
  },
  {
    "question_id": 64579258,
    "title": "Sentence embedding using T5",
    "body": "<p>I would like to use state-of-the-art LM T5 to get sentence embedding vector.\nI found this repository <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"noreferrer\">https://github.com/UKPLab/sentence-transformers</a>\nAs I know, in BERT I should take the first token as [CLS] token, and it will be the sentence embedding.\nIn this repository I see the same behaviour on T5 model:</p>\n<pre><code>cls_tokens = output_tokens[:, 0, :]  # CLS token is first token\n</code></pre>\n<p>Does this behaviour correct? I have taken encoder from T5 and encoded two phrases with it:</p>\n<pre><code>&quot;I live in the kindergarden&quot;\n&quot;Yes, I live in the kindergarden&quot;\n</code></pre>\n<p>The cosine similarity between them was only &quot;0.2420&quot;.</p>\n<p>I just need to understand how sentence embedding works - should I train network to find similarity to reach correct results? Or I it is enough of base pretrained language model?</p>\n",
    "score": 6,
    "creation_date": 1603910123,
    "view_count": 6973,
    "answer_count": 1,
    "tags": "python;nlp;pytorch;word-embedding"
  },
  {
    "question_id": 62743531,
    "title": "Using Gensim Fasttext model with LSTM nn in keras",
    "body": "<p>I have trained fasttext model with Gensim over the corpus of very short sentences (up to 10 words). I know that my test set includes words that are not in my train corpus, i.e some of the words in my corpus are like &quot;Oxytocin&quot; &quot;Lexitocin&quot;, &quot;Ematrophin&quot;,'Betaxitocin&quot;</p>\n<p>given a new word in the test set, fasttext knows pretty well to generate a vector with high cosine-similarity to the other similar words in the train set by using the characters level n-gram</p>\n<p>How do i incorporate the fasttext model inside a LSTM keras network without losing the fasttext model to just a list of vectors in the vocab? because then I won't handle any OOV even when fasttext do it well.</p>\n<p>Any idea?</p>\n",
    "score": 6,
    "creation_date": 1593967147,
    "view_count": 4093,
    "answer_count": 1,
    "tags": "tensorflow;keras;nlp;gensim;word-embedding"
  },
  {
    "question_id": 61787853,
    "title": "How to get the probability of a particular token(word) in a sentence given the context",
    "body": "<p>I'm trying to calculate the probability or any type of score for words in a sentence using NLP. I've tried this approach with GPT2 model using Huggingface Transformers library, but, I couldn't get satisfactory results due to the model's unidirectional nature which for me didn't seem to predict within context. So I was wondering whether there is a way, to calculate the above said using BERT since it's Bidirectional.</p>\n\n<p>I've found this <a href=\"https://github.com/huggingface/transformers/issues/473#issuecomment-482674742\" rel=\"noreferrer\">post</a> relatable, which I randomly saw the other day but didn't see any answer which would be useful for me as well.</p>\n\n<p>Hope I will be able to receive ideas or a solution for this. Any help is appreciated. Thank you. </p>\n",
    "score": 6,
    "creation_date": 1589420741,
    "view_count": 5721,
    "answer_count": 1,
    "tags": "nlp;pytorch;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 60188598,
    "title": "Accessing Google cloud API from local Project not Hosted on Google cloud platform",
    "body": "<p>I want to use google cloud natural language API from local python code. Because of project constraints I can not run my code on GCP Platform. I have google cloud account and credits to enable and use the API. Does google allow to use the API to run on local platforms. Any sample code will be helpful.</p>\n",
    "score": 6,
    "creation_date": 1581511273,
    "view_count": 1016,
    "answer_count": 3,
    "tags": "python;google-cloud-platform;nlp"
  },
  {
    "question_id": 59989449,
    "title": "Conditional word frequency count in Pandas",
    "body": "<p>I have a dataframe like below: </p>\n\n<pre><code>data = {'speaker':['Adam','Ben','Clair'],\n        'speech': ['Thank you very much and good afternoon.',\n                   'Let me clarify that because I want to make sure we have got everything right',\n                   'By now you should have some good rest']}\ndf = pd.DataFrame(data)\n</code></pre>\n\n<p>I want to count the number of words in the speech column but only for the words from a pre-defined list. For example, the list is: </p>\n\n<pre><code>wordlist = ['much', 'good','right']\n</code></pre>\n\n<p>I want to generate a new column which shows the frequency of these three words in each row. My expected output is therefore: </p>\n\n<pre><code>     speaker                   speech                               words\n0   Adam          Thank you very much and good afternoon.             2\n1   Ben        Let me clarify that because I want to make sur...      1\n2   Clair        By now you should have received a copy of our ...    1\n</code></pre>\n\n<p>I tried: </p>\n\n<pre><code>df['total'] = 0\nfor word in df['speech'].str.split():\n    if word in wordlist: \n        df['total'] += 1\n</code></pre>\n\n<p>But I after running it, the <code>total</code> column is always zero. I am wondering what's wrong with my code? </p>\n",
    "score": 6,
    "creation_date": 1580398502,
    "view_count": 1996,
    "answer_count": 3,
    "tags": "python;string;pandas;dataframe;nlp"
  },
  {
    "question_id": 57264594,
    "title": "How to split a sentence string into words, but also make punctuation a separate element",
    "body": "<p>I'm currently trying to tokenize some language data using Python and was curious if there was an efficient or built-in method for splitting strings of sentences into separate words and also separate punctuation characters. For example:</p>\n\n<pre><code>'Hello, my name is John. What's your name?'\n</code></pre>\n\n<p>If I used <code>split()</code> on this sentence then I would get</p>\n\n<pre><code>['Hello,', 'my', 'name', 'is', 'John.', \"What's\", 'your', 'name?']\n</code></pre>\n\n<p>What I want to get is:</p>\n\n<pre><code>['Hello', ',', 'my', 'name', 'is', 'John', '.', \"What's\", 'your', 'name', '?']\n</code></pre>\n\n<p>I've tried to use methods such as searching the string, finding punctuation, storing their indices, removing them from the string and then splitting the string, and inserting the punctuation accordingly but this method seems too inefficient especially when dealing with large corpora.</p>\n\n<p>Does anybody know if there's a more efficient way to do this?</p>\n\n<p>Thank you.</p>\n",
    "score": 6,
    "creation_date": 1564463008,
    "view_count": 8155,
    "answer_count": 5,
    "tags": "python;nlp;token"
  },
  {
    "question_id": 55531061,
    "title": "How can I create and fit vocab.bpe file (GPT and GPT2 OpenAI models) with my own corpus text?",
    "body": "<p>This question is for those who are familiar with GPT or <a href=\"https://github.com/openai/gpt-2\" rel=\"noreferrer\">GPT2</a> OpenAI models. In particular, with the encoding task (Byte-Pair Encoding). This is my problem:</p>\n\n<p>I would like to know how I could create my own vocab.bpe file.</p>\n\n<p>I have a spanish corpus text that I would like to use to fit my own bpe encoder. I have succeedeed in creating the encoder.json with the <a href=\"https://github.com/soaxelbrooke/python-bpe\" rel=\"noreferrer\">python-bpe</a> library, but I have no idea on how to obtain the vocab.bpe file.\nI have reviewed the code in <a href=\"https://github.com/openai/gpt-2/blob/master/src/encoder.py\" rel=\"noreferrer\">gpt-2/src/encoder.py</a> but, I have not been able to find any hint. Any help or idea?</p>\n\n<p>Thank you so much in advance.</p>\n",
    "score": 6,
    "creation_date": 1554452151,
    "view_count": 3222,
    "answer_count": 2,
    "tags": "python;encoding;nlp;gpt-2"
  },
  {
    "question_id": 52458404,
    "title": "Custom sentence segmentation in Spacy",
    "body": "<p>I want <code>spaCy</code> to use the sentence segmentation boundaries as I provide instead of its own processing. </p>\n\n<p>For example:</p>\n\n<pre><code>get_sentences(\"Bob meets Alice. @SentBoundary@ They play together.\")\n# =&gt; [\"Bob meets Alice.\", \"They play together.\"]  # two sents\n\nget_sentences(\"Bob meets Alice. They play together.\")\n# =&gt; [\"Bob meets Alice. They play together.\"]  # ONE sent\n\nget_sentences(\"Bob meets Alice, @SentBoundary@ they play together.\")\n# =&gt; [\"Bob meets Alice,\", \"they play together.\"] # two sents\n</code></pre>\n\n<p>This is what I have so far (borrowing things from documentation <a href=\"https://spacy.io/usage/processing-pipelines#component-example1\" rel=\"noreferrer\">here</a>):</p>\n\n<pre><code>import spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef mark_sentence_boundaries(doc):\n    for i, token in enumerate(doc):\n        if token.text == '@SentBoundary@':\n            doc[i+1].sent_start = True\n    return doc\n\nnlp.add_pipe(mark_sentence_boundaries, before='parser')\n\ndef get_sentences(text):\n    doc = nlp(text)\n    return (list(doc.sents))\n</code></pre>\n\n<p>But the results I get are as follows:</p>\n\n<pre><code># Ex1\nget_sentences(\"Bob meets Alice. @SentBoundary@ They play together.\")\n#=&gt; [\"Bob meets Alice.\", \"@SentBoundary@\", \"They play together.\"]\n\n# Ex2\nget_sentences(\"Bob meets Alice. They play together.\")\n#=&gt; [\"Bob meets Alice.\", \"They play together.\"]\n\n# Ex3\nget_sentences(\"Bob meets Alice, @SentBoundary@ they play together.\")\n#=&gt; [\"Bob meets Alice, @SentBoundary@\", \"they play together.\"]\n</code></pre>\n\n<p>Following are main problems I am facing:</p>\n\n<ol>\n<li>When sentence break is found, how to get rid of <code>@SentBoundary@</code> token.</li>\n<li>How to disallow <code>spaCy</code> from splitting if <code>@SentBoundary@</code> is not present.</li>\n</ol>\n",
    "score": 6,
    "creation_date": 1537632181,
    "view_count": 5919,
    "answer_count": 2,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 50770858,
    "title": "Python: How to compute Jaccard Similarity more quickly",
    "body": "<p>There are about 98,000 sentences (length from 5 - 100 words) in <code>lst_train</code> and about 1000 sentences (length from 5 - 100 words) in <code>lst_test</code>. For each sentence in <code>lst_test</code> I want to find if it's plagiarized from a sentence in <code>lst_train</code>. If the sentence is plagiarized, I should return the id in lst_train or else null.</p>\n\n<p>Now I want to compute the jaccard similarity of each sentence in <code>lst_test</code> relative to each sentence in <code>lst_train</code>. Here's my code, b.JaccardSim computes two sentences' jaccard similarity:</p>\n\n<pre><code>lst_all_p = []\nfor i in range(len(lst_test)):\n    print('i:', i)\n    lst_p = []\n    for j in range(len(lst_train)):\n        b = textSimilarity.TextSimilarity(lst_test[i], lst_train[j])\n        lst_p.append(b.JaccardSim(b.str_a,b.str_b))\n    lst_all_p.append(lst_p)\n</code></pre>\n\n<p>But I found that each time of computing one sentence with each sentence in lst_train is more than 1 minutes. Since there are about 1000 sentences, it may take about 1000 minutes to finish it. It is too long.</p>\n\n<p>Do you guys know how to make the computing speed faster or better method to solve the issue to detect the sentence is plagiarized from a sentence in lst_train?</p>\n",
    "score": 6,
    "creation_date": 1528517667,
    "view_count": 4093,
    "answer_count": 2,
    "tags": "python-3.x;nlp"
  },
  {
    "question_id": 49083826,
    "title": "Get trouble to load glove 840B 300d vector",
    "body": "<p>It seems the format is, for every line, the string is like 'word number number .....'. So it easy to split it.\nBut when I split them with the script below</p>\n\n<pre><code>import numpy as np\ndef loadGloveModel(gloveFile):\n    print \"Loading Glove Model\"\n    f = open(gloveFile,'r')\n    model = {}\n    for line in f:\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print \"Done.\",len(model),\" words loaded!\"\n    return model\n</code></pre>\n\n<p>I load the glove 840B 300d.txt. but get error and I print the splitLine I got</p>\n\n<pre><code>['contact', 'name@domain.com', '0.016426', '0.13728', '0.18781', '0.75784', '0.44012', '0.096794' ... ]\n</code></pre>\n\n<p>or</p>\n\n<pre><code>['.', '.', '.', '.', '0.033459', '-0.085658', '0.27155', ...]\n</code></pre>\n\n<p>Please notice that this script works fine in glove.6b.*</p>\n",
    "score": 6,
    "creation_date": 1520078057,
    "view_count": 6659,
    "answer_count": 2,
    "tags": "python;nlp;stanford-nlp;word2vec"
  },
  {
    "question_id": 47053698,
    "title": "Spacy Japanese Tokenizer",
    "body": "<p>I am trying to use Spacy's Japanese tokenizer. </p>\n\n<pre><code>import spacy\nQuestion= 'すぺいんへ いきました。'\nnlp(Question.decode('utf8'))\n</code></pre>\n\n<p>I am getting the below error, </p>\n\n<pre><code>TypeError: Expected unicode, got spacy.tokens.token.Token\n</code></pre>\n\n<p>Any ideas on how to fix this?</p>\n\n<p>Thanks!</p>\n",
    "score": 6,
    "creation_date": 1509535364,
    "view_count": 5346,
    "answer_count": 3,
    "tags": "python;nlp;spacy;cjk"
  },
  {
    "question_id": 39707654,
    "title": "What are &quot;derivationally related forms&quot; in WordNet?",
    "body": "<p>WordNet lemmas can have <em>derivationally related forms</em>. For instance, the noun \"butter\" (meaning the spread you put on bread) is deemed to be derivationally-related to the verb \"butter\" (meaning the act of spreading butter on bread):</p>\n\n\n\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn\n&gt;&gt;&gt;\n&gt;&gt;&gt; wn.lemma('butter.n.01.butter')\nLemma('butter.n.01.butter')\n&gt;&gt;&gt; wn.lemma('butter.n.01.butter').synset().definition()\nu'an edible emulsion of fat globules made by churning milk or cream; for cooking and table use'\n&gt;&gt;&gt; wn.lemma('butter.n.01.butter').derivationally_related_forms()\n[Lemma('butter.v.01.butter'), Lemma('buttery.s.02.buttery'), Lemma('butyraceous.a.01.butyraceous')]\n&gt;&gt;&gt; wn.lemma('butter.n.01.butter').derivationally_related_forms()[0]\nLemma('butter.v.01.butter')\n&gt;&gt;&gt; wn.lemma('butter.n.01.butter').derivationally_related_forms()[0].synset().definition()\nu'spread butter on'\n&gt;&gt;&gt;\n&gt;&gt;&gt; wn.lemma('flood.n.01.flood').synset().definition()\nu'the rising of a body of water and its overflowing onto normally dry land'\n&gt;&gt;&gt; wn.lemma('flood.n.01.flood').derivationally_related_forms()\n[Lemma('flood.v.04.flood'), Lemma('deluge.v.01.flood'), Lemma('flood.v.02.flood')]\n&gt;&gt;&gt; wn.lemma('flood.n.01.flood').derivationally_related_forms()[0]\nLemma('flood.v.04.flood')\n&gt;&gt;&gt; wn.lemma('flood.n.01.flood').derivationally_related_forms()[0].synset().definition()\nu'become filled to overflowing'\n&gt;&gt;&gt;\n</code></pre>\n\n<p>However, it's not clear precisely what the term \"derivationally related\" means. For instance, I could argue that \"television\" and \"telescope\" are \"derivationally related\", since both words are derived from the Ancient Greek \"têle\", meaning \"far\". But WordNet disagrees:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; wn.lemma('telescope.n.01.telescope').derivationally_related_forms()\n[Lemma('telescopic.s.01.telescopic'), Lemma('telescopic.s.02.telescopic')]\n</code></pre>\n\n<p>What precisely, then, is WordNet's definition of a <em>\"derivationally related form\"</em>? Is this documented anywhere?</p>\n",
    "score": 6,
    "creation_date": 1474906519,
    "view_count": 3157,
    "answer_count": 1,
    "tags": "nlp;wordnet"
  },
  {
    "question_id": 38976670,
    "title": "Syntactic similarity/distance between 2 sentences/string/text using nltk",
    "body": "<p>I have 2 texts as below</p>\n\n<p>Text1 : <strong>John likes apple</strong></p>\n\n<p>Text2 : <strong>Mike hates orange</strong></p>\n\n<p>If you check above 2 texts, both of them are similar syntactically but semantically have a different meaning.</p>\n\n<p>I want to find</p>\n\n<p>1) Syntactic distance between 2 texts</p>\n\n<p>2) Semantic distance between 2 texts</p>\n\n<p>Is there any way to do this using nltk, as I am newbie to NLP?</p>\n",
    "score": 6,
    "creation_date": 1471355219,
    "view_count": 3068,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;scikit-learn;nltk"
  },
  {
    "question_id": 38109869,
    "title": "Named Entity Recognition with Syntaxnet",
    "body": "<p>I am trying to understand and learn SyntaxNet. I am trying to figure out whether is there any way to use SyntaxNet for Name Entity Recognition of a corpus. Any sample code or helpful links would be appreciated. </p>\n",
    "score": 6,
    "creation_date": 1467233753,
    "view_count": 4045,
    "answer_count": 3,
    "tags": "nlp;tensorflow;syntaxnet"
  },
  {
    "question_id": 34968716,
    "title": "Why Stanford parser with nltk is not correctly parsing a sentence?",
    "body": "<p>I am using Stanford parser with nltk in python and got help from <a href=\"https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk\">Stanford Parser and NLTK</a>  to set up Stanford nlp libraries.</p>\n\n<pre><code>from nltk.parse.stanford import StanfordParser\nfrom nltk.parse.stanford import StanfordDependencyParser\nparser     = StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\ndep_parser = StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\none = (\"John sees Bill\")\nparsed_Sentence = parser.raw_parse(one)\n# GUI\nfor line in parsed_Sentence:\n       print line\n       line.draw()\n\nparsed_Sentence = [parse.tree() for parse in dep_parser.raw_parse(one)]\nprint parsed_Sentence\n\n# GUI\nfor line in parsed_Sentence:\n        print line\n        line.draw()\n</code></pre>\n\n<p>I am getting wrong parse and dependency trees as shown in the example below, it is treating 'sees' as noun instead of verb.</p>\n\n<p><a href=\"https://i.sstatic.net/I3hSv.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/I3hSv.png\" alt=\"Example parse tree\"></a>\n<a href=\"https://i.sstatic.net/C6gL8.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/C6gL8.png\" alt=\"Example dependency tree\"></a></p>\n\n<p>What should I do?\nIt work perfectly right when I change sentence e.g.(one = 'John see Bill').\nThe correct ouput for this sentence can be viewed from here <a href=\"https://stackoverflow.com/questions/10401076/difference-between-constituency-parser-and-dependency-parser\">correct ouput of parse tree</a></p>\n\n<p>Example of correct output is also shown below:</p>\n\n<p><a href=\"https://i.sstatic.net/68vBR.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/68vBR.png\" alt=\"correctly parsed\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/evbZ8.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/evbZ8.png\" alt=\"correct dependency parsed tree\"></a></p>\n",
    "score": 6,
    "creation_date": 1453582323,
    "view_count": 4653,
    "answer_count": 1,
    "tags": "python;parsing;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 33717131,
    "title": "Training Tagger with Custom Tags in NLTK",
    "body": "<p>I have a document with tagged data in the format <code>Hi here's my [KEYWORD phone number], let me know when you wanna hangout: [PHONE 7802708523]. I live in a [PROP_TYPE condo] in [CITY New York]</code>. I want to train a model based on a set of these type of tagged documents, and then use my model to tag new documents. Is this possible in NLTK? I have looked at <a href=\"http://www.cnts.ua.ac.be/conll2000/chunking/\" rel=\"noreferrer\">chunking</a> and <a href=\"https://github.com/japerk/nltk-trainer\" rel=\"noreferrer\">NLTK-Trainer</a> scripts, but these have a restricted set of tags and corpora, while my dataset has custom tags.</p>\n",
    "score": 6,
    "creation_date": 1447569475,
    "view_count": 5518,
    "answer_count": 2,
    "tags": "nlp;nltk;information-extraction;supervised-learning"
  },
  {
    "question_id": 33248791,
    "title": "Using scikit-learn to training an NLP log linear model for NER",
    "body": "<p>I wonder how to use <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\" rel=\"noreferrer\"><code>sklearn.linear_model.LogisticRegression</code></a> to train an NLP log linear model for named-entity recognition (NER).</p>\n\n<p>A typical log-linear model for defines a conditional probability as follows:</p>\n\n<p><a href=\"https://i.sstatic.net/4Yp7D.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/4Yp7D.png\" alt=\"enter image description here\"></a></p>\n\n<p>with:</p>\n\n<ul>\n<li>x: the current word</li>\n<li>y: the class of a word being considered</li>\n<li>f: the feature vector function, which maps a word x and a class y to a vector of scalars.</li>\n<li>v: the feature weight vector</li>\n</ul>\n\n<p>Can <code>sklearn.linear_model.LogisticRegression</code> train such a model?</p>\n\n<p>The issue is that features depend on the class.</p>\n",
    "score": 6,
    "creation_date": 1445385554,
    "view_count": 1848,
    "answer_count": 1,
    "tags": "nlp;scikit-learn"
  },
  {
    "question_id": 32262863,
    "title": "Subjectivity and objectivity detection",
    "body": "<p>I am trying to separate subjective and objective sentences from eachother, I tried to find some good research papers in this area but all of the work is in sentiment analysis and I could not find any good paper just focusing on subjectivity and objectivity of the text... So now my question is how should I interpret interpret subjective and objective text? Is it possible to have a subjective sentence which is neutral in terms of sentiment? or when I say \"I went to school\", is it subjective or objective(I assume it is subjective since it is not about general fact)?  </p>\n",
    "score": 6,
    "creation_date": 1440732332,
    "view_count": 2507,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 30832357,
    "title": "Language detection API/Library",
    "body": "<p>Is there a service/library (free or paid) that takes a piece of text and return the language of it?</p>\n\n<p>I need to go over a million blog posts and determine their languages.</p>\n",
    "score": 6,
    "creation_date": 1434302549,
    "view_count": 227,
    "answer_count": 2,
    "tags": "api;nlp"
  },
  {
    "question_id": 24007812,
    "title": "Can I control the way the CountVectorizer vectorizes the corpus in scikit learn?",
    "body": "<p>I am working with a CountVectorizer from scikit learn, and I'm possibly attempting to do some things that the object was not made for...but I'm not sure.</p>\n\n<p>In terms of getting counts for occurrence:</p>\n\n<pre><code>vocabulary = ['hi', 'bye', 'run away!']\ncorpus = ['run away!']\ncv = CountVectorizer(vocabulary=vocabulary)\nX = cv.fit_transform(corpus)\nprint X.toarray()\n</code></pre>\n\n<p>gives: </p>\n\n<pre><code>[[0 0 0 0]]\n</code></pre>\n\n<p>What I'm realizing is that the CountVectorizer will break the corpus into what I believe is unigrams:  </p>\n\n<pre><code>vocabulary = ['hi', 'bye', 'run']\ncorpus = ['run away!']\ncv = CountVectorizer(vocabulary=vocabulary)\nX = cv.fit_transform(corpus)\nprint X.toarray()\n</code></pre>\n\n<p>which gives:  </p>\n\n<pre><code>[[0 0 1]]\n</code></pre>\n\n<p>Is there any way to tell the CountVectorizer exactly how you'd like to vectorize the corpus? Ideally I would like an outcome along the lines of the first example.  </p>\n\n<p>In all honestly, however, I'm wondering if it is at all possible to get an outcome along these lines:  </p>\n\n<pre><code>vocabulary = ['hi', 'bye', 'run away!']\ncorpus = ['I want to run away!']\ncv = CountVectorizer(vocabulary=vocabulary)\nX = cv.fit_transform(corpus)\nprint X.toarray()\n\n[[0 0 1]]\n</code></pre>\n\n<p>I don't see much information in the documentation for the fit_transform method, which only takes one argument as it is.  If anyone has any ideas I would be grateful.  Thanks!</p>\n",
    "score": 6,
    "creation_date": 1401773762,
    "view_count": 2300,
    "answer_count": 1,
    "tags": "python;nlp;scikit-learn;text-parsing;corpus"
  },
  {
    "question_id": 23063980,
    "title": "Stanford CoreNLP sentiment",
    "body": "<p>I'm trying to implement the coreNLP sentiment analyzer in eclipse. Getting the error: </p>\n\n<pre><code>Unable to resolve \"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\"\n</code></pre>\n\n<p>As either class path, filename or URL. I installed all of the NLP files using maven so I am not sure why it is looking for something else. Here is the code I am getting the error on.</p>\n\n<pre><code>import java.util.Properties;\n\n\nimport edu.stanford.nlp.ling.CoreAnnotations;\nimport edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;\nimport edu.stanford.nlp.pipeline.Annotation;\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP;\nimport edu.stanford.nlp.sentiment.SentimentCoreAnnotations;\nimport edu.stanford.nlp.trees.Tree;\nimport edu.stanford.nlp.util.CoreMap;\n\npublic class StanfordSentiment {\n\n\nStanfordCoreNLP pipeline; \n\n\n\npublic StanfordSentiment(){\n    Properties props = new Properties();\n    props.setProperty(\"annotators\", \"tokenize, ssplit, parse, sentiment\");\n\n    pipeline = new StanfordCoreNLP(props);\n\n\n}\n\npublic float calculateSentiment (String text) {\n\n\n        float mainSentiment = 0;\n\n        int longest = 0;\n        Annotation annotation = pipeline.process(text);\n        for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {\n            Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);\n            int sentiment = RNNCoreAnnotations.getPredictedClass(tree) - 2;\n            String partText = sentence.toString();\n            if (partText.length() &gt; longest) {\n                mainSentiment = sentiment;\n                longest = partText.length();\n            }\n\n        }\n\n       return mainSentiment;\n\n\n\n}\n}\n</code></pre>\n",
    "score": 6,
    "creation_date": 1397489091,
    "view_count": 3455,
    "answer_count": 1,
    "tags": "java;dependencies;nlp;stanford-nlp"
  },
  {
    "question_id": 11920494,
    "title": "most efficient edit distance to identify misspellings in names?",
    "body": "<p>Algorithms for edit distance give a measure of the distance between two strings.</p>\n\n<p>Question: which of these measures would be most relevant to detect two different persons names which are actually the same? (different because of a mispelling). The trick is that it should minimize false positives. Example:</p>\n\n<p>Obaama\nObama\n=> should probably be merged</p>\n\n<p>Obama\nIbama\n=> should not be merged.</p>\n\n<p>This is just an oversimple example. Are their programmers and computer scientists who worked out this issue in more detail?</p>\n",
    "score": 6,
    "creation_date": 1344756865,
    "view_count": 1091,
    "answer_count": 2,
    "tags": "algorithm;nlp;spelling;edit-distance"
  },
  {
    "question_id": 7604492,
    "title": "NLP algorithm to &#39;fill out&#39; search terms",
    "body": "<p>I'm trying to write an algorithm (which I'm assuming will rely on natural language processing techniques) to 'fill out' a list of search terms. There is probably a name for this kind of thing which I'm unaware of. What is this kind of problem called, and what kind of algorithm will give me the following behavior?</p>\n\n<p>Input:</p>\n\n<pre><code>    docs = [\n    \"I bought a ticket to the Dolphin Watching cruise\",\n    \"I enjoyed the Dolphin Watching tour\",\n    \"The Miami Dolphins lost again!\",\n    \"It was good going to that Miami Dolphins game\"\n    ], \n    search_term = \"Dolphin\"\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[\"Dolphin Watching\", \"Miami Dolphins\"]\n</code></pre>\n\n<p>It should basically figure out that if \"Dolphin\" appears at all, it's virtually always either in the bigrams \"Dolphin Watching\" or \"Miami Dolphins\". Solutions in Python preferred.</p>\n",
    "score": 6,
    "creation_date": 1317339009,
    "view_count": 378,
    "answer_count": 2,
    "tags": "python;nlp;n-gram"
  },
  {
    "question_id": 7548479,
    "title": "Converting adjectives and adverbs to their noun forms",
    "body": "<p>I am experimenting with word sense disambiguation using wordnet for my project.  As a part of the project, I would like to convert a derived adjective or an adverb form to it's root noun form.</p>\n\n<p>For example</p>\n\n<p>beautiful ==> beauty\nwonderful ==> wonder</p>\n\n<ol>\n<li><p>How can I achieve this? Is there any other dict other than wordnet that provides this kind of transformation?</p></li>\n<li><p>It would be an added bonus for me if I can map the exact sense of the adjective word to its noun form with exact sense.  Is that possible?</p></li>\n</ol>\n\n<p>Thank you</p>\n",
    "score": 6,
    "creation_date": 1316983475,
    "view_count": 3526,
    "answer_count": 1,
    "tags": "python;nlp;wordnet;linguistics"
  },
  {
    "question_id": 4150722,
    "title": "Is there a library or web service that provides pronunciations for text?",
    "body": "<p>Is there a library or web service that can tell you the pronunciation of a string? I'm thinking of character-based languages, where the pronunciation of the word is not apparent from how it's written.</p>\n",
    "score": 6,
    "creation_date": 1289439349,
    "view_count": 3349,
    "answer_count": 3,
    "tags": "nlp;multilingual;text-to-speech"
  },
  {
    "question_id": 64904840,
    "title": "Why we need a decoder_start_token_id during generation in HuggingFace BART?",
    "body": "<p>During the generation phase in HuggingFace's code:\n<a href=\"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L88-L100\" rel=\"noreferrer\">https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L88-L100</a></p>\n<p>They pass in a <code>decoder_start_token_id</code>, I'm not sure why they need this. And in the BART config, the <code>decoder_start_token_id</code> is actually <code>2</code> (<a href=\"https://huggingface.co/facebook/bart-base/blob/main/config.json\" rel=\"noreferrer\">https://huggingface.co/facebook/bart-base/blob/main/config.json</a>), which is the end of sentence token <code>&lt;/s&gt;</code>.</p>\n<p>And I tried a simple example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import *\n\nimport torch\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\ninput_ids = torch.LongTensor([[0, 894, 213, 7, 334, 479, 2]])\nres = model.generate(input_ids, num_beams=1, max_length=100)\n\nprint(res)\n\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True).strip() for g in res]\nprint(preds)\n</code></pre>\n<p>The results I obtained:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>tensor([[  2,   0, 894, 213,   7, 334, 479,   2]])\n['He go to school.'] \n</code></pre>\n<p>Though it does not affect the final &quot;tokenization decoding&quot; results. But it seems weird to me that the first token we generate is actually <code>2</code>(<code>&lt;/s&gt;</code>).</p>\n",
    "score": 6,
    "creation_date": 1605755357,
    "view_count": 8356,
    "answer_count": 1,
    "tags": "nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 64823332,
    "title": "Gradients returning None in huggingface module",
    "body": "<p>I want to get the gradient of an embedding layer from a pytorch/huggingface model. Here's a minimal working example:</p>\n<pre><code>from transformers import pipeline\n\nnlp = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)\n\nresponses = [&quot;I'm having a great day!!&quot;]\nhypothesis_template = 'This person feels {}'\ncandidate_labels = ['happy', 'sad']\nnlp(responses, candidate_labels, hypothesis_template=hypothesis_template)\n</code></pre>\n<p>I can extract the logits just fine,</p>\n<pre><code>inputs = nlp._parse_and_tokenize(responses, candidate_labels, hypothesis_template)\npredictions = nlp.model(**inputs, return_dict=True, output_hidden_states=True)\npredictions['logits']   \n</code></pre>\n<p>and the model returns a layer I'm interested in. I tried to retain the gradient and backprop with respect to a single logit I'm interested in:</p>\n<pre class=\"lang-py prettyprint-override\"><code>layer = predictions['encoder_hidden_states'][0]\nlayer.retain_grad()\npredictions['logits'][0][2].backward(retain_graph=True)\n</code></pre>\n<p>However, <code>layer.grad == None</code> no matter what I try. The other named parameters of the model have their gradients computed, so I'm not sure what I'm doing wrong. How do I get the grad of the encoder_hidden_states?</p>\n",
    "score": 6,
    "creation_date": 1605280324,
    "view_count": 1474,
    "answer_count": 1,
    "tags": "python;nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 63380543,
    "title": "How many characters can be input into the &quot;prompt&quot; for GPT-2",
    "body": "<p>I'm using the OpenAI GPT-2 model from <a href=\"https://github.com/openai/gpt-2\" rel=\"noreferrer\">github</a></p>\n<p>I think that the top_k parameter dictates how many tokens are sampled. Is this also the parameter that dictates how large of a prompt can be given?</p>\n<p>If top_k = 40, how large can the prompt be?</p>\n",
    "score": 6,
    "creation_date": 1597248585,
    "view_count": 7205,
    "answer_count": 1,
    "tags": "python;nlp;openai-api;gpt-2"
  },
  {
    "question_id": 62357239,
    "title": "Add attention layer to Seq2Seq model",
    "body": "<p>I have build a Seq2Seq model of encoder-decoder. I want to add an attention layer to it. I tried adding attention layer <a href=\"https://www.kaggle.com/residentmario/seq-to-seq-rnn-models-attention-teacher-forcing\" rel=\"noreferrer\">through this</a> but it didn't help.</p>\n\n<p>Here is my initial code without attention</p>\n\n<pre><code># Encoder\nencoder_inputs = Input(shape=(None,))\nenc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\nencoder_lstm = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\ndec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\ndec_emb = dec_emb_layer(decoder_inputs)\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(dec_emb,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.summary()\n</code></pre>\n\n<p>And this is the code after I added attention layer in decoder (the encoder layer is same as in initial code)</p>\n\n<pre><code># Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\ndec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\ndec_emb = dec_emb_layer(decoder_inputs)\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\nattention = dot([decoder_lstm, encoder_lstm], axes=[2, 2])\nattention = Activation('softmax')(attention)\ncontext = dot([attention, encoder_lstm], axes=[2,1])\ndecoder_combined_context = concatenate([context, decoder_lstm])\ndecoder_outputs, _, _ = decoder_combined_context(dec_emb,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.summary()\n</code></pre>\n\n<p>While doing this, I got an error </p>\n\n<pre><code> Layer dot_1 was called with an input that isn't a symbolic tensor. Received type: &lt;class 'keras.layers.recurrent.LSTM'&gt;. Full input: [&lt;keras.layers.recurrent.LSTM object at 0x7f8f77e2f3c8&gt;, &lt;keras.layers.recurrent.LSTM object at 0x7f8f770beb70&gt;]. All inputs to the layer should be tensors.\n</code></pre>\n\n<p>Can someone please help in fitting an attention layer in this architecture?</p>\n",
    "score": 6,
    "creation_date": 1592036428,
    "view_count": 1535,
    "answer_count": 2,
    "tags": "python-3.x;tensorflow;keras;nlp;machine-translation"
  },
  {
    "question_id": 57744725,
    "title": "How to convert emojis/emoticons to their meanings in python?",
    "body": "<p>I am trying to clean up tweets to analyze their sentiments. I want to turn emojis to what they mean. </p>\n\n<p>For instance, I want my code to convert</p>\n\n<pre><code>'I ❤ New York' \n'Python is 👍'\n</code></pre>\n\n<p>to </p>\n\n<pre><code>'I love New York' \n'Python is cool'\n</code></pre>\n\n<p>I have seen packages such as <code>emoji</code> but they turn the emoji's to what they represent, not what they mean. for instance, they turn my tweets to :</p>\n\n<pre><code>print(emoji.demojize('Python is 👍'))\n'Python is :thumbs_up:'\n\nprint(emoji.demojize('I ❤ New York'))\n'I :heart: New York'\n</code></pre>\n\n<p>since \"heart\" or \"thumbs_up\" do not carry a positive or negative meaning in <code>textblob</code>, this kind of conversion is useless. But if \"❤\" is converted to \"love\", the results of sentiment analysis will improve drastically.</p>\n",
    "score": 6,
    "creation_date": 1567329583,
    "view_count": 10764,
    "answer_count": 2,
    "tags": "python;nlp;emoji;emoticons"
  },
  {
    "question_id": 53929134,
    "title": "How to handle text classification problems when multiple features are involved",
    "body": "<p>I am working on a text classification problem where multiple text features and need to build a model to predict salary range. Please refer the <a href=\"https://i.sstatic.net/MedzR.png\" rel=\"noreferrer\">Sample dataset</a>\nMost of the resources/tutorials deal with feature extraction on only one column and then predicting target. I am aware of the processes such as text pre-processing, feature extraction (CountVectorizer or TF-IDF) and then the applying algorithms. </p>\n\n<p>In this problem, I have multiple input text features. <strong>How to handle text classification problems when multiple features are involved?</strong> These are the methods I have already tried but I am not sure if these are the right methods. Kindly provide your inputs/suggestion.</p>\n\n<p>1) Applied data cleaning on each feature separately followed by TF-IDF and then logistic regression. Here I tried to see if I can use only one feature for classification.   </p>\n\n<p>2) Applied Data cleaning on all the columns separately and then applied TF-IDF for each feature and then merged the all feature vectors to create only one feature vector. Finally logistic regression. </p>\n\n<p>3) Applied Data cleaning on all the columns separately and merged all the cleaned columns to create one feature 'merged_text'. Then applied TF-IDF on this merged_text and followed by logistic regression.</p>\n\n<p>All these 3 methods gave me around 35-40% accuracy on cross-validation &amp; test set. I am expecting at-least 60% accuracy on the test set which is not provided.</p>\n\n<p>Also, I didn't understand how use to <strong>'company_name'</strong> &amp; <strong>'experience'</strong> with text data. there are about 2000+ unique values in company_name. Please provide input/pointer on how to handle numeric data in text classification problem.</p>\n",
    "score": 6,
    "creation_date": 1545810999,
    "view_count": 4190,
    "answer_count": 1,
    "tags": "python;nlp;feature-extraction;text-classification"
  },
  {
    "question_id": 49263374,
    "title": "Keyword/keyphrase extraction from text",
    "body": "<p>I am working on a project where I need to extract \"technology related keywords/keyphrases\" from text. For example, my text is: </p>\n\n<p><strong>\"ABC Inc has been working on a project related to machine learning which makes use of the existing libraries for finding information from big data.\"</strong> </p>\n\n<p>The extracted keywords/keyphrase should be: {machine learning, big data}.</p>\n\n<p>My text documents are stored as BSON documents in MongoDb.</p>\n\n<p>What are the best nlp libraries(with sufficient documentation and examples) out there to perform this task and how? </p>\n\n<p>Thanks!</p>\n",
    "score": 6,
    "creation_date": 1520965718,
    "view_count": 8358,
    "answer_count": 1,
    "tags": "machine-learning;nlp;text-mining;jnlp;text-extraction"
  },
  {
    "question_id": 47148247,
    "title": "NLP - Embeddings selection of `start` and `end` of sentence tokens",
    "body": "<p>Suppose we're training a neural network model to learn the mapping from the following input to output, where the output is <a href=\"https://en.wikipedia.org/wiki/Named-entity_recognition\" rel=\"nofollow noreferrer\">Name Entity</a> (NE).</p>\n\n<p><strong>Input</strong>: EU rejects German call to boycott British lamb .</p>\n\n<p><strong>Output</strong>: ORG O MISC O O O MISC O O</p>\n\n<p>A sliding window is created to capture the context information and its outcomes are fed into the training model as model_input. The sliding window generates results as following:</p>\n\n<pre><code> [['&lt;s&gt;', '&lt;s&gt;', 'EU', 'rejects', 'German'],\\\n ['&lt;s&gt;', 'EU', 'rejects', 'German', 'call'],\\\n ['EU', 'rejects', 'German', 'call', 'to'],\\\n ['rejects', 'German', 'call', 'to', 'boycott'],\\\n ['German', 'call', 'to', 'boycott', 'British'],\\\n ['call', 'to', 'boycott', 'British', 'lamb'],\\\n ['to', 'boycott', 'British', 'lamb', '.'],\\\n ['boycott', 'British', 'lamb', '.', '&lt;/s&gt;'],\\\n ['British', 'lamb', '.', '&lt;/s&gt;', '&lt;/s&gt;']]\n</code></pre>\n\n<p><code>&lt;s&gt;</code> represents start of sentence token and <code>&lt;/s&gt;</code> represents end of sentence token, and every sliding window corresponds to one NE in output. </p>\n\n<p>To process these tokens, a pre-trained embedding model is used converting words to vectors (e.g., Glove), but those pre-trained models do not include tokens such as <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code>. I think random initialization for <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> won't be a good idea here, because the scale of such random results might not be consistent with other Glove embeddings.</p>\n\n<p><strong>Question</strong>:\nWhat suggestions of setting up embeddings for <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> and why?</p>\n",
    "score": 6,
    "creation_date": 1510015873,
    "view_count": 6638,
    "answer_count": 1,
    "tags": "machine-learning;nlp;deep-learning;word2vec;word-embedding"
  },
  {
    "question_id": 42825655,
    "title": "Using predict on new text with kmeans (sklearn)?",
    "body": "<p>I have a <strong>very small</strong>  list of <strong>short strings</strong> which I want to (1) cluster and (2) use that model to predict which cluster a new string belongs to.</p>\n\n<p>Running the first part works fine, getting a prediction for the new string does not.</p>\n\n<h2>First Part</h2>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\n# List of \ndocuments_lst = ['a small, narrow river',\n                'a continuous flow of liquid, air, or gas',\n                'a continuous flow of data or instructions, typically one having a constant or predictable rate.',\n                'a group in which schoolchildren of the same age and ability are taught',\n                '(of liquid, air, gas, etc.) run or flow in a continuous current in a specified direction',\n                'transmit or receive (data, especially video and audio material) over the Internet as a steady, continuous flow.',\n                'put (schoolchildren) in groups of the same age and ability to be taught together',\n                'a natural body of running water flowing on or under the earth']\n\n\n# 1. Vectorize the text\ntfidf_vectorizer  = TfidfVectorizer(stop_words='english')\ntfidf_matrix = tfidf_vectorizer.fit_transform(documents_lst)\nprint('tfidf_matrix.shape: ', tfidf_matrix.shape)\n\n# 2. Get the number of clusters to make .. (find a better way than random)\nnum_clusters = 3\n\n# 3. Cluster the defintions\nkm = KMeans(n_clusters=num_clusters, init='k-means++').fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()\n\nprint(clusters)\n</code></pre>\n\n<p>Which returns:</p>\n\n<pre><code>tfidf_matrix.shape:  (8, 39)\n[0, 1, 0, 2, 1, 0, 2, 0]\n</code></pre>\n\n<h2>Second Part</h2>\n\n<p>The failing part:</p>\n\n<pre><code>predict_doc = ['A stream is a body of water with a current, confined within a bed and banks.']\n\ntfidf_vectorizer  = TfidfVectorizer(stop_words='english')\ntfidf_matrix = tfidf_vectorizer.fit_transform(predict_doc)\nprint('tfidf_matrix.shape: ', tfidf_matrix.shape)\n\nkm.predict(tfidf_matrix)\n</code></pre>\n\n<h3>The error:</h3>\n\n<pre><code>ValueError: Incorrect number of features. Got 7 features, expected 39\n</code></pre>\n\n<p>FWIW: I somewhat understand that the training and predict have a different amount of features after vectorizing ... </p>\n\n<p>I am open to any solution including changing from kmeans to an algorithm more suitable for short text clustering.</p>\n\n<p>Thanks in advance</p>\n",
    "score": 6,
    "creation_date": 1489640457,
    "view_count": 8703,
    "answer_count": 1,
    "tags": "python-3.x;scikit-learn;nlp;k-means"
  },
  {
    "question_id": 32003294,
    "title": "Sentence tokenization for texts that contains quotes",
    "body": "<p>Code:</p>\n\n<pre><code>from nltk.tokenize import sent_tokenize           \npprint(sent_tokenize(unidecode(text)))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[After Du died of suffocation, her boyfriend posted a heartbreaking message online: \"Losing consciousness in my arms, your breath and heartbeat became weaker and weaker.',\n 'Finally they pushed you out of the cold emergency room.',\n 'I failed to protect you.',\n '\"Li Na, 23, a migrant worker from a farming family in Jiangxi province, was looking forward to getting married in 2015.',]\n</code></pre>\n\n<p>Input:</p>\n\n<blockquote>\n  <p>After Du died of suffocation, her boyfriend posted a heartbreaking\n  message online: \"Losing consciousness in my arms, your breath and\n  heartbeat became weaker and weaker. Finally they pushed you out of the\n  cold emergency room. I failed to protect you.\"</p>\n  \n  <p>Li Na, 23, a migrant worker from a farming family in Jiangxi province,\n  was looking forward to getting married in 2015.</p>\n</blockquote>\n\n<p>Quotes should be included in previous sentence. Instead of  <code>\" Li.</code></p>\n\n<p>It fails at <code>.\"</code> How to fix this?</p>\n\n<p><strong>Edit:</strong>\nExplaining the extraction of text.</p>\n\n<pre><code>html = open(path, \"r\").read()                           #reads html code\narticle = extractor.extract(raw_html=html)              #extracts content\ntext = unidecode(article.cleaned_text)                  #changes encoding \n</code></pre>\n\n<p>Here, article.cleaned_text is in unicode. The idea behind using this to change characters “ to \".</p>\n\n<p>Solutions @alvas Incorrect Result:</p>\n\n<pre><code>['After Du died of suffocation, her boyfriend posted a heartbreaking message online: \"Losing consciousness in my arms, your breath and heartbeat became weaker and weaker.',\n 'Finally they pushed you out of the cold emergency room.',\n 'I failed to protect you.',\n '\"',\n 'Li Na, 23, a migrant worker from a farming family in Jiangxi province, was looking forward to getting married in 2015.'\n]\n</code></pre>\n\n<p><strong>Edit2:</strong>\n(Updated) nltk and python version</p>\n\n<pre><code>python -c \"import nltk; print nltk.__version__\"\n3.0.4\npython -V\nPython 2.7.9\n</code></pre>\n",
    "score": 6,
    "creation_date": 1439532239,
    "view_count": 1943,
    "answer_count": 1,
    "tags": "python;nlp;nltk;tokenize"
  },
  {
    "question_id": 29954476,
    "title": "Grammatical framework GF and owl",
    "body": "<p>I am interested in the filed on Computational Linguistics and NLP. I read a lot about Grammatical Framework (GF), which is divided into abstract syntax and concrete syntax. And I know a little bit about OWL, RDF and WordNet. I am confused about the differences between the 2 technologies.</p>\n\n<ol>\n<li>Can we use GF rather than OWL as syntax builders?</li>\n<li>Can we eliminate Parser by using GF?</li>\n<li>Does GF contains all terms so we don't need to use WordNet?</li>\n</ol>\n",
    "score": 6,
    "creation_date": 1430341307,
    "view_count": 428,
    "answer_count": 2,
    "tags": "nlp;owl;wordnet;gf"
  },
  {
    "question_id": 29059871,
    "title": "Chunking NP, VP and PP phrases in Java (CoreNLP)",
    "body": "<p>I'm using Stanford CoreNLP and I'm aware it doesn't support chunking of sentences. What I'm looking for is, given an input sentence, to have something like this as output:</p>\n\n<pre><code> [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP to ] [NP only # 1.8 billion ] [PP in ] [NP September ] . \n</code></pre>\n\n<p>I also know OpenNLP apparently supports this feature, but I already wrote quite a lot of code using CoreNLP and I would't like having to switch. So, what I'm looking for is either an external library that can do this for me or ideas about implementing this feature in the most simple way (references to publications, links, everything is welcome), starting from the parse tree. I don't need this to be as accurate as state of the art chunkers, at least for now, so I'm looking to implement this fast and maybe change it in the future if needed.</p>\n",
    "score": 6,
    "creation_date": 1426417315,
    "view_count": 1416,
    "answer_count": 1,
    "tags": "java;nlp"
  },
  {
    "question_id": 26284950,
    "title": "Sentiments Analysis Vs emotion Analysis",
    "body": "<p>What is difference between sentiments (positive and negative) and emotions in text mining (NLP)? For example Anger is negative emotion as well as negative sentiment both seems the same.\n Vijay Nadadur, Creator of SentiRank, an algorithm which ranks sentiment in a text, Suggest Bio\nSentiments can be expressed mostly in binary format (+ve &amp; -ve) in a simplistic way. To add further, you may varying degrees of +ve and -ve sentiments, and perhaps neutral. However, emotions have multi-dimensions. Anger for sure is -ve sentiment, and so is sadness, but they aren't really the same. </p>\n\n<p>To talk more specifically about text mining (NLP based), it's much simpler to do sentiment analysis but very hard to carry out emotional analysis. The next level of sentiment analysis is the field of intent analysis where few researchers have been working on mining out intent from the chunk of text, which seems of very high business value.</p>\n",
    "score": 6,
    "creation_date": 1412877332,
    "view_count": 3805,
    "answer_count": 2,
    "tags": "java;nlp;semantic-analysis;emotion"
  },
  {
    "question_id": 23509699,
    "title": "Understanding LDA Transformed Corpus in Gensim",
    "body": "<p>I tried to examine the contents of the BOW corpus vs. the LDA[BOW Corpus] (transformed by LDA model trained on that corpus with, say,  35 topics)\nI found the following output:</p>\n\n<pre><code>DOC 1 : [(1522, 1), (2028, 1), (2082, 1), (6202, 1)]  \nLDA 1 : [(29, 0.80571428571428572)]  \nDOC 2 : [(1522, 1), (5364, 1), (6202, 1), (6661, 1), (6983, 1)]  \nLDA 2 : [(29, 0.83809523809523812)]  \nDOC 3 : [(3079, 1), (3395, 1), (4874, 1)]  \nLDA 3 : [(34, 0.75714285714285712)]  \nDOC 4 : [(1482, 1), (2806, 1), (3988, 1)]  \nLDA 4 : [(22, 0.50714288283121989), (32, 0.25714283145449457)]  \nDOC 5 : [(440, 1), (533, 1), (1264, 1), (2433, 1), (3012, 1), (3902, 1), (4037, 1), (4502, 1), (5027, 1), (5723, 1)]  \nLDA 5 : [(12, 0.075870715371114297), (30, 0.088821329943986921), (31, 0.75219107156801579)]  \nDOC 6 : [(705, 1), (3156, 1), (3284, 1), (3555, 1), (3920, 1), (4306, 1), (4581, 1), (4900, 1), (5224, 1), (6156, 1)]  \nLDA 6 : [(6, 0.63896110435842401), (20, 0.18441557445724915), (28, 0.09350643806744402)]  \nDOC 7 : [(470, 1), (1434, 1), (1741, 1), (3654, 1), (4261, 1)]  \nLDA 7 : [(5, 0.17142855723258577), (13, 0.17142856888458904), (19, 0.50476192150187316)]  \nDOC 8 : [(2227, 1), (2290, 1), (2549, 1), (5102, 1), (7651, 1)]  \nLDA 8 : [(12, 0.16776844589094803), (19, 0.13980868559963203), (22, 0.1728575716782704), (28, 0.37194624921210206)]  \n</code></pre>\n\n<p>Where, \n    DOC N is the document from the BOW corpus \n    LDA N is the transformation of DOC N by that LDA model</p>\n\n<p>Am I correct in understanding the output for each transformed document \"LDA N\" to be the topics that the document N belongs to? By that understanding, I can see some documents like 4, 5, 6, 7 and 8 to belong to more than 1 topic like DOC 8 belongs to topics 12, 19, 22 and 28 with the respective probabilities.</p>\n\n<p>Could you please explain the output of LDA N and correct my understanding of this output, especially since in another thread <a href=\"https://groups.google.com/forum/#!msg/gensim/Vv7SSC8HR8k/nQGNRE9HjacJ\" rel=\"noreferrer\">HERE</a> - by the creator of Gensim himself, it's been mentioned that a document belongs to ONE topic? </p>\n",
    "score": 6,
    "creation_date": 1399441696,
    "view_count": 2715,
    "answer_count": 1,
    "tags": "python;nlp;lda;gensim"
  },
  {
    "question_id": 22428020,
    "title": "How to extract character ngram from sentences? - python",
    "body": "<p>The following <code>word2ngrams</code> function extracts character 3grams from a word:</p>\n\n<pre><code>&gt;&gt;&gt; x = 'foobar'\n&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; [x[i:i+n] for i in range(len(x)-n+1)]\n['foo', 'oob', 'oba', 'bar']\n</code></pre>\n\n<p>This post shows the character ngrams extraction for a single word, <a href=\"https://stackoverflow.com/questions/18658106/quick-implementation-of-character-n-grams-using-python\">Quick implementation of character n-grams using python</a>. </p>\n\n<p>But what if i have sentences and i want to extract the character ngrams, <strong>is there a faster method other than iteratively call the <code>word2ngram()</code></strong>?</p>\n\n<p><strong>What will be the regex version of achieving the same <code>word2ngram</code> and <code>sent2ngram</code> output? would it be faster?</strong></p>\n\n<p>I've tried:</p>\n\n<pre><code>import string, random, time\nfrom itertools import chain\n\ndef word2ngrams(text, n=3):\n  \"\"\" Convert word into character ngrams. \"\"\"\n  return [text[i:i+n] for i in range(len(text)-n+1)]\n\ndef sent2ngrams(text, n=3):\n    return list(chain(*[word2ngrams(i,n) for i in text.lower().split()]))\n\ndef sent2ngrams_simple(text, n=3):\n    text = text.lower()\n    return [text[i:i+n] for i in range(len(text)-n+1) if not \" \" in text[i:i+n]]\n\n# Generate 10000 random strings of length 100.\nsents = [\" \".join([''.join(random.choice(string.ascii_uppercase) for j in range(10)) for i in range(100)]) for k in range(100)]\n\nstart = time.time()\nx = [sent2ngrams(i) for i in sents]\nprint time.time() - start        \n\nstart = time.time()\ny = [sent2ngrams_simple(i) for i in sents]\nprint time.time() - start        \n\nprint x==y\n</code></pre>\n\n<p>[out]:</p>\n\n<pre><code>0.0205280780792\n0.0271739959717\nTrue\n</code></pre>\n\n<p><strong>EDITED</strong></p>\n\n<p>The regex method looks elegant but it performs slower than iteratively calling <code>word2ngram()</code>:</p>\n\n<pre><code>import string, random, time, re\nfrom itertools import chain\n\ndef word2ngrams(text, n=3):\n  \"\"\" Convert word into character ngrams. \"\"\"\n  return [text[i:i+n] for i in range(len(text)-n+1)]\n\ndef sent2ngrams(text, n=3):\n    return list(chain(*[word2ngrams(i,n) for i in text.lower().split()]))\n\ndef sent2ngrams_simple(text, n=3):\n    text = text.lower()\n    return [text[i:i+n] for i in range(len(text)-n+1) if not \" \" in text[i:i+n]]\n\ndef sent2ngrams_regex(text, n=3):\n    rgx = '(?=('+'\\S'*n+'))'\n    return re.findall(rgx,text)\n\n# Generate 10000 random strings of length 100.\nsents = [\" \".join([''.join(random.choice(string.ascii_uppercase) for j in range(10)) for i in range(100)]) for k in range(100)]\n\nstart = time.time()\nx = [sent2ngrams(i) for i in sents]\nprint time.time() - start        \n\nstart = time.time()\ny = [sent2ngrams_simple(i) for i in sents]\nprint time.time() - start        \n\nstart = time.time()\nz = [sent2ngrams_regex(i) for i in sents]\nprint time.time() - start  \n\nprint x==y==z\n</code></pre>\n\n<p>[out]:</p>\n\n<pre><code>0.0211708545685\n0.0284190177917\n0.0303599834442\nTrue\n</code></pre>\n",
    "score": 6,
    "creation_date": 1394908321,
    "view_count": 8350,
    "answer_count": 1,
    "tags": "python;regex;string;nlp;n-gram"
  },
  {
    "question_id": 20069781,
    "title": "How to perform Paragraph boundary detection in NLP frameworks?",
    "body": "<p>I am working on extracting names of people from various ads appearing in English newspapers . </p>\n\n<p>However , i have noticed that I need to identify the boundary of an Ad , before extracting the names present in it ,since I need only the first occurring name to be extracted .I started with Stanford NLP . I was successful in extracting names . But I got stuck in identifying the paragraph boundary.</p>\n\n<p>Is there any way of  identifying the paragraph boundary . ?</p>\n",
    "score": 6,
    "creation_date": 1384859058,
    "view_count": 4117,
    "answer_count": 2,
    "tags": "nlp;text-processing;stanford-nlp;opennlp;apache-stanbol"
  },
  {
    "question_id": 17734534,
    "title": "How do I use the book functions (e.g. concoordance) in NLTK?",
    "body": "<p>I am going through this <a href=\"http://nltk.org/book/ch01.html\" rel=\"nofollow\">wonderful tutorial</a>.</p>\n\n<p>I downloaded a collection called <code>book</code>:</p>\n\n<pre><code>&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; nltk.download()\n</code></pre>\n\n<p>and imported texts:</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.book import *\n*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\n</code></pre>\n\n<p>I can then run commands on these texts:</p>\n\n<pre><code>&gt;&gt;&gt; text1.concordance(\"monstrous\")\n</code></pre>\n\n<p><strong>How can I run these nltk commands on my own dataset? Are these collections the same as the object <code>book</code> in python?</strong></p>\n",
    "score": 6,
    "creation_date": 1374184058,
    "view_count": 1232,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 13294254,
    "title": "Counting with scipy.sparse",
    "body": "<p>I am using the Python sklearn libraries.\nI have 150,000+ sentences.</p>\n\n<p>I need an array-like object, where each row is for a sentences, each column corresponds to a word, and each element is the number of words in that sentence.</p>\n\n<p>For example: If the two sentences were \"The dog ran\" and \"The boy ran\", I need</p>\n\n<pre><code>[ [1, 1, 1, 0]\n, [0, 1, 1, 1] ]\n</code></pre>\n\n<p>(the order of the columns is irrelevant, and depends on which column is assigned to which word)</p>\n\n<p>My array will be sparse (each sentence will have a fraction of the possible words), and so I am using scipy.sparse.</p>\n\n<pre><code>def word_counts(texts, word_map):\n    w_counts = sp.???_matrix((len(texts),len(word_map)))\n\n    for n in range(0,len(texts)-1):\n        for word in re.findall(r\"[\\w']+\", texts[n]):\n            index = word_map.get(word)\n            if index != None:\n                w_counts[n,index] += 1\n    return w_counts\n\n...\nnb = MultinomialNB() #from sklearn\nwords = features.word_list(texts)\nnb.fit(features.word_counts(texts,words), classes)\n</code></pre>\n\n<p>I want to know what sparse matrix would be best.</p>\n\n<p>I tried using coo_matrix but got an error:</p>\n\n<blockquote>\n  <p>TypeError: 'coo_matrix' object has no attribute '__getitem__'</p>\n</blockquote>\n\n<p>I looked at the <a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix\" rel=\"noreferrer\">documentation</a> for COO but was <strong>very confused by the following</strong>:</p>\n\n<blockquote>\n  <p>Sparse matrices can be used in arithmetic operations ...<br>\n  Disadvantages of the COO format ...\n  does not directly support: \n  arithmetic operations</p>\n</blockquote>\n\n<p>I used dok_matrix, and that worked, but I don't know if this performs best in this case.</p>\n\n<p>Thanks in advance.</p>\n",
    "score": 6,
    "creation_date": 1352395136,
    "view_count": 2842,
    "answer_count": 1,
    "tags": "python;nlp;scipy;sparse-matrix;scikit-learn"
  },
  {
    "question_id": 11470985,
    "title": "How google recognises 2 words without spaces?",
    "body": "<p>I want to understand how google handles no space between 2 words. For example there are 2 words - word1 and word2. I write in search box 'word1word2', it says do you mean 'word1 word2' or just understands to look for 'word1 word2'. Any information what data structure and algorithm they use? I see in this answer <a href=\"https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words\">How to split text without spaces into list of words?</a>, it is suggested to use trie data structure.</p>\n",
    "score": 6,
    "creation_date": 1342184242,
    "view_count": 997,
    "answer_count": 2,
    "tags": "algorithm;search;nlp"
  },
  {
    "question_id": 9638479,
    "title": "Intelligent spell checking",
    "body": "<p>I'm using <a href=\"http://nhunspell.sourceforge.net/\" rel=\"nofollow\">NHunspell</a> to check a string for spelling errors like so:</p>\n\n<pre><code>var words = content.Split(' ');\nstring[] incorrect;\nusing (var spellChecker = new Hunspell(affixFile, dictionaryFile))\n{\n    incorrect = words.Where(x =&gt; !spellChecker.Spell(x))\n        .ToArray();\n}\n</code></pre>\n\n<p>This generally works, but it has some problems. For example, if I'm checking the sentence \"This is a (very good) example\", it will report \"(very\" and \"good)\" as being misspelled. Or if the string contains a time such as \"8:30\", it will report that as a misspelled word. It also has problems with commas, etc.</p>\n\n<p>Microsoft Word is smart enough to recognize a time, fraction, or comma-delimited list of words. It knows when not to use an English dictionary, and it knows when to ignore symbols. How can I get a similar, more intelligent spell check in my software? Are there any libraries that provide a little more intelligence?</p>\n\n<p>EDIT:\nI don't want to force users to have Microsoft Word installed on their machine, so using COM interop is not an option.</p>\n",
    "score": 6,
    "creation_date": 1331314411,
    "view_count": 1089,
    "answer_count": 3,
    "tags": ".net;artificial-intelligence;nlp;spell-checking;hunspell"
  },
  {
    "question_id": 9456145,
    "title": "C++ Sentiment Analysis Library",
    "body": "<p>I'm looking for a C++ sentiment analysis library that I could use in my own application. Something that would take a text written by a human as an argument and return information on its mood (positive, negative, neutral, angry, happy, ...). Any ideas?</p>\n\n<p>A few remarks:</p>\n\n<ul>\n<li>I'm not looking for a library with \"just\" NLP tools (as text tokenization, PoS tagging etc.), but really something that does sentiment analysis / opinion mining / mood analysis. Of course an NLP library with sentiment analysis tool is great.</li>\n<li>Something very simple would be ok (e.g. just returning +1/-1/0)</li>\n<li>I don't care which underlying technique it may use (dictionaries, bayesian stuffs, SVMs, rule-based...)</li>\n<li>C++ only!</li>\n</ul>\n\n<p>I know it probably does not exist just like that, but hey.</p>\n",
    "score": 6,
    "creation_date": 1330283023,
    "view_count": 5069,
    "answer_count": 3,
    "tags": "c++;nlp;sentiment-analysis"
  },
  {
    "question_id": 8768920,
    "title": "How can I tweak Levenshtein distance in classifying linguistically similar words (e.g. verb tenses, adjective comparisons, singular and plural)",
    "body": "<p>I am out of ideas on how to complete this task. I am counting the frequency of a word, actually the base form of the word (e.g. running will be counted as run). I looked up on some implementations of Levenshtein distance (one implementation I run into is <a href=\"http://dotnetperls.com/levenshtein\" rel=\"noreferrer\">from dotnerperls</a>).</p>\n\n<p>I also tried the double Metaphone, but it isn't what I'm looking for.</p>\n\n<p>So, please give me some ideas on how to tweak Levenshtein distance algorithm in classifying linguistically similar words since the algorithm is only for determining the number of edits needed not considering if they are linguistically similar or not</p>\n\n<p>Example:\n1. \"running\" will be counted as one occurrence of the word \"run\"\n2. \"word\" will likewise be an occurrence of \"word\"\n3. \"fear\" will NOT be counted as an occurrence of \"gear\"</p>\n\n<p>Also, I am implementing it in C#.</p>\n\n<p>Thanks in advance.</p>\n\n<p>Edit: I edited it as Rene suggested.\nAnother note:\nI am trying to consider to consider if a word is a substring of another word but that implementation will not be as much dynamic.\nAnother idea I think is: \"if adding -s or -ing to string1, string1 == string2, then string2 is an occurrence of string1.\" However, this is not the case as some words have irregular plurals.</p>\n",
    "score": 6,
    "creation_date": 1325931315,
    "view_count": 853,
    "answer_count": 1,
    "tags": "nlp;levenshtein-distance;similarity"
  },
  {
    "question_id": 5143788,
    "title": "Extracting nouns from Noun Phase in NLP",
    "body": "<p>Could anyone please tell me how to extract only the nouns from the following output:</p>\n\n<p>I have tokenized and parsed the string \"Give me the review of movie\" based on a given grammar using following procedure:-</p>\n\n<pre><code>sent=nltk.word_tokenize(msg)\nparser=nltk.ChartParser(grammar)\ntrees=parser.nbest_parse(sent)\nfor tree in trees:\n    print tree\ntokens=find_all_NP(tree)\ntokens1=nltk.word_tokenize(tokens[0])\nprint tokens1\n</code></pre>\n\n<p>and obtained the following output:</p>\n\n<pre><code>&gt;&gt;&gt; \n(S\n  (VP (V Give) (Det me))\n  (NP (Det the) (N review) (PP (P of) (N movie))))\n(S\n  (VP (V Give) (Det me))\n  (NP (Det the) (N review) (NP (PP (P of) (N movie)))))\n['the', 'review', 'of', 'movie']\n&gt;&gt;&gt; \n</code></pre>\n\n<p>Now I would like to only obtain the nouns. How do I do that?</p>\n",
    "score": 6,
    "creation_date": 1298906088,
    "view_count": 6184,
    "answer_count": 1,
    "tags": "python;django;nlp"
  },
  {
    "question_id": 67342988,
    "title": "Verifying the implementation of Multihead Attention in Transformer",
    "body": "<p>I have implemented the <code>MultiAttention head</code> in <code>Transformers</code>. There are so many implementations around so it's confusing. Can someone please verify if my implementation is correct:</p>\n<p>DotProductAttention referred from: <a href=\"https://www.tensorflow.org/tutorials/text/transformer#setup\" rel=\"noreferrer\">https://www.tensorflow.org/tutorials/text/transformer#setup</a></p>\n<pre><code>import tensorflow as tf\n\ndef scaled_dot_product(q,k,v):\n    #calculates Q . K(transpose)\n    qkt = tf.matmul(q,k,transpose_b=True)\n    #caculates scaling factor\n    dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n    scaled_qkt = qkt/dk\n    softmax = tf.nn.softmax(scaled_qkt,axis=-1)\n    \n    z = tf.matmul(softmax,v)\n    #shape: (m,Tx,depth), same shape as q,k,v\n    return z\n\nclass MultiAttention(tf.keras.layers.Layer):\n    def __init__(self,d_model,num_of_heads):\n        super(MultiAttention,self).__init__()\n        self.d_model = d_model\n        self.num_of_heads = num_of_heads\n        self.depth = d_model//num_of_heads\n        self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n        self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n        self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n        self.wo = tf.keras.layers.Dense(d_model)\n        \n    def call(self,x):\n        \n        multi_attn = []\n        for i in range(self.num_of_heads):\n            Q = self.wq[i](x)\n            K = self.wk[i](x)\n            V = self.wv[i](x)\n            multi_attn.append(scaled_dot_product(Q,K,V))\n            \n        multi_head = tf.concat(multi_attn,axis=-1)\n        multi_head_attention = self.wo(multi_head)\n        return multi_head_attention\n\n#Calling the attention \nmulti = MultiAttention(d_model=512,num_of_heads=8)\nm = 5; sequence_length = 4; word_embedding_dim = 512\nsample_ip = tf.constant(tf.random.normal(shape=(m,sequence_length,word_embedding_dim)))\nattn =multi(sample_ip)\n#shape of op (attn): (5,4,512)\n</code></pre>\n",
    "score": 6,
    "creation_date": 1619844613,
    "view_count": 2756,
    "answer_count": 1,
    "tags": "tensorflow;keras;deep-learning;nlp;lstm"
  },
  {
    "question_id": 58834647,
    "title": "Is it a good idea to use word2vec for encoding of categorical features?",
    "body": "<p>I am facing a binary prediction task and have a set of features of which all are categorical. A key challenge is therefore to encode those categorical features to numbers and I was looking for smart ways to do so. \nI stumbled over word2vec, which is mostly used for NLP, but I was wondering whether I could use it to encode my variables, i.e. simply take the weights of the neural net as the encoded features.</p>\n\n<p>However, I am not sure, whether it is a good idea since, the context words, which serve as the input features in word2vec are in my case more or less random, in contrast to real sentences which word2vec was originially made for. </p>\n\n<p>Do you guys have any advice, thoughts, recommendations on this?</p>\n",
    "score": 6,
    "creation_date": 1573639623,
    "view_count": 2035,
    "answer_count": 2,
    "tags": "machine-learning;nlp;word2vec;categorical-data;feature-engineering"
  },
  {
    "question_id": 54968055,
    "title": "How to recognize entities in text that is the output of optical character recognition (OCR)?",
    "body": "<p>I am trying to do multi-class classification with textual data. Problem I am facing that I have unstructured textual data. I'll explain the problem with an example.\nconsider this image for example:</p>\n\n<p><a href=\"https://i.sstatic.net/mS1fx.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/mS1fx.jpg\" alt=\"example data\"></a></p>\n\n<p>I want to extract and classify text information given in image. Problem is when I extract information OCR engine will give output something like this:</p>\n\n<pre><code>18\nEURO 46\nKEEP AWAY\nFROM FIRE\nMADE IN CHINA\n2226249917581\n7412501\nDOROTHY\nPERKINS\n</code></pre>\n\n<p>Now target classes here are:</p>\n\n<pre><code>18 -&gt; size\nEURO 46 -&gt; price\nKEEP AWAY FROM FIRE -&gt; usage_instructions\nMADE IN CHINA -&gt; manufacturing_location\n2226249917581 -&gt; product_id\n7412501 -&gt; style_id\nDOROTHY PERKINS -&gt; brand_name\n</code></pre>\n\n<p>Problem I am facing is that input text is not separable, meaning \"multiple lines can belong to same class\" and there can be cases where \"single line can have multiple classes\".</p>\n\n<p>So I don't know how I can split/merge lines before passing it to classification model.<br> Is there any way using NLP I can split paragraph based on target class. In other words given input paragraph split it based on target labels.</p>\n",
    "score": 6,
    "creation_date": 1551610366,
    "view_count": 1436,
    "answer_count": 1,
    "tags": "nlp;recurrent-neural-network;text-classification;named-entity-recognition;named-entity-extraction"
  },
  {
    "question_id": 54873721,
    "title": "PyTorch Huggingface BERT-NLP for Named Entity Recognition",
    "body": "<p>I have been using the PyTorch implementation of Google's <a href=\"https://github.com/google-research/bert#fine-tuning-with-bert\" rel=\"nofollow noreferrer\">BERT</a> by <a href=\"https://github.com/huggingface/pytorch-pretrained-BERT\" rel=\"nofollow noreferrer\">HuggingFace</a> for the MADE 1.0 dataset for quite some time now. Up until last time (11-Feb), I had been using the library and getting an <strong>F-Score</strong> of <strong>0.81</strong> for my Named Entity Recognition task by Fine Tuning the model. But this week when I ran the exact same code which had compiled and run earlier, it threw an error when executing this statement:</p>\n\n<pre><code>input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n</code></pre>\n\n<blockquote>\n  <p>ValueError: Token indices sequence length is longer than the specified\n  maximum  sequence length for this BERT model (632 > 512). Running this\n  sequence through BERT will result in indexing errors</p>\n</blockquote>\n\n<p>The full code is available in this <a href=\"https://colab.research.google.com/drive/1JxWdw1BjXZCFC2a8IwtZxvvq4rFGcxas\" rel=\"nofollow noreferrer\">colab notebook</a>.</p>\n\n<p>To get around this error I modified the above statement to the one below by taking the first 512 tokens of any sequence and made the necessary changes to add the index of [SEP] to the end of the truncated/padded sequence as required by BERT.</p>\n\n<pre><code>input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt[:512]) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n</code></pre>\n\n<p>The result shouldn't have changed because I am only considering the first 512 tokens in the sequence and later truncating to 75 as my (MAX_LEN=75) but my <strong>F-Score</strong> has dropped to <strong>0.40</strong> and my <strong>precision</strong> to <strong>0.27</strong> while the <strong>Recall</strong> remains the same <strong>(0.85)</strong>. I am unable to share the dataset as I have signed a confidentiality clause but I can assure all the preprocessing as required by BERT has been done and all extended tokens like (Johanson --> Johan ##son) have been tagged with X and replaced later after the prediction as said in the <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow noreferrer\">BERT Paper</a>.</p>\n\n<p>Has anyone else faced a similar issue or can elaborate on what might be the issue or what changes the PyTorch (Huggingface) people have done on their end recently?</p>\n",
    "score": 6,
    "creation_date": 1551124537,
    "view_count": 2850,
    "answer_count": 2,
    "tags": "python;nlp;data-science;named-entity-recognition;huggingface-transformers"
  },
  {
    "question_id": 54072496,
    "title": "Explaining CNN (Keras) outputs with LIME",
    "body": "<p>I am trying to explain the outputs of my convolutional neural network bult in Keras with <a href=\"https://github.com/marcotcr/lime\" rel=\"noreferrer\">LIME</a>.</p>\n\n<p>My neural network is a multi-class text classifier where every class is independent. Thus, a text can contain class 1 and 2 or only 1 etc. A fifth \"class\" (None) for cases where no classes are in the text.</p>\n\n<p>However, while i managed to explain a binary classification case with Keras and Lime, I just cannot get the multi-class case with independent classes. A first help was found <a href=\"https://github.com/marcotcr/lime/issues/267\" rel=\"noreferrer\">here</a>: </p>\n\n<p>However, my code does not work, I get internal errors from Lime such as: \"ValueError: Found input variables with inconsistent numbers of samples: [5000, 100000]\"</p>\n\n<pre><code>from lime.lime_text import LimeTextExplainer, TextDomainMapper\nexplainer = LimeTextExplainer(class_names=encoder.classes_)\n\n\nchosen_text = 2\n\ndef flatten_predict(i):\n    global model   \n    # catch single string inputs and convert them to list\n    if i.__class__ != list:\n        i = [i]\n        print(\"## Caught and transformed single string.\")\n    # list for predictions\n    predStorage = []\n    # loop through input list and predict\n    for textInput in i:\n        textInput = preprocess(textInput)\n        textInput = make_predictable(textInput)\n        pred = model.predict(textInput)\n        pred = np.append(pred, 1-pred, axis=1)\n        # control output of function\n\n        predStorage.extend(pred)\n    return np.asarray(predStorage)\n\n\ndef get_predict_proba_fn_of_class(label):\n    \"\"\"assuming wrapped_predict outputs an (n, d) array of prediction probabilities, where d is the number of labels\"\"\"\n    def rewrapped_predict(strings): \n        preds = flatten_predict(strings)[:, np.where(flatten_predict(strings)==label)].reshape(-1, 1)\n        ret = np.asarray(np.hstack([(1 - preds), preds]))\n        return ret\n\n    return rewrapped_predict\n\nstr = 'Ein sehr freundlicher Arzt.'\npreds = flatten_predict(str)\nlabels_to_explain = preds# \nprint(labels_to_explain)\n\nexplanation_for_label = {}\nfor label in labels_to_explain:\n    wrapped = get_predict_proba_fn_of_class(label)\n    explanation_for_label[label] = explainer.explain_instance(str, wrapped)\n    explanation_for_label[label].show_in_notebook()\n</code></pre>\n\n<p>Error Message: </p>\n\n<pre><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-26-8df61aaa23f4&gt; in &lt;module&gt;()\n     53 for label in labels_to_explain:\n     54     wrapped = get_predict_proba_fn_of_class(label)\n---&gt; 55     explanation_for_label[label] = explainer.explain_instance(str, wrapped)\n     56     explanation_for_label[label].show_in_notebook()\n     57 \n\n/usr/local/lib/python3.6/dist-packages/lime/lime_text.py in explain_instance(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\n    405                 data, yss, distances, label, num_features,\n    406                 model_regressor=model_regressor,\n--&gt; 407                 feature_selection=self.feature_selection)\n    408         return ret_exp\n    409 \n\n/usr/local/lib/python3.6/dist-packages/lime/lime_base.py in explain_instance_with_data(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\n    155                                                weights,\n    156                                                num_features,\n--&gt; 157                                                feature_selection)\n    158 \n    159         if model_regressor is None:\n\n/usr/local/lib/python3.6/dist-packages/lime/lime_base.py in feature_selection(self, data, labels, weights, num_features, method)\n    104                 n_method = 'highest_weights'\n    105             return self.feature_selection(data, labels, weights,\n--&gt; 106                                           num_features, n_method)\n    107 \n    108     def explain_instance_with_data(self,\n\n/usr/local/lib/python3.6/dist-packages/lime/lime_base.py in feature_selection(self, data, labels, weights, num_features, method)\n     78             clf = Ridge(alpha=0, fit_intercept=True,\n     79                         random_state=self.random_state)\n---&gt; 80             clf.fit(data, labels, sample_weight=weights)\n     81             feature_weights = sorted(zip(range(data.shape[0]),\n     82                                          clf.coef_ * data[0]),\n\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/ridge.py in fit(self, X, y, sample_weight)\n    678         self : returns an instance of self.\n    679         \"\"\"\n--&gt; 680         return super(Ridge, self).fit(X, y, sample_weight=sample_weight)\n    681 \n    682 \n\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/ridge.py in fit(self, X, y, sample_weight)\n    489 \n    490         X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=_dtype,\n--&gt; 491                          multi_output=True, y_numeric=True)\n    492 \n    493         if ((sample_weight is not None) and\n\n/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\n    764         y = y.astype(np.float64)\n    765 \n--&gt; 766     check_consistent_length(X, y)\n    767 \n    768     return X, y\n\n/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py in check_consistent_length(*arrays)\n    233     if len(uniques) &gt; 1:\n    234         raise ValueError(\"Found input variables with inconsistent numbers of\"\n--&gt; 235                          \" samples: %r\" % [int(l) for l in lengths])\n    236 \n    237 \n\nValueError: Found input variables with inconsistent numbers of samples: [5000, 100000]\n</code></pre>\n\n<p>Does anyone know what I am doing wrong? I am pretty sure it has to do with the <strong>input format</strong>.</p>\n",
    "score": 6,
    "creation_date": 1546856648,
    "view_count": 3492,
    "answer_count": 2,
    "tags": "python;tensorflow;keras;nlp;lime"
  },
  {
    "question_id": 48153854,
    "title": "ValueError: operands could not be broadcast together with shapes in Naive bayes classifier",
    "body": "<p>Getting straight to the point:<br></p>\n\n<p><strong>1)</strong> My goal was to apply NLP and Machine learning algorithm to classify a dataset containing sentences into 5 different types of categories(numeric). For e.g. \"I want to know details of my order -> 1\".<br></p>\n\n<p><strong>Code:</strong><br></p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndataset = pd.read_csv('Ecom.tsv', delimiter = '\\t', quoting = 3)\n\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\ncorpus = []\nfor i in range(0, len(dataset)):\n    review = re.sub('[^a-zA-Z]', ' ', dataset['User'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)\n\n# # Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(corpus).toarray()\ny = dataset.iloc[:, 1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n</code></pre>\n\n<p>Everything works fine here, the model is trained well and predicts correct results for test data.</p>\n\n<p><strong>2)</strong> Now i wanted to use this trained model to predict a category for a new sentence. So i pre-processed the text in the same way i did for my dataset.</p>\n\n<p><strong>Code:</strong></p>\n\n<pre><code>#Pre processing the new input\nnew_text = \"Please tell me the details of this order\"\nnew_text = new_text.split()\nps = PorterStemmer()\nprocessed_text = [ps.stem(word) for word in new_text if not word in set(stopwords.words('english'))]\n\nvect = CountVectorizer()\nZ = vect.fit_transform(processed_text).toarray()\nclassifier.predict(Z)\n</code></pre>\n\n<p><strong>ValueError: operands could not be broadcast together with shapes (4,4) (33,)</strong></p>\n\n<p>The only thing i am able to understand is that when i transformed my <strong>corpus</strong> the first time i trained my model, the shape of the numpy array is (18, 33). Second time when i am trying to predict for a new input, when i transformed my <strong>processed_text</strong> using <strong>fit_transform()</strong>, the numpy array shape is (4, 4).</p>\n\n<p>I am not able to figure out is there any process here that i applied incorrectly? What can be the resolution. Thanks in advance! :)</p>\n",
    "score": 6,
    "creation_date": 1515427214,
    "view_count": 1582,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;classification;naivebayes"
  },
  {
    "question_id": 38419395,
    "title": "Extract grocery list out of free text",
    "body": "<p>I am looking for a python library / algorithm / paper to extract a list of groceries out of free text.</p>\n\n<p>For example:</p>\n\n<blockquote>\n  <p>\"One salad and two beers\"</p>\n</blockquote>\n\n<p>Should be converted to:</p>\n\n<pre><code>{'salad':1, 'beer': 2}\n</code></pre>\n",
    "score": 6,
    "creation_date": 1468744796,
    "view_count": 411,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 35465315,
    "title": "Intuition behind tf-idf for term extraction",
    "body": "<p>I'm trying to build a dictionary of words using <em>tf-idf</em>. However, intuitively it doesn't make sense.</p>\n\n<p>If the <em>inverse document frequency</em> (<em>idf</em>) part of <em>tf-idf</em> calculates the relevance of a term with respect to entire corpus, then that implies some of the important words might have a lower relevance.</p>\n\n<p>If we look at a corpus of legal documents, a term like \"license\" or \"legal\" might occur in every document. Due to <em>idf</em>, the score for these terms will be very low. However, intuitively speaking, these terms should have a higher score since these are clearly legal terms. </p>\n\n<p>Is <em>tf-idf</em> a bad approach for building a dictionary of terms?</p>\n",
    "score": 6,
    "creation_date": 1455735424,
    "view_count": 659,
    "answer_count": 1,
    "tags": "machine-learning;nlp;tf-idf"
  },
  {
    "question_id": 34626555,
    "title": "Result Difference in Stanford NER tagger NLTK (python) vs JAVA",
    "body": "<p>I am using both python and java to run the Stanford NER tagger but I am seeing the difference in the results.</p>\n\n<p>For example, when I input the sentence \"Involved in all aspects of data modeling using ERwin as the primary software for this.\",</p>\n\n<p>JAVA Result:</p>\n\n<pre><code>\"ERwin\": \"PERSON\"\n</code></pre>\n\n<p>Python Result:</p>\n\n<pre><code>In [6]: NERTagger.tag(\"Involved in all aspects of data modeling using ERwin as the primary software for this.\".split())\nOut [6]:[(u'Involved', u'O'),\n (u'in', u'O'),\n (u'all', u'O'),\n (u'aspects', u'O'),\n (u'of', u'O'),\n (u'data', u'O'),\n (u'modeling', u'O'),\n (u'using', u'O'),\n (u'ERwin', u'O'),\n (u'as', u'O'),\n (u'the', u'O'),\n (u'primary', u'O'),\n (u'software', u'O'),\n (u'for', u'O'),\n (u'this.', u'O')]\n</code></pre>\n\n<p>Python nltk wrapper can't catch \"ERwin\" as PERSON.</p>\n\n<p>What's interesting here is both Python and Java uses the same trained data (english.all.3class.caseless.distsim.crf.ser.gz) released in 2015-04-20.</p>\n\n<p>My ultimate goal is to make python work in the same way Java does.</p>\n\n<p>I'm looking at StanfordNERTagger in nltk.tag to see if there's anything I can modify. Below is the wrapper code:</p>\n\n<pre><code>class StanfordNERTagger(StanfordTagger):\n\"\"\"\nA class for Named-Entity Tagging with Stanford Tagger. The input is the paths to:\n\n- a model trained on training data\n- (optionally) the path to the stanford tagger jar file. If not specified here,\n  then this jar file must be specified in the CLASSPATH envinroment variable.\n- (optionally) the encoding of the training data (default: UTF-8)\n\nExample:\n\n    &gt;&gt;&gt; from nltk.tag import StanfordNERTagger\n    &gt;&gt;&gt; st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') # doctest: +SKIP\n    &gt;&gt;&gt; st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) # doctest: +SKIP\n    [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'),\n     ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'),\n     ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]\n\"\"\"\n\n_SEPARATOR = '/'\n_JAR = 'stanford-ner.jar'\n_FORMAT = 'slashTags'\n\ndef __init__(self, *args, **kwargs):\n    super(StanfordNERTagger, self).__init__(*args, **kwargs)\n\n@property\ndef _cmd(self):\n    # Adding -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions tokenizeNLs=false for not using stanford Tokenizer  \n    return ['edu.stanford.nlp.ie.crf.CRFClassifier',\n            '-loadClassifier', self._stanford_model, '-textFile',\n            self._input_file_path, '-outputFormat', self._FORMAT, '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions','\\\"tokenizeNLs=false\\\"']\n\ndef parse_output(self, text, sentences):\n    if self._FORMAT == 'slashTags':\n        # Joint together to a big list    \n        tagged_sentences = []\n        for tagged_sentence in text.strip().split(\"\\n\"):\n            for tagged_word in tagged_sentence.strip().split():\n                word_tags = tagged_word.strip().split(self._SEPARATOR)\n                tagged_sentences.append((''.join(word_tags[:-1]), word_tags[-1]))\n\n        # Separate it according to the input\n        result = []\n        start = 0 \n        for sent in sentences:\n            result.append(tagged_sentences[start:start + len(sent)])\n            start += len(sent);\n        return result \n\n    raise NotImplementedError\n</code></pre>\n\n<p>Or, if it's because of using different Classifier (In java code, it seems to use AbstractSequenceClassifier, on the other hand, python nltk wrapper uses the CRFClassifier.) is there a way that I can use AbstractSequenceClassifier in python wrapper?</p>\n",
    "score": 6,
    "creation_date": 1452059807,
    "view_count": 1070,
    "answer_count": 1,
    "tags": "python;nlp;nltk;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 34363250,
    "title": "Understanding Word2Vec&#39;s Skip-Gram Structure and Output",
    "body": "<p>I have a two-fold question about the Skip-Gram model in Word2Vec:</p>\n<ul>\n<li><p>The first part is about structure: as far as I understand it, the Skip-Gram model is based on one neural network with one input weight matrix <strong>W</strong>, one hidden layer of size N, and C output weight matrices <strong>W'</strong> each used to produce one of the C output vectors. Is this correct?</p>\n</li>\n<li><p>The second part is about the output vectors: as far as I understand it, each output vector is of size V and is a result of a Softmax function. Each output vector <em>node</em> corresponds to the index of a word in the vocabulary, and the value of each node is the probability that the corresponding word occurs at that context location (for a given input word). The target output vectors are not, however, one-hot encoded, even if the training instances are. Is this correct?</p>\n</li>\n</ul>\n<p>The way I imagine it is something along the following lines (made-up example):</p>\n<p>Assuming the vocabulary ['quick', 'fox', 'jumped', 'lazy', 'dog'] and a context of C=1, and assuming that for the input word 'jumped' I see the two output vectors looking like this:</p>\n<p>[0.2 <strong>0.6</strong> 0.01 0.1 0.09]</p>\n<p>[0.2 0.2 0.01 0.16 <strong>0.43</strong>]</p>\n<p>I would interpret this as 'fox' being the most likely word to show up before 'jumped' (p=0.6), and 'dog' being the most likely to show up after it (p=0.43).</p>\n<p>Do I have this right? Or am I completely off?</p>\n",
    "score": 6,
    "creation_date": 1450469528,
    "view_count": 4463,
    "answer_count": 2,
    "tags": "vector;machine-learning;nlp;word2vec"
  },
  {
    "question_id": 34083039,
    "title": "NLTK - WordNet: list of long words",
    "body": "<p>I would like to find words in WordNet that are at least 18 character long. I tried the following code:</p>\n\n<pre><code>from nltk.corpus import wordnet as wn\nsorted(w for w in wn.synset().name() if len(w)&gt;18)\n</code></pre>\n\n<p>I get the following error message:</p>\n\n<blockquote>\n<pre><code>sorted(w for w in wn.synset().name() if len(w)&gt;18)\n</code></pre>\n  \n  <p>TypeError: synset() missing 1 required positional argument: 'name'</p>\n</blockquote>\n\n<p>I am using Python 3.4.3.</p>\n\n<p>How can I fix my code?</p>\n",
    "score": 6,
    "creation_date": 1449213974,
    "view_count": 5506,
    "answer_count": 3,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 27769121,
    "title": "Algorithm for Determining Word Type using WordNet Database",
    "body": "<p>I'm working on a project which requires scanning through paragraphs of natural text in English and detecting what type of word they are. The application works with AJAX, PHP, and MySQL.</p>\n\n<p>My application doesn't need to be 100% accurate and simply tries to find the best content that matches text input. To do this I've used an SQL version of the WordNet database which allows me to search for words and their types as so using the <code>dict</code> view.</p>\n\n<pre><code>SELECT lemma, pos FROM dict WHERE lemma = 'fool' ORDER BY lemma;\n</code></pre>\n\n<p>The above is an example of what the database sees but my PHP actually creates dynamic bound parameters based on the text from the AJAX calls and in reality, will contain many keywords.</p>\n\n<p>This will return an array of records with each word searched for and their type.</p>\n\n<p>My problem however is that most words can be multiple types, for example, with the fool example, it brings back three as a noun, and four as a verb. The minute differences aren't needed for me but I would like to know if the word is a noun or a verb in it's usage.</p>\n\n<p>This problem persists across most words which means I cannot accurately detect different types of words because it could be any of the uses.</p>\n\n<p>I am wondering if anybody could point me in the right direction of an algorithm or what I may be able to do in order to give at the very least a best guess at what the word type is.</p>\n\n<p>The ones most important to get right are adjectives and nouns.</p>\n",
    "score": 6,
    "creation_date": 1420397598,
    "view_count": 1075,
    "answer_count": 1,
    "tags": "php;mysql;algorithm;nlp;wordnet"
  },
  {
    "question_id": 20907909,
    "title": "Stanford CoreNLP remove/stop red information print outs",
    "body": "<p>I'm using Stanford's CoreNLP Java API and while running it prints out information in red.\nIt just fills up the command lines when i don't want to see it.\nis there anyway of disabling this feature?</p>\n<p>Example of the red info lines:</p>\n<pre><code>Searching for resource: StanfordCoreNLP.properties\nSearching for resource: edu/stanford/nlp/pipeline/StanfordCoreNLP.properties\nAdding annotator tokenize\nAdding annotator ssplit\nAdding annotator pos\nReading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.2 sec].\nAdding annotator lemma\nAdding annotator ner\nLoading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [3.0 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.7 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [2.0 sec].\nInitializing JollyDayHoliday for sutime\nReading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt\nReading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt\nJan 03, 2014 3:52:37 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules\nINFO: Ignoring inactive rule: temporal-composite-8:ranges\nReading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt\nAdding annotator parse\nLoading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.8 sec].\nAdding annotator dcoref\nSearching for resource: StanfordCoreNLP.properties\nSearching for resource: edu/stanford/nlp/pipeline/StanfordCoreNLP.properties\nAdding annotator tokenize\nAdding annotator ssplit\nAdding annotator pos\nAdding annotator lemma\nAdding annotator ner\nAdding annotator parse\nAdding annotator dcoref\nSearching for resource: StanfordCoreNLP.properties\nSearching for resource: edu/stanford/nlp/pipeline/StanfordCoreNLP.properties\nAdding annotator tokenize\nAdding annotator ssplit\nAdding annotator pos\nAdding annotator lemma\nAdding annotator ner\nAdding annotator parse\nAdding annotator dcoref\n</code></pre>\n",
    "score": 6,
    "creation_date": 1388765390,
    "view_count": 2213,
    "answer_count": 1,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 18230269,
    "title": "where can I download the ispell *.dict and *.affix files?",
    "body": "<p>I am quite new to postgresql full text search and I am setting up the configuration as where can I download the ispell *.dict and *.affix filefollowing (exactly as in <a href=\"http://www.postgresql.org/docs/8.3/static/textsearch-dictionaries.html\" rel=\"nofollow noreferrer\">docs</a>):</p>\n\n<pre><code>CREATE TEXT SEARCH DICTIONARY english_ispell (\n    TEMPLATE = ispell,\n    DictFile = english, \n    AffFile = english, \n    StopWords = english\n);\n</code></pre>\n\n<p>So, this I think expects files <code>english.dict</code> and <code>english.affix</code> on for example:</p>\n\n<pre><code>/usr/share/postgresql/9.2/tsearch_data\n</code></pre>\n\n<p>But these files are not there. I just have <code>ispell_sample.dict</code> and <code>ispell_sample.affix</code> - which when included above work fine - no problem.</p>\n\n<p>So... I followed this <a href=\"https://stackoverflow.com/questions/14300874/tsvector-only-supports-english\">post</a> and downloaded the required dictionary from <a href=\"http://extensions.openoffice.org/en/project/english-dictionaries-apache-openoffice\" rel=\"nofollow noreferrer\">the open office people</a> and renamed the <code>.dic</code> to <code>.dict</code> and <code>.aff</code> to <code>.affix</code>. Then I have checked (using <code>file -bi dict.affix</code> and <code>file -bi english.dict</code> and they are UTF8 encoded).</p>\n\n<p>When I run the above text search dictionary, I get the error:</p>\n\n<pre><code> ERROR:  wrong affix file format for flag\n CONTEXT:  line 2778 of configuration file \"/usr/share/postgresql/9.2/tsearch_data/english.affix\": \"COMPOUNDMIN 1\n \"\n</code></pre>\n\n<p>I was wondering if anyone had clues on how to solve this problem or if anyone had encountered this before..\nThanks./.</p>\n\n<p>UPDATE:1: I guess the question can be rephrased as follows:\n<code>where can I download the ispell *.dict and *.affix file for postgres</code></p>\n",
    "score": 6,
    "creation_date": 1376479021,
    "view_count": 5378,
    "answer_count": 2,
    "tags": "dictionary;full-text-search;nlp;ispell"
  },
  {
    "question_id": 16823609,
    "title": "Natural Language Processing - Converting Text Features Into Feature Vectors",
    "body": "<p>So I've been working on a natural language processing project in which I need to classify different styles of writing. Assuming that semantic features from texts have already been extracted for me, I plan to use Weka in Java to train SVM classifiers using these features that can be used to classify other different texts. </p>\n\n<p>The part I'm having trouble on is that to train an SVM, the features must be converted into a feature vector. I'm not sure how you would be able to represent features such as vocabulary richness, n-grams, punctuation, number of paragraphs, and paragraph length as numbers in a vector. If somebody could point in the right direction, that would be greatly appreciated.</p>\n",
    "score": 6,
    "creation_date": 1369860880,
    "view_count": 1573,
    "answer_count": 1,
    "tags": "java;nlp;svm;text-classification"
  },
  {
    "question_id": 4710261,
    "title": "How can I programmatically generate relevant tags for a database of URLs?",
    "body": "<p>I'm writing an RSS reader in python as a learning exercise, and I would really like to be able to tag individual entries with keywords for searching.  Unfortunately, most real-world feeds don't include keyword metadata.  I currently have about 60,000 entries in my test database from about 600 feeds, so manually tagging is not going to be effective.  So far I have only been able to find two solutions:</p>\n\n<p><strong>1: Use <a href=\"http://code.google.com/p/nltk/\" rel=\"noreferrer\">Natural Language Toolkit</a> to extract keywords:</strong></p>\n\n<ul>\n<li>Pros: flexible; no dependencies on external services;</li>\n<li>Cons: can only index the article summary, not the article; non-trivial: writing a high quality keyword extraction tool is a project in itself;</li>\n</ul>\n\n<p><strong>2: Use the <a href=\"http://code.google.com/apis/adwords/docs/reference/latest/TargetingIdeaService.RelatedToUrlSearchParameter.html\" rel=\"noreferrer\">Google Adwords API</a> to fetch keyword suggestions from the article url:</strong></p>\n\n<ul>\n<li>Pros: Super high quality keywords; based on entire article text; easy to use;</li>\n<li>Cons: Not free(?); Query rate limits unknown; I'm terrified of getting my account banned and not being able to run adwords campaigns for my commercial sites;</li>\n</ul>\n\n<p>Can anyone offer any suggestions?  Are my fears about getting my adwords account banned unfounded?</p>\n",
    "score": 6,
    "creation_date": 1295242125,
    "view_count": 2185,
    "answer_count": 2,
    "tags": "python;nlp;keyword;google-ads-api"
  },
  {
    "question_id": 62526950,
    "title": "What machine instance to use for running GPU workloads in Google Cloud Platform",
    "body": "<p>I am trying to run Elasticsearch BERT application and would like to understand the minimal configuration for fine-tuning the model using GPU. What machine configuration should I be using?</p>\n<p>Reference github: <a href=\"https://github.com/kaushaltrivedi/fast-bert\" rel=\"noreferrer\">Fast-Bert</a></p>\n",
    "score": 6,
    "creation_date": 1592883599,
    "view_count": 435,
    "answer_count": 1,
    "tags": "google-cloud-platform;nlp;pytorch;tensorflow2.0"
  },
  {
    "question_id": 59332776,
    "title": "Error: Class advice impossible in Python3 topia.termextract",
    "body": "<p>I am performing an nlp task. I have written the following code for topia.termextract. While executing it isshowing errors. It will be helpful, if you suggest to resolve the errors. </p>\n\n<pre><code>from topia.termextract import extract\nfrom topia.termextract import tag\n\n# Setup Term Extractor\nextractor = extract.TermExtractor()\n\n# Some sample text\ntext ='''\nPolice shut Palestinian theatre in Jerusalem.\n\nIsraeli police have shut down a Palestinian theatre in East Jerusalem.\n\nThe action, on Thursday, prevented the closing event of an international\nliterature festival from taking place.\"\"\"\n\n# Extract Keywords\nkeywords_topica = extractor(text)\nprint(keywords_topica)\n</code></pre>\n\n<p>I am using Python 3 in google colab. </p>\n\n<pre><code>---------------------------------------------------------------------------      \nTypeError                                 Traceback (most recent call last)      \n&lt;ipython-input-23-9a094f024dfe&gt; in &lt;module&gt;()      \n ----&gt; 1 from topia.termextract import extract    \n       2 from topia.termextract import tag     \n\n 3 frames      \n /usr/local/lib/python3.6/dist-packages/zope/interface/declarations.py in implements(*interfaces)    \n     481     # the coverage for this block there. :(     \n     482     if PYTHON3:     \n --&gt; 483         raise TypeError(_ADVICE_ERROR % 'implementer')    \n     484     _implements(\"implements\", interfaces, classImplements)    \n     485      \n\n     TypeError: Class advice impossible in Python3.  Use the @implementer class decorator instead.     \n</code></pre>\n",
    "score": 6,
    "creation_date": 1576303934,
    "view_count": 671,
    "answer_count": 2,
    "tags": "python;python-3.x;nlp;google-colaboratory;keyword-search"
  },
  {
    "question_id": 57598318,
    "title": "How to use sklearn TfidfVectorizer on new data",
    "body": "<p>I have a fairly simple NLTK and sklearn classifier (I'm a complete noob at this). </p>\n\n<p>I do the usual imports</p>\n\n<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import RegexpTokenizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn import metrics\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n</code></pre>\n\n<p>I load the data (I already cleaned it). It is a very simple dataframe with two columns. The first is <code>'post_clean'</code> which contains the cleaned text, the second is <code>'uk'</code> which is either <code>True</code> or <code>False</code></p>\n\n<pre><code>data = pd.read_pickle('us_uk_posts.pkl')\n</code></pre>\n\n<p>Then I Vectorize with tfidf and split the dataset, followed by creating the model</p>\n\n<pre><code>tf = TfidfVectorizer()\ntext_tf = tf.fit_transform(data['post_clean'])\nX_train, X_test, y_train, y_test = train_test_split(text_tf, data['uk'], test_size=0.3, random_state=123)\n\n\nclf = MultinomialNB().fit(X_train, y_train)\npredicted = clf.predict(X_test)\nprint(\"MultinomialNB Accuracy:\" , metrics.accuracy_score(y_test,predicted))\n</code></pre>\n\n<p>Apparently, unless I'm completely missing something here, I have Accuracy of 93%</p>\n\n<p>My two questions are:</p>\n\n<p>1) How do I now use this model to actually classify some items that don't have a known <code>UK</code> value? </p>\n\n<p>2) How do I test this model using a completely separate test set (that I haven't split)?</p>\n\n<p>I have tried</p>\n\n<p><code>new_data = pd.read_pickle('new_posts.pkl')</code></p>\n\n<p>Where the new_posts data is in the same format</p>\n\n<pre><code>new_text_tf = tf.fit_transform(new_data['post_clean'])\n\npredicted = clf.predict(new_X_train)\npredicted\n</code></pre>\n\n<p>and</p>\n\n<pre><code>new_text_tf = tf.fit_transform(new_data['post_clean'])\n\nnew_X_train, new_X_test, new_y_train, new_y_test = train_test_split(new_text_tf, new_data['uk'], test_size=1)\n\npredicted = clf.predict(new_text_tf)\npredicted\n</code></pre>\n\n<p>but both  return \"ValueError: dimension mismatch\"</p>\n",
    "score": 6,
    "creation_date": 1566416882,
    "view_count": 7369,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp;nltk"
  },
  {
    "question_id": 56556837,
    "title": "Does keras-tokenizer perform the task of lemmatization and stemming?",
    "body": "<p>Does keras tokenizer provide the functions such as stemming and lemmetization? If it does, then how is it done? Need an intuitive understanding. Also, what does <code>text_to_sequence</code> do in that?</p>\n",
    "score": 6,
    "creation_date": 1560324783,
    "view_count": 2422,
    "answer_count": 1,
    "tags": "keras;nlp;tokenize;stemming;lemmatization"
  },
  {
    "question_id": 53488264,
    "title": "How node2vec works",
    "body": "<p>I have been reading about the <a href=\"https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf\" rel=\"noreferrer\">node2vec</a> embedding algorithm and I am a little confused how it works. </p>\n\n<p>For reference, node2vec is parametrised by p and q and works by simulating a bunch of random walks from nodes and just running word2vec embeddings on these walks as \"sentences\". By setting p and q in different ways, you can get more BFS or more DFS type random walks in the simulataion phase, capturing different network structure in the embedding.</p>\n\n<p>Setting q > 1 gives us more BFS behaviour in that the samples of walks comprise of nodes within a small locality.  The thing I am confused about is that the paper says this is equivalent to embedding nodes with similar structural properties close to each other.</p>\n\n<p>I don't quite understand how that works. If I have two separate say star/hub structured nodes in my network that are far apart, why would embedding based on the random walks from those two nodes put those two nodes close together in the embedding?</p>\n",
    "score": 6,
    "creation_date": 1543262845,
    "view_count": 1253,
    "answer_count": 2,
    "tags": "machine-learning;nlp;graph-theory"
  },
  {
    "question_id": 50697092,
    "title": "How to get the wikipedia corpus text with punctuation by using gensim wikicorpus?",
    "body": "<p>I'm trying to get the text with its punctuation as it is important to consider the latter in my doc2vec model.  However, the wikicorpus retrieve only the text. After searching the web I found these pages:</p>\n\n<ol>\n<li>Page from gensim github issues section. It was a question by someone where the answer was to subclass WikiCorpus (answered by Piskvorky). Luckily, in the same page, there was a code representing the suggested 'subclass' solution. The code was provided by Rhazegh. (<a href=\"https://github.com/RaRe-Technologies/gensim/issues/552\" rel=\"noreferrer\">link</a>)</li>\n<li>Page from stackoverflow with a title: \"Disabling Gensim's removal of punctuation etc. when parsing a wiki corpus\". However, no clear answer was provided and was treated in the context of spaCy. (<a href=\"https://stackoverflow.com/questions/43500996/disabling-gensims-removal-of-punctuation-etc-when-parsing-a-wiki-corpus\">link</a>)</li>\n</ol>\n\n<p>I decided to use the code provided in page 1. My current code (mywikicorpus.py):</p>\n\n<pre><code>import sys\nimport os\nsys.path.append('C:\\\\Users\\\\Ghaliamus\\\\Anaconda2\\\\envs\\\\wiki\\\\Lib\\\\site-packages\\\\gensim\\\\corpora\\\\')\n\nfrom wikicorpus import *\n\ndef tokenize(content):\n    # override original method in wikicorpus.py\n    return [token.encode('utf8') for token in utils.tokenize(content, lower=True, errors='ignore')\n        if len(token) &lt;= 15 and not token.startswith('_')]\n\ndef process_article(args):\n   # override original method in wikicorpus.py\n    text, lemmatize, title, pageid = args\n    text = filter_wiki(text)\n    if lemmatize:\n        result = utils.lemmatize(text)\n    else:\n        result = tokenize(text)\n    return result, title, pageid\n\n\nclass MyWikiCorpus(WikiCorpus):\ndef __init__(self, fname, processes=None, lemmatize=utils.has_pattern(), dictionary=None, filter_namespaces=('0',)):\n    WikiCorpus.__init__(self, fname, processes, lemmatize, dictionary, filter_namespaces)\n\n    def get_texts(self):\n        articles, articles_all = 0, 0\n        positions, positions_all = 0, 0\n        texts = ((text, self.lemmatize, title, pageid) for title, text, pageid in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces))\n        pool = multiprocessing.Pool(self.processes)\n        for group in utils.chunkize(texts, chunksize=10 * self.processes, maxsize=1):\n            for tokens, title, pageid in pool.imap(process_article, group):  # chunksize=10):\n                articles_all += 1\n                positions_all += len(tokens)\n            if len(tokens) &lt; ARTICLE_MIN_WORDS or any(title.startswith(ignore + ':') for ignore in IGNORED_NAMESPACES):\n                continue\n            articles += 1\n            positions += len(tokens)\n            if self.metadata:\n                yield (tokens, (pageid, title))\n            else:\n                yield tokens\n    pool.terminate()\n\n    logger.info(\n        \"finished iterating over Wikipedia corpus of %i documents with %i positions\"\n        \" (total %i articles, %i positions before pruning articles shorter than %i words)\",\n        articles, positions, articles_all, positions_all, ARTICLE_MIN_WORDS)\n    self.length = articles  # cache corpus length\n</code></pre>\n\n<p>And then, I used another code by Pan Yang (<a href=\"https://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim\" rel=\"noreferrer\">link</a>). This code initiates WikiCorpus object and retrieve the text. The only change in my current code is initiating MyWikiCorpus instead of WikiCorpus. The code (process_wiki.py):</p>\n\n<pre><code>from __future__ import print_function\nimport logging\nimport os.path\nimport six\nimport sys\nimport mywikicorpus as myModule\n\n\n\nif __name__ == '__main__':\n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program)\n\n    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(\"running %s\" % ' '.join(sys.argv))\n\n    # check and process input arguments\n    if len(sys.argv) != 3:\n        print(\"Using: python process_wiki.py enwiki-20180601-pages-    articles.xml.bz2 wiki.en.text\")\n        sys.exit(1)\n    inp, outp = sys.argv[1:3]\n    space = \" \"\n    i = 0\n\n    output = open(outp, 'w')\n    wiki = myModule.MyWikiCorpus(inp, lemmatize=False, dictionary={})\n    for text in wiki.get_texts():\n        if six.PY3:\n            output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n        else:\n            output.write(space.join(text) + \"\\n\")\n        i = i + 1\n        if (i % 10000 == 0):\n            logger.info(\"Saved \" + str(i) + \" articles\")\n\n    output.close()\n    logger.info(\"Finished Saved \" + str(i) + \" articles\")\n</code></pre>\n\n<p>Through command line I ran the process_wiki.py code. I got text of the corpus with the last line in the command prompt: </p>\n\n<p>(2018-06-05 09:18:16,480: INFO: Finished Saved 4526191 articles)</p>\n\n<p>When I read the file in python, I checked the first article and it was without punctuation. Example:</p>\n\n<p>(anarchism is a political philosophy that advocates self governed societies based on voluntary institutions these are often described as stateless societies although several authors have defined them more specifically as institutions based on non hierarchical or free associations anarchism holds the state to be undesirable unnecessary and harmful while opposition to the state is central anarchism specifically entails opposing authority or hierarchical)</p>\n\n<p>My two relevant questions, and I wish you can help me with them, please:</p>\n\n<ol>\n<li>is there any thing wrong in my reported pipeline above?</li>\n<li>regardless such pipeline, if I opened the gensim  wikicorpus python code (<a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/wikicorpus.py\" rel=\"noreferrer\">wikicorpus.py</a>) and wanted to edit it, what is the line that I should add it or remove it or update it (with what if possible) to get the same results but with punctuation?</li>\n</ol>\n\n<p>Many thanks for your time reading this long post. </p>\n\n<p>Best wishes,</p>\n\n<p>Ghaliamus </p>\n",
    "score": 6,
    "creation_date": 1528192130,
    "view_count": 3339,
    "answer_count": 2,
    "tags": "python;nlp;gensim;doc2vec"
  },
  {
    "question_id": 47584738,
    "title": "NLTK CoreNLPDependencyParser: Failed to establish connection",
    "body": "<p>I'm trying to use the Stanford Parser through NLTK, following the example <a href=\"http://www.nltk.org/api/nltk.parse.html#nltk.parse.corenlp.CoreNLPDependencyParser%20tutorial%20here\" rel=\"noreferrer\">here</a>.</p>\n\n<p>I follow the first two lines of the example (with the necessary import)</p>\n\n<pre><code>from nltk.parse.corenlp import CoreNLPDependencyParser\ndep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\nparse, = dep_parser.raw_parse('The quick brown fox jumps over the lazy dog.')\n</code></pre>\n\n<p>but I get an error saying:</p>\n\n<pre><code>[...] Failed to establish a new connection: [Errno 61] Connection refused\"\n</code></pre>\n\n<p>I realize that it must be an issue with trying to connect to the url given as input to the constructor.</p>\n\n<pre><code>dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n</code></pre>\n\n<p>What url should I be connecting to, if not this? If this is correct, what is the issue?</p>\n",
    "score": 6,
    "creation_date": 1512087312,
    "view_count": 3579,
    "answer_count": 1,
    "tags": "python;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 36034602,
    "title": "Is it possible to returned the analyzed fields in an ElasticSearch &gt;2.0 search?",
    "body": "<p>This question feels very similar to an old question posted here: <a href=\"https://stackoverflow.com/questions/13404722/retrieve-analyzed-tokens-from-elasticsearch-documents\">Retrieve analyzed tokens from ElasticSearch documents</a>, but to see if there are any changes I thought it would make sense to post it again for the latest version of ElasticSearch.</p>\n\n<p>We are trying to search bodies of text in ElasticSearch with the <strong>search-query</strong> and <strong>field-mapping</strong> using the <strong>snowball</strong> stemmer built into ElasticSearch. The performance and results are great, but because we need to have the stemmed text-body for post-analysis we would like to have the search result return the actual stemmed tokens for the text-field per document in the search results.</p>\n\n<p>The mapping for the field currently looks like:</p>\n\n<pre><code>      \"TitleEnglish\": {\n        \"type\": \"string\",\n        \"analyzer\": \"standard\",\n        \"fields\": {\n          \"english\": {\n            \"type\": \"string\",\n            \"analyzer\": \"english\"\n          },\n          \"stemming\": {\n            \"type\": \"string\",\n            \"analyzer\": \"snowball\"\n          }\n        }\n      }\n</code></pre>\n\n<p>and the search query is performed specifically on <strong>TitleEnglish.stemming</strong>. Ideally I would like it to return that field, but returning that does not return the analyzed field but the original field.</p>\n\n<p>Does anybody know of any way to do this? We have looked at <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-termvectors.html\" rel=\"noreferrer\">Term Vectors</a>, but they only seem to be returnable for individual documents or a body of documents, not for a search result?</p>\n\n<p>Or perhaps other solutions like Solr or Sphinx do offer this option?</p>\n\n<hr>\n\n<p>To add some extra information. If we run the following query:</p>\n\n<pre><code>GET /_analyze?analyzer=snowball&amp;text=Eight issue of Industrial Lorestan eliminate barriers to facilitate the Committees review of\n</code></pre>\n\n<p>It returns the stemmed words: <code>eight</code>, <code>issu</code>, <code>industri</code>, etc. This is exactly the result we would like back for each matching document for all of the words in the text (so not just the matches).</p>\n",
    "score": 6,
    "creation_date": 1458128245,
    "view_count": 575,
    "answer_count": 1,
    "tags": "elasticsearch;lucene;nlp"
  },
  {
    "question_id": 35070452,
    "title": "How to correct spelling in a Pandas DataFrame",
    "body": "<p>Using the <a href=\"http://textblob.readthedocs.org/en/dev/quickstart.html#spelling-correction\" rel=\"noreferrer\">TextBlob</a> library it is possible to improve the spelling of strings by defining them as TextBlob objects first and then using the <code>correct</code> method. </p>\n\n<p>Example:</p>\n\n<pre><code>from textblob import TextBlob\ndata = TextBlob('Two raods diverrged in a yullow waod and surry I culd not travl bouth')\nprint (data.correct())\nTwo roads diverged in a yellow wood and sorry I could not travel both\n</code></pre>\n\n<p>Is it possible to do this to strings in a Pandas DataFrame series such as this one:</p>\n\n<pre><code>data = [{'one': '3', 'two': 'two raods'}, \n         {'one': '7', 'two': 'diverrged in a yullow'}, \n        {'one': '8', 'two': 'waod and surry I'}, \n        {'one': '9', 'two': 'culd not travl bouth'}]\ndf = pd.DataFrame(data)\ndf\n\n    one   two\n0   3     Two raods\n1   7     diverrged in a yullow\n2   8     waod and surry I\n3   9     culd not travl bouth\n</code></pre>\n\n<p>To return this:</p>\n\n<pre><code>    one   two\n0   3     Two roads\n1   7     diverged in a yellow\n2   8     wood and sorry I\n3   9     could not travel both\n</code></pre>\n\n<p>Either using TextBlob or some other method. </p>\n",
    "score": 6,
    "creation_date": 1454009749,
    "view_count": 13323,
    "answer_count": 2,
    "tags": "python;pandas;nlp;textblob"
  },
  {
    "question_id": 27217103,
    "title": "Using Wordnet to generate superlative, comparative and adjectives",
    "body": "<p>I have a wordnet database setup, and I'm trying to generate synonyms for various words.</p>\n\n<p>For example, the word, \"greatest\". I'll look through and find several different synonyms, but none of them really fit the definition - for example, one is \"superlative\".</p>\n\n<p>I'm guessing that I need to do some sort of check by frequency in a given language or stemming a word to get the base word (for example, greatest -> great, great -> best).</p>\n\n<p>What table should I be using to ensure my words make some modicum of sense?</p>\n",
    "score": 6,
    "creation_date": 1417374710,
    "view_count": 1738,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 22797393,
    "title": "Exactly replicating R text preprocessing in python",
    "body": "<p>I would like to preprocess a corpus of documents using Python in the same way that I can in R. For example, given an initial corpus, <code>corpus</code>, I would like to end up with a preprocessed corpus that corresponds to the one produced using the following R code:</p>\n\n<pre><code>library(tm)\nlibrary(SnowballC)\n\ncorpus = tm_map(corpus, tolower)\ncorpus = tm_map(corpus, removePunctuation)\ncorpus = tm_map(corpus, removeWords, c(\"myword\", stopwords(\"english\")))\ncorpus = tm_map(corpus, stemDocument)\n</code></pre>\n\n<p>Is there a simple or straightforward — preferably pre-built — method of doing this in Python? Is there a way to ensure exactly the same results?</p>\n\n<hr>\n\n<p>For example, I would like to preprocess</p>\n\n<blockquote>\n  <p>@Apple ear pods are AMAZING! Best sound from in-ear headphones I've\n  ever had!</p>\n</blockquote>\n\n<p>into</p>\n\n<blockquote>\n  <p>ear pod  amaz best sound  inear headphon ive ever</p>\n</blockquote>\n",
    "score": 6,
    "creation_date": 1396388323,
    "view_count": 2738,
    "answer_count": 2,
    "tags": "python;r;nlp;analytics;scikit-learn"
  },
  {
    "question_id": 22642668,
    "title": "Extracting information from unstructured text",
    "body": "<p>I have a collection of \"articles\", each 1 to 10 sentences long, written in a noisy, informal english (i.e. social media style).\nI need to extract some information from each article, where available, like date and time. I also need to understand what the article is talking about and who is the main \"actor\". </p>\n\n<p>Example, given the sentence: <em>\"Everybody's presence is required tomorrow morning starting from 10.30 to discuss the company's financial forecast.\"</em>, I need to extract: </p>\n\n<ul>\n<li>the date/time => \"10.30 tomorrow morning\". </li>\n<li>the topic => \"company's financial forecast\".</li>\n<li>the actor => \"Everybody\".</li>\n</ul>\n\n<p>As far as I know, the date and time could be extracted without using NLP techniques but I haven't found anything as good as Natty (<a href=\"http://natty.joestelmach.com/\" rel=\"noreferrer\">http://natty.joestelmach.com/</a>) in Python.</p>\n\n<p>My understanding on how to proceed after reading some chapters of the NLTK book and watching some videos of the NLP courses on Coursera is the following:</p>\n\n<ol>\n<li>Use part of the data to create an annotated corpus. I can't use off-the-shelf corpus because of the informal nature of the text (e.g. spelling errors, uninformative capitalization, word abbreviations, etc...). </li>\n<li>Manually (sigh...) annotate each article with tags from the Penn TreeBank tagset. <strong>Is there any way to automate this step and just check/fix the results ?</strong></li>\n<li>Train a POS tagger on the annotated article. I've found the NLTK-trainer project that seems promising (<a href=\"http://nltk-trainer.readthedocs.org/en/latest/train_tagger.html\" rel=\"noreferrer\">http://nltk-trainer.readthedocs.org/en/latest/train_tagger.html</a>). </li>\n<li>Chunking/Chinking, which means I'll have to manually annotate the corpus again (...) using the IOB notation. Unfortunately according to this bug report n-gram chunkers are broken: <a href=\"https://github.com/nltk/nltk/issues/367\" rel=\"noreferrer\">https://github.com/nltk/nltk/issues/367</a>. This seems like a major issue, and makes me wonder whether I should keep using NLTK given that it's more than a year old. </li>\n<li>At this point, if I have done everything correctly, I assume I'll find actor, topic and datetime in the chunks. <strong>Correct ?</strong></li>\n</ol>\n\n<p>Could I (temporarily) skip 1,2 and 3 and produce a working, but possibly with a high error rate, implementation ? <strong>Which corpus should I use ?</strong></p>\n\n<p>I was also thinking of a pre-process step to correct common spelling mistakes or shortcuts like \"yess\", \"c u\" and other abominations. <strong>Anything already existing I can take advantage of ?</strong></p>\n\n<p><strong>THE question, in a nutshell, is: is my approach at solving this problem correct ? If not, what am I doing wrong ?</strong></p>\n",
    "score": 6,
    "creation_date": 1395770341,
    "view_count": 2335,
    "answer_count": 2,
    "tags": "nlp;nltk"
  },
  {
    "question_id": 13786635,
    "title": "How can I use text analysis in order to investigate questionnaire responses?",
    "body": "<p>I'm the \"programmer\" of a team of pupils that aims to investigate satisfaction and general problems in my grammar school. We have a questionary that is built upon a scale from 1-6 and we interpret these answers by a diagram software that I wrote in python.</p>\n\n<p>Now there's a <code>&lt;textarea&gt;</code> at the end of our questionary that one can use as he likes.\nI'm currently thinking of ways to make this data usable (we don't want to read more than 800+ answers). </p>\n\n<p>How can I use text analysis in Python to investigate what pupils write?\nI was thinking of a way to \"tag\" any sentence that is written down, like:</p>\n\n<pre><code>I don't like being in school. [wellbeing][negative]\nI have way too much homework. [homework][much]\nI think there should be more interesting projects. [projects][more]\n</code></pre>\n\n<p>Are there any usable approaches to obtain that? Does it make sense to use an existing tokenizer?</p>\n\n<p>Thanks for your help!</p>\n",
    "score": 6,
    "creation_date": 1355048761,
    "view_count": 1494,
    "answer_count": 4,
    "tags": "python;statistics;computer-science;lexical-analysis;text-analysis"
  },
  {
    "question_id": 13466584,
    "title": "korean language tokenizer",
    "body": "<p>What is the best tokenizer exist for processing Korean language?  </p>\n\n<p>I have tried <strong><em><a href=\"http://lucene.apache.org/core/old_versioned_docs/versions/2_9_0/api/all/org/apache/lucene/analysis/cjk/CJKTokenizer.html\" rel=\"noreferrer\">CJKTokenizer</a> in Solr4.0</em></strong>. It is doing the tokenization, but accuracy is very low.</p>\n",
    "score": 6,
    "creation_date": 1353385516,
    "view_count": 2894,
    "answer_count": 1,
    "tags": "localization;solr;nlp;tokenize"
  },
  {
    "question_id": 10760330,
    "title": "How to implement Knowledge graph",
    "body": "<p>I'm looking forward to implement something like google direct answers which uses knowledge graph, is there any useful resource can I read ? also Where can I find data for that?</p>\n",
    "score": 6,
    "creation_date": 1337974247,
    "view_count": 2084,
    "answer_count": 3,
    "tags": "nlp;semantic-web;nlp-question-answering;knowledge-graph"
  },
  {
    "question_id": 7752785,
    "title": "Which is the best document clustering open-source package?",
    "body": "<p>Which open-source package is the best for clustering a large corpus of documents? It should either decide the number of clusters by itself or it can also accept that as a parameter.</p>\n\n<p>We have a large corpus of documents that don't really revolve around a particular topic - they are documents produced by sales and management people on various projects and clients in the organisation. I know that having such a spread corpus will degrade the performance, but we are trying to live with the best that we can get. Now, what is the best we can get :-)</p>\n",
    "score": 6,
    "creation_date": 1318501800,
    "view_count": 5172,
    "answer_count": 1,
    "tags": "nlp;machine-learning;cluster-analysis"
  },
  {
    "question_id": 7290197,
    "title": "Paraphrasing for Math Word Problems (Changing sentence structure without changing meaning)",
    "body": "<p>I'm working on Khan Academy's <a href=\"https://github.com/Khan/khan-exercises\" rel=\"nofollow\">exercise framework</a>, and more specifically, word problems.</p>\n\n<p>When doing a word problem exercise, students often get the same word problem, only with numbers and names changed. This is not ideal, as students can quickly learn the pattern and extract relevant data without reading the entire problem. </p>\n\n<p>Are there any ways of changing sentence structure without changing the meaning of the word problem? Any other ideas of how to solve this repetition problem are also welcomed.</p>\n",
    "score": 6,
    "creation_date": 1315006867,
    "view_count": 1756,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 6615833,
    "title": "Clustering conceptually similar documents together?",
    "body": "<p>This is more of a conceptual question than an actual implementation and am hoping someone could clarify. My goal is the following: Given a set of documents, I want to cluster them such that documents belonging to the same cluster have the same \"concept\". </p>\n\n<p>From what I understand, <a href=\"http://en.wikipedia.org/wiki/Latent_semantic_analysis\" rel=\"nofollow noreferrer\">Latent Semantic Analysis</a> lets me find a low rank approximation of a term-document matrix i.e. given a matrix <strong>X</strong>, it will decompose <strong>X</strong> as a product of three matrices, out of which one would be a diagonal matrix <strong>Σ</strong>:</p>\n\n<p><img src=\"https://i.sstatic.net/Ejhy5.png\" alt=\"SVD\"></p>\n\n<p>Now, I would proceed by choosing a low rank approximation i.e. choose only the top-k values from <strong>Σ</strong>, and then calculate <strong>X'</strong>. Once I have this matrix, I have to apply some clustering algorithm and the end result would be set of clusters grouping documents with similar concepts. Is this the right way of applying clustering? I mean, calculating <strong>X'</strong> and then applying clustering on top of it or is there some other method that is followed?</p>\n\n<p>Also, in a somewhat <a href=\"https://stackoverflow.com/questions/5751114/nearest-neighbors-in-high-dimensional-data\">related question</a> of mine, I was told that the meaning of a <em>neighbor</em> is lost as the number of dimensions increases. In that case, what is the justification for clustering these high dimensional data points from <strong>X'</strong>? I am guessing that the requirement to cluster similar documents is a real-world requirement in which case, how does one go about addressing this?</p>\n",
    "score": 6,
    "creation_date": 1310066107,
    "view_count": 1328,
    "answer_count": 1,
    "tags": "python;numpy;nlp;machine-learning;data-mining"
  },
  {
    "question_id": 5781648,
    "title": "Word Map for Emotions",
    "body": "<p>I am looking for a resource similar to WordNet. However, I want to be able to look up the positive/negative connotation of a word. For example:</p>\n\n<pre><code>bribe - negative\noffer - positive\n</code></pre>\n\n<p>I'm curious as to whether anyone has run across any tool like this in AI/NLP research, or even in linguistics.</p>\n\n<p><strong>UPDATE:</strong>\nFor the curious, the accepted answer below put me on the right track towards what I needed. Wikipedia listed several different resources. The two I would recommend (because of ease of use/free use for a small number of API calls) are <a href=\"http://www.alchemyapi.com/\" rel=\"nofollow\">AlchemyAPI</a> and <a href=\"http://www.lymbix.com/\" rel=\"nofollow\">Lymbix</a>. I decided to go with AlchemyAPI, since people affiliated with academic institutions (like myself) and non-profits can get even more API calls per day if they just email the company.</p>\n",
    "score": 6,
    "creation_date": 1303755887,
    "view_count": 1515,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 4379137,
    "title": "Natural language processing library for auto-tagging (.NET)",
    "body": "<p>Dose anyone know of any good libraries out there for .NET that could help pull keywords out of blocks of natural language.</p>\n\n<p>I'm basically trying to strip out stop words and ignore tenses, plurals and generally    find words that are essentially the same. </p>\n\n<p>Some abilities to find synonyms would be nice, especially if it includes things like business/technology/non-dictionary words. </p>\n",
    "score": 6,
    "creation_date": 1291740008,
    "view_count": 3849,
    "answer_count": 1,
    "tags": "c#;.net;parsing;nlp"
  },
  {
    "question_id": 2652752,
    "title": "Generating easy-to-remember random identifiers",
    "body": "<p>As all developers do, we constantly deal with some kind of identifiers as part of our daily work. Most of the time, it's about bugs or support tickets. Our software, upon detecting a bug, creates a package that has a name formatted from a timestamp and a version number, which is a cheap way of creating reasonably unique identifiers to avoid mixing packages up. Example: \"<em>Bug Report 20101214 174856 6.4b2</em>\".</p>\n\n<p>My brain just isn't that good at remembering numbers. What I would love to have is a simple way of <strong>generating alpha-numeric identifiers that are easy to remember</strong>.</p>\n\n<p>It takes about 5 minutes to whip up an algorithm like the following in python, which produces halfway usable results:</p>\n\n<pre><code>import random\n\nvowels = 'aeiuy' # 0 is confusing\nconsonants = 'bcdfghjklmnpqrstvwxz'\nnumbers = '0123456789'\n\nrandom.seed()\n\nfor i in range(30):\n    chars = list()\n    chars.append(random.choice(consonants))\n    chars.append(random.choice(vowels))\n    chars.append(random.choice(consonants + numbers))\n    chars.append(random.choice(vowels))\n    chars.append(random.choice(vowels))\n    chars.append(random.choice(consonants))\n    print ''.join(chars)\n</code></pre>\n\n<p>The results look like this:</p>\n\n<pre><code>re1ean\nmeseux\nle1ayl\nkuteef\nneluaq\ntyliyd\nki5ias\n</code></pre>\n\n<p>This is already quite good, but I feel it is still easy to forget how they are spelled exactly, so that if you walk over to a colleagues desk and want to look one of those up, there's still potential for difficulty.</p>\n\n<p>I know of algorithms that perform trigram analysis on text (say you feed them a whole book in German) and that can generate strings that look and feel like German words and are thus easier to handle generally. This requires lots of data, though, and makes it slightly less suitable for embedding in an application just for this purpose.</p>\n\n<p>Do you know of any published algorithms that solve this problem?</p>\n\n<p>Thanks!</p>\n\n<p>Carl</p>\n",
    "score": 6,
    "creation_date": 1271419713,
    "view_count": 648,
    "answer_count": 2,
    "tags": "random;nlp;mnemonics"
  },
  {
    "question_id": 1655782,
    "title": "Should I use LingPipe or NLTK for extracting names and places?",
    "body": "<p>I'm looking to extract names and places from very short bursts of text example</p>\n\n<pre>\n \"cardinals vs jays in toronto\"\n \" Daniel Nestor and Nenad Zimonjic play Jonas Bjorkman w/ Kevin Ullyett, paris time to be announced\"\n\"jenson button - pole position, brawn-mercedes - monaco\".\n</pre>\n\n<p>This data is currently in a MySQL database, and I (pretty much) have a separate record for each athlete, though names are sometimes spelled wrong, etc.</p>\n\n<p>I would like to extract the athletes and locations.\nI usually work in PHP, but haven't been able to find a library for entity extraction (and I may want to get deeper into some <a href=\"http://en.wikipedia.org/wiki/Natural_language_processing\" rel=\"nofollow noreferrer\">NLP</a> and <a href=\"http://en.wikipedia.org/wiki/Machine_learning\" rel=\"nofollow noreferrer\">ML</a> in the future). </p>\n\n<p>From what I've found, <a href=\"http://alias-i.com/lingpipe/\" rel=\"nofollow noreferrer\">LingPipe</a> and <a href=\"http://en.wikipedia.org/wiki/Natural_Language_Toolkit\" rel=\"nofollow noreferrer\">NLTK</a> seem to be the most recommended, but I can't figure out if either will really suit my purpose, or if something else would be better.  </p>\n\n<p>I haven't programmed in either Java or Python, so before I start learning new languages, I'm hoping to get some advice on what route I should follow, or other recommendations.</p>\n",
    "score": 6,
    "creation_date": 1257027736,
    "view_count": 4166,
    "answer_count": 1,
    "tags": "nlp;nltk;lingpipe"
  },
  {
    "question_id": 68742863,
    "title": "Error while trying to fine-tune the ReformerModelWithLMHead (google/reformer-enwik8) for NER",
    "body": "<p>I'm trying to fine-tune the ReformerModelWithLMHead (google/reformer-enwik8) for NER. I used the padding sequence length same as in the encode method (max_length = max([len(string) for string in list_of_strings])) along with attention_masks. And I got this error:</p>\n<p><strong>ValueError: If training, make sure that config.axial_pos_shape factors: (128, 512) multiply to sequence length. Got prod((128, 512)) != sequence_length: 2248. You might want to consider padding your sequence length to 65536 or changing config.axial_pos_shape.</strong></p>\n<ul>\n<li>When I changed the sequence length to 65536, my colab session crashed by getting all the inputs of 65536 lengths.</li>\n<li>According to the second option(changing config.axial_pos_shape), I cannot change it.</li>\n</ul>\n<p>I would like to know, Is there any chance to change config.axial_pos_shape while fine-tuning the model? Or I'm missing something in encoding the input strings for reformer-enwik8?</p>\n<p>Thanks!</p>\n<p><strong>Question Update: I have tried the following methods:</strong></p>\n<ol>\n<li>By giving paramteres at the time of model instantiation:</li>\n</ol>\n<blockquote>\n<p>model = transformers.ReformerModelWithLMHead.from_pretrained(&quot;google/reformer-enwik8&quot;, num_labels=9, max_position_embeddings=1024, axial_pos_shape=[16,64], axial_pos_embds_dim=[32,96],hidden_size=128)</p>\n</blockquote>\n<p>It gives me the following error:</p>\n<blockquote>\n<p>RuntimeError: Error(s) in loading state_dict for ReformerModelWithLMHead:\nsize mismatch for reformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([258, 1024]) from checkpoint, the shape in current model is torch.Size([258, 128]).\nsize mismatch for reformer.embeddings.position_embeddings.weights.0: copying a param with shape torch.Size([128, 1, 256]) from checkpoint, the shape in current model is torch.Size([16, 1, 32]).</p>\n</blockquote>\n<p>This is quite a long error.</p>\n<ol start=\"2\">\n<li>Then I tried this code to update the config:</li>\n</ol>\n<blockquote>\n<p>model1 = transformers.ReformerModelWithLMHead.from_pretrained('google/reformer-enwik8', num_labels = 9)</p>\n</blockquote>\n<h4>Reshape Axial Position Embeddings layer to match desired max seq length</h4>\n<pre><code>model1.reformer.embeddings.position_embeddings.weights[1] = torch.nn.Parameter(model1.reformer.embeddings.position_embeddings.weights[1][0][:128])\n</code></pre>\n<h4>Update the config file to match custom max seq length</h4>\n<pre><code>model1.config.axial_pos_shape = 16,128\nmodel1.config.max_position_embeddings = 16*128 #2048\nmodel1.config.axial_pos_embds_dim= 32,96\nmodel1.config.hidden_size = 128\noutput_model_path = &quot;model&quot;\nmodel1.save_pretrained(output_model_path)\n</code></pre>\n<p>By this implementation, I am getting this error:</p>\n<blockquote>\n<p>RuntimeError: The expanded size of the tensor (512) must match the existing size (128) at non-singleton dimension 2.  Target sizes: [1, 128, 512, 768].  Tensor sizes: [128, 768]</p>\n</blockquote>\n<p>Because updated size/shape doesn't match with the original config parameters of pretrained model. The original parameters are: axial_pos_shape = 128,512 max_position_embeddings = 128*512 #65536 axial_pos_embds_dim= 256,768 hidden_size = 1024</p>\n<p>Is it the right way I'm changing the config parameters or do I have to do something else?</p>\n<p>Is there any example where ReformerModelWithLMHead('google/reformer-enwik8') model fine-tuned.</p>\n<p>My main code implementation is as follow:</p>\n<pre><code>class REFORMER(torch.nn.Module):\ndef __init__(self):\n    super(REFORMER, self).__init__()\n    self.l1 = transformers.ReformerModelWithLMHead.from_pretrained(&quot;google/reformer-enwik8&quot;, num_labels=9)\n\ndef forward(self, input_ids, attention_masks, labels):\n    output_1= self.l1(input_ids, attention_masks, labels = labels)\n    return output_1\n\n\nmodel = REFORMER()\n\ndef train(epoch):\n    model.train()\n    for _, data in enumerate(training_loader,0):\n        ids = data['input_ids'][0]   # input_ids from encode method of the model https://huggingface.co/google/reformer-enwik8#:~:text=import%20torch%0A%0A%23%20Encoding-,def%20encode,-(list_of_strings%2C%20pad_token_id%3D0\n        input_shape = ids.size()\n        targets = data['tags']\n        print(&quot;tags: &quot;, targets, targets.size())\n        least_common_mult_chunk_length = 65536 \n        padding_length = least_common_mult_chunk_length - input_shape[-1] % least_common_mult_chunk_length\n        #pad input \n        input_ids, inputs_embeds, attention_mask, position_ids, input_shape = _pad_to_mult_of_chunk_length(self=model.l1,\n                input_ids=ids,\n                inputs_embeds=None,\n                attention_mask=None,\n                position_ids=None,\n                input_shape=input_shape,\n                padding_length=padding_length,\n                padded_seq_length=None,\n                device=None,\n            )\n        outputs = model(input_ids, attention_mask, labels=targets) # sending inputs to the forward method\n        print(outputs)\n        loss = outputs.loss\n        logits = outputs.logits\n        if _%500==0:\n           print(f'Epoch: {epoch}, Loss:  {loss}')\n\nfor epoch in range(1):\n    train(epoch)\n</code></pre>\n",
    "score": 6,
    "creation_date": 1628688044,
    "view_count": 928,
    "answer_count": 2,
    "tags": "python;nlp;pytorch;huggingface-transformers;named-entity-recognition"
  },
  {
    "question_id": 68377628,
    "title": "Using BERT to generate similar word or synonyms through word embeddings",
    "body": "<p>As we all know the capability of <code>BERT</code> model for word embedding, it is probably better than the <code>word2vec</code> and any other models.</p>\n<p>I want to create a model on <code>BERT</code> word embedding to generate synonyms or similar words. The same like we do in the <code>Gensim</code> <code>Word2Vec</code>. I want to create method of Gensim <code>model.most_similar()</code> into BERT word embedding.</p>\n<p>I researched a lot about it, seems that it is possible to do that, but the problem is it is only showing the embeddings in the form of number, there is no way to get the actual word from it. Can anybody help me regarding this?</p>\n",
    "score": 6,
    "creation_date": 1626263764,
    "view_count": 6038,
    "answer_count": 1,
    "tags": "python;nlp;gensim;word2vec;bert-language-model"
  },
  {
    "question_id": 62978957,
    "title": "Sliding window for long text in BERT for Question Answering",
    "body": "<p>I've read post which explains how the sliding window works but I cannot find any information on how it is actually implemented.</p>\n<p>From what I understand if the input are too long, sliding window can be used to process the text.</p>\n<p>Please correct me if I am wrong.\nSay I have a text <em><strong>&quot;In June 2017 Kaggle announced that it passed 1 million registered users&quot;</strong></em>.</p>\n<p>Given some <code>stride</code> and <code>max_len</code>, the input can be split into chunks with over lapping words (not considering padding).</p>\n<pre><code>In June 2017 Kaggle announced that # chunk 1\nannounced that it passed 1 million # chunk 2\n1 million registered users # chunk 3\n</code></pre>\n<p>If my questions were <em><strong>&quot;when did Kaggle make the announcement&quot;</strong></em> and <em><strong>&quot;how many registered users&quot;</strong></em> I can use <code>chunk 1</code> and <code>chunk 3</code> and <strong>not use</strong> <code>chunk 2</code> <strong>at all</strong> in the model. Not quiet sure if I should still use <code>chunk 2</code> to train the model</p>\n<p>So the input will be:\n<code>[CLS]when did Kaggle make the announcement[SEP]In June 2017 Kaggle announced that[SEP]</code>\nand\n<code>[CLS]how many registered users[SEP]1 million registered users[SEP]</code></p>\n<hr>\n<p>Then if I have a question with no answers do I feed it into the model with all chunks like and indicate the starting and ending index as <strong>-1</strong>? For example <em><strong>&quot;can pigs fly?&quot;</strong></em></p>\n<p><code>[CLS]can pigs fly[SEP]In June 2017 Kaggle announced that[SEP]</code></p>\n<p><code>[CLS]can pigs fly[SEP]announced that it passed 1 million[SEP]</code></p>\n<p><code>[CLS]can pigs fly[SEP]1 million registered users[SEP]</code></p>\n<hr>\n<p>As suggested in the comments, II tried to run <code>squad_convert_example_to_features</code> (<a href=\"https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L134\" rel=\"noreferrer\">source code</a>) to investigate the problem I have above, but it doesn't seem to work, nor there are any documentation. It seems like <code>run_squad.py</code> from huggingface uses <code>squad_convert_example_to_features</code> with the <code>s</code> in <code>example</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor, squad_convert_example_to_features\nfrom transformers import AutoTokenizer, AutoConfig, squad_convert_examples_to_features\n\nFILE_DIR = &quot;.&quot;\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nprocessor = SquadV2Processor()\nexamples = processor.get_train_examples(FILE_DIR)\n\nfeatures = squad_convert_example_to_features(\n    example=examples[0],\n    max_seq_length=384,\n    doc_stride=128,\n    max_query_length=64,\n    is_training=True,\n)\n</code></pre>\n<p>I get the error.</p>\n<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 159.95it/s]\nTraceback (most recent call last):\n  File &quot;&lt;input&gt;&quot;, line 25, in &lt;module&gt;\n    sub_tokens = tokenizer.tokenize(token)\nNameError: name 'tokenizer' is not defined\n</code></pre>\n<p>The error indicates that there are no <code>tokenizers</code> but it does not allow us to pass a <code>tokenizer</code>. Though it does work if I add a tokenizer while I am inside the function in debug mode. So how exactly do I use the <code>squad_convert_example_to_features</code> function?</p>\n",
    "score": 6,
    "creation_date": 1595153912,
    "view_count": 8541,
    "answer_count": 1,
    "tags": "nlp;text-classification;huggingface-transformers;nlp-question-answering;bert-language-model"
  },
  {
    "question_id": 62771845,
    "title": "Using BERT Embeddings in Keras Embedding layer",
    "body": "<p>I want to use the BERT Word Vector Embeddings in the Embeddings layer of LSTM instead of the usual default embedding layer. Is there any way I can do it?</p>\n",
    "score": 6,
    "creation_date": 1594113170,
    "view_count": 9276,
    "answer_count": 1,
    "tags": "python-3.x;keras;nlp;embedding;bert-language-model"
  },
  {
    "question_id": 62385002,
    "title": "Latest Pre-trained Multilingual Word Embedding",
    "body": "<p>Are there any latest <strong>pre-trained multilingual word embeddings</strong> (multiple languages are jointly mapped to a same vector space)?</p>\n\n<p>I have looked at the following but they don't fit my needs:</p>\n\n<ol>\n<li>FastText / MUSE (<a href=\"https://fasttext.cc/docs/en/aligned-vectors.html\" rel=\"noreferrer\">https://fasttext.cc/docs/en/aligned-vectors.html</a>): this one seems too old, and the word vectors are not using subwords / wordpiece information.</li>\n<li>LASER (<a href=\"https://github.com/yannvgn/laserembeddings\" rel=\"noreferrer\">https://github.com/yannvgn/laserembeddings</a>): I'm now using this one, it's using subword information (via BPE), however, it's suggested that not to use this for word embedding because it's designed to embed sentences (<a href=\"https://github.com/facebookresearch/LASER/issues/69\" rel=\"noreferrer\">https://github.com/facebookresearch/LASER/issues/69</a>).</li>\n<li>BERT multilingual (bert-base-multilingual-uncased in <a href=\"https://huggingface.co/transformers/pretrained_models.html\" rel=\"noreferrer\">https://huggingface.co/transformers/pretrained_models.html</a>): it's contextualised embeddings that can be used to embed sentences, and seems not good at embedding words without contexts.</li>\n</ol>\n\n<p>Here is the problem I'm trying to solve:</p>\n\n<p>I have a list of company names, which can be in any language (mainly English), and I have a list of keywords in English to measure how close a given company name is with regards to the keywords. Now I have a simple keyword matching solution, but I want to improve it using pretrained embeddings. As you can see in the following examples, there are several challenges:</p>\n\n<ol>\n<li>keyword and brand name is not separated by space (now I'm using package \"wordsegment\" to split words into subwords), so embedding with subword info should help a lot</li>\n<li>keyword list is not extensive and company name could be in different languages (that's why I want to use embedding, because \"soccer\" is close to \"football\")</li>\n</ol>\n\n<p>Examples of company names: \"cheapfootball ltd.\", \"wholesalefootball ltd.\", \"footballer ltd.\", \"soccershop ltd.\"</p>\n\n<p>Examples of keywords: \"football\"</p>\n",
    "score": 6,
    "creation_date": 1592212432,
    "view_count": 7903,
    "answer_count": 2,
    "tags": "nlp;word-embedding;pre-trained-model;fasttext;bert-language-model"
  },
  {
    "question_id": 59382579,
    "title": "Replace specific text with a redacted version using Python",
    "body": "<p>I am looking to do the opposite of what has been done here:</p>\n\n<pre><code>import re\n\ntext = '1234-5678-9101-1213 1415-1617-1819-hello'\n\nre.sub(r\"(\\d{4}-){3}(?=\\d{4})\", \"XXXX-XXXX-XXXX-\", text)\n\noutput = 'XXXX-XXXX-XXXX-1213 1415-1617-1819-hello'\n</code></pre>\n\n<p><em><a href=\"https://stackoverflow.com/questions/16327590/partial-replacement-with-re-sub\">Partial replacement with re.sub()</a></em></p>\n\n<p>My overall goal is to replace all <code>XXXX</code> within a text using a neural network. <code>XXXX</code> can represent names, places, numbers, dates, etc. that are in a .csv file.</p>\n\n<p>The end result would look like:</p>\n\n<pre><code>XXXX went to XXXX XXXXXX\n</code></pre>\n\n<p>Sponge Bob went to Disney World.</p>\n\n<p>In short, I am unmasking text and replacing it with a generated dataset using fuzzy.</p>\n",
    "score": 6,
    "creation_date": 1576619210,
    "view_count": 132600,
    "answer_count": 1,
    "tags": "python-3.x;nlp;lstm"
  },
  {
    "question_id": 49834096,
    "title": "How to configure input shape for bidirectional LSTM in Keras",
    "body": "<p>I'm facing the following issue. \nI have a large number of documents that I want to encode using a bidirectional LSTM. Each document has a different number of words and word can be thought of as a timestep. </p>\n\n<p>When configuring the bidirectional LSTM we are expected to provide the timeseries length. \nWhen I am training the model this value will be different for each batch.\nShould I choose a number for the <code>timeseries_size</code> which is the biggest document size I will allow? Any documents bigger than this will not be encoded? </p>\n\n<p>Example config:</p>\n\n<pre><code>Bidirectional(LSTM(128, return_sequences=True), input_shape=(timeseries_size, encoding_size))\n</code></pre>\n",
    "score": 6,
    "creation_date": 1523726062,
    "view_count": 5433,
    "answer_count": 2,
    "tags": "machine-learning;nlp;keras;lstm;recurrent-neural-network"
  },
  {
    "question_id": 43942476,
    "title": "Load Custom NER Model Stanford CoreNLP",
    "body": "<p>I have created my own NER model with Stanford's \"Stanford-NER\" software and by following <a href=\"https://nlp.stanford.edu/software/crf-faq.html#a\" rel=\"noreferrer\">these</a> directions. </p>\n\n<p>I am aware that CoreNLP loads three NER models out of the box in the following order:</p>\n\n<ol>\n<li><code>edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz</code></li>\n<li><code>edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz</code></li>\n<li><code>edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz</code></li>\n</ol>\n\n<p>I now want to include my NER model in the list above and have the text tagged by my NER model first.</p>\n\n<p>I have found two previous StackOverflow questions regarding this topic and they are <a href=\"https://stackoverflow.com/questions/41232187/stanford-openie-using-customized-ner-model\">'Stanford OpenIE using customized NER model'</a> and <a href=\"https://stackoverflow.com/questions/33905412/why-does-stanford-corenlp-ner-annotator-load-3-models-by-default?rq=1\">'Why does Stanford CoreNLP NER-annotator load 3 models by default?'</a></p>\n\n<p>Both of these posts have good answers. The general message of the answers is that you have to edit code within a file. </p>\n\n<p><strong>Stanford OpenIE using customized NER model</strong></p>\n\n<p>From this post it says to edit <code>corenlpserver.sh</code> but I cannot find this file within the Stanford CoreNLP downloaded software. Can anyone point me to this file's location? </p>\n\n<p><strong>does Stanford CoreNLP NER-annotator load 3 models by default?</strong></p>\n\n<p>This post says that I can use the argument of <code>-ner.model</code> to specifically call which NER models to load. I added this argument to the initial server command (<code>java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 -ner.model *modlefilepathhere*</code>). This did not work as the server still loaded all three models. </p>\n\n<p>It also states that you have to change some java code though it does not specifically call out where to make the change. </p>\n\n<p>Do I need to modify or add this code <code>props.put(\"ner.model\", \"model_path1,model_path2\");</code> to a specific class file in the CoreNLP software? </p>\n\n<p><strong>QUESTION:</strong> From my research it seems that I need to add/modify some code to call my unique NER model. These 'edits' are outlined above and this information has been pulled from other StackOverflow questions. What files specifically do I need to edit? Where exactly are these files located (i.e. edu/Stanford/nlp/...etc)?</p>\n\n<p><strong>EDIT:</strong> My system is running on a local server and I'm using the API pycorenlp in order to open a pipeline to my local server and to make requests against it. the two critical lines of python/pycorenlp code are:</p>\n\n<ol>\n<li><code>nlp = StanfordCoreNLP('http://localhost:9000')</code></li>\n<li><code>output = nlp.annotate(evalList[line], properties={'annotators': 'ner, openie','outputFormat': 'json', 'openie.triple.strict':'True', 'openie.max_entailments_per_clause':'1'})</code></li>\n</ol>\n\n<p>I do <em>NOT</em> think this will affect my ability to call my unique NER model but I wanted to present all the situational data I can in order to obtain the best possible answer.</p>\n",
    "score": 6,
    "creation_date": 1494606541,
    "view_count": 3826,
    "answer_count": 1,
    "tags": "java;python;python-3.x;nlp;stanford-nlp"
  },
  {
    "question_id": 40631146,
    "title": "efficient way to calculate distance between combinations of pandas frame columns",
    "body": "<p><strong>Task</strong></p>\n\n<p>I have a pandas dataframe where:</p>\n\n<ul>\n<li>the columns are document names</li>\n<li>the rows are words in those documents</li>\n<li>numbers inside the frame cells are a measure of word relevance (word count if you want to keep it simple)</li>\n</ul>\n\n<p>I need to calculate a new matrix of doc1-doc similarity where:</p>\n\n<ul>\n<li>rows and columns are document names</li>\n<li>the cells inside the frame are a similarity measure, (1 - cosine distance) between the two documents</li>\n</ul>\n\n<p>The cosine distance is conveniently provided by <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\" rel=\"nofollow noreferrer\">script.spatial.distance.cosine</a>.</p>\n\n<p>I'm currently doing this:</p>\n\n<ol>\n<li>use itertools to create a list of all 2-combinations of the document names (dataframe columns names)</li>\n<li>loop over these and create a update a dictionary of {doc1: {doc2: similarity}} </li>\n<li>after the loop, create a new frame using pandas.DataFrame(dict)</li>\n</ol>\n\n<p><strong>Problem</strong></p>\n\n<p>But it takes a very very long time. The following shows current speed on a MacBook Pro 13 with 16GB ram and 2.9GHz i5cpu running latest anaconda python 3.5 ... plotting time taken against combinations of docs.</p>\n\n<p><a href=\"https://i.sstatic.net/0EZkE.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/0EZkE.png\" alt=\"distance calculation performance\"></a></p>\n\n<p>You can see that 100,000 combinations takes 1200 seconds. Extrapolating that to my corpus of <strong>7944</strong> documents, which creates 3<strong>1,549,596</strong> combinations, would take <strong>5 days</strong> to calculate this similarity matrix!</p>\n\n<p><strong>Any ideas?</strong></p>\n\n<ul>\n<li>I <a href=\"http://makeyourowntextminingtoolkit.blogspot.co.uk/2016/11/more-performance-memory-stuff-to-fix.html\" rel=\"nofollow noreferrer\">previously</a> was dynamically creating the dataframe df.ix[doc1,doc2]\n= similarity .. which was very very much slower.</li>\n<li>I've considered numba @git but it fails with pandas data structures.</li>\n<li>I can't find a built in function which will do all the work internally (in C?)</li>\n<li>What I have to do tactically is to randomly sample the documents to create a much smaller set to work with ... currently a fraction of 0.02 leads to about 20 minutes of calculation!</li>\n</ul>\n\n<p><strong>Here's the code (<a href=\"https://github.com/makeyourowntextminingtoolkit/makeyourowntextminingtoolkit/tree/master/text_mining_toolkit\" rel=\"nofollow noreferrer\">github</a>)</strong></p>\n\n<pre><code>docs_combinations = itertools.combinations(docs_sample, 2)\nfor doc1, doc2 in docs_combinations:\n    # scipy cosine similarity function includes normalising the vectors but is a distance .. so we need to take it from 1.0\n    doc_similarity_dict[doc2].update({doc1: 1.0 - scipy.spatial.distance.cosine(relevance_index[doc1],relevance_index[doc2])})\n    pass\n\n#convert dict to pandas dataframe\ndoc_similarity_matrix = pandas.DataFrame(doc_similarity_dict)\n</code></pre>\n\n<p><strong>Simple Example</strong></p>\n\n<p>@MaxU asked for an illustrative example.</p>\n\n<p>Relevance matrix (wordcount here, just to keep it simple):</p>\n\n<pre><code>...     doc1 doc2 doc3\nwheel   2.   3.   0.\nseat    2.   2.   0.\nlights  0.   1.   1.\ncake    0.   0.   5.\n</code></pre>\n\n<p>calculated similarity matrix based on 2-combinations (doc1, doc2), (doc2, doc3), (doc1, doc3)</p>\n\n<pre><code>...     doc2 doc3\ndoc1    0.9449  0.\ndoc2    -       0.052\n</code></pre>\n\n<p>Take that top left value 0.889 .. thats the dot product (2*3 + 2*2 + 0 + 0) = 10 but normalised by the lengths of the vectors ... so divide by sqrt(8) and sqrt(14) gives 0.9449. You can see that there is no similarity between doc1 and doc3 .. the dot product is zero.</p>\n\n<p>Scale this from 3 documents with 4 words ... to <strong>7944</strong> documents, which creates 3<strong>1,549,596</strong> combinations ...</p>\n",
    "score": 6,
    "creation_date": 1479296198,
    "view_count": 2029,
    "answer_count": 2,
    "tags": "python;performance;pandas;numpy;nlp"
  },
  {
    "question_id": 37375653,
    "title": "How to get constituency-based parse tree from Parsey McParseface",
    "body": "<p>Parsey McParsey returns a dependency-based parse tree by default, but is their a way to get a constituency-based parse tree from it?</p>\n\n<p>EDIT: To clarify, by \"to get from it\" I mean from the Parsey itself. Though building a tree from ConLL output would be an option too.</p>\n",
    "score": 6,
    "creation_date": 1463927581,
    "view_count": 766,
    "answer_count": 1,
    "tags": "nlp;syntaxnet;parsey-mcparseface"
  },
  {
    "question_id": 36535206,
    "title": "Name Entity Resolution Algorithm",
    "body": "<p>I was trying to build an entity resolution system, where my entities are,</p>\n\n<pre><code>(i) General named entities, that is organization, person, location,date, time, money, and percent.\n(ii) Some other entities like, product, title of person like president,ceo, etc. \n(iii) Corefererred entities like, pronoun, determiner phrase,synonym, string match, demonstrative noun phrase, alias, apposition. \n</code></pre>\n\n<p>From various literature and other references, I have defined its scope as I would not consider the ambiguity of each of the entity beyond its entity category. That is, I am taking Oxford of Oxford University\nas different from Oxford as place, as the previous one is the first word of an organization entity and second one is the entity of location. </p>\n\n<p>My task is to construct one resolution algorithm, where I would extract \nand resolve the entities. </p>\n\n<p>So, I am working out an entity extractor in the first place. \nIn the second place, if I try to relate the coreferences as I found from \nvarious literatures like this <a href=\"http://anthology.aclweb.org/J/J01/J01-4004.pdf\" rel=\"noreferrer\">seminal work</a>, they are trying to work out \na decision tree based algorithm, with some features like, distance,\ni-pronoun, j-pronoun, string match, definite noun\nphrase, demonstrative noun phrase, number agreement feature,\nsemantic class agreement, gender agreement, both proper names, alias, apposition\netc. </p>\n\n<p>The algorithm seems a nice one where enities are extracted with Hidden Markov Model(HMM).</p>\n\n<p>I could work out one entity recognition system with HMM.\nNow I am trying to work out a coreference as well as an entity\nresolution system. I was trying to feel instead of using so many\nfeatures if I use an annotated corpus and train it directly with \nHMM based tagger, with a view to solve a relationship extraction like,</p>\n\n<pre><code>*\"Obama/PERS is/NA delivering/NA a/NA lecture/NA in/NA Washington/LOC, he/PPERS knew/NA it/NA was/NA going/NA to/NA be/NA\nsmall/NA as/NA it/NA may/NA not/NA be/NA his/PoPERS speech/NA as/NA Mr. President/APPERS\"\n\nwhere, PERS-&gt; PERSON\n       PPERS-&gt;PERSONAL PRONOUN TO PERSON\n       PoPERS-&gt; POSSESSIVE PRONOUN TO PERSON\n       APPERS-&gt; APPOSITIVE TO PERSON\n       LOC-&gt; LOCATION\n       NA-&gt; NOT AVAILABLE*\n</code></pre>\n\n<p>would I be wrong? I made an experiment with around 10,000 words. Early results seem\nencouraging. With a support from one of my colleague I am trying to insert some\nsemantic information like,\nPERSUSPOL, LOCCITUS, PoPERSM, etc. for PERSON OF US IN POLITICS, LOCATION CITY US, POSSESSIVE PERSON MALE, in the tagset to incorporate entity disambiguation at one go. My feeling relationship extraction would be much better now. \nPlease see this new thought too. \nI got some good results with Naive Bayes classifier also where sentences\nhaving predominately one set of keywords are marked as one class. </p>\n\n<p>If any one may suggest any different approach, please feel free to suggest so.</p>\n\n<p>I use Python2.x on MS-Windows and try to use libraries like NLTK, Scikit-learn, Gensim,\npandas, Numpy, Scipy etc. </p>\n\n<p>Thanks in Advance.  </p>\n",
    "score": 6,
    "creation_date": 1460320212,
    "view_count": 4072,
    "answer_count": 1,
    "tags": "python;algorithm;machine-learning;nlp"
  },
  {
    "question_id": 35653631,
    "title": "Using different word2vec training data in spaCy",
    "body": "<p>So I'd like to use some of <a href=\"https://code.google.com/archive/p/word2vec/\" rel=\"noreferrer\">this training data</a> in spaCy when I use the <code>similarity()</code> method.</p>\n\n<p>I'd also like to maybe use the pre-trained vectors also on this page.</p>\n\n<p>But the spaCy docs seem lacking here, does anyone know how to do this?</p>\n",
    "score": 6,
    "creation_date": 1456495126,
    "view_count": 2114,
    "answer_count": 1,
    "tags": "python;nlp;word2vec;spacy"
  },
  {
    "question_id": 30145850,
    "title": "Only ignore stop words for ngram_range=1",
    "body": "<p>I am using CountVectorizer from sklearn...looking to provide a list of stop words and apply the count vectorizer for ngram_range of (1,3). </p>\n\n<p>From what I can tell, if a word - say \"me\" - is in the list of stop words, then it doesn't get seen for higher ngrams i.e., \"tell me\" would not be a feature. Is there a way that I can specify something like, \"consider stop words only when ngram is 1\"?</p>\n",
    "score": 6,
    "creation_date": 1431211801,
    "view_count": 1107,
    "answer_count": 1,
    "tags": "python;nlp;scikit-learn"
  },
  {
    "question_id": 27052267,
    "title": "In natural language processing (NLP), how do you make an efficient dimension reduction?",
    "body": "<p>In NLP, it's always the case that the dimension of the features are very huge. For example, for one project at hand, the dimension of features  is almost 20 thousands (p = 20,000), and each feature is a 0-1 integer to show whether a specific word or bi-gram is presented in a paper (one  paper is a data point $x \\in R^{p}$).</p>\n\n<p>I know the redundancy among the features is huge, so dimension reduction is necessary. I have three questions:</p>\n\n<p>1) I have 10 thousands data points (n = 10,000), and each data points has 10 thousands features (p = 10,000). What is the effieient  way to conduct dimension reduction? The matrix $X \\in R^{n \\times p}$ is so huge that both PCA (or SVD, truncated SVD is OK, but I don't think SVD is a good way to reduce dimention for binary features) and Bag of Words (or K-means) is hard be be directly conducted on $X$ (Sure, it is sparse). I don't have a server, I just use my PC:-(.</p>\n\n<p>2) How to judge the similarity or distance among two data points? I think the Euclidean distance may not work well for binary features. How about L0 norm? What do you use?</p>\n\n<p>3) If I want to use SVM machine (or other kernel methods) to conduct classification, which kernel should I use?</p>\n\n<p>Many Thanks!</p>\n",
    "score": 6,
    "creation_date": 1416530129,
    "view_count": 1737,
    "answer_count": 1,
    "tags": "text;machine-learning;nlp;dimensionality-reduction;dimension-reduction"
  },
  {
    "question_id": 25741209,
    "title": "Diminutive words stemming / lemmatization",
    "body": "<p>Currently I use 'lucene' and 'elasticsearch', and have next problem.\nI need get stemmed form or lemma for <a href=\"http://en.wikipedia.org/wiki/Diminutive\">diminutive</a> word. For instance :</p>\n\n<ul>\n<li><em>doggy -> dog</em> </li>\n<li><em>kitty -> cat</em></li>\n</ul>\n\n<p>etc.</p>\n\n<p>But I get next results :</p>\n\n<ul>\n<li><em>doggy -> doggi</em> </li>\n<li><em>kitty -> kitti</em></li>\n</ul>\n\n<p>Is there any way (not important ready to use library, any algorithm, approach etc.) to get root / original word form for <a href=\"http://en.wikipedia.org/wiki/Diminutive\">diminutive</a> word forms?</p>\n\n<p>Target language : Russian.\nFor example :</p>\n\n<ul>\n<li><em>собачка -> собака</em></li>\n<li><em>кошечка -> кошка</em></li>\n</ul>\n\n<p>Thanks in advance!</p>\n",
    "score": 6,
    "creation_date": 1410255184,
    "view_count": 632,
    "answer_count": 1,
    "tags": "java;lucene;elasticsearch;nlp;morphological-analysis"
  },
  {
    "question_id": 25256195,
    "title": "Python NLP: TypeError: not all arguments converted during string formatting",
    "body": "<p>I tried the code on \"Natural language processing with python\", but a type error occurred.</p>\n\n<pre><code>import nltk\nfrom nltk.corpus import brown\n\nsuffix_fdist = nltk.FreqDist()\nfor word in brown.words():\n    word = word.lower()\n    suffix_fdist.inc(word[-1:])\n    suffix_fdist.inc(word[-2:])\n    suffix_fdist.inc(word[-3:])\ncommon_suffixes = suffix_fdist.items()[:100]\n\ndef pos_features(word):\n    features = {}\n    for suffix in common_suffixes:\n        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n    return features\npos_features('people')\n</code></pre>\n\n<p>the error is below:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"/home/wanglan/javadevelop/TestPython/src/FirstModule.py\", line 323, in &lt;module&gt;\n    pos_features('people')\n  File \"/home/wanglan/javadevelop/TestPython/src/FirstModule.py\", line 321, in pos_features\n    features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\nTypeError: not all arguments converted during string formatting\n</code></pre>\n\n<p>Does anyone could help me find out where i am wrong?</p>\n",
    "score": 6,
    "creation_date": 1407817847,
    "view_count": 6857,
    "answer_count": 1,
    "tags": "python;nlp;typeerror"
  },
  {
    "question_id": 23000696,
    "title": "How do I get the Synset offset in Wordnet for use in Imagenet",
    "body": "<p>I plan to use <a href=\"http://image-net.org/index\" rel=\"noreferrer\">Image-Net</a> to build a list of synonyms for a language task.  According to the <a href=\"http://image-net.org/download-API\" rel=\"noreferrer\">Image-Net API Docs</a>, </p>\n\n<blockquote>\n  <p>ImageNet is based upon WordNet 3.0. To uniquely identify a synset, we use \"WordNet ID\" (wnid), which is a concatenation of POS ( i.e. part of speech ) and SYNSET OFFSET of WordNet.</p>\n</blockquote>\n\n<p>This all seems well and good, however there is not a single bit of documentation on how to get the SYNSET OFFSET for a synset in WordNet.  This <a href=\"http://shiffman.net/teaching/a2z/wordnet/\" rel=\"noreferrer\">RiTaWN tutorial</a> explains how to get the Sense ID, however these are not the same values.</p>\n\n<p>How can I get the SYNSET OFFSET so I can begin to use the Image-Net API to build my list of picturable nouns and synonyms?</p>\n",
    "score": 6,
    "creation_date": 1397171402,
    "view_count": 2679,
    "answer_count": 1,
    "tags": "java;nlp;wordnet"
  },
  {
    "question_id": 20281062,
    "title": "Multi-label classification for large dataset",
    "body": "<p>I am solving a multilabel classification problem. I have about 6 Million of rows to be processed which are huge chunks of text. They are tagged with multiple tags in a separate column. </p>\n\n<p>Any advice on what scikit libraries can help me scale up my code. I am using One-vs-Rest and SVM within it. But they don't scale beyond 90-100k rows.</p>\n\n<pre><code>classifier = Pipeline([\n('vectorizer', CountVectorizer(min_df=1)), \n('tfidf', TfidfTransformer()),\n('clf', OneVsRestClassifier(LinearSVC()))])\n</code></pre>\n",
    "score": 6,
    "creation_date": 1385712761,
    "view_count": 2498,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;classification;scikit-learn"
  },
  {
    "question_id": 10398294,
    "title": "Negating sentences using POS-tagging",
    "body": "<p>I'm trying to find a way to negate sentences based on POS-tagging. Please consider:</p>\n\n<pre><code>include_once 'class.postagger.php';\n\nfunction negate($sentence) {  \n  $tagger = new PosTagger('includes/lexicon.txt');\n  $tags = $tagger-&gt;tag($sentence);\n  foreach ($tags as $t) {\n    $input[] = trim($t['token']) . \"/\" . trim($t['tag']) .  \" \";\n  }\n  $sentence = implode(\" \", $input);\n  $postagged = $sentence;\n\n  // Concatenate \"not\" to every JJ, RB or VB\n  // Todo: ignore negative words (not, never, neither)\n  $sentence = preg_replace(\"/(\\w+)\\/(JJ|MD|RB|VB|VBD|VBN)\\b/\", \"not$1/$2\", $sentence);\n\n  // Remove all POS tags\n  $sentence = preg_replace(\"/\\/[A-Z$]+/\", \"\", $sentence);\n\n  return \"$postagged&lt;br&gt;$sentence\";\n}\n</code></pre>\n\n<p>BTW: In this example, I'm using the <a href=\"http://phpir.com/part-of-speech-tagging\" rel=\"noreferrer\">POS-tagging implementation</a> and <a href=\"http://phpir.com/user/files/text/lexicon.txt\" rel=\"noreferrer\">lexicon</a> of Ian Barber. An example of this code running would be:</p>\n\n<pre><code>echo negate(\"I will never go to their place again\");\nI/NN will/MD never/RB go/VB to/TO their/PRP$ place/NN again/RB \nI notwill notnever notgo to their place notagain\n</code></pre>\n\n<p>As you can see, (and this issue is also commented in the code), negating words themselves are being negated as wel: <code>never</code> becomes <code>notnever</code>, which obviously shouldn't happen. Since my regex skills aren't all that, is there a way to exclude these words from the regex used?</p>\n\n<p>[edit] Also, I would very much welcome other comments / critiques you might have in this negating implementation, since I'm sure it's (still) quite flawed :-)</p>\n",
    "score": 6,
    "creation_date": 1335878647,
    "view_count": 851,
    "answer_count": 1,
    "tags": "php;regex;nlp"
  },
  {
    "question_id": 10224818,
    "title": "Wordnet edit tree structure",
    "body": "<p>I'm developing an application that uses the Wordnet conceptual hierarchy for its operation. I found that some words I need are missing in the database. Is there an API or tool, or any other way I can insert new words, edit the structure etc.? (I'm using Wordnet 3.0.)</p>\n\n<p>Thanks.</p>\n",
    "score": 6,
    "creation_date": 1334826062,
    "view_count": 955,
    "answer_count": 1,
    "tags": "nlp;wordnet"
  },
  {
    "question_id": 5025426,
    "title": "Heuristic Approaches to Finding Main Content",
    "body": "<p>Wondering if anybody could point me in the direction of academic papers or related implementations of heuristic approaches to finding the real meat content of a particular webpage.</p>\n\n<p>Obviously this is not a trivial task, since the problem description is so vague, but I think that we all have a general understanding about what is meant by the primary content of a page.  </p>\n\n<p>For example, it may include the story text for a news article, but might not include any navigational elements, legal disclaimers, related story teasers, comments, etc.  Article titles, dates, author names, and other metadata fall in the grey category.</p>\n\n<p>I imagine that the application value of such an approach is large, and would expect Google to be using it in some way in their search algorithm, so it would appear to me that this subject has been treated by academics in the past.</p>\n\n<p>Any references?</p>\n",
    "score": 6,
    "creation_date": 1297920680,
    "view_count": 1001,
    "answer_count": 1,
    "tags": "parsing;nlp;web-crawler"
  },
  {
    "question_id": 4749678,
    "title": "Server-side software for translating languages?",
    "body": "<p>I am searching for a server-side application (not a service, we need to host this ourselves) that can take a given string and translate it to another language. Open-source, paid, doesn't matter.</p>\n\n<p>Can anyone provide some recommendations?</p>\n",
    "score": 6,
    "creation_date": 1295540647,
    "view_count": 1424,
    "answer_count": 1,
    "tags": "translation;nlp;server-side;machine-translation"
  },
  {
    "question_id": 4250491,
    "title": "Natural Language Processing - Truecaser classifier",
    "body": "<p>Please suggest a good machine learning classifier for truecasing of dataset.\nAlso, Is it possible to specify out own rules/features for truecasing in such a classifier? Thanks for all your suggestions.</p>\n\n<p>Thanks</p>\n",
    "score": 6,
    "creation_date": 1290462088,
    "view_count": 1356,
    "answer_count": 2,
    "tags": "nlp;classification"
  },
  {
    "question_id": 3677902,
    "title": "Regular expression for counting sentences in a block of text",
    "body": "<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"https://stackoverflow.com/questions/2158296/php-how-to-split-a-paragraph-into-sentences\">PHP - How to split a paragraph into sentences.</a>  </p>\n</blockquote>\n\n\n\n<p>I have a block of text that I would like to separate into sentences, what would be the best way of doing this? I thought of looking for '.','!','?' characters, but I realized there were some problems with this, such as when people use acronyms, or end a sentence with something like !?.  What would be the best way to handle this? I figured there would be some regex that could handle this, but I'm open to a non-regex solution if that fits the problem better.</p>\n",
    "score": 6,
    "creation_date": 1284045098,
    "view_count": 688,
    "answer_count": 3,
    "tags": "php;regex;nlp"
  },
  {
    "question_id": 3445358,
    "title": "Extract inconsistently formatted date from string (date parsing, NLP)",
    "body": "<p>I have a large list of files, some of which have dates embedded in the filename.  The format of the dates is inconsistent and often incomplete, e.g. \"Aug06\", \"Aug2006\", \"August 2006\", \"08-06\", \"01-08-06\", \"2006\", \"011004\" etc.  In addition to that, some filenames have unrelated numbers that look somewhat like dates, e.g. \"20202010\".</p>\n\n<p>In short, the dates are normally incomplete, sometimes not there, are inconsistently formatted and are embedded in a string with other information, e.g. \"Report Aug06.xls\".</p>\n\n<p>Are there any Perl modules available which will do a decent job of guessing the date from such a string?  It doesn't have to be 100% correct, as it will be verified by a human manually, but I'm trying to make things as easy as possible for that person and there are thousands of entries to check :)</p>\n",
    "score": 6,
    "creation_date": 1281402965,
    "view_count": 1890,
    "answer_count": 3,
    "tags": "perl;date;nlp"
  },
  {
    "question_id": 2867895,
    "title": "Natural language grammar and user-entered names",
    "body": "<p>Some languages, particularly Slavic languages, change the endings of people's names according to the grammatical context. (For those of you who know grammar or studied languages that do this to words, such as German or Russian, and to help with search keywords, I'm talking about noun declension.)</p>\n\n<p>This is probably easiest with a set of examples (in Polish, to save the whole different-alphabet problem):</p>\n\n<ol>\n<li>Dorothy saw the cat — <em>Dorota zobaczyła kota</em></li>\n<li>The cat saw Dorothy — <em>Kot zobaczył Dorotę</em></li>\n<li>It is Dorothy’s cat — <em>To jest kot Doroty</em></li>\n<li>I gave the cat to Dorothy — <em>Dałam kota Dorotie</em></li>\n<li>I went for a walk with Dorothy — <em>Poszłam na spacer z Dorotą</em></li>\n<li>“Hello, Dorothy!” — <em>“Witam, Doroto!”</em></li>\n</ol>\n\n<p>Now, if, in these examples, the name here were to be user-entered, that introduces a world of grammar nightmares. Importantly, if I went for Katie (<em>Kasia</em>), the <a href=\"http://en.wikibooks.org/wiki/Polish/Feminine_noun_declension\" rel=\"nofollow noreferrer\">examples are not directly comparable</a> — 3 and 4 are both <em>Kasi</em>, rather than <em>*Kasy</em> and <em>*Kasie</em> — and male names will be <a href=\"http://en.wikibooks.org/wiki/Polish/Masculine_noun_declension\" rel=\"nofollow noreferrer\">wholly different again</a>.</p>\n\n<p>I'm guessing someone has dealt with this situation before, but my Google-fu appears to be weak today. I can find a lot of links about natural-language processing, but I don'think that's quite what I want. To be clear: I'm only ever gonna have one user-entered name per user and I'm gonna need to decline them into known configurations — I'll have a localised text that will have placeholders something like <code>{name nominative}</code> and <code>{name dative}</code>, for the sake of argument. I really don't want to have to do lexical analysis of text to work stuff out, I'll only ever need to decline that one user-entered name.</p>\n\n<p>Anyone have any recommendations on how to do this, or do I need to start calling round localisation agencies  ;o)</p>\n\n<hr/>\n\n<p>Further reading (all on Wikipedia) for the interested:</p>\n\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Declension\" rel=\"nofollow noreferrer\">Declension</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Grammatical_case\" rel=\"nofollow noreferrer\">Grammatical case</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Polish_language#Nouns_and_adjectives\" rel=\"nofollow noreferrer\">Declension in Polish</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Russian_grammar#Nouns\" rel=\"nofollow noreferrer\">Declension in Russian</a></li>\n<li>Declension in Czech <a href=\"http://en.wikipedia.org/wiki/Czech_declension#Nouns\" rel=\"nofollow noreferrer\">nouns</a> and <a href=\"http://en.wikipedia.org/wiki/Czech_declension#Pronouns\" rel=\"nofollow noreferrer\">pronouns</a></li>\n</ul>\n\n<p>Disclaimer: I know this happens in many other languages; highlighting Slavic languages is merely because I have a project that is going to be localised into some Slavic languages.</p>\n",
    "score": 6,
    "creation_date": 1274290054,
    "view_count": 625,
    "answer_count": 2,
    "tags": "internationalization;nlp;grammar;linguistics"
  },
  {
    "question_id": 2345105,
    "title": "Text mining, fact extraction, semantic analysis using .Net",
    "body": "<p>I'm looking for any free tools/components/libraries that allow me to take anvantage of text mining, fact extraction and semantic analysis in my .NET application. </p>\n\n<p>The <a href=\"http://gate.ac.uk\" rel=\"noreferrer\">GATE</a> project is what I need but it is written in Java. Is there something like GATE in the .NET world?</p>\n\n<p>My challange is to extract certain facts out of website text content. I plan to use some NLP algorithms to achieve such functionality, but I'm not sure how do I implement them, so I'm gonna use any existing solutions if they were available.</p>\n\n<p>I'll appreciate if you could give me some tips. I'm new in this area, so any related info would be very usefull for me.</p>\n",
    "score": 6,
    "creation_date": 1267221325,
    "view_count": 2899,
    "answer_count": 1,
    "tags": ".net;nlp;text-mining;semantic-analysis"
  },
  {
    "question_id": 76227244,
    "title": "&quot;The model &#39;MPTForCausalLM&#39; is not supported for text-generation&quot;- The following warning is coming when trying to use MPT-7B instruct",
    "body": "<p>I am using a VM of GCP(e2-highmem-4 (Efficient Instance, 4 vCPUs, 32 GB RAM)) to load the model and use it. Here is the code I have written-</p>\n<pre><code>import torch\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport transformers\nconfig = transformers.AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n  trust_remote_code=True,\n)\n# config.attn_config['attn_impl'] = 'flash'\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n  config=config,\n  torch_dtype=torch.bfloat16,\n  trust_remote_code=True,\n  cache_dir=&quot;./cache&quot;\n)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-neox-20b&quot;, cache_dir=&quot;./cache&quot;)\ntext_gen = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer)\ntext_gen(text_inputs=&quot;what is 2+2?&quot;)\n</code></pre>\n<p>Now the code is taking way too long to generate the text. Am I doing something wrong? or is there any way to make things faster?\nAlso, when creating the pipeline, I am getting the following warning-\\</p>\n<p><code>The model 'MPTForCausalLM' is not supported for text-generation</code></p>\n<p>I tried generating text by using it but it was stuck for a long time.</p>\n",
    "score": 6,
    "creation_date": 1683805184,
    "view_count": 6306,
    "answer_count": 1,
    "tags": "python-3.x;deep-learning;nlp"
  },
  {
    "question_id": 70501324,
    "title": "How to predict the probability of an empty string using BERT",
    "body": "<p>Suppose we have a template sentence like this:</p>\n<ul>\n<li>&quot;The ____ house is our meeting place.&quot;</li>\n</ul>\n<p>and we have a list of adjectives to fill in the blank, e.g.:</p>\n<ul>\n<li>&quot;yellow&quot;</li>\n<li>&quot;large&quot;</li>\n<li>&quot;&quot;</li>\n</ul>\n<p>Note that one of these is an empty string.</p>\n<p>The goal is to compare the probabilities to select the most likely word to describe &quot;house&quot; given the context of the sentence. If it's more likely to have <i>nothing</i>, this should also be taken into consideration.</p>\n<p>We can predict the probability of each word filling in the blank, but how would we predict that the likelihood of there being no adjective to describe &quot;house&quot;?</p>\n<p>To predict the probability of a word:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import BertTokenizer, BertForMaskedLM\nimport torch\nfrom torch.nn import functional as F\n\n# Load BERT tokenizer and pre-trained model\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\nmodel = BertForMaskedLM.from_pretrained('bert-large-uncased', return_dict=True)\n\ntargets = [&quot;yellow&quot;, &quot;large&quot;]\nsentence = &quot;The [MASK] house is our meeting place.&quot;\n\n# Using BERT, compute probability over its entire vocabulary, returning logits\ninput = tokenizer.encode_plus(sentence, return_tensors = &quot;pt&quot;) \nmask_index = torch.where(input[&quot;input_ids&quot;][0] == tokenizer.mask_token_id)[0] \nwith torch.no_grad():\n    output = model(**input) \n\n# Run softmax over the logits to get the probabilities\nsoftmax = F.softmax(output.logits[0], dim=-1)\n\n# Find the words' probabilities in this probability distribution\ntarget_probabilities = {t: softmax[mask_index, tokenizer.vocab[t]].numpy()[0] for t in targets}\ntarget_probabilities\n</code></pre>\n<p>This outputs a list of the words and their associated probabilities:</p>\n<pre><code>{'yellow': 0.0061520976, 'large': 0.00071377633}\n</code></pre>\n<p>If I try to add an empty string to the list, I get the following error:</p>\n<pre><code>---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-62-6f726220a108&gt; in &lt;module&gt;\n     18 \n     19 # Find the words' probabilities in this probability distribution\n---&gt; 20 target_probabilities = {t: softmax[mask_index, tokenizer.vocab[t]].numpy()[0] for t in targets}\n     21 target_probabilities\n\n&lt;ipython-input-62-6f726220a108&gt; in &lt;dictcomp&gt;(.0)\n     18 \n     19 # Find the words' probabilities in this probability distribution\n---&gt; 20 target_probabilities = {t: softmax[mask_index, tokenizer.vocab[t]].numpy()[0] for t in targets}\n     21 target_probabilities\n\nKeyError: ''\n</code></pre>\n<p>This is because BERT's vocabulary contains no empty string, so we can't look up the probability of something that doesn't exist in the model.</p>\n<p>How should we get the probability of there being no word to fill in the blank? Is this possible with the model? Does it make sense to use the empty token <code>[PAD]</code> instead of an empty string? (I've only seen <code>[PAD]</code> used at the end of sentences, to make a group of sentences the same length.)</p>\n",
    "score": 6,
    "creation_date": 1640646776,
    "view_count": 1120,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 66360602,
    "title": "spaCy Tokenizer LEMMA and ORTH Exceptions Not Working",
    "body": "<p>I'm following an example from Chapter #2 in the book: Natural Language Processing with Python and spaCy by Yuli Vasiliev 2020</p>\n<p><a href=\"https://i.sstatic.net/sfrZ7.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/sfrZ7.png\" alt=\"enter image description here\" /></a></p>\n<p><strong>The example is suppose to produce the lemmatization output:</strong></p>\n<p>['I', 'am' , 'flying' , 'to', 'Frisco']</p>\n<p>['-PRON-',  'be'  , 'fly' , 'to',  'San Francisco']</p>\n<p>I get the following error:</p>\n<pre><code>nlp.tokenizer.add_special_case(u'Frisco', sf_special_case)\n  File &quot;spacy\\tokenizer.pyx&quot;, line 601, in spacy.tokenizer.Tokenizer.add_special_case\n  File &quot;spacy\\tokenizer.pyx&quot;, line 589, in spacy.tokenizer.Tokenizer._validate_special_case\nValueError: [E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Frisco'. Tokenizer exceptions are only allowed to specify ORTH and NORM.\n</code></pre>\n<p>Could someone please advise for a workaround? I'm not sure if SpaCy version  3.0.3 was changed to no longer allow LEMMA to be part of tokenizer exception? Thanks!</p>\n",
    "score": 6,
    "creation_date": 1614211365,
    "view_count": 2432,
    "answer_count": 2,
    "tags": "nlp;spacy"
  },
  {
    "question_id": 56275693,
    "title": "Don&#39;t understand the HashingVectorizer from sklearn",
    "body": "<p><strong>I'm using HashingVectorizer function from sklearn.feature_extraction.text  but I do not understand how it works.</strong></p>\n\n<p>My code</p>\n\n<pre><code>from sklearn.feature_extraction.text import HashingVectorizer\ncorpus = [ 'This is the first document.',\n'This document is the second document.',\n'And this is the third one.',\n'Is this the first document?']\nvectorizer = HashingVectorizer(n_features=2**3)\nX = vectorizer.fit_transform(corpus)\nprint(X)\n</code></pre>\n\n<p>My result</p>\n\n<pre><code>(0, 0)        -0.8944271909999159\n(0, 5)        0.4472135954999579\n(0, 6)        0.0\n(1, 0)        -0.8164965809277261\n(1, 3)        0.4082482904638631\n(1, 5)        0.4082482904638631\n(1, 6)        0.0\n(2, 4)        -0.7071067811865475\n(2, 5)        0.7071067811865475\n(2, 6)        0.0\n(3, 0)        -0.8944271909999159\n(3, 5)        0.4472135954999579\n(3, 6)        0.0\n</code></pre>\n\n<p>I read a lot of paper on the Hashing Trick, like this article <a href=\"https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f\" rel=\"noreferrer\">https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f</a> </p>\n\n<p>I understand this article but do not see the relationship with the result obtained above.</p>\n\n<p><strong>Can you explain me with simple example how work HashingVectorizer please</strong></p>\n",
    "score": 6,
    "creation_date": 1558615998,
    "view_count": 3233,
    "answer_count": 2,
    "tags": "python-3.x;scikit-learn;nlp;vectorization;text-classification"
  },
  {
    "question_id": 51634328,
    "title": "WordNetLemmatizer: Different handling of wn.ADJ and wn.ADJ_SAT?",
    "body": "<p>I need to lemmatize text using nltk. In order to do this, I apply <code>nltk.pos_tag</code> to each sentence and then convert the resulting Penn Treebank tags (<a href=\"http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\" rel=\"noreferrer\">http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>) to WordNet tags. I need to do this because <code>WordNetLemmatizer.lemmatize()</code> expects both the word and its correct pos_tag as arguments, otherwise it will just assume everything is a verb.</p>\n\n<p>I just found that there are five different tags defined in WordNet: </p>\n\n<ul>\n<li>wn.VERB</li>\n<li>wn.ADV</li>\n<li>wn.NOUN</li>\n<li>wn.ADJ</li>\n<li>wn.ADJ_SAT</li>\n</ul>\n\n<p>However, <strong>every example I found on the internet just ignores wn.ADJ_SAT</strong> when converting Treebank tags to WordNet tags. They are all just mapping Penn tags to WordNet tags like this:</p>\n\n<ul>\n<li>If Penn tag starts with J: convert to wn.ADJ</li>\n<li>If Penn tag starts with V: convert to wn.VERB</li>\n<li>If Penn tag starts with N: convert to wn.NOUN</li>\n<li>If Penn tag starts with R: convert to wn.ADV</li>\n</ul>\n\n<p>So wn.ADJ_SAT is never used.</p>\n\n<p><strong>My question</strong> now is if there are cases where the lemmatizer returns a different result for ADJ_SAT than for ADJ. What are examples for words that are satellite adjectives (ADJ_SAT) and no normal adjectives (ADJ)?</p>\n",
    "score": 6,
    "creation_date": 1533129420,
    "view_count": 2687,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet;lemmatization"
  },
  {
    "question_id": 49779853,
    "title": "Extract only body text from arXiv articles formatted as .tex",
    "body": "<p>My dataset is composed of arXiv astrophysics articles as .tex files, and I need to extract only text from the article body, not from any other part of the article (e.g. tables, figures, abstract, title, footnotes, acknowledgements, citations, etc.). </p>\n\n<p>I've been trying with Python3 and <a href=\"https://drive.google.com/file/d/1tv44UbdA2_pAQqI6iL8-h46kXabkTZKR/view?usp=sharing\" rel=\"noreferrer\">tex2py</a>, but I'm struggling with getting a clean corpus, because the files differ in labeling &amp; the text is broken up between labels. </p>\n\n<p>I have attached a SSCCE, a couple sample Latex files and their pdfs, and the parsed corpus. The corpus shows my struggles: Sections and subsections are not extracted in order, text breaks at some labels, and some tables and figures are included.</p>\n\n<p>Code: </p>\n\n<pre><code>import os\nfrom tex2py import tex2py\n\ncorpus = open('corpus2.tex', 'a')\n\ndef parseFiles():\n    \"\"\"\n    Parses downloaded document .tex files for word content.\n    We are only interested in the article body, defined by /section tags.\n    \"\"\"\n\n    for file in os.listdir(\"latex\"):\n        if file.endswith('.tex'):\n            print('\\nChecking ' + file + '...')\n            with open(\"latex/\" + file) as f:\n                try:\n                    toc = tex2py(f) # toc = tree of contents\n                    # If file is a document, defined as having \\begin{document}\n                    if toc.source.document:\n                        # Iterate over each section in document\n                        for section in toc:\n                            # Parse the section\n                            getText(section)\n                    else:\n                        print(file + ' is not a document. Discarded.')\n                except (EOFError, TypeError, UnicodeDecodeError): \n                    print('Error: ' + file + ' was not correctly formatted. Discarded.')\n\n\n\ndef getText(section):\n    \"\"\"\n    Extracts text from given \"section\" node and any nested \"subsection\" nodes. \n\n    Parameters\n    ----------\n    section : list\n        A \"section\" node in a .tex document \n    \"\"\"\n\n    # For each element within the section \n    for x in section:\n        if hasattr(x.source, 'name'):\n            # If it is a subsection or subsubsection, parse it\n            if x.source.name == 'subsection' or x.source.name == 'subsubsection':\n                corpus.write('\\nSUBSECTION!!!!!!!!!!!!!\\n')\n                getText(x)\n            # Avoid parsing past these sections\n            elif x.source.name == 'acknowledgements' or x.source.name == 'appendix':\n                return\n        # If element is text, add it to corpus\n        elif isinstance(x.source, str):\n            # If element is inline math, worry about it later\n            if x.source.startswith('$') and x.source.endswith('$'):\n                continue\n            corpus.write(str(x))\n        # If element is 'RArg' labelled, e.g. \\em for italic, add it to corpus\n        elif type(x.source).__name__ is 'RArg':\n            corpus.write(str(x.source))\n\n\nif __name__ == '__main__':\n    \"\"\"Runs if script called on command line\"\"\"\n    parseFiles()\n</code></pre>\n\n<p>Links to the rest:</p>\n\n<ul>\n<li><a href=\"https://drive.google.com/file/d/1tv44UbdA2_pAQqI6iL8-h46kXabkTZKR/view?usp=sharing\" rel=\"noreferrer\">Sample .tex file 1</a> and its <a href=\"https://drive.google.com/file/d/13XjQ-NbENqrbkgdAL3QyhWC-BbNmWAdl/view?usp=sharing\" rel=\"noreferrer\">pdf</a></li>\n<li><a href=\"https://drive.google.com/file/d/1iCAIuLQtw7wj8J2BPfvvhywWyq4Sey6s/view?usp=sharing\" rel=\"noreferrer\">Sample .tex file 2</a> and its <a href=\"https://drive.google.com/file/d/1B1e_Ptet1tjMRiZofGanQgNH4Fc9uinL/view?usp=sharing\" rel=\"noreferrer\">pdf</a></li>\n<li>Resulting <a href=\"https://drive.google.com/file/d/1KN5yEGiEh494DS-FQPtPI_oBAz0FiOQz/view?usp=sharing\" rel=\"noreferrer\">corpus</a></li>\n</ul>\n\n<p>I'm aware of a related question (<a href=\"https://stackoverflow.com/questions/4792065/programmatically-converting-parsing-latex-code-to-plain-text?rq=1\">Programatically converting/parsing latex code to plain text</a>), but there seems not to be a conclusive answer. </p>\n",
    "score": 6,
    "creation_date": 1523462909,
    "view_count": 1569,
    "answer_count": 1,
    "tags": "python;nlp;latex;extract;tex"
  },
  {
    "question_id": 44355031,
    "title": "State-of-the-art method for large-scale near-duplicate detection of documents?",
    "body": "<p>To my understanding, the scientific consensus in NLP is that the most effective method for near-duplicate detection in large-scale scientific  document collections (more than 1 billion documents) is the one found here: </p>\n\n<p><a href=\"http://infolab.stanford.edu/~ullman/mmds/ch3.pdf\" rel=\"noreferrer\">http://infolab.stanford.edu/~ullman/mmds/ch3.pdf</a></p>\n\n<p>which can be briefly described by: </p>\n\n<p>a) shingling of documents \nb) minhashing to obtain theminhash signatures of the shingles\nc) locality-sensitive hashing to avoid doing pairwise similarity calculations for all signatures but instead focus only to pairs within buckets.</p>\n\n<p>I am ready to implement this algorithm in Map-Reduce or Spark, but because I am new to the field (I have been reading upon large-scale near-duplicate detection for about two weeks) and the above was published quite a few years ago, I am wondering whether there are known limitations of the above algorithm and whether there are different approaches that are more efficient (offering a more appealing performance/complexity trade-off ). </p>\n\n<p>Thanks in advance! </p>\n",
    "score": 6,
    "creation_date": 1496585593,
    "view_count": 1088,
    "answer_count": 1,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 43598212,
    "title": "Add word embedding to word2vec gensim model",
    "body": "<p>I'm looking for a way to dinamically add pre-trained word vectors to a word2vec gensim model.</p>\n\n<p>I have a pre-trained word2vec model in a txt (words and their embedding) and I need to get Word Mover's Distance (for example via <a href=\"https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.wmdistance.html\" rel=\"noreferrer\" title=\"gensim.models.Word2Vec.wmdistance\">gensim.models.Word2Vec.wmdistance</a>) between documents in a specific corpus and a new document. </p>\n\n<p>To prevent the need to load the whole vocabulary, I would want to load only the subset of the pre-trained model's words that are found in the corpus. But if the new document has words that are not found in the corpus but they are in the original model vocabulary add them to the model so they are considered in the computation.</p>\n\n<p>What I want is to save RAM, so possible things that would help me:</p>\n\n<ul>\n<li>Is there a way to add the word vectors directly to the model?</li>\n<li>Is there a way to load to gensim from a matrix or another object? I could have that object in RAM and append to it the new words before loading them in the model</li>\n<li>I don't need it to be on gensim, so if you know a different implementation for WMD that gets the vectors as input that would work (though I do need it in Python)</li>\n</ul>\n\n<p>Thanks in advance. </p>\n",
    "score": 6,
    "creation_date": 1493070229,
    "view_count": 2544,
    "answer_count": 1,
    "tags": "python;nlp;word2vec"
  },
  {
    "question_id": 38545726,
    "title": "How to use the link grammar parser as a grammar checker",
    "body": "<p>Abiword uses the <a href=\"http://www.abisource.com/projects/link-grammar/\">link grammar parser</a> as a simple grammar checker. I'd like to duplicate this feature with Python. </p>\n\n<p>Poorly documented Python bindings exist, but I don't know how to use them to mimic the grammar checker in Abiword. </p>\n\n<p>(I'm not interested in the actual parsing results. I only need to know if a sentence parses OK with the link grammar parser and if not which words can't be linked.)</p>\n\n<p>What would be the best method to achieve this?</p>\n",
    "score": 6,
    "creation_date": 1469302401,
    "view_count": 1334,
    "answer_count": 1,
    "tags": "python;parsing;nlp;grammar;link-grammar"
  },
  {
    "question_id": 38030703,
    "title": "NLTK - Download all nltk data except corpara from command line without Downloader UI",
    "body": "<p>We can download all nltk data using:</p>\n\n<pre><code>&gt; import nltk\n&gt; nltk.download('all')\n</code></pre>\n\n<p>Or specific data using:</p>\n\n<pre><code>&gt; nltk.download('punkt')\n&gt; nltk.download('maxent_treebank_pos_tagger')\n</code></pre>\n\n<p>But I want to download all data except 'corpara' files, \nfor example -  all chunkers, grammers, models, stemmers, taggers, tokenizers, etc</p>\n\n<p>is there any way to do so without Downloader UI? something like, </p>\n\n<pre><code>&gt; nltk.download('all-taggers')\n</code></pre>\n",
    "score": 6,
    "creation_date": 1466873188,
    "view_count": 5382,
    "answer_count": 1,
    "tags": "python;nlp;nltk;corpus;nltk-trainer"
  },
  {
    "question_id": 31562880,
    "title": "What can I do to speed up Stanford CoreNLP (dcoref/ner)?",
    "body": "<p>I'm processing a large amount of documents using Stanford's CoreNLP library alongside the <a href=\"https://github.com/brendano/stanford_corenlp_pywrapper\" rel=\"noreferrer\">Stanford CoreNLP Python Wrapper</a>. I'm using the following annotators: </p>\n\n<pre><code>tokenize, ssplit, pos, lemma, ner, entitymentions, parse, dcoref\n</code></pre>\n\n<p>along with the shift-reduce parser model <code>englishSR.ser.gz</code>. I'm mainly using CoreNLP for its co-reference resolution / named entity recognition, and as far as I'm aware I'm using the minimal set of annotators for this purpose.</p>\n\n<p><strong>What methods can I take to speed up the annotation of documents?</strong></p>\n\n<p>The other SO answers all suggest not loading the models for every document, but I'm already doing that (since the wrapper starts the server once and then passes documents/results back and forth).</p>\n\n<p>The documents I am processing have an average length of 20 sentences, with some as long as 400 sentences and some as short as 1. The average parse time per sentence is 1 second. I can parse ~2500 documents per day with one single-threaded process running on one machine, but I'd like to double that (if not more).</p>\n",
    "score": 6,
    "creation_date": 1437567398,
    "view_count": 2420,
    "answer_count": 2,
    "tags": "python;nlp;stanford-nlp"
  },
  {
    "question_id": 21533837,
    "title": "How to implement category based text tagging using WordNet or related to wordnet?",
    "body": "<p>How to tag text using wordnet by word's category  (java as a interfacer ) ?</p>\n\n<p><strong>Example</strong> <br></p>\n\n<p>Consider the sentences:</p>\n\n<p>1) Computers need keyboard , moniter , CPU to work. <br>\n2) Automobile uses gears and clutch . <br></p>\n\n<p>Now my objective is , the example sentences have to be tagged as <br></p>\n\n<ul>\n<li>1st sentence</li>\n</ul>\n\n<blockquote>\n  <p>Computer/electronic<br>\n  keyboard/electronic <br> \n  CPU / electronic  <br></p>\n</blockquote>\n\n<p><ul>\n<li>2nd sentence <br></p>\n\n<blockquote>\n  <p>Automobile / mechanical <br>\n  gears / mechanical<br>\n  clutch / mechanical <br></li>\n  </ul>\n  some extra example ...</p>\n</blockquote>\n\n<p>\"Clutch and gear is monitored using microchip \" -> clutch /mechanical , gear/mechanical , microchip / electronic</p>\n\n<p>\"software used here to monitor hydrogen levels\" -> software/computer , hydrogen / chemistry ..</p>\n\n<p>I want to implement above mentions objective in java, that is to tag nouns by it related category such as technical , mechanical , electrical etc.</p>\n\n<p>How to do this using wordnet . </p>\n\n<p><strong>My Previous Works</strong> <br></p>\n\n<p>To achieve my objective I created a index of terms in text files  for each category and matched it with a title .. if it contains a word in text files , then title get classified.</p>\n\n<p>For example </p>\n\n<p><code>Automobile.txt</code> have  <code>car , gear , wheel , clutch</code>. <Br>\n<code>networking.txt</code> have <code>server,IP Address,TCP , RIP</code></p>\n\n<p>This is the Algorithm:</p>\n\n<pre><code>String Classify (String title)\n{\n String area;\n if (compareWordsFrom (\"Automobile.txt\",title) == true ) area = \"Auto\";\n if (compareWordsFrom (\"Netoworking.txt\",title) == true ) area = \"Networking\";\n if (compareWordsFrom (\"metels.txt\",title) == true ) area = \"Metallurgy\";\n return area;\n}\n</code></pre>\n\n<p>it is very difficult to find related words to build the index. That is , the field automobile have 1000 of related terms which difficult to find.  </p>\n\n<p>To be precise , building index of terms manually is a heart-breaking process</p>\n\n<p>I already used Stanford NLP , Open NLP , but they are tagging POS , but not satisfying what is need. </p>\n\n<p><strong>My Need</strong> <br>\nI need an automated way for my work . Do Natural Language Processing  techniques able to  do  it. ? </p>\n\n<p>Some suggesting to use wordnet library , but how can I use it since it is like dictionary , but I wants like .. </p>\n\n<p>mechanical = {gear , turbine , engine ....) \n electronic  = {microchip , RAM , ROM ,...)</p>\n\n<p>Is there any word database available like in above mentioned structure ..</p>\n\n<p>OR I is there is an ready-made library available ?</p>\n",
    "score": 6,
    "creation_date": 1391448956,
    "view_count": 1911,
    "answer_count": 1,
    "tags": "java;machine-learning;nlp;classification;wordnet"
  },
  {
    "question_id": 14491340,
    "title": "detect allusions (e.g. very fuzzy matches) in language of inaugural addresses",
    "body": "<p>I'm trying to develop a Python script to examine every sentence in Barack Obama's second inaugural address and find similar sentences in past inaugurals. I've developed a very crude fuzzy match, and I'm hoping to improve it.</p>\n\n<p>I start by reducing all inaugurals to lists of stopword-free sentences. I then build a frequency index.</p>\n\n<p>Next, I compare each sentence in Obama's 2013 address to each sentence of every other address, and evaluate the similarity like so:</p>\n\n<pre><code>#compare two lemmatized sentences. Assumes stop words already removed. frequencies is dict of frequencies across all inaugural    \ndef compare(sentA, sentB, frequencies):\n    intersect = [x for x in sentA if x in sentB]\n    N = [frequencies[x] for x in intersect]\n    #calculate sum that weights uncommon words based on frequency inaugurals\n    n = sum([10.0 / (x + 1) for x in N])\n    #ratio of matches to total words in both sentences. (John Adams and William Harrison both favored loooooong sentences that tend to produce matches by sheer probability.)\n    c = float(len(intersect)) / (len(sentA) + len(sentB))\n    return (intersect, N, n, c)\n</code></pre>\n\n<p>Last, I filter out results based on arbitrary cutoffs for n and c. </p>\n\n<p>It works better than one might think, identifying sentences that share uncommon words in a non-negligible proportion to total words. </p>\n\n<p>For example, it picked up these matches:</p>\n\n<hr>\n\n<p><strong>Obama, 2013:</strong>\nFor history tells us that while these truths may be self-evident, they have never been self-executing; that while freedom is a gift from God, it must be secured by His people here on Earth.</p>\n\n<p><strong>Kennedy, 1961:</strong>\nWith a good conscience our only sure reward, with history the final judge of our deeds, let us go forth to lead the land we love, asking His blessing and His help, but knowing that here on earth God's work must truly be our own. </p>\n\n<hr>\n\n<p><strong>Obama, 2013</strong>\nThrough blood drawn by lash and blood drawn by sword, we learned that no union founded on the principles of liberty and equality could survive half-slave and half-free.</p>\n\n<p><strong>Lincoln, 1861</strong>\nYet, if God wills that it continue until all the wealth piled by the bondsman's two hundred and fifty years of unrequited toil shall be sunk, and until every drop of blood drawn with the lash shall be paid by another drawn with the sword, as was said three thousand years ago, so still it must be said \"the judgments of the Lord are true and righteous altogether.</p>\n\n<hr>\n\n<p><strong>Obama, 2013</strong>\nThis generation of Americans has been tested by crises that steeled our resolve and proved our resilience</p>\n\n<p><strong>Kennedy, 1961</strong>\nSince this country was founded, each generation of Americans has been summoned to give testimony to its national loyalty.</p>\n\n<hr>\n\n<p>But it's very crude.</p>\n\n<p>I don't have the chops for a major machine-learning project, but I do want to apply more theory if possible. I understand bigram searching, but I'm not sure that will work here -- it's not so much exact bigrams we're interested in as general proximity of two words that are shared between quotes. Is there a fuzzy sentence comparison that looks at probability and distribution of words without being too rigid? The nature of allusion is that it's very approximate.</p>\n\n<p>Current effort <a href=\"https://c9.io/wilson428/inaugurals\" rel=\"nofollow noreferrer\">available on Cloud9IDE</a></p>\n\n<p><strong>UPDATE, 1/24/13</strong>\nPer the accepted answer, here's a simple Python function for bigram windows:</p>\n\n<pre><code>def bigrams(tokens, blur=1):\n    grams = []\n    for c in range(len(tokens) - 1):\n        for i in range(c + 1, min(c + blur + 1, len(tokens))):\n            grams.append((tokens[c], tokens[i]))\n    return grams\n</code></pre>\n",
    "score": 6,
    "creation_date": 1358983795,
    "view_count": 210,
    "answer_count": 1,
    "tags": "python;text;nlp;nltk"
  },
  {
    "question_id": 10649581,
    "title": "Tokenization, and indexing with Lucene, how to handle external tokenize and part-of-speech?",
    "body": "<p>i would like to build my own - here am not sure which one - tokenizer (from Lucene point of view) or my own analyzer. I already write a code that tokenize my documents in word (as a List &lt; String > or a List &lt; <em>Word</em> > where <em>Word</em> is a class with only a kind of container with 3 public String : word, pos, lemma - pos stand for part-of-speech tag).</p>\n\n<p>i'm not sure what i am going to index, maybe only \"<em>Word.lemma</em>\" or something like \"<em>Word.lemma + '#' + Word.pos</em>\", probably i will do some filtering from a stop word list based on part-of-speech.</p>\n\n<p>btw here is my misunderstanding : i am not sure where i should plug to the Lucene API,</p>\n\n<p>should i wrap my own tokenizer inside a new tokenizer ? should i rewrite TokenStream ? should i consider that this is the job of the analyzer rather than the tokenizer ? or shoud i bypass everything and directly build my index by adding my word directly inside index, using IndexWriter, Fieldable and so on ? (if so do you know of any documentation on how to create it's own index from scratch when bypass ing the analysis process)</p>\n\n<p>best regards</p>\n\n<p><strong>EDIT</strong> : may be the simplest way should be to org.apache.commons.lang.StringUtils.join my <em>Word</em>-s with a space on the exit of my personal tokenizer/analyzer and rely on the WhiteSpaceTokenizer to feed Lucene (and other classical filters) ?</p>\n\n<p><strong>EDIT</strong> : so, i have read <strong>EnglishLemmaTokenizer</strong> pointed by <strong>Larsmans</strong>... but where i am still confused, is the fact that i end my own analysis/tokenization process with a complete *List &lt; Word > * (<em>Word</em> class wrapping <em>.form/.pos/.lemma</em>) , this process rely on an external binary that i had wrapped in Java (this is a must do / can not do otherwise - it is not on a consumer point of view, i get the full list as a result) and i still not see how i should wrap it again to get back to the normal Lucene analysis process.</p>\n\n<p>also i will be using the TermVector feature with TF.IDF like scoring (may be redefining my own), i may also be interested in the proximty searching, thus, discarding some words from their part-of- speech before providing them to a Lucene built-in tokenizer or internal analyzer may seem a bad idea.  And i have difficulties in thinking of a \"proper\" way to wrap a Word.form / Word.pos / Word.lemma (or even other Word.anyOtherUnterestingAttribute) to the Lucene way.</p>\n\n<p><strong>EDIT:</strong> \nBTW, here is a piece of code that i write inspired by the one of @Larsmans :</p>\n\n<pre><code>class MyLuceneTokenizer extends TokenStream {\n\n    private PositionIncrementAttribute posIncrement;\n    private CharTermAttribute termAttribute;\n\n    private List&lt;TaggedWord&gt; tagged;\n    private int position;\n\n    public MyLuceneTokenizer(Reader input, String language, String pathToExternalBinary) {\n        super();\n\n        posIncrement = addAttribute(PositionIncrementAttribute.class);\n        termAttribute = addAttribute(CharTermAttribute.class); // TermAttribute is deprecated!\n\n        // import com.google.common.io.CharStreams;            \n        text = CharStreams.toString(input); //see http://stackoverflow.com/questions/309424/in-java-how-do-i-read-convert-an-inputstream-to-a-string\n        tagged = MyTaggerWrapper.doTagging(text, language, pathToExternalBinary);\n        position = 0;\n    }\n\n    public final boolean incrementToken()\n            throws IOException {\n        if (position &gt; tagged.size() -1) {\n            return false;\n        }\n\n        int increment = 1; // will probably be changed later depending upon any POS filtering or insertion @ same place...\n        String form = (tagged.get(position)).word;\n        String pos = (tagged.get(position)).pos;\n        String lemma = (tagged.get(position)).lemma;\n\n        // logic filtering should be here...\n        // BTW we have broken the idea behing the Lucene nested filters or analyzers! \n        String kept = lemma;\n\n        if (kept != null) {\n            posIncrement.setPositionIncrement(increment);\n            char[] asCharArray = kept.toCharArray();\n            termAttribute.copyBuffer(asCharArray, 0, asCharArray.length);\n            //termAttribute.setTermBuffer(kept);\n            position++;\n        }\n\n        return true;\n    }\n}\n\nclass MyLuceneAnalyzer extends Analyzer {\n    private String language;\n    private String pathToExternalBinary;\n\n    public MyLuceneAnalyzer(String language, String pathToExternalBinary) {\n        this.language = language;\n        this.pathToExternalBinary = pathToExternalBinary;\n    }\n\n    @Override\n    public TokenStream tokenStream(String fieldname, Reader input) {\n        return new MyLuceneTokenizer(input, language, pathToExternalBinary);\n    }\n}\n</code></pre>\n",
    "score": 6,
    "creation_date": 1337331355,
    "view_count": 2619,
    "answer_count": 2,
    "tags": "java;lucene;nlp;tokenize"
  },
  {
    "question_id": 5555170,
    "title": "Term Extraction and Sentiment Analysis Open Source Project",
    "body": "<p>I want to extract important terms from a text and create a domain specific term set. Then I want to learn how these words are used in text, positively or negatively. </p>\n\n<p>Do you know any open source project which will help me to accomplish this tasks?</p>\n\n<p>Edit: </p>\n\n<p>Example Text:</p>\n\n<pre><code>\"Although car is not comfortable, I like the design of it.\"\n</code></pre>\n\n<p>From this text, I want to extract something like these:</p>\n\n<pre><code>design:        positive\ncomfort(able): negative\n</code></pre>\n",
    "score": 6,
    "creation_date": 1302020870,
    "view_count": 2796,
    "answer_count": 1,
    "tags": "open-source;nlp;machine-learning"
  },
  {
    "question_id": 4990936,
    "title": "Dutch Grammar in python&#39;s NLTK",
    "body": "<p>I am working on a Dutch corpus and I want to know if NLTK has dutch grammar embedded in it so I can parse my sentences? In general does NLTK only work on English? I know that it has the Alpino dutch copora, but there is no indication that the functions (like parsing using CFGs) are made for Dutch also.\nThanks</p>\n",
    "score": 6,
    "creation_date": 1297678370,
    "view_count": 2762,
    "answer_count": 2,
    "tags": "python;parsing;nlp;nltk;context-free-grammar"
  },
  {
    "question_id": 4827365,
    "title": "Problems with Prolog&#39;s DCG",
    "body": "<p>The project is about translating semi-natural language to SQL tables. The code:</p>\n\n<pre><code>label(S) --&gt; label_h(C), {atom_codes(A, C), string_to_atom(S, A)}, !.\n\nlabel_h([C|D]) --&gt; letter(C), letters_or_digits(D), !.\n\nletters_or_digits([C|D]) --&gt; letter_or_digit(C), letters_or_digits(D), !.\nletters_or_digits([C]) --&gt; letter_or_digit(C), !.\nletters_or_digits([]) --&gt; \"\", !.\n\nletter(C) --&gt; [C], {\"a\"=&lt;C, C=&lt;\"z\"}, !.\nletter(C) --&gt; [C], {\"A\"=&lt;C, C=&lt;\"Z\"}, !.\nletter_or_digit(C) --&gt; [C], {\"a\"=&lt;C, C=&lt;\"z\"}, !.\nletter_or_digit(C) --&gt; [C], {\"A\"=&lt;C, C=&lt;\"Z\"}, !.\nletter_or_digit(C) --&gt; [C], {\"0\"=&lt;C, C=&lt;\"9\"}, !.\n\ntable(\"student\").\n\nsbvr2sql --&gt; label(Name), \" is an integer.\", {assert(fields(Name, \"INT\"))}.\nsbvr2sql --&gt; label(Name), \" is a string.\", {assert(fields(Name, \"VARCHAR(64)\"))}.\n\nsbvr2sql(Table, Property)  --&gt; label(Table), \" has \", label(Property), \".\".\n</code></pre>\n\n<p>Here is how it works fine:</p>\n\n<pre><code>?- sbvr2sql(\"age is an integer.\", []).\ntrue \n\n?- sbvr2sql(\"firstName is a string.\", []).\ntrue.\n\n?- sbvr2sql(T, P, \"student has firstName.\", []).\nT = \"student\",\nP = \"firstName\".\n\n?- fields(F, T).\nF = \"age\",\nT = [73, 78, 84] n\nF = \"firstName\",\nT = [86, 65, 82, 67, 72, 65, 82, 40, 54|...].\n\n?- sbvr2sql(T, P, \"student has firstName.\", []), fields(P, _).\nT = \"student\",\nP = \"firstName\".\n</code></pre>\n\n<p>But it doesn't work here:</p>\n\n<pre><code>?- table(T).\nT = [115, 116, 117, 100, 101, 110, 116]. % \"student\"\n\n?- sbvr2sql(T, P, \"student has firstName.\", []), table(T).\nfalse.\n</code></pre>\n\n<p>Apparently it doesn't recognise <code>table(\"student\")</code> as true. It recognises \"student\" as a label as seen above. What gives?</p>\n",
    "score": 6,
    "creation_date": 1296211791,
    "view_count": 427,
    "answer_count": 1,
    "tags": "prolog;nlp;grammar;dcg"
  },
  {
    "question_id": 2580367,
    "title": "How to detect if two news articles have the same topic? (Python semantic similarity)",
    "body": "<p>I'm trying to scrape headlines and body text from articles on a few specific sites, similar to what Google does with Google News.</p>\n<p>The problem is that across different sites, they may have articles on the same subject worded slightly differently.</p>\n<p>Can anyone tell me what I need to know in order to write a comparison algorithm to auto-detect similar articles? Or, is there any library that can be used for text comparisons and return some type of similarity rating? Solutions that use Python are desired.</p>\n",
    "score": 6,
    "creation_date": 1270493558,
    "view_count": 2933,
    "answer_count": 1,
    "tags": "python;nlp;comparison;similarity"
  },
  {
    "question_id": 72776834,
    "title": "Blenderbot FineTuning",
    "body": "<p>I have been trying to fine-tune a conversational model of HuggingFace: Blendebot. I have tried the conventional method given on the official hugging face website which asks us to do it using the trainer.train() method. I tried it using the .compile() method. I have tried fine-tuning using PyTorch as well as TensorFlow on my dataset. Both methods seem to fail and give us an error saying that there is no method called compile or train for the Blenderbot model. I have even looked everywhere online to check how Blenderbot could be fine-tuned on my custom data and nowhere does it mention properly that runs without throwing an error. I have gone through Youtube tutorials, blogs, and StackOverflow posts but none answer this question. Hoping someone would respond here and help me out. I am open to using other HuggingFace Conversational Models as well for fine-tuning.</p>\n<p>Here is a link I am using to fine-tune the blenderbot model.</p>\n<p>Fine-tuning methods: <a href=\"https://huggingface.co/docs/transformers/training\" rel=\"noreferrer\">https://huggingface.co/docs/transformers/training</a></p>\n<p>Blenderbot: <a href=\"https://huggingface.co/docs/transformers/model_doc/blenderbot\" rel=\"noreferrer\">https://huggingface.co/docs/transformers/model_doc/blenderbot</a></p>\n<pre><code>from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\nmname = &quot;facebook/blenderbot-400M-distill&quot;\nmodel = BlenderbotForConditionalGeneration.from_pretrained(mname)\ntokenizer = BlenderbotTokenizer.from_pretrained(mname)\n\n\n#FOR TRAINING: \n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n\n#OR\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=tf.metrics.SparseCategoricalAccuracy(),\n)\n\nmodel.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n</code></pre>\n<p>None of these work.</p>\n",
    "score": 6,
    "creation_date": 1656354623,
    "view_count": 1150,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 71600683,
    "title": "Reproducibility issue with PyTorch",
    "body": "<p>I'm running a script with the same seed and I see results are reproduced on consecutive runs but somehow running the same script with the same seed changes the output after a few days. I'm only getting a short-term reproducibility which is weird. For reproducibility my script includes the following statements already:</p>\n<pre><code>torch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\ntorch.use_deterministic_algorithms(True)\n\nrandom.seed(args.seed)\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\n</code></pre>\n<p>I also checked the sequence of instance ids created by the RandomSampler for train Dataloader which is maintained across runs. Also set the num_workers=0 in the dataloader.What could be causing the output to change?</p>\n",
    "score": 6,
    "creation_date": 1648117373,
    "view_count": 4665,
    "answer_count": 2,
    "tags": "python;nlp;pytorch;seed"
  },
  {
    "question_id": 68500136,
    "title": "Training a basic spacy text classification model",
    "body": "<p>I am trying to train a basic text classification model using spaCy. I have a list of texts and I want to build a model which will classify either text as <code>outcome1</code> or <code>outcome2</code>. Let's say my data looks like this:</p>\n<pre><code>texts = [&quot;This is the first example text&quot;,\n         &quot;This is the second example text&quot;,\n         &quot;This is yet another text&quot;]\ny = [&quot;outcome2&quot;, &quot;outcome1&quot;, &quot;outcome1&quot;]\n</code></pre>\n<p>My problem is, I have trouble even processing the texts into docs:</p>\n<pre><code>nlp = spacy.blank(&quot;en&quot;)\n\ntextcat = nlp.create_pipe(&quot;textcat&quot;)\ntextcat.add_label(&quot;outcome1&quot;)\ntextcat.add_label(&quot;outcome2&quot;)\ntextcat = nlp.add_pipe(&quot;textcat&quot;, last = True)\n\nnlp.pipe_names\n</code></pre>\n<pre><code>&gt;&gt;&gt; ['textcat']\n</code></pre>\n<p>But when I try to process any text I get an error:</p>\n<pre><code>doc = nlp(&quot;This is a sentence&quot;)\n</code></pre>\n<pre><code>&gt;&gt;&gt; ValueError: Cannot get dimension 'nO' for model 'sparse_linear': value unset\n</code></pre>\n<p>I've tried to follow <a href=\"https://www.machinelearningplus.com/nlp/custom-text-classification-spacy/\" rel=\"noreferrer\">this</a> tutorial (which is a bit outdated) and setup a project using the <a href=\"https://spacy.io/usage/training\" rel=\"noreferrer\">spaCy quickstart widget</a>, but I keep running into errors when initialising the config file. What am I missing?</p>\n",
    "score": 6,
    "creation_date": 1627047605,
    "view_count": 3001,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 63307009,
    "title": "What decoder_input_ids should be for sequence-to-sequence Transformer model?",
    "body": "<p>I use the HuggingFace's Transformers library for building a <strong>sequence-to-sequence model</strong> based on BART and T5. I carefully read the documentation and the research paper and I can't find what the input to the decoder (decoder_input_ids) should be for sequence-to-sequence tasks.</p>\n<p>Should decoder input for both models (BART and T5) be same as lm_labels (output of the LM head) or should it be same as input_ids (input to the encoder)?</p>\n",
    "score": 6,
    "creation_date": 1596823096,
    "view_count": 8804,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers"
  },
  {
    "question_id": 62154230,
    "title": "BPE vs WordPiece Tokenization - when to use / which?",
    "body": "<p>What's the general tradeoff between choosing BPE vs WordPiece Tokenization? When is one preferable to the other? Are there any differences in model performance between the two? I'm looking for a general overall answer, backed up with specific examples. Thanks!</p>\n",
    "score": 6,
    "creation_date": 1591107807,
    "view_count": 2019,
    "answer_count": 1,
    "tags": "machine-learning;nlp;lstm;transformer-model;huggingface-transformers"
  },
  {
    "question_id": 59458952,
    "title": "Universal sentence encoder for big document similarity",
    "body": "<p>I need to create a 'search engine' experience : from a short query (few words), I need to find the relevant documents in a corpus of thousands documents.</p>\n\n<p>After analyzing few approaches, I got very good results with the Universal Sentence Encoder from Google.\nThe problem is that my documents can be very long. For these very long texts it looks like the performance are decreasing so my idea was to cut the text in sentences/paragraph.</p>\n\n<p>So I ended up with getting a list of vectors for each document (representing each part of the document).</p>\n\n<p>My question is : is there a state-of-the-art algorithm/methodology to compute a scoring from a list of vector ? I don't really want to merge them into one as it would create the same effect than before (the relevant part would be diluted in the document). Any scoring algorithms to sum up the multiple cosine similarities between the query and the different parts of the text ?</p>\n\n<p>important information : I can have short and long text. So I can have 1 up to 10 vectors for a document.</p>\n",
    "score": 6,
    "creation_date": 1577120760,
    "view_count": 1198,
    "answer_count": 1,
    "tags": "machine-learning;nlp;cosine-similarity;word-embedding"
  },
  {
    "question_id": 55783903,
    "title": "Efficient way to check if a large list of words exists in millions of search queries",
    "body": "<ol>\n<li>I have a list of strings containing 50 million search queries. [1-500+ words in each query]. </li>\n<li>I also have a list of strings containing 500 words and phrases \nI need to return indices of search queries (1) that contain any word or phrase (2). </li>\n</ol>\n\n<p>The goal is to only keep queries related to a certain topic (movies) and then use NLP to cluster these filtered queries (stemming -> tf_idf -> pca -> kmeans).</p>\n\n<p>I tried to filter queries using nested loops, but it would take more than 10 hours to finish. </p>\n\n<pre><code>filtered = []\nwith open('search_logs.txt', 'r', encoding='utf-8') as f:\n    for i, line in enumerate(f):\n        query, timestamp = line.strip().split('\\t')\n        for word in key_words:\n            if word in query:\n                filtered.append(i)\n</code></pre>\n\n<p>I looked into solutions which use regex (word1|word2|...|wordN), but the problem is that i cannot combine queries into a large string since i need to filter irrelevant queries.</p>\n\n<p>UPDATE: examples of logs and keywords</p>\n\n<pre><code>search_logs.txt\n'query  timestamp\\n'\n'the dark knight    2019-02-17 19:05:12\\n'\n'how to do a barrel roll    2019-02-17 19:05:13\\n'\n'watch movies   2019-02-17 19:05:13\\n'\n'porn   2019-02-17 19:05:13\\n'\n'news   2019-02-17 19:05:14\\n'\n'rami malek 2019-02-17 19:05:14\\n'\n'Traceback (most recent call last): File \"t.py\" 2019-02-17 19:05:15\\n'\n.......... # millions of other search queries\n</code></pre>\n\n<pre><code>key_words = [\n    'movie',\n    'movies',\n    'cinema',\n    'oscar',\n    'oscars',\n    'george lucas',\n    'ben affleck',\n    'netflix',\n    .... # hundreds of other words and phrases\n]\n</code></pre>\n",
    "score": 6,
    "creation_date": 1555859086,
    "view_count": 1152,
    "answer_count": 2,
    "tags": "python;regex;nlp"
  },
  {
    "question_id": 50942315,
    "title": "Which romanization standard should be used to improve ICU4j transliteration for Arabic-Latin?",
    "body": "<p>We have a requirement to transliterate Arabic text to Latin characters(without diacritical marks) and display them to users. </p>\n\n<p>We are currently using IBM ICU4j for this. \nThe API doesn't trasliterate well the Arabic text into proper readable latin characters. Refer the below examples:</p>\n\n<p>Example</p>\n\n<ul>\n<li><p>Arabic text :</p>\n\n<p>صدام حسين التكريتي </p></li>\n<li><p>Google's transliteration output</p>\n\n<p>: <code>Sadaam Hussein al-tikriti</code></p></li>\n<li><p>ICU4J's transliteration outuput</p>\n\n<p>: <code>ṣdạm ḥsyn ạltkryty</code></p></li>\n</ul>\n\n<p>How can we improve the transliterated output of ICU4j library? </p>\n\n<p>ICU4J gives us an option to write our own rules but we are currently stuck as no one from our team knows Arabic and are unable to find any proper standard that can be followed.</p>\n",
    "score": 6,
    "creation_date": 1529478756,
    "view_count": 1121,
    "answer_count": 1,
    "tags": "java;nlp;transliteration;transcription;icu4j"
  },
  {
    "question_id": 49748640,
    "title": "Vectorize list of lists uisng countvectorizer() &amp; tfidfvectorizer()",
    "body": "<p>So I have the following list of lists which is tokenized:</p>\n\n<pre><code>tokenized_list = [['ALL', 'MY', 'CATS', 'IN', 'A', 'ROW'], ['WHEN', 'MY', \n                   'CAT', 'SITS', 'DOWN', ',', 'SHE', 'LOOKS', 'LIKE', 'A', \n                   'FURBY', 'TOY', '!'], ['THE', CAT', 'FROM', 'OUTER', \n                   'SPACE'], ['SUNSHINE', 'LOVES', 'TO', 'SIT', \n                   'LIKE', 'THIS', 'FOR', 'SOME', 'REASON', '.']]\n</code></pre>\n\n<p>When i try to vectorize it using the CountVectorizer() or TfIdfVectorizer()</p>\n\n<pre><code> from sklearn.feature_extraction.text import CountVectorizer\n vectorizer = CountVectorizer()\n print(vectorizer.fit_transform(tokenized_list).todense()) \n print(vectorizer.vocabulary_)\n</code></pre>\n\n<p>I am getting the following error: </p>\n\n<pre><code>AttributeError: 'list' object has no attribute 'lower'\n</code></pre>\n\n<p>And if I put a <strong>simple list</strong> inside the <strong>vectorizer.fit_transform()</strong> function it works properly.</p>\n\n<p>How do I remove this error?</p>\n",
    "score": 6,
    "creation_date": 1523348161,
    "view_count": 2266,
    "answer_count": 1,
    "tags": "python;pandas;scikit-learn;nlp;countvectorizer"
  },
  {
    "question_id": 48690415,
    "title": "Load vectors into gensim Word2Vec model - not KeyedVectors",
    "body": "<p>I'm attempting to load some pre-trained vectors into a gensim <code>Word2Vec</code> model, so they can be retrained with new data. My understanding is I can do the retraining with <code>gensim.Word2Vec.train()</code>. However, the only way I can find to load the vectors is with <code>gensim.models.KeyedVectors.load_word2vec_format('path/to/file.bin', binary=True)</code> which creates an object of what is usually the <code>wv</code> attribute of a <code>gensim.Word2Vec</code> model. But this object, on it's own, does not have a <code>train()</code> method, which is what I need to retrain the vectors. </p>\n\n<p>So how do I get these vectors into an actual <code>gensim.Word2Vec</code> model?</p>\n",
    "score": 6,
    "creation_date": 1518107704,
    "view_count": 3628,
    "answer_count": 1,
    "tags": "machine-learning;nlp;word2vec;gensim;word-embedding"
  },
  {
    "question_id": 43985683,
    "title": "Automatic labeling of LDA generated topics",
    "body": "<p>I'm trying to categorize customer feedback and I ran an LDA in python and got the following output for 10 topics:</p>\n\n<pre><code>(0, u'0.559*\"delivery\" + 0.124*\"area\" + 0.018*\"mile\" + 0.016*\"option\" + 0.012*\"partner\" + 0.011*\"traffic\" + 0.011*\"hub\" + 0.011*\"thanks\" + 0.010*\"city\" + 0.009*\"way\"')\n(1, u'0.397*\"package\" + 0.073*\"address\" + 0.055*\"time\" + 0.047*\"customer\" + 0.045*\"apartment\" + 0.037*\"delivery\" + 0.031*\"number\" + 0.026*\"item\" + 0.021*\"support\" + 0.018*\"door\"')\n(2, u'0.190*\"time\" + 0.127*\"order\" + 0.113*\"minute\" + 0.075*\"pickup\" + 0.074*\"restaurant\" + 0.031*\"food\" + 0.027*\"support\" + 0.027*\"delivery\" + 0.026*\"pick\" + 0.018*\"min\"')\n(3, u'0.072*\"code\" + 0.067*\"gps\" + 0.053*\"map\" + 0.050*\"street\" + 0.047*\"building\" + 0.043*\"address\" + 0.042*\"navigation\" + 0.039*\"access\" + 0.035*\"point\" + 0.028*\"gate\"')\n(4, u'0.434*\"hour\" + 0.068*\"time\" + 0.034*\"min\" + 0.032*\"amount\" + 0.024*\"pay\" + 0.019*\"gas\" + 0.018*\"road\" + 0.017*\"today\" + 0.016*\"traffic\" + 0.014*\"load\"')\n(5, u'0.245*\"route\" + 0.154*\"warehouse\" + 0.043*\"minute\" + 0.039*\"need\" + 0.039*\"today\" + 0.026*\"box\" + 0.025*\"facility\" + 0.025*\"bag\" + 0.022*\"end\" + 0.020*\"manager\"')\n(6, u'0.371*\"location\" + 0.110*\"pick\" + 0.097*\"system\" + 0.040*\"im\" + 0.038*\"employee\" + 0.022*\"evening\" + 0.018*\"issue\" + 0.015*\"request\" + 0.014*\"while\" + 0.013*\"delivers\"')\n(7, u'0.182*\"schedule\" + 0.181*\"please\" + 0.059*\"morning\" + 0.050*\"application\" + 0.040*\"payment\" + 0.026*\"change\" + 0.025*\"advance\" + 0.025*\"slot\" + 0.020*\"date\" + 0.020*\"tomorrow\"')\n(8, u'0.138*\"stop\" + 0.110*\"work\" + 0.062*\"name\" + 0.055*\"account\" + 0.046*\"home\" + 0.043*\"guy\" + 0.030*\"address\" + 0.026*\"city\" + 0.025*\"everything\" + 0.025*\"feature\"') \n</code></pre>\n\n<p>Is there a way to automatically label them? I do have a csv file which has feedbacks manually labeled, but I do not want to supply these labels myself. I want the model to create labels. Is it possible?</p>\n",
    "score": 6,
    "creation_date": 1494870075,
    "view_count": 2521,
    "answer_count": 1,
    "tags": "python;nlp;lda;topic-modeling;labeling"
  },
  {
    "question_id": 42621652,
    "title": "Tensorflow : ValueError: Shape must be rank 2 but is rank 3",
    "body": "<p>I'm new to tensorflow and I'm trying to update some code for a bidirectional LSTM from an old version of tensorflow to the newest (1.0), but I get this error: </p>\n\n<blockquote>\n  <p>Shape must be rank 2 but is rank 3 for 'MatMul_3' (op: 'MatMul') with input shapes: [100,?,400], [400,2].</p>\n</blockquote>\n\n<p>The error happens on pred_mod.</p>\n\n<pre><code>    _weights = {\n    # Hidden layer weights =&gt; 2*n_hidden because of foward + backward cells\n        'w_emb' : tf.Variable(0.2 * tf.random_uniform([max_features,FLAGS.embedding_dim], minval=-1.0, maxval=1.0, dtype=tf.float32),name='w_emb',trainable=False),\n        'c_emb' : tf.Variable(0.2 * tf.random_uniform([3,FLAGS.embedding_dim],minval=-1.0, maxval=1.0, dtype=tf.float32),name='c_emb',trainable=True),\n        't_emb' : tf.Variable(0.2 * tf.random_uniform([tag_voc_size,FLAGS.embedding_dim], minval=-1.0, maxval=1.0, dtype=tf.float32),name='t_emb',trainable=False),\n        'hidden_w': tf.Variable(tf.random_normal([FLAGS.embedding_dim, 2*FLAGS.num_hidden])),\n        'hidden_c': tf.Variable(tf.random_normal([FLAGS.embedding_dim, 2*FLAGS.num_hidden])),\n        'hidden_t': tf.Variable(tf.random_normal([FLAGS.embedding_dim, 2*FLAGS.num_hidden])),\n        'out_w': tf.Variable(tf.random_normal([2*FLAGS.num_hidden, FLAGS.num_classes]))}\n\n    _biases = {\n         'hidden_b': tf.Variable(tf.random_normal([2*FLAGS.num_hidden])),\n         'out_b': tf.Variable(tf.random_normal([FLAGS.num_classes]))}\n\n\n    #~ input PlaceHolders\n    seq_len = tf.placeholder(tf.int64,name=\"input_lr\")\n    _W = tf.placeholder(tf.int32,name=\"input_w\")\n    _C = tf.placeholder(tf.int32,name=\"input_c\")\n    _T = tf.placeholder(tf.int32,name=\"input_t\")\n    mask = tf.placeholder(\"float\",name=\"input_mask\")\n\n    # Tensorflow LSTM cell requires 2x n_hidden length (state &amp; cell)\n    istate_fw = tf.placeholder(\"float\", shape=[None, 2*FLAGS.num_hidden])\n    istate_bw = tf.placeholder(\"float\", shape=[None, 2*FLAGS.num_hidden])\n    _Y = tf.placeholder(\"float\", [None, FLAGS.num_classes])\n\n    #~ transfortm into Embeddings\n    emb_x = tf.nn.embedding_lookup(_weights['w_emb'],_W)\n    emb_c = tf.nn.embedding_lookup(_weights['c_emb'],_C)\n    emb_t = tf.nn.embedding_lookup(_weights['t_emb'],_T)\n\n    _X = tf.matmul(emb_x, _weights['hidden_w']) + tf.matmul(emb_c, _weights['hidden_c']) + tf.matmul(emb_t, _weights['hidden_t']) + _biases['hidden_b']\n\n    inputs = tf.split(_X, FLAGS.max_sent_length, axis=0, num=None, name='split')\n\n    lstmcell = tf.contrib.rnn.BasicLSTMCell(FLAGS.num_hidden, forget_bias=1.0, \n    state_is_tuple=False)\n\n    bilstm = tf.contrib.rnn.static_bidirectional_rnn(lstmcell, lstmcell, inputs, \n    sequence_length=seq_len, initial_state_fw=istate_fw, initial_state_bw=istate_bw)\n\n\n    pred_mod = [tf.matmul(item, _weights['out_w']) + _biases['out_b'] for item in bilstm]\n</code></pre>\n\n<p>Any help appreciated.</p>\n",
    "score": 6,
    "creation_date": 1488791743,
    "view_count": 5224,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;lstm;bidirectional"
  },
  {
    "question_id": 28343181,
    "title": "Memory efficient way of union a sequence of RDDs from Files in Apache Spark",
    "body": "<p>I'm currently trying to train a set of Word2Vec Vectors on the UMBC Webbase Corpus (around 30GB of text in 400 files).</p>\n\n<p>I often run into out of memory situations even on 100 GB plus Machines. I run Spark in the application itself. I tried to tweak a little bit, but I am not able to perform this operation on more than 10 GB of textual data. The clear bottleneck of my implementation is the union of the previously computed RDDs, that where the out of memory exception comes from.</p>\n\n<p>Maybe one you have the experience to come up with a more memory efficient implementation than this:</p>\n\n<pre><code> object SparkJobs {\n  val conf = new SparkConf()\n    .setAppName(\"TestApp\")\n    .setMaster(\"local[*]\")\n    .set(\"spark.executor.memory\", \"100g\")\n    .set(\"spark.rdd.compress\", \"true\")\n\n  val sc = new SparkContext(conf)\n\n\n  def trainBasedOnWebBaseFiles(path: String): Unit = {\n    val folder: File = new File(path)\n\n    val files: ParSeq[File] = folder.listFiles(new TxtFileFilter).toIndexedSeq.par\n\n\n    var i = 0;\n    val props = new Properties();\n    props.setProperty(\"annotators\", \"tokenize, ssplit\");\n    props.setProperty(\"nthreads\",\"2\")\n    val pipeline = new StanfordCoreNLP(props);\n\n    //preprocess files parallel\n    val training_data_raw: ParSeq[RDD[Seq[String]]] = files.map(file =&gt; {\n      //preprocess line of file\n      println(file.getName() +\"-\" + file.getTotalSpace())\n      val rdd_lines: Iterator[Option[Seq[String]]] = for (line &lt;- Source.fromFile(file,\"utf-8\").getLines) yield {\n          //performs some preprocessing like tokenization, stop word filtering etc.\n          processWebBaseLine(pipeline, line)    \n      }\n      val filtered_rdd_lines = rdd_lines.filter(line =&gt; line.isDefined).map(line =&gt; line.get).toList\n      println(s\"File $i done\")\n      i = i + 1\n      sc.parallelize(filtered_rdd_lines).persist(StorageLevel.MEMORY_ONLY_SER)\n\n    })\n\n    val rdd_file =  sc.union(training_data_raw.seq)\n\n    val starttime = System.currentTimeMillis()\n    println(\"Start Training\")\n    val word2vec = new Word2Vec()\n\n    word2vec.setVectorSize(100)\n    val model: Word2VecModel = word2vec.fit(rdd_file)\n\n    println(\"Training time: \" + (System.currentTimeMillis() - starttime))\n    ModelUtil.storeWord2VecModel(model, Config.WORD2VEC_MODEL_PATH)  \n  }}\n}\n</code></pre>\n",
    "score": 6,
    "creation_date": 1423136931,
    "view_count": 4339,
    "answer_count": 2,
    "tags": "scala;nlp;apache-spark;bigdata;word2vec"
  },
  {
    "question_id": 28072231,
    "title": "Text summarization: how to choose the right n-gram size",
    "body": "<p>I am working on summarizing texts, using nltk library I am able to extract bigrams unigrams and trigrams and order them by frequency</p>\n\n<p>As I am very new to this area (NLP) I was wondering if I can use a statistical model that will allow me to automatically choose the right size of Ngrams (what I mean by size the length of the N-gram  one word unigram, two words bigram, or 3 words trigram)</p>\n\n<p>Example, let's say I have this text that I want to summarize, and as a summary I will keep just the 5 most relevant N-grams: </p>\n\n<pre><code>\"A more principled way to estimate sentence importance is using random walks \nand eigenvector centrality. LexRank[5] is an algorithm essentially identical \nto TextRank, and both use this approach for document summarization. The two \nmethods were developed by different groups at the same time, and LexRank \nsimply focused on summarization, but could just as easily be used for\nkeyphrase extraction or any other NLP ranking task.\" wikipedia\n</code></pre>\n\n<p>Then as an output I want to have, \"random walks\", \"texRank\", \"lexRanks\", \"document summarization\", \"keyphrase extraction\", \" NLP ranking task\" </p>\n\n<p>In other words my is question : How to infer that a unigram will be more relevant than a bigram or trigram? (using just frequency as measure of the relevance of an N-gram will not give me the results that I want to have)</p>\n\n<p>Can anyone point to me a research paper, an algorithm or a course where such a method has been already used or explained</p>\n\n<p>Thank you in advance.</p>\n",
    "score": 6,
    "creation_date": 1421857993,
    "view_count": 4582,
    "answer_count": 3,
    "tags": "nlp;data-mining;information-retrieval;text-mining;summary"
  },
  {
    "question_id": 16890718,
    "title": "SQL queries to their natural language description",
    "body": "<p>Are there any open source tools that can generate a natural language description of a given SQL query? If not, some general pointers would be appreciated.</p>\n\n<p>I don't know much about NLP, so I am not sure how difficult this is, although I saw from some previous discussion that the vice versa conversion is still an active area of research. It might help to say that the SQL tables I will be handling are not arbitrary in any sense, yet mine, which means that I know exact semantics of each table and its columns.</p>\n",
    "score": 6,
    "creation_date": 1370238869,
    "view_count": 4004,
    "answer_count": 2,
    "tags": "sql;nlp;transformation"
  },
  {
    "question_id": 7863830,
    "title": "Error building Stanford CoreNLP",
    "body": "<p>When I build Core-NLP on my own, I get the following message:</p>\n\n<pre><code>incompatible types; no instance(s) of type variable(s) VALUE exist so that VALUE conforms to Map&lt;Integer,String&gt;\n</code></pre>\n\n<p>The offending line:</p>\n\n<pre><code>Map&lt;Integer,String&gt; roleMap = ((CoreLabel)t1.label()).get(CoreAnnotations.CoNLLSRLAnnotation.class);\n</code></pre>\n\n<p>The offending function:</p>\n\n<pre><code>  @SuppressWarnings(\"unchecked\")\n  public &lt;VALUE, KEY extends Key&lt;CoreMap, VALUE&gt;&gt;\n    VALUE get(Class&lt;KEY&gt; key) {\n    for (int i = size; i &gt; 0; ) {\n   if (keys[--i] == key) {\n    return (VALUE)values[i];\n  }\n}\n    return null;\n}\n</code></pre>\n\n<p>I really have no clue how to fix this. I'm trying to build CoreNLP with Maven so I can use it easily in my project. Ideas?</p>\n",
    "score": 6,
    "creation_date": 1319334224,
    "view_count": 983,
    "answer_count": 1,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 7424832,
    "title": "Is there a microformat for labeling sentences, words, parts-of-speech, etc",
    "body": "<p>Is there a microformat for basic natural language process that has tags for sentences, words, parts-of-speech, etc...? I have searched the web but could not find any.</p>\n",
    "score": 6,
    "creation_date": 1316049166,
    "view_count": 289,
    "answer_count": 1,
    "tags": "nlp;microformats;part-of-speech"
  },
  {
    "question_id": 7085287,
    "title": "How can I convert CLAWS7 tags to Penn tags?",
    "body": "<p>Does anyone of you know a way to convert a tag from CLAWS7 tagset to it's equivalent in Penn tagset?</p>\n\n<p>CLAWS7 tagset: <a href=\"http://ucrel.lancs.ac.uk/claws7tags.html\">http://ucrel.lancs.ac.uk/claws7tags.html</a></p>\n\n<p>Penn tagset: <a href=\"http://www.mozart-oz.org/mogul/doc/lager/brill-tagger/penn.html\">http://www.mozart-oz.org/mogul/doc/lager/brill-tagger/penn.html</a></p>\n",
    "score": 6,
    "creation_date": 1313530273,
    "view_count": 335,
    "answer_count": 1,
    "tags": "nlp;pos-tagger;part-of-speech"
  },
  {
    "question_id": 5873601,
    "title": "Multilingual spell checking with language detection",
    "body": "<p>I'm working on spell checking of mixed language webpages, and haven't been able to find any existing research on the subject.</p>\n\n<p>The aim is to automatically detect language <em>at a sentence level</em> within mixed language webpages and spell check each against their appropriate language automatically. Assume that we can ignore sentences which mix multiple languages together (e.g. \"He has a certain je ne sais quoi\"), and assume webpages can't contain more than 2 or 3 languages. </p>\n\n<p>Trivial example (Welsh + English): <a href=\"http://wales.gov.uk/\">http://wales.gov.uk/</a></p>\n\n<p>I'm currently using a mix of:</p>\n\n<ul>\n<li>Character distribution (e.g. 0600-06FF = Arabic etc)</li>\n<li>n-Grams to discern languages with similar characters</li>\n<li>Dictionary lookup to discern locale, i.e. en-US, en-GB </li>\n</ul>\n\n<p>I have working code but am concerned it may be naive or needlessly re-inventing a wheel. Has anyone else done this before?</p>\n",
    "score": 6,
    "creation_date": 1304445263,
    "view_count": 2236,
    "answer_count": 2,
    "tags": "language-agnostic;nlp;multilingual;spell-checking"
  },
  {
    "question_id": 76937361,
    "title": "When to set `add_special_tokens=False` in huggingface transformers tokenizer?",
    "body": "<p>this is the default way of setting <code>tokenizer</code> in the Hugging Face &quot;transformers&quot; library:</p>\n<pre><code>from transformers import BertForSequenceClassification,BertTokenizer\ntokenizer=BertTokenizer.from_pretrained('ProsusAI/finbert')\ntokens=tokenizer.encode_plus(text,add_special_tokens=True, max_length=512, truncation=True, padding=&quot;max_length&quot;)\n</code></pre>\n<p>As far as I understand, setting <code>add_special_tokens=True</code> adds the special tokens like [CLS], [SEP], and padding tokens to the input sequences. This is useful for the model's correct interpretation of the input. However, I've come across code samples where people set it to <code>False</code>.</p>\n<p>I'm wondering in which specific situations should I set add_special_tokens=True and when should I set it to False when using tokenizer.encode_plus()? Are there any scenarios where managing special tokens manually after chunking or other preprocessing steps would be beneficial?</p>\n",
    "score": 6,
    "creation_date": 1692486428,
    "view_count": 1872,
    "answer_count": 1,
    "tags": "python;nlp;tokenize;huggingface-transformers"
  },
  {
    "question_id": 72462408,
    "title": "knn search query using python and elasticsearch",
    "body": "<p>I try to do this query with elasticsearch python client :</p>\n<pre><code>curl -X GET &quot;localhost:9200/articles/_knn_search&quot; -H 'Content-Type: application/json' -d '\n{\n  &quot;knn&quot;: {\n    &quot;field&quot;: &quot;title_vector&quot;,\n    &quot;query_vector&quot;: [-0.01807806, 0.024579186,...],\n    &quot;k&quot;: 10,\n    &quot;num_candidates&quot;: 100\n  },\n  &quot;_source&quot;: [&quot;title&quot;, &quot;category&quot;]\n}\n'\n</code></pre>\n<p>If anyone can help me with thanks.</p>\n<p><strong>EDIT :</strong> with elasticsearch python client &gt; 8.0 there is a new function named knn_search so we can run knn_search very easy:</p>\n<pre><code>query = {\n    &quot;field&quot;: &quot;title_vector&quot;,\n    &quot;query_vector&quot;: [-0.01807806, 0.024579186,...],\n    &quot;k&quot;: 10,\n    &quot;num_candidates&quot;: 100\n}\nes = Elasticsearch(request_timeout=600, hosts='http://localhost:9200')\nres = es.knn_search(index=&quot;index_name&quot;, knn=query, source=[&quot;filed1&quot;, &quot;field2&quot;])\n</code></pre>\n",
    "score": 6,
    "creation_date": 1654088833,
    "view_count": 4108,
    "answer_count": 0,
    "tags": "python;elasticsearch;nlp;word2vec;elasticsearch-dsl"
  },
  {
    "question_id": 70066746,
    "title": "cannot import name &#39;TrainingArguments&#39; from &#39;transformers&#39;",
    "body": "<p>I am trying to fine-tune a pretrained huggingface BERT model. I am importing the following</p>\n<pre><code>from transformers import (AutoTokenizer, AutoConfig,\n                              AutoModelForSequenceClassification, TrainingArguments, Trainer)\n</code></pre>\n<p>and get the following error:</p>\n<pre><code>cannot import name 'TrainingArguments' from 'transformers' \n</code></pre>\n<p>Trainer also cannot import.</p>\n<p>I currently have tensorflow 2.2.0, pytorch 1.7.1, and transformers 2.1.1 installed</p>\n",
    "score": 6,
    "creation_date": 1637588603,
    "view_count": 6084,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 60824589,
    "title": "How can I get RoBERTa word embeddings?",
    "body": "<p>Given a sentence of the type 'Roberta is a heavily optimized version of BERT.', I need to get the embeddings for each of the words in this sentence with RoBERTa. I have tried to look at the sample codes online, failing to find a definite answer. </p>\n\n<p>My take is the following:</p>\n\n<pre><code>tokens = roberta.encode(headline)\nall_layers = roberta.extract_features(tokens, return_all_hiddens=True)\nembedding = all_layers[0]\nn = embedding.size()[1] - 1\nembedding = embedding[:,1:n,:]\n</code></pre>\n\n<p>where <code>embedding[:,1:n,:]</code> is used to extract only the embeddings for the words in the sentence, without the start and end tokens. </p>\n\n<p>Is it correct?</p>\n",
    "score": 6,
    "creation_date": 1585020790,
    "view_count": 5152,
    "answer_count": 2,
    "tags": "encoding;nlp;word-embedding"
  },
  {
    "question_id": 56741334,
    "title": "How to get news feed out of Bloomberg API regarding a particular security(equity) and date range?",
    "body": "<p>I'm working on a project that requires I source news articles from the Bloomberg API regarding a particular security ( say Netflix) within a specific date range. I want to do this in Python and get the news articles in a structure (JSON/XML) format. I believe this can be done using EDTF(Event-Driven Trading Feed) using the Bloomberg Terminal, but I want to do this using the Bloomberg API.</p>\n\n<p>I need these news articles to perform a sentiment-analysis on the articles.</p>\n\n<p>I read the answer to this question: <a href=\"https://stackoverflow.com/questions/49547440/scrape-news-feed-from-bloomberg-terminal\">Scrape News feed from Bloomberg Terminal</a> </p>\n\n<p>I understand I do have access to EDTF feed but don't know how to get the feed out programmatically in Python as there is really little to no documentation around it. If I could use PDBLP (<a href=\"https://matthewgilbert.github.io/pdblp/api.html\" rel=\"noreferrer\">https://matthewgilbert.github.io/pdblp/api.html</a>), it would be even greater!</p>\n\n<p>Please link some documentation, code examples as to how to go about this problem. If you've worked on a similar project on Bloomberg, it would be great if you could share some code examples. Thank you!</p>\n",
    "score": 6,
    "creation_date": 1561397207,
    "view_count": 4200,
    "answer_count": 0,
    "tags": "python;python-3.x;nlp;sentiment-analysis;bloomberg"
  },
  {
    "question_id": 56129165,
    "title": "How to handle labels when using the BERTs&#39; wordpiece tokenizer",
    "body": "<p>I am trying to do multi-class sequence classification using the BERT uncased based model and tensorflow/keras. However, I have an issue when it comes to labeling my data following the BERT wordpiece tokenizer. I am unsure as to how I should modify my labels following the tokenization procedure.</p>\n\n<p>I have read several open and closed issues on Github about this problem and I've also read the BERT paper published by Google. Specifically in section 4.3 of the paper there is an explanation of how to adjust the labels but I'm having trouble translating it to my case. I've also read the official BERT repository README which has a section on tokenization and mentions how to create a type of dictionary that maps the original tokens to the new tokens and that this can be used as a way to project my labels.</p>\n\n<p>I have used the code provided in the README and managed to create labels in the way I think they should be. However, I am not sure if this is the correct way to do it. Below is an example of a tokenized sentence and it's labels before and after using the BERT tokenizer. Just a side-note. I have adjusted some of the code in the tokenizer so that it does not tokenize certain words based on punctuation as I would like them to remain whole.</p>\n\n<p>This is the code to create the mapping:</p>\n\n<pre><code>bert_tokens = []\nlabel_to_token_mapping = []\n\nbert_tokens.append(\"[CLS]\")\n\nfor token in original_tokens:\n   label_to_token_mapping.append(len(bert_tokens))\n   bert_tokens.extend(tokenizer.tokenize(token, ignore_set=ignore_set))\n\nbert_tokens.append(\"[SEP]\")\n\n</code></pre>\n\n<pre><code>original_tokens = ['The', &lt;start&gt;', 'eng-30-01258617-a', '&lt;end&gt;', 'frailty']\ntokens = ['[CLS]', 'the', '&lt;start&gt;', 'eng-30-01258617-a', '&lt;end&gt;', 'frail', '##ty', '[SEP]']\nlabels = [0,2, 3, 4, 1]\nlabel_to_token_mapping = [1, 2, 3, 4, 5]\n</code></pre>\n\n<p>Using the mapping I adjust my label array and it becomes like the following:</p>\n\n<pre><code>labels = [0, 2, 3, 4, 1, 1]\n</code></pre>\n\n<p>Following this I add padding labels (let's say that the maximum sequence length is 10) and so finally my label array looks like this:</p>\n\n<pre><code>labels = [0, 2, 3, 4, 1, 1, 5, 5, 5, 5, 5]\n</code></pre>\n\n<p>As you can see since the last token (labeled 1) was split into two pieces I now label both word pieces as '1'. </p>\n\n<p>I am not sure if this is correct. In section 4.3 of the paper they are labelled as 'X' but I'm not sure if this is what I should also do in my case. So in the paper (<a href=\"https://arxiv.org/abs/1810.04805\" rel=\"noreferrer\">https://arxiv.org/abs/1810.04805</a>) the following example is given: </p>\n\n<pre><code>Jim    Hen    ##son  was  a puppet  ##eer\nI-PER  I-PER    X     O   O   O       X\n</code></pre>\n\n<p>My final goal is to input a sentence into the model and as a result get back an array which can look something like [0, 0, 1, 1, 2, 3, 4, 5, 5, 5]. So one label per word piece. Then I can reconstruct the words back together to get the original length of the sentence and therefore the way the prediction values should actually look like.</p>\n\n<p>Also, after training the model for a couple of epochs I attempt to make predictions and get weird values. For example a word is marked with the label '5' for padding and padding values get marked with the label '1'. This makes me think that there is something wrong with the way I create labels. Initially I did not adjust the labels so I would leave the labels as they were originally even after tokenizing the original sentence. This did not give me good results.</p>\n\n<p>Any help would be greatly appreciated as I've been trying hard to find what I should do online but I haven't been able to figure it out yet. Thank you in advance!</p>\n\n<p>Also, the following is the code I use to create my model:</p>\n\n<pre><code>from tensorflow.python.keras.layers import Input, Dense\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras import backend as K\nimport tensorflow_hub as hub\nimport tensorflow as tf\n\n\nclass BertLayer(tf.layers.Layer):\n    def __init__(self, bert_path, max_seq_length, n_fine_tune_layers=10, **kwargs):\n        self.n_fine_tune_layers = n_fine_tune_layers\n        self.trainable = True\n        self.output_size = 768\n        self.bert_path = bert_path\n        self.max_seq_length = max_seq_length\n\n        super(BertLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.bert = hub.Module(\n            self.bert_path,\n            trainable=self.trainable,\n            name=\"{}_module\".format(self.name)\n        )\n        trainable_vars = self.bert.variables\n\n        # Remove unused layers\n        # trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n        trainable_vars = [var for var in trainable_vars\n                          if not (\"/cls/\" in var.name) and not (\"/pooler/\" in var.name)]\n\n        # Select how many layers to fine tune\n        trainable_vars = trainable_vars[-self.n_fine_tune_layers:]\n\n        # Add to trainable weights\n        for var in trainable_vars:\n            self._trainable_weights.append(var)\n\n        for var in self.bert.variables:\n            if var not in self._trainable_weights:\n                self._non_trainable_weights.append(var)\n\n        super(BertLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n        input_ids, input_mask, segment_ids = inputs\n        bert_inputs = dict(\n            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n        )\n        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n            \"sequence_output\"\n        ]\n        return result\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], self.output_size\n\n    def get_config(self):\n        config = {'bert_path': self.bert_path, 'max_seq_length': self.max_seq_length}\n        base_config = super(BertLayer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\n# Build model\ndef build_model(bert_path, max_seq_length):\n    in_id = Input(shape=(None,), name=\"input_ids\")\n    in_mask = Input(shape=(None,), name=\"input_masks\")\n    in_segment = Input(shape=(None,), name=\"segment_ids\")\n    bert_inputs = [in_id, in_mask, in_segment]\n\n    bert_output = BertLayer(bert_path=bert_path, n_fine_tune_layers=3, max_seq_length=max_seq_length)(bert_inputs)\n    dense = Dense(128, activation='relu')(bert_output)\n    pred = Dense(8, activation='softmax',)(dense)\n\n    model = Model(inputs=bert_inputs, outputs=pred)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n    model.summary()\n\n    return model\n\n\ndef initialize_vars(sess):\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    K.set_session(sess)\n</code></pre>\n",
    "score": 6,
    "creation_date": 1557832610,
    "view_count": 2503,
    "answer_count": 0,
    "tags": "python;tensorflow;machine-learning;keras;nlp"
  },
  {
    "question_id": 55367637,
    "title": "How to combine the results of multiple OCR tools to get better text recognition",
    "body": "<p>Imagine, you have different OCR tools to read text from images but none of them gives you a 100% accurate output. Combined however, the result could come very close to the ground truth - What would be the best technique to \"fuse\" the text together to get good results?</p>\n\n<p>Example:</p>\n\n<p>Actual text</p>\n\n<pre><code>§ 5.1: The contractor is obliged to announce the delay by 01.01.2019 at the latest. The identification-number to be used is OZ-771LS.\n</code></pre>\n\n<p>OCR tool 1</p>\n\n<pre><code>5 5.1 The contractor is obliged to announce the delay by O1.O1.2019 at the latest. The identification-number to be used is OZ77lLS.\n</code></pre>\n\n<p>OCR tool 2</p>\n\n<pre><code>§5.1: The contract or is obliged to announce theedelay by 01.O1. 2O19 at the latest. The identification number to be used is O7-771LS\n</code></pre>\n\n<p>OCR tool 3</p>\n\n<pre><code>§ 5.1: The contractor is oblige to do announced he delay by 01.01.2019 at the latest. T he identification-number ti be used is OZ-771LS.\n</code></pre>\n\n<p>What could be a promising algorithm to fuse OCR 1, 2 and 3 to get the actual text?</p>\n\n<p>My first idea was creating a \"tumbling window\" of an arbitrary length, compare the words in the window and take the words 2 out of 3 tools predict for every position. </p>\n\n<p>For example with window size 3:</p>\n\n<pre><code>[5 5.1 The] \n</code></pre>\n\n<pre><code>[§5.1: The contract] \n</code></pre>\n\n<pre><code>[§ 5.1: The] \n</code></pre>\n\n<p>As you see, the algorithm doesn't work as all three tools have different candidates for position one (5, §5.1: and §).</p>\n\n<p>Of course it would be possible to add some tricks like Levenshtein distance to allow some deviations but I fear this will not really be robust enough.</p>\n",
    "score": 6,
    "creation_date": 1553642911,
    "view_count": 947,
    "answer_count": 1,
    "tags": "nlp;computer-vision;ocr;sensor-fusion"
  },
  {
    "question_id": 55243829,
    "title": "How to convert Sentence to Question using spacy library in python [Refer my code Below for Correction]",
    "body": "<p>I need to convert any sentence to question using spacy in python. </p>\n\n<p>My Code below goes so long and i need even more to be done to complete any sentence into question format. Now in this code I make condition based on <em>be form, need form, have form, do form</em> by checking <em>past tense and present tense</em>.</p>\n\n<p>INPUT:   Nina plays the violin. </p>\n\n<p>OUTPUT:  Does Nina play the violin? </p>\n\n<p>INPUT:   Barbara gave me the chocolates.</p>\n\n<p>OUTPUT:  Who gave you the chocolates?</p>\n\n<p>INPUT:   He is seeing Joe tomorrow.</p>\n\n<p>OUTPUT:  Who is he seeing tomorrow?</p>\n\n<p>INPUT:   She comes from Madrid.</p>\n\n<p>OUTPUT:  Where does she come from?</p>\n\n<p><strong>Anybody can help me in this! Want to form question for all type of sentence?</strong></p>\n\n<pre><code> from textacy.spacier import utils\n    import spacy\n    nlp = spacy.load(\"en_core_web_sm\")\n    inp = input()                       \n    doc = nlp(inp)                      \n    string,label = [],\"\"\n\n    for sentence in doc.sents:\n        root = sentence.root\n        for i in sentence.ents:\n            if len(utils.get_subjects_of_verb(root)) or len(utils.get_objects_of_verb(root)) &gt; 0:\n                label = i.label_\n        print(root.tag_)\n        print(root.lemma_)\n        print(label)\n        if len(utils.get_subjects_of_verb(root)) &gt; 0:\n            if root.lemma_ == 'be':\n                if label == \"PERSON\" :\n                    ques = 'Who ' + str(root)+\" \"+ str(utils.get_subjects_of_verb(root)[0]) +' ?'\n                elif label == \"QUANTITY\":\n                    ques = 'How ' + str(root)+\" \"+ str(utils.get_subjects_of_verb(root)[0]) +' ?'\n                elif label == \"MONEY\":\n                    ques = 'How much ' + str(root) + \" \" + str(utils.get_subjects_of_verb(root)[0]) + ' ?'\n                elif label == \"TIME\":\n                    ques = 'When ' + str(root)+\" \"+ str(utils.get_subjects_of_verb(root)[0]) +' ?'\n                elif label == \"GPE\":\n                    ques = 'Where ' + str(root)+\" \"+ str(utils.get_subjects_of_verb(root)[0]) +' ?'\n                elif label == 'PRODUCT':\n                    ques = 'What ' + str(root)+\" \"+ str(utils.get_subjects_of_verb(root)[0]) +' ?'\n\n            string.append(ques)\n\n    print(string)\n</code></pre>\n\n<p><strong>OR IN ANOTHER WAY</strong></p>\n\n<p><em>Works for these type of formats:</em></p>\n\n<p>This is for John - Who is this for?</p>\n\n<p>He was watching a film - What was he watching?</p>\n\n<p>Sam will be back on Friday - When will Sam be back?</p>\n\n<p>They were walking fast - How were they walking?</p>\n\n<p>She left because it was late - Why did she leave?</p>\n\n<pre><code>from textacy.spacier import utils\nfrom textacy.spacier import utils as spacy_utils\n import spacy\n import re\n nlp = spacy.load(\"en_core_web_sm\")\n-inp = input()\n-doc = nlp(inp)\n-stop,modal_root,form_root,root = \"\",\"\",\"\",\"\"\n-\n-\n-for sentence in doc:\n-    if sentence.dep_ in ['aux','ROOT']:\n-        if sentence.lemma_ in ['must', 'shall', 'will', 'should', 'would', 'can', 'could', 'may','might']:\n-            modal_root = sentence\n-        elif sentence.lemma_ in ['be','have']:\n-            form_root = sentence\n-        else:\n-            root = sentence\n-        for children in sentence.subtree:\n-            if children.text in ['because', 'so because']:\n-                check = children\n-            if children.dep_ in ['dobj','pobj','advmod','acomp']:\n-                child = children\n-                for prep in child.subtree:\n-                    if prep.is_stop:\n-                        stop = prep\n-                if child.ent_type_:\n-                    label = child.ent_type_\n-                elif child.pos_ == \"ADV\":\n-                    label = \"QUANTITY\"\n-                else:\n-                    label = \"\"\n-\n-if modal_root and form_root:\n-    root = modal_root\n-elif modal_root:\n-    root = modal_root\n-elif form_root:\n-    root = form_root\n-\n-\n-for lemma in doc:\n-    if lemma.text in ['we','I']:\n-        lem = lemma.text\n-    else:\n-        lem = \"\"\n-\n-if stop:\n-    sent = doc.text.replace(str(child),\"\")\n-    sent = sent.replace(\" \"+str(stop)+\" \", \"\")\n-else:\n-    sent = doc.text.replace(str(child), \"\")\n-\n-if lem:  sent = sent.replace(lem, \"you\")\n-\n-if root.lemma_ in ['be','have','must', 'shall', 'will', 'should', 'would', 'can', 'could', 'may', 'might']:\n-    if label == 'PERSON':\n-        ques = 'who '+str(root) + \" \" + re.sub('{}'.format(\" \"+str(root)+\" \").lower(), ' ', sent)+'?'\n-    elif label in ['GPE','LOC']:\n-        ques = 'where '+str(root) + \" \" + re.sub('{}'.format(\" \"+str(root)+\" \").lower(), ' ', sent)+'?'\n-    elif label in ['TIME','DATE']:\n-        ques = 'when ' + str(root) + \" \" + re.sub('{}'.format(\" \"+str(root)+\" \").lower(), ' ', sent)+'?'\n-    elif label in ['QUANTITY']:\n-        ques = 'How ' + str(root) + \" \" + re.sub('{}'.format(\" \"+str(root)+\" \").lower(), ' ', sent)+'?'\n-    else:\n-        ques = 'what ' + str(root) + \" \" + re.sub('{}'.format(\" \" + str(root) + \" \").lower(), ' ', sent) + '?'\n-else:\n-    if root.tag_ == 'VBD' or str(utils.get_subjects_of_verb(root)[0]).upper() in ['I', 'You', 'We', 'They', 'He', 'She','It']:\n-        if check.text in ['because','so because']:\n-            ques = 'Why did ' + str(utils.get_subjects_of_verb(root)[0]) + \" \" + root.lemma_ + '?'\n-        else:\n-            ques = 'Did ' + str(utils.get_subjects_of_verb(root)[0]) + \" \" + root.lemma_ + '?'\n-    elif str(utils.get_subjects_of_verb(root)[0]).upper() in ['I', 'You', 'We', 'They']:\n-        ques = 'Do ' + str(utils.get_subjects_of_verb(root)[0]) + \" \" + root.lemma_ + '?'\n-    elif str(utils.get_subjects_of_verb(root)[0]).upper() in ['He', 'She', 'It'] or label == \"PERSON\":\n-        ques = 'Does ' + str(utils.get_subjects_of_verb(root)[0]) + \" \" + root.lemma_ + '?'\n-\n-print(ques)\n-\n-\n</code></pre>\n\n<p><strong>HOW TO ACHIEVE CONVERTING SENTENCE INTO QUESTION USING SPACY LIBRARY IN PYTHON?</strong></p>\n",
    "score": 6,
    "creation_date": 1553007120,
    "view_count": 2686,
    "answer_count": 0,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 50583937,
    "title": "NLP: Pre-processing in doc2vec / word2vec",
    "body": "<p>A few papers on the topics of word and document embeddings (word2vec, doc2vec) mention that they used the Stanford CoreNLP framework to tokenize/lemmatize/POS-tag the input words/sentences:</p>\n\n<blockquote>\n  <p>The  corpora  were  lemmatized and POS-tagged with the Stanford CoreNLP (Manning  et  al.,  2014)  and  each  token  was  replaced with its lemma and POS tag</p>\n</blockquote>\n\n<p>(<a href=\"http://www.ep.liu.se/ecp/131/039/ecp17131039.pdf\" rel=\"noreferrer\">http://www.ep.liu.se/ecp/131/039/ecp17131039.pdf</a>)</p>\n\n<blockquote>\n  <p>For pre-processing, we tokenise and lowercase the words using Stanford CoreNLP</p>\n</blockquote>\n\n<p>(<a href=\"https://arxiv.org/pdf/1607.05368.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1607.05368.pdf</a>)</p>\n\n<p>So my questions are:</p>\n\n<ul>\n<li><p>Why does the first paper apply POS-tagging? Would each token then be replaced with something like <code>{lemma}_{POS}</code> and the whole thing used to train the model? Or are the tags used to filter tokens? \nFor example, gensims WikiCorpus applies lemmatization per default and then only keeps a few types of part of speech (verbs, nouns, etc.) and gets rid of the rest. So what is the recommended way?</p></li>\n<li><p>The quote from the second paper seems to me like they only split up words and then lowercase them. This is also what I first tried before I used WikiCorpus. In my opinion, this should give better results for document embeddings as most of POS types contribute to the meaning of a sentence. Am I right?</p></li>\n</ul>\n\n<p>In the original doc2vec paper I did not find details about their pre-processing.</p>\n",
    "score": 6,
    "creation_date": 1527595380,
    "view_count": 999,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;word2vec;gensim;doc2vec"
  },
  {
    "question_id": 47175888,
    "title": "How to add attention layer to seq2seq model on Keras",
    "body": "<p>Based on <a href=\"https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\" rel=\"noreferrer\">this</a> article, I wrote this model:</p>\n\n<pre><code>enc_in=Input(shape=(None,in_alphabet_len))\nlstm=LSTM(lstm_dim,return_sequences=True,return_state=True,use_bias=False)\nenc_out,h,c=lstm(enc_in)\ndec_in=Input(shape=(None,in_alphabet_len))\ndecoder,_,_=LSTM(decoder_dim,return_sequences=True,return_state=True)(dec_in,initial_state=[h,c])\ndecoder=Dense(units=in_alphabet_len,activation='softmax')(decoder)\nmodel=Model([enc_in,dec_in],decoder) \n</code></pre>\n\n<p>How can I add attention layer to this model before decoder?</p>\n",
    "score": 6,
    "creation_date": 1510133122,
    "view_count": 1492,
    "answer_count": 1,
    "tags": "nlp;deep-learning;keras;lstm;attention-model"
  },
  {
    "question_id": 42245050,
    "title": "NLP bag-of-words/TF-IDF for clustering (and classifying) short sentences",
    "body": "<p>I want to cluster Javascript objects by one of their string key values (<code>description</code>). I already tried multiple solutions and would like some guidance on how to approach the problem.</p>\n\n<p>What I want:\nLet's say I have a database of objects. There can be a lot of them (thousands probably, maybe tens of thousands). I need to be able to:</p>\n\n<ol>\n<li>Cluster objects by similarity in logical (kinda) groups. Semantic matching would be awesome but for now just string similarity would be enough. After they are clustered I need to assign some <code>categoryId</code> to each of them (representing the cluster they belong to).</li>\n<li>Whenever new objects are added to database I need to classify them to the existing groups/propose new clusters.</li>\n</ol>\n\n<p>I haven't tried solving problem #2 yet, but here is what I have tried with #1.</p>\n\n<ul>\n<li><p><strong>hierarchical clustering with Levenshtein distance (single linkage)</strong> - the problem here was the performance, the results were satisfactory (I used <code>hierarchical-clustering</code> library from <code>npm</code>) but at around 150 I would have to wait around a minute. Not going to work for thousands.</p></li>\n<li><p><strong>TF-IDF, vectorizing + k-means</strong> - the performance was great. It would go through 5000 objects with ease. But the results were definitively off (might be a bug in my implementation). I used (<code>natural</code> library from <code>npm</code> to calculate TF-IDFs and <code>node-kmeans</code>).</p></li>\n<li><p><strong>Bag-of-Words + k-means</strong> - I am trying to implement this one right now, haven't got any luck yet.</p></li>\n</ul>\n\n<p>For the #2 I thought of using Naive Bayes (but I haven't given it a try yet).</p>\n\n<p>Any suggestions? It would be fine if the objects would be just clustered. It would be even better if I could extract tags (like from TF-IDF) by which the group was clustered.</p>\n",
    "score": 6,
    "creation_date": 1487150011,
    "view_count": 1960,
    "answer_count": 1,
    "tags": "algorithm;nlp;k-means;hierarchical-clustering;tensorflow.js"
  },
  {
    "question_id": 31969235,
    "title": "can&#39;t use punkt tokenizer with pyspark",
    "body": "<p>I am trying to use the punkt tokenizer from the NLTK package with pyspark on a Spark standalone cluster.  NLTK has been installed on the individual nodes, but the nltk_data folder doesn't reside where NLTK expects it (/usr/share/nltk_data).</p>\n\n<p>I am attempting to use the punkt tokenizer, which is located in (whatever/my_user/nltk_data).</p>\n\n<p>I have set:</p>\n\n<pre><code>envv1   = \"/whatever/my_user/nltk_data\"\nos.environ['NLTK_DATA'] = envv1   \n</code></pre>\n\n<p>Printing nltk.data.path indicates that the first entry is where my nltk_data folder is actually located.  </p>\n\n<p>The <code>from nltk import word_tokenize</code> goes fine, but when it comes to calling the function <code>word_tokenize()</code> I get the following error:</p>\n\n<pre><code>ImportError: No module named nltk.tokenize\n</code></pre>\n\n<p>For whatever reason, I have no trouble accessing resources from nltk.corpus.  When I try nltk.download(), it is clear I already have the punkt tokenizer downloaded.  I can even use the punkt tokenizer outside of pyspark.</p>\n",
    "score": 6,
    "creation_date": 1439392498,
    "view_count": 1800,
    "answer_count": 0,
    "tags": "python;apache-spark;nlp;nltk;pyspark"
  },
  {
    "question_id": 30285706,
    "title": "Detecting similar paragraphs in two documents",
    "body": "<p>I am trying to find similar paragraphs in 2 documents. Each document has many paragraphs of multiple lines of text. The text in paragraphs has some changes. The words can be inserted or deleted or miss-spelled. For example</p>\n\n<p>Doc1.Para</p>\n\n<p>This is one line of text</p>\n\n<p>Doc2.Para</p>\n\n<p>This is one lin text</p>\n\n<p>You can see here that some words are missing('of') and some are spelled differently. Hence the paras are <b>not exactly same but similar</b>. And the similarity is not based on the semantics or essence. Its just based on the words.</p>\n\n<p>The paragraphs are not in same order. For example</p>\n\n<p>Doc 1<br><br>Para 1<br>Para 2<br>Para 3<br>Para 4</p>\n\n<p>Doc 2<br><br>Para 3<br>Para 4<br>Para 1.1<br>Para 2<br>Para 1.2</p>\n\n<p>Here you can see order is not same. Also paras can be <b>splited</b> like Doc1.Para1 is splited into 2 paras Doc2.Para1.1 + Doc2.Para1.2.</p>\n\n<p>I have to detect which para in Doc1 is similar to which para in Doc2. Looking for some open source tool or some algorithm.</p>\n",
    "score": 6,
    "creation_date": 1431857239,
    "view_count": 8092,
    "answer_count": 6,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 22094303,
    "title": "How do I add a new dictionary database to cTAKES",
    "body": "<p>How do I add a new database to the cTAKES pipeline to perform lookup from? How do I specify what columns to look up and how to annotate the text with the returned hits? I have gone through the DictionaryLookupAnnotatorDB.xml and LookupDesc_Db.xml files. However, I could not understand the meanings of the terms like \"lookupField\", \"metaField\", \"maxPermutationLevel\" and \"exclusionTags\". If I add a new database, I need to configure this xml file properly. Please guide me regarding these problems.</p>\n",
    "score": 6,
    "creation_date": 1393587971,
    "view_count": 1054,
    "answer_count": 0,
    "tags": "database;nlp;ctakes"
  },
  {
    "question_id": 17891932,
    "title": "Open-source rule-based pattern matching / information extraction frameworks?",
    "body": "<p>I'm shopping for an open-source framework for writing natural language grammar rules for pattern matching over annotations. You could think of it like regexps but matching at the token rather than character level. Such a framework should enable the match criteria to reference other attributes attached to the input tokens or spans, as well as modify such attributes in an action. </p>\n\n<p>There are three options I know of which fit this description:</p>\n\n<ul>\n<li><a href=\"http://gate.ac.uk/sale/tao/splitch8.html#chap%3ajape\" rel=\"nofollow\">GATE Java Expressions over Annotations (JAPE)</a></li>\n<li><a href=\"http://nlp.stanford.edu/software/tokensregex.shtml#Mail\" rel=\"nofollow\">Stanford CoreNLP's TokensRegex</a></li>\n<li><a href=\"http://uima.apache.org/\" rel=\"nofollow\">UIMA</a> <a href=\"http://uima.apache.org/ruta.html\" rel=\"nofollow\">Ruta</a> (<a href=\"http://uima.apache.org/gscl13.html#gscl.tutorial\" rel=\"nofollow\">Tutorial</a>)</li>\n<li><a href=\"http://code.google.com/p/graph-expression/\" rel=\"nofollow\">Graph Expression (GExp)</a>*</li>\n</ul>\n\n<p><strong>Are there any other options like these available at this time?</strong></p>\n\n<p><em>Related Tools</em></p>\n\n<ul>\n<li>While I know that general parser generators like <a href=\"http://www.antlr.org/\" rel=\"nofollow\">Antlr</a> can also serve this purpose, I'm looking for something which are more specifically tailored for natural language processing or information extraction. </li>\n<li><a href=\"http://uima.apache.org/\" rel=\"nofollow\">UIMA</a> includes a <a href=\"http://uima.apache.org/d/uima-addons-current/RegularExpressionAnnotator/RegexAnnotatorUserGuide.html\" rel=\"nofollow\">Regex Annotator</a> plugin for declaring rules in XML, but appears to operate at the character rather than high-level objects.</li>\n<li>I know that this kind of task is often performed with statistical models, but for narrow, structured domains there's benefit in hand-crafting rules.</li>\n</ul>\n\n<p>* With GExp 'rules' are actually implemented in code but since there are so few options I chose to include it.</p>\n",
    "score": 6,
    "creation_date": 1374877209,
    "view_count": 2052,
    "answer_count": 2,
    "tags": "text;open-source;nlp;named;information-extraction"
  },
  {
    "question_id": 16775694,
    "title": "Multi-column layout handling with pdfminer pdf2txt.py module",
    "body": "<p>So far I am using <a href=\"http://www.unixuser.org/~euske/python/pdfminer/index.html#pdf2txt\" rel=\"noreferrer\">pdfminer pdf2txt.py</a> module with success.   </p>\n\n<p>But a problem arises in pdf files formatted in two columns. The module retrieves text into a single column which results into many split words, at the end of lines. example:</p>\n\n<blockquote>\n  <p>and functional properties of cellu-<br>\n  lar components negatively, both physically and chemically.</p>\n</blockquote>\n\n<p>*Note that the words are separated by the '-' character.</p>\n\n<p>What I want is to customize the command in order for the words, in the end of the line, to appear as a whole and therefore do not lose information.\nProbably by adding a line parameter or a character margin, specific for '-' character to be replaced by a backslash?</p>\n\n<p>I would also like to know if there is way to loop the command and make it parse a directory full of pdf files, each time generating a different output text file named after the original?</p>\n\n<p>I am not sure how to do it though.</p>\n",
    "score": 6,
    "creation_date": 1369666324,
    "view_count": 1198,
    "answer_count": 0,
    "tags": "python;pdf;text;nlp"
  },
  {
    "question_id": 13274783,
    "title": "How can I generate parse trees of English sentences on iOS?",
    "body": "<p>I would like to generate constituency-based parsed trees of English sentences within an iOS application.\n<a href=\"http://en.wikipedia.org/wiki/Parse_tree\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Parse_tree</a></p>\n\n<p>My current options appear to be:</p>\n\n<ul>\n<li>Write my own tree generation on top of POS tagging from NSLinguisticTagger.</li>\n<li>Embed the python-based NLTK into my app.</li>\n<li>Create or use a server based approach.</li>\n</ul>\n\n<p>Could anyone recommend one of these approaches, or perhaps suggest another?</p>\n",
    "score": 6,
    "creation_date": 1352308593,
    "view_count": 798,
    "answer_count": 1,
    "tags": "ios;nlp;linguistics"
  },
  {
    "question_id": 13986518,
    "title": "How to efficiently compute similarity between documents in a stream of documents",
    "body": "<p>I gather Text documents (in Node.js) where one document <code>i</code> is represented as a list of words.\nWhat is an efficient way to compute the similarity between these documents, taking into account that new documents are coming as a sort of stream of documents?</p>\n<p>I currently use cos-similarity on the Normalized Frequency of the words within each document. I don't use the TF-IDF (Term frequency, Inverse document frequency) because of the scalability issue since I get more and more documents.</p>\n<h2>Initially</h2>\n<p>My first version was to start with the currently available documents, compute a big Term-Document matrix <code>A</code>, and then compute <code>S = A^T x A</code> so that <code>S(i, j)</code> is (after normalization by both <code>norm(doc(i))</code> and <code>norm(doc(j))</code>) the cos-similarity between documents <code>i</code> and <code>j</code> whose word frequencies are respectively <code>doc(i)</code> and <code>doc(j)</code>.</p>\n<h2>For new documents</h2>\n<p>What do I do when I get a new document <code>doc(k)</code>? Well, I have to compute the similarity of this document with all the previous ones, which doesn't require to build a whole matrix. I can just take the inner-product of <code>doc(k) dot doc(j)</code> for all previous <code>j</code>, and that result in <code>S(k, j)</code>, which is great.</p>\n<h2>The troubles</h2>\n<ol>\n<li><p>Computing <code>S</code> in Node.js is really long. Way too long in fact! So I decided to create a C++ module which would do the whole thing much faster. And it does! But I cannot wait for it, I should be able to use intermediate results. And what I mean by &quot;not wait for it&quot; is both</p>\n<p>a. wait for the computation to be done, but also<br />\nb. wait for the matrix <code>A</code> to be built (it's a big one).</p>\n</li>\n<li><p>Computing new <code>S(k, j)</code> can take advantage of the fact that documents have way less words than the set of all the given words (which I use to build the whole matrix <code>A</code>). Thus, it looks faster to do it in Node.js, avoiding a lot of extra-resource to be taken to access the data.</p>\n</li>\n</ol>\n<p>But is there any better way to do that?</p>\n<p><strong>Note</strong> : the reason I started computing <code>S</code> is that I can easily build <code>A</code> in Node.js where I have access to all the data, and then do the matrix multiplication in C++ and get it back in Node.js, which speeds the whole thing a lot. But now that computing <code>S</code> gets impracticable, it looks useless.</p>\n<p><strong>Note 2</strong> : yep, I don't have to compute the whole <code>S</code>, I can just compute the upper-right elements (or the lower-left ones), but that's not the issue. The time computation issue is not of that order.</p>\n",
    "score": 6,
    "creation_date": 1356077873,
    "view_count": 1552,
    "answer_count": 1,
    "tags": "node.js;stream;nlp;cosine-similarity;term-document-matrix"
  },
  {
    "question_id": 22206095,
    "title": "Error in creating the StanfordCoreNLP object",
    "body": "<p>I have downloaded and installed required jar files from <a href=\"http://nlp.stanford.edu/software/corenlp.shtml#Download\" rel=\"noreferrer\">http://nlp.stanford.edu/software/corenlp.shtml#Download</a>.</p>\n\n<p>I have include the five jar files</p>\n\n<p>Satnford-postagger.jar</p>\n\n<p>Stanford-psotagger-3.3.1.jar</p>\n\n<p>Stanford-psotagger-3.3.1.jar-javadoc.jar</p>\n\n<p>Stanford-psotagger-3.3.1.jar-src.jar</p>\n\n<p>stanford-corenlp-3.3.1.jar</p>\n\n<p>and the code is </p>\n\n<pre><code>public class lemmafirst {\n\n    protected StanfordCoreNLP pipeline;\n\n    public lemmafirst() {\n        // Create StanfordCoreNLP object properties, with POS tagging\n        // (required for lemmatization), and lemmatization\n        Properties props;\n        props = new Properties();\n        props.put(\"annotators\", \"tokenize, ssplit, pos, lemma\");\n\n        /*\n         * This is a pipeline that takes in a string and returns various analyzed linguistic forms. \n         * The String is tokenized via a tokenizer (such as PTBTokenizerAnnotator), \n         * and then other sequence model style annotation can be used to add things like lemmas, \n         * POS tags, and named entities. These are returned as a list of CoreLabels. \n         * Other analysis components build and store parse trees, dependency graphs, etc. \n         * \n         * This class is designed to apply multiple Annotators to an Annotation. \n         * The idea is that you first build up the pipeline by adding Annotators, \n         * and then you take the objects you wish to annotate and pass them in and \n         * get in return a fully annotated object.\n         * \n         *  StanfordCoreNLP loads a lot of models, so you probably\n         *  only want to do this once per execution\n         */\n        ***this.pipeline = new StanfordCoreNLP(props);***\n}\n</code></pre>\n\n<p>My Problem is in creating a the pipline.</p>\n\n<p>The ERROR that i got is:</p>\n\n<pre><code>Exception in thread \"main\" java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:563)\n    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:262)\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:129)\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:125)\n    at lemmafirst.&lt;init&gt;(lemmafirst.java:39)\n    at lemmafirst.main(lemmafirst.java:83)\nCaused by: edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model\n    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:758)\n    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:289)\n    at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:253)\n    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:88)\n    at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:76)\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:561)\n    ... 6 more\nCaused by: java.io.IOException: Unable to resolve \"edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger\" as either class path, filename or URL\n    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:434)\n    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:753)\n    ... 11 more\n</code></pre>\n\n<p>Can anyone please correct the errors? Thank you</p>\n",
    "score": 5,
    "creation_date": 1394043946,
    "view_count": 14593,
    "answer_count": 3,
    "tags": "java;maven;jar;nlp;stanford-nlp"
  },
  {
    "question_id": 58057021,
    "title": "OSError: [E050] Can&#39;t find model &#39;en&#39;",
    "body": "<p>I am trying to use this pytextrank library of python- <a href=\"https://github.com/DerwenAI/pytextrank/blob/master/example.ipynb\" rel=\"nofollow noreferrer\">https://github.com/DerwenAI/pytextrank/blob/master/example.ipynb</a>\n but i am unable to resolve this error , earlier i was getting an error that ip.json can't be found, but then was resolved </p>\n\n<pre><code>    import pytextrank\n    import sys\n    path_stage0=\"data/ip.json\" \n    path_stage1=\"o1.json\"\n\n    with open(path_stage1,'w') as f:\n        for graf in pytextrank.parse_doc(pytextrank.json_iter(path_stage0)):\n            f.write(\"%s\\n\" % pytextrank.pretty_print(graf._asdict()))\n            print(pytextrank.pretty_print(graf))\n\n\n    OSError                                   Traceback (most recent call last)\n    &lt;ipython-input-12-a20b437ea0f1&gt; in &lt;module&gt;\n          6 \n          7 with open(path_stage1,'w') as f:\n    ----&gt; 8     for graf in pytextrank.parse_doc(pytextrank.json_iter(path_stage0)):\n          9         f.write(\"%s\\n\" % pytextrank.pretty_print(graf._asdict()))\n         10         print(pytextrank.pretty_print(graf))\n\n~\\Anaconda3\\lib\\site-packages\\pytextrank\\pytextrank.py in parse_doc(json_iter)\n    259                 print(\"graf_text:\", graf_text)\n    260 \n--&gt; 261             grafs, new_base_idx = parse_graf(meta[\"id\"], graf_text, base_idx)\n    262             base_idx = new_base_idx\n    263 \n\n~\\Anaconda3\\lib\\site-packages\\pytextrank\\pytextrank.py in parse_graf(doc_id, graf_text, base_idx, spacy_nlp)\n    185     if not spacy_nlp:\n    186         if not SPACY_NLP:\n--&gt; 187             SPACY_NLP = spacy.load(\"en\")\n    188 \n    189         spacy_nlp = SPACY_NLP\n\n~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py in load(name, **overrides)\n     25     if depr_path not in (True, False, None):\n     26         deprecation_warning(Warnings.W001.format(path=depr_path))\n---&gt; 27     return util.load_model(name, **overrides)\n     28 \n     29 \n\n~\\Anaconda3\\lib\\site-packages\\spacy\\util.py in load_model(name, **overrides)\n    137     elif hasattr(name, \"exists\"):  # Path or Path-like to model data\n    138         return load_model_from_path(name, **overrides)\n--&gt; 139     raise IOError(Errors.E050.format(name=name))\n    140 \n    141 \n\nOSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n</code></pre>\n",
    "score": 5,
    "creation_date": 1569221469,
    "view_count": 18329,
    "answer_count": 7,
    "tags": "python;nlp;spacy;pytextrank"
  },
  {
    "question_id": 39323325,
    "title": "Can I find subject from Spacy Dependency tree using NLTK in python?",
    "body": "<p>I want to find the <strong>subject</strong> from a sentence using <code>Spacy</code>. The code below is working fine and giving a <strong>dependency tree</strong>.</p>\n\n<pre><code>import spacy\nfrom nltk import Tree\n\nen_nlp = spacy.load('en')\n\ndoc = en_nlp(\"The quick brown fox jumps over the lazy dog.\")\n\ndef to_nltk_tree(node):\n    if node.n_lefts + node.n_rights &gt; 0:\n        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n    else:\n        return node.orth_\n\n\n[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n</code></pre>\n\n<p><a href=\"https://i.sstatic.net/OhAFy.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/OhAFy.png\" alt=\"enter image description here\"></a></p>\n\n<p>From this dependency tree code, Can I find the <strong>subject</strong> of this sentence?</p>\n",
    "score": 5,
    "creation_date": 1473044043,
    "view_count": 8793,
    "answer_count": 3,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 12247768,
    "title": "unigrams &amp; bigrams (tf-idf) less accurate than just unigrams (ff-idf)?",
    "body": "<p>This is a question about linear regression with ngrams, using Tf-IDF (term frequency - inverse document frequency). To do this, I am using numpy sparse matrices and sklearn for linear regression.</p>\n\n<p>I have 53 cases and over 6000 features when using unigrams. The predictions are based on cross validation using LeaveOneOut. </p>\n\n<p>When I create a tf-idf sparse matrix of only unigram scores, I get slightly better predictions than when I create a tf-idf sparse matrix of unigram+bigram scores. The more columns I add to the matrix (columns for trigram, quadgram, quintgrams, etc.), the less accurate the regression prediction.</p>\n\n<p>Is this common? How is this possible? I would have thought that the more features, the better.</p>\n",
    "score": 5,
    "creation_date": 1346674786,
    "view_count": 15644,
    "answer_count": 2,
    "tags": "machine-learning;scikit-learn;nlp;regression;tf-idf"
  },
  {
    "question_id": 54617296,
    "title": "Can a token be removed from a spaCy document during pipeline processing?",
    "body": "<p>I am using spaCy (a great Python NLP library) to process a number of very large documents, however, my corpus has a number of common words that I would like to eliminate in the document processing pipeline.  Is there a way to remove a token from the document within a pipeline component?</p>\n",
    "score": 5,
    "creation_date": 1549808476,
    "view_count": 5133,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 52688019,
    "title": "How do I download en for spacy using conda?",
    "body": "<p>I am currently using Windows OS. I have installed Anaconda for creating environments. I have successfully created another environment with python 2.7. I have also installed spacy on that environment using:</p>\n\n<p><code>conda install --name myenv -c spacy spacy</code></p>\n\n<p>But now I need to download 'en module to run it. On spacy official site they have mentioned the following commands for the same:\n<code>conda install -c conda-forge spacy\npython -m spacy download en\n</code>\nthen I activated the 2nd environment using:</p>\n\n<pre><code>activate myenv\n</code></pre>\n\n<p>But running the second command(python -m spacy..)it gives me an error:</p>\n\n<pre><code> No module named spacy.__main__; 'spacy' is a package and cannot be\n directly executed\n</code></pre>\n\n<p>Please help me..</p>\n",
    "score": 5,
    "creation_date": 1538911508,
    "view_count": 21775,
    "answer_count": 3,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 135777,
    "title": "A StringToken Parser which gives Google Search style &quot;Did you mean:&quot; Suggestions",
    "body": "<h2>Seeking a method to:</h2>\n\n<h2>Take whitespace separated tokens in a String; return a suggested Word</h2>\n\n<p><br>\n<strong>ie:</strong><br>\nGoogle Search can take <em>\"fonetic wrd nterpreterr\"</em>,<br>\nand atop of the result page it shows <em>\"Did you mean: phonetic word interpreter\"</em></p>\n\n<p><em>A solution in any of the C* languages or Java would be preferred.</em></p>\n\n<p><br>\n<strong>Are there any existing Open Libraries which perform such functionality?</strong></p>\n\n<p><strong>Or is there a way to Utilise a Google API to request a suggested word?</strong></p>\n",
    "score": 5,
    "creation_date": 1222373983,
    "view_count": 1903,
    "answer_count": 8,
    "tags": "language-agnostic;parsing;nlp"
  },
  {
    "question_id": 14148986,
    "title": "How do you write a program to find if certain words are similar?",
    "body": "<p>Ie: \"college\" and \"schoolwork\" and \"academy\" belong in the same cluster, \nthe words \"essay\", \"scholarships\" , \"money\" also belong in the same cluster. Is this a ML or NLP problem? </p>\n",
    "score": 5,
    "creation_date": 1357255777,
    "view_count": 6388,
    "answer_count": 5,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 11799971,
    "title": "Is there a set of adjective word list for positive or negative polarity",
    "body": "<p>I am working on sentiment analysis. I thought if there is any available set of adjectives indicating positive/negative(like for positive: good,awesome,amazing,) meaning? and the second thing is a set of data from which i can use as a test case.</p>\n",
    "score": 5,
    "creation_date": 1344012551,
    "view_count": 11212,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp;sentiment-analysis"
  },
  {
    "question_id": 479825,
    "title": "Problem trimming Japanese string in java",
    "body": "<p>I have the following string (japanese) \"　ユーザー名\" , the first character is \"like\" whitespace but its number in unicode is 12288, so if I do \"　ユーザー名\".trim() I get the same string (trim doesn't work).\nIf i do trim in c++ it works ok.\nDoes anyone know how to solve this issue in java?\nIs there a special trim method for unicode?</p>\n",
    "score": 5,
    "creation_date": 1232977194,
    "view_count": 6014,
    "answer_count": 6,
    "tags": "java;string;nlp"
  },
  {
    "question_id": 68676637,
    "title": "AttributeError: &#39;Word2Vec&#39; object has no attribute &#39;most_similar&#39; (Word2Vec)",
    "body": "<p>I am using Word2Vec and using a wiki trained model that gives out the most similar words. I ran this before and it worked but now it gives me this error even after rerunning the whole program. I tried to take off  <code>return_path=True</code> but im still getting the same error</p>\n<pre><code>print(api.load('glove-wiki-gigaword-50', return_path=True))\nmodel.most_similar(&quot;glass&quot;)\n</code></pre>\n<p>#ERROR:</p>\n<pre><code>/Users/me/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-153-3bf32168d154&gt; in &lt;module&gt;\n      1 print(api.load('glove-wiki-gigaword-50', return_path=True))\n----&gt; 2 model.most_similar(&quot;glass&quot;) \n\nAttributeError: 'Word2Vec' object has no attribute 'most_similar'\n</code></pre>\n<p>#MODEL\nthis is the model I used</p>\n<pre><code>    print(\n        '%s (%d records): %s' % (\n            model_name,\n            model_data.get('num_records', -1),\n            model_data['description'][:40] + '...',\n        )\n    )\n</code></pre>\n<p>Edit: here is my gensim download &amp; output</p>\n<pre><code>!python -m pip install -U gensim\n</code></pre>\n<p>OUTPUT:</p>\n<p>Requirement already satisfied: gensim in ./opt/anaconda3/lib/python3.8/site-packages (4.0.1)</p>\n<p>Requirement already satisfied: numpy&gt;=1.11.3 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)</p>\n<p>Requirement already satisfied: smart-open&gt;=1.8.1 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.1.0)</p>\n<p>Requirement already satisfied: scipy&gt;=0.18.1 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)</p>\n",
    "score": 5,
    "creation_date": 1628228466,
    "view_count": 17537,
    "answer_count": 2,
    "tags": "python;nlp;gensim;word2vec;doc2vec"
  },
  {
    "question_id": 43841467,
    "title": "How to compute perplexity using KenLM?",
    "body": "<p>Let's say we build a model on this:</p>\n\n<pre><code>$ wget https://gist.githubusercontent.com/alvations/1c1b388456dc3760ffb487ce950712ac/raw/86cdf7de279a2b9bceeb3adb481e42691d12fbba/something.txt\n$ lmplz -o 5 &lt; something.txt &gt; something.arpa\n</code></pre>\n\n<p>From the perplexity formula (<a href=\"https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf\" rel=\"noreferrer\">https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf</a>) </p>\n\n<p>Applying the sum of inverse log formula to get the inner variable and then taking the nth root, the perplexity number is unusually small:</p>\n\n<pre><code>&gt;&gt;&gt; import kenlm\n&gt;&gt;&gt; m = kenlm.Model('something.arpa')\n\n# Sentence seen in data.\n&gt;&gt;&gt; s = 'The development of a forward-looking and comprehensive European migration policy,'\n&gt;&gt;&gt; list(m.full_scores(s))\n[(-0.8502398729324341, 2, False), (-3.0185394287109375, 3, False), (-0.3004383146762848, 4, False), (-1.0249041318893433, 5, False), (-0.6545327305793762, 5, False), (-0.29304179549217224, 5, False), (-0.4497605562210083, 5, False), (-0.49850910902023315, 5, False), (-0.3856896460056305, 5, False), (-0.3572353720664978, 5, False), (-1.7523181438446045, 1, False)]\n&gt;&gt;&gt; n = len(s.split())\n&gt;&gt;&gt; sum_inv_logs = -1 * sum(score for score, _, _ in m.full_scores(s))\n&gt;&gt;&gt; math.pow(sum_inv_logs, 1.0/n)\n1.2536033936438895\n</code></pre>\n\n<p>Trying again with a sentence not found in the data:</p>\n\n<pre><code># Sentence not seen in data.\n&gt;&gt;&gt; s = 'The European developement of a forward-looking and comphrensive society is doh.'\n&gt;&gt;&gt; sum_inv_logs = -1 * sum(score for score, _, _ in m.full_scores(s))\n&gt;&gt;&gt; sum_inv_logs\n35.59524390101433\n&gt;&gt;&gt; n = len(s.split())\n&gt;&gt;&gt; math.pow(sum_inv_logs, 1.0/n)\n1.383679905428275\n</code></pre>\n\n<p>And trying again with totally out of domain data:</p>\n\n<pre><code>&gt;&gt;&gt; s = \"\"\"On the evening of 5 May 2017, just before the French Presidential Election on 7 May, it was reported that nine gigabytes of Macron's campaign emails had been anonymously posted to Pastebin, a document-sharing site. In a statement on the same evening, Macron's political movement, En Marche!, said: \"The En Marche! Movement has been the victim of a massive and co-ordinated hack this evening which has given rise to the diffusion on social media of various internal information\"\"\"\n&gt;&gt;&gt; sum_inv_logs = -1 * sum(score for score, _, _ in m.full_scores(s))\n&gt;&gt;&gt; sum_inv_logs\n282.61719834804535\n&gt;&gt;&gt; n = len(list(m.full_scores(s)))\n&gt;&gt;&gt; n\n79\n&gt;&gt;&gt; math.pow(sum_inv_logs, 1.0/n)\n1.0740582373271952\n</code></pre>\n\n<p>Although, it is expected that the longer sentence has lower perplexity, it's strange that the difference is less than 1.0 and in the range of decimals. </p>\n\n<p><strong>Is the above the right way to compute perplexity with KenLM? If not, does anyone know how to computer perplexity with the KenLM through the Python API?</strong></p>\n",
    "score": 5,
    "creation_date": 1494226368,
    "view_count": 8044,
    "answer_count": 4,
    "tags": "python;nlp;language-model;kenlm;perplexity"
  },
  {
    "question_id": 32850155,
    "title": "Why does the ngrams() function give distinct bigrams?",
    "body": "<p>I am writing an <a href=\"http://en.wikipedia.org/wiki/R_%28programming_language%29\" rel=\"nofollow\">R</a> script and am using library(ngram).</p>\n\n<p>Suppose I have a string,</p>\n\n<p>\"good qualiti <strong>dog food</strong> bought sever vital can <strong>dog food</strong> product found good qualiti product look like stew process meat smell better labrador finicki appreci product better\"</p>\n\n<p>and want to find bi-grams.</p>\n\n<p>The ngram library is giving me bi-grams as follows:</p>\n\n<p>\"appreci product\" \"process meat\" \"food product\" \"food bought\" \"qualiti dog\" \"product found\" \"product look\" \"look like\" \"like stew\" \"good qualiti\" \"labrador finicki\" \"bought sever\" \"qualiti product\" \"better labrador\"\n<strong>\"dog food\"</strong> \"smell better\" \"vital can\" \"meat smell\" \"found good\" \"sever vital\" \"stew process\" \"can dog\" \"finicki appreci\" \"product better\"</p>\n\n<p>As the sentence contains \"dog food\" two times, I want this bi-gram two times. But I am getting it once!</p>\n\n<p>Is there an option in thengram library or any other library that gives all the bi-grams of my sentence in R?</p>\n",
    "score": 5,
    "creation_date": 1443547504,
    "view_count": 510,
    "answer_count": 5,
    "tags": "r;nlp;n-gram"
  },
  {
    "question_id": 14297674,
    "title": "Computing Hamming weight, also called popcount in Java?",
    "body": "<p>I am not sure how to translate this from C++ to Java.\nIt is a function that computes the Hamming weight.</p>\n\n<pre><code>/** This is popcount_3() from:\n * http://en.wikipedia.org/wiki/Hamming_weight */\nunsigned int popcnt32(uint32_t n) const\n{\n    n -= ((n &gt;&gt; 1) &amp; 0x55555555);\n    n = (n &amp; 0x33333333) + ((n &gt;&gt; 2) &amp; 0x33333333);\n    return (((n + (n &gt;&gt; 4))&amp; 0xF0F0F0F)* 0x1010101) &gt;&gt; 24;\n}\n</code></pre>\n\n<p>More concretely, I don't know what to use instead of uint32_t,\nand if I use that type whatever it is, can I just leave the rest\ncode unchanged?</p>\n\n<p>Thanks</p>\n",
    "score": 5,
    "creation_date": 1358022640,
    "view_count": 2919,
    "answer_count": 1,
    "tags": "java;c++;nlp;computer-vision;porting"
  },
  {
    "question_id": 1377020,
    "title": "Can you programmatically detect pluralizations of English words, and derive the singular form?",
    "body": "<p><strong>Given some (English) word that we shall assume is a plural</strong>, is it possible to derive the singular form? I'd like to avoid lookup/dictionary tables if possible.</p>\n\n<p>Some examples:</p>\n\n<pre>\nExamples  -> Example    a simple 's' suffix\nGlitch    -> Glitches   'es' suffix, as opposed to above\nCountries -> Country    'ies' suffix.\nSheep     -> Sheep      no change: possible fallback for indeterminate values\n</pre>\n\n<p>Or, <a href=\"http://en.wiktionary.org/wiki/Appendix:Irregular_plurals:English\" rel=\"nofollow noreferrer\">this seems to be a fairly exhaustive list.</a></p>\n\n<p>Suggestions of libraries in language <code>x</code> are fine, as long as they are open-source (ie, so that someone can examine them to determine how to do it in language <code>y</code>)</p>\n",
    "score": 5,
    "creation_date": 1252033792,
    "view_count": 4943,
    "answer_count": 6,
    "tags": "language-agnostic;nlp;stemming;lemmatization"
  },
  {
    "question_id": 65160277,
    "title": "Spacy tokenizer with only &quot;Whitespace&quot; rule",
    "body": "<p>I would like to know if the spacy tokenizer could tokenize words only using the &quot;space&quot; rule.\nFor example:</p>\n<pre><code>sentence= &quot;(c/o Oxford University )&quot;\n</code></pre>\n<p>Normally, using the following configuration of spacy:</p>\n<pre><code>nlp = spacy.load(&quot;en_core_news_sm&quot;)\ndoc = nlp(sentence)\nfor token in doc:\n   print(token)\n</code></pre>\n<p>the result would be:</p>\n<pre><code> (\n c\n /\n o\n Oxford\n University\n )\n</code></pre>\n<p>Instead, I would like an output like the following (using spacy):</p>\n<pre><code>(c/o \nOxford \nUniversity\n)\n</code></pre>\n<p>Is it possible to obtain a result like this using spacy?</p>\n",
    "score": 5,
    "creation_date": 1607191466,
    "view_count": 7082,
    "answer_count": 3,
    "tags": "python;python-3.x;nlp;spacy"
  },
  {
    "question_id": 49256079,
    "title": "Best way to compare meaning of text documents?",
    "body": "<p>I'm trying to find the best way to compare two text documents using AI and machine learning methods. I've used the TF-IDF-Cosine Similarity and other similarity measures, but this compares the documents at a word (or n-gram) level.</p>\n\n<p>I'm looking for a method that allows me to compare the <em>meaning</em> of the documents. What is the best way to do that?</p>\n",
    "score": 5,
    "creation_date": 1520943671,
    "view_count": 9247,
    "answer_count": 5,
    "tags": "machine-learning;nlp;artificial-intelligence;text-mining"
  },
  {
    "question_id": 29151329,
    "title": "Arabic lemmatization and Stanford NLP",
    "body": "<p>I try to make lemmatization, ie identifying the lemma and possibly the Arabic root of a verb, for example:\nيتصل ==> lemma (infinitive of the verb) ==> اتصل ==> root (triliteral root / Jidr thoulathi)\n==> و ص ل</p>\n\n<p>Do you think Stanford NLP can do that?</p>\n\n<p>Best Regards,</p>\n",
    "score": 5,
    "creation_date": 1426786434,
    "view_count": 5314,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp;lexical-analysis;stemming;lemmatization"
  },
  {
    "question_id": 19727261,
    "title": "How to count the number of spoken syllables in an audio file?",
    "body": "<p>I have many audio files with clean audio and only spoken voice in Mandarin Chinese. I need to estimate of how many syllables are spoken in each file. Is there a tool for OS X, Windows, or Linux that can estimate these?</p>\n\n<pre><code>sample01.wav 15\nsample02.wav 8\nsample03.wav 5\nsample04.wav 1\nsample05.wav 18\n</code></pre>\n\n<p>As there are many files, command-line or batch-capable software is preferred, e.g.:</p>\n\n<pre><code>$ application sample01.wav\n15\n</code></pre>\n\n<ul>\n<li>A solution that uses speech-to-text, then counts the number of characters present would be suitable to.</li>\n</ul>\n",
    "score": 5,
    "creation_date": 1383310816,
    "view_count": 5923,
    "answer_count": 4,
    "tags": "nlp;speech-recognition"
  },
  {
    "question_id": 62072566,
    "title": "How to speed up Spacy&#39;s nlp call?",
    "body": "<p>I have to process hundreds of thousands of texts. I have found that the thing that is taking the longest in the following:</p>\n\n<pre><code>nlp = English()\nruler = EntityRuler(nlp)\npatterns = [...]\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler)\n...\n#This line takes longer than I would like\ndoc = nlp(whole_chat)\n</code></pre>\n\n<p>Granted, I have many patterns. But is there a way to speed this up? I only have the entity ruler pipe, no others.</p>\n",
    "score": 5,
    "creation_date": 1590692629,
    "view_count": 7043,
    "answer_count": 2,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 55764766,
    "title": "Calculate TD-IDF for a single word in Textacy",
    "body": "<p>I'm trying to use <a href=\"https://github.com/chartbeat-labs/textacy\" rel=\"noreferrer\">Textacy</a> to calculate the TF-IDF score for a single word across the standard corpus, but am a bit unclear about the result I am receiving.</p>\n\n<p>I was expecting a single float which represented the frequency of the word in the corpus. So why am I receiving a list (?) of 7 results?</p>\n\n<p>\"acculer\" is actually a French word, so was expecting a result of 0 from an English corpus.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>word = 'acculer'\nvectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=True, idf_type='smooth')\ntf_idf = vectorizer.fit_transform(word)\nlogger.info(\"tf_idf:\")\nlogger.info(tfidf)\n</code></pre>\n\n<p>Output</p>\n\n<pre><code>tf_idf:\n(0, 0)  2.386294361119891\n(1, 1)  1.9808292530117262\n(2, 1)  1.9808292530117262\n(3, 5)  2.386294361119891\n(4, 3)  2.386294361119891\n(5, 2)  2.386294361119891\n(6, 4)  2.386294361119891\n</code></pre>\n\n<p>The second part of the question is how can I provide my own corpus to the TF-IDF function in Textacy, esp. one in a different language?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>As mentioned by @Vishal I have logged the ouput using this line:</p>\n\n<pre><code>logger.info(vectorizer.vocabulary_terms)\n</code></pre>\n\n<p>It seems the provided word <code>acculer</code> has been split into characters. </p>\n\n<pre><code>{'a': 0, 'c': 1, 'u': 5, 'l': 3, 'e': 2, 'r': 4}\n</code></pre>\n\n<p>(1) How can I get the TF-IDF for this word against the corpus, rather than each character?</p>\n\n<p>(2) How can I provide my own corpus and point to it as a param?</p>\n\n<p>(3) Can TF-IDF be used at a sentence level? ie: what is the relative frequency of this sentence's terms against the corpus.</p>\n",
    "score": 5,
    "creation_date": 1555690744,
    "view_count": 1487,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;spacy;textacy"
  },
  {
    "question_id": 53212374,
    "title": "How to get token ids using spaCy (I want to map a text sentence to sequence of integers)",
    "body": "<p>I want to use spacy to tokenize sentences to get a sequence of integer token-ids that I can use for downstream tasks. I expect to use it something like below. Please fill in <code>???</code></p>\n<pre><code>import spacy\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nnlp = spacy.load('en_core_web_lg')\n\n# Process whole documents\ntext = (u&quot;When Sebastian Thrun started working on self-driving cars at &quot;)\n\ndoc = nlp(text)\n\nidxs = ???\n\nprint(idxs)\n</code></pre>\n<p>I want the output to be something like:</p>\n<blockquote>\n<p>array([ 8045, 70727, 24304, 96127, 44091, 37596, 24524, 35224, 36253])</p>\n</blockquote>\n<p>Preferably the integers refers to some special embedding id in <code>en_core_web_lg</code>..</p>\n<p>spacy.io/usage/vectors-similarity does not give a hint what attribute in doc to look for.</p>\n<p>I asked this on <a href=\"https://stats.stackexchange.com/questions/376011/how-to-get-token-ids-using-spacy-i-want-to-map-a-text-sentence-to-sequence-of-i\">crossvalidated</a> but it was determined as OT. Proper terms for googling/describing this problem is also helpful.</p>\n",
    "score": 5,
    "creation_date": 1541695545,
    "view_count": 3882,
    "answer_count": 2,
    "tags": "nlp;spacy;word-embedding"
  },
  {
    "question_id": 42399565,
    "title": "Save gensim Word2vec model in binary format .bin with save_word2vec_format",
    "body": "<p>I'm training my own word2vec model using different data. To implement the resulting model into my classifier and compare the results with the original pre-trained Word2vec model I need to save the model in binary extension .bin. Here is my code, <em>sentences</em> is a list of short messages.</p>\n\n<pre><code>import gensim, logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nsentences = gensim.models.word2vec.LineSentence('dati.txt')\nmodel = gensim.models.Word2Vec(\nsentences, size=300, window=5, min_count=5, workers=5,\nsg=1, hs=1, negative=0\n)\nmodel.save_word2vec_format('model.bin', binary=True)\n</code></pre>\n\n<p>The last method, save_word2vec_format, gives me this error:</p>\n\n<p><code>\nAttributeError: 'Word2Vec' object has no attribute 'save_word2vec_format'\n</code></p>\n\n<p>What am I missing here? I've read the documentation of gensim and other forums. This <a href=\"https://github.com/devmount/GermanWordEmbeddings/blob/c2b603a07d968146995ee9dde54a25fd0aa8586a/training.py#L56\" rel=\"noreferrer\">repo on github</a> uses almost the same configuration so I cannot understand what's wrong. I've tried to switch from skipgram to cbow and from hierarchical softmax to negative sampling with no results.</p>\n\n<p>Thank you in advance!</p>\n",
    "score": 5,
    "creation_date": 1487788394,
    "view_count": 18728,
    "answer_count": 2,
    "tags": "python;attributes;nlp;gensim;word2vec"
  },
  {
    "question_id": 42220764,
    "title": "Elasticsearch: getting the tf-idf of every term in a given document",
    "body": "<p>I have a document in my elasticsearch with the following id: <code>AVosj8FEIaetdb3CXpP-</code> I'm trying to access for every words in the fields it's tf-idf I did the following:</p>\n\n<pre><code>GET /cnn/cnn_article/AVosj8FEIaetdb3CXpP-/_termvectors\n{\n  \"fields\" : [\"author_wording\"],\n  \"term_statistics\" : true,\n  \"field_statistics\" : true\n}'\n</code></pre>\n\n<p>The response I've got is:</p>\n\n<pre><code>{\n  \"_index\": \"dailystormer\",\n  \"_type\": \"dailystormer_article\",\n  \"_id\": \"AVosj8FEIaetdb3CXpP-\",\n  \"_version\": 3,\n  \"found\": true,\n  \"took\": 1,\n  \"term_vectors\": {\n    \"author_wording\": {\n      \"field_statistics\": {\n        \"sum_doc_freq\": 3408583,\n        \"doc_count\": 16111,\n        \"sum_ttf\": 7851321\n      },\n      \"terms\": {\n        \"318\": {\n          \"doc_freq\": 4,\n          \"ttf\": 4,\n          \"term_freq\": 1,\n          \"tokens\": [\n            {\n              \"position\": 121,\n              \"start_offset\": 688,\n              \"end_offset\": 691\n            }\n          ]\n        },\n        \"742\": {\n          \"doc_freq\": 1,\n          \"ttf\": 1,\n          \"term_freq\": 1,\n          \"tokens\": [\n            {\n              \"position\": 122,\n              \"start_offset\": 692,\n              \"end_offset\": 695\n            }\n          ]\n        },\n        \"9971\": {\n          \"doc_freq\": 1,\n          \"ttf\": 1,\n          \"term_freq\": 1,\n          \"tokens\": [\n            {\n              \"position\": 123,\n              \"start_offset\": 696,\n              \"end_offset\": 700\n            }\n          ]\n        },\n        \"a\": {\n          \"doc_freq\": 14921,\n          \"ttf\": 163268,\n          \"term_freq\": 11,\n          \"tokens\": [\n            {\n              \"position\": 1,\n              \"start_offset\": 13,\n              \"end_offset\": 14\n            },\n            ...\n            \"you’re\": {\n          \"doc_freq\": 1112,\n          \"ttf\": 1647,\n          \"term_freq\": 1,\n          \"tokens\": [\n            {\n              \"position\": 80,\n              \"start_offset\": 471,\n              \"end_offset\": 477\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>\n\n<p>It returns me some interesting fields like the term frequency (tf) but not the tf-idf. Should I recompute it myself? Is that a good idea? How can I do so?</p>\n",
    "score": 5,
    "creation_date": 1487059357,
    "view_count": 14409,
    "answer_count": 2,
    "tags": "elasticsearch;nlp;tf-idf"
  },
  {
    "question_id": 34791491,
    "title": "Where to start: Natural language processing and AI using Python",
    "body": "<p>My goal is to write a program capable of extracting tone, personality, and intent from human language inquiries (e.g. I type: How are you doing today? And the AI system responds with something like: Fine. How are you?)</p>\n\n<p>I'm aware this is a non-trivial problem, so what deep-learning topics should I start becoming familiar with and what Python modules are most useful? I've already started looking at NLTK. Thanks.</p>\n",
    "score": 5,
    "creation_date": 1452780049,
    "view_count": 3695,
    "answer_count": 2,
    "tags": "nlp;artificial-intelligence;deep-learning"
  },
  {
    "question_id": 16225667,
    "title": "What does &quot;word count&quot; refer to when calculating unigram probabilities in an unigram language model?",
    "body": "<p>I'm using an unigram language model. I want to calculate the probability of each unigram. Should I divide the number of occurrences of an unigram with the number of distinct unigrams, or by the count of all unigrams?</p>\n",
    "score": 5,
    "creation_date": 1366928154,
    "view_count": 9796,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 13367066,
    "title": "Date Extraction from Text",
    "body": "<p>I am trying to use Stanford NLP tool to extract dates ( 8/11/2012 ) form text.</p>\n\n<p>Here's <a href=\"http://nlp.stanford.edu:8080/ner/process/\" rel=\"noreferrer\">a link</a>! for the demo of this tool </p>\n\n<p>Can u help me in how to train the classifier to identify date ( 8/11/2012 ).</p>\n\n<p>I tried using training data as</p>\n\n<p>Woodhouse   PERS\n8/18/2012 Date\n,   O\nhandsome    O </p>\n\n<p>but does not work for same test data .</p>\n",
    "score": 5,
    "creation_date": 1352833562,
    "view_count": 9810,
    "answer_count": 3,
    "tags": "date;nlp;stanford-nlp"
  },
  {
    "question_id": 137380,
    "title": "NLP: Building (small) corpora, or &quot;Where to get lots of not-too-specialized English-language text files?&quot;",
    "body": "<p>Does anyone have a suggestion for where to find archives or collections of everyday English text for use in a small corpus?  I have been using Gutenberg Project books for a working prototype, and would like to incorporate more contemporary language.  A <a href=\"https://stackoverflow.com/questions/122595/nlp-qualitatively-positive-vs-negative-sentence#126378\">recent answer</a> here pointed indirectly to a great <a href=\"http://us.imdb.com/Reviews/\" rel=\"nofollow noreferrer\">archive of usenet movie reviews</a>, which hadn't occurred to me, and is very good.  For this particular program technical usenet archives or programming mailing lists would tilt the results and be hard to analyze, but any kind of general blog text, or chat transcripts, or anything that may have been useful to others, would be very helpful.  Also, a partial or downloadable research corpus that isn't too marked-up, or some heuristic for finding an appropriate subset of wikipedia articles, or any other idea, is very appreciated.</p>\n\n<p>(BTW, I am being a good citizen w/r/t downloading, using a deliberately slow script that is not demanding on servers hosting such material, in case you perceive a moral hazard in pointing me to something enormous.)</p>\n\n<p><strong>UPDATE</strong>:  User S0rin points out that wikipedia requests no crawling and provides <a href=\"http://en.wikipedia.org/wiki/Special:Export\" rel=\"nofollow noreferrer\">this export tool</a> instead.  Project Gutenberg has a policy specified <a href=\"http://www.gutenberg.org/wiki/Gutenberg:Information_About_Robot_Access_to_our_Pages\" rel=\"nofollow noreferrer\">here</a>, bottom line, try not to crawl, but if you need to: \"Configure your robot to wait at least 2 seconds between requests.\"</p>\n\n<p><strong>UPDATE 2</strong>  The wikpedia dumps are the way to go, thanks to the answerers who pointed them out.  I ended up using the English version from here: <a href=\"http://download.wikimedia.org/enwiki/20090306/\" rel=\"nofollow noreferrer\">http://download.wikimedia.org/enwiki/20090306/</a> , and a Spanish dump about half the size.  They are some work to clean up, but well worth it, and they contain a lot of useful data in the links.</p>\n\n<hr>\n",
    "score": 5,
    "creation_date": 1222395349,
    "view_count": 1658,
    "answer_count": 7,
    "tags": "nlp;linguistics;corpus"
  },
  {
    "question_id": 75357936,
    "title": "How to install Detectron2",
    "body": "<p>I am installing layout-parser and following this <a href=\"https://layout-parser.readthedocs.io/en/latest/notes/installation.html\" rel=\"nofollow noreferrer\">link</a>. Did not face any issues with the following packages.  </p>\n<pre><code>pip install layoutparser    \npip install &quot;layoutparser[effdet]&quot;    \npip install layoutparser torchvision     \npip install &quot;layoutparser[paddledetection]&quot;    \npip install &quot;layoutparser[ocr]&quot; \n</code></pre>\n<p>But I am not able to install detectron2</p>\n<pre><code>pip install &quot;git+https://github.com/facebookresearch/detectron2.git@v0.5#egg=detectron2&quot; \n</code></pre>\n<p>while installing this package I am getting this error    </p>\n<blockquote>\n<p>ERROR: Could not find a version that satisfies the requirement detectron2 (unavailable) (from versions: none)</p>\n<p>ERROR: No matching distribution found for detectron2 (unavailable)</p>\n</blockquote>\n<p>I followed the same installation guide in Google collab and it worked but not able to install them in my Azure workspace.  </p>\n",
    "score": 5,
    "creation_date": 1675664351,
    "view_count": 30842,
    "answer_count": 4,
    "tags": "python;nlp;data-science;ocr;python-3.10"
  },
  {
    "question_id": 62244453,
    "title": "How to fix “ValueError: not enough values to unpack (expected 2, got 1)”",
    "body": "<p>I am trying to do sentiment analysis on a german tweet-data-set with the bert-base-german-cased modell which i imported over transformers from hugginface.</p>\n\n<p>To be able to calculate the predicted probabilities i want to Softmax of Numpy and here does the issue begin. </p>\n\n<pre><code>F.softmax(model(input_ids, attention_mask), dim=1)\n</code></pre>\n\n<p>I got the error:</p>\n\n<pre><code>ValueError: not enough values to unpack (expected 2, got 1)\n</code></pre>\n\n<p>Does anyone know, which values are here expected? </p>\n\n<p>All works  when i try to run it with:</p>\n\n<pre><code>self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n</code></pre>\n\n<p>getting the error when i switch to </p>\n\n<pre><code>self.bert = AutoModelWithLMHead.from_pretrained(\"bert-base-german-cased\")\n</code></pre>\n\n<p>As you can probaly see, i am a noob. therefore I please ask for simple and detailed explanations (understandable for a fish :D). </p>\n\n<p><a href=\"https://i.sstatic.net/lzmuB.png\" rel=\"noreferrer\">Code 0</a></p>\n\n<p><a href=\"https://i.sstatic.net/YY80e.png\" rel=\"noreferrer\">Code 1</a></p>\n\n<p><a href=\"https://i.sstatic.net/NX62A.png\" rel=\"noreferrer\">Code 2</a></p>\n\n<p>Input_ID' and 'Attention_mask' are output values of the tokenizations process. </p>\n",
    "score": 5,
    "creation_date": 1591528255,
    "view_count": 11126,
    "answer_count": 2,
    "tags": "python;numpy;nlp;softmax"
  },
  {
    "question_id": 58011563,
    "title": "Module Not Found Error when importing Pytorch_Transformers",
    "body": "<p>After downloading pytorch_transformers through Anaconda and executing the import command through the Jupyter Notebook, I am facing several errors related to missing modules. </p>\n\n<p>I tried searching sacremoses to import the package via Anaconda, but it is only available for Linux machines. Has anyone else faced similar issues? Thanks in advance!</p>\n\n<p><code>from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM</code></p>\n\n<p>This is the error:</p>\n\n<hr>\n\n<pre><code>&lt;ipython-input-5-218d0858d00f&gt; in &lt;module&gt;\n----&gt; 1 from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n\n~\\Anaconda3\\lib\\site-packages\\pytorch_transformers\\__init__.py in &lt;module&gt;\n      1 __version__ = \"1.2.0\"\n----&gt; 2 from .tokenization_auto import AutoTokenizer\n      3 from .tokenization_bert import BertTokenizer, BasicTokenizer, WordpieceTokenizer\n      4 from .tokenization_openai import OpenAIGPTTokenizer\n      5 from .tokenization_transfo_xl import (TransfoXLTokenizer, TransfoXLCorpus)\n\n~\\Anaconda3\\lib\\site-packages\\pytorch_transformers\\tokenization_auto.py in &lt;module&gt;\n     24 from .tokenization_transfo_xl import TransfoXLTokenizer\n     25 from .tokenization_xlnet import XLNetTokenizer\n---&gt; 26 from .tokenization_xlm import XLMTokenizer\n     27 from .tokenization_roberta import RobertaTokenizer\n     28 from .tokenization_distilbert import DistilBertTokenizer\n\n~\\Anaconda3\\lib\\site-packages\\pytorch_transformers\\tokenization_xlm.py in &lt;module&gt;\n     25 from io import open\n     26 \n---&gt; 27 import sacremoses as sm\n     28 \n     29 from .tokenization_utils import PreTrainedTokenizer\n\nModuleNotFoundError: No module named 'sacremoses'```\n</code></pre>\n",
    "score": 5,
    "creation_date": 1568897916,
    "view_count": 12784,
    "answer_count": 3,
    "tags": "nlp;anaconda;pytorch"
  },
  {
    "question_id": 53772907,
    "title": "No such file or directory &#39;nltk_data/corpora/stopwords/English&#39; when using colab",
    "body": "<p>First of all I am using Google colab for the work and\nI have downloaded nltk stopwords for English with following:</p>\n\n<pre><code>nltk.download('stopwords')\n</code></pre>\n\n<p>The download was successful</p>\n\n<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...\n</code></pre>\n\n<p>but when I run <code>stop = stopwords.words('English')</code></p>\n\n<p>I am getting <code>OSError: No such file or directory: '/root/nltk_data/corpora/stopwords/English'</code></p>\n",
    "score": 5,
    "creation_date": 1544756171,
    "view_count": 11365,
    "answer_count": 2,
    "tags": "python;nlp;nltk;google-colaboratory"
  },
  {
    "question_id": 48117508,
    "title": "How to find a similar substring inside a large string with a similarity score in python?",
    "body": "<p>What I'm looking for is not just a plain similarity score between two texts. But a similarity score of a substring inside a string. Say:</p>\n\n<pre><code>text1 = 'cat is sleeping on the mat'.\n\ntext2 = 'The cat is sleeping on the red mat in the living room'.\n</code></pre>\n\n<p>In the above example, all the words of <code>text1</code> are present in the <code>text2</code> completely, hence the similarity should be 100%. </p>\n\n<p>If some words of <code>text1</code> are missing, the score shall be less.</p>\n\n<p>I'm working with a large dataset of varying paragraph size, hence finding a smaller paragraph inside a bigger one with such similarity score is crucial. </p>\n\n<p>I found only string similarities such as cosine similarities, difflib similarity etc. which compares two strings. But not about a score of substring inside another string. </p>\n",
    "score": 5,
    "creation_date": 1515169469,
    "view_count": 6994,
    "answer_count": 4,
    "tags": "python;string;nlp;distance;similarity"
  },
  {
    "question_id": 44714142,
    "title": "Finding relations between Pronouns and Nouns in sentences",
    "body": "<p>I am working on an NLP project and I need the following functionality illustrated by an example. Say there is a sentence </p>\n\n<blockquote>\n  <p>Tell Sam that he will have to leave without Arthur, as he is sick.</p>\n</blockquote>\n\n<p>In this statement, the first <code>he</code> has to be tagged to Sam and the second <code>he</code> to Arthur. I work in Python. Any suggestions on what I can use to get the following functionality?</p>\n",
    "score": 5,
    "creation_date": 1498197892,
    "view_count": 2753,
    "answer_count": 3,
    "tags": "python;nlp;semantics;part-of-speech"
  },
  {
    "question_id": 42830248,
    "title": "how to write spacy matcher of POS regex",
    "body": "<p>Spacy has two features I'd like to combine - <a href=\"https://spacy.io/docs/usage/pos-tagging\" rel=\"noreferrer\">part-of-speech</a> (POS) and <a href=\"https://spacy.io/docs/usage/rule-based-matching\" rel=\"noreferrer\">rule-based matching</a>.</p>\n\n<p>How can I combine them in a neat way? </p>\n\n<p>For example - let's say input is a single sentence and I'd like to verify it meets some POS ordering condition - for example the verb is after the noun (something like noun**verb regex). result should be true or false. Is that doable? or the matcher is specific like in the example</p>\n\n<p>Rule-based matching can have POS rules?</p>\n\n<p>If not - here is my current plan - gather everything in one string and apply regex</p>\n\n<pre><code>    import spacy\nnlp = spacy.load('en')\n#doc = nlp(u'is there any way you can do it')\ntext=u'what are the main issues'\ndoc = nlp(text)\n\nconcatPos = ''\nprint(text)\nfor word in doc:\n    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)\n    concatPos += word.text +\"_\" + word.tag_ + \"_\" + word.pos_ + \"-\"\nprint('-----------')\nprint(concatPos)\nprint('-----------')\n\n# output of string- what_WP_NOUN-are_VBP_VERB-the_DT_DET-main_JJ_ADJ-issues_NNS_NOUN-\n</code></pre>\n",
    "score": 5,
    "creation_date": 1489657156,
    "view_count": 6041,
    "answer_count": 2,
    "tags": "nlp;spacy"
  },
  {
    "question_id": 20695825,
    "title": "English word segmentation in NLP?",
    "body": "<p>I am new in the NLP domain, but my current research needs some text parsing (or called keyword extraction) from URL addresses, e.g. a fake URL,</p>\n<pre><code>http://ads.goole.com/appid/heads\n</code></pre>\n<p>Two constraints are put on my parsing,</p>\n<ol>\n<li><p>The first &quot;ads&quot; and last &quot;heads&quot; should be distinct because &quot;ads&quot; in the &quot;heads&quot; means more suffix rather than an advertisement.</p>\n</li>\n<li><p>The &quot;appid&quot; can be parsed into two parts; that is 'app' and 'id', both taking semantic meanings on the Internet.</p>\n</li>\n</ol>\n<p>I have tried the <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"nofollow noreferrer\">Stanford NLP</a> toolkit and Google search engine. The former tries to classify each word in a grammar meaning which is under my expectation. The Google engine shows more smartness about &quot;appid&quot; which gives me suggestions about &quot;app id&quot;.</p>\n<p>I can not look over the reference of search history in Google search so that it gives me &quot;app id&quot; because there are many people have searched these words. Can I get some offline line methods to perform similar parsing??</p>\n<hr />\n<p><strong>UPDATE:</strong></p>\n<p>Please skip the regex suggestions because there is a potentially unknown number of compositions of words like &quot;appid&quot; in even simple URLs.</p>\n<p>Thanks,</p>\n<p>Jamin</p>\n",
    "score": 5,
    "creation_date": 1387510283,
    "view_count": 7904,
    "answer_count": 2,
    "tags": "web;nlp;text-segmentation"
  },
  {
    "question_id": 4512590,
    "title": "Latin to English alphabet hashing",
    "body": "<p>I have to convert all the latin characters to their corresponding English alphabets. Can I use Python to do it? Or is there a mapping available? </p>\n\n<p>Unicode values to non-unicode characters </p>\n\n<p><code>Ramírez Sánchez</code> should be converted to <code>Ramirez Sanchez</code>.</p>\n",
    "score": 5,
    "creation_date": 1293043925,
    "view_count": 2984,
    "answer_count": 1,
    "tags": "python;nlp"
  },
  {
    "question_id": 75854700,
    "title": "How to fine tune a Huggingface Seq2Seq model with a dataset from the hub?",
    "body": "<p>I want to train the <code>&quot;flax-community/t5-large-wikisplit&quot;</code> model with the <code>&quot;dxiao/requirements-ner-id&quot;</code> dataset. (Just for some experiments)</p>\n<p>I think my general procedure is not correct, but I don't know how to go further.</p>\n<p>My Code:</p>\n<p>Load tokenizer and model:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\ncheckpoint = &quot;flax-community/t5-large-wikisplit&quot;\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).cuda()\n</code></pre>\n<p>Load dataset that I want to train:</p>\n<pre><code>from datasets import load_dataset\nraw_dataset = load_dataset(&quot;dxiao/requirements-ner-id&quot;)\n</code></pre>\n<p>The raw_dataset looks like this ['id', 'tokens', 'tags', 'ner_tags']</p>\n<p>I want to get the sentences as sentence and not as tokens.</p>\n<pre><code>def tokenToString(tokenarray):\n  string = tokenarray[0]\n  for x in tokenarray[1:]:\n    string += &quot; &quot; + x\n  return string\n\ndef sentence_function(example):\n  return {&quot;sentence&quot; :  tokenToString(example[&quot;tokens&quot;]),\n          &quot;simplefiedSentence&quot; : tokenToString(example[&quot;tokens&quot;]).replace(&quot;The&quot;, &quot;XXXXXXXXXXX&quot;)}\n\nwikisplit_req_set = raw_dataset.map(sentence_function)\nwikisplit_req_set\n</code></pre>\n<p>I tried to restructure the dataset such that it looks like the wikisplit dataset:</p>\n<pre><code>simple1dataset = wikisplit_req_set.remove_columns(['id', 'tags', 'ner_tags', 'tokens']);\ncomplexdataset = wikisplit_req_set.remove_columns(['id', 'tags', 'ner_tags', 'tokens']);\ncomplexdataset[&quot;train&quot;] = complexdataset[&quot;train&quot;].add_column(&quot;simple_sentence_1&quot;,simple1dataset[&quot;train&quot;][&quot;sentence&quot;]).add_column(&quot;simple_sentence_2&quot;,simple1dataset[&quot;train&quot;][&quot;simplefiedSentence&quot;])\ncomplexdataset[&quot;test&quot;] = complexdataset[&quot;test&quot;].add_column(&quot;simple_sentence_1&quot;,simple1dataset[&quot;test&quot;][&quot;sentence&quot;]).add_column(&quot;simple_sentence_2&quot;,simple1dataset[&quot;test&quot;][&quot;simplefiedSentence&quot;])\ncomplexdataset[&quot;validation&quot;] = complexdataset[&quot;validation&quot;].add_column(&quot;simple_sentence_1&quot;,simple1dataset[&quot;validation&quot;][&quot;sentence&quot;]).add_column(&quot;simple_sentence_2&quot;,simple1dataset[&quot;validation&quot;][&quot;simplefiedSentence&quot;])\ntrainingDataSet = complexdataset.rename_column(&quot;sentence&quot;, &quot;complex_sentence&quot;)\ntrainingDataSet\n</code></pre>\n<p>Tokenize it:</p>\n<pre><code>def tokenize_function(example):\n    model_inputs = tokenizer(example[&quot;complex_sentence&quot;],truncation=True, padding=True)\n    targetS1 = tokenizer(example[&quot;simple_sentence_1&quot;],truncation=True, padding=True)\n    targetS2 = tokenizer(example[&quot;simple_sentence_2&quot;],truncation=True, padding=True)\n    model_inputs['simple_sentence_1'] = targetS1['input_ids']\n    model_inputs['simple_sentence_2'] = targetS2['input_ids']\n    model_inputs['decoder_input_ids'] = targetS2['input_ids']\n    return model_inputs\n\ntokenized_datasets = trainingDataSet.map(tokenize_function, batched=True)\ntokenized_datasets=tokenized_datasets.remove_columns(&quot;complex_sentence&quot;)\ntokenized_datasets=tokenized_datasets.remove_columns(&quot;simple_sentence_1&quot;)\ntokenized_datasets=tokenized_datasets.remove_columns(&quot;simple_sentence_2&quot;)\ntokenized_datasets=tokenized_datasets.remove_columns(&quot;simplefiedSentence&quot;)\ntokenized_datasets\n</code></pre>\n<p>DataLoader:</p>\n<pre><code>from transformers import DataCollatorForLanguageModeling\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\ndata_collator\n</code></pre>\n<p>Training:</p>\n<pre><code>from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainingArguments, EvalPrediction, DataCollatorWithPadding, Trainer\n\nbleu = evaluate.load(&quot;bleu&quot;)\n\ntraining_args = Seq2SeqTrainingArguments(\n  output_dir = &quot;/&quot;,\n  log_level = &quot;error&quot;,\n  num_train_epochs = 0.25,\n  learning_rate = 5e-4,\n  lr_scheduler_type = &quot;linear&quot;,\n  warmup_steps = 50,\n  optim = &quot;adafactor&quot;,\n  weight_decay = 0.01,\n  per_device_train_batch_size = 1,\n  per_device_eval_batch_size = 1,\n  gradient_accumulation_steps = 16,\n  evaluation_strategy = &quot;steps&quot;,\n  eval_steps = 50,\n  predict_with_generate=True,\n  generation_max_length = 128,\n  save_steps = 500,\n  logging_steps = 10,\n  push_to_hub = False,\n  auto_find_batch_size=True\n)\n\ntrainer = Seq2SeqTrainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[&quot;train&quot;],\n    eval_dataset=tokenized_datasets[&quot;validation&quot;],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=bleu,\n\n)\ntrainer.train()\n</code></pre>\n<p>The Problem is, that I do not understand how the model know the expected value and how it calculate its loss. Can someone give me some ideas what happens where?</p>\n<p>I hope some one can help me understand my own code, because the documentation by Hugging Face does not help me enough. Maybe someone have some Codeexamples or something else. I do not completely understand how I fine tune the model and how I get the parameters the model expects to train it. I also do not understand how the training works and what the parameters do.</p>\n",
    "score": 5,
    "creation_date": 1679913186,
    "view_count": 7824,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers;huggingface-tokenizers;huggingface"
  },
  {
    "question_id": 70321680,
    "title": "TypeError: add() takes exactly 2 positional arguments (3 given)",
    "body": "<p>Why I am getting this error Can anyone tell please\nor explain me how to use it using simple example</p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/tmp/ipykernel_33/3577035061.py in &lt;module&gt;\n      6 # Matcher class object\n      7 matcher = Matcher(nlp.vocab)\n----&gt; 8 matcher.add(&quot;matching_1&quot;, None, pattern)\n      9 \n     10 matches = matcher(doc)\n\n/opt/conda/lib/python3.7/site-packages/spacy/matcher/matcher.pyx in spacy.matcher.matcher.Matcher.add()\n\nTypeError: add() takes exactly 2 positional arguments (3 given)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/UtrgJ.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/UtrgJ.png\" alt=\"enter image description here\" /></a></p>\n<p>In Below link\n<a href=\"https://spacy.io/api/matcher\" rel=\"noreferrer\">https://spacy.io/api/matcher</a>\n<a href=\"https://i.sstatic.net/P0BRd.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/P0BRd.png\" alt=\"enter image description here\" /></a></p>\n",
    "score": 5,
    "creation_date": 1639293498,
    "view_count": 8423,
    "answer_count": 4,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 64646867,
    "title": "Downloading huggingface pre-trained models",
    "body": "<p>Once I have downloaded a pre-trained model on a Colab Notebook, it disappears after I reset the notebook variables.\nIs there a way I can download the model to use it for a second occasion?</p>\n<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n</code></pre>\n",
    "score": 5,
    "creation_date": 1604325157,
    "view_count": 9286,
    "answer_count": 1,
    "tags": "python;nlp;google-colaboratory;huggingface-transformers"
  },
  {
    "question_id": 59877761,
    "title": "How to strip string from punctuation except apostrophes for NLP",
    "body": "<p>I am using the below \"fastest\" way of removing punctuation from a string:</p>\n\n<pre><code>text = file_open.translate(str.maketrans(\"\", \"\", string.punctuation))\n</code></pre>\n\n<p>However, it removes all punctuation including apostrophes from tokens such as <code>shouldn't</code> turning it into <code>shouldnt</code>. </p>\n\n<p>The problem is I am using NLTK library for stopwords and the standard stopwords don't include such examples without apostrophes but instead have tokens that NLTK would generate if I used the NLTK tokenizer to split my text. For example for <code>shouldnt</code> the stopwords included are <code>shouldn, shouldn't, t</code>.</p>\n\n<p>I can either add the additional stopwords or remove the apostrophes from the NLTK stopwords. But both solutions don't seem \"correct\" in a way as I think the apostrophes should be left when doing punctuation cleaning.</p>\n\n<p>Is there a way I can leave the apostrophes when doing fast punctuation cleaning?</p>\n",
    "score": 5,
    "creation_date": 1579779724,
    "view_count": 5513,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 50004797,
    "title": "Anaphora resolution in stanford-nlp using python",
    "body": "<p>I am trying to do anaphora resolution and for that below is my code.</p>\n\n<p>first i navigate to the folder where i have downloaded the stanford module. Then i run the command in command prompt to initialize stanford nlp module</p>\n\n<pre><code>java -mx4g -cp \"*;stanford-corenlp-full-2017-06-09/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n</code></pre>\n\n<p>After that i execute below code in Python</p>\n\n<pre><code>from pycorenlp import StanfordCoreNLP\nnlp = StanfordCoreNLP('http://localhost:9000')\n</code></pre>\n\n<p>I want to change the sentence <code>Tom is a smart boy. He know a lot of thing.</code> into <code>Tom is a smart boy. Tom know a lot of thing.</code> and there is no tutorial or any help available in Python.</p>\n\n<p>All i am able to do is annotate by below code in Python</p>\n\n<p>coreference resolution</p>\n\n<pre><code>output = nlp.annotate(sentence, properties={'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})\n</code></pre>\n\n<p>and by parsing for coref </p>\n\n<pre><code>coreferences = output['corefs']\n</code></pre>\n\n<p>i get below JSON</p>\n\n<pre><code>coreferences\n\n{u'1': [{u'animacy': u'ANIMATE',\n   u'endIndex': 2,\n   u'gender': u'MALE',\n   u'headIndex': 1,\n   u'id': 1,\n   u'isRepresentativeMention': True,\n   u'number': u'SINGULAR',\n   u'position': [1, 1],\n   u'sentNum': 1,\n   u'startIndex': 1,\n   u'text': u'Tom',\n   u'type': u'PROPER'},\n  {u'animacy': u'ANIMATE',\n   u'endIndex': 6,\n   u'gender': u'MALE',\n   u'headIndex': 5,\n   u'id': 2,\n   u'isRepresentativeMention': False,\n   u'number': u'SINGULAR',\n   u'position': [1, 2],\n   u'sentNum': 1,\n   u'startIndex': 3,\n   u'text': u'a smart boy',\n   u'type': u'NOMINAL'},\n  {u'animacy': u'ANIMATE',\n   u'endIndex': 2,\n   u'gender': u'MALE',\n   u'headIndex': 1,\n   u'id': 3,\n   u'isRepresentativeMention': False,\n   u'number': u'SINGULAR',\n   u'position': [2, 1],\n   u'sentNum': 2,\n   u'startIndex': 1,\n   u'text': u'He',\n   u'type': u'PRONOMINAL'}],\n u'4': [{u'animacy': u'INANIMATE',\n   u'endIndex': 7,\n   u'gender': u'NEUTRAL',\n   u'headIndex': 4,\n   u'id': 4,\n   u'isRepresentativeMention': True,\n   u'number': u'SINGULAR',\n   u'position': [2, 2],\n   u'sentNum': 2,\n   u'startIndex': 3,\n   u'text': u'a lot of thing',\n   u'type': u'NOMINAL'}]}\n</code></pre>\n\n<p>Any help on this?</p>\n",
    "score": 5,
    "creation_date": 1524581694,
    "view_count": 4440,
    "answer_count": 3,
    "tags": "python;nlp;stanford-nlp;linguistics;pycorenlp"
  },
  {
    "question_id": 46527403,
    "title": "SpaCy model training data: WikiNER",
    "body": "<p>For the model <code>xx_ent_wiki_sm</code> of 2.0 version of SpaCy there is mention of \"WikiNER\" dataset, which leads to article 'Learning multilingual named entity recognition from Wikipedia'. </p>\n\n<p>Is there any resource for downloading of such dataset for retraining that model? Or script for Wikipedia dump processing?</p>\n",
    "score": 5,
    "creation_date": 1506954000,
    "view_count": 3169,
    "answer_count": 1,
    "tags": "python;nlp;dataset;spacy"
  },
  {
    "question_id": 32476336,
    "title": "How to abstract bigram topics instead of unigrams using Latent Dirichlet Allocation (LDA) in python- gensim?",
    "body": "<h1>LDA Original Output</h1>\n\n<ul>\n<li><p>Uni-grams </p>\n\n<ul>\n<li><p>topic1 -scuba,water,vapor,diving</p></li>\n<li><p>topic2 -dioxide,plants,green,carbon</p></li>\n</ul></li>\n</ul>\n\n<h1>Required Output</h1>\n\n<ul>\n<li><p>Bi-gram topics</p>\n\n<ul>\n<li><p>topic1 -scuba diving,water vapor</p></li>\n<li><p>topic2 -green plants,carbon dioxide</p></li>\n</ul></li>\n</ul>\n\n<p>Any idea?</p>\n",
    "score": 5,
    "creation_date": 1441792262,
    "view_count": 13423,
    "answer_count": 2,
    "tags": "nlp;text-mining;lda;gensim"
  },
  {
    "question_id": 23710214,
    "title": "Lexicon dictionary for synonym words",
    "body": "<p>There are few dictionaries available for natural language processing. Like positive, negative words dictionaries etc. </p>\n\n<p>Is there any dictionary available which contains list of synonym for all dictionary words? </p>\n\n<p>Like for <code>nice</code> </p>\n\n<pre><code>synonyms: enjoyable, pleasant, pleasurable, agreeable, delightful, satisfying, gratifying, acceptable, to one's liking, entertaining, amusing, diverting, marvellous, good; \n</code></pre>\n",
    "score": 5,
    "creation_date": 1400322454,
    "view_count": 4519,
    "answer_count": 2,
    "tags": "dictionary;nlp;stanford-nlp;data-processing;text-classification"
  },
  {
    "question_id": 23053688,
    "title": "How to detect duplicates among text documents and return the duplicates&#39; similarity?",
    "body": "<p>I'm writing a crawler to get content from some website, but the content can duplicated, I want \nto avoid that. So I need a function can return the same percent between two text to detect two content maybe duplicated Example: </p>\n\n<ul>\n<li>Text 1:\"I'm writing a crawler to\"</li>\n<li>Text 2:\"I'm writing a some text crawler to get\"</li>\n</ul>\n\n<p>The compare function will return text 2 as the same text 1 by 5/8%(with 5 is words number of text 2 same text 1(compare by word order), and 8 is total words of text 2). If remove the \"some text\" then text 2 as the same text 1(I need detect the situation).How can I do that?</p>\n",
    "score": 5,
    "creation_date": 1397458224,
    "view_count": 3646,
    "answer_count": 3,
    "tags": "algorithm;information-retrieval;text-analysis"
  },
  {
    "question_id": 13139821,
    "title": "Algorithm to generate context free grammar from any regex",
    "body": "<p>Can anyone outline for me an algorithm that can convert any given regex into an equivalent set of CFG rules?</p>\n\n<p>I know how to tackle the elementary stuff such as (a|b)*:</p>\n\n<pre><code>S -&gt; a A\nS -&gt; a B\nS -&gt; b A\nS -&gt; b B\nA -&gt; a A\nA -&gt; a B\nA -&gt; epsilon\nB -&gt; b A\nB -&gt; b B\nB -&gt; epsilon\nS -&gt; epsilon (end of string)\n</code></pre>\n\n<p>However, I'm having some problem formalizing it into a proper algorithm especially with more complex expressions that can have many nested operations.</p>\n",
    "score": 5,
    "creation_date": 1351602924,
    "view_count": 4315,
    "answer_count": 1,
    "tags": "regex;algorithm;nlp;context-free-grammar;computation-theory"
  },
  {
    "question_id": 12357066,
    "title": "stanford Core NLP: Splitting sentences from text",
    "body": "<p>I am new to stanford Core NLP. I would like to use it for splitting sentences from text in English, German,French. Which class does this work?Thanks in advance.</p>\n",
    "score": 5,
    "creation_date": 1347299927,
    "view_count": 11731,
    "answer_count": 4,
    "tags": "java;nlp;stanford-nlp;sentence"
  },
  {
    "question_id": 8169827,
    "title": "using Dependency Parser in Stanford coreNLP",
    "body": "<p>I am using the Stanford coreNLP ( <a href=\"http://nlp.stanford.edu/software/corenlp.shtml\" rel=\"nofollow\">http://nlp.stanford.edu/software/corenlp.shtml</a> ) in order to parse sentences and extract dependencies between the words.</p>\n\n<p>I have managed to create the dependencies graph like in the example in the supplied link, but I don't know how to work with it. I can print the entire graph using the <code>toString()</code> method, but the problem I have is that the methods that search for certain words in the graph, such as <code>getChildList</code>, require an IndexedWord object as a parameter. Now, it is clear why they do because the nodes of the graph are of IndexedWord type, but it's not clear to me how I create such an object in order to search for a specific node.</p>\n\n<p>For example: I want to find the children of the node that represents the word \"problem\" in my sentence. How I create an IndexWord object that represents the word \"problem\" so I can search for it in the graph?</p>\n",
    "score": 5,
    "creation_date": 1321544027,
    "view_count": 4312,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 4613773,
    "title": "Anyone know of some good Word Sense Disambiguation software?",
    "body": "<p>What represents the state-of-the-art in Word Sense Disambiguation (WSD) software? What metrics determine the state-of-the-art, and what toolkits / open source packages are available?</p>\n",
    "score": 5,
    "creation_date": 1294308504,
    "view_count": 5518,
    "answer_count": 1,
    "tags": "nlp;disambiguation;word-sense-disambiguation"
  },
  {
    "question_id": 2386652,
    "title": "Natural Language Processing Package",
    "body": "<p>I have started working on a project which requires Natural Language Processing. We have do the spell checking as well as mapping sentences to phrases and their synonyms. I first thought of using GATE but i am confused on what to use? I found an interesting post here which got me even more confused. </p>\n\n<p><a href=\"http://lordpimpington.com/codespeaks/drupal-5.1/?q=node/5\" rel=\"noreferrer\">http://lordpimpington.com/codespeaks/drupal-5.1/?q=node/5</a></p>\n\n<p>Please help me decide on what suits my purpose the best. I am working a web application which will us this NLP tool as a service.</p>\n",
    "score": 5,
    "creation_date": 1267792181,
    "view_count": 1044,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 1875765,
    "title": "How to define person&#39;s names in text (Java)",
    "body": "<p>I have some input text, which contains one or more human person names. I do not have any dictionary for these names. Which Java library can help me to define names from my input text?\nI looked through OpenNLP, but did not find any example or guide or at least description of how it can be applied into my code. (I saw javadoc, but it is pretty poor documentation for such a project.)</p>\n\n<p>I want to find names from some random text. If the input text is \"My friend Joe Smith went to the store.\", then I want to get \"Joe Smith\". I think there should be some large enough dictionaries on smart engines, based on smaller dictionaries, that can understand human names.</p>\n",
    "score": 5,
    "creation_date": 1260382474,
    "view_count": 6448,
    "answer_count": 10,
    "tags": "java;nlp;named-entity-recognition"
  },
  {
    "question_id": 75794919,
    "title": "How to segment and transcribe an audio from a video into timestamped segments?",
    "body": "<p>I want to segment a video transcript into chapters based on the content of each line of speech. The transcript would be used to generate a series of start and end timestamps for each chapter. This is similar to how YouTube now &quot;auto-chapters&quot; videos.</p>\n<p>Example .srt transcript:</p>\n<pre><code>...\n\n70\n00:02:53,640 --&gt; 00:02:54,760\nAll right, coming in at number five,\n\n71\n00:02:54,760 --&gt; 00:02:57,640\nwe have another habit that saves me around 15 minutes a day\n...\n</code></pre>\n<p>I have had minimal luck doing this with ChatGPT as it finds it difficult to both segment by topic and recollect start and end timestamps accurately. I am now exploring whether there are other options for doing this.</p>\n<p>I know topic modeling based on time series is possible with some python libraries. I have also read about text tiling as another option. <strong>What options are there for achieving an outcome like this?</strong></p>\n<p>Note: The format above (.srt) is not necessary. It's just the idea that the input is a list of text-content with start and end timestamps.</p>\n",
    "score": 5,
    "creation_date": 1679343286,
    "view_count": 7121,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;openai-api;automatic-speech-recognition"
  },
  {
    "question_id": 71512301,
    "title": "ERROR: Could not build wheels for spacy, which is required to install pyproject.toml-based projects",
    "body": "<p><a href=\"https://i.sstatic.net/ipmqI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/ipmqI.png\" alt=\"enter image description here\" /></a></p>\n<p>Hi Guys, I am trying to install spacy model == 2.3.5 but I am getting this error, please help me!</p>\n",
    "score": 5,
    "creation_date": 1647520185,
    "view_count": 46582,
    "answer_count": 4,
    "tags": "python;python-3.x;pip;nlp;spacy"
  },
  {
    "question_id": 62546815,
    "title": "How to use word embeddings (i.e., Word2vec, GloVe or BERT) to calculate the most word similarity in a set of N words?",
    "body": "<p>I am trying to calculate the semantic similarity by inputting the word list and output a word, which is the most word similarity in the list.</p>\n<p>E.g.</p>\n<p>If I pass in a list of words</p>\n<pre class=\"lang-py prettyprint-override\"><code>words = ['portugal', 'spain', 'belgium', 'country', 'netherlands', 'italy']\n</code></pre>\n<p>It should output me something like this-</p>\n<pre class=\"lang-py prettyprint-override\"><code>['country']\n</code></pre>\n",
    "score": 5,
    "creation_date": 1592966966,
    "view_count": 9828,
    "answer_count": 2,
    "tags": "python;nlp;word2vec;bert-language-model;cosine-similarity"
  },
  {
    "question_id": 55770009,
    "title": "How to use a pre-trained embedding matrix in tensorflow 2.0 RNN as initial weights in an embedding layer?",
    "body": "<p>I'd like to use a pretrained GloVe embedding as the initial weights for an embedding layer in an RNN encoder/decoder. The code is in Tensorflow 2.0. Simply adding the embedding matrix as a weights = [embedding_matrix] parameter to the tf.keras.layers.Embedding layer won't do it because the encoder is an object and I'm not sure now to effectively pass the embedding_matrix to this object at training time.</p>\n\n<p>My code closely follows the <a href=\"https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention\" rel=\"noreferrer\">neural machine translation example in the Tensorflow 2.0 documentation</a>. How would I add a pre-trained embedding matrix to the encoder in this example? The encoder is an object. When I get to training, the GloVe embedding matrix is unavailable to the Tensorflow graph. I get the error message: </p>\n\n<p>RuntimeError: Cannot get value inside Tensorflow graph function. </p>\n\n<p>The code uses the GradientTape method and teacher forcing in the training process. </p>\n\n<p>I've tried modifying the encoder object to include the embedding_matrix at various points, including in the encoder's <strong>init</strong>, call and initialize_hidden_state. All of these fail. The other questions on stackoverflow and elsewhere are for Keras or older versions of Tensorflow, not Tensorflow 2.0.</p>\n\n<pre><code>class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix])\n        self.gru = tf.keras.layers.GRU(self.enc_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\n# sample input\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n\n# ... Bahdanau Attention, Decoder layers, and train_step defined, see link to full tensorflow code above ...\n\n# Relevant training code\n\nEPOCHS = 10\n\ntraining_record = pd.DataFrame(columns = ['epoch', 'training_loss', 'validation_loss', 'epoch_time'])\n\n\nfor epoch in range(EPOCHS):\n    template = 'Epoch {}/{}'\n    print(template.format(epoch +1,\n                 EPOCHS))\n    start = time.time()\n\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n    total_val_loss = 0\n\n    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_hidden)\n        total_loss += batch_loss\n\n        if batch % 100 == 0:\n            template = 'batch {} ============== train_loss: {}'\n            print(template.format(batch +1,\n                            round(batch_loss.numpy(),4)))\n</code></pre>\n",
    "score": 5,
    "creation_date": 1555731210,
    "view_count": 8351,
    "answer_count": 2,
    "tags": "tensorflow;nlp;recurrent-neural-network;embedding;glove"
  },
  {
    "question_id": 53219016,
    "title": "Detecting sections of a pdf with pdfminer",
    "body": "<p>I am trying to transform pdfs from conference/journal papers into .txt files. I basically want to have a structure a bit cleaner that the current pdf: no line break before the end of a sentence and highlighting sections of the paper. The problem I am dealing with currently is to try and detect sections automatically. That is, in the following image, I want to be able to find ABSTRACT, CSS CONCEPT, 1 INTRODUCTION, 2 THE BODY OF THE PAPER.<a href=\"https://i.sstatic.net/yW9dr.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/yW9dr.png\" alt=\"Example PDF\"></a>.</p>\n\n<p>If currently use a simply idea which works-ish. I basically let pdf miner do its job and then use NTLK to find sentences.</p>\n\n<pre><code>def convert_pdf_to_txt(path, year):\n    rsrcmgr = PDFResourceManager()\n    retstr = StringIO()\n    codec = 'utf-8'\n    laparams = LAParams()\n    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n    fp = open(path, 'rb')\n    interpreter = PDFPageInterpreter(rsrcmgr, device)\n    password = \"\"\n    maxpages = 0\n    caching = True\n    pagenos=set()\n\n    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n        interpreter.process_page(page)\n\n    text = retstr.getvalue()\n\n    sentences = sent_tokenize(text)\n\n    size = len(sentences)\n    i = 0\n    path = path[:-3]\n\n\n    output = open(\"out.txt\", 'w')\n    for s in sentences:\n        s = s.replace(\"-\\n\", '') #remove hyphens\n        lines = s.split(\"\\n\")\n        for line in lines:\n            if(line.isupper()): #section are only uppercase. \n            #however, other things are also only uppercase hence my errors \n                line = \"--SECTION-- \" +line\n                output.write(\"\\n\\n\"+line+\"\\n\")\n            else:\n                output.write(line)\n        output.write(\"\\n\")\n    fp.close()\n    device.close()\n    retstr.close()\n</code></pre>\n\n<p>This gives me on the whole file the following output:</p>\n\n<pre><code>12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758SIG Proceedings Paper in LaTeX Format∗Extended Abstract†\n\n--SECTION-- G.K.M.\n\nTobin§Dublin, OhioSean Fogartywebmaster@marysville-ohio.comLars Thørväld¶The Thørväld GroupHekla, Icelandlarst@affiliation.orgCharles PalmerInstitute for Clarity in DocumentationInstitute for Clarity in DocumentationBen Trovato‡Dublin, Ohiotrovato@corporation.comLawrence P. LeipunerBrookhaven Laboratorieslleipuner@researchlabs.orgJohn SmithPalmer Research LaboratoriesSan Antonio, Texascpalmer@prl.comJulius P. KumquatNASA Ames Research CenterMoffett Field, Californiafogartys@amesres.orgThe Thørväld Groupjsmith@affiliation.orgThe Kumquat Consortiumjpkumquat@consortium.netcolumns), a specified set of fonts (Arial or Helvetica and TimesRoman) in certain specified sizes, a specified live area, centeredon the page, specified size of margins, specified column width andgutter size.\n\n\n--SECTION-- ABSTRACT\nThis paper provides a sample of a LATEX document which conforms,somewhat loosely, to the formatting guidelines for ACM SIG Proceedings.1Unpublishedworkingdraft.\nNotfordistribution.\n\n\n--SECTION-- CCS CONCEPTS\n• Computer systems organization → Embedded systems; Redundancy; Robotics; • Networks → Network reliability;\n\n--SECTION-- KEYWORDS\nACM proceedings, LATEX, text taggingACM Reference Format:Ben Trovato, G.K.M.\nTobin, Lars Thørväld, Lawrence P. Leipuner, SeanFogarty, Charles Palmer, John Smith, and Julius P. Kumquat.\n1997.\n\n\n--SECTION-- SIG\nProceedings Paper in LaTeX Format: Extended Abstract.\nIn Proceedings ofACM Woodstock conference (WOODSTOCK’97).\nACM, New York, NY, USA,5 pages.\nhttps://doi.org/10.475/123_4\n\n--SECTION-- 2 THE BODY OF THE PAPER\nTypically, the body of a paper is organized into a hierarchical structure, with numbered or unnumbered headings for sections, subsections, sub-subsections, and even smaller sections.\nThe command\\section that precedes this paragraph is part of such a hierarchy.3LATEX handles the numbering and placement of these headings foryou, when you use the appropriate heading commands aroundthe titles of the headings.\nIf you want a sub-subsection or smallerpart to be unnumbered in your output, simply append an asteriskto the command name.\nExamples of both numbered and unnumbered headings will appear throughout the balance of this sampledocument.\nBecause the entire article is contained in the document environment, you can indicate the start of a new paragraph with a blankline in your input file; that is why this sentence forms a separateparagraph.\n\n\n--SECTION-- 1 INTRODUCTION\nThe proceedings are the records of a conference.2 ACM seeks to givethese conference by-products a uniform, high-quality appearance.\nTo do this, ACM has some rigid requirements for the format of theproceedings documents: there is a specified format (balanced double∗Produces the permission block, and copyright information†The full version of the author’s guide is available as acmart.pdf document‡Dr.\nTrovato insisted his name be first.\n§The secretary disavows any knowledge of this author’s actions.\n¶This author is the one who did all the really hard work.\n1This is an abstract footnote2This is a footnotePermission to make digital or hard copies of part or all of this work for personal orUnpublished working draft.\nNot for distribution.\nclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page.\nCopyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nWOODSTOCK’97, July 1997, El Paso, Texas USA© 2016 Copyright held by the owner/author(s).\n\n\n--SECTION-- ACM ISBN 123-4567-24-567/08/06.\n\nhttps://doi.org/10.475/123_4Submission ID: 123-A12-B3.\n2018-10-20 12:29.\nPage 1 of 1–5.\n2.1 Type Changes and Special CharactersWe have already seen several typeface changes in this sample.\nYou can indicate italicized words or phrases in your text with thecommand \\textit; emboldening with the command \\textbf andtypewriter-style (for instance, for computer code) with \\texttt.\nBut remember, you do not have to indicate typestyle changes whensuch changes are part of the structural elements of your article;for instance, the heading of this subsection will be in a sans serif4typeface, but that is handled by the document class file.\nTake carewith the use of5 the curly braces in typeface changes; they mark thebeginning and end of the text that is to be in the different typeface.\n3This is a footnote.\n4Another footnote here.\nLet’s make this a rather long one to see how it looks.\n5Another footnote.\n5960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116WOODSTOCK’97, July 1997, El Paso, Texas USAB. Trovato et al.\nYou can use whatever symbols, accented characters, or nonEnglish characters you need anywhere in your document; you canfind a complete list of what is available in the LATEX User’s Guide[26].\n2.2 Math EquationsYou may want to display math equations in three distinct styles:inline, numbered or non-numbered display.\nEach of the three arediscussed in the next sections.\nTable 1: Frequency of Special CharactersNon-English or Math\n\n--SECTION-- Ø\nπ$2\n\n--SECTION-- Ψ\n1Frequency Comments1 in 1,0001 in 54 in 5For Swedish namesCommon in mathUsed in business1 in 40,000 Unexplained usagemand, using \\cite.\nThis article shows only the plainest form of the citation comInline (In-text) Equations.\nA formula that appears in the2.2.1running text is called an inline or in-text formula.\nIt is producedby the math environment, which can be invoked with the usual\\begin .\n.\n.\n\\end construction or with the short form $ .\n.\n.\n$.\nYou can use any of the symbols and structures, from α to ω, availablein LATEX [26]; this section will simply show a few examples of intext equations in context.\nNotice how this equation: limn→∞ x = 0,set here in in-line math style, looks slightly different when set indisplay style.\n(See next section).\n2.2.2 Display Equations.\nA numbered display equation—one setoff by vertical space from the text and centered horizontally—isproduced by the equation environment.\nAn unnumbered displayequation is produced by the displaymath environment.\nAgain, in either environment, you can use any of the symbolsand structures available in LATEX; this section will just give a coupleof examples of display equations in context.\nFirst, consider theequation, shown as an inline equation above:Some examples.\nA paginated journal article [2], an enumeratedjournal article [11], a reference to an entire issue [10], a monograph(whole book) [25], a monograph/whole book in a series (see 2ain spec.\ndocument) [18], a divisible-book such as an anthology orcompilation [13] followed by the same example, however we onlyoutput the series if the volume number is given [14] (so Editor00a’sseries should NOT be present since it has no vol.\nno.\n), a chapterin a divisible book [37], a chapter in a divisible book in a series[12], a multi-volume work as book [24], an article in a proceedings(of a conference, symposium, workshop for example) (paginatedproceedings article) [4], a proceedings article with all possible elements [36], an example of an enumerated proceedings article [16],an informally published work [17], a doctoral dissertation [9], amaster’s thesis: [5], an online document / world wide web resource[1, 30, 38], a video game (Case 1) [29] and (Case 2) [28] and [27] and(Case 3) a patent [35], work accepted for publication [31], ’YYYYb’test for prolific author [32] and [33].\nOther cites might contain’duplicate’ DOI and URLs (some SIAM articles) [23].\nBoris / BarbaraBeeton: multi-volume works as books [21] and [20].\nUnpublishedworkingdraft.\nNotfordistribution.\n2.4 TablesBecause tables cannot be split across pages, the best placement forthem is typically the top of the page nearest their initial cite.\nTo ensure this proper “floating” placement of tables, use the environmenttable to enclose the table’s contents and the table caption.\nThe contents of the table itself must go in the tabular environment, to bealigned properly in rows and columns, with the desired horizontaland vertical rules.\nAgain, detailed instructions on tabular materialare found in the LATEX User’s Guide.\nImmediately following this sentence is the point at which Table 1is included in the input file; compare the placement of the tablehere with the table in the printed output of this document.\nNotice how it is formatted somewhat differently in the displaymath environment.\nNow, we’ll enter an unnumbered equation:A couple of citations with DOIs: [22, 23].\nOnline citations: [38–40].\njust to demonstrate LATEX’s able handling of numbering.\nand follow it with another numbered equation:x + 1∫ π +2(1)(2)limn→∞ x = 0∞i =0∞i =0xi =0f2.3 CitationsCitations to articles [6–8, 19], conference proceedings [8] or maybebooks [26, 34] listed in the Bibliography section of your article willoccur throughout the text of your article.\nYou should use BibTeX toautomatically produce this bibliography; you simply need to insertone of several citation commands with a key of the item cited in theproper location in the .tex file [26].\nThe key is a short referenceyou invent to uniquely identify each work; in this sample document,the key is the first author’s surname and a word from the title.\nThisidentifying key is included with each item in the .bib file for yourarticle.\nThe details of the construction of the .bib file are beyond thescope of this sample document, but more information can be foundin the Author’s Guide, and exhaustive details in the LATEX User’sGuide by Lamport [26].\nTo set a wider table, which takes up the whole width of the page’slive area, use the environment table* to enclose the table’s contentsand the table caption.\nAs with a single-column table, this widetable will “float” to a location deemed more desirable.\nImmediatelyfollowing this sentence is the point at which Table 2 is included inthe input file; again, it is instructive to compare the placement ofthe table here with the table in the printed output of this document.\nIt is strongly recommended to use the package booktabs [15]and follow its main principles of typography with respect to tables:(1) Never, ever use vertical rules.\n(2) Never use double rules.\nSubmission ID: 123-A12-B3.\n2018-10-20 12:29.\nPage 2 of 1–5.\n117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232SIG Proceedings Paper in LaTeX FormatWOODSTOCK’97, July 1997, El Paso, Texas USATable 2: Some Typical CommandsCommand A Number Comments\\author\\table\\table*100300400AuthorFor tablesFor wider tablesDefinition 2.2.\nIf z is irrational, then by ez we mean the uniquenumber that has logarithm z:f (x)д(x) = L.(cid:21)log ez = z.\nThe pre-defined theorem-like constructs are theorem, conjecture, proposition, lemma and corollary.\nThe pre-defined definition-like constructs are example and definition.\nYou can add yourown constructs using the amsthm interface [3].\nThe styles used inthe \\theoremstyle command are acmplain and acmdefinition.\nAnother construct is proof, for example,Proof.\nSuppose on the contrary there exists a real number Lsuch thatFigure 1: A sample black and white graphic.\nIt is also a good idea not to overuse horizontal rules.\nFigure 2: A sample black and white graphic that has beenresized with the includegraphics command.\nUnpublishedworkingdraft.\nNotfordistribution.\nlim(cid:20)x→∞дx · f (x)д(x)f (x) = limx→c2.5 FiguresLike tables, figures cannot be split across pages; the best placementfor them is typically the top or the bottom of the page nearest theirinitial cite.\nTo ensure this proper “floating” placement of figures,use the environment figure to enclose the figure and its caption.\nThis sample document contains examples of .eps files to bedisplayable with LATEX.\nIf you work with pdfLATEX, use files in the.pdf format.\nNote that most modern TEX systems will convert .epsto .pdf for you on the fly.\nMore details on each of these are foundin the Author’s Guide.\nAs was the case with tables, you may want a figure that spans twocolumns.\nTo do this, and still to ensure proper “floating” placementof tables, use the environment figure* to enclose the figure and itscaption.\nAnd don’t forget to end the environment with figure*, notfigure!\n= limx→cд(x)· limx→cf (x)д(x) = 0·L = 0,Thenl = limx→cwhich contradicts our assumption that l (cid:44) 0.\n\n\n--SECTION-- 3 CONCLUSIONS\nThis paragraph will end the body of this sample document.\nRemember that you might still have Acknowledgments or Appendices;brief samples of these follow.\nThere is still the Bibliography to dealwith; and we will make a disclaimer about that here: with the exception of the reference to the LATEX book, the citations in this paperare to articles which have nothing to do with the present subjectand are used as examples only.\n□\n\n--SECTION-- A HEADINGS IN APPENDICES\nThe rules about hierarchical headings discussed above for the bodyof the article are different in the appendices.\nIn the appendix environment, the command section is used to indicate the start ofeach Appendix, with alphabetic order designation (i.e., the first isA, the second B, etc.)\nand a title (if you include one).\nSo, if you needhierarchical structure within an Appendix, start with subsectionas the highest level.\nHere is an outline of the body of this documentin Appendix-appropriate form:2.6 Theorem-like ConstructsOther common constructs that may occur in your article are theforms for logical constructs like theorems, axioms, corollaries andproofs.\nACM uses two types of these constructs: theorem-like anddefinition-like.\nHere is a theorem:Theorem 2.1.\nLet f be continuous on [a, b].\nIf G is an antiderivative for f on [a, b], then∫ baHere is a definition:f (t) dt = G(b) − G(a).\nSubmission ID: 123-A12-B3.\n2018-10-20 12:29.\nPage 3 of 1–5.\nA.1 IntroductionA.2 The Body of the PaperA.2.1 Type Changes and Special Characters.\nA.2.2 Math Equations.\nInline (In-text) Equations.\nDisplay Equations.\nA.2.3 Citations.\n233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348WOODSTOCK’97, July 1997, El Paso, Texas USAB. Trovato et al.\nFigure 4: A sample black and white graphic that has beenresized with the includegraphics command.\nA.2.4 Tables.\n\n\n--SECTION-- A.2.5\nFigures.\nA.2.6 Theorem-like Constructs.\nFigure 3: A sample black and white graphic that needs to span two columns of text.\nUnpublishedworkingdraft.\nNotfordistribution.\n(Nov. 1996).\nbooktabs.\nMathematical Society.\nhttp://www.ctan.org/pkg/amsthm.\n[2] Patricia S. Abril and Robert Plant.\n2007.\nThe patent holder’s dilemma: Buy, sell,or troll?\nCommun.\nACM 50, 1 (Jan. 2007), 36–44.\nhttps://doi.org/10.1145/1188913.\n1188915[3] American Mathematical Society 2015.\nUsing the amsthm Package.\nAmerican[4] Sten Andler.\n1979.\nPredicate Path expressions.\nIn Proceedings of the 6th.\n\n\n--SECTION-- ACM\nSIGACT-SIGPLAN symposium on Principles of Programming Languages (POPL ’79).\nACM Press, New York, NY, 226–236.\nhttps://doi.org/10.1145/567752.567774[5] David A. Anisi.\n2003.\nOptimal Motion Control of a Ground Vehicle.\nMaster’s thesis.\n[6] Mic Bowman, Saumya K. Debray, and Larry L. Peterson.\n1993.\nReasoning AboutNaming Systems.\nACM Trans.\nProgram.\nLang.\nSyst.\n15, 5 (November 1993), 795–825. https://doi.org/10.1145/161468.161471[7] Johannes Braams.\n1991.\nBabel, a Multilingual Style-Option System for Use withRoyal Institute of Technology (KTH), Stockholm, Sweden.\nLaTeX’s Standard Document Styles.\nTUGboat 12, 2 (June 1991), 291–301.\nTeX Users Group, 84–89.\n[8] Malcolm Clark.\n1991.\nPost Congress Tristesse.\nIn TeX90 Conference Proceedings.\n[9] Kenneth L. Clarkson.\n1985.\nAlgorithms for Closest-Point Problems (ComputationalGeometry).\nPh.D. Dissertation.\nStanford University, Palo Alto, CA.\nUMI OrderNumber: AAT 8506171.\n[10] Jacques Cohen (Ed.).\n1996.\nSpecial issue: Digital Libraries.\nCommun.\n\n\n--SECTION-- ACM 39, 11\n[11] Sarah Cohen, Werner Nutt, and Yehoshua Sagic.\n2007.\nDeciding equivalancesamong conjunctive aggregate queries.\nJ. ACM 54, 2, Article 5 (April 2007),50 pages.\nhttps://doi.org/10.1145/1219092.1219093[12] Bruce P. Douglass, David Harel, and Mark B. Trakhtenbrot.\n1998.\nStatecarts inuse: structured analysis and object-orientation.\nIn Lectures on Embedded Systems,Grzegorz Rozenberg and Frits W. Vaandrager (Eds.).\nLecture Notes in ComputerScience, Vol.\n1494.\nSpringer-Verlag, London, 368–394.\nhttps://doi.org/10.1007/3-540-65193-4_29[13] Ian Editor (Ed.).\n2007.\nThe title of book one (1st.\ned.).\nThe name of the seriesone, Vol.\n9.\nUniversity of Chicago Press, Chicago.\nhttps://doi.org/10.1007/3-540-09237-4Chicago, Chapter 100. https://doi.org/10.1007/3-540-09237-4[14] Ian Editor (Ed.).\n2008.\nThe title of book two (2nd.\ned.).\nUniversity of Chicago Press,[15] Simon Fear.\n2005.\nPublication quality tables in LATEX.\nhttp://www.ctan.org/pkg/[16] Matthew Van Gundy, Davide Balzarotti, and Giovanni Vigna.\n2007.\nCatch me, ifyou can: Evading network signatures with web-based polymorphic worms.\nInProceedings of the first USENIX workshop on Offensive Technologies (WOOT ’07).\nUSENIX Association, Berkley, CA, Article 7, 9 pages.\n[17] David Harel.\n1978.\nLOGICS of Programs: AXIOMATICS and DESCRIPTIVE POWER.\nMIT Research Lab Technical Report TR-200.\nMassachusetts Institute of Technology, Cambridge, MA.\n[18] David Harel.\n1979.\nFirst-Order Dynamic Logic.\nLecture Notes in Computer Science,Vol.\n68.\nSpringer-Verlag, New York, NY.\nhttps://doi.org/10.1007/3-540-09237-4[19] Maurice Herlihy.\n1993.\nA Methodology for Implementing Highly ConcurrentData Objects.\nACM Trans.\nProgram.\nLang.\nSyst.\n15, 5 (November 1993), 745–770.\nhttps://doi.org/10.1145/161468.161469[20] Lars Hörmander.\n1985.\nThe analysis of linear partial differential operators.\n\n\n--SECTION-- III.\n\nGrundlehren der Mathematischen Wissenschaften [Fundamental Principles ofMathematical Sciences], Vol.\n275.\nSpringer-Verlag, Berlin, Germany.\nviii+525pages.\nPseudodifferential operators.\n[21] Lars Hörmander.\n1985.\nThe analysis of linear partial differential operators.\n\n\n--SECTION-- IV.\n\nGrundlehren der Mathematischen Wissenschaften [Fundamental Principles ofMathematical Sciences], Vol.\n275.\nSpringer-Verlag, Berlin, Germany.\nvii+352pages.\nFourier integral operators.\nA Caveat for the TEX Expert.\nA.3 ConclusionsA.4 ReferencesGenerated by bibtex from your .bib file.\nRun latex, then bibtex, thenlatex twice (to resolve references) to create the .bbl file.\nInsert that.bbl file into the .tex source file and comment out the command\\thebibliography.\n\n\n--SECTION-- B MORE HELP FOR THE HARDY\nOf course, reading the source code is always useful.\nThe file acmart.\npdf contains both the user guide and the commented code.\n\n\n--SECTION-- ACKNOWLEDGMENTS\nThe authors would like to thank Dr. Yuhua Li for providing theMATLAB code of the BEPS method.\nThe authors would also like to thank the anonymous refereesfor their valuable comments and helpful suggestions.\nThe work issupported by the National Natural Science Foundation of Chinaunder Grant No.\n: 61273304 and Young Scientists’ Support Program(http://www.nnsf.cn/youngscientists).\n\n\n--SECTION-- REFERENCES\n[1] Rafal Ablamowicz and Bertfried Fauser.\n2007.\nCLIFFORD: a Maple 11 Package forClifford Algebra Computations, version 11.\nRetrieved February 28, 2008 fromhttp://math.tntech.edu/rafal/cliff11/index.htmlSubmission ID: 123-A12-B3.\n2018-10-20 12:29.\nPage 4 of 1–5.\n349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464SIG Proceedings Paper in LaTeX FormatWOODSTOCK’97, July 1997, El Paso, Texas USAAlgorithms (3rd.\ned.).\nAddison Wesley Longman Publishing Co., Inc.New York, NY.\n\n\n--SECTION-- [22] IEEE 2004.\n\nIEEE TCSC Executive Committee.\nIn Proceedings of the IEEE International Conference on Web Services (ICWS ’04).\nIEEE Computer Society, Washington,\n\n--SECTION-- DC, USA, 21–22.\n\nhttps://doi.org/10.1109/ICWS.2004.64[23] Markus Kirschmer and John Voight.\n2010.\nAlgorithmic Enumeration of IdealClasses for Quaternion Orders.\nSIAM J. Comput.\n39, 5 (Jan. 2010), 1714–1747.\nhttps://doi.org/10.1137/080734467[24] Donald E. Knuth.\n1997.\nThe Art of Computer Programming, Vol.\n1: Fundamental[25] David Kosiur.\n2001.\nUnderstanding Policy-Based Networking (2nd.\ned.).\nWiley,[26] Leslie Lamport.\n1986.\nLATEX: A Document Preparation System.\nAddison-Wesley,[27] Newton Lee.\n2005.\nInterview with Bill Kinder: January 13, 2005.\nVideo.\nComput.\nEntertain.\n3, 1, Article 4 (Jan.-March 2005).\nhttps://doi.org/10.1145/1057270.\n1057278[28] Dave Novak.\n2003.\nSolder man.\nVideo.\nIn ACM SIGGRAPH 2003 Video Review onAnimation theater Program: Part I - Vol.\n145 (July 27–27, 2003).\nACM Press, NewYork, NY, 4. https://doi.org/99.9999/woot07-S422[29] Barack Obama.\n2008.\nA more perfect union.\nVideo.\nRetrieved March 21, 2008Reading, MA.\nfrom http://video.google.com/videoplay?docid=6528042696351994555[30] Poker-Edge.Com.\n2006.\nStats and Analysis.\nRetrieved June 7, 2006 from http://www.poker-edge.com/stats.phpArticle 5 (July 2008).\nTo appear.\n[31] Bernard Rous.\n2008.\nThe Enabling of Digital Libraries.\nDigital Libraries 12, 3,[32] Mehdi Saeedi, Morteza Saheb Zamani, and Mehdi Sedighi.\n2010.\nA library-basedsynthesis methodology for reversible logic.\nMicroelectron.\n\n\n--SECTION-- J.\n\n41, 4 (April 2010),185–194.\n[33] Mehdi Saeedi, Morteza Saheb Zamani, Mehdi Sedighi, and Zahra Sasanian.\n2010.\nSynthesis of Reversible Circuit Using Cycle-Based Approach.\nJ. Emerg.\nTechnol.\nComput.\nSyst.\n6, 4 (Dec. 2010).\n\n\n--SECTION-- [34] S.L.\n\nSalas and Einar Hille.\n1978.\nCalculus: One and Several Variable.\nJohn Wileyand Sons, New York.\n[35] Joseph Scientist.\n2009.\nThe fountain of youth.\nPatent No.\n12345, Filed July 1st.,Unpublishedworkingdraft.\nNotfordistribution.\n2008, Issued Aug.\n9th., 2009.\n[36] Stan W. Smith.\n2010.\nAn experiment in bibliographic mark-up: Parsing metadatafor XML export.\nIn Proceedings of the 3rd.\nannual workshop on Librarians andComputers (LAC ’10), Reginald N. Smythe and Alexander Noble (Eds.\n), Vol.\n3.\nPaparazzi Press, Milan Italy, 422–431.\nhttps://doi.org/99.9999/woot07-S422In DistributedSystems (2nd.\ned.\n), Sape Mullender (Ed.).\nACM Press, New York, NY, 19–33.\nhttps://doi.org/10.1145/90417.90738[38] Harry Thornburg.\n2001.\nIntroduction to Bayesian Statistics.\nRetrieved March 2,[37] Asad Z. Spector.\n1990.\nAchieving application requirements.\n2005 from http://ccrma.stanford.edu/~jos/bayes/bayes.html\n\n--SECTION-- [39] TUG 2017.\n\nInstitutional members of the TEX Users Group.\nRetrieved May 27,[40] Boris Veytsman.\n[n. d.].\nacmart—Class for typesetting publications of ACM.\n2017 from http://wwtug.org/instmem.htmlRetrieved May 27, 2017 from http://www.ctan.org/pkg/acmartSubmission ID: 123-A12-B3.\n2018-10-20 12:29.\nPage 5 of 1–5.\n465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580\n</code></pre>\n\n<p>This output is partially right. I mean all section are rightfully detected but I also get a lot of false positives. Can you think of any better way (less false-positive prone) to implement this.</p>\n\n<p>PS: if you need the pdf, it is available <a href=\"https://www.acm.org/binaries/content/assets/publications/consolidated-tex-template/acmart-master.zip\" rel=\"noreferrer\">here</a>, filename is </p>\n\n<blockquote>\n  <p>sample-sigconf-authordraft.pdf</p>\n</blockquote>\n",
    "score": 5,
    "creation_date": 1541730653,
    "view_count": 10227,
    "answer_count": 1,
    "tags": "python;pdf;nlp;text-processing;pdfminer"
  },
  {
    "question_id": 52881494,
    "title": "Removing particular string in python pandas column",
    "body": "<p>I have a data frame with a column gender. It consists of predictions of the gender. Now the gender column has values such as mostly_male, mostly_female. I want to remove mostly. So I tried<code>df['gender'] = df['gender'].map(lambda x: x.lstrip('mostly_'))</code></p>\n\n<p>But I got a column with values of 'male' corresponding to 'ale'</p>\n",
    "score": 5,
    "creation_date": 1539892194,
    "view_count": 9450,
    "answer_count": 3,
    "tags": "python;python-3.x;pandas;nlp"
  },
  {
    "question_id": 52371951,
    "title": "How can a machine learning model handle unseen data and unseen label?",
    "body": "<p>I am trying to solve a text classification problem. I have a limited number of labels that capture the category of my text data. If the incoming text data doesn't fit any label, it is tagged as 'Other'. In the below example, I built a text classifier to classify text data as 'breakfast' or 'italian'. In the test scenario, I included couple of text data that do not fit into the labels that I used for training. This is the challenge that I'm facing. Ideally, I want the model to say - 'Other' for 'i like hiking' and 'everyone should understand maths'. How can I do this? </p>\n\n<pre><code>import numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nX_train = np.array([\"coffee is my favorite drink\",\n                    \"i like to have tea in the morning\",\n                    \"i like to eat italian food for dinner\",\n                    \"i had pasta at this restaurant and it was amazing\",\n                    \"pizza at this restaurant is the best in nyc\",\n                    \"people like italian food these days\",\n                    \"i like to have bagels for breakfast\",\n                    \"olive oil is commonly used in italian cooking\",\n                    \"sometimes simple bread and butter works for breakfast\",\n                    \"i liked spaghetti pasta at this italian restaurant\"])\n\ny_train_text = [\"breakfast\",\"breakfast\",\"italian\",\"italian\",\"italian\",\n                \"italian\",\"breakfast\",\"italian\",\"breakfast\",\"italian\"]\n\nX_test = np.array(['this is an amazing italian place. i can go there every day',\n                   'i like this place. i get great coffee and tea in the morning',\n                   'bagels are great here',\n                   'i like hiking',\n                   'everyone should understand maths'])\n\nclassifier = Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', MultinomialNB())])\n\nclassifier.fit(X_train, y_train_text)\npredicted = classifier.predict(X_test)\nproba = classifier.predict_proba(X_test)\nprint(predicted)\nprint(proba)\n\n['italian' 'breakfast' 'breakfast' 'italian' 'italian']\n[[0.25099411 0.74900589]\n [0.52943091 0.47056909]\n [0.52669142 0.47330858]\n [0.42787443 0.57212557]\n [0.4        0.6       ]]\n</code></pre>\n\n<p>I consider the 'Other' category as noise and I cannot model this category. </p>\n",
    "score": 5,
    "creation_date": 1537200923,
    "view_count": 3888,
    "answer_count": 4,
    "tags": "machine-learning;scikit-learn;nlp;text-classification;naivebayes"
  },
  {
    "question_id": 49387699,
    "title": "Extracting the person names in the named entity recognition in NLP using Python",
    "body": "<p>I have a sentence for which i need to identify the Person names alone:</p>\n\n<p>For example:</p>\n\n<pre><code>sentence = \"Larry Page is an American business magnate and computer scientist who is the co-founder of Google, alongside Sergey Brin\"\n</code></pre>\n\n<p>I have used the below code to identify the NERs.</p>\n\n<pre><code>from nltk import word_tokenize, pos_tag, ne_chunk\nprint(ne_chunk(pos_tag(word_tokenize(sentence))))\n</code></pre>\n\n<p>The output i received was:</p>\n\n<pre><code>(S\n  (PERSON Larry/NNP)\n  (ORGANIZATION Page/NNP)\n  is/VBZ\n  an/DT\n  (GPE American/JJ)\n  business/NN\n  magnate/NN\n  and/CC\n  computer/NN\n  scientist/NN\n  who/WP\n  is/VBZ\n  the/DT\n  co-founder/NN\n  of/IN\n  (GPE Google/NNP)\n  ,/,\n  alongside/RB\n  (PERSON Sergey/NNP Brin/NNP))\n</code></pre>\n\n<p>I want to extract all the person names, such as</p>\n\n<pre><code>Larry Page\nSergey Brin\n</code></pre>\n\n<p>In order to achieve this, I refereed this <a href=\"https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk\">link</a> and tried this. </p>\n\n<pre><code>from nltk.tag.stanford import StanfordNERTagger\nst = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz','/usr/share/stanford-ner/stanford-ner.jar')\n</code></pre>\n\n<p>However i continue to get this error: </p>\n\n<pre><code>LookupError: Could not find stanford-ner.jar jar file at /usr/share/stanford-ner/stanford-ner.jar\n</code></pre>\n\n<p>Where can i download this file?</p>\n\n<p>As informed above, the result that i am expecting in the form of list or dictionary is :</p>\n\n<pre><code>Larry Page\nSergey Brin\n</code></pre>\n",
    "score": 5,
    "creation_date": 1521558218,
    "view_count": 18653,
    "answer_count": 2,
    "tags": "python;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 48431173,
    "title": "Is there a way to get only the IDF values of words using scikit or any other python package?",
    "body": "<p>I have a text column in my dataset and using that column I want to have a IDF calculated for all the words that are present. TFID implementations in scikit, like <em><code>tfidf</code> vectorize</em>, are giving me TFIDF values directly as against just word IDFs. Is there a way to get word IDFs give a set of documents?</p>\n",
    "score": 5,
    "creation_date": 1516826187,
    "view_count": 5503,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp;tf-idf;tfidfvectorizer"
  },
  {
    "question_id": 40595828,
    "title": "Retrieving the start and end character indices in the original document, for those sentences returned by Spacy",
    "body": "<p>I am using something similar to the following pattern to retrieve the start and end indices of Spacy's sentences in the original document:</p>\n\n<pre><code>nlp = spacy.en.English()\ndoc = nlp(fulltext)\n\ntot = 0\nprev_end=0\nfor sent in doc.sents:\n    x = re.search(re.escape(sent.text), fulltext)\n    print (x.start(), x.end(), \"&gt;&gt;&gt;\", sent.text)\n    tot += (x.end()-prev_end)\n    prev_end = x.end()\n\nif len(fulltext) == tot: print (\"works\")\n</code></pre>\n\n<p>This seems to work for those few test docs I used. But worried if I am overlooking any 'gotchas' like spacy sometimes stripping off some characters that I am not aware of . Am I?</p>\n\n<p>PS: If it helps, I need these indices to compare with indices I have from Brat's annotation file.</p>\n",
    "score": 5,
    "creation_date": 1479149743,
    "view_count": 3862,
    "answer_count": 1,
    "tags": "nlp;spacy"
  },
  {
    "question_id": 28470165,
    "title": "How to plot SVC classification for an unbalanced dataset with scikit-learn and matplotlib?",
    "body": "<p>I have a text classification task with 2599 documents and five labels from 1 to 5. The documents are</p>\n\n<pre><code>label | texts\n----------\n5     |1190\n4     |839\n3     |239\n1     |204\n2     |127\n</code></pre>\n\n<p>All ready classified this textual data with very low performance, and also get warnings about ill defined metrics:</p>\n\n<pre><code>Accuracy: 0.461057692308\n\nscore: 0.461057692308\n\nprecision: 0.212574195636\n\nrecall: 0.461057692308\n\n  'precision', 'predicted', average, warn_for)\n confussion matrix:\n[[  0   0   0   0 153]\n  'precision', 'predicted', average, warn_for)\n [  0   0   0   0  94]\n [  0   0   0   0 194]\n [  0   0   0   0 680]\n [  0   0   0   0 959]]\n\n clasification report:\n             precision    recall  f1-score   support\n\n          1       0.00      0.00      0.00       153\n          2       0.00      0.00      0.00        94\n          3       0.00      0.00      0.00       194\n          4       0.00      0.00      0.00       680\n          5       0.46      1.00      0.63       959\n\navg / total       0.21      0.46      0.29      2080\n</code></pre>\n\n<p>Clearly this is happening by the fact that I have an unbalanced dataset, so I found this <a href=\"http://sci2s.ugr.es/keel/pdf/specific/congreso/akbani_svm_2004.pdf\" rel=\"nofollow noreferrer\">paper</a> where the authors propose several aproaches to deal with this issue:</p>\n\n<blockquote>\n  <p>The problem is that with imbalanced datasets, the learned boundary is\n  too close to the positive instances. We need to bias SVM in a way that\n  will push the boundary away from the positive instances. Veropoulos et\n  al [14] suggest using different error costs for the positive (C +) and\n  negative (C - ) classes</p>\n</blockquote>\n\n<p>I know that this could be very complicated but SVC offers several hyper parameters, So my question is: Is there any way to bias SVC in a way that push the boundary away from possitive instances with the hyper parameters that offer SVC classifier?. I know that this could be a difficult problem but any help is welcome, thanks in advance guys.</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ntfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=False, ngram_range=(2,2))\nfrom sklearn.cross_validation import train_test_split, cross_val_score\n\nimport pandas as pd\ndf = pd.read_csv('/path/of/the/file.csv',\n                     header=0, sep=',', names=['id', 'text', 'label'])\n\n\n\nreduced_data = tfidf_vect.fit_transform(df['text'].values)\ny = df['label'].values\n\n\n\nfrom sklearn.decomposition.truncated_svd import TruncatedSVD\nsvd = TruncatedSVD(n_components=5)\nreduced_data = svd.fit_transform(reduced_data)\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(reduced_data,\n                                                    y, test_size=0.33)\n\n#with no weights:\n\nfrom sklearn.svm import SVC\nclf = SVC(kernel='linear', class_weight={1: 10})\nclf.fit(reduced_data, y)\nprediction = clf.predict(X_test)\n\nw = clf.coef_[0]\na = -w[0] / w[1]\nxx = np.linspace(-5, 5)\nyy = a * xx - clf.intercept_[0] / w[1]\n\n\n# get the separating hyperplane using weighted classes\nwclf = SVC(kernel='linear', class_weight={1: 10})\nwclf.fit(reduced_data, y)\n\nww = wclf.coef_[0]\nwa = -ww[0] / ww[1]\nwyy = wa * xx - wclf.intercept_[0] / ww[1]\n\n# plot separating hyperplanes and samples\nimport matplotlib.pyplot as plt\nh0 = plt.plot(xx, yy, 'k-', label='no weights')\nh1 = plt.plot(xx, wyy, 'k--', label='with weights')\nplt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)\nplt.legend()\n\nplt.axis('tight')\nplt.show()\n</code></pre>\n\n<p>But I get nothing and I cant understand what happened, this is the plot:</p>\n\n<p><img src=\"https://i.sstatic.net/9eEqX.png\" alt=\"weighted vs normal\"></p>\n\n<p>then:</p>\n\n<pre><code>#Let's show some metrics[unweighted]:\nfrom sklearn.metrics.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, accuracy_score\nprint '\\nAccuracy:', accuracy_score(y_test, prediction)\nprint '\\nscore:', clf.score(X_train, y_train)\nprint '\\nrecall:', recall_score(y_test, prediction)\nprint '\\nprecision:', precision_score(y_test, prediction)\nprint '\\n clasification report:\\n', classification_report(y_test, prediction)\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, prediction)\n\n#Let's show some metrics[weighted]:\nprint 'weighted:\\n'\n\nfrom sklearn.metrics.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, accuracy_score\nprint '\\nAccuracy:', accuracy_score(y_test, prediction)\nprint '\\nscore:', wclf.score(X_train, y_train)\nprint '\\nrecall:', recall_score(y_test, prediction)\nprint '\\nprecision:', precision_score(y_test, prediction)\nprint '\\n clasification report:\\n', classification_report(y_test, prediction)\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, prediction)\n</code></pre>\n\n<p>This is the <a href=\"http://pastebin.com/0cwiLGG2\" rel=\"nofollow noreferrer\">data</a> that Im using. How can I fix this and plot in a right way this problem?. thanks in advance guys!.</p>\n\n<p>From an answer in this question I remove this lines:</p>\n\n<pre><code>#\n# from sklearn.decomposition.truncated_svd import TruncatedSVD\n# svd = TruncatedSVD(n_components=5)\n# reduced_data = svd.fit_transform(reduced_data)\n\n\n#\n# w = clf.coef_[0]\n# a = -w[0] / w[1]\n# xx = np.linspace(-10, 10)\n# yy = a * xx - clf.intercept_[0] / w[1]\n\n# ww = wclf.coef_[0]\n# wa = -ww[0] / ww[1]\n# wyy = wa * xx - wclf.intercept_[0] / ww[1]\n#\n# # plot separating hyperplanes and samples\n# import matplotlib.pyplot as plt\n# h0 = plt.plot(xx, yy, 'k-', label='no weights')\n# h1 = plt.plot(xx, wyy, 'k--', label='with weights')\n# plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)\n# plt.legend()\n#\n# plt.axis('tight')\n# plt.show()\n\nThis where the results:\n\nAccuracy: 0.787878787879\n\nscore: 0.779437105112\n\nrecall: 0.787878787879\n\nprecision: 0.827705441238\n</code></pre>\n\n<p>This metrics improved. <strong>How can I plot this results in order to have a nice example like the documentation one. I would like to see the behavior of the two hyper planes?</strong>. Thanks guys!</p>\n",
    "score": 5,
    "creation_date": 1423719847,
    "view_count": 2534,
    "answer_count": 5,
    "tags": "machine-learning;nlp;artificial-intelligence;scikit-learn;svm"
  },
  {
    "question_id": 27475658,
    "title": "Difference between Semantic Web and NLP?",
    "body": "<p>What exactly is the difference between Semantic Web and Natural Language Processing? </p>\n\n<p>Is Semantic Web a part of Natural Language Processing?</p>\n",
    "score": 5,
    "creation_date": 1418602390,
    "view_count": 3239,
    "answer_count": 5,
    "tags": "nlp;terminology;semantics;semantic-web"
  },
  {
    "question_id": 24363145,
    "title": "Quick NLTK parse into syntax tree",
    "body": "<p>I am trying to parse several hundreds of sentences into their syntax trees and i need to do that fast, the problem is that if i use NLTK then i need to define a grammar, and i cant know that i only know its gonna be english. I tried using <a href=\"https://github.com/emilmont/pyStatParser\" rel=\"noreferrer\">this</a> statistical parser, and it works great for my purposes however the speed could be a lot better, is there a way to use nltk parsing without a grammar?\nIn this snippet i am using a processing pool to do the processing in \"parallel\" but the speed leaves a lot to be desired.</p>\n\n<pre><code>import pickle\nimport re\nfrom stat_parser.parser import Parser\nfrom multiprocessing import Pool\nimport HTMLParser\ndef multy(a):\n    global parser\n    lst=re.findall('(\\S.+?[.!?])(?=\\s+|$)',a[1])\n    if len(lst)==0:\n        lst.append(a[1])\n    try:\n        ssd=parser.norm_parse(lst[0])\n    except:\n        ssd=['NNP','nothing']\n    with open('/var/www/html/internal','a') as f:\n        f.write(\"[[ss\")\n        pickle.dump([a[0],ssd], f)\n        f.write(\"ss]]\")\nif __name__ == '__main__':\n    parser=Parser()\n    with open('/var/www/html/interface') as f:\n        data=f.read()\n    data=data.split(\"\\n\")\n    p = Pool(len(data))\n    Totalis_dict=dict()\n    listed=list()\n    h = HTMLParser.HTMLParser()\n    with open('/var/www/html/internal','w') as f:\n        f.write(\"\")\n    for ind,each in enumerate(data):\n        listed.append([str(ind),h.unescape(re.sub('[^\\x00-\\x7F]+','',each))])\n    p.map(multy,listed)\n</code></pre>\n",
    "score": 5,
    "creation_date": 1403517589,
    "view_count": 4810,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 21800325,
    "title": "Are start and end states in HMM, necessary when implementing the Viterbi Algorithm for POS tagging?",
    "body": "<p>I do not fully understand how to use the start and end states in the Hidden Markov Model. Are these necessary in order to design and implement the transition and emission matrices?</p>\n",
    "score": 5,
    "creation_date": 1392481742,
    "view_count": 1902,
    "answer_count": 2,
    "tags": "nlp;hidden-markov-models;viterbi"
  },
  {
    "question_id": 11463396,
    "title": "Using WordNet to determine semantic similarity between two texts?",
    "body": "<p>How can you determine the semantic similarity between two texts in python using WordNet? </p>\n\n<p>The obvious preproccessing would be removing stop words and stemming, but then what?</p>\n\n<p>The only way I can think of would be to calculate the WordNet path distance between each word in the two texts. This is standard for unigrams. But these are large (400 word) texts, that are natural language documents, with words that are not in any particular order or structure (other than those imposed by English grammar). So, which words would you compare between texts? How would you do this in python? </p>\n",
    "score": 5,
    "creation_date": 1342146952,
    "view_count": 7567,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet;semantic-analysis"
  },
  {
    "question_id": 10588912,
    "title": "Fake reviews datasets",
    "body": "<p>There are datasets with usual mail spam in the Internet, but I need datasets with fake reviews to conduct some research and I can't find any of them. \nCan anybody give me advices on where fake reviews datasets can be obtained?</p>\n",
    "score": 5,
    "creation_date": 1337019883,
    "view_count": 4947,
    "answer_count": 2,
    "tags": "nlp;spam;review;corpus"
  },
  {
    "question_id": 9984026,
    "title": "Algorithm to compare similarity of ideas (as strings)",
    "body": "<p>Consider an arbitrary text box that records the answer to the question, what do you want to do before you die?</p>\n\n<p>Using a collection of response strings (max length 240), I'd like to somehow sort and group them and count them by idea (which may be just string similarity as described in <a href=\"https://stackoverflow.com/questions/653157/a-better-similarity-ranking-algorithm-for-variable-length-strings\">this question</a>).</p>\n\n<ol>\n<li>Is there another or better way to do something like this?</li>\n<li><em>Is this any different</em> than string similarity?</li>\n<li>Is this the right question to be asking?</li>\n</ol>\n\n<p>The idea here is to have people write in a text box over and over again, and me to provide a number that describes, generally speaking, that 802 people <em>wrote approximately the same thing</em></p>\n",
    "score": 5,
    "creation_date": 1333401711,
    "view_count": 1247,
    "answer_count": 3,
    "tags": "algorithm;nlp;artificial-intelligence"
  },
  {
    "question_id": 8551053,
    "title": "Automatic semantic role labeling in FrameNet",
    "body": "<p>I would like to do automatic semantic role labeling in FrameNet Lexicon using some machine learning methods. Could you please suggest me some java packages most suitable for this project?</p>\n",
    "score": 5,
    "creation_date": 1324202979,
    "view_count": 1933,
    "answer_count": 2,
    "tags": "java;nlp;machine-learning"
  },
  {
    "question_id": 4003840,
    "title": "How to build a conceptual search engine?",
    "body": "<p>I would like to build an internal search engine (I have a very large collection of thousands of XML files) that is able to map queries to concepts.  For example, if I search for \"big cats\", I would want highly ranked results to return documents with \"large cats\" as well.  But I may also be interested in having it return \"huge animals\", albeit at a much lower relevancy score.  </p>\n\n<p>I'm currently reading through the Natural Language Processing in Python book, and it seems WordNet has some word mappings that might prove useful, though I'm not sure how to integrate that into a search engine.  Could I use Lucene to do this? How?</p>\n\n<p>From further research, it seems \"latent semantic analysis\" is relevant to what I'm looking for but I'm not sure how to implement it.  </p>\n\n<p>Any advice on how to get this done?</p>\n",
    "score": 5,
    "creation_date": 1287835102,
    "view_count": 2123,
    "answer_count": 4,
    "tags": "python;search;lucene;nlp;lsa"
  },
  {
    "question_id": 3943646,
    "title": "Input CNF for SAT4J solver",
    "body": "<p>I a totally new to sat4j solver..</p>\n\n<p>it says some cnf file should be given as input</p>\n\n<p>is there any possible way to give the rule as input and get whether it is satisfiable or not?</p>\n\n<p>my rule will be of the kind :</p>\n\n<pre><code>Problem = ( \n\n     ( staff_1         &lt;=&gt;          staff_2 ) AND \n     ( doctor_1        &lt;=&gt;      physician_2 ) \n\n) AND ( \n\n     ( staff_1         AND         doctor_1 )\n\n)  AND (\n\n    NOT( ward_2             AND physician_2 ) AND \n    NOT( clinic_2           AND physician_2 ) AND \n    NOT( admission_record_2 AND physician_2 ) \n\n) AND (\n\n   NOT( hospital_2          AND physician_2 ) AND \n   NOT( department_2        AND physician_2 ) AND \n   NOT( staff_2             AND physician_2 )\n)\n</code></pre>\n\n<p>Can someone help me how to solve this using sat4j solver?</p>\n",
    "score": 5,
    "creation_date": 1287156199,
    "view_count": 4311,
    "answer_count": 3,
    "tags": "java;nlp;ontology;sat-solvers;sat4j"
  },
  {
    "question_id": 3690195,
    "title": "True definition of an English word?",
    "body": "<p>What would be the best definition of an English word?</p>\n\n<p>What are the other cases of an English word than just <code>\\w+</code>?\nSome may include <code>\\w+-\\w+</code> or <code>\\w+'\\w+</code>; some may exclude cases like <code>\\b[0-9]+\\b</code>. But I haven't seen \nany general consensus on those cases. \nDo we have a formal defintion of such?\nCan any of you clarify?</p>\n\n<p>(Edit: broaden the question so it doesn't depend on regexp only.)</p>\n",
    "score": 5,
    "creation_date": 1284190307,
    "view_count": 669,
    "answer_count": 6,
    "tags": "regex;nlp"
  },
  {
    "question_id": 68343073,
    "title": "&#39;Seq2SeqModelOutput&#39; object has no attribute &#39;logits&#39; BART transformers",
    "body": "<p>I am trying to generate summary of long PDF. So, what I did, first I converted my pdf to text using <code>pdfminer.six</code> library. Next, I used 2 functions which were provided in a discuss <a href=\"https://github.com/huggingface/transformers/issues/4224#issuecomment-694650789\" rel=\"noreferrer\">here</a>.</p>\n<p>The code:</p>\n<pre><code>bart_tokenizer = BartTokenizer.from_pretrained(&quot;facebook/bart-large&quot;)\nbart_model = BartModel.from_pretrained(&quot;facebook/bart-large&quot;, return_dict=True)\n\n# generate chunks of text \\ sentences &lt;= 1024 tokens\ndef nest_sentences(document):\n  nested = []\n  sent = []\n  length = 0\n  for sentence in nltk.sent_tokenize(document):\n    length += len(sentence)\n    if length &lt; 1024:\n      sent.append(sentence)\n    else:\n      nested.append(sent)\n      sent = [sentence]\n      length = len(sentence)\n\n  if sent:\n    nested.append(sent)\n  return nested\n\n# generate summary on text with &lt;= 1024 tokens\ndef generate_summary(nested_sentences):\n  device = 'cuda'\n  summaries = []\n  for nested in nested_sentences:\n    input_tokenized = bart_tokenizer.encode(' '.join(nested), truncation=True, return_tensors='pt')\n    input_tokenized = input_tokenized.to(device)\n    summary_ids = bart_model.to(device).generate(\n        input_tokenized,\n        length_penalty=3.0,\n        min_length=30,\n        max_length=100,\n    )\n    output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n    summaries.append(output)\n  summaries = [sentence for sublist in summaries for sentence in sublist]\n  return summaries\n</code></pre>\n<p>Then, to get the summary, I do:</p>\n<pre><code>nested_sentences = nest_sentences(text)\n</code></pre>\n<p>Where, <code>text</code> is a text of string having length around 10K which I converted using pdf library.</p>\n<pre><code>summary = generate_summary(nested_sentences)\n</code></pre>\n<p>Then, I get the following error:</p>\n<pre><code>---------------------------------------------------------------------------\n\nAttributeError                            Traceback (most recent call last)\n\n&lt;ipython-input-15-d5aa7709bb5f&gt; in &lt;module&gt;()\n----&gt; 1 summary = generate_summary(nested_sentences)\n\n3 frames\n\n&lt;ipython-input-11-8554509269e0&gt; in generate_summary(nested_sentences)\n     28         length_penalty=3.0,\n     29         min_length=30,\n---&gt; 30         max_length=100,\n     31     )\n     32     output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n\n/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)\n     26         def decorate_context(*args, **kwargs):\n     27             with self.__class__():\n---&gt; 28                 return func(*args, **kwargs)\n     29         return cast(F, decorate_context)\n     30 \n\n/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\n   1061                 return_dict_in_generate=return_dict_in_generate,\n   1062                 synced_gpus=synced_gpus,\n-&gt; 1063                 **model_kwargs,\n   1064             )\n   1065 \n\n/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py in beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\n   1799                 continue  # don't waste resources running the code we don't need\n   1800 \n-&gt; 1801             next_token_logits = outputs.logits[:, -1, :]\n   1802 \n   1803             # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`\n\nAttributeError: 'Seq2SeqModelOutput' object has no attribute 'logits'\n\n\n</code></pre>\n<p>I cannot find anything related to this error, so I would really appreciate it if anyone could help or is there any better approach to generate summary for long texts?</p>\n<p>Thank you in advance!</p>\n",
    "score": 5,
    "creation_date": 1626072881,
    "view_count": 10588,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers"
  },
  {
    "question_id": 62032372,
    "title": "Coherence score (u_mass) -18 is good or bad?",
    "body": "<p>I read this question (<a href=\"https://stackoverflow.com/questions/54762690/coherence-score-0-4-is-good-or-bad\">Coherence score 0.4 is good or bad?</a>)  and found that the coherence score (u_mass) is from -14 to 14. But when I did my experiments, I got a score of -18 for u_mass and 0.67 for c_v. I wonder how is my u_mass score out of range (-14, 14)?</p>\n\n<p>Update: I used gensim library and scanned   the numbers of topics from 2 to 50. For u_mass, it starts from 0 to the lowest negative point and turn back a bit, like an upsidedown version of c_v.</p>\n",
    "score": 5,
    "creation_date": 1590531767,
    "view_count": 16713,
    "answer_count": 3,
    "tags": "nlp;lda;topic-modeling;lsa;topicmodels"
  },
  {
    "question_id": 56968434,
    "title": "Bleu score in python from scratch",
    "body": "<p>After watching Andrew Ng's video about <a href=\"https://www.youtube.com/watch?v=DejHQYAGb7Q\" rel=\"nofollow noreferrer\">Bleu score</a> I wanted to implement one from scratch in python. I wrote the code full in python with numpy sparingly. This is the full code</p>\n\n<pre><code>import numpy as np\n\ndef n_gram_generator(sentence,n= 2,n_gram= False):\n    '''\n    N-Gram generator with parameters sentence\n    n is for number of n_grams\n    The n_gram parameter removes repeating n_grams \n    '''\n    sentence = sentence.lower() # converting to lower case\n    sent_arr = np.array(sentence.split()) # split to string arrays\n    length = len(sent_arr)\n\n    word_list = []\n    for i in range(length+1):\n        if i &lt; n:\n            continue\n        word_range = list(range(i-n,i))\n        s_list = sent_arr[word_range]\n        string = ' '.join(s_list) # converting list to strings\n        word_list.append(string) # append to word_list\n        if n_gram:\n            word_list = list(set(word_list))\n    return word_list\n\ndef bleu_score(original,machine_translated):\n    '''\n    Bleu score function given a orginal and a machine translated sentences\n    '''\n    mt_length = len(machine_translated.split())\n    o_length = len(original.split())\n\n    # Brevity Penalty \n    if mt_length&gt;o_length:\n        BP=1\n    else:\n        penality=1-(mt_length/o_length)\n        BP=np.exp(penality)\n\n    # calculating precision\n    precision_score = []\n    for i in range(mt_length):\n        original_n_gram = n_gram_generator(original,i)\n        machine_n_gram = n_gram_generator(machine_translated,i)\n        n_gram_list = list(set(machine_n_gram)) # removes repeating strings\n\n        # counting number of occurence \n        machine_score = 0\n        original_score = 0\n        for j in n_gram_list:\n            machine_count = machine_n_gram.count(j)\n            original_count = original_n_gram.count(j)\n            machine_score = machine_score+machine_count\n            original_score = original_score+original_count\n\n        precision = original_score/machine_score\n        precision_score.append(precision)\n    precisions_sum = np.array(precision_score).sum()\n    avg_precisions_sum=precisions_sum/mt_length\n    bleu=BP*np.exp(avg_precisions_sum)\n    return bleu\n\nif __name__ == \"__main__\":\n    original = \"this is a test\"\n    bs=bleu_score(original,original)\n    print(\"Bleu Score Original\",bs)\n</code></pre>\n\n<p>I tried to test my score with nltk's </p>\n\n<pre><code>from nltk.translate.bleu_score import sentence_bleu\nreference = [['this', 'is', 'a', 'test']]\ncandidate = ['this', 'is', 'a', 'test']\nscore = sentence_bleu(reference, candidate)\nprint(score)\n</code></pre>\n\n<p>The problem is my bleu score is about <code>2.718281</code> and nltk's is <code>1</code>. What am I doing wrong? </p>\n\n<p>Here are some possible reason's:</p>\n\n<p>1) I calculated ngrams with respect to the length of the machine translated sentence. Here from 1 to 4</p>\n\n<p>2) <code>n_gram_generator</code> function which I wrote myself and not sure about its accuracy</p>\n\n<p>3) Some how I used wrong function or miscalculated bleu score</p>\n\n<p>Can some one look my code up and tell me where I did the mistake?</p>\n",
    "score": 5,
    "creation_date": 1562753393,
    "view_count": 4180,
    "answer_count": 4,
    "tags": "python;machine-learning;nlp;nltk"
  },
  {
    "question_id": 52145992,
    "title": "How do we analyse a loss vs epochs graph?",
    "body": "<p>I'm training a language model and the loss vs epochs is plotted each time of training. I'm attaching two samples from it. </p>\n\n<p><a href=\"https://i.sstatic.net/LZp7x.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/LZp7x.png\" alt=\"image 1\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/DJrhv.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/DJrhv.png\" alt=\"image 2\"></a></p>\n\n<p>Obviously, the second one is showing better performance. But, from these graphs, when do we take a decision to stop training (early stopping)?</p>\n\n<p>Can we understand overfitting and underfitting from these graphs or do I need to plot additional learning curves?  </p>\n\n<p>What are the additional inferences that can be made from these plots? </p>\n",
    "score": 5,
    "creation_date": 1535963680,
    "view_count": 20984,
    "answer_count": 1,
    "tags": "machine-learning;nlp;lstm;pytorch;recurrent-neural-network"
  },
  {
    "question_id": 48602779,
    "title": "Searching for all matches in texts with Pandas",
    "body": "<p>I have a list of particular words ('tokens') and need to find all of them (if any of them are present) in plain texts. I prefer using Pandas, to load text and perform the search. I'm using pandas as my collection of short text are timestamped and it is quite easy to organise these short text in a single data structure as pandas. </p>\n\n<p><strong>For example:</strong></p>\n\n<p>Consider a collection of fetched twitters uploaded in Pandas:</p>\n\n<pre><code>                                              twitts\n0                       today is a great day for BWM\n1                    prices of german cars increased\n2             Japan introduced a new model of Toyota\n3  German car makers, such as BMW, Audi and VW mo...\n</code></pre>\n\n<p>and a list of car makers:</p>\n\n<pre><code>list_of_car_makers = ['BMW', 'Audi','Mercedes','Toyota','Honda', 'VW']\n</code></pre>\n\n<p>Ideally, I need to get the following data frame:</p>\n\n<pre><code>                                              twitts  cars_mentioned\n0                       today is a great day for BMW  [BMW]\n1                    prices of german cars increased  []\n2             Japan introduced a new model of Toyota  [Toyota]\n3  German car makers, such as BMW, Audi and VW mo...  [BMW, Audi, VW]\n</code></pre>\n\n<p>I'm very new to NLP and text mining methods, and I read/search on the internet a lot of materials on that topic. My guess is that I can use <code>regex</code> and use <code>re.findall()</code>, but then I need to iterate over the list of tokens (car makers) the entire dataframe.</p>\n\n<p>Are there more succinct ways of doing this simple task, especially with Panads? </p>\n",
    "score": 5,
    "creation_date": 1517697316,
    "view_count": 886,
    "answer_count": 3,
    "tags": "python;regex;pandas;nlp"
  },
  {
    "question_id": 46826541,
    "title": "Methods for creating training data for SpaCy models?",
    "body": "<p>I recently began a NLP journey using SpaCy, and I have ~5,500 strings which I want to label up. For the first 100, I did this using a spreadsheet with custom columns, which was then run through a script to generate Python dictionaries. In the sheet, I have strored the string, label type, label value. The script then works out the position of the label value from within the string.</p>\n\n<p>It's rather time consuming to product training data in this way, and it's open to error.</p>\n\n<p>Are there any tools available to assist with this? I literally just need the ability to highlight a substring, and then choose the label type. I could build it myself, but I feel it may already exist.</p>\n",
    "score": 5,
    "creation_date": 1508404612,
    "view_count": 6675,
    "answer_count": 3,
    "tags": "nlp;training-data;spacy"
  },
  {
    "question_id": 36780491,
    "title": "Predicting next word with text2vec in R",
    "body": "<p>I am building a language model in R to predict a next word in the sentence based on the previous words. Currently my model is a simple ngram model with Kneser-Ney smoothing. It predicts next word by finding ngram with maximum probability (frequency) in the training set, where smoothing offers a way to interpolate lower order ngrams, which can be advantageous in the cases where higher order ngrams have low frequency and may not offer a reliable prediction. While this method works reasonably well, it 'fails in the cases where the n-gram cannot not capture the context. For example, \"It is warm and sunny outside, let's go to the...\" and \"It is cold and raining outside, let's go to the...\" will suggest the same prediction, because the context of weather is not captured in the last n-gram (assuming n&lt;5). </p>\n\n<p>I am looking into more advanced methods and I found <a href=\"https://cran.r-project.org/web/packages/text2vec/index.html\" rel=\"nofollow\">text2vec</a> package, which allows to map words into vector space where words with similar meaning are represented with similar (close) vectors. I have a feeling that this representation can be helpful for the next word prediction, but i cannot figure out how exactly to define the training task. My quesiton is if text2vec is the right tool to use for next word prediction and if yes, what is the suitable prediction algorithm that can be used for this task?</p>\n",
    "score": 5,
    "creation_date": 1461272800,
    "view_count": 2649,
    "answer_count": 3,
    "tags": "r;nlp;n-gram;text2vec"
  },
  {
    "question_id": 36262860,
    "title": "How to use CNN to train input data of different size?",
    "body": "<p>CNN seems to be implemented mostly for fixed size input. Now I want to use CNN to train some sentences of different size, what are some common methods?</p>\n",
    "score": 5,
    "creation_date": 1459169881,
    "view_count": 10058,
    "answer_count": 2,
    "tags": "machine-learning;nlp;deep-learning"
  },
  {
    "question_id": 32939527,
    "title": "How to use cTAKES from the command line?",
    "body": "<p>I wonder how to use <a href=\"http://ctakes.apache.org/\" rel=\"noreferrer\">Apache cTAKES</a> from the command line.</p>\n\n<p>E.g. :</p>\n\n<ul>\n<li>I have a file note.txt that contains some text like \"Patient had\nelevated blood sugar but tests confirm no diabetes.  Patient's father had\nadult onset diabetes.\"</li>\n<li>I want to use the provided analysis engine\n<code>\\apache-ctakes-3.2.2-bin\\apache-ctakes-3.2.2\\desc\\ctakes-clinical-pipeline\\desc\\analysis_engine\\AggregatePlaintextUMLSProcessor.xml</code></li>\n</ul>\n\n<p>How can I get the analyse engine's output (viz. the annotations) using the\ncommand line (i.e. without using graphical user interfaces such as UIMA CAS\nVisual Debugger or the Collection Processing Engine)? I'd prefer to\nuse the provided JAR files rather than having to compile the code.</p>\n\n<p>The question is fairly simple but I couldn't find the information in\n<a href=\"https://svn.apache.org/repos/asf/ctakes/trunk/README\" rel=\"noreferrer\">cTAKES's README</a> or\non <a href=\"https://cwiki.apache.org/confluence/display/CTAKES/cTAKES+3.0\" rel=\"noreferrer\">Confluence</a>.</p>\n",
    "score": 5,
    "creation_date": 1444002051,
    "view_count": 3422,
    "answer_count": 2,
    "tags": "nlp;ctakes"
  },
  {
    "question_id": 32864480,
    "title": "Is natural language interface to database a dead end",
    "body": "<p>Just want to know whether the field called the natural language interface to database is a dead end\nNothing more has been developed on it since past 20 years and no latest papers are also being published.</p>\n\n<p>Asking any expert in the field to shed the light </p>\n",
    "score": 5,
    "creation_date": 1443611015,
    "view_count": 3721,
    "answer_count": 5,
    "tags": "database;user-interface;nlp"
  },
  {
    "question_id": 25806677,
    "title": "which prolog implementation will be helpful in my case",
    "body": "<p>I was going through Prolog. I want to use it for natural language processing. I came across this paper for <a href=\"http://www.cs.nmsu.edu/ALP/2011/03/natural-language-processing-with-prolog-in-the-ibm-watson-system/\" rel=\"nofollow\"><strong>natural language processing with Prolog in the IBM Watson system</strong></a>. As stated in the paper I want to try it out in some what similar way. \nNow I was wondering which of the Prolog implementation to use. I came across all of these <a href=\"http://en.wikipedia.org/wiki/Comparison_of_Prolog_implementations\" rel=\"nofollow\">Comparison onto Prolog onto wiki which is stated in this link</a>. So which one of these implementations can be used for the purpose of \nNLP using onto Ubunutu. Also the one which will easily integrate with python and good in speed. Has anyone ever worked any of these implementations. Is SWI-Prolog good?</p>\n\n<p>Help is appreciated. Thankz :)</p>\n",
    "score": 5,
    "creation_date": 1410519250,
    "view_count": 457,
    "answer_count": 2,
    "tags": "prolog;nlp"
  },
  {
    "question_id": 20844217,
    "title": "How to remove UIMA annotations?",
    "body": "<p>I'm using some UIMA annotators in a pipeline. It run tasks like: </p>\n\n<ul>\n<li>tokenizer </li>\n<li>sentence splitter</li>\n<li>gazetizer</li>\n<li><strong>My Annotator</strong> </li>\n</ul>\n\n<p>The problem is that I don't want to write ALL the annotations (Token, Sentence, SubToken, Time, myAnnotations, etc..) to the disk because the files gets very large quicky. </p>\n\n<p>I want to remove all the annotations and keep only the created by <strong>My Annotator</strong>.</p>\n\n<p>I'm working with the next libraries:</p>\n\n<ol>\n<li>uimaFIT 2.0.0</li>\n<li>ClearTK 1.4.1</li>\n<li>Maven</li>\n</ol>\n\n<p>And I'm using a <code>org.apache.uima.fit.pipeline.SimplePipeline</code> with:</p>\n\n<pre><code>SimplePipeline.runPipeline(\n    UriCollectionReader.getCollectionReaderFromDirectory(filesDirectory), //directory with text files\n    UriToDocumentTextAnnotator.getDescription(),\n    StanfordCoreNLPAnnotator.getDescription(),//stanford tokenize, ssplit, pos, lemma, ner, parse, dcoref\n    AnalysisEngineFactory.createEngineDescription(//\n        XWriter.class, \n        XWriter.PARAM_OUTPUT_DIRECTORY_NAME, outputDirectory,\n        XWriter.PARAM_FILE_NAMER_CLASS_NAME, ViewURIFileNamer.class.getName())\n);\n</code></pre>\n\n<p>What I'm trying to do is to use the Standford NLP annotator(from ClearTK) and remove the useless annotation.</p>\n\n<p>How do I do this?</p>\n\n<p>From what I know, you can use the <code>removeFromIndexes();</code> method from with an Annotation instance.</p>\n\n<p>Do I need to create an UIMA processor and add it to my pipeline?</p>\n",
    "score": 5,
    "creation_date": 1388423655,
    "view_count": 989,
    "answer_count": 3,
    "tags": "java;nlp;uima"
  },
  {
    "question_id": 16992883,
    "title": "Natural Language Processing - Features for Text Classification",
    "body": "<p>So I'm trying to classify texts using Weka SVM. So far, my feature vectors used for training the SVM are composed of TF-IDF statistics for unigrams and bigrams that appear in the training texts. But, the results I get from testing the trained SVM model haven't been accurate at all, so can someone give me feedback on my procedure? I am following these steps to classify texts:</p>\n\n<ol>\n<li>Construct a dictionary made up of extracted unigrams and bigrams from the training texts</li>\n<li>Count how many times each unigram/bigram appears in each training text, as well as how many training texts the unigram/bigram appears in</li>\n<li>Use the data from step 2 to calcuate the TF-IDF for each unigram/bigram </li>\n<li>For each document, construct a feature vector that is the length of the dictionary, and store the corresponding TF-IDF statistic in each element of the vector (so for example, the first element in the feature vector for document one would correspond to the TF-IDF for the first word in the dictionary relative to document one)</li>\n<li>Append class label to each feature vector to distinguish which text belongs to which author</li>\n<li>Train SVM using these feature vectors</li>\n<li>Feature vectors for the testing texts are constructed in the same way as the training texts, and are classified by the SVM</li>\n</ol>\n\n<p>Also, could it be that I need to train the SVM with more features? If so, what features are most effective in this case? Any help would be greatly appreciated, thanks.</p>\n",
    "score": 5,
    "creation_date": 1370638855,
    "view_count": 2631,
    "answer_count": 2,
    "tags": "java;nlp;weka;feature-selection"
  },
  {
    "question_id": 12514621,
    "title": "extracting the text from output parse Tree",
    "body": "<p>I am new to nlp, I am trying to use stanford parser to extract the (NP ) sentence from a text,  I want to retrieve the parts of the text where it's tagged (NP )</p>\n\n<p>if a part is tagged (NP ) and a smaller part inside it is also tagged (NP ) I want to take the smaller part.</p>\n\n<p>till now I managed to do what I wanted in the following method:</p>\n\n<pre><code>private static ArrayList&lt;Tree&gt; extract(Tree t) \n{\n    ArrayList&lt;Tree&gt; wanted = new ArrayList&lt;Tree&gt;();\n   if (t.label().value().equals(\"NP\") )\n    {\n       wanted.add(t);\n        for (Tree child : t.children())\n        {\n            ArrayList&lt;Tree&gt; temp = new ArrayList&lt;Tree&gt;();\n            temp=extract(child);\n            if(temp.size()&gt;0)\n            {\n                int o=-1;\n                o=wanted.indexOf(t);\n                if(o!=-1)\n                    wanted.remove(o);\n            }\n            wanted.addAll(temp);\n        }\n    }\n\n    else\n        for (Tree child : t.children())\n            wanted.addAll(extract(child));\n    return wanted;\n}\n</code></pre>\n\n<p>The return type of this method is a list of trees, When I do the following:</p>\n\n<pre><code>     LexicalizedParser parser = LexicalizedParser.loadModel();\n        x = parser.apply(\"Who owns club barcelona?\");\n     outs=extract(x);\n    for(int i=0;i&lt;outs.size();i++){System.out.println(\"tree #\"+i+\": \"+outs.get(i));}\n</code></pre>\n\n<p>is :</p>\n\n<pre><code>tree #0: (NP (NN club) (NN barcelona))\n</code></pre>\n\n<p>I want the output to be <code>\"club barcelona\"</code> right away, without the tags, I tried the <code>.labels();</code> property and <code>.label().value();</code> they return the tags instead</p>\n",
    "score": 5,
    "creation_date": 1348150690,
    "view_count": 3722,
    "answer_count": 1,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 11083036,
    "title": "How to get the first sentence from the following paragraph?",
    "body": "<p>I know this might sound easy. I thought about using the first dot(.) which comes as the benchmark, but when abbreviations and short forms come, I am rendered helpless. </p>\n\n<p>e.g. - </p>\n\n<blockquote>\n  <p>Sir Winston Leonard Spencer-Churchill, KG, OM, CH, TD, PC, DL, FRS,\n  Hon. RA (30 November 1874 – 24 January 1965) was a British politician\n  and statesman known for his leadership of the United Kingdom during\n  the Second World War. He is widely regarded as one of the great\n  wartime leaders and served as Prime Minister twice. A noted statesman\n  and orator, Churchill was also an officer in the British Army, a\n  historian, a writer, and an artist.</p>\n</blockquote>\n\n<p>Here, the 1st dot is Hon., but I want the complete first line ending at Second World War .</p>\n\n<p>Is it possible people ???</p>\n",
    "score": 5,
    "creation_date": 1340022965,
    "view_count": 2936,
    "answer_count": 6,
    "tags": "python;nlp;text-segmentation"
  },
  {
    "question_id": 7400533,
    "title": "which similarity function of nltk.corpus.wordnet is Appropriate for find similarity of two words?",
    "body": "<p>which similarity function in <code>nltk.corpus.wordnet</code> is Appropriate for find similarity of two words?</p>\n\n<pre><code> path_similarity()?\n    lch_similarity()?\n    wup_similarity()?\n    res_similarity()?\n    jcn_similarity()?\n    lin_similarity()?\n</code></pre>\n\n<p>I want use a function for <code>word clustering</code> and <code>yarowsky</code> algorightm for find similar <code>collocation</code> in a large text.</p>\n",
    "score": 5,
    "creation_date": 1315910576,
    "view_count": 2162,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet;corpus"
  },
  {
    "question_id": 7086708,
    "title": "Comparing two English strings for similarities",
    "body": "<p>So here is my problem. I have two paragraphs of text and I need to see if they are similar. Not in the sense of string metrics but in meaning. The following two paragraphs are related but I need to find out if they cover the 'same' topic. Any help or direction to solving this problem would be greatly appreciated. </p>\n\n<blockquote>\n  <p>Fossil fuels are fuels formed by natural processes such as anaerobic\n  decomposition of buried dead organisms. The age of the organisms and\n  their resulting fossil fuels is typically millions of years, and\n  sometimes exceeds 650 million years. The fossil fuels, which contain\n  high percentages of carbon, include coal, petroleum, and natural gas.\n  Fossil fuels range from volatile materials with low carbon:hydrogen\n  ratios like methane, to liquid petroleum to nonvolatile materials\n  composed of almost pure carbon, like anthracite coal. Methane can be\n  found in hydrocarbon fields, alone, associated with oil, or in the\n  form of methane clathrates. It is generally accepted that they formed\n  from the fossilized remains of dead plants by exposure to heat and\n  pressure in the Earth's crust over millions of years. This biogenic\n  theory was first introduced by Georg Agricola in 1556 and later by\n  Mikhail Lomonosov in the 18th century.</p>\n</blockquote>\n\n<p>Second:</p>\n\n<blockquote>\n  <p>Fossil fuel reforming is a method of producing hydrogen or other\n  useful products from fossil fuels such as natural gas. This is\n  achieved in a processing device called a reformer which reacts steam\n  at high temperature with the fossil fuel. The steam methane reformer\n  is widely used in industry to make hydrogen. There is also interest in\n  the development of much smaller units based on similar technology to\n  produce hydrogen as a feedstock for fuel cells. Small-scale steam\n  reforming units to supply fuel cells are currently the subject of\n  research and development, typically involving the reforming of\n  methanol or natural gas but other fuels are also being considered such\n  as propane, gasoline, autogas, diesel fuel, and ethanol.</p>\n</blockquote>\n",
    "score": 5,
    "creation_date": 1313540850,
    "view_count": 349,
    "answer_count": 3,
    "tags": "algorithm;text;comparison;nlp;compare"
  },
  {
    "question_id": 5571519,
    "title": "Named entity recognition with Java",
    "body": "<p>I would like to use named entity recognition (NER) to find adequate tags for texts in a database. Instead of using tools like NLTK or Lingpipe I want to build my own tool.</p>\n\n<p>So my questions are:</p>\n\n<ul>\n<li><p>Which algorithm should I use?</p></li>\n<li><p>How hard is to build this tool?</p></li>\n</ul>\n",
    "score": 5,
    "creation_date": 1302116264,
    "view_count": 2724,
    "answer_count": 5,
    "tags": "java;tags;nlp;semantics"
  },
  {
    "question_id": 3388330,
    "title": "Where can I find get a dump of raw text on the web?",
    "body": "<p>I am looking to do some text analysis in a program I am writing. I am looking for alternate sources of text in its raw form similar to what is provided in the Wikipedia dumps (download.wikimedia.com).</p>\n\n<p>I'd rather not have to go through the trouble of crawling websites, trying to parse the html , extracting text etc.. </p>\n",
    "score": 5,
    "creation_date": 1280756681,
    "view_count": 666,
    "answer_count": 3,
    "tags": "parsing;text;nlp;wikipedia"
  },
  {
    "question_id": 1714657,
    "title": "find some sentences",
    "body": "<p>I'd like to find good way to find some (let it be two) sentences in some text. What will be better - use regexp or split-method? Your ideas?</p>\n\n<p>As requested by Jeremy Stein - there are some examples</p>\n\n<p><strong>Examples:</strong></p>\n\n<p><strong>Input:</strong></p>\n\n<blockquote>\n  <p>The first thing to do is to create the Comment model. We’ll create this in the normal way, but with one small difference. If we were just creating comments for an Article we’d have an integer field called article_id in the model to store the foreign key, but in this case we’re going to need something more abstract.</p>\n</blockquote>\n\n<p><strong>First two sentences:</strong></p>\n\n<blockquote>\n  <p>The first thing to do is to create the Comment model. We’ll create this in the normal way, but with one small difference.</p>\n</blockquote>\n\n<p><strong>Input:</strong></p>\n\n<blockquote>\n  <p>Mr. T is one mean dude.  I'd hate to get in a fight with him.</p>\n</blockquote>\n\n<p><strong>First two sentences:</strong></p>\n\n<blockquote>\n  <p>Mr. T is one mean dude.  I'd hate to get in a fight with him.</p>\n</blockquote>\n\n<p><strong>Input:</strong></p>\n\n<blockquote>\n  <p>The D.C. Sniper was executed was executed by lethal injection at a Virginia prison. Death was pronounced at 9:11 p.m. ET.</p>\n</blockquote>\n\n<p><strong>First two sentences:</strong></p>\n\n<blockquote>\n  <p>The D.C. Sniper was executed was executed by lethal injection at a Virginia prison. Death was pronounced at 9:11 p.m. ET.</p>\n</blockquote>\n\n<p><strong>Input:</strong></p>\n\n<blockquote>\n  <p>In her concluding remarks, the opposing attorney said that \"...in this and so many other instances, two wrongs won’t make a right.\"  The jury seemed to agree.</p>\n</blockquote>\n\n<p><strong>First two sentences:</strong></p>\n\n<blockquote>\n  <p>In her concluding remarks, the opposing attorney said that \"...in this and so many other instances, two wrongs won’t make a right.\"  The jury seemed to agree.</p>\n</blockquote>\n\n<p>Guys, as you can see - it's not so easy to determine two sentences from text. :(</p>\n",
    "score": 5,
    "creation_date": 1257939680,
    "view_count": 1383,
    "answer_count": 7,
    "tags": "ruby;regex;nlp"
  },
  {
    "question_id": 66892154,
    "title": "Phrase extraction with Spacy",
    "body": "<p>Does <code>spacy</code> have some APIs to do phrase* extraction as one would do when using <code>word2phrase</code> or the <code>Phrases</code> class from <code>gensim</code>? Thank you.</p>\n<p>PS. Phrases meant as collocations in Linguistics.</p>\n",
    "score": 5,
    "creation_date": 1617210094,
    "view_count": 3620,
    "answer_count": 2,
    "tags": "nlp;spacy;gensim;phrase"
  },
  {
    "question_id": 64805354,
    "title": "How to find singular in the plural when some letters change? What is the best approach?",
    "body": "<p>How can I find the singular in the plural when some letters change?</p>\n<p>Following situation:</p>\n<ul>\n<li>The German word <code>Schließfach</code> is a lockbox.</li>\n<li>The plural is <code>Schließfächer.</code></li>\n</ul>\n<p>As you see, the letter <code>a</code> has changed in <code>ä</code>. For this reason, the first word is not a substring of the second one anymore, they are &quot;regex-technically&quot; different.</p>\n<p>Maybe I'm not in the right corner with my chosen tags below. Maybe Regex is not the right tool for me. I've seen <code>naturaljs</code> (<code>natural.NounIflector()</code>) provides this functionality out of the box for English words. Maybe there are also solutions for the German language in the same way?</p>\n<p>What is the best approach, how can I find singular in the plural in German?</p>\n",
    "score": 5,
    "creation_date": 1605189625,
    "view_count": 777,
    "answer_count": 2,
    "tags": "javascript;nlp;diacritics"
  },
  {
    "question_id": 62728985,
    "title": "How do I translate using HuggingFace from Chinese to English?",
    "body": "<p>I want to translate from Chinese to English using HuggingFace's transformers using a pretrained <code>&quot;xlm-mlm-xnli15-1024&quot;</code> model. <a href=\"https://huggingface.co/transformers/usage.html#translation\" rel=\"noreferrer\">This tutorial</a> shows how to do it from English to German.</p>\n<p>I tried following the tutorial but it doesn't detail how to manually change the language or to decode the result. I am lost on where to start. Sorry that this question could not be more specific.</p>\n<p>Here is what I tried:</p>\n<pre><code>from transformers import AutoModelWithLMHead, AutoTokenizer\nbase_model = &quot;xlm-mlm-xnli15-1024&quot;\nmodel = AutoModelWithLMHead.from_pretrained(base_model)\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\ninputs = tokenizer.encode(&quot;translate English to Chinese: Hugging Face is a technology company based in New York and Paris&quot;, return_tensors=&quot;pt&quot;)\noutputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n\nprint(tokenizer.decode(outputs.tolist()[0]))\n</code></pre>\n<pre><code>'&lt;s&gt;translate english to chinese : hugging face is a technology company based in new york and paris &lt;/s&gt;china hug ™ ™ ™ ™ ™ ™ ™ ™ ™ ™ ™ ™ ™ ™ ™ ™ ™'\n</code></pre>\n",
    "score": 5,
    "creation_date": 1593864966,
    "view_count": 7745,
    "answer_count": 2,
    "tags": "nlp;translation;huggingface-transformers;machine-translation;huggingface-tokenizers"
  },
  {
    "question_id": 61866947,
    "title": "Install older (but stable) NLTK version compatible with python 2",
    "body": "<p>I would like to install an older (but stable) version of NLTK for python2.7.\nI tried to run the command: <code>pip install nltk===x.x.x</code> but the terminal reports many errors.\nI was wondering if there's a repository where nltk can be downloaded or whether there are some other ways to solve the problem. \nThanks  </p>\n",
    "score": 5,
    "creation_date": 1589795719,
    "view_count": 2681,
    "answer_count": 1,
    "tags": "python-2.7;nlp;nltk;python-2.x"
  },
  {
    "question_id": 61133531,
    "title": "How does spaCy generate vectors for phrases?",
    "body": "<p>Medium and large vocabularies of spaCy can generate vectors for words and phrases.  Let's consider the following example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\n    \nnlp = spacy.load(&quot;en_core_web_md&quot;)\ntokens = nlp(&quot;apple cat sky&quot;)\n    \nprint(tokens.text, tokens.vector[:3], tokens.vector_norm) # Only the first three components of the vector \n    \nfor token in tokens:\n    print(token.text, token.vector[:3], token.vector_norm)\n</code></pre>\n<p>Output:</p>\n<pre class=\"lang-none prettyprint-override\"><code>apple cat sky [-0.06734333  0.03672066 -0.13952099] 4.845729844425328\napple [-0.36391  0.43771 -0.20447] 7.1346846\ncat [-0.15067  -0.024468 -0.23368 ] 6.6808186\nsky [ 0.31255  -0.30308   0.019587] 6.617719\n</code></pre>\n<p>It is clear that the vocabulary contains vectors for each word, but how are the vectors for the entire phase generated? As one can see it is not just simple sum of vectors.</p>\n",
    "score": 5,
    "creation_date": 1586488763,
    "view_count": 6868,
    "answer_count": 2,
    "tags": "nlp;spacy;word2vec"
  },
  {
    "question_id": 57837315,
    "title": "How to install a language model",
    "body": "<p>I am exploring using NLP for some machine learning projects. I normally code all of my projects using python through Anaconda using either Jupyter notebooks or PyCharm as my IDE. </p>\n\n<p>I would like to start using spacy and am planning on attending a workshop on it in the near future. Two recommendations were made that I do first. Install spacy and install the <code>en_core_web_lg</code> language model. I completed the first step, just by searching for the spacy package in Anaconda environments (the conventional way) and installed it. However, as far as installing the language model, I am less familiar with how to do this to get this on my computer since it is not a traditional package.</p>\n\n<p>The spacy installation website cites here: <a href=\"https://spacy.io/models/en#en_core_web_lg\" rel=\"noreferrer\">https://spacy.io/models/en#en_core_web_lg</a> that this language model can be installed by using:</p>\n\n<pre><code>INSTALLATION\n\n$ python -m spacy download en_core_web_lg\n</code></pre>\n\n<p>I am assuming that this is a command through terminal? I am not very experienced using terminal but tried typing in the above command in one of the command lines and pressed enter and nothing happened. Is this the correct way to install this model? How should I install it? Also, for pedagogical purposes, what exactly is happening when we install the model? It exists on our computer and then can be utilized for NLP in say a Jupyter notebook if called. </p>\n\n<p>Sorry if these questions seem fairly basic, I am still trying to learn these new techniques. Any help, references, or advice would be greatly appreciated.</p>\n\n<p>Thanks.</p>\n",
    "score": 5,
    "creation_date": 1567888917,
    "view_count": 6468,
    "answer_count": 4,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 52039847,
    "title": "Excluding the Header and Footer Contents of a page of a PDF file while extracting text?",
    "body": "<p>Is it possible to exclude the <code>contents of footers and headers of a page</code> from a pdf file during extracting the text from it. As these contents are least important and almost redundant.</p>\n\n<p>Note: For extracting the text from the .pdf file, I am using the PyPDF2 package on python version = 3.7. </p>\n\n<p>How to exclude the contents of the footers and headers in PyPDF2. Any help is appreciated.</p>\n\n<p>The code snippet is as follows:</p>\n\n<pre><code>import PyPDF2\n\ndef Read(startPage, endPage):\n    global text\n    text = []\n    cleanText = \" \"\n    pdfFileObj = open('C:\\\\Users\\\\Rocky\\\\Desktop\\\\req\\\\req\\\\0000 - gamma j.pdf', 'rb')\n    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n    num_pages = pdfReader.numPages\n    print(num_pages)\n    while (startPage &lt;= endPage):\n        pageObj = pdfReader.getPage(startPage)\n        text += pageObj.extractText()\n        startPage += 1\n    pdfFileObj.close()\n    for myWord in text:\n        if myWord != '\\n':\n           cleanText += myWord\n    text = cleanText.strip().split()\n    print(text)\n\nRead(1, 1)\n</code></pre>\n",
    "score": 5,
    "creation_date": 1535374402,
    "view_count": 10490,
    "answer_count": 3,
    "tags": "python-3.x;pdf;text;nlp;pypdf"
  },
  {
    "question_id": 48356421,
    "title": "What is the difference between syntactic analogy and semantic analogy?",
    "body": "<p>At 15:10 of <a href=\"https://www.youtube.com/watch?v=CHcExDsDeHU&amp;feature=youtu.be&amp;t=15m\" rel=\"noreferrer\">this</a> video about fastText it mentions syntactic analogy and semantic analogy. But I am not sure what the difference is between them.</p>\n\n<p>Could anybody help explain the difference with examples?</p>\n",
    "score": 5,
    "creation_date": 1516453113,
    "view_count": 2303,
    "answer_count": 1,
    "tags": "nlp;word-embedding;fasttext"
  },
  {
    "question_id": 48197869,
    "title": "Text generation: character prediction RNN vs. word prediction RNN",
    "body": "<p>I've been researching text generation with RNNs, and it seems as though the common technique is to input text character by character, and have the RNN predict the next character.</p>\n\n<p>Why wouldn't you do the same technique but using words instead of characters.\nThis seems like a much better technique to me because the RNN won't make any typos and it will be faster to train.</p>\n\n<p>Am I missing something?</p>\n\n<p>Furthermore, is it possible to create a word prediction RNN but with somehow inputting words pre-trained on word2vec, so that the RNN can understand their meaning?</p>\n",
    "score": 5,
    "creation_date": 1515627876,
    "view_count": 2976,
    "answer_count": 1,
    "tags": "machine-learning;nlp;deep-learning;recurrent-neural-network"
  },
  {
    "question_id": 47523112,
    "title": "Detect stopword after lemma in Spacy",
    "body": "<p>How to detect if word is a stopword after stemming and lemmatization in <code>spaCy</code>?</p>\n\n<p>Assume sentence</p>\n\n<pre><code>s = \"something good\\nsomethings 2 bad\"\n</code></pre>\n\n<p>In this case <code>something</code> is a stopword. Obviously (to me?) <code>Something</code> and <code>somethings</code> are also stopwords, but it needs to stemmed before. Following script will say that the first is true, but latter isn't.</p>\n\n<pre><code>import spacy\nfrom spacy.tokenizer import Tokenizer\nnlp = spacy.load('en')\ntokenizer = Tokenizer(nlp.vocab)\n\ns = \"something good\\nSomething 2 somethings\"\ntokens = tokenizer(s)\n\nfor token in tokens:\n  print(token.lemma_, token.is_stop)\n</code></pre>\n\n<p>Returns:</p>\n\n<pre><code>something True\ngood False\n\"\\n\" False\nSomething False\n2 False\nsomethings False\n</code></pre>\n\n<p>Is there a way to detect that through <code>spaCy</code> API?</p>\n",
    "score": 5,
    "creation_date": 1511836991,
    "view_count": 3839,
    "answer_count": 1,
    "tags": "python;nlp;spacy;stop-words;lemmatization"
  },
  {
    "question_id": 47301140,
    "title": "How can I access the raw documents from the Brown corpus?",
    "body": "<p>For all other NLTK corpora, calling <code>corpus.raw()</code> yields the original text from the files.\nFor example:</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.corpus import webtext\n&gt;&gt;&gt; webtext.raw()[:10]\n'Cookie Man'\n</code></pre>\n\n<p>However, when calling <code>brown.raw()</code> you get tagged text.</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.corpus import brown\n&gt;&gt;&gt; brown.raw()[:10]\n'\\n\\n\\tThe/at '\n</code></pre>\n\n<p>I've read all the documentation I can find but can't seem to find an obvious explanation or way to get the un-tagged version. Is there a reason this corpus is tagged and the others aren't?</p>\n",
    "score": 5,
    "creation_date": 1510728902,
    "view_count": 12162,
    "answer_count": 2,
    "tags": "python;nlp;nltk;corpus;tagged-corpus"
  },
  {
    "question_id": 31234168,
    "title": "How do I calculate the shortest path (geodesic) distance between two adjectives in WordNet using Python NLTK?",
    "body": "<p>Computing the semantic similarity between two synsets in WordNet can be easily done with several built-in similarity measures, such as:</p>\n\n<pre><code>synset1.path_similarity(synset2)\n</code></pre>\n\n<p><code>synset1.lch_similarity(synset2)</code>, Leacock-Chodorow Similarity</p>\n\n<p><code>synset1.wup_similarity(synset2)</code>, Wu-Palmer Similarity</p>\n\n<p><a href=\"https://stackoverflow.com/questions/22031968/how-to-find-distance-between-two-synset-using-python-nltk-in-wordnet-hierarchy\">(as seen here)</a> </p>\n\n<p>However, all of these exploit WordNet's taxonomic relations, which are relations for nouns and verbs. Adjectives and adverbs are related via synonymy, antonymy and pertainyms. How can one measure the distance (number of hops) between two adjectives? </p>\n\n<p>I tried <code>path_similarity()</code>, but as expected, it returns <code>'None'</code>:</p>\n\n<pre><code>from nltk.corpus import wordnet as wn\nx = wn.synset('good.a.01')\ny = wn.synset('bad.a.01')\n\n\nprint(wn.path_similarity(x,y))\n</code></pre>\n\n<p>If there is any way to compute the distance between one adjective and another, pointing it out would be greatly appreciated.</p>\n",
    "score": 5,
    "creation_date": 1436124385,
    "view_count": 3829,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet;cosine-similarity"
  },
  {
    "question_id": 27865825,
    "title": "How to get dependency parse output exactly as online demo?",
    "body": "<p>How can I programmatically get the same dependency parse using stanford corenlp as seen in the online demo?</p>\n\n<p>I am using the corenlp package to obtain the dependency parse for the following sentence.</p>\n\n<p><strong>Second healthcare worker in Texas tests positive for Ebola , authorities say .</strong></p>\n\n<p>I try to obtain the parse programmatically using the code below</p>\n\n<pre><code>            Properties props = new Properties();\n            props.put(\"annotators\", \"tokenize, ssplit, pos, lemma, ner, parse, dcoref\");\n            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n\n            String text = \"Second healthcare worker in Texas tests positive for Ebola , authorities say .\"; // Add your text here!\n            Annotation document = new Annotation(text);\n            pipeline.annotate(document);\n            String[] myStringArray = {\"SentencesAnnotation\"};\n            List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);\n            for(CoreMap sentence: sentences) {\n                SemanticGraph dependencies = sentence.get(BasicDependenciesAnnotation.class);\n                IndexedWord root = dependencies.getFirstRoot();\n                System.out.printf(\"root(ROOT-0, %s-%d)%n\", root.word(), root.index());\n                for (SemanticGraphEdge e : dependencies.edgeIterable()) {\n                    System.out.printf (\"%s(%s-%d, %s-%d)%n\", e.getRelation().toString(), e.getGovernor().word(), e.getGovernor().index(), e.getDependent().word(), e.getDependent().index());\n                }\n            }\n\n    }\n</code></pre>\n\n<p>I get the following output using the stanford corenlp 3.5.0 package.</p>\n\n<pre><code>root(ROOT-0, worker-3)\namod(worker-3, Second-1)\nnn(worker-3, healthcare-2)\nprep(worker-3, in-4)\namod(worker-3, positive-7)\ndep(worker-3, say-12)\npobj(in-4, tests-6)\nnn(tests-6, Texas-5)\nprep(positive-7, for-8)\npobj(for-8, ebola-9)\nnsubj(say-12, authorities-11)\n</code></pre>\n\n<p>But the online demo gives a different answer that marks say as the root and has other relationships like ccomp between words in the parse.</p>\n\n<pre><code>amod(worker-3, Second-1)\nnn(worker-3, healthcare-2)\nnsubj(tests-6, worker-3)\nprep(worker-3, in-4)\npobj(in-4, Texas-5)\nccomp(say-12, tests-6)\nacomp(tests-6, positive-7)\nprep(positive-7, for-8)\npobj(for-8, Ebola-9)\nnsubj(say-12, authorities-11)\nroot(ROOT-0, say-12)\n</code></pre>\n\n<p>How can I resolve my output to match with the online demo?</p>\n",
    "score": 5,
    "creation_date": 1420823967,
    "view_count": 3883,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 21418946,
    "title": "AI: What kind of process would sites like Wit use to train Natural language",
    "body": "<p>I am working on a project where I would like to achieve a sense of natural language understanding. However, I am going to start small and would like to train it on specific queries. </p>\n\n<p>So for example, starting out I might tell it:</p>\n\n<p> songs.</p>\n\n<p>Then if it sees a sentence like \"Kanye Wests songs\" it can match against that. </p>\n\n<p>BUT then I would like to give it some extra sentences that could mean the same thing so that it eventually learns to be able to predict unknown sentences into a set that I have trained it on.</p>\n\n<p>So I might add the sentence: \"Songs by </p>\n\n<p>And of course  would be a database of names it can match agains.</p>\n\n<p>I came across a neat website, Wit.ai that does something like I talk about. However, they resolve their matches to an intent, where I would like to match it to a simplified query or BETTER a database like query (like facebook graph search).</p>\n\n<p>I understand a context free grammar would work well for this (anything else?). But what are good methods to train several CFG that I say have similar meaning and then when it sees unknown sentences it can try and predict.</p>\n\n<p>Any thoughts would be great.</p>\n\n<p>Basically I would like to be able to take a natural language sentence and convert it to some form that can be run better understood to my system and presented to the user in a nice way. Not sure if there is a better stackexchange for this!</p>\n",
    "score": 5,
    "creation_date": 1390950823,
    "view_count": 1345,
    "answer_count": 1,
    "tags": "regex;nlp;artificial-intelligence;wit.ai"
  },
  {
    "question_id": 7986900,
    "title": "Preserving only domain-specific keywords?",
    "body": "<p>I am trying to determine the most popular keywords for certain class of documents in my collection. Assuming that the domain is \"computer science\" (which of course, includes networking, computer architecture, etc.) what is the best way to preserve these domain-specific keywords from text? I tried using Wordnet but I am not quite how to best use it to extract this information.</p>\n\n<p>Are there any well-known list of words that I can use as a whitelist considering the fact that I am not aware of all domain-specific keywords beforehand? Or are there any good nlp/machine learning techniques to identity domain specific keywords?</p>\n",
    "score": 5,
    "creation_date": 1320265633,
    "view_count": 1195,
    "answer_count": 2,
    "tags": "python;nlp;machine-learning;nltk"
  },
  {
    "question_id": 4361971,
    "title": "Convert chinese characters to hanyu pinyin",
    "body": "<p>How to convert <strong><em>from</em></strong> chinese characters <strong><em>to</em></strong> hanyu pinyin?</p>\n\n<p>E.g.</p>\n\n<p>你 --> Nǐ</p>\n\n<p>马 --> Mǎ</p>\n\n<hr>\n\n<p>More Info:</p>\n\n<p>Either accents or numerical forms of hanyu pinyin are acceptable, the numerical form being my preference.</p>\n\n<p>A Java library is preferred, however, a library in another language that can be put in a wrapper is also OK.</p>\n\n<p>I would like anyone who has <strong><em>personally used</em></strong> such a library before to recommend or comment on it, in terms of its quality/ reliabilitty.</p>\n",
    "score": 5,
    "creation_date": 1291591674,
    "view_count": 4644,
    "answer_count": 4,
    "tags": "language-agnostic;nlp;cjk"
  },
  {
    "question_id": 1853378,
    "title": "Java Stanford NLP: Spell checking",
    "body": "<p>I'm trying to check spelling accuracy of text samples using the Stanford NLP. It's just a metric of the text, not a filter or anything, so if it's off by a bit it's fine, as long as the error is uniform.</p>\n\n<p>My first idea was to check if the word is known by the lexicon:</p>\n\n<pre><code>private static LexicalizedParser lp = new LexicalizedParser(\"englishPCFG.ser.gz\");\n\n@Analyze(weight=25, name=\"Spelling\")\n    public double spelling() {\n        int result = 0;\n\n        for (List&lt;? extends HasWord&gt; list : sentences) {\n            for (HasWord w : list) {\n                if (! lp.getLexicon().isKnown(w.word())) {\n                    System.out.format(\"misspelled: %s\\n\", w.word());\n                    result++;\n                }\n            }\n        }\n\n        return result / sentences.size();\n    }\n</code></pre>\n\n<p>However, this produces quite a lot of false positives:</p>\n\n<pre><code>misspelled: Sincerity\nmisspelled: Sisyphus\nmisspelled: Sisyphus\nmisspelled: fidelity\nmisspelled: negates\nmisspelled: gods\nmisspelled: henceforth\nmisspelled: atom\nmisspelled: flake\nmisspelled: Sisyphus\nmisspelled: Camus\nmisspelled: foandf\nmisspelled: foandf\nmisspelled: babby\nmisspelled: formd\nmisspelled: gurl\nmisspelled: pregnent\nmisspelled: babby\nmisspelled: formd\nmisspelled: gurl\nmisspelled: pregnent\nmisspelled: Camus\nmisspelled: Sincerity\nmisspelled: Sisyphus\nmisspelled: Sisyphus\nmisspelled: fidelity\nmisspelled: negates\nmisspelled: gods\nmisspelled: henceforth\nmisspelled: atom\nmisspelled: flake\nmisspelled: Sisyphus\n</code></pre>\n\n<p>Any ideas on how to do this better?</p>\n",
    "score": 5,
    "creation_date": 1260045374,
    "view_count": 4569,
    "answer_count": 2,
    "tags": "java;nlp;stanford-nlp;spell-checking"
  },
  {
    "question_id": 257125,
    "title": "(human) Language of a document",
    "body": "<p>Is there a way (a program, a library) to approximately know which language a document is written in?</p>\n\n<p>I have a bunch of text documents (~500K) in mixed languages to import in a i18n enabled CMS (Drupal)..</p>\n\n<p>I don't need perfect matches, only some guess.</p>\n",
    "score": 5,
    "creation_date": 1225648872,
    "view_count": 1014,
    "answer_count": 5,
    "tags": "nlp;classification;language-detection"
  },
  {
    "question_id": 75674773,
    "title": "Creating HuggingFace Dataset to train an BIO tagger",
    "body": "<p>I have a list of dictionaries:</p>\n<pre><code>sentences = [ \n{'text': ['I live in Madrid'], 'labels':[O, O, O, B-LOC]},\n{'text': ['Peter lives in Spain'], 'labels':[B-PER, O, O, B-LOC]},\n{'text': ['He likes pasta'], 'labels':[O, O, B-FOOD]},\n...\n]\n</code></pre>\n<p>I want to create a HuggingFace dataset object from this data so that I can later preprocess it and feed to a transformer model much more easily, but so far I have not found a viable way to do this.</p>\n",
    "score": 5,
    "creation_date": 1678287997,
    "view_count": 3260,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers;named-entity-recognition;huggingface-datasets"
  },
  {
    "question_id": 66290815,
    "title": "LightGBM on Numerical+Categorical+Text Features &gt;&gt; TypeError: Unknown type of parameter:boosting_type, got:dict",
    "body": "<p>Im trying to train a lightGBM model on a dataset consisting of numerical, Categorical and Textual data. However, during the training phase, i get the following error:</p>\n<pre><code>params = {\n'num_class':5,\n'max_depth':8,\n'num_leaves':200,\n'learning_rate': 0.05,\n'n_estimators':500\n}\n\nclf = LGBMClassifier(params)\ndata_processor = ColumnTransformer([\n    ('numerical_processing', numerical_processor, numerical_features),\n    ('categorical_processing', categorical_processor, categorical_features),\n    ('text_processing_0', text_processor_1, text_features[0]),\n    ('text_processing_1', text_processor_1, text_features[1])\n                                    ]) \npipeline = Pipeline([\n    ('data_processing', data_processor),\n    ('lgbm', clf)\n                    ])\npipeline.fit(X_train, y_train)\n</code></pre>\n<p>and the error is:</p>\n<pre><code>TypeError: Unknown type of parameter:boosting_type, got:dict\n</code></pre>\n<p>Here's my pipeline:\n<a href=\"https://i.sstatic.net/TLiPT.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/TLiPT.png\" alt=\"enter image description here\" /></a></p>\n<p>I basically have two textual features, both are some form of names on which im performing stemming mainly .</p>\n<p>Any pointers would be highly appreciated.</p>\n",
    "score": 5,
    "creation_date": 1613819338,
    "view_count": 5597,
    "answer_count": 2,
    "tags": "python;machine-learning;scikit-learn;nlp;lightgbm"
  },
  {
    "question_id": 63032396,
    "title": "How can i remove strings from sentences if string matches with strings in list",
    "body": "<p>I have a <code>pandas.Series</code> with sentences like this:</p>\n<pre><code>0    mi sobrino carlos bajó conmigo el lunes       \n1    juan antonio es un tio guay                   \n2    voy al cine con ramón                         \n3    pepe el panadero siempre se porta bien conmigo\n4    martha me hace feliz todos los días \n</code></pre>\n<p>on the other hand, I have a list of names and surnames like this:</p>\n<p><code>l = ['juan', 'antonio', 'esther', 'josefa', 'mariano', 'cristina', 'carlos']</code></p>\n<p>I want to match sentences from the series to the names in the list. The real data is much much bigger than this examples, so I thought that element-wise comparison between the series and the list was not going to be efficient, so I created a big string containing all the strings in the name list like this:</p>\n<p><code>'|'.join(l)</code></p>\n<p>I tried to create a boolean mask that later allows me to index the sentences that contains the names in the name list by true or false value like this:</p>\n<pre><code>series.apply(lambda x: x in '|'.join(l))\n</code></pre>\n<p>but it returns:</p>\n<pre><code>0    False\n1    False\n2    False\n3    False\n4    False\n</code></pre>\n<p>which is clearly not ok.</p>\n<p>I also tried using <code>str.contains()</code> but it doesn't behave as I expect, because this method will look if any substring in the series is present in the name list, and this is not what I need (i.e. I need an exact match).</p>\n<p>Could you please point me in the right direction here?</p>\n<p>Thank you very much in advance</p>\n",
    "score": 5,
    "creation_date": 1595414664,
    "view_count": 976,
    "answer_count": 6,
    "tags": "python;python-3.x;pandas;list;nlp"
  },
  {
    "question_id": 62175452,
    "title": "Topic modeling on short texts Python",
    "body": "<p>I want to do topic modeling on short texts. I did some research on LDA and found that it doesn't go well with short texts. What methods would be better and do they have Python implementations?</p>\n",
    "score": 5,
    "creation_date": 1591194744,
    "view_count": 5594,
    "answer_count": 4,
    "tags": "python;python-3.x;nlp;lda;topic-modeling"
  },
  {
    "question_id": 59105346,
    "title": "Longest match only with Spacy Phrasematcher",
    "body": "<p>I have created a <a href=\"https://spacy.io/api/phrasematcher\" rel=\"nofollow noreferrer\">Spacy Phrasematcher</a> to match names in a document, following the <a href=\"https://spacy.io/usage/rule-based-matching#phrasematcher\" rel=\"nofollow noreferrer\">tutorial</a>. I want to use the resulting matches as additional training data in order to train a Spacy NER model.\nMy patterns, however, contain both full names (e.g. 'Barack Obama') and last names ('Obama') separately.</p>\n\n<p>Hence, in a sentence that contains 'Barack Obama', both patterns match, resulting in overlapping matches. This overlap, however, triggers an exception when I try to use the data for training, e.g.:</p>\n\n<pre><code>ValueError: [E103] Trying to set conflicting doc.ents: '(19, 33, 'PERSON')' and '(29, 33, 'PERSON')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.\n</code></pre>\n\n<p>I've been considering to filter out overlapping matches before using the data for training, but this seems like a very inefficient approach, resulting in a significant increase in processing time for large data.</p>\n\n<p>Is there a way to set up a <code>PhraseMatcher</code> so that it only matches the longest match for overlapping matches?</p>\n",
    "score": 5,
    "creation_date": 1575032481,
    "view_count": 1306,
    "answer_count": 1,
    "tags": "python;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 55979818,
    "title": "Finetuning BERT on custom data",
    "body": "<p>I want to train a <strong>21 class</strong> text classification model using Bert. But I have very little training data, so a downloaded a similar dataset with <strong>5 classes</strong> with 2 million samples.t \nAnd finetuned downloaded data with uncased pretrained model provided by bert.\nAnd got about 98% validation accuracy.\nNow, I want to use this model as pretrained model for my small custom data.\nBut I am getting <code>shape mismatch with tensor output_bias from checkpoint reader</code> error as the check-point model has 5 classes and my custom data has 21 classes.</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>NFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Running train on CPU\r\nINFO:tensorflow:*** Features ***\r\nINFO:tensorflow:  name = input_ids, shape = (32, 128)\r\nINFO:tensorflow:  name = input_mask, shape = (32, 128)\r\nINFO:tensorflow:  name = is_real_example, shape = (32,)\r\nINFO:tensorflow:  name = label_ids, shape = (32, 21)\r\nINFO:tensorflow:  name = segment_ids, shape = (32, 128)\r\nTensor(\"IteratorGetNext:3\", shape=(32, 21), dtype=int32)\r\nWARNING:tensorflow:From /home/user/Spine_NLP/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nWARNING:tensorflow:From /home/user/Spine_NLP/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.dense instead.\r\nINFO:tensorflow:num_labels:21;logits:Tensor(\"loss/BiasAdd:0\", shape=(32, 21), dtype=float32);labels:Tensor(\"loss/Cast:0\", shape=(32, 21), dtype=float32)\r\nINFO:tensorflow:Error recorded from training_loop: Shape of variable output_bias:0 ((21,)) doesn't match with shape of tensor output_bias ([5]) from checkpoint reader.</code></pre>\r\n</div>\r\n</div>\r\n</p>\n",
    "score": 5,
    "creation_date": 1556948406,
    "view_count": 1940,
    "answer_count": 1,
    "tags": "tensorflow;deep-learning;nlp;text-classification;bert-language-model"
  },
  {
    "question_id": 55883389,
    "title": "The `device` argument should be set by using `torch.device` or passing a string as an argument",
    "body": "<p>My data iterator currently runs on the CPU as <code>device=0</code> argument is deprecated. But I need it to run on the GPU with the rest of the model etc. </p>\n\n<p>Here is my code: </p>\n\n<pre><code>pad_idx = TGT.vocab.stoi[\"&lt;blank&gt;\"]\nmodel = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\nmodel = model.to(device)\ncriterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\ncriterion = criterion.to(device)\nBATCH_SIZE = 12000\ntrain_iter = MyIterator(train, device, batch_size=BATCH_SIZE,\n                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n                        batch_size_fn=batch_size_fn, train=True)\nvalid_iter = MyIterator(val, device, batch_size=BATCH_SIZE,\n                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n                        batch_size_fn=batch_size_fn, train=False)\n#model_par = nn.DataParallel(model, device_ids=devices)\n</code></pre>\n\n<p>The above code gives this error: </p>\n\n<pre><code>The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\nThe `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n</code></pre>\n\n<p>I have tried passing in <code>'cuda'</code> as an argument instead of <code>device=0</code> but I receive this error:</p>\n\n<pre><code>&lt;ipython-input-50-da3b1f7ed907&gt; in &lt;module&gt;()\n    10     train_iter = MyIterator(train, 'cuda', batch_size=BATCH_SIZE,\n    11                             repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n---&gt; 12                             batch_size_fn=batch_size_fn, train=True)\n    13     valid_iter = MyIterator(val, 'cuda', batch_size=BATCH_SIZE,\n    14                             repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n\nTypeError: __init__() got multiple values for argument 'batch_size'\n</code></pre>\n\n<p>I have also tried passing in <code>device</code> as an argument. Device being defined as <code>device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')</code></p>\n\n<p>But receive the same error as just above. </p>\n\n<p>Any suggestions would be much appreciated, thanks.</p>\n",
    "score": 5,
    "creation_date": 1556388016,
    "view_count": 9707,
    "answer_count": 2,
    "tags": "python;machine-learning;deep-learning;nlp;pytorch"
  },
  {
    "question_id": 53878141,
    "title": "How to show wordcloud from a dataframe in Python",
    "body": "<p>Currently, i have a dataframe contain words and weight (tf*idf) and i wanna show words which are arranged following weight in wordcloud.</p>\n\n<p>Dataframe is on the left image.</p>\n\n<pre><code>def generate_wordcloud(words_tem):\n    word_cloud = WordCloud(width = 512, height = 512, background_color='white', stopwords= None, max_words=20).generate(words_tem)\n    plt.figure(figsize=(10,8),facecolor = 'white', edgecolor='blue')\n    plt.imshow(word_cloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()\n\n\ntfidf = TfidfVectorizer(data, lowercase = False)\ntfs = tfidf.fit_transform([data]) \n\nfeature_names = tfidf.get_feature_names()\n\ndf = pd.DataFrame(tfs.T.toarray(), index=feature_names, columns= ['weight'])\ndf = df.sort_values(by = 'weight', ascending = False)\nword_lists = df.index.values\nunique_str  = ' '.join(word_lists)\nprint(df[0:20])\ngenerate_wordcloud(unique_str)\n</code></pre>\n\n<p><a href=\"https://i.sstatic.net/nhXuv.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/nhXuv.png\" alt=\"enter image description here\"></a></p>\n",
    "score": 5,
    "creation_date": 1545355681,
    "view_count": 10340,
    "answer_count": 1,
    "tags": "python;nlp;word-cloud"
  },
  {
    "question_id": 51707282,
    "title": "Example of NLTK&#39;s Vader Scoring Text",
    "body": "<p>I would like someone to correct my understanding of how VADER scores text. I've read an explanation of this process <a href=\"http://datameetsmedia.com/vader-sentiment-analysis-explained/\" rel=\"noreferrer\">here</a>, however I cannot match the compound score of test sentences to Vader's output when recreating the process it describes. Lets say we have the sentence:</p>\n\n<pre><code>\"I like using VADER, its a fun tool to use\"\n</code></pre>\n\n<p>The words VADER picks up are 'like' (+1.5 score), and 'fun' (+2.3). According to the documentation, these values are summed (so +3.8), and then normalized to a range between 0 and 1 using the following function:</p>\n\n<pre><code>(alpha = 15)\nx / x2 + alpha \n</code></pre>\n\n<p>With our numbers, this should become:</p>\n\n<pre><code>3.8 / 14.44 + 15 = 0.1290\n</code></pre>\n\n<p>VADER, however, outputs the returned compound score as follows:</p>\n\n<pre><code>Scores: {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.7003}\n</code></pre>\n\n<p>Where am I going wrong in my reasoning? <a href=\"https://stackoverflow.com/questions/40325980/how-is-the-vader-compound-polarity-score-calculated-in-python-nltk\">Similar</a> questions have been asked several times, however an actual example of VADER classifying has not yet been provided. Any help would be appreciated.</p>\n",
    "score": 5,
    "creation_date": 1533557224,
    "view_count": 1664,
    "answer_count": 1,
    "tags": "python;nlp;nltk;lexicon;vader"
  },
  {
    "question_id": 47219389,
    "title": "Compute word n-grams on original text or after lemma/stemming process?",
    "body": "<p>I'm thinking about use word n-grams techniques on a raw text. But I have a doubt:</p>\n\n<p>does it have sense use word n-grams after applying lemma/stemming on text? If not, why should I use word n-grams only on raw files? What are pros and cons?</p>\n",
    "score": 5,
    "creation_date": 1510305720,
    "view_count": 2403,
    "answer_count": 1,
    "tags": "information-retrieval;n-gram;text-analysis;stemming;lemmatization"
  },
  {
    "question_id": 45232671,
    "title": "Obtain tf-idf weights of words with sklearn",
    "body": "<p>I have a set of texts of wikipedia.<br>\nUsing <strong>tf-idf</strong>, I can define the weight of each word.\nBelow is the code:</p>\n\n<pre><code>import pandas as pd                                             \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nwiki = pd.read_csv('people_wiki.csv')\n\ntfidf_vectorizer = TfidfVectorizer(max_features= 1000000)\ntfidf = tfidf_vectorizer.fit_transform(wiki['text'])\n</code></pre>\n\n<p>The goal is to see the weights like shown in the <strong>tf-idf</strong> column:</p>\n\n<p><a href=\"https://i.sstatic.net/qKNCj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/qKNCj.png\" alt=\"enter image description here\"></a></p>\n\n<p>The file 'people_wiki.csv' is here:</p>\n\n<p><a href=\"https://ufile.io/udg1y\" rel=\"nofollow noreferrer\">https://ufile.io/udg1y</a></p>\n",
    "score": 5,
    "creation_date": 1500625402,
    "view_count": 7516,
    "answer_count": 1,
    "tags": "python;machine-learning;scikit-learn;nlp;tf-idf"
  },
  {
    "question_id": 44858741,
    "title": "NLTK tokenizer and Stanford corenlp tokenizer cannot distinct 2 sentences without space at period (.)",
    "body": "<p>I have 2 sentences in my dataset:</p>\n\n<p>w1 = I am Pusheen the cat.I am so cute. # no space after period <br>\nw2 = I am Pusheen the cat. I am so cute. # with space after period</p>\n\n<p>When I use NKTL tokenizer (both word and sent), nltk cannot distinct the between cat.I.</p>\n\n<p>Here is word tokenize</p>\n\n<pre><code>&gt;&gt;&gt; nltk.word_tokenize(w1, 'english')\n['I', 'am', 'Pusheen', 'the', 'cat.I', 'am', 'so', 'cute']\n&gt;&gt;&gt; nltk.word_tokenize(w2, 'english')\n['I', 'am', 'Pusheen', 'the', 'cat', '.', 'I', 'am', 'so', 'cute']\n</code></pre>\n\n<p>and sent tokenize</p>\n\n<pre><code>&gt;&gt;&gt; nltk.sent_tokenize(w1, 'english')\n['I am Pusheen the cat.I am so cute']\n&gt;&gt;&gt; nltk.sent_tokenize(w2, 'english')\n['I am Pusheen the cat.', 'I am so cute']\n</code></pre>\n\n<p>I would like to ask how to fix that ? i.e: make nlkt detect as w2 while in my dataset, sometime word and punctuation are stick together.</p>\n\n<p>Update: \nTried Stanford CoreNLP 3.7.0, they also cannot distinct 'cat.I' as 'cat', '.', 'I'</p>\n\n<pre><code>meow@meow-server:~/projects/stanfordcorenlp$ java edu.stanford.nlp.process.PTBTokenizer sample.txt\nI\nam\nPusheen\nthe\ncat.I\nam\nso\ncute\n.\nPTBTokenizer tokenized 9 tokens at 111.21 tokens per second.\n</code></pre>\n",
    "score": 5,
    "creation_date": 1498896257,
    "view_count": 2144,
    "answer_count": 1,
    "tags": "python;nlp;nltk;stanford-nlp;tokenize"
  },
  {
    "question_id": 34427678,
    "title": "&#39;utf-8&#39; decode error when loading a word2vec module",
    "body": "<p>I have to use a word2vec module containing tons of Chinese characters. The module was trained by my coworkers using Java and is saved as a bin file. </p>\n\n<p>I installed <a href=\"https://radimrehurek.com/gensim/models/word2vec.html\" rel=\"nofollow\">gensim</a> and tries to load the module, but following error occurred: </p>\n\n<pre><code>In [1]: import gensim  \n\nIn [2]: model = gensim.models.Word2Vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=True)\n\nUnicodeDecodeError: 'utf-8' codec can't decode bytes in position 96-97: unexpected end of data\n</code></pre>\n\n<p>I tried to load the module both in python 2.7 and 3.5, failed in the same way. So how can I load the module in gensim? Thanks.</p>\n",
    "score": 5,
    "creation_date": 1450837493,
    "view_count": 8571,
    "answer_count": 3,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 31023099,
    "title": "How can I easily draw a parse tree from Stanford parsing data in python?",
    "body": "<p>So I have this Stanford-style parsing of an english sentence:</p>\n\n<pre><code>\"There is a tree behind a car\"\nParse: [S [NP There_EX NP] [VP is_VBZ [NP [NP a_DT tree_NN NP] [PP behind_IN [NP a_DT car_NN NP] PP] NP] VP] S]\n</code></pre>\n\n<p>I want to use some of the tree drawing methods in python to draw a parsing tree from the data.</p>\n\n<p>Is there an easy way to use that parsing representation to draw a tree with python or should I change the representation somehow?</p>\n",
    "score": 5,
    "creation_date": 1435138940,
    "view_count": 3739,
    "answer_count": 3,
    "tags": "python;parsing;nlp;parse-tree"
  },
  {
    "question_id": 29275614,
    "title": "Using my own corpus instead of movie_reviews corpus for Classification in NLTK",
    "body": "<p>I use following code and I get it form <a href=\"https://stackoverflow.com/questions/21107075/classification-using-movie-review-corpus-in-nltk-python\">Classification using movie review corpus in NLTK/Python</a></p>\n\n<pre><code>import string\nfrom itertools import chain\nfrom nltk.corpus import movie_reviews as mr\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk.classify import NaiveBayesClassifier as nbc\nimport nltk\n\nstop = stopwords.words('english')\ndocuments = [([w for w in mr.words(i) if w.lower() not in stop and w.lower() not in string.punctuation], i.split('/')[0]) for i in mr.fileids()]\n\nword_features = FreqDist(chain(*[i for i,j in documents]))\nword_features = word_features.keys()[:100]\n\nnumtrain = int(len(documents) * 90 / 100)\ntrain_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag in documents[:numtrain]]\ntest_set = [({i:(i in tokens) for i in word_features}, tag) for tokens,tag  in documents[numtrain:]]\n\nclassifier = nbc.train(train_set)\nprint nltk.classify.accuracy(classifier, test_set)\nclassifier.show_most_informative_features(5)\n</code></pre>\n\n<p>output:</p>\n\n<pre><code>0.655\nMost Informative Features\n                 bad = True              neg : pos    =      2.0 : 1.0\n              script = True              neg : pos    =      1.5 : 1.0\n               world = True              pos : neg    =      1.5 : 1.0\n             nothing = True              neg : pos    =      1.5 : 1.0\n                 bad = False             pos : neg    =      1.5 : 1.0\n</code></pre>\n\n<p>I want to create my own folder instead of <code>movie_reviews</code> in nltk, and put my own files inside it.</p>\n",
    "score": 5,
    "creation_date": 1427364190,
    "view_count": 5017,
    "answer_count": 1,
    "tags": "python-2.7;nlp;classification;nltk;corpus"
  },
  {
    "question_id": 28437945,
    "title": "Extract Dates and events associated with the date from Text corpus",
    "body": "<p>I am currently running a python code that runs through every line of the text file and parses the line for Dates. If it does find the date in the line, the line is copied to a new Output file.\nI am repeating this process on 100 documents and at the end, I get an output file containing lines that have Dates Like &quot;2013, August 2014, 01-11-1987 and so on.&quot;</p>\n<p>The problem with this is, that it does not give accurate information about the events associated with some Dates.</p>\n<p>Is there a more elegant approach to this problem?\nBelow is the file in which I am trying to extract events for the date December 2010</p>\n<blockquote>\n<p>Taipei is the most competitive place among all major cities and\ncounties, according to a study published by a local magazine\nyesterday. Taipei came in first in each of the categories - economy,\nemployment, education, environmental protection, public safety,\nmedical care and local finances - evaluated in the study by Global\nView Magazine. In terms of overall competitiveness, Taipei is\ntherefore number one, followed by Hsinchu City, Chiayi City and New\nTaipei. Taipei, with more than six decades of privileged development\nheavily funded by the central government, will remain unchallenged in\nthe foreseeable future, Global View commented. Taipei and New Taipei\nare two of the country's five Cabinet-level special municipalities,\nbut the other three - Taichung, Tainan and Kaohsiung - failed to\nreceive good ratings in the study though they have more resources than\nmost other local governments. Taichung ranks seventh, Tainan 12th and\nKaohsiung 15th of all 19 local governments graded in the study. The\nthree special municipalities grew to the present size by merging\nneighboring counties in December 2010. But Global View said the\nmergers crippled their competitiveness. But all five special\nmunicipalities are in the top-10 in terms of economic competitiveness.\nAt the bottom is the agricultural Pingtung County. But another\nagricultural county, Taitung, made it to the top-10, occupying the\neighth place mainly because of its low crime rate, the magazine said.</p>\n</blockquote>\n<p>As you can see when I parse the line containing December 2010 I don't really get any meaningful information\nBut actually, there is one major event which is the merging of neighboring counties.\nThis is not captured. Hence I need to know is there any algorithm/library which can help me capture events that have occurred on a particular date.</p>\n",
    "score": 5,
    "creation_date": 1423589121,
    "view_count": 8878,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;nltk"
  },
  {
    "question_id": 27927556,
    "title": "What do the parameters of the csvIterator mean in Mallet?",
    "body": "<p>I am using mallet topic modelling sample code and though it runs fine, I would like to know what the parameters of this statement actually mean?</p>\n\n<pre><code>instances.addThruPipe(new CsvIterator(new FileReader(dataFile),\n                                      \"(\\\\w+)\\\\s+(\\\\w+)\\\\s+(.*)\",\n                                      3, 2, 1)  // (data, target, name) field indices                    \n                     );\n</code></pre>\n",
    "score": 5,
    "creation_date": 1421168665,
    "view_count": 734,
    "answer_count": 2,
    "tags": "machine-learning;nlp;topic-modeling;text-analysis;mallet"
  },
  {
    "question_id": 27869416,
    "title": "NLTK: can I add terminal to grammar that is already generated",
    "body": "<p>I have generated grammar from atis grammar, now I wanted to add some rules of my own especially terminals from sentence could I do that?</p>\n\n<pre><code>import nltk\ngrammar = nltk.data.load('grammars/large_grammars/atis.cfg')\n</code></pre>\n\n<p>to <code>grammar</code> I want to add more terminals.</p>\n",
    "score": 5,
    "creation_date": 1420838326,
    "view_count": 3793,
    "answer_count": 1,
    "tags": "python;nlp;nltk;context-free-grammar"
  },
  {
    "question_id": 26962725,
    "title": "How to convert text file to CoNLL format for malt parser?",
    "body": "<p>I'm trying to use malt parser with the pre made english model. However, I do not know how to convert a text corpus of English sentences into the CoNLL format that is necessary for Malt Parser to operate on. I could not find any documentation on the site. How should I go about this?</p>\n\n<p>Update. I am referring to this post <a href=\"https://stackoverflow.com/questions/17450652/create-conll-file-as-output-of-stanford-parser\">Create .conll file as output of Stanford Parser</a> to create a .conll. However, this is using Stanford Parser.</p>\n",
    "score": 5,
    "creation_date": 1416176423,
    "view_count": 7583,
    "answer_count": 1,
    "tags": "parsing;nlp;stanford-nlp;pos-tagger"
  },
  {
    "question_id": 25729204,
    "title": "Bias towards negative sentiments from Stanford CoreNLP",
    "body": "<p>I'm experimenting with deriving sentiment from Twitter using Stanford's CoreNLP library, a la <a href=\"https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java\" rel=\"nofollow\">https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java</a> - so see here for the code that I'm implementing. </p>\n\n<p>I am getting results, but I've noticed that there appears to be a bias towards 'negative' results, both in my target dataset and another dataset I use with ground truth - the Sanders Analytics Twitter Sentiment Corpus <a href=\"http://www.sananalytics.com/lab/twitter-sentiment/\" rel=\"nofollow\">http://www.sananalytics.com/lab/twitter-sentiment/</a> - even though the ground truth data do not have this bias. </p>\n\n<p>I'm posting this question on the off chance that someone else has experienced this and/or may know if this is the result of something I've done or some bug in the CoreNLP code.</p>\n\n<p>(edit - sorry it took me so long to respond)\nI am posting links to plots showing what I mean. I don't have enough reputation to post the images, and can only include two links in this post, so I'll add the links in the comments. </p>\n",
    "score": 5,
    "creation_date": 1410194981,
    "view_count": 1317,
    "answer_count": 2,
    "tags": "java;twitter;nlp;stanford-nlp;sentiment-analysis"
  },
  {
    "question_id": 19084585,
    "title": "algorithm to detect time, date and place from invitation text",
    "body": "<p>I am researching some Natural Language Processing algorithms to read a piece of text, and if the text seems to be trying to suggest a meeting request, it sets up that meeting for you automatically.</p>\n\n<p>For example, if an email text reads: </p>\n\n<blockquote>\n  <p>Let's <strong>meet tomorrow</strong> someplace in <strong>Downtown at 7pm</strong>\". </p>\n</blockquote>\n\n<p>The algorithm should be able to detect the Time, date and place of the event.</p>\n\n<p>Does someone know of some already existing NLP algorithms that I could use for this purpose? I have been researching some NLP resources (like <a href=\"http://nltk.org\" rel=\"noreferrer\">NLTK</a> and <a href=\"http://cran.r-project.org/web/views/NaturalLanguageProcessing.html\" rel=\"noreferrer\">some tools in R</a>), but did not have much success.</p>\n\n<p>Thanks</p>\n",
    "score": 5,
    "creation_date": 1380498137,
    "view_count": 3260,
    "answer_count": 4,
    "tags": "algorithm;api;machine-learning;nlp;artificial-intelligence"
  },
  {
    "question_id": 17739006,
    "title": "Remove stop words from the parsed content using OpenNLP",
    "body": "<p>I have parsed the document using OpenNLP parser code provided in this <a href=\"http://www.programcreek.com/2012/05/opennlp-tutorial/\" rel=\"nofollow\">link</a> and I got the following output:</p>\n\n<pre><code>(TOP (S (NP (NN Programcreek)) (VP (VBZ is) (NP (DT a) (ADJP (RB very) (JJ huge) (CC and) (JJ useful)) (NN website)))))\n</code></pre>\n\n<p>From this I want to extract only meaningful words, meaning I want to remove all stopwords because I want to do classification further based on these meaningful words. Can you please suggest to me how to remove stopwords from the parsed output?</p>\n\n<p>Finally I want to get the below output</p>\n\n<pre><code>   (TOP (S (NP (NN Programcreek)) (JJ useful)) (NN website)))))\n</code></pre>\n\n<p>Please help me with this, if it is not possible with OpenNLP then suggest me any other Java library for natural language processing. Because my main aim is to parse the document and get the meaningful words only.</p>\n",
    "score": 5,
    "creation_date": 1374213427,
    "view_count": 6622,
    "answer_count": 2,
    "tags": "java;nlp;stop-words;opennlp"
  },
  {
    "question_id": 13881425,
    "title": "Get WordNet&#39;s domain name for the specified word",
    "body": "<p>I know WordNet has Domains Hierarchy: e.g. sport->football.</p>\n\n<p>1) Is it possible to list all words related, for example, to the 'sport->football' sub-domain?</p>\n\n<pre><code>  Response: goalkeeper, forward, penalty, ball, field, stadium, referee and so on.\n</code></pre>\n\n<p>2) Get domain's name for a given word , e.g. 'goalkeeper'?</p>\n\n<pre><code> Need something like [sport-&gt;football; sport-&gt;hockey] or [football;hockey] or just 'football'.\n</code></pre>\n\n<p>It is for a document classification task.</p>\n",
    "score": 5,
    "creation_date": 1355498296,
    "view_count": 3213,
    "answer_count": 1,
    "tags": "nlp;cluster-analysis;semantic-web;wordnet;document-classification"
  },
  {
    "question_id": 11182606,
    "title": "Check English grammar",
    "body": "<p>I am looking for a simple C# library that does the following: Takes a string representing a single sentence, and returns a boolean saying if it's grammatically correct.</p>\n\n<p>I have not been able to find a single, self-contained library that does this after extensive searching.</p>\n",
    "score": 5,
    "creation_date": 1340588220,
    "view_count": 5025,
    "answer_count": 1,
    "tags": "c#;nlp;grammar"
  },
  {
    "question_id": 9843424,
    "title": "Building a lemmatizer: speed optimization",
    "body": "<p>I am building a lemmatizer in python. As I need it to run in realtime/process fairly large amount of data the processing speed\nis of the essence. \nData: I have all possible suffixes that are linked to all wordtypes that they can be combined with. Additionally I have lemmaforms that are linked to both their wordtype(s) and lemma(s). The program takes a word as input and outputs its lemma.\nword = lemmafrom + suffix</p>\n\n<p>For example (Note: although the example is given in English I am not building a lemmatizer for English):</p>\n\n<p>word:  forbidding</p>\n\n<p>lemmaform: forbidd</p>\n\n<p>suffix: ing</p>\n\n<p>lemma: forbid</p>\n\n<p>My solution:</p>\n\n<p>I have converted the data to (nested) dicts:</p>\n\n<pre><code>suffixdict : {suffix1:[type1,type2, ... , type(n)], suffix2:[type1,type2, ... ,\ntype(n)]}    \nlemmaformdict : {lemmaform:{type1:lemma}}\n</code></pre>\n\n<p>1) Find all possible suffixes and word types that they are linked to.\nIf the longest possible suffix is 3 characters long, the program tries to match 'ing', 'ng', 'n' to the keys in \nsuffixdict. If the key exists it returns a value (a set of wordtypes).</p>\n\n<p>2) For each matching suffix search the lemmaform from the dict.\nIf lemmaform exists it returns the wordtypes.</p>\n\n<p>3) Finally, the program tries to intersect the wordtypes produced in steps 1) ans 2) and if the intersection is\nsucessful it returns the lemma of the word.</p>\n\n<p>My question: could there be a better solution to my problem from the prespective of speed? (Disregarding the option to keep frequent words and lemmas in the dictionary) \nHelp much appriciated.</p>\n",
    "score": 5,
    "creation_date": 1332522279,
    "view_count": 1349,
    "answer_count": 2,
    "tags": "python;optimization;nlp;lemmatization"
  },
  {
    "question_id": 7709684,
    "title": "How to create a bag of words using Weka?",
    "body": "<p>I have a corpus of documents and I want to represent each document as a vector. Basically, the vector would have 1 for words that are present inside a document and for other words (which are present in other documents in the corpus and not in this particular document) it would have a 0. How do I create this vector for all the documents in Weka?</p>\n\n<p>Is there a quick way to do this using Weka? I also want Weka to remove stopwords and so some pre-processing if possible before it creates this vector.</p>\n\n<p>Thanks\nAbhishek S</p>\n",
    "score": 5,
    "creation_date": 1318231612,
    "view_count": 5750,
    "answer_count": 1,
    "tags": "nlp;weka"
  },
  {
    "question_id": 6870218,
    "title": "Bucketing sentences by mood",
    "body": "<p>Let's start with a simple problem. Let's say that I have a 350 char sentence and would like to bucket the sentence into either a \"Good mood\" bucket or a \"Bad mood\" bucket. </p>\n\n<p>What would be the best way to design an algorithm to bucket the sentence?</p>\n",
    "score": 5,
    "creation_date": 1311926458,
    "view_count": 837,
    "answer_count": 6,
    "tags": "algorithm;nlp;sentiment-analysis;document-classification"
  },
  {
    "question_id": 6629165,
    "title": "k-fold Cross Validation for determining k in k-means?",
    "body": "<p>In a document clustering process, as a data pre-processing step, I first applied singular vector decomposition to obtain <code>U</code>, <code>S</code> and <code>Vt</code> and then by choosing a suitable number of eigen values I truncated <code>Vt</code>, which now gives me a good document-document correlation from what I read <a href=\"http://en.wikipedia.org/wiki/Latent_semantic_analysis\" rel=\"nofollow noreferrer\">here</a>. Now I am performing clustering on the columns of the matrix <code>Vt</code> to cluster similar documents together and for this I chose k-means and the initial results looked acceptable to me (with k = 10 clusters) but I wanted to dig a bit deeper on choosing the k value itself. To determine the number of clusters <code>k</code> in k-means, I was <a href=\"https://stackoverflow.com/questions/6615665/kmeans-without-knowing-the-number-of-clusters\">suggested</a> to look at cross-validation. </p>\n\n<p>Before implementing it I wanted to figure out if there is a built-in way to achieve it using numpy or scipy. Currently, the way I am performing <code>kmeans</code> is to simply use the function from scipy.</p>\n\n<pre><code>import numpy, scipy\n\n# Preprocess the data and compute svd\nU, S, Vt = svd(A) # A is the TFIDF representation of the original term-document matrix\n\n# Obtain the document-document correlations from Vt\n# This 50 is the threshold obtained after examining a scree plot of S\ndocvectors = numpy.transpose(self.Vt[0:50, 0:]) \n\n# Prepare the data to run k-means\nwhitened = whiten(docvectors)\nres, idx = kmeans2(whitened, 10, iter=20)\n</code></pre>\n\n<p>Assuming my methodology is correct so far (please correct me if I am missing some step), at this stage, what is the standard way of using the output to perform cross-validation? Any reference/implementations/suggestions on how this would be applied to k-means would be greatly appreciated.</p>\n",
    "score": 5,
    "creation_date": 1310151611,
    "view_count": 22096,
    "answer_count": 3,
    "tags": "python;statistics;numpy;nlp;machine-learning"
  },
  {
    "question_id": 6482507,
    "title": "Probabilistic latent semantic analysis/Indexing - Introduction",
    "body": "<p>But recently I found this link quite helpful to understand the principles of LSA without  too much math.  <a href=\"http://www.puffinwarellc.com/index.php/news-and-articles/articles/33-latent-semantic-analysis-tutorial.html\">http://www.puffinwarellc.com/index.php/news-and-articles/articles/33-latent-semantic-analysis-tutorial.html</a>. It forms a good basis on which I can build further.</p>\n\n<p>currently, I'm looking out for a similar introduction to Probabilistic Latent Semantic Analysis/Indexing.  Less of math and more of examples explaining the principles behind it.  If you would know such an introduction, please let me know.</p>\n\n<p>Can it be used to find the measure of similarity between sentences?  Does it handle polysemy?</p>\n\n<p>Is there a python implementation for the same?</p>\n\n<p>Thank you.</p>\n",
    "score": 5,
    "creation_date": 1309069947,
    "view_count": 4413,
    "answer_count": 1,
    "tags": "nlp;lsa;latent-semantic-indexing"
  },
  {
    "question_id": 3902044,
    "title": "How do I count words in an nltk plaintextcorpus faster?",
    "body": "<p>I have a set of documents, and I want to return a list of tuples where each tuple has the date of a given document and the number of times a given search term appears in that document.  My code (below) works, but is slow, and I'm a n00b.  Are there obvious ways to make this faster?  Any help would be much appreciated, mostly so that I can learn better coding, but also so that I can get this project done faster!  </p>\n\n<pre><code>def searchText(searchword):\n    counts = []\n    corpus_root = 'some_dir'\n    wordlists = PlaintextCorpusReader(corpus_root, '.*')\n    for id in wordlists.fileids():\n        date = id[4:12]\n        month = date[-4:-2]\n        day = date[-2:]\n        year = date[:4]\n        raw = wordlists.raw(id)\n        tokens = nltk.word_tokenize(raw)\n        text = nltk.Text(tokens)\n        count = text.count(searchword)\n        counts.append((month, day, year, count))\n\n    return counts\n</code></pre>\n",
    "score": 5,
    "creation_date": 1286742337,
    "view_count": 7293,
    "answer_count": 1,
    "tags": "python;nlp;nltk;corpus"
  },
  {
    "question_id": 2500732,
    "title": "Defining the context of a word - Python",
    "body": "<p>I think this is an interesting question, at least for me.</p>\n\n<hr>\n\n<p>I have a <strong>list of words</strong>, let's say: </p>\n\n<blockquote>\n  <p>photo, free, search, image, css3, css, tutorials, webdesign, tutorial, google, china, censorship, politics, internet</p>\n</blockquote>\n\n<p>and I have a <strong>list of contexts</strong>:</p>\n\n<ul>\n<li>Programming</li>\n<li>World news</li>\n<li>Technology</li>\n<li>Web Design</li>\n</ul>\n\n<hr>\n\n<p>I need to try and match words with the appropriate context/contexts if possible.</p>\n\n<p>Maybe discovering word relationships in some way.</p>\n\n<p><img src=\"https://i.sstatic.net/44UvG.png\" alt=\"alt text\"></p>\n\n<hr>\n\n<p>Any ideas?</p>\n\n<p>Help would be much appreciated!</p>\n",
    "score": 5,
    "creation_date": 1269355050,
    "view_count": 1435,
    "answer_count": 4,
    "tags": "python;django;dictionary;nlp"
  },
  {
    "question_id": 2341907,
    "title": "Transformation-Based Part-of-Speech Tagging(Brill Tagging)",
    "body": "<p>What are the weaknesses and strengths of the Brill Tagger? Can you suggest some possible improvements for the tagger?</p>\n",
    "score": 5,
    "creation_date": 1267191055,
    "view_count": 1997,
    "answer_count": 2,
    "tags": "nlp;tagging;part-of-speech"
  },
  {
    "question_id": 2171469,
    "title": "Run GATE pipeline from inside a Java program without the GUI. build a tomcat app with gate",
    "body": "<p>i have built some plugin components to GATE and in combination with ANNIE tools, im running a pipeline in GATE platform. </p>\n\n<p>Does anyone know how can i run a pipeline from the console?  I want to build a web application in Tomcat that will be taking a plain text from the web page, passing it to the GATE pipeline i have built and do something. So i need to run GATE in a simple Java file, how can it be done?</p>\n\n<p>Thanks in advance and sorry for my poor grammar</p>\n",
    "score": 5,
    "creation_date": 1264937193,
    "view_count": 9914,
    "answer_count": 3,
    "tags": "java;tomcat;nlp;gate"
  },
  {
    "question_id": 72338808,
    "title": "How to calculate per document probabilities under respective topics with BERTopics?",
    "body": "<p>I am trying to use <code>BERTopic</code> to analyze the topic distribution of documents, after <code>BERTopic</code> is performed, I would like to calculate the probabilities under respective topics per document, how should I did it?</p>\n<pre class=\"lang-py prettyprint-override\"><code># define model\nmodel = BERTopic(verbose=True,\n                 vectorizer_model=vectorizer_model,\n                 embedding_model='paraphrase-MiniLM-L3-v2',\n                 min_topic_size= 50,\n                 nr_topics=10)\n\n#  train model\nheadline_topics, _ = model.fit_transform(df1.review_processed3)\n\n# examine one of the topic\na_topic = freq.iloc[0][&quot;Topic&quot;] # Select the 1st topic\nmodel.get_topic(a_topic) # Show the words and their c-TF-IDF scores\n</code></pre>\n<p>Below is the words and their c-TF-IDF scores for one of the Topics\n<a href=\"https://i.sstatic.net/XOwsD.png\" rel=\"noreferrer\">image 1</a></p>\n<p>How should I change the result into Topic Distribution as below in order to calculate the topic distribution score and also identify the main topic?\n<a href=\"https://i.sstatic.net/4MX0j.png\" rel=\"noreferrer\">image 2</a></p>\n",
    "score": 5,
    "creation_date": 1653232307,
    "view_count": 2803,
    "answer_count": 1,
    "tags": "python;nlp;bert-language-model;topic-modeling"
  },
  {
    "question_id": 71113363,
    "title": "huggingface transformers longformer optimizer warning AdamW",
    "body": "<p>I get below warning when I try to run the code from this <a href=\"https://github.com/jlealtru/website_tutorials/blob/main/notebooks/Longformer%20with%20IMDB.ipynb\" rel=\"noreferrer\">page</a>.</p>\n<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n</code></pre>\n<p>I am super confused because the code doesn't seem to set the optimizer at all. The most probable places where the optimizer was set could be below but I dont know how to change the optimizer then</p>\n<pre><code># define the training arguments\ntraining_args = TrainingArguments(\n    output_dir = '/media/data_files/github/website_tutorials/results',\n    num_train_epochs = 5,\n    per_device_train_batch_size = 8,\n    gradient_accumulation_steps = 8,    \n    per_device_eval_batch_size= 16,\n    evaluation_strategy = &quot;epoch&quot;,\n    disable_tqdm = False, \n    load_best_model_at_end=True,\n    warmup_steps=200,\n    weight_decay=0.01,\n    logging_steps = 4,\n    fp16 = True,\n    logging_dir='/media/data_files/github/website_tutorials/logs',\n    dataloader_num_workers = 0,\n    run_name = 'longformer-classification-updated-rtx3090_paper_replication_2_warm'\n)\n\n# instantiate the trainer class and check for available devices\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_data,\n    eval_dataset=test_data\n)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n</code></pre>\n<p>I tried another transformer such as <code>distilbert-base-uncased</code> using the identical code but it seems to run without any warnings.</p>\n<ol>\n<li>Is this warning more specific to <code>longformer</code>?</li>\n<li>How should I change the optimizer?</li>\n</ol>\n",
    "score": 5,
    "creation_date": 1644848377,
    "view_count": 3793,
    "answer_count": 2,
    "tags": "python;nlp;huggingface-transformers"
  },
  {
    "question_id": 70583246,
    "title": "sentence transformer how to predict new example",
    "body": "<p>I am exploring sentence transformers and came across this <a href=\"https://www.sbert.net/docs/training/overview.html\" rel=\"noreferrer\">page</a>.\nIt shows how to train on our custom data. But I am not sure how to predict. If there are two new sentences such as 1) this is the third example, 2) this is the example number three. How could I get a prediction about how similar those sentences are?</p>\n<pre><code>from sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\n\n#Define the model. Either from scratch of by loading a pre-trained model\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\n\n#Define your train examples. You need more than just two examples...\ntrain_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n\n#Define your train dataset, the dataloader and the train loss\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\ntrain_loss = losses.CosineSimilarityLoss(model)\n\n#Tune the model\nmodel.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n</code></pre>\n<p>----------------------------update 1</p>\n<p>I updated the code as below</p>\n<pre><code>from sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\n\n#Define the model. Either from scratch of by loading a pre-trained model\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\n\n#Define your train examples. You need more than just two examples...\ntrain_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n\n#Define your train dataset, the dataloader and the train loss\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\ntrain_loss = losses.CosineSimilarityLoss(model)\n</code></pre>\n<p>Saved the model...main change as compared to the old code</p>\n<pre><code>model_save_path2 = '/content/gdrive/MyDrive/folderName1/folderName2/model_try-'+datetime.now().strftime(&quot;%Y-%m-%d_%H-%M-%S&quot;)\n\n#Tune the model and save it too\nmodel.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100,output_path=model_save_path2)\n</code></pre>\n<p>Not sure about the below steps</p>\n<pre><code>#loading the new model\nmodel_new = SentenceTransformer(model_save_path)\n\n#predicting\nsentences = [&quot;This is an example sentence&quot;, &quot;Each sentence is converted&quot;]\nmodel_new.encode(sentences)\n</code></pre>\n<p>question 1)</p>\n<p>is this a correct approach to get sentence embedding after training old model and creating a new model? I am confused because during fitting process we fed two sentences along with similarity measure. While for output we are inputting one sentence at a time and getting a sentence embedding for each sentence.</p>\n<p>question 2)</p>\n<p>If I would like to get similarity scores for two sentences, is the only option is to take sentence embeddings from output of this model and then use cosine similarity?</p>\n",
    "score": 5,
    "creation_date": 1641319725,
    "view_count": 3592,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers;sentence;sentence-similarity"
  },
  {
    "question_id": 66518316,
    "title": "How do I make a paraphrase generation using BERT/ GPT-2",
    "body": "<p>I am trying hard to understand how to make a paraphrase generation using BERT/GPT-2. I cannot understand how do I make it. Could you please provide me with any resources where I will be able to make a paraphrase generation model?\n<strong>&quot;The input would be a sentence and the output would be a paraphrase of the sentence&quot;</strong></p>\n",
    "score": 5,
    "creation_date": 1615131938,
    "view_count": 4287,
    "answer_count": 3,
    "tags": "nlp;gpt-2"
  },
  {
    "question_id": 60997438,
    "title": "How to revert BERT/XLNet embeddings?",
    "body": "<p>I've been experimenting with stacking language models recently and noticed something interesting: the output embeddings of BERT and XLNet are not the same as the input embeddings. For example, this code snippet:</p>\n\n<pre><code>bert = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\ntok = transformers.BertTokenizer.from_pretrained(\"bert-base-cased\")\n\nsent = torch.tensor(tok.encode(\"I went to the store the other day, it was very rewarding.\"))\nenc = bert.get_input_embeddings()(sent)\ndec = bert.get_output_embeddings()(enc)\n\nprint(tok.decode(dec.softmax(-1).argmax(-1)))\n</code></pre>\n\n<p>Outputs this for me:</p>\n\n<pre><code>,,,,,,,,,,,,,,,,,\n</code></pre>\n\n<p>I would have expected the (formatted) input sequence to be returned since I was under the impression that the input and output token embeddings were tied.</p>\n\n<p>What's interesting is that most other models do not exhibit this behavior. For example, if you run the same code snippet on GPT2, Albert or Roberta, it outputs the input sequence.</p>\n\n<p>Is this a bug? Or is it expected for BERT/XLNet?</p>\n",
    "score": 5,
    "creation_date": 1585848223,
    "view_count": 2236,
    "answer_count": 2,
    "tags": "python;nlp;pytorch;huggingface-transformers;transformer-model"
  },
  {
    "question_id": 59515740,
    "title": "How to find the vocabulary size of a spaCy model?",
    "body": "<p>I am trying to find the vocabulary size of the large English model, i.e. <code>en_core_web_lg</code>, and I find three different sources of information:</p>\n<ul>\n<li><p>spaCy's docs: 685k keys, 685k unique vectors</p>\n</li>\n<li><p><code>nlp.vocab.__len__()</code>: 1340242 # (number of lexemes)</p>\n</li>\n<li><p><code>len(vocab.strings)</code>: 1476045</p>\n</li>\n</ul>\n<p>What is the difference between the three? I have not been able to find the answer in the docs.</p>\n",
    "score": 5,
    "creation_date": 1577575665,
    "view_count": 3489,
    "answer_count": 2,
    "tags": "nlp;documentation;spacy;vocabulary"
  },
  {
    "question_id": 56980515,
    "title": "How to extract all adjectives from a strings of text in a pandas dataframe?",
    "body": "<p>I am loading a CSV into a pandas data frame. One of the columns in the dataframe is \"reviews\" which contain strings of text. I need to identify all the adjectives in this column in all the rows of the dataframe and then create a new column \"adjectives\" that contains a list of all the adjectives from that review.  </p>\n\n<p>I've tried using TextBlobs and was able to tag the parts of speech for each case using the code posted. </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom textblob import TextBlob\n\ndf=pd.read_csv('./data.csv')\n\ndef pos_tag(text):\n    try:\n        return TextBlob(text).tags\n    except:\n        return None\n\ndf['pos'] = df['reviews'].apply(pos_tag)\n\ndf.to_csv('dataadj.csv', index=False)\n</code></pre>\n",
    "score": 5,
    "creation_date": 1562808367,
    "view_count": 6311,
    "answer_count": 1,
    "tags": "python;nlp"
  },
  {
    "question_id": 43010286,
    "title": "How do you concatenate symbols in mxnet",
    "body": "<p>I have 2 symbols in MXNet and would like to concatenate them. How can i do this:</p>\n\n<p>eg: <code>a = [100,200]</code>, <code>b = [300,400]</code>, Id like to get</p>\n\n<p><code>c = [100,200,300,400]</code></p>\n",
    "score": 5,
    "creation_date": 1490395320,
    "view_count": 1829,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;deep-learning;mxnet"
  },
  {
    "question_id": 42947733,
    "title": "Use spacy Spanish Tokenizer",
    "body": "<p>I always used spacy library with english or german. </p>\n\n<p>To load the library I used this code:</p>\n\n<pre><code>import spacy\nnlp = spacy.load('en')\n</code></pre>\n\n<p>I would like to use the Spanish tokeniser, but I do not know how to do it, because spacy does not have a spanish model.\nI've tried this</p>\n\n<pre><code>python -m spacy download es\n</code></pre>\n\n<p>and then:</p>\n\n<pre><code>nlp = spacy.load('es')\n</code></pre>\n\n<p>But obviously without any success.</p>\n\n<p>Does someone know how to tokenise a spanish sentence with spanish in the proper way?</p>\n",
    "score": 5,
    "creation_date": 1490175631,
    "view_count": 6709,
    "answer_count": 3,
    "tags": "python;nlp;tokenize;spacy"
  },
  {
    "question_id": 40513544,
    "title": "Why are there different Lemmatizers in NLTK library?",
    "body": "<pre><code>&gt;&gt; from nltk.stem import WordNetLemmatizer as lm1\n&gt;&gt; from nltk import WordNetLemmatizer as lm2\n&gt;&gt; from nltk.stem.wordnet import WordNetLemmatizer as lm3\n</code></pre>\n\n<p>For me all of the three works the same way, but just to confirm, do they provide anything different?</p>\n",
    "score": 5,
    "creation_date": 1478715663,
    "view_count": 912,
    "answer_count": 2,
    "tags": "python;nlp;nltk;lemmatization"
  },
  {
    "question_id": 39144991,
    "title": "NLTK - nltk.tokenize.RegexpTokenizer - regex not working as expected",
    "body": "<p>I am trying to Tokenize text using RegexpTokenizer.</p>\n\n<p><strong>Code:</strong></p>\n\n<pre><code>from nltk.tokenize import RegexpTokenizer\n#from nltk.tokenize import word_tokenize\n\nline = \"U.S.A Count U.S.A. Sec.of U.S. Name:Dr.John Doe J.Doe 1.11 1,000 10--20 10-20\"\npattern = '[\\d|\\.|\\,]+|[A-Z][\\.|A-Z]+\\b[\\.]*|[\\w]+|\\S'\ntokenizer = RegexpTokenizer(pattern)\n\nprint tokenizer.tokenize(line)\n#print word_tokenize(line)\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<blockquote>\n  <p>['U', '.', 'S', '.', 'A', 'Count', 'U', '.', 'S', '.', 'A', '.', 'Sec', '.', 'of', 'U', '.', 'S', '.', 'Name', ':', 'Dr', '.', 'John', 'Doe', 'J', '.', 'Doe', '1.11', '1,000', '10', '-', '-', '20', '10', '-', '20']</p>\n</blockquote>\n\n<p><strong>Expected Output:</strong></p>\n\n<blockquote>\n  <p>['U.S.A', 'Count', 'U.S.A.', 'Sec', '.', 'of', 'U.S.', 'Name', ':', 'Dr', '.', 'John', 'Doe', 'J.', 'Doe', '1.11', '1,000', '10', '-', '-', '20', '10', '-', '20']</p>\n</blockquote>\n\n<p>Why tokenizer is also spiltting my expected tokens \"U.S.A\" , \"U.S.\"?\nHow can I resolve this issue?</p>\n\n<p>My regex : <a href=\"https://regex101.com/r/dS1jW9/1\" rel=\"noreferrer\">https://regex101.com/r/dS1jW9/1</a></p>\n",
    "score": 5,
    "creation_date": 1472127125,
    "view_count": 10508,
    "answer_count": 2,
    "tags": "python;regex;nlp;nltk;tokenize"
  },
  {
    "question_id": 33389184,
    "title": "SimpleNLG - How to get the plural of a noun?",
    "body": "<p>I'm using <code>SimpleNLG 4.4.2</code> to get plural form for a noun:</p>\n\n<pre><code>final XMLLexicon xmlLexicon = new XMLLexicon();\nfinal WordElement word = xmlLexicon.getWord(\"apple\", LexicalCategory.NOUN);\nSystem.out.println(word);\nSystem.out.println(word.getFeature(LexicalFeature.PLURAL));\n</code></pre>\n\n<p>However even for this simple example, <code>getFeature</code> returns <code>null</code> instead of <code>apples</code>. What am I doing wrong?</p>\n",
    "score": 5,
    "creation_date": 1446029789,
    "view_count": 1022,
    "answer_count": 1,
    "tags": "java;nlp;nlg;simplenlg"
  },
  {
    "question_id": 32733510,
    "title": "NLTK agreement with distance metric",
    "body": "<p>I have a task to calculate <a href=\"https://en.wikipedia.org/wiki/Inter-rater_reliability\" rel=\"nofollow noreferrer\">inter-annotator agreement</a> in <a href=\"https://en.wikipedia.org/wiki/Multi-label_classification\" rel=\"nofollow noreferrer\">multi-label classification</a>, where for each example more than one label can be assigned. I found that <a href=\"http://www.nltk.org/api/nltk.metrics.html\" rel=\"nofollow noreferrer\">NLTK</a> can measure agreement based on a distance metric.</p>\n<p>I am looking for an example of calculating krippendorff alpha with MASI distance.</p>\n<p>This is what I have.</p>\n<pre><code>import nltk\nfrom nltk.metrics import masi_distance\n\n\ntoy_data = [['1', 5723, [1,2]],['2', 5723, [2,3]]]\n\ntask = nltk.metrics.agreement.AnnotationTask(data=toy_data, distance=masi_distance)\nprint task.alpha()\n</code></pre>\n<p>This code fails with</p>\n<pre><code>TypeError: unhashable type: 'list'\n</code></pre>\n<p>The following doesn't work either:</p>\n<pre><code>toy_data = [['1', 5723, set([1,2])],['2', 5723, set([2,3])]]\n</code></pre>\n<p>Do you have a working example?\nThank you!</p>\n",
    "score": 5,
    "creation_date": 1442993289,
    "view_count": 1595,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;nltk"
  },
  {
    "question_id": 31858305,
    "title": "Relationship Extraction using Stanford CoreNLP",
    "body": "<p>I'm trying to extract information from natural language content using the Stanford CoreNLP library.</p>\n\n<p>My goal is to extract \"subject-action-object\" pairs (simplified) from sentences.</p>\n\n<p>As an example consider the following sentence:</p>\n\n<blockquote>\n  <p>John Smith only eats an apple and a banana for lunch. He's on a diet and his mother told him that it would be very healthy to eat less for lunch. John doesn't like it at all but since he's very serious with his diet, he doesn't want to stop.</p>\n</blockquote>\n\n<p>From this sentence I would like to get results as followed:</p>\n\n<ul>\n<li>John Smith - eats - only an apple and a banana for lunch</li>\n<li>He - is - on a diet</li>\n<li>His mother - told - him - that it would be very healthy to eat less for lunch</li>\n<li>John - doesn't like - it (at all)</li>\n<li>He - is - very serious with his diet</li>\n</ul>\n\n<p>How would one do this?</p>\n\n<p>Or to be more specific:\nHow can I parse a dependency tree (or a better-suited tree?) to obtain results as specified above?</p>\n\n<p>Any hint, resource or code snippet given this task would be highly appreciated.</p>\n\n<p>Side note:\nI managed to replace coreferences with their representative mention which would then change the <code>he</code> and <code>his</code> to the corresponding entity (John Smith in that case).</p>\n",
    "score": 5,
    "creation_date": 1438870959,
    "view_count": 5128,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp;text-mining"
  },
  {
    "question_id": 26824771,
    "title": "How do people use n-grams for sentiment analysis, considering that as n increases, the memory requirement also increases rapidly?",
    "body": "<p>I am trying to do Sentiment Analysis on Tweets using Python.</p>\n\n<p>To begin with, I've implemented an n-grams model. So, lets say our training data is</p>\n\n<pre><code>I am a good kid\n\nHe is a good kid, but he didn't get along with his sister much\n</code></pre>\n\n<p><strong>Unigrams</strong>: </p>\n\n<pre><code>&lt;i, am, a, good, kid, he, but, didnt, get, along, with, his, sister, much&gt;\n</code></pre>\n\n<p><strong>Bigrams</strong>:</p>\n\n<pre><code>&lt;(i am), (am a), (a good), (good kid), (he is), (is a), (kid but), (but he), (he didnt), (didnt get), (get along), (along with), (with his), (his sister), (sister much)&gt;\n</code></pre>\n\n<p><strong>Trigrams</strong>:</p>\n\n<pre><code>&lt;(i am a), (am a good), (a good kid), .........&gt;\n</code></pre>\n\n<p><strong>Final feature vector</strong>:</p>\n\n<pre><code>&lt;i, am, a, good, kid, he, but, didnt, get, along, with, his, sister, much, (i am), (am a), (a good), (good kid), (he is), (is a), (kid but), (but he), (he didnt), (didnt get), (get along), (along with), (with his), (his sister), (sister much), (i am a), (am a good), (a good kid), .........&gt;\n</code></pre>\n\n<p>When we do this for a large training data, of 8000 or so entries, the dimensionality of the feature vector becomes too HUGE, as a result of which, my computer (RAM=16GB) crashes.</p>\n\n<p>So, when people mention using \"n-grams\" as features, in 100s of papers out there, what are they talking about? Am I doing something wrong? </p>\n\n<p>Do people always do some feature selection for \"n-grams\"? If so, what kind of feature selection should I look into?</p>\n\n<p>I am using scikit-learn to do this</p>\n",
    "score": 5,
    "creation_date": 1415504727,
    "view_count": 2139,
    "answer_count": 2,
    "tags": "python;nlp;scikit-learn;sentiment-analysis;n-gram"
  },
  {
    "question_id": 25636670,
    "title": "Word and Text relation using python and NLP",
    "body": "<p>I have a word, according to that i want to find out whether the text is related to that word or not using <strong>python and nltk</strong> is it possible ?</p>\n\n<p>For example I have a word called \"<strong>phosphorous</strong>\". I would like to find out that the particular text file is <strong>related to this word or not</strong>?</p>\n\n<p>I cant use bag of words in nltk as I have only one word and no training data.</p>\n\n<p>Any Suggestions?</p>\n\n<p>Thanks in Advance.</p>\n",
    "score": 5,
    "creation_date": 1409721116,
    "view_count": 1218,
    "answer_count": 2,
    "tags": "python;nlp;artificial-intelligence;classification;nltk"
  },
  {
    "question_id": 24186742,
    "title": "Is UIMA provides only a wrapper or is it like StandfordCore NLP and GATE?",
    "body": "<p>The Standford Core NLP and the GATE provides the various NLP operation like NER, POS tagging. There are some of the NLP operation like Tokenizer, Snowball Stemmer available as a UIMA component.\nSo, Is UIMA comparable with the StandfordCore NLP/GATE or it is to be used to wrap these kind of APIs for the pipeline ?</p>\n",
    "score": 5,
    "creation_date": 1402582749,
    "view_count": 1002,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;opennlp;gate;uima"
  },
  {
    "question_id": 14810944,
    "title": "Feature extraction from a single word",
    "body": "<p>Usually one wants to get a feature from a text by using the bag of words approach, counting the words and calculate different measures, for example tf-idf values, like this:  <a href=\"https://stackoverflow.com/questions/4207057/how-to-include-words-as-numerical-feature-in-classification\">How to include words as numerical feature in classification</a></p>\n\n<p>But my problem is different, I want to extract a feature vector from a single word. I want to know for example that potatoes and french fries are close to each other in the vector space, since they are both made of potatoes. I want to know that milk and cream also are close, hot and warm, stone and hard and so on.</p>\n\n<p>What is this problem called? Can I learn the similarities and features of words by just looking at a large number documents?</p>\n\n<p>I will not make the implementation in English, so I can't use databases.</p>\n",
    "score": 5,
    "creation_date": 1360580636,
    "view_count": 2199,
    "answer_count": 3,
    "tags": "machine-learning;nlp;feature-extraction"
  },
  {
    "question_id": 13386554,
    "title": "Does this neural network model exist?",
    "body": "<p>I'm looking for a neural network model with specific characteristics. This model may not exist...</p>\n\n<p>I need a network which doesn't use \"layers\" as traditional artificial neural networks do. Instead, I want [what I believe to be] a more biological model.</p>\n\n<p>This model will house a large cluster of interconnected neurons, like the image below. A few neurons (at bottom of diagram) will receive input signals, and a cascade effect will cause successive, connected neurons to possibly fire depending on signal strength and connection weight. This is nothing new, but, there are no explicit layers...just more and more distant, indirect connections.</p>\n\n<p>As you can see, I also have the network divided into sections (circles). Each circle represents a semantic domain (a linguistics concept) which is the core information surrounding a concept; essentially a semantic domain is a concept.</p>\n\n<p>Connections between nodes within a section have higher weights than connections between nodes of different sections. So the nodes for \"car\" are more connected to one another than nodes connecting \"English\" to \"car\". Thus, when a neuron in a single section fires (is activated), it is likely that the entire (or most of) the section will also be activated.</p>\n\n<p><strong>All in all, I need output patterns to be used as input for further output, and so on.</strong> A cascade effect is what I am after.</p>\n\n<p>I hope this makes sense. Please ask for clarification where needed.</p>\n\n<p>Are there any suitable models in existence that model what I've described, already?</p>\n\n<p><img src=\"https://i.sstatic.net/q8qAz.jpg\" alt=\"enter image description here\"></p>\n",
    "score": 5,
    "creation_date": 1352923904,
    "view_count": 1247,
    "answer_count": 4,
    "tags": "nlp;artificial-intelligence;neural-network"
  },
  {
    "question_id": 10789834,
    "title": "Phrase corpus for sentimental analysis",
    "body": "<p>Good day,\nI'm attempting to write a sentimental analysis application in python (Using naive-bayes classifier) with the aim to categorize phrases from news as being positive or negative.\nAnd I'm having a bit of trouble finding an appropriate corpus for that.\nI tried using \"General Inquirer\" (http://www.wjh.harvard.edu/~inquirer/homecat.htm) which works OK but I have one big problem there.\nSince it is a word list, not a phrase list I observe the following problem when trying to label the following sentence:</p>\n\n<blockquote>\n  <p>He is not expected to win.</p>\n</blockquote>\n\n<p>This sentence is categorized as being positive, which is wrong. The reason for that is that \"win\" is positive, but \"not\" does not carry any meaning since \"not win\" is a phrase.\nCan anyone suggest either a corpus or a work around for that issue?\nYour help and insight is greatly appriciated. </p>\n",
    "score": 5,
    "creation_date": 1338234961,
    "view_count": 1538,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 9000385,
    "title": "NLP text tagging",
    "body": "<p>I am a newbie in NLP, just doing it for the first time. \nI am trying to solve a problem. </p>\n\n<p>My problem is I have some documents which are manually tagged like:</p>\n\n<pre><code>doc1 - categoryA, categoryB\ndoc2 - categoryA, categoryC\ndoc3 - categoryE, categoryF, categoryG\n.\n.\n.\n.\ndocN - categoryX\n</code></pre>\n\n<p>Here I have a fixed set of categories and any document can have any number of tags associated with it.\nI want to train the classifier using this input, so that this tagging process can be automated.</p>\n\n<p>Thanks</p>\n",
    "score": 5,
    "creation_date": 1327484017,
    "view_count": 5735,
    "answer_count": 3,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 7223859,
    "title": "Javascript Verbs Detection",
    "body": "<p>I've the following problem. I need to find <strong>verbs</strong> in a string using <code>JavaScript</code>.\nI would like to know, if there is something like (JAWS), the Java API for <code>Wordnet</code>, but for JavaScript.</p>\n\n<p>More specifically, i'm searching for some kind of RESTful webservice able to return the list of verbs in a text or webpage.</p>\n\n<p>If you know something useful, please don't hesitate to post your answers.</p>\n",
    "score": 5,
    "creation_date": 1314564595,
    "view_count": 3479,
    "answer_count": 1,
    "tags": "javascript;nlp"
  },
  {
    "question_id": 5410505,
    "title": "Feature selection and unsupervised learning for multilingual data + machine learning algorithm selection",
    "body": "<p><strong>Questions</strong></p>\n\n<p>I want to classify/categorize/cluster/group together a set of several thousand websites. There's data that we can train on, so we can do supervised learning, but it's not data that we've gathered and we're not adamant about using it -- so we're also considering unsupervised learning.</p>\n\n<ul>\n<li><p>What features can I use in a machine learning algorithm to deal with multilingual data? Note that some of these languages might not have been dealt with in the Natural Language Processing field.</p></li>\n<li><p>If I were to use an unsupervised learning algorithm, should I just partition the data by language and deal with each language differently? Different languages might have different relevant categories (or not, depending on your psycholinguistic theoretical tendencies), which might affect the decision to partition.</p></li>\n<li><p>I was thinking of using decision trees, or maybe Support Vector Machines (SVMs) to allow for more features (from my understanding of them). <a href=\"https://stackoverflow.com/questions/3789856/newbie-where-to-start-given-a-problem-to-predict-future-success-or-not/3791608#3791608\">This post</a> suggests random forests instead of SVMs. Any thoughts?</p></li>\n</ul>\n\n<p>Pragmatical approaches are welcome! (Theoretical ones, too, but those might be saved for later fun.)</p>\n\n<p><strong>Some context</strong></p>\n\n<p>We are trying to classify a corpus of many thousands of websites in 3 to 5 languages (maybe up to 10, but we're not sure).</p>\n\n<p>We have training data in the form of hundreds of websites already classified. However, we may choose to use that data set or not -- if other categories make more sense, we're open to not using the training data that we have, since it is not something we gathered in the first place. We are on the final stages of scraping data/text from websites.</p>\n\n<p>Now we must decide on the issues above. I have done some work with the Brown Corpus and the Brill tagger, but this will not work because of the multiple-languages issue.</p>\n\n<p>We intend to use the <a href=\"http://orange.biolab.si/\" rel=\"nofollow noreferrer\">Orange</a> machine learning package.</p>\n",
    "score": 5,
    "creation_date": 1300908234,
    "view_count": 1178,
    "answer_count": 3,
    "tags": "artificial-intelligence;nlp;machine-learning;data-mining;classification"
  },
  {
    "question_id": 4882486,
    "title": "How to identify tags (key words) automatically from a given text?",
    "body": "<p>It should behave like <a href=\"https://addons.mozilla.org/en-us/firefox/addon/delicious-bookmarks/\" rel=\"nofollow noreferrer\">Delicious toolbar</a> for Firefox does; it lists possible tags to click. The effect is shown as below: </p>\n\n<p><img src=\"https://i.sstatic.net/8PU8T.png\" alt=\"enter image description here\"></p>\n\n<p>The code should be able to find key words for the text. Any good algorithm or open source project to recommend?</p>\n\n<p>I found <a href=\"https://stackoverflow.com/questions/2853384/how-to-identify-ideas-and-concepts-in-a-given-text\">this post</a>, but it is a bit too general for my specific need.</p>\n",
    "score": 5,
    "creation_date": 1296704707,
    "view_count": 4692,
    "answer_count": 1,
    "tags": "algorithm;full-text-search;text-analysis"
  },
  {
    "question_id": 4467193,
    "title": "Trying to use MEGAM as an NLTK ClassifierBasedPOSTagger?",
    "body": "<p>I am currently trying to build a general purpose (or as general as is practical) POS tagger with NLTK. I have dabbled with the brown and treebank corpora for training, but will probably be settling on the treebank corpus.</p>\n\n<p>Learning as I go, I am finding the classifier POS taggers are the most accurate. The Maximum Entity classifier is meant to be the most accurate, but I find it uses so much memory (and processing time) that I have to significantly reduce the training dataset, so the end result is less accurate than using the default Naive Bayes classifier.</p>\n\n<p>It has been suggested that I use MEGAM. NLTK has some support for MEGAM, but all the examples I have found are for general classifiers (eg. a text classifier that uses a vector of word features), rather than a more specific POS tagger. Without having to recreate my own POS feature extractor and compiler (ie. I prefer to use the one already in NLTK), how can I used the MEGAM MaxEnt classifier?  Ie. how can I drop it in some existing MaxEnt code that is along the lines of:</p>\n\n<pre><code>maxent_tagger = ClassifierBasedPOSTagger(train=training_sentences,\n                                        classifier_builder=MaxentClassifier.train )\n</code></pre>\n",
    "score": 5,
    "creation_date": 1292552981,
    "view_count": 2780,
    "answer_count": 2,
    "tags": "python;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 74062240,
    "title": "Using Arabert model with SpaCy",
    "body": "<p>SpaCy doesn't support the Arabic language, but Can I use SpaCy with the pretrained Arabert model?</p>\n<p>Is it possible to modify this code so it can accept bert-large-arabertv02 instead of en_core_web_lg?</p>\n<pre><code>!python -m spacy download en_core_web_lg\nimport spacy\nnlp = spacy.load(&quot;en_core_web_lg&quot;)\n</code></pre>\n<p>Here How we can call AraBertV.02</p>\n<pre><code>from arabert.preprocess import ArabertPreprocessor\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\nmodel_name=&quot;aubmindlab/bert-large-arabertv02&quot;\narabert_prep = ArabertPreprocessor(model_name=model_name)  \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n</code></pre>\n",
    "score": 5,
    "creation_date": 1665698412,
    "view_count": 2968,
    "answer_count": 1,
    "tags": "nlp;spacy;bert-language-model"
  },
  {
    "question_id": 73033651,
    "title": "How do I extract full entity names from a hugging face model without IO tags",
    "body": "<p>I am using a model from hugging face, specifically <code>Davlan/distilbert-base-multilingual-cased-ner-hrl</code>. However, I am not able to extract full entity names from the result.</p>\n<p>If I run the following code:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;Davlan/distilbert-base-multilingual-cased-ner-hrl&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;Davlan/distilbert-base-multilingual-cased-ner-hrl&quot;)\nnlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)\n\nexample = &quot;My name is Johnathan Smith and I work at Apple&quot;\nner_results = nlp(example, aggregation_strategy=&quot;max&quot;)\nprint(ner_results)\n</code></pre>\n<p>Then I get output:</p>\n<pre><code>[{'entity': 'B-PER', 'score': 0.9998949, 'index': 4, 'word': 'Johna', 'start': 11, 'end': 16}, {'entity': 'I-PER', 'score': 0.999726, 'index': 5, 'word': '##tha', 'start': 16, 'end': 19}, {'entity': 'I-PER', 'score': 0.9997751, 'index': 6, 'word': '##n', 'start': 19, 'end': 20}, {'entity': 'I-PER', 'score': 0.99974835, 'index': 7, 'word': 'Smith', 'start': 21, 'end': 26}, {'entity': 'B-ORG', 'score': 0.99870986, 'index': 12, 'word': 'Apple', 'start': 41, 'end': 46}]\n</code></pre>\n<p>It looks like I might be able to post process this so <code>Jonathan Smith</code> is all one word. But ideally I would like this to be done for me and have no partial words identified.</p>\n",
    "score": 5,
    "creation_date": 1658219430,
    "view_count": 1696,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;named-entity-recognition"
  },
  {
    "question_id": 68814074,
    "title": "How to save parameters just related to classifier layer of pretrained bert model due to the memory concerns?",
    "body": "<p>I fine tuned the pretrained model <a href=\"https://huggingface.co/dbmdz/bert-base-turkish-cased\" rel=\"noreferrer\">here</a> by freezing all layers except the classifier layers. And I saved weight file with using pytorch as .bin format.</p>\n<p>Now instead of loading the 400mb pre-trained model, is there a way to load the parameters of the just Classifier layer I retrained it? By the way, I know that I have to load the original pretrained model, I just don't want to load the entire fine tuned model. due to memory concerns.</p>\n<p>I can access the last layer's parameters from state_dict as below, but how can I save them in a separate file to use them later for less memory usage?</p>\n<pre><code>model = PosTaggingModel(num_pos_tag=num_pos_tag)\nstate_dict = torch.load(&quot;model.bin&quot;)\nprint(&quot;state dictionary:&quot;,state_dict)\nwith torch.no_grad():\n    model.out_pos_tag.weight.copy_(state_dict['out_pos_tag.weight'])\n    model.out_pos_tag.bias.copy_(state_dict['out_pos_tag.bias'])\n</code></pre>\n<p>Here is the model class:</p>\n<pre><code>class PosTaggingModel(nn.Module):\n    def __init__(self, num_pos_tag):\n        super(PosTaggingModel, self).__init__()\n        self.num_pos_tag = num_pos_tag\n        self.model = AutoModel.from_pretrained(&quot;dbmdz/bert-base-turkish-cased&quot;)\n        for name, param in self.model.named_parameters():\n            if 'classifier' not in name: # classifier layer\n                param.requires_grad = False\n        self.bert_drop = nn.Dropout(0.3)\n        self.out_pos_tag = nn.Linear(768, self.num_pos_tag)\n        \n    def forward(self, ids, mask, token_type_ids, target_pos_tag):\n        o1, _ = self.model(ids, attention_mask = mask, token_type_ids = token_type_ids)\n        \n        bo_pos_tag = self.bert_drop(o1)\n        pos_tag = self.out_pos_tag(bo_pos_tag)\n\n        loss = loss_fn(pos_tag, target_pos_tag, mask, self.num_pos_tag)\n        return pos_tag, loss\n</code></pre>\n<p>I don't know if this is possible but I'm just looking for a way to save and reuse the last layer's parameters, without the need for parameters of frozen layers. I couldn't find it in the <a href=\"https://pytorch.org/tutorials/beginner/saving_loading_models.html?highlight=save\" rel=\"noreferrer\">documentation</a>.\nThanks in advance to those who will help.</p>\n",
    "score": 5,
    "creation_date": 1629188926,
    "view_count": 915,
    "answer_count": 1,
    "tags": "python;nlp;pytorch;bert-language-model;transfer-learning"
  },
  {
    "question_id": 66171956,
    "title": "Number of learnable parameters of MultiheadAttention",
    "body": "<p>While testing (using PyTorch's <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\" rel=\"nofollow noreferrer\">MultiheadAttention</a>), I noticed that increasing or decreasing the number of heads of the multi-head attention does not change the total number of learnable parameters of my model.</p>\n<p>Is this behavior correct? And if so, why?</p>\n<p>Shouldn't the number of heads affect the number of parameters the model can learn?</p>\n",
    "score": 5,
    "creation_date": 1613133106,
    "view_count": 2667,
    "answer_count": 1,
    "tags": "python;python-3.x;nlp;pytorch;attention-model"
  },
  {
    "question_id": 65221079,
    "title": "What do the logits and probabilities from RobertaForSequenceClassification represent?",
    "body": "<p>Being new to the &quot;Natural Language Processing&quot; scene, I am experimentally learning and have implemented the following segment of code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification\nimport torch\n    \npath = &quot;D:/LM/rb/&quot;\ntokenizer = RobertaTokenizer.from_pretrained(path)\nmodel = RobertaForSequenceClassification.from_pretrained(path)\n    \ninputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)\noutputs = model(**inputs)\npred_logits = outputs.logits\nprint(pred_logits)\nprobs = pred_logits.softmax(dim=-1).detach().cpu().flatten().numpy().tolist()\nprint(probs)\n</code></pre>\n<p>I <em>understand</em> that applying the model returns a <em>&quot;<code>torch.FloatTensor</code> comprising various elements depending on the configuration (RobertaConfig) and inputs&quot;</em>, and that the logits are accessible using <code>.logits</code>. As demonstrated I have applied the <em>.softmax</em> function to the tensor to return normalised probabilities and have converted the result into a list. I am outputted with the following:</p>\n<pre><code>[0.5022980570793152, 0.49770188331604004]\n</code></pre>\n<p>Do these probabilities represent some kind of overall &quot;masked&quot; probability?</p>\n<p><strong>What do the first and second index represent in context of the input?</strong></p>\n<hr />\n<p>EDIT:</p>\n<pre><code>model.num_labels\n</code></pre>\n<p>Output:</p>\n<pre><code>2\n</code></pre>\n<p><a href=\"https://stackoverflow.com/users/6664872/cronoik\">@cronoik</a> explains that the model &quot;tries to classify if a sequence belongs to one class or another&quot;</p>\n<p>Am I to assume that because there are no trained output layers these classes don't mean anything yet?</p>\n<p>For example, I <em>can</em> assume that the probability that the sentence, post analysis, belongs to class 1 is 0.5. However, what is class 1?</p>\n<p>Additionally, model cards with pre-trained output layers such as the <a href=\"https://huggingface.co/roberta-large-openai-detector\" rel=\"nofollow noreferrer\">open-ai detector</a> help differentiate between what is <a href=\"https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46\" rel=\"nofollow noreferrer\">&quot;real&quot;</a> and <a href=\"https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46\" rel=\"nofollow noreferrer\">&quot;fake&quot;</a>, and so I can assume the class that a sentence belongs to. However, how can I confirm these &quot;labels&quot; without some type of &quot;mapping.txt&quot; file?</p>\n",
    "score": 5,
    "creation_date": 1607532186,
    "view_count": 11293,
    "answer_count": 1,
    "tags": "python;nlp;pytorch;text-classification;huggingface-transformers"
  },
  {
    "question_id": 61787119,
    "title": "FastText 0.9.2 - why is recall &#39;nan&#39;?",
    "body": "<p>I trained a supervised model in FastText using the Python interface and I'm getting weird results for precision and recall.</p>\n<p>First, I trained a model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model = fasttext.train_supervised(&quot;train.txt&quot;, wordNgrams=3, epoch=100, pretrainedVectors=pretrained_model)\n</code></pre>\n<p>Then I get results for the test data:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def print_results(N, p, r):\n    print(&quot;N\\t&quot; + str(N))\n    print(&quot;P@{}\\t{:.3f}&quot;.format(1, p))\n    print(&quot;R@{}\\t{:.3f}&quot;.format(1, r))\n\nprint_results(*model.test('test.txt'))\n</code></pre>\n<p>But the results are always odd, because they show precision and recall @1 as identical, even for different datasets, e.g. one output is:</p>\n<pre><code>N   46425\nP@1 0.917\nR@1 0.917\n</code></pre>\n<p>Then when I look for the precision and recall for each label, I always get recall as 'nan':</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(model.test_label('test.txt'))\n</code></pre>\n<p>And the output is:</p>\n<pre><code>{'__label__1': {'precision': 0.9202150724134941, 'recall': nan, 'f1score': 1.8404301448269882}, '__label__5': {'precision': 0.9134956983264135, 'recall': nan, 'f1score': 1.826991396652827}}\n</code></pre>\n<p>Does anyone know why this might be happening?</p>\n<p>P.S.: To try a reproducible example of this behavior, please refer to <a href=\"https://github.com/facebookresearch/fastText/issues/1072\" rel=\"nofollow noreferrer\">https://github.com/facebookresearch/fastText/issues/1072</a> and run it with FastText 0.9.2</p>\n",
    "score": 5,
    "creation_date": 1589415685,
    "view_count": 2124,
    "answer_count": 1,
    "tags": "python-3.x;nlp;text-classification;precision-recall;fasttext"
  },
  {
    "question_id": 61569900,
    "title": "Getting embedding lookup result from BERT",
    "body": "<p>Prior to passing my tokens through BERT, I would like to perform some processing on their embeddings, (the result of the embedding lookup layer). The <a href=\"https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel\" rel=\"noreferrer\">HuggingFace BERT TensorFlow implementation</a> allows us to access the output of embedding lookup using:</p>\n\n<pre><code>import tensorflow as tf\nfrom transformers import BertConfig, BertTokenizer, TFBertModel\n\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ninput_ids = tf.constant(bert_tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[None, :]\nattention_mask = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])\ntoken_type_ids = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])\n\nconfig = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\nbert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n\nresult = bert_model(inputs={'input_ids': input_ids, \n                            'attention_mask': attention_mask, \n                            'token_type_ids': token_type_ids})\ninputs_embeds = result[-1][0]  # output of embedding lookup\n</code></pre>\n\n<p>Subsequently, one can process <code>inputs_embeds</code> and then send this in as an input to the same model using:</p>\n\n<pre><code>inputs_embeds = process(inputs_embeds)  # some processing on inputs_embeds done here (dimensions kept the same)\nresult = bert_model(inputs={'inputs_embeds': inputs_embeds, \n                            'attention_mask': attention_mask, \n                            'token_type_ids': token_type_ids})\noutput = result[0]\n</code></pre>\n\n<p>where <code>output</code> now contains the output of BERT for the modified input. However, this requires two full passes through BERT. Instead of running BERT all the way through just to perform embedding lookup, I would like to just get the output of the embedding lookup layer. <strong>Is this possible, and if so, how?</strong></p>\n",
    "score": 5,
    "creation_date": 1588482634,
    "view_count": 2779,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 60942550,
    "title": "BERT training with character embeddings",
    "body": "<p>Does it make sense to change the tokenization paradigm in the BERT model, to something else? Maybe just a simple word tokenization or character level tokenization?</p>\n",
    "score": 5,
    "creation_date": 1585621842,
    "view_count": 2940,
    "answer_count": 2,
    "tags": "nlp;pytorch;tokenize;transformer-model"
  },
  {
    "question_id": 60574112,
    "title": "Can we use GPT-2 sentence embedding for classification tasks?",
    "body": "<p>I am experimenting on the use of transformer embeddings in sentence classification tasks <strong>without finetuning them</strong>. I have used BERT embeddings and those experiments gave me very good results. Now I want to use GPT-2 embeddings (without fine-tuning). So I have two questions,</p>\n\n<ol>\n<li>Can I use GPT-2 embeddings like that (because I know Gpt-2 is\ntrained on the left to right) </li>\n<li>Is there any example uses of GPT-2 in\n    classification tasks other than generation tasks?</li>\n<li>If I can use GPT-2embeddings, how should I do it?</li>\n</ol>\n",
    "score": 5,
    "creation_date": 1583551686,
    "view_count": 7952,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;gpt-2"
  },
  {
    "question_id": 60121107,
    "title": "pytorch nllloss function target shape mismatch",
    "body": "<p>I'm training a LSTM model using pytorch with batch size of 256 and NLLLoss() as loss function.\nThe loss function is having problem with the data shape.</p>\n\n<p>The softmax output from the forward passing has shape of <code>torch.Size([256, 4, 1181])</code> where 256 is batch size, 4 is sequence length, and 1181 is vocab size.</p>\n\n<p>The target is in the shape of <code>torch.Size([256, 4])</code> where 256 is batch size and 4 is the output sequence length.</p>\n\n<p>When I was testing earlier with batch size of 1, the model works fine but when I add batch size, it is breaking. I read that NLLLoss() can take class target as input instead of one hot encoded target.</p>\n\n<p>Am I misunderstanding it? Or did I not format the shape of the target correctly?</p>\n\n<pre><code>class LSTM(nn.Module):\n\n    def __init__(self, embed_size=100, hidden_size=100, vocab_size=1181, embedding_matrix=...):\n        super(LSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n        self.word_embeddings.load_state_dict({'weight': torch.Tensor(embedding_matrix)})\n        self.word_embeddings.weight.requires_grad = False\n        self.lstm = nn.LSTM(embed_size, hidden_size)\n        self.hidden2out = nn.Linear(hidden_size, vocab_size)\n\n\n    def forward(self, tokens):\n        batch_size, num_steps = tokens.shape\n        embeds = self.word_embeddings(tokens)\n        lstm_out, _ = self.lstm(embeds.view(batch_size, num_steps, -1))\n        out_space = self.hidden2out(lstm_out.view(batch_size, num_steps, -1))\n        out_scores = F.log_softmax(out_space, dim=1)\n        return out_scores\n\nmodel = LSTM(self.config.embed_size, self.config.hidden_size, self.config.vocab_size, self.embedding_matrix)\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=self.config.lr)\n</code></pre>\n\n<p>Error:</p>\n\n<pre><code>~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n   1846         if target.size()[1:] != input.size()[2:]:\n   1847             raise ValueError('Expected target size {}, got {}'.format(\n-&gt; 1848                 out_size, target.size()))\n   1849         input = input.contiguous().view(n, c, 1, -1)\n   1850         target = target.contiguous().view(n, 1, -1)\n\nValueError: Expected target size (256, 554), got torch.Size([256, 4])\n</code></pre>\n",
    "score": 5,
    "creation_date": 1581109782,
    "view_count": 3693,
    "answer_count": 1,
    "tags": "deep-learning;nlp;pytorch;lstm;loss-function"
  },
  {
    "question_id": 58876630,
    "title": "How to export a fasttext model created by gensim, to a binary file?",
    "body": "<p>I'm trying to export the fasttext model created by gensim to a binary file. But the docs are unclear about how to achieve this. \nWhat I've done so far: </p>\n\n<pre><code>model.wv.save_word2vec_format('model.bin')\n</code></pre>\n\n<p>But this does not seems like the best solution. Since later when I want to load the model using the :</p>\n\n<pre><code>fasttext.load_facebook_model('model.bin')\n</code></pre>\n\n<p>I get into an infinite loop. While loading the <code>fasttext.model</code> created by <code>model.save('fasttext.model)</code> function gets completed in around 30 seconds.</p>\n",
    "score": 5,
    "creation_date": 1573819271,
    "view_count": 5524,
    "answer_count": 1,
    "tags": "python;nlp;gensim;fasttext"
  },
  {
    "question_id": 58299587,
    "title": "How can I keep multi-word names in tokenization together?",
    "body": "<p>I want to classify documents using TF-IDF features. One way to do it:</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nimport string\nimport re\nimport nltk\n\n\ndef tokenize(document):\n    document = document.lower()\n    for punct_char in string.punctuation:\n        document = document.replace(punct_char, \" \")\n    document = re.sub('\\s+', ' ', document).strip()\n\n    tokens = document.split(\" \")\n\n    # Contains more than I want:\n    # from spacy.lang.de.stop_words import STOP_WORDS\n    stopwords = nltk.corpus.stopwords.words('german')\n    tokens = [token for token in tokens if token not in stopwords]\n    return tokens\n\n# How I intend to use it\ntransformer = TfidfVectorizer(tokenizer=tokenize)\n\nexample = \"Jochen Schweizer ist eines der interessantesten Unternehmen der Welt, hat den Sitz allerdings nicht in der Schweizerischen Eidgenossenschaft.\"\ntransformer.fit([example])\n\n# Example of the tokenizer\nprint(tokenize(example))\n</code></pre>\n\n<p>One flaw of this tokenizer is that it splits words that belong together: \"Jochen Schweizer\" and \"schweizerische Eidgenossenschaft\". Also lemmatization (word stemming) is missing. I would like to get the following tokens:</p>\n\n<pre><code>[\"Jochen Schweizer\", \"interessantesten\", \"unternehmen\", \"Welt\", \"Sitz\", \"allerdings\", \"nicht\", \"Schweizerische Eidgenossenschaft\"]\n</code></pre>\n\n<p>I know that Spacy can identify those named entities (NER):</p>\n\n<pre><code>import en_core_web_sm  # python -m spacy download en_core_web_sm --user\nparser = en_core_web_sm.load()\ndoc = parser(example)\nprint(doc.ents)  # (Jochen Schweizer, Welt, Sitz)\n</code></pre>\n\n<p>Is there a good way to use spacy to tokenize in a way that keeps the named entity words together?</p>\n",
    "score": 5,
    "creation_date": 1570608176,
    "view_count": 2817,
    "answer_count": 1,
    "tags": "scikit-learn;nlp;nltk;spacy;named-entity-recognition"
  },
  {
    "question_id": 57767854,
    "title": "keras.preprocessing.text.Tokenizer equivalent in Pytorch?",
    "body": "<p>Basically the title; is there any equivalent to<code>keras.preprocessing.text.Tokenizer</code> in Pytorch? I have yet to find any that gives all the utilities without handcrafting things.</p>\n",
    "score": 5,
    "creation_date": 1567499423,
    "view_count": 5707,
    "answer_count": 2,
    "tags": "tensorflow;keras;nlp;pytorch"
  },
  {
    "question_id": 54850657,
    "title": "Intent classification with large number of intent classes",
    "body": "<p>I am working on a data set of approximately 3000 questions and I want to perform intent classification. <strong>The data set is not labelled yet</strong>, but from the business perspective, there's a requirement of identifying approximately <strong>80 various intent classes</strong>. Let's assume my training data has approximately equal number of each classes and is not majorly skewed towards some of the classes. I am intending to convert the text to word2vec or Glove and then feed into my classifier.</p>\n\n<p>I am familiar with cases in which I have a smaller number of intent classes, such as 8 or 10 and the choice of machine learning classifiers such as SVM, naive bais or deeplearning (CNN or LSTM).</p>\n\n<p>My question is that if you have had experience with such large number of intent classes before, and which of machine learning algorithm do you think will perform reasonably? do you think if i use deep learning frameworks, still large number of labels will cause poor performance given the above training data?</p>\n\n<p>We need to start labelling the data and it is rather laborious to come up with 80 classes of labels and then realise that it is not performing well, so I want to ensure that I am making the right decision on <strong>how many classes of intent maximum</strong> I should consider and what machine learning algorithm do you suggest?</p>\n\n<p>Thanks in advance...</p>\n",
    "score": 5,
    "creation_date": 1551001831,
    "view_count": 2559,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;text-classification"
  },
  {
    "question_id": 51486374,
    "title": "How to implement word embedding for persian language",
    "body": "<p>I have this code that works for English language but does not work for Persian language</p>\n\n<pre><code>from gensim.models import Word2Vec as wv\nfor sentence in sentences:\n    tokens = sentence.strip().lower().split(\" \")\n    tokenized.append(tokens)\nmodel = wv(tokenized\n    ,size=5,\n          min_count=1)\nprint('done2')\nmodel.save('F:/text8/text8-phrases1')\nprint('done3')\nprint(model)\nmodel = wv.load('F:/text8/text8-phrases1')\n\nprint(model.wv.vocab)\n</code></pre>\n\n<p>output</p>\n\n<pre><code>&gt; 'بر': &lt;gensim.models.keyedvectors.Vocab object at 0x0000027716EEB0B8&gt;,\n&gt; 'اساس': &lt;gensim.models.keyedvectors.Vocab object at\n&gt; 0x0000027716EEB160&gt;, 'قوانين': &lt;gensim.models.keyedvectors.Vocab\n&gt; object at 0x0000027716EEB198&gt;, 'دانشگاه':\n&gt; &lt;gensim.models.keyedvectors.Vocab object at 0x0000027716EEB1D0&gt;,\n&gt; 'اصفهان،': &lt;gensim.models.keyedvectors.Vocab object at\n&gt; 0x0000027716EEB208&gt;, 'نويسنده': &lt;gensim.models.keyedvectors.Vocab\n&gt; object at 0x0000027716EEB240&gt;, 'مسؤول':\n&gt; &lt;gensim.models.keyedvectors.Vocab object at 0x0000027716EEB278&gt;,\n&gt; 'مقاله': &lt;gensim.models.keyedvectors.Vocab object at\n&gt; 0x0000027716EEB2B0&gt;, 'بايد'\n</code></pre>\n\n<p>plesae take example with code\nthanks</p>\n",
    "score": 5,
    "creation_date": 1532375826,
    "view_count": 2854,
    "answer_count": 1,
    "tags": "keras;nlp;persian;word-embedding"
  },
  {
    "question_id": 47419335,
    "title": "Apertium translator. Is there a way to get the original phrase",
    "body": "<p>Is there a way in apertium translator to get the original phrase for a translation?</p>\n\n<p>I.E. get something like:</p>\n\n<pre><code>phrase: {\n  original: { Hola, buenos días},\n  translated: {Hello, good morning}\n}\n</code></pre>\n\n<p>I need that in order to make a mechanism to improve the translations.</p>\n",
    "score": 5,
    "creation_date": 1511285979,
    "view_count": 349,
    "answer_count": 1,
    "tags": "nlp;translation;translate;apertium"
  },
  {
    "question_id": 46965585,
    "title": "NLTK words vs word_tokenize",
    "body": "<p>I'm exploring some of NLTK's corpora and came across the following behaviour: <strong>word_tokenize() and words produce different sets of words()</strong>.</p>\n\n<p>Here is an example using webtext:</p>\n\n<pre><code>from nltk.corpus import webtext\n</code></pre>\n\n<p>When I run the following,</p>\n\n<pre><code>len(set(word_tokenize(webtext.raw('wine.txt'))))\n</code></pre>\n\n<p>I get: 3488</p>\n\n<p>When I run the following,</p>\n\n<pre><code>len(set(webtext.words('wine.txt')))\n</code></pre>\n\n<p>I get: 3414</p>\n\n<p>All I can find in the documentation is that word_tokenize is a list of punctuation and words. But it also says words is a list of punctuation and words. I'm wondering, what's going on here? Why are they different?</p>\n\n<p>I've already tried looking at the set differences.</p>\n\n<pre><code>U = set(word_tokenize(webtext.raw('wine.txt')))\nV = set(webtext.words('wine.txt'))\n\ntok_not_in_words = U.difference(V) # in tokenize but not in words\nwords_not_in_tok = V.difference(U) # in words but not in tokenize\n</code></pre>\n\n<p>All I can see is that word_tokenize contains hyphenated words and words splits the hyphenated words.</p>\n\n<p>Any help is appreciated. Thank you!  </p>\n",
    "score": 5,
    "creation_date": 1509062743,
    "view_count": 4305,
    "answer_count": 1,
    "tags": "python;nlp;nltk;tokenize;corpus"
  },
  {
    "question_id": 46129903,
    "title": "precision and recall in fastText?",
    "body": "<p>I implement the fastText for text classification, link <a href=\"https://github.com/facebookresearch/fastText/blob/master/tutorials/supervised-learning.md\" rel=\"noreferrer\">https://github.com/facebookresearch/fastText/blob/master/tutorials/supervised-learning.md</a>\nI was wondering what's the precision@1, or P@5 means? I did a binary classification, but I tested different number, I don't understand results:</p>\n\n<pre><code>haos-mbp:fastText hao$ ./fasttext test trainmodel.bin train.valid 2\nN   312\nP@2 0.5\nR@2 1\nNumber of examples: 312\nhaos-mbp:fastText hao$ ./fasttext test trainmodel.bin train.valid 1\nN   312\nP@1 0.712\nR@1 0.712\nNumber of examples: 312\nhaos-mbp:fastText hao$ ./fasttext test trainmodel.bin train.valid 3\nN   312\nP@3 0.333\nR@3 1\nNumber of examples: 312\n</code></pre>\n",
    "score": 5,
    "creation_date": 1504954494,
    "view_count": 3525,
    "answer_count": 2,
    "tags": "nlp;classification;precision;fasttext"
  },
  {
    "question_id": 38551591,
    "title": "How can I enter data using non English (Bangla) language into this database table?",
    "body": "<p>How can I enter data using non English (Bangla) language into this database table ?\n<a href=\"https://i.sstatic.net/YePmc.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/YePmc.png\" alt=\"enter image description here\"></a></p>\n",
    "score": 5,
    "creation_date": 1469360265,
    "view_count": 4000,
    "answer_count": 1,
    "tags": "mysql;sql;database;netbeans;nlp"
  },
  {
    "question_id": 35942129,
    "title": "Remove accent marks from characters while preserving other diacritics",
    "body": "<p>In a few Slavic languages, written in both Latin and Cyrillic, rising and falling accent marks are used only for disambiguation in context, ie inconsistently, only on vowels.</p>\n\n<p><strong>I would like a Python code or lib remove to acute and grave accents from vowels, while preserving other diacritics.</strong></p>\n\n<p>For example:<br>\n    <strong>жѝзнеспосо́бный</strong> -> <strong>жизнеспособный</strong><br>\n    <strong>сè се фаќа</strong> -> <strong>се се фаќа</strong><br>\n    <strong>kȕćica</strong> -> <strong>kućica</strong></p>\n\n<p>If it's any help, here is a complete list of all the actual (ie unaccented) letters in Cyrillic alphabets for Slavic languages, including those with diacritics:</p>\n\n<pre><code>абвгдежзиклмнпорстуфхцшєґіїёыіўщъьюяйјњљџђћз́с́ќѓѕ\n</code></pre>\n\n<p>Note:</p>\n\n<ol>\n<li><p><strong>їёыіўй</strong> are vowels that should keep their diacritics even when acute and grave accent marks are stripped away.  But it is very rare or perhaps impossible, we can ignore that case.</p></li>\n<li><p><strong>з́с́ќѓ</strong> are consonants, like Latin <strong>ćǵśź</strong>.  They should keep their acute accent marks - they will not have any added for pronunciation or disambiguation purposes.</p></li>\n<li><p>In the alphabets in which precise formal mappings are official, the Cyrillic equivalent of a Latin consonant with an acute accent will not necessarily have an acute accent.  (Perhaps it is helpful.)</p></li>\n<li><p>Double acute and double grave are a low priority.</p></li>\n</ol>\n\n<p>Background reading on these characters:<br>\n    <a href=\"https://en.wikipedia.org/wiki/I_with_grave_(Cyrillic)#East_Slavic_languages\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/I_with_grave_(Cyrillic)#East_Slavic_languages</a>\n    <a href=\"https://en.wikipedia.org/wiki/Shtokavian#Accentuation\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/Shtokavian#Accentuation</a><br>\n    <a href=\"https://en.wikipedia.org/wiki/Pitch_accent#Serbo-Croatian\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/Pitch_accent#Serbo-Croatian</a><br>\n    <a href=\"https://en.wikipedia.org/wiki/Bulgarian_alphabet#.D0.8D\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/Bulgarian_alphabet#.D0.8D</a><br>\n    <a href=\"https://en.wikipedia.org/wiki/Macedonian_alphabet#Accented_letters\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/Macedonian_alphabet#Accented_letters</a>   </p>\n\n<p>Similar questions:<br>\n    <a href=\"https://stackoverflow.com/questions/522715/removing-accents-diacritics-from-string-while-preserving-other-special-chars-tr\">Removing accents/diacritics from string while preserving other special chars (tried mb_chars.normalize and iconv)</a><br>\n    <a href=\"https://stackoverflow.com/questions/33328645/how-to-remove-accent-in-python-3-5-and-get-a-string-with-unicodedata-or-other-so\">How to remove accent in Python 3.5 and get a string with unicodedata or other solutions?</a></p>\n",
    "score": 5,
    "creation_date": 1457704695,
    "view_count": 1901,
    "answer_count": 2,
    "tags": "python;unicode;internationalization;nlp;cyrillic"
  },
  {
    "question_id": 35458896,
    "title": "Python: map NLTK Stanford POS tags to WordNet POS tags",
    "body": "<p>I'm reading a list of sentences and tagging each word with NLTK's Stanford POS tagger.  I get outputs like so:</p>\n\n<pre><code>wordnet_sense = []\n\nfor o in output:\n    a = st.tag(o)\n    wordnet_sense.append(a)\n</code></pre>\n\n<p>outputs: <code>[[(u'feel', u'VB'), (u'great', u'JJ')], [(u'good', u'JJ')]]</code></p>\n\n<p>I want to map these words with their POS, so that they are recognised in WordNet.</p>\n\n<p>I've attempted this:</p>\n\n<pre><code>sense = []\n\nfor i in wordnet_sense:\n    tmp = []\n\n    for tok, pos in i:\n        lower_pos = pos[0].lower()\n\n        if lower_pos in ['a', 'n', 'v', 'r', 's']:\n            res = wn.synsets(tok, lower_pos)\n            if len(res) &gt; 0:\n                a = res[0]\n        else:\n            a = \"[{0}, {1}]\".format(tok, pos)\n\n        tmp.append(a)\n\n    sense.append(tmp)\n\nprint sense\n</code></pre>\n\n<p>outputs: <code>[Synset('feel.v.01'), '[great, JJ]'], ['[good, JJ]']]</code></p>\n\n<p>So <code>feel</code> is recognised as a verb, but <code>great</code> and <code>good</code> are not recognised as adjectives.  I've also checked if <code>great</code> and <code>good</code> actually belong in Wordnet because I thought they weren't being mapped if they weren't there, but they are.  Can anyone help?</p>\n",
    "score": 5,
    "creation_date": 1455717972,
    "view_count": 3699,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet;part-of-speech"
  },
  {
    "question_id": 34053021,
    "title": "Stanford Dependency Parser Setup and NLTK",
    "body": "<p>So I got the \"standard\" Stanford Parser to work thanks to danger89's answers to this previous post, <a href=\"https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk\">Stanford Parser and NLTK</a>.</p>\n\n<p>However, I am now trying to get the dependency parser to work and it seems the method highlighted in the previous link no longer works. Here is my code:</p>\n\n<pre><code>import nltk\nimport os\njava_path = \"C:\\\\Program Files\\\\Java\\\\jre1.8.0_51\\\\bin\\\\java.exe\" \nos.environ['JAVAHOME'] = java_path\n\n\nfrom nltk.parse import stanford\nos.environ['STANFORD_PARSER'] = 'path/jar'\nos.environ['STANFORD_MODELS'] = 'path/jar'\nparser = stanford.StanfordDependencyParser(model_path=\"path/jar/englishPCFG.ser.gz\")\n\nsentences = parser.raw_parse_sents(nltk.sent_tokenize(\"The iPod is expensive but pretty.\"))\n</code></pre>\n\n<p>I get the following error: <strong><em>'module' object has no attribute 'StanfordDependencyParser'</em></strong></p>\n\n<p>The only thing I changed was \"StanfordDependencyParser\" from \"StanfordParser\". Any ideas how I can get this to work?</p>\n\n<p>I also tried the Stanford Neural Dependency parser by importing it as shown in the documentation here: <a href=\"http://www.nltk.org/_modules/nltk/parse/stanford.html\" rel=\"nofollow noreferrer\">http://www.nltk.org/_modules/nltk/parse/stanford.html</a></p>\n\n<p>This one didn't work either.</p>\n\n<p>Pretty new to NLTK. Thanks in advance for any helpful input.</p>\n",
    "score": 5,
    "creation_date": 1449090526,
    "view_count": 10207,
    "answer_count": 2,
    "tags": "python;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 31478152,
    "title": "How to use the language option in synsets (nltk) if you load a wordnet manually?",
    "body": "<p>For specific purposes I have to use the Wordnet 1.6 instead of the current version implemented in the nltk package. I then downloaded the old version <a href=\"http://wordnet.princeton.edu/wordnet/download/old-versions/\" rel=\"noreferrer\">here</a> and tried to run a simple extract of code using the french option. </p>\n\n<pre><code>from collections import defaultdict\nimport nltk\n#nltk.download() \nimport os\nimport sys\nfrom nltk.corpus import WordNetCorpusReader\n\ncwd = os.getcwd()\nnltk.data.path.append(cwd)\nwordnet16_dir=\"wordnet-1.6/\"\nwn16_path = \"{0}/dict\".format(wordnet16_dir)\nwn = WordNetCorpusReader(os.path.abspath(\"{0}/{1}\".format(cwd, wn16_path)), nltk.data.find(wn16_path))\n\nsenses=wn.synsets('gouvernement',lang=u'fre')\n</code></pre>\n\n<p>It seems that the wordnet I manually downloaded cannot be linked to the files of the nltk module dealing with foreign languages, the error I get is the following : </p>\n\n<pre><code>Traceback (most recent call last):\nFile \"C:/Users/Stephanie/Test/temp.py\", line 16, in &lt;module&gt;\nsenses=wn.synsets('gouvernement',lang=u'fre')\nFile \"C:\\Python27\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\", line 1419, in synsets\nself._load_lang_data(lang)\nFile \"C:\\Python27\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\", line 1064, in _load_lang_data\nif lang not in self.langs():\nFile \"C:\\Python27\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\", line 1088, in langs\nfileids = self._omw_reader.fileids()\nAttributeError: 'FileSystemPathPointer' object has no attribute 'fileids'\n</code></pre>\n\n<p>Using an english word doesn't generate any error (so it's not that I did not load the dictionary well) : </p>\n\n<pre><code>senses=wn.synsets('government')\nprint senses\n\n[Synset('government.n.01'), Synset('government.n.02'), Synset('government.n.03'), Synset('politics.n.02')]\n</code></pre>\n\n<p>If I use the current version of Wordnet loaded with the nltk module I don't have any problem using french (so it's not a syntax problem with the optional argument) </p>\n\n<pre><code>from nltk.corpus import wordnet as wn\nsenses=wn.synsets('gouvernement',lang=u'fre')\nprint senses\n[Synset('government.n.02'), Synset('opinion.n.05'), Synset('government.n.03'), Synset('rule.n.01'), Synset('politics.n.02'), Synset('government.n.01'), Synset('regulation.n.03'), Synset('reign.n.03')]\n</code></pre>\n\n<p>But, as precised, I really have to use the old version. I guess this might be a path problem. I've been trying to read the code of the WordNetCorpusReader function but I am quite new with python I don't really see what the problem is so far, except that it doesn't find a special file.</p>\n\n<p>The needed file seems to be wn-data-fre.tab which is located in \\nltk_data\\corpora\\omw\\fre. I am pretty sure that I have to change the file with a version compatible with wordnet 1.6 but still, why the function WordNetCorpusReader can't find it ? </p>\n",
    "score": 5,
    "creation_date": 1437143194,
    "view_count": 6866,
    "answer_count": 1,
    "tags": "python;path;nlp;nltk;wordnet"
  },
  {
    "question_id": 30812859,
    "title": "Extract word from a list of synsets in NLTK for Python",
    "body": "<p>Using this <code>[x for x in wn.all_synsets('n')]</code> I am able to get a list <code>allnouns</code> with all nouns from Wordnet with help from NLTK.</p>\n\n<p>The list <code>allnouns</code> looks like this <code>Synset('pile.n.01'), Synset('compost_heap.n.01'), Synset('mass.n.03')</code> and so on. Now I am able to get any element by using <code>allnouns[2]</code> and this should be <code>Synset('mass.n.03')</code>. </p>\n\n<p>I would like to extract only the word <em>mass</em> but for some reason I cannot treat it like a string and everything I try shows a <code>AttributeError: 'Synset' object has no attribute</code> or <code>TypeError: 'Synset' object is not subscriptable</code> or <code>&lt;bound method Synset.name of Synset('mass.n.03')&gt;</code> if I try to use .name or .pos</p>\n",
    "score": 5,
    "creation_date": 1434146772,
    "view_count": 4241,
    "answer_count": 2,
    "tags": "python;nlp;nltk;list-comprehension;wordnet"
  },
  {
    "question_id": 29275063,
    "title": "How to extend the stopword list from NLTK and remove stop words with the extended list?",
    "body": "<p>I have tried two ways of removing stopwords, both of which I run into issues:</p>\n\n<p>Method 1:</p>\n\n<pre><code>cachedStopWords = stopwords.words(\"english\")\nwords_to_remove = \"\"\"with some your just have from it's /via &amp;amp; that they your there this into providing would can't\"\"\"\nremove = tu.removal_set(words_to_remove, query)\nremove2 = tu.removal_set(cachedStopWords, query)\n</code></pre>\n\n<p>In this case, only the first remove function works. remove2 doesn't work.</p>\n\n<p>Method 2:</p>\n\n<pre><code>lines = tu.lines_cleanup([sentence for sentence in sentence_list], remove=remove)\nwords = '\\n'.join(lines).split()\nprint words # list of words\n</code></pre>\n\n<p>output looks like this <code>[\"Hello\", \"Good\", \"day\"]</code></p>\n\n<p>I try to remove stopwords from words. This is my code:</p>\n\n<pre><code>for word in words:\n    if word in cachedStopwords:\n        continue\n    else:\n        new_words='\\n'.join(word)\n\nprint new_words\n</code></pre>\n\n<p>The output looks like this:</p>\n\n<pre><code>H\ne\nl\nl\no\n</code></pre>\n\n<p>Cant figure out what is wrong with the above 2 methods. Please advice.</p>\n",
    "score": 5,
    "creation_date": 1427362488,
    "view_count": 16544,
    "answer_count": 4,
    "tags": "python;nlp;nltk;stop-words"
  },
  {
    "question_id": 16074238,
    "title": "Stanford POS Tagger not tagging Chinese text",
    "body": "<p>I'm using Stanford POS Tagger (for the first time) and while it tags English correctly, it does not seem to recognize (Simplified) Chinese even when changing the model parameter. Have I overlooked something?</p>\n<p>I've downloaded and unpacked the latest full version from here:\n<a href=\"http://nlp.stanford.edu/software/tagger.shtml\" rel=\"nofollow noreferrer\">http://nlp.stanford.edu/software/tagger.shtml</a></p>\n<p>Then I've inputed sample text into the &quot;sample-input.txt&quot;.</p>\n<blockquote>\n<p>这是一个测试的句子。这是另一个句子。</p>\n</blockquote>\n<p>Then I simply run</p>\n<blockquote>\n<p>./stanford-postagger.sh models/chinese-distsim.tagger sample-input.txt</p>\n</blockquote>\n<p>The expected output is to tag each of the words with a part of speech, but instead it recognizes the entire string of text as one word:</p>\n<blockquote>\n<p>Loading default properties from tagger models/chinese-distsim.tagger</p>\n<p>Reading POS tagger model from models/chinese-distsim.tagger ... done [3.5 sec].</p>\n<p>這是一個測試的句子。這是另一個句子。#NR</p>\n<p>Tagged 1 words at 30.30 words per second.</p>\n</blockquote>\n<p>I appreciate any help.</p>\n",
    "score": 5,
    "creation_date": 1366257616,
    "view_count": 934,
    "answer_count": 1,
    "tags": "linux;nlp;stanford-nlp;pos-tagger"
  },
  {
    "question_id": 15835563,
    "title": "How can one resolve synonyms in named-entity recognition?",
    "body": "<p>In natural language processing, named-entity recognition is the challenge of, well, recognizing named entities such as organizations, places, and most importantly <em>names</em>.</p>\n\n<p>There is a major challenge in this though that I call that of <em>synonymy</em>: <em>The Count</em> and <em>Dracula</em> are in fact referring to the same person, but it it possible that this is never discussed directly in the text.</p>\n\n<p>What would be the best algorithm to resolve these synonyms?</p>\n\n<hr>\n\n<p>If there is a feature for this in any Python-based library, I'm eager to be educated.  I'm using NLTK.</p>\n",
    "score": 5,
    "creation_date": 1365169421,
    "view_count": 3093,
    "answer_count": 1,
    "tags": "nlp;nltk;named-entity-recognition"
  },
  {
    "question_id": 15503388,
    "title": "TreeTagger installation successful but cannot open .par file",
    "body": "<p>Do anyone know how to resolve this file reading error in <code>TreeTagger</code> that is a common Natural Language Processing tool used to <code>POS</code> tag, lemmatize and chunk sentences?</p>\n\n<pre><code>alvas@ikoma:~/treetagger$ echo 'Hello world!' | cmd/tree-tagger-english \n        reading parameters ...\n\nERROR: Can't open for reading: /home/alvas/treetagger/lib/english.par\naborted.\n</code></pre>\n\n<p>I didn't encounter any possible installation problems as hinted on <a href=\"http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/installation-hints.txt\" rel=\"nofollow\">http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/installation-hints.txt</a>. \nI've followed the instructions on the webpage and it's installed properly (<a href=\"http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/#Linux\" rel=\"nofollow\">http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/#Linux</a>):</p>\n\n<pre><code>alvas@ikoma:~$ mkdir treetagger\nalvas@ikoma:~$ cd treetagger\nalvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/tree-tagger-linux-3.2.tar.gz\nalvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/tagger-scripts.tar.gz\nalvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/install-tagger.sh\nalvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/dutch-par-linux-3.2-utf8.bin.gz\nalvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/german-par-linux-3.2-utf8.bin.gz\nalvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/italian-par-linux-3.2-utf8.bin.gz\nalvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/spanish-par-linux-3.2-utf8.bin.gz\nalvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/french-par-linux-3.2-utf8.bin.gz\n\nalvas@ikoma:~/treetagger$ sh install-tagger.sh \n\nLinux version of TreeTagger installed.\nTagging scripts installed.\nGerman parameter file (Linux, UTF8) installed.\nGerman chunker parameter file (Linux) installed.\nFrench parameter file (Linux, UTF8) installed.\nFrench chunker parameter file (Linux, UTF8) installed.\nItalian parameter file (Linux, UTF8) installed.\nSpanish parameter file (Linux, UTF8) installed.\nDutch parameter file (Linux, UTF8) installed.\nPath variables modified in tagging scripts.\n\nYou might want to add /home/alvas/treetagger/cmd and /home/alvas/treetagger/bin to the PATH variable so that you do not need to specify the full path to run the tagging scripts.\n</code></pre>\n\n<p><strong>But when i try to test the software i get these errors:</strong></p>\n\n<pre><code>alvas@ikoma:~/treetagger$ echo 'Hello world!' | cmd/tree-tagger-english \n    reading parameters ...\n\nERROR: Can't open for reading: /home/alvas/treetagger/lib/english.par\naborted.\nalvas@ikoma:~/treetagger$ echo 'Das ist ein Test.' | cmd/tagger-chunker-german\n\nERROR: Can't open for reading: /home/alvas/treetagger/lib/german-chunker.par\naborted.\n\nERROR: Can't open for reading: /home/alvas/treetagger/lib/german.par\naborted.\n    reading parameters ...\n\nERROR: Can't open for reading: /home/alvas/treetagger/lib/german.par\naborted.\n</code></pre>\n",
    "score": 5,
    "creation_date": 1363706247,
    "view_count": 8263,
    "answer_count": 3,
    "tags": "installation;nlp;stemming;pos-tagger;lemmatization"
  },
  {
    "question_id": 12248648,
    "title": "Filtering out meaningless phrases",
    "body": "<p>I have an algorithm (that I can't change) that outputs a list of phrases. These phrases are intended to be \"topics\". However, some of them are meaningless on their own. Take this list:\n<br/><br/></p>\n\n<pre><code>is the fear\nfreesat\nare more likely to\nfirst sight\nan hour of\nsue apple\ndepression and\nitunes\n</code></pre>\n\n<p><br/>\nHow can I filter out those phrases that don't make sense on their own, to leave a list like the following?\n<br/><br/></p>\n\n<pre><code>freesat\nfirst sight\nsue apple\nitunes\n</code></pre>\n\n<p><br/>This will be applied to sets of phrases in many languages, but English is the priority.</p>\n",
    "score": 5,
    "creation_date": 1346678149,
    "view_count": 1435,
    "answer_count": 4,
    "tags": "nlp"
  },
  {
    "question_id": 11148405,
    "title": "Good Examples: English Parsing / Natural Language Processing",
    "body": "<p>I would like to make a calendar application that accepts plain english input better than those that exist. I have found Stanford's NLP which seems cool, but I was wondering if it's helpful to this kind of task. I can't find examples of people using it for anything. Should an app actually understand the language?  It seems like the natural english calendars that exist are looking for keywords / patterns and trying to parse that way, but I think an app could do better than that.</p>\n\n<p>My real question: Could someone tell me how to find examples of people using the NLP or a different (publicly available) english parser to make a really useful app?</p>\n",
    "score": 5,
    "creation_date": 1340322332,
    "view_count": 6408,
    "answer_count": 4,
    "tags": "parsing;nlp;stanford-nlp"
  },
  {
    "question_id": 10851959,
    "title": "How can I match words regardless of tense or form?",
    "body": "<p>I am currently working on a script that runs through a document, pulls out all keywords, and then attempts to match these keywords with those found in other documents. There are some specifics that complicate this, but they are not very pertinent to me question. Basically I would like to be able to match words regardless of the tense in which they appear. </p>\n\n<p>For example: If given the strings \"swim\", \"swam\", and \"swimming\", I would like a program that can recognize that these are all the same word, though whether it would store the word as swim, swam or swimming doesn't matter all that much to me.</p>\n\n<p>I'm aware that this problem could be mostly solved with a dictionary containing all of these word forms, but I am unaware of any dictionary that is mapped in such a way to be useful for this. I would prefer a solution or library that is compatible with Python, since that is what I am currently using for this scripting, but I would be fine with a solution in just about any language (save haskell or eiffel or something similarly obscure/difficult to work with)</p>\n",
    "score": 5,
    "creation_date": 1338560223,
    "view_count": 1591,
    "answer_count": 3,
    "tags": "python;nlp;nltk;string-matching"
  },
  {
    "question_id": 9709293,
    "title": "Interesting NLP/machine-learning style project -- analyzing privacy policies",
    "body": "<p>I wanted some input on an interesting problem I've been assigned.  The task is to analyze hundreds, and eventually thousands, of privacy policies and identify core characteristics of them.  For example, do they take the user's location?, do they share/sell with third parties?, etc. </p>\n\n<p>I've talked to a few people, read a lot about privacy policies, and thought about this myself.  Here is my current plan of attack:</p>\n\n<p>First, read a lot of privacy and find the major \"cues\" or indicators that a certain characteristic is met.  For example, if hundreds of privacy policies have the same line: \"We will take your location.\", that line could be a cue with 100% confidence that that privacy policy includes taking of the user's location.  Other cues would give much smaller degrees of confidence about a certain characteristic.. For example, the presence of the word \"location\" might increase the likelihood that the user's location is store by 25%.</p>\n\n<p>The idea would be to keep developing these cues, and their appropriate confidence intervals to the point where I could categorize all privacy policies with a high degree of confidence.  An analogy here could be made to email-spam catching systems that use Bayesian filters to identify which mail is likely commercial and unsolicited.</p>\n\n<p>I wanted to ask whether you guys think this is a good approach to this problem.  How exactly would you approach a problem like this?  Furthermore, are there any specific tools or frameworks you'd recommend using.  Any input is welcome.  This is my first time doing a project which touches on artificial intelligence, specifically machine learning and NLP.</p>\n",
    "score": 5,
    "creation_date": 1331755038,
    "view_count": 749,
    "answer_count": 3,
    "tags": "language-agnostic;artificial-intelligence;nlp;machine-learning"
  },
  {
    "question_id": 8841569,
    "title": "Find subject in incomplete sentence with NLTK",
    "body": "<p>I have a list of products that I am trying to classify into categories.  They will be described with incomplete sentences like:</p>\n\n<p>\"Solid State Drive Housing\"</p>\n\n<p>\"Hard Drive Cable\"</p>\n\n<p>\"1TB Hard Drive\"</p>\n\n<p>\"500GB Hard Drive, Refurbished from Manufacturer\"</p>\n\n<p>How can I use python and NLP to get an output like \"Housing, Cable, Drive, Drive\", or a tree that describes which word is modifying which?\nThank you in advance</p>\n",
    "score": 5,
    "creation_date": 1326398883,
    "view_count": 3414,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 8751071,
    "title": "Convert one-document-per-line to Blei&#39;s lda-c/dtm format for topic modeling?",
    "body": "<p>I am doing Latent Dirichlet Analyses for some research and keep running into a problem. Most lda software requires documents to be in doclines format, meaning a CSV or other delimited file in which each line represents the entirety of a document. However, <a href=\"http://www.cs.princeton.edu/~blei/lda-c/readme.txt\" rel=\"nofollow\">Blei's lda-c</a> and dynamic topic model software requires that data be in the format: <code>[M] [term_1]:[count] [term_2]:[count] ...  [term_N]:[count]</code> where <code>[M]</code> is the number of unique terms in the document, and the [count] associated with each term is how many times that term appeared\nin the document.  Note that <code>[term_1]</code> is an integer which indexes the\nterm; it is not a string.</p>\n\n<p>Does anyone know of a utility that will let me quickly convert to this format? Thank you.</p>\n",
    "score": 5,
    "creation_date": 1325804006,
    "view_count": 3309,
    "answer_count": 4,
    "tags": "nlp;dataform;lda"
  },
  {
    "question_id": 8502387,
    "title": "python module to remove internet jargon/slang/acronym",
    "body": "<p>Is there any python module (may be in nltk python) to remove internet slang/ chat slang like \"lol\",\"brb\" etc. If not can some one provide me a CSV file comprising of such vast list of slang? </p>\n\n<p>The website <a href=\"http://www.netlingo.com/acronyms.php\" rel=\"noreferrer\">http://www.netlingo.com/acronyms.php</a> gives the list of acronyms but I am not able to find any CSV files for using them in my program. </p>\n",
    "score": 5,
    "creation_date": 1323855965,
    "view_count": 6786,
    "answer_count": 2,
    "tags": "python;nlp;acronym"
  },
  {
    "question_id": 8157331,
    "title": "language detection",
    "body": "<p>I am using <a href=\"http://code.google.com/p/tesseract-ocr/\" rel=\"nofollow noreferrer\">tesseract</a> for OCR, mainly on invoices. However, tesseract requires to specify the language before it starts processing a file. </p>\n\n<p>I thought I am going to perform ocr based on a predefined default language. Then I'd like use the resulting text to check which language is used. If it is not the default language, I process it again in order to get a better result from tesseract. </p>\n\n<p>But how can I implement a language detection algorithm? Is there a C++ library I could use?</p>\n",
    "score": 5,
    "creation_date": 1321470916,
    "view_count": 2372,
    "answer_count": 3,
    "tags": "c++;nlp;ocr;language-detection"
  },
  {
    "question_id": 7800872,
    "title": "Brute-Force language detection",
    "body": "<p>I need an algorithm (any programming language) to test the vitality with an hill climbing algorithm for breaking a cipher for a crypto challenge. The algorithm should test how likely it is that an random-decryption (has no spaces) is an English text (also giving points for yet incomplete words!)  or just a random sequence of characters.</p>\n\n<p>I tried it with several algorithms I developed but they were not so good.</p>\n\n<p>My research:</p>\n\n<p>An enigma M4 crypto project ( <a href=\"http://www.bytereef.org/m4_project.html\" rel=\"nofollow\">http://www.bytereef.org/m4_project.html</a> ) uses the Sinkov statistics, which I want to use, too.</p>\n\n<p>The only thing I found was a document of «quebra -pedra», a Java framework that includes the Sinkov log-weight analysis I am searching for.</p>\n\n<p><a href=\"http://www.google.com/m?client=ms-android-samsung&amp;source=android-home#q=Quebra-pedra+framework+java\" rel=\"nofollow\">http://www.google.com/m?client=ms-android-samsung&amp;source=android-home#q=Quebra-pedra+framework+java</a> </p>\n\n<p>But I have not found where to download the framework. Also I have not found any implementation or description of the Sinkov test.</p>\n\n<p>I would be glad for any hints. Thanks.</p>\n",
    "score": 5,
    "creation_date": 1318894659,
    "view_count": 494,
    "answer_count": 1,
    "tags": "java;algorithm;cryptography;nlp"
  },
  {
    "question_id": 6527276,
    "title": "Discovering &quot;templates&quot; in a given text?",
    "body": "<p>If I have significant amounts of text and am trying to discover templates that occur most frequently, I was thinking of solving it using the N-Gram approach and in fact it was suggested as a solution in <a href=\"https://stackoverflow.com/questions/1426383/what-techniques-tools-are-there-for-discovering-common-phrases-in-chunks-of-text\">this</a> question as well but my requirement is slightly different. Just to clarify, I have some text like this:</p>\n\n<pre><code>I wake up every day morning and read the newspaper and then go to work\nI wake up every day morning and eat my breakfast and then go to work\nI am not sure that this is the solution but I will try\nI am not sure that this is the answer but I will try\nI am not feeling well today but I will get the work done and deliver it tomorrow\nI was not feeling well yesterday but I will get the work done and let you know by tomorrow\n</code></pre>\n\n<p>and am trying to extract \"templates\" like this:</p>\n\n<pre><code>I wake up every day morning and ... and then go to work\nI am not sure that this is the ... but I will try\nI ... not feeling well ... but I will get the work done and ... tomorrow\n</code></pre>\n\n<p>I am looking for an approach that can scale to million of lines of text so I was just wondering if I can adapt the same N-gram approach to solve this problem or are there any alternatives? </p>\n",
    "score": 5,
    "creation_date": 1309381651,
    "view_count": 169,
    "answer_count": 1,
    "tags": "language-agnostic;nlp;machine-learning;data-mining;nltk"
  },
  {
    "question_id": 5459612,
    "title": "Mallet CRF SimpleTagger Performance Tuning",
    "body": "<p>A question for anyone who has used the Java library Mallet's SimpleTagger class for Conditional Random Fields (CRF). Assume that I'm already using the multi-thread option for the maximum number of CPUs I have available (this is the case): where would I start, and would kind of things should I try if I need it to run faster?</p>\n\n<p>A related question is whether there is a way to do something similar to Stochastic Gradient Descent, which would speed up the training process?</p>\n\n<p>The type of training I want to do is simple:</p>\n\n<pre><code>Input:\nFeature1 ... FeatureN SequenceLabel\n...\n\nTest Data:\nFeature1 ... FeatureN\n...\n\nOutput:\n\nFeature1 ... FeatureN SequenceLabel\n...\n</code></pre>\n\n<p>(Where features are the output of processing I have done on the data in my own code.)</p>\n\n<p>I've had problems getting any CRF classifier other than Mallet to approximately work, but I may have to backtrack again and revisit one of the other implementations, or try a new one.</p>\n",
    "score": 5,
    "creation_date": 1301318618,
    "view_count": 2708,
    "answer_count": 2,
    "tags": "machine-learning;nlp;mallet;crf"
  },
  {
    "question_id": 4550570,
    "title": "Where to find wordlists with gender and plural for German?",
    "body": "<p>I'm trying to write a simple text mining application to try to tell a German word's gender and plural form.</p>\n\n<p>So, first of all, I need a big wordlist for training. I've searched around but could not find any list having either gender nor plural.</p>\n",
    "score": 5,
    "creation_date": 1293581872,
    "view_count": 1536,
    "answer_count": 2,
    "tags": "nlp;linguistics;corpus"
  },
  {
    "question_id": 4495762,
    "title": "Using Markov models to convert all-caps to mixed-case and related problems",
    "body": "<p>I've been thinking about using Markov techniques to restore missing information to natural language text.</p>\n\n<ul>\n<li>Restore all-caps text to mixed-case.</li>\n<li>Restore accents / diacritics to languages which should have them but have been converted to plain ASCII.</li>\n<li>Convert rough phonetic transcriptions back into native alphabets.</li>\n</ul>\n\n<p>That seems to be in order of least difficult to most difficult. Basically the problem is resolving ambiguities based on context.</p>\n\n<p>I can use Wiktionary as a dictionary and Wikipedia as a corpus using n-grams and Hidden Markov Models to resolve the ambiguities.</p>\n\n<p>Am I on the right track? Are there already some services, libraries, or tools for this sort of thing?</p>\n\n<p><strong>Examples</strong></p>\n\n<ul>\n<li>GEORGE LOST HIS SIM CARD IN THE BUSH   ⇨   George lost his SIM card in the bush</li>\n<li>tantot il rit a gorge deployee   ⇨   tantôt il rit à gorge déployée</li>\n</ul>\n",
    "score": 5,
    "creation_date": 1292897716,
    "view_count": 288,
    "answer_count": 2,
    "tags": "unicode;nlp;ambiguity;n-gram;markov-models"
  },
  {
    "question_id": 3809257,
    "title": "Variations in spelling of first name",
    "body": "<p>As part of a contact management system I have a large database of names. People frequently edit this and as a result we run into issues of the same person existing in different forms (John Smith and Jonathan Smith). I looked into word similarity but it's easy to think of name variations which are not similar at all (Richard vs Dick). I was wondering if there was a list of common English first name variations that I could use to detect and correct such errors.</p>\n",
    "score": 5,
    "creation_date": 1285640463,
    "view_count": 3387,
    "answer_count": 2,
    "tags": "database;duplicates;nlp;synonym"
  },
  {
    "question_id": 3779289,
    "title": "Natural Language Processing Algorithm for mood of an email",
    "body": "<p>One simple question (but I haven't quite found an obvious answer in the NLP stuff I've been reading, which I'm very new to):</p>\n\n<p>I want to classify emails with a probability along certain dimensions of mood. Is there an NLP package out there specifically dealing with this? Is there an obvious starting point in the literature I start reading at?</p>\n\n<p>For example, if I got a short email something like \"Hi, I'm not very impressed with your last email - you said the order amount would only be $15.95! Regards, Tom\" then it might get 8/10 for Frustration and 0/10 for Happiness.</p>\n\n<p>The actual list of moods isn't so important, but a short list of generally positive vs generally negative moods would be useful.</p>\n\n<p>Thanks in advance!</p>\n\n<p>--Trindaz on Fedang #NLP</p>\n",
    "score": 5,
    "creation_date": 1285251322,
    "view_count": 2192,
    "answer_count": 3,
    "tags": "nlp"
  },
  {
    "question_id": 1093699,
    "title": "How to implement a SIMPLE &quot;You typed ACB, did you mean ABC?&quot;",
    "body": "<p><em>I know this is not a straight up question, so if you need me to provide more information about the scope of it, let me know. There are a bunch of questions that address almost the same issue (they are linked here), but never the exact same one with the same kind of scope and objective - at least as far as I know.</em> </p>\n\n<p><strong>Context:</strong> </p>\n\n<ul>\n<li>I have a MP3 file with ID3 tags for\nartist name and song title.</li>\n<li>I have two tables Artists and Songs</li>\n<li>The ID3 tags might be slightly off (e.g. Mikaell Jacksonne)</li>\n<li>I'm using ASP.NET + C# and a MSSQL database</li>\n</ul>\n\n<p>I need to synchronize the MP3s with the database. Meaning:</p>\n\n<ol>\n<li>The user launches a script</li>\n<li>The script browses through all the MP3s</li>\n<li>The script says \"<em>Is 'Mikaell Jacksonne' 'Michael Jackson' YES/NO</em>\"</li>\n<li>The user pick and we start over</li>\n</ol>\n\n<p>Examples of what the system could find:</p>\n\n<p><em>In the database...</em></p>\n\n<pre><code>SONGS = {\"This is a great song title\", \"This is a song title\"}\nARTISTS = {\"Michael Jackson\"}\n</code></pre>\n\n<p><em>Outputs...</em></p>\n\n<pre><code>\"This is a grt song title\" did you mean \"This is a great song title\" ?\n\"This is song title\" did you mean \"This is a song title\" ?\n\"This si a song title\"  did you mean \"This is a song title\" ?\n\"This si song a title\"  did you mean \"This is a song title\" ?\n\"Jackson, Michael\" did you mean \"Michael Jackson\" ?\n\"JacksonMichael\" did you mean \"Michael Jackson\" ?\n\"Michael Jacksno\" did you mean \"Michael Jackson\" ?\n</code></pre>\n\n<p><em>etc.</em></p>\n\n<p>I read some documentation from this <a href=\"https://stackoverflow.com/questions/41424/how-do-you-implement-a-did-you-mean\">/how-do-you-implement-a-did-you-mean</a> and this is not exactly what I need since I don't want to check an entire dictionary. I also can't really use a web service since it's depending a lot on what I already have in my database. If possible I'd also like to avoid dealing with <a href=\"http://en.wikipedia.org/wiki/Edit_distance\" rel=\"nofollow noreferrer\">distances</a> and other <a href=\"https://rads.stackoverflow.com/amzn/click/com/0262133601\" rel=\"nofollow noreferrer\" rel=\"nofollow noreferrer\">complicated things</a>.</p>\n\n<hr>\n\n<p>I could use the <a href=\"http://code.google.com/apis/soapsearch/reference.html#1_3\" rel=\"nofollow noreferrer\">google api</a> (or something similar) to do this, meaning that the script will try spell checking and test it with the database, but I feel there could be a better solution since my database might end up being really specific with weird songs and artists, making spell checking useless.</p>\n\n<p>I could also try something like what has been explained <a href=\"https://stackoverflow.com/questions/39240/similar-posts-like-functionality-using-ms-sql-server\">on this post</a>, using <a href=\"http://en.wikipedia.org/wiki/Soundex\" rel=\"nofollow noreferrer\">Soundex</a> <a href=\"http://www.csharphelp.com/archives2/archive394.html\" rel=\"nofollow noreferrer\">for c#</a>.</p>\n\n<p>Using a regular spell checker won't work because I won't be using words but names and 'titles'. </p>\n\n<hr>\n\n<p>So my question is: is there a <em>relatively</em> simple way of doing this, and if so, what is it?</p>\n\n<p>Any kind of help would be appreciated.</p>\n\n<p>Thanks!</p>\n",
    "score": 5,
    "creation_date": 1246988114,
    "view_count": 1016,
    "answer_count": 4,
    "tags": "nlp;spell-checking"
  },
  {
    "question_id": 74428413,
    "title": "Understanding dimensions in MultiHeadAttention layer of Tensorflow",
    "body": "<p>I'm learning multi-head attention with <a href=\"https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853\" rel=\"noreferrer\">this article</a>.\nAs the writer claimed, the structure of MHA (by the original paper) is as follows:</p>\n<p><a href=\"https://i.sstatic.net/hwDQq.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/hwDQq.png\" alt=\"enter image description here\" /></a></p>\n<p>But the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention\" rel=\"noreferrer\"><code>MultiHeadAttention</code></a> layer of Tensorflow seems to be more flexible:</p>\n<ol>\n<li>It does not require <code>key_dim * num_heads = embed_dim</code>. Like:</li>\n</ol>\n<pre><code>layer = tf.keras.layers.MultiHeadAttention(num_heads = 2, key_dim = 4)\nx = tf.keras.Input(shape=[3, 5])\nlayer(x, x)\n# no error\n</code></pre>\n<p>Does the depth of the weight matrix in <code>tf.MHA</code> layer set to <code>key_dim * num_heads</code> regardless of <code>embed_dim</code>? So that Q/K/V can still be properly split by <code>num_heads</code>.</p>\n<ol start=\"2\">\n<li>However, the output depth of tf.MHA layer is (by default) guaranteed to be <code>embed_dim</code>. So there is a final dense layer with <code>embed_dim</code> nodes to ensure the dimension？</li>\n</ol>\n",
    "score": 5,
    "creation_date": 1668412228,
    "view_count": 4994,
    "answer_count": 2,
    "tags": "tensorflow;nlp;transformer-model;attention-model"
  },
  {
    "question_id": 72601714,
    "title": "Pytorch NLP Huggingface: model not loaded on GPU",
    "body": "<p>I have this code that init a class with a model and a tokenizer from Huggingface.\nOn Google Colab this code works fine, it loads the model on the GPU memory without problems.\nOn Google Cloud Platform it does not work, it loads the model on gpu, whatever I try.</p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>class OPT:\n    def __init__(self, model_name: str = \"facebook/opt-2.7b\", use_gpu: bool = False):\n        self.model_name = model_name\n        self.use_gpu = use_gpu and torch.cuda.is_available()\n        print(f\"Use gpu:: {self.use_gpu}\")\n\n        if self.use_gpu:\n            print(\"Using gpu\")\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_name, torch_dtype=torch.float16\n            ).cuda()\n        else:\n            print(\"Using cpu\")\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_name, torch_dtype=torch.float32, low_cpu_mem_usage=True\n            )\n\n        # the fast tokenizer currently does not work correctly\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)</code></pre>\r\n</div>\r\n</div>\r\n</p>\n<p>The printed output is correct:</p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>Use gpu:: True\nUsing gpu</code></pre>\r\n</div>\r\n</div>\r\n</p>\n<p>But the nvidia-smi says that there is no process running on the gpu:</p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+</code></pre>\r\n</div>\r\n</div>\r\n</p>\n<p>And with htop I can see that the process is using the cpu ram.</p>\n",
    "score": 5,
    "creation_date": 1655117921,
    "view_count": 8574,
    "answer_count": 2,
    "tags": "google-cloud-platform;nlp;pytorch;torch;huggingface-transformers"
  },
  {
    "question_id": 69127120,
    "title": "Gensim fasttext cannot get latest training loss",
    "body": "\n<h4>Problem description</h4>\n<p>It seems that the <code>get_latest_training_loss</code> function in <code>fasttext</code> returns only 0. Both gensim <strong>4.1.0</strong> and <strong>4.0.0</strong> do not work.</p>\n<pre><code>from gensim.models.callbacks import CallbackAny2Vec\nfrom pprint import pprint as print\nfrom gensim.models.fasttext import FastText\nfrom gensim.test.utils import datapath\n\nclass callback(CallbackAny2Vec):\n    '''Callback to print loss after each epoch.'''\n\n    def __init__(self):\n        self.epoch = 0\n\n    def on_epoch_end(self, model):\n        loss = model.get_latest_training_loss()\n        print('Loss after epoch {}: {}'.format(self.epoch, loss))\n        self.epoch += 1\n\n# Set file names for train and test data\ncorpus_file = datapath('lee_background.cor')\n\nmodel = FastText(vector_size=100, callbacks=[callback()])\n\n# build the vocabulary\nmodel.build_vocab(corpus_file=corpus_file)\n\n# train the model\nmodel.train(\n    corpus_file=corpus_file, epochs=model.epochs,\n    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n    callbacks=model.callbacks, compute_loss=True,\n)\n\nprint(model)\n</code></pre>\n<pre><code>'Loss after epoch 0: 0.0'\n'Loss after epoch 1: 0.0'\n'Loss after epoch 2: 0.0'\n'Loss after epoch 3: 0.0'\n'Loss after epoch 4: 0.0'\n</code></pre>\n<p><strong>If currently FastText does not support <code>get_latest_training_loss</code>, the documentation here needs to be removed:</strong></p>\n<p><a href=\"https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss\" rel=\"noreferrer\">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss</a></p>\n<h4>Versions</h4>\n<p>I have tried this in three different environments and neither of them works.</p>\n<p><strong>First environment:</strong></p>\n<pre><code>[GCC 9.3.0] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import platform; print(platform.platform())\nLinux-3.10.0-1160.36.2.el7.x86_64-x86_64-with-glibc2.17\n&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)\nPython 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48)\n[GCC 9.3.0]\n&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))\nBits 64\n&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)\nNumPy 1.21.2\n&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)\nSciPy 1.7.1\n&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)\ngensim 4.1.0\n&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)\nFAST_VERSION 0\n</code></pre>\n<p><strong>Second environment:</strong></p>\n<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import platform; print(platform.platform())\nmacOS-10.16-x86_64-i386-64bit\n&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)\nPython 3.9.5 (default, May 18 2021, 12:31:01)\n[Clang 10.0.0 ]\n&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))\nBits 64\n&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)\nNumPy 1.20.3\n&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)\nSciPy 1.7.1\n&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)\ngensim 4.1.0\n&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)\nFAST_VERSION 0\n</code></pre>\n<p><strong>Third environment:</strong></p>\n<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import platform; print(platform.platform())\nmacOS-10.16-x86_64-i386-64bit\n&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)\nPython 3.9.5 (default, May 18 2021, 12:31:01)\n[Clang 10.0.0 ]\n&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))\nBits 64\n&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)\nNumPy 1.20.3\n&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)\nSciPy 1.7.1\n&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)\n/Users/jinhuawang/miniconda3/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package &lt;https://pypi.org/project/python-Levenshtein/&gt; is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\ngensim 4.0.0\n&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)\nFAST_VERSION 0\n</code></pre>\n",
    "score": 5,
    "creation_date": 1631246558,
    "view_count": 681,
    "answer_count": 1,
    "tags": "python;nlp;gensim;word2vec;fasttext"
  },
  {
    "question_id": 65552688,
    "title": "How to remove English and Spanish stop words",
    "body": "<p>I am trying to delete stop words for English and Spanish. My code is working for English but not Spanish:</p>\n<pre><code>stopword = nltk.corpus.stopwords.words('english', 'spanish')\n\ndef remove_stopwords(text):\n    text = [word for word in text if word not in stopword]\n    return text\n    \ndf['Tweet_nonstop'] = df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n</code></pre>\n",
    "score": 5,
    "creation_date": 1609694110,
    "view_count": 7146,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 60631437,
    "title": "Laplace smoothing function in nltk",
    "body": "<p>I'm building a text generate model using <code>nltk.lm.MLE</code>, I notice they also have <code>nltk.lm.Laplace</code> that I can use to smooth the data to avoid a division by zero, the documentation is <pre><a href=\"https://www.nltk.org/api/nltk.lm.html\" rel=\"noreferrer\">https://www.nltk.org/api/nltk.lm.html</a> </pre> However, there's no clear example on how to use this function to smooth out test data. Can anyone kindly provides me an example.</p>\n",
    "score": 5,
    "creation_date": 1583913413,
    "view_count": 2990,
    "answer_count": 1,
    "tags": "nlp;nltk;nltk-trainer"
  },
  {
    "question_id": 58541373,
    "title": "Training times for Spacy Entity Linking model",
    "body": "<p>I'm trying to train a Spacy Entity Linking model using Wikidata and Wikipedia, using the scripts in <a href=\"https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking\" rel=\"noreferrer\">https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking</a>. I've generated the KB and moved to training the model, but that is not done yet after more than a week. How long should that take normally? (I'm not using a GPU)</p>\n\n<p>Alternatively, is there a pretrained Wikidata entity linking model I can use?</p>\n\n<p>Thanks</p>\n",
    "score": 5,
    "creation_date": 1571919956,
    "view_count": 1603,
    "answer_count": 2,
    "tags": "python;nlp;spacy;wikidata;entity-linking"
  },
  {
    "question_id": 56746191,
    "title": "Why are the matrices in BERT called Query, Key, and Value?",
    "body": "<p>Within the transformer units of <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow noreferrer\">BERT</a>, there are modules called Query, Key, and Value, or simply Q,K,V.</p>\n\n<p>Based on the BERT <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow noreferrer\">paper</a> and <a href=\"https://github.com/google-research/bert\" rel=\"nofollow noreferrer\">code</a> (particularly in <a href=\"https://github.com/google-research/bert/blob/master/modeling.py\" rel=\"nofollow noreferrer\">modeling.py</a>), my pseudocode understanding of the forward-pass of an attention module (using Q,K,V) with a single attention-head is as follows:</p>\n\n<pre><code>q_param = a matrix of learned parameters\nk_param = a matrix of learned parameters\nv_param = a matrix of learned parameters\nd = one of the matrix dimensions (scalar value)\n\ndef attention(to_tensor, from_tensor, attention_mask):\n    q = from_tensor * q_param\n    k = to_tensor * k_param\n    v = to_tensor * v_param\n\n    attention_scores = q * transpose(k) / sqrt(d)\n    attention_scores += some_function(attention_mask) #attention_mask is usually just ones\n    attention_probs = dropout(softmax(attention_scores))\n    context = attention_probs * v\n\n    return context\n</code></pre>\n\n<p>Note that BERT uses \"self-attention,\" so <code>from_tensor</code> and <code>to_tensor</code> are the same in BERT; I think both of these are simply the output from the previous layer.</p>\n\n<p><strong>Questions</strong></p>\n\n<ol>\n<li>Why are the matrices called Query, Key, and Value?</li>\n<li>Did I make any mistakes in my pseudocode representation of the algorithm?</li>\n</ol>\n",
    "score": 5,
    "creation_date": 1561430948,
    "view_count": 3895,
    "answer_count": 2,
    "tags": "python;tensorflow;deep-learning;nlp;bert-language-model"
  },
  {
    "question_id": 51951021,
    "title": "Removing German stop words in R",
    "body": "<p>I have survey data with a comments column. I am looking to so sentiment analysis on the responses. The problem is there are many languages in the data and I can't figure out how to eliminate multiple language stopwords from the set</p>\n\n<p>'nps' is my data source, nps$customer.feedback is the comments column.</p>\n\n<p>First I tokenize the data</p>\n\n<pre><code>#TOKENISE\ncomments &lt;- nps %&gt;% \n  filter(!is.na(cusotmer.feedback)) %&gt;% \n  select(cat, Comment) %&gt;% \n  group_by(row_number(), cat) \n\n  comments &lt;- comments %&gt;% ungroup()\n</code></pre>\n\n<p>Getting rid of stopwords</p>\n\n<pre><code>nps_words &lt;-  nps_words %&gt;% anti_join(stop_words, by = c('word'))\n</code></pre>\n\n<p>Then use Stemming and get_sentimets(\"bing\") to show word counts by sentiment.</p>\n\n<pre><code> #stemgraph\n  nps_words %&gt;% \n  mutate(word = wordStem(word)) %&gt;% \n  inner_join(get_sentiments(\"bing\") %&gt;% mutate(word = wordStem(word)), by = \n  c('word')) %&gt;%\n  count(cat, word, sentiment) %&gt;%\n  group_by(cat, sentiment) %&gt;%\n  top_n(7) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=reorder(word, n), y = n, fill = sentiment)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap( ~cat, scales = \"free\")  +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"Word counts by Sentiment by Category - Bing (Stemmed)\", x = \n  `\"Words\", y = \"Count\")`\n</code></pre>\n\n<p>However, \"di\" and \"die\" appear in 'negative' graph due to German text being analyzed.</p>\n\n<p>Can someone help?</p>\n\n<p>My goal is to get eliminate German stopwords using the above code. </p>\n",
    "score": 5,
    "creation_date": 1534861921,
    "view_count": 4770,
    "answer_count": 1,
    "tags": "r;text;text-mining;text-analysis"
  },
  {
    "question_id": 51659523,
    "title": "eli5: show_weights() with two labels",
    "body": "<p>I'm trying <a href=\"http://eli5.readthedocs.io/en/latest/autodocs/eli5.html#eli5.show_prediction\" rel=\"noreferrer\">eli5</a> in order to understand the contribution of terms to the prediction of certain classes.</p>\n\n<p><strong>You can run this script:</strong></p>\n\n<pre><code>import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.datasets import fetch_20newsgroups\n\n#categories = ['alt.atheism', 'soc.religion.christian']\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics']\n\nnp.random.seed(1)\ntrain = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=7)\ntest = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=7)\n\nbow_model = CountVectorizer(stop_words='english')\nclf = LogisticRegression()\npipel = Pipeline([('bow', bow),\n                 ('classifier', clf)])\n\npipel.fit(train.data, train.target)\n\nimport eli5\neli5.show_weights(clf, vec=bow, top=20)\n</code></pre>\n\n<p><strong>Problem:</strong></p>\n\n<p>When working with two labels, the output is unfortunately limited to only one table:</p>\n\n<pre><code>categories = ['alt.atheism', 'soc.religion.christian']\n</code></pre>\n\n<p><a href=\"https://i.sstatic.net/Xb5Vd.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Xb5Vd.png\" alt=\"Image 1\"></a></p>\n\n<p>However, when using three labels, it also outputs three tables.</p>\n\n<pre><code>categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics']\n</code></pre>\n\n<p><a href=\"https://i.sstatic.net/9oBn4.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/9oBn4.png\" alt=\"enter image description here\"></a></p>\n\n<p><em>Is it a bug in the software that it misses y=0 in the first output, or do I miss a statistical point?</em> I would expect to see two tables for the first case.</p>\n",
    "score": 5,
    "creation_date": 1533232138,
    "view_count": 6407,
    "answer_count": 1,
    "tags": "scikit-learn;nlp;regression"
  },
  {
    "question_id": 50598129,
    "title": "What is the default smartirs for gensim TfidfModel?",
    "body": "<p>Using <code>gensim</code>:</p>\n\n<pre><code>from gensim.models import TfidfModel\nfrom gensim.corpora import Dictionary\n\nsent0 = \"The quick brown fox jumps over the lazy brown dog .\".lower().split()\nsent1 = \"Mr brown jumps over the lazy fox .\".lower().split()\n\ndataset = [sent0, sent1]\nvocab = Dictionary(dataset)\ncorpus = [vocab.doc2bow(sent) for sent in dataset] \nmodel = TfidfModel(corpus)\n\n# To retrieve the same pd.DataFrame format.\ndocuments_tfidf_lol = [{vocab[word_idx]:tfidf_value for word_idx, tfidf_value in sent} for sent in model[corpus]]\ndocuments_tfidf = pd.DataFrame(documents_tfidf_lol)\ndocuments_tfidf.fillna(0, inplace=True)\n\ndocuments_tfidf\n</code></pre>\n\n<p>[out]:</p>\n\n<pre><code>    dog mr  quick\n0   0.707107    0.0 0.707107\n1   0.000000    1.0 0.000000\n</code></pre>\n\n<p>If we do the TF-IDF computation manually, </p>\n\n<pre><code>sent0 = \"The quick brown fox jumps over the lazy brown dog .\".lower().split()\nsent1 = \"Mr brown jumps over the lazy fox .\".lower().split()\n\ndocuments = pd.DataFrame.from_dict(list(map(Counter, [sent0, sent1])))\ndocuments.fillna(0, inplace=True, downcast='infer')\ndocuments = documents.apply(lambda x: x/sum(x))  # Normalize the TF.\ndocuments.head()\n\n# To compute the IDF for all words.\nnum_sentences, num_words = documents.shape\n\nidf_vector = [] # Lets save an ordered list of IDFS w.r.t. order of the column names.\n\nfor word in documents:\n  word_idf = math.log(num_sentences/len(documents[word].nonzero()[0]))\n  idf_vector.append(word_idf)\n\n# Compute the TF-IDF table.\ndocuments_tfidf = pd.DataFrame(documents.as_matrix() * np.array(idf_vector), \n                               columns=list(documents))\ndocuments_tfidf\n</code></pre>\n\n<p>[out]:</p>\n\n<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the\n0   0.0 0.0 0.693147    0.0 0.0 0.0 0.000000    0.0 0.693147    0.0\n1   0.0 0.0 0.000000    0.0 0.0 0.0 0.693147    0.0 0.000000    0.0\n</code></pre>\n\n<p>If we use <code>math.log2</code> instead of <code>math.log</code>:</p>\n\n<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the\n0   0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0\n1   0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0\n</code></pre>\n\n<p>It looks like <code>gensim</code>:</p>\n\n<ul>\n<li>remove the non-salient words from the TF-IDF model, it's evident when we <code>print(model[corpus])</code></li>\n<li>maybe the log base seem to be different from the log_2</li>\n<li>maybe there's some normalization going on. </li>\n</ul>\n\n<p>Looking at <a href=\"https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel\" rel=\"noreferrer\">https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel</a> , the <code>smart</code> scheme difference would have output different values but it's not clear in the docs what is the default value.</p>\n\n<p><strong>What is the default smartirs for gensim TfidfModel?</strong></p>\n\n<p><strong>What are the other default parameters that've caused the difference between a natively implemented TF-IDF and gensim's?</strong></p>\n",
    "score": 5,
    "creation_date": 1527663258,
    "view_count": 1587,
    "answer_count": 1,
    "tags": "python;nlp;gensim;information-retrieval;tf-idf"
  },
  {
    "question_id": 50448271,
    "title": "nltk.concordance gives maximum of 25 lines, no matter how i change that argument",
    "body": "<p>So i started to learn NLP via nltk book and it seems i immediately ran into a problem nobody mentioned before. </p>\n\n<p>Let's import data from nltk.book just as the book says:</p>\n\n<pre><code>from nltk.book import *\n</code></pre>\n\n<p>Now i want to continue with examples from the book:</p>\n\n<pre><code>text1.concordance(\"monstrous\")\n</code></pre>\n\n<p>Gives me:</p>\n\n<pre><code>Displaying 11 of 11 matches:\nong the former , one was of a most monstrous size . ... This came towards us ,\nON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\nll over with a heathenish array of monstrous clubs and spears . Some were thick\nd as you gazed , and wondered what monstrous cannibal and savage could ever hav\nthat has survived the flood ; most monstrous and most mountainous ! That Himmal\nthey might scout at Moby Dick as a monstrous fable , or still worse and more de\nth of Radney .'\" CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere l\ning Scenes . In connexion with the monstrous pictures of whales , I am strongly\nere to enter upon those still more monstrous stories of them which are to be fo\nght have been rummaged out of this monstrous cabinet there is no telling . But\nof Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n</code></pre>\n\n<p>So far, so good. Now i want to know concordance for word <code>whale</code> in Moby Dick.</p>\n\n<pre><code>text1.concordance(\"whale\")\nDisplaying 25 of 25 matches:\ns , and to teach them by what name a whale - fish is to be called in our tongue\nt which is not true .\" -- HACKLUYT \" WHALE . ... Sw . and Dan . HVAL . This ani\nulted .\" -- WEBSTER ' S DICTIONARY \" WHALE . ... It is more immediately from th\nISH . WAL , DUTCH . HWAL , SWEDISH . WHALE , ICELANDIC . WHALE , ENGLISH . BALE\nHWAL , SWEDISH . WHALE , ICELANDIC . WHALE , ENGLISH . BALEINE , FRENCH . BALLE\nleast , take the higgledy - piggledy whale statements , however authentic , in\n dreadful gulf of this monster ' s ( whale ' s ) mouth , are immediately lost a\n patient Job .\" -- RABELAIS . \" This whale ' s liver was two cartloads .\" -- ST\n Touching that monstrous bulk of the whale or ork we have received nothing cert\n of oil will be extracted out of one whale .\" -- IBID . \" HISTORY OF LIFE AND D\nise .\" -- KING HENRY . \" Very like a whale .\" -- HAMLET . \" Which to secure , n\nrestless paine , Like as the wounded whale to shore flies thro ' the maine .\" -\n. OF SPERMA CETI AND THE SPERMA CETI WHALE . VIDE HIS V . E . \" Like Spencer '\nt had been a sprat in the mouth of a whale .\" -- PILGRIM ' S PROGRESS . \" That\nEN ' S ANNUS MIRABILIS . \" While the whale is floating at the stern of the ship\ne ship called The Jonas - in - the - Whale . ... Some say the whale can ' t ope\n in - the - Whale . ... Some say the whale can ' t open his mouth , but that is\n masts to see whether they can see a whale , for the first discoverer has a duc\n for his pains . ... I was told of a whale taken near Shetland , that had above\noneers told me that he caught once a whale in Spitzbergen that was white all ov\n2 , one eighty feet in length of the whale - bone kind came in , which ( as I w\nn master and kill this Sperma - ceti whale , for I could never hear of any of t\n . 1729 . \"... and the breath of the whale is frequendy attended with such an i\ned with hoops and armed with ribs of whale .\" -- RAPE OF THE LOCK . \" If we com\ncontemptible in the comparison . The whale is doubtless the largest animal in c\n</code></pre>\n\n<p>Whait, it can't be right. There is no way word \"whale\" is only occurs 25 times in Moby Dick. How about word \"it\"?</p>\n\n<pre><code>text1.concordance(\"it\")\nDisplaying 25 of 25 matches:\n</code></pre>\n\n<p>Ok, lets increase amount of lines shown:</p>\n\n<pre><code>text1.concordance(\"it\", lines=100)\nDisplaying 25 of 25 matches:\n</code></pre>\n\n<p>How about decreasing it?</p>\n\n<pre><code>text1.concordance(\"it\", lines=10)\nDisplaying 10 of 25 matches:\n</code></pre>\n\n<p>It wants me to believe there is only 25 occurrences of the word \"it\"?\nWhile this is definitely a malfunction, it gets even worse with <code>width</code> argument (it does not take it into account at all). </p>\n\n<p><strong>System i use nltk with:</strong></p>\n\n<p>Win 10 64 bit;</p>\n\n<p>Python 3.6.5 32 bit</p>\n\n<p><strong>What's going on and how can i fix that?</strong></p>\n",
    "score": 5,
    "creation_date": 1526903438,
    "view_count": 1092,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 49410893,
    "title": "How can I look up an english dictionary in python?",
    "body": "<p>I am developing a Python program in order to find the etymology of words in a text. I have found out there are basically two options: parsing an online dictionary that provides etymology or using an API. I found this reply here but I don't seem to understand how to link the Oxford API with my Python program.</p>\n\n<p>Can anyone explain me how to look up a word in an english dictionary? Thank you in advance.</p>\n\n<p>Link to the question <a href=\"https://stackoverflow.com/questions/21395011/python-module-with-access-to-english-dictionaries-including-definitions-of-words\">here</a></p>\n\n<blockquote>\n  <p>Note that while WordNet does not have all English words, what about the Oxford English Dictionary? (<a href=\"http://developer.oxforddictionaries.com/\" rel=\"noreferrer\">http://developer.oxforddictionaries.com/</a>). Depending on the scope of your project, it could be a killer API.\n  Have you tried looking at Grady Ward's Moby? [link] (<a href=\"http://icon.shef.ac.uk/Moby/\" rel=\"noreferrer\">http://icon.shef.ac.uk/Moby/</a>).\n  You could add it as a lexicon in NLTK (see notes on \"Loading your own corpus\" in Section 2.1).</p>\n</blockquote>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>from nltk.corpus import PlaintextCorpusReader\r\ncorpus_root = '/usr/share/dict'\r\nwordlists = PlaintextCorpusReader(corpus_root, '.*')</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>from nltk.corpus import BracketParseCorpusReader\r\ncorpus_root = r\"C:\\corpora\\penntreebank\\parsed\\mrg\\wsj\"\r\nfile_pattern = r\".*/wsj_.*\\.mrg\"\r\nptb = BracketParseCorpusReader(corpus_root, file_pattern)</code></pre>\r\n</div>\r\n</div>\r\n</p>\n",
    "score": 5,
    "creation_date": 1521647460,
    "view_count": 5970,
    "answer_count": 2,
    "tags": "python;parsing;dictionary;nlp;nltk"
  },
  {
    "question_id": 49312591,
    "title": "tensorflow code TypeError: unsupported operand type(s) for *: &#39;int&#39; and &#39;Flag&#39;",
    "body": "<p><a href=\"https://i.sstatic.net/VN1nG.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/VN1nG.png\" alt=\"I have give the mode,but it shows error,run it in tensorflow1.6\"></a> \n<code>BATCH_QUEUE_MAX = 100</code>   </p>\n\n<pre><code>self._data_path = data_path\nself._vocab = vocab\nself._hps = hps\nself._single_pass = single_pass\n\n# Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched\nself._batch_queue = Queue.Queue(self.BATCH_QUEUE_MAX)\nself._example_queue = Queue.Queue(self.BATCH_QUEUE_MAX * self._hps.batch_size)；\n</code></pre>\n\n<p>this code suddenly can't run.Because tensorflow becomes 1.6 version?</p>\n\n<p><a href=\"https://i.sstatic.net/S0wZY.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/S0wZY.png\" alt=\"enter image description here\"></a></p>\n",
    "score": 5,
    "creation_date": 1521170021,
    "view_count": 4349,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;seq2seq"
  },
  {
    "question_id": 44742809,
    "title": "How to get a binary parse in Python",
    "body": "<p>I have data from natural language inference corpora (<a href=\"https://nlp.stanford.edu/projects/snli/\" rel=\"nofollow noreferrer\">SNLI</a>, <a href=\"http://www.nyu.edu/projects/bowman/multinli/\" rel=\"nofollow noreferrer\">multiNLI</a>) that comes in this form:</p>\n\n<pre><code>'( ( Two ( blond women ) ) ( ( are ( hugging ( one another ) ) ) . ) )'\n</code></pre>\n\n<p>They are supposed to be a binary trees (some are not very clean).</p>\n\n<p>I want to parse some of my own sentences into this format. How can I do that with NLTK or similar?</p>\n\n<p>I have found the StanfordParser, but I have not been able to find how to get this kind of a parse.</p>\n",
    "score": 5,
    "creation_date": 1498359832,
    "view_count": 1363,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 44112675,
    "title": "Identify a list of items using Natural Language Processing",
    "body": "<p>Is there a way for NLP parsers to identify a list? <br>\nFor example, <em>\"a tiger, a lion and a gorilla\"</em> should be identified as a list<br>\n(I don't need it to be identified as a list of animals; just a list would be sufficient).</p>\n\n<p>My ultimate aim is to link a common verb/word to all the items in the list.\nFor example, consider the sentence <strong>\"He found a pen, a book and a flashlight\"</strong>. Here, <em>\"found\"</em> verb should be linked to all the 3 items.</p>\n\n<p>Another example, <strong>\"He was tested negative for cancer, anemia and diabetes\"</strong>. Here, the word <em>\"negative\"</em> should be linked to the three diseases. </p>\n\n<p>Is this possible with any of the open-source NLP packages like OpenNLP or Stanford CoreNLP? Any other solution?\n<br><br><br>\n<strong>EDIT:</strong><br>\nLike mentioned in one of the answers, my initial idea was to manually parse the list and find the items by looking at the placement of commas, etc.</p>\n\n<p>But then I discovered Stanford NLP's OpenIE model. This seems to be doing a pretty good job. <br>For example, <strong>\"He has a pen and a book\"</strong> gives the 2 relations <strong>(He;has;a pen)</strong> and <strong>(He;has;a book)</strong>. </p>\n\n<p>The problem with the model is that it doesn't work for incomplete sentences like, <strong>\"has a pen and a book\"</strong>. <br>(From what I understood, this is because OpenIE can only extract triples)<br>\nIt also fails when negations are involved. Eg, <strong>\"He has no pens\"</strong>.</p>\n\n<p>Is there a solution to these problems? What are the best solutions available currently for information extraction?</p>\n",
    "score": 5,
    "creation_date": 1495455505,
    "view_count": 2667,
    "answer_count": 2,
    "tags": "algorithm;text;nlp;stanford-nlp;opennlp"
  },
  {
    "question_id": 42216995,
    "title": "What exactly are WordNet lexicographer files? Understanding how WordNet works",
    "body": "<p>I'm trying to understand the file formats of the WordNet, and the main documents are <a href=\"https://wordnet.princeton.edu/wordnet/man/wndb.5WN.html\" rel=\"noreferrer\">WNDB</a> and <a href=\"https://wordnet.princeton.edu/wordnet/man/wninput.5WN.html\" rel=\"noreferrer\">WNINPUT</a>. As I understood in WNDB, there are the files called <code>index.something</code> and <code>data.something</code>, where this <code>something</code> can be <code>noun, adv, vrb, adj</code>.</p>\n\n<p>So, if I want to know something about the word <code>dog</code> as a <code>noun</code>, I'd look into the <code>index.noun</code>, search for the word <code>dog</code>, which gives me the line:</p>\n\n<pre><code>dog n 7 5 @ ~ #m #p %p 7 1 02086723 10133978 10042764 09905672 07692347 03907626 02712903  \n</code></pre>\n\n<p>According to the WNDB documment, this line represents these data:</p>\n\n<pre><code>lemma  pos  synset_cnt  p_cnt  [ptr_symbol...]  sense_cnt  tagsense_cnt   synset_offset  [synset_offset...] \n</code></pre>\n\n<p>Where <code>lemma</code> is the word, <code>pos</code> is the identifier that tells it's a noun, <code>synset_cnt</code> tells us in how many synsets this word is included, <code>p_cnt</code> tells us how many pointers to these synsets we have, <code>[ptr_symbol]</code> is an array of pointers, <code>sense_cnt</code> and <code>tagsense_cnt</code> I didn't understand and would like an explanation, and <code>synset_offset</code> is one or more synsets to be looked into the <code>data.noun</code> file</p>\n\n<p>Ok, so I know those pointers point to something, and here are their descriptions, as written in WNINPUT:</p>\n\n<pre><code>@    Hypernym \n ~    Hyponym \n#m    Member holonym \n#p    Part holonym \n%p    Part meronym \n</code></pre>\n\n<p>I don't know how to find a Hypernym for this noun, but let's continue:</p>\n\n<p>The other important data are the <code>synset_offset</code>s, which are:</p>\n\n<pre><code>02086723 10133978 10042764 09905672 07692347 03907626 02712903  \n</code></pre>\n\n<p>Let's look at the first one, <code>02086723</code>, in <code>data.noun</code>:</p>\n\n<pre><code>02086723 05 n 03 dog 0 domestic_dog 0 Canis_familiaris 0 023 @ 02085998 n 0000 @ 01320032 n 0000 #m 02086515 n 0000 #m 08011383 n 0000 ~ 01325095 n 0000 ~ 02087384 n 0000 ~ 02087513 n 0000 ~ 02087924 n 0000 ~ 02088026 n 0000 ~ 02089774 n 0000 ~ 02106058 n 0000 ~ 02112993 n 0000 ~ 02113458 n 0000 ~ 02113610 n 0000 ~ 02113781 n 0000 ~ 02113929 n 0000 ~ 02114152 n 0000 ~ 02114278 n 0000 ~ 02115149 n 0000 ~ 02115478 n 0000 ~ 02115987 n 0000 ~ 02116630 n 0000 %p 02161498 n 0000 | a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds; \"the dog barked all night\" \n</code></pre>\n\n<p>As you can see, we've found the line that begins with <code>02086723</code>. The contents of this line are described in WNDB as:</p>\n\n<pre><code>synset_offset  lex_filenum  ss_type  w_cnt  word  lex_id  [word  lex_id...]  p_cnt  [ptr...]  [frames...]  |   gloss \n</code></pre>\n\n<p>synset_offset we already know, </p>\n\n<p><strong><code>lex_filenum</code> says in which of the lexicographers file is our word (this is the part that I don't understand the most)</strong>, </p>\n\n<p><code>ss_type</code> is <code>n</code> which tells us that it's a noun, </p>\n\n<p><code>w_cnt</code>: two digit hexadecimal integer indicating the number of words in the synset, which in this case is <code>03</code>, which means we have 3 words in this synset: <code>dog 0 domestic_dog 0 Canis_familiaris 0</code>, each one followed by a number called:</p>\n\n<p><code>lex_id</code>: one digit hexadecimal integer that, when appended onto lemma , uniquely identifies a sense within a lexicographer file</p>\n\n<pre><code>p_cnt: counts the number of pointers, which in our case is `023`, so we have 23 pointers, wow\n</code></pre>\n\n<p>After <code>p_cnt</code>, then comes the pointers, each one in the format:</p>\n\n<pre><code>pointer_symbol  synset_offset  pos  source/target \n</code></pre>\n\n<p>Where <code>pointer_symbol</code> is about the symbols like the ones I showed (@, ~, ...), </p>\n\n<p><code>synset_offset</code>: is the byte offset of the target synset in the data file corresponding to <code>pos</code> </p>\n\n<p><code>source/target</code>: field distinguishes lexical and semantic pointers. It is a four byte field, containing two two-digit hexadecimal integers. The first two digits indicates the word number in the current (source) synset, the last two digits indicate the word number in the target synset. A value of 0000 means that pointer_symbol represents a semantic relation between the current (source) synset and the target synset indicated by synset_offset .</p>\n\n<p>Ok, so let's examine the first pointer:</p>\n\n<pre><code>@ 02085998 n 0000\n</code></pre>\n\n<p>It's a pointer with symbol <code>@</code>, indicating it's a <code>Hypernym</code>, and points to the synset wiuth offset <code>02085998</code> of type <code>n</code> (noun), and <code>source/target</code> is <code>0000</code></p>\n\n<p>When I search for  in data.noun, I get </p>\n\n<pre><code>02085998 05 n 02 canine 0 canid 0 011 @ 02077948 n 0000 #m 02085690 n 0000 + 02688440 a 0101 ~ 02086324 n 0000 ~ 02086723 n 0000 ~ 02116752 n 0000 ~ 02117748 n 0000 ~ 02117987 n 0000 ~ 02119787 n 0000 ~ 02120985 n 0000 %p 02442560 n 0000 | any of various fissiped mammals with nonretractile claws and typically long muzzles  \n</code></pre>\n\n<p>which is an <code>Hypernym</code> of <code>dog</code>. So that's how you find relations betweet synsets. I guess the pointer symbols in the line for dog were just to inform which types of relations I could find for the word dog? Isn't it redundant? Because these pointer symbols are already in each of the <code>synset_offsets</code> as we seen. When we look at each <code>synset_offset</code> in <code>data.noun</code>, we can see those pointer symbols, so why they're necessary in the <code>index.noun</code> file?</p>\n\n<p>Also, see that I didn't use the lexicographers file at all. I know that in <code>data.noun</code>, specifically in the field <code>lex_filenum</code>, I can know where the data structure for <code>dog</code> is located, but <strong>what is this structure for</strong>? As you can see, I could find hypernym, and many other relations, just by looking at the <code>index</code> and <code>data</code> files, I didn't use any of the so called lexicographer files</p>\n",
    "score": 5,
    "creation_date": 1487040267,
    "view_count": 1589,
    "answer_count": 2,
    "tags": "nlp;artificial-intelligence;ontology;wordnet"
  },
  {
    "question_id": 41709318,
    "title": "What is gensim&#39;s &#39;docvecs&#39;?",
    "body": "<p><a href=\"https://i.sstatic.net/ofJqR.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ofJqR.png\" alt=\"Doc2Vec Figure 2\"></a></p>\n\n<p>The above picture is from <a href=\"https://cs.stanford.edu/~quocle/paragraph_vector.pdf\" rel=\"noreferrer\">Distributed Representations of Sentences and Documents</a>, the paper introducing Doc2Vec. I am using Gensim's implementation of Word2Vec and Doc2Vec, which are great, but I am looking for clarity on a few issues.</p>\n\n<ol>\n<li>For a given doc2vec model <code>dvm</code>, what is <code>dvm.docvecs</code>? My impression is that it is the averaged or concatenated vector that includes all of the word embedding <em>and</em> the paragraph vector, <code>d</code>. Is this correct, or is it d?</li>\n<li>Supposing <code>dvm.docvecs</code> is not <code>d</code>, can one access d by itself? How?</li>\n<li>As a bonus, how is <code>d</code> calculated? The paper only says:</li>\n</ol>\n\n<blockquote>\n  <p>In our Paragraph Vector framework (see Figure 2), every\n  paragraph is mapped to a unique vector, represented by a\n  column in matrix D and every word is also mapped to a\n  unique vector, represented by a column in matrix W.</p>\n</blockquote>\n\n<p>Thanks for any leads!</p>\n",
    "score": 5,
    "creation_date": 1484698502,
    "view_count": 8277,
    "answer_count": 1,
    "tags": "python;nlp;gensim;doc2vec"
  },
  {
    "question_id": 37933803,
    "title": "How to use SyntaxNet parser/tagger with spaCy API?",
    "body": "<p>I have been using <a href=\"https://spacy.io/\" rel=\"noreferrer\">spaCy</a> Python package to parse and tag text and using the resulting dependency tree and other attributes to derive meaning. Now I would like to use SyntaxNet's Parsey McParseface for parsing and dependency tagging (which seems better), but I would like to keep using spaCy API because it is so easy to use and it does many things that Parsey doesn't. SyntaxNet outputs POS tags and dependency tags/tree in a CoNLL-format: </p>\n\n<ol>\n<li>Bob    _    NOUN    NNP    _    2    nsubj    _    _</li>\n<li>brought    _    VERB    VBD    _    0    ROOT    _    _</li>\n<li>the    _    DET    DT    _    4    det    _    _</li>\n<li>pizza    _    NOUN    NN    _    2    dobj    _    _</li>\n<li>to    _    ADP    IN    _    2    prep    _    _</li>\n<li>Alice    _    NOUN    NNP    _    5    pobj    _    _</li>\n<li>.    _    .    .    _    2    punct    _    _</li>\n</ol>\n\n<p>and spaCy seems to be able to read CoNLL format right <a href=\"https://github.com/spacy-io/spaCy/blob/master/bin/parser/conll_parse.py\" rel=\"noreferrer\">here</a>. But I can't figure out where in spaCy's API does it take a CoNLL-fromatted string.</p>\n",
    "score": 5,
    "creation_date": 1466468523,
    "view_count": 2215,
    "answer_count": 3,
    "tags": "python;nlp;syntaxnet;spacy"
  },
  {
    "question_id": 37464591,
    "title": "Annotating a Corpus (Syntaxnet)",
    "body": "<p>I downloaded and installed SyntaxNet following <a href=\"https://github.com/tensorflow/models/tree/master/syntaxnet\">Syntax official documentation on Github</a>. following  the documentation (annotating corpus) I tried to read a <code>.conll</code> file named <code>wj.conll</code> by SyntaxNet and write the results in <code>wj-tagged.conll</code> but I could not. My questions are:</p>\n\n<ol>\n<li><p>does SyntaxNet always reads <code>.conll</code> files? (not <code>.txt</code> files?). I got a bit confused as I knew SyntaxNet reads <code>.conll</code> file for training and testing process but I am a bit suspicious that it is necessary to convert a <code>.txt</code> file to <code>.conll</code> file in order to have their Part Of Speach and Dependancy Parsing.</p></li>\n<li><p>How can I make SyntaxNet reads from files (I tired all possible ways explain in GitHub documentation about SyntaxNet and It didn't work for me)</p></li>\n</ol>\n",
    "score": 5,
    "creation_date": 1464274969,
    "view_count": 1314,
    "answer_count": 2,
    "tags": "nlp;tensorflow;syntaxnet"
  },
  {
    "question_id": 32235272,
    "title": "Normalizing a list of restaurant dishes",
    "body": "<p>I have a large data set of restaurant dishes (for example, \"Pulled Pork\", \"Beef Brisket\"...)</p>\n\n<p>I am trying to \"normalize\" (wrong word) the dishes. I want \"Pulled Pork\" and \"Pulled Pork Sandwich\" and \"Jumbo Pork Slider\" all to map to a single dish, \"Pulled Pork\". </p>\n\n<p>So far I have gotten started with NLTK using Python and had some fun playing around with frequency distributions and such.</p>\n\n<p>Does anyone have a high-level strategy to approach this problem? Perhaps some keywords I could google?</p>\n\n<p>Thanks</p>\n",
    "score": 5,
    "creation_date": 1440617828,
    "view_count": 245,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;nltk"
  },
  {
    "question_id": 31482751,
    "title": "Using SVD to plot word vector to measure similarity",
    "body": "<p>This is the code I am using to calculate a word co-occurrence matrix for immediate neighbor counts. I found the following code on the net, which uses SVD. </p>\n\n<pre><code> import numpy as np\n la = np.linalg\n words = ['I','like','enjoying','deep','learning','NLP','flying','.']\n ### A Co-occurence matrix which counts how many times the word before and after a particular word appears ( ie, like appears after I 2 times)\n arr = np.array([[0,2,1,0,0,0,0,0],[2,0,0,1,0,1,0,0],[1,0,0,0,0,0,1,0],[0,0,0,1,0,0,0,1],[0,1,0,0,0,0,0,1],[0,0,1,0,0,0,0,8],[0,2,1,0,0,0,0,0],[0,0,1,1,1,0,0,0]])\n u, s, v = la.svd(arr, full_matrices=False)\n import matplotlib.pyplot as plt\n for i in xrange(len(words)):\n     plt.text(u[i,2], u[i,3], words[i])\n</code></pre>\n\n<p>In the last line of code, the first element of U is used as an x-coordinate and the second element of U is used as a y-coordinate to project the words, to see the similarity. <em>What is the intuition behind this approach?</em> Why they are taking the 1st and 2nd elements in each row (each row represents each word) as x and y to represent a word? Please help.    </p>\n",
    "score": 5,
    "creation_date": 1437159089,
    "view_count": 2029,
    "answer_count": 2,
    "tags": "python;matplotlib;nlp;svd"
  },
  {
    "question_id": 29807175,
    "title": "How to NER and POS tag a pre-tokenized text with Stanford CoreNLP?",
    "body": "<p>I'm using the Stanford's CoreNLP Named Entity Recognizer (NER) and Part-of-Speech (POS) tagger in my application. The problem is that my code tokenizes the text beforehand and then I need to NER and POS tag each token. However I was only able to find out how to do that using the command line options but not programmatically.</p>\n\n<p>Can someone please tell me how programmatically can I NER and POS tag pretokenized text using Stanford's CoreNLP?</p>\n\n<p>Edit: </p>\n\n<p>I'm actually using the individual NER and POS instructions. So my code was written as instructed in the tutorials given in the Stanford's NER and POS packages. But I have CoreNLP in my classpath. So I have the CoreNLP in my classpath but using the tutorials in the NER and POS packages. </p>\n\n<p>Edit:</p>\n\n<p>I just found that there are instructions as how one can set the properties for CoreNLP here <a href=\"http://nlp.stanford.edu/software/corenlp.shtml\" rel=\"noreferrer\">http://nlp.stanford.edu/software/corenlp.shtml</a> but I wish if there was a quick way to do what I want with Stanford NER and POS taggers so I don't have to recode everything!</p>\n",
    "score": 5,
    "creation_date": 1429731207,
    "view_count": 1712,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp;named-entity-recognition;pos-tagger"
  },
  {
    "question_id": 28510944,
    "title": "use scikit learn tfidf vectorizer starting from counts data frame",
    "body": "<p>I have a pandas data frame with counts of words for a series of documents. Can I apply <code>sklearn.feature_extraction.text.TfidfVectorizer</code> to it to return a term-document matrix? </p>\n\n<pre><code>import pandas as pd\n\na = [1,2,3,4]\nb = [1,3,4,6]\nc = [3,4,6,1]\n\ndf = pd.DataFrame([a,b,c])\n</code></pre>\n\n<p>How can I get tfidf version of counts in df?</p>\n",
    "score": 5,
    "creation_date": 1423873210,
    "view_count": 2245,
    "answer_count": 1,
    "tags": "python;nlp;scikit-learn;tf-idf"
  },
  {
    "question_id": 25763425,
    "title": "Using FST libraries in Python",
    "body": "<p><strong>How do I install <a href=\"http://www.openfst.org/twiki/bin/view/FST/WebHome\" rel=\"nofollow noreferrer\">OpenFST</a>?</strong></p>\n<p>I have been doing it as such:</p>\n<pre><code>wget http://www.openfst.org/twiki/pub/FST/FstDownload/openfst-1.4.1.tar.gz\ntar -zxvf openfst-1.4.1.tar.gz\ncd openfst-1.4.1\n./configure\nmake\nmake install\n</code></pre>\n<p><strong>Is there any other way to install this?</strong></p>\n<p>Actually what I eventually wanted is to use OpenFST in Python, I've been using this wrapper: <a href=\"https://github.com/vchahun/pyfst\" rel=\"nofollow noreferrer\">https://github.com/vchahun/pyfst</a></p>\n<p>After installing <code>OpenFST</code>, when installing <code>pyfst</code>, I had the following problem. Anyone knows how to resolve that?:</p>\n<pre><code>$ sudo pip install pyfst\n ....\n\nerror: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n\n----------------------------------------\nCleaning up...\nCommand /usr/bin/python -c &quot;import setuptools, tokenize;__file__='/tmp/pip_build_root/pyfst/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))&quot; install --record /tmp/pip-O7BSyr-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip_build_root/pyfst\nTraceback (most recent call last):\n  File &quot;/usr/local/bin/pip&quot;, line 9, in &lt;module&gt;\n    load_entry_point('pip==1.5.5', 'console_scripts', 'pip')()\n  File &quot;/usr/local/lib/python2.7/dist-packages/pip-1.5.5-py2.7.egg/pip/__init__.py&quot;, line 185, in main\n    return command.main(cmd_args)\n  File &quot;/usr/local/lib/python2.7/dist-packages/pip-1.5.5-py2.7.egg/pip/basecommand.py&quot;, line 161, in main\n    text = '\\n'.join(complete_log)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 42: ordinal not in range(128)\n</code></pre>\n<p><strong>But is there a pure Python port of OpenFST?</strong></p>\n",
    "score": 5,
    "creation_date": 1410345985,
    "view_count": 2793,
    "answer_count": 2,
    "tags": "python;nlp;state-machine;openfst;fst"
  },
  {
    "question_id": 19991491,
    "title": "Natural Language Processing for Abstract and Concrete Text?",
    "body": "<p>I'm looking for a tool/wordlist that can identify <a href=\"http://www.cse.unsw.edu.au/~billw/nlpdict.html#abstractnoun\" rel=\"noreferrer\">abstract</a> and <a href=\"http://www.cse.unsw.edu.au/~billw/nlpdict.html#concretenoun\" rel=\"noreferrer\">concrete</a> adjectives and nouns. The closest thing I've found is Euro Wordnet, which identifies entities. <a href=\"http://www.illc.uva.nl/EuroWordNet/corebcs/ewnTopOntology.html\" rel=\"noreferrer\">First word entities</a> are concrete in nature. Second and third levels are abstract. However, Euro Wordnet is not free, and the wordlist is over a decade old.</p>\n\n<p>Anyone done this?</p>\n",
    "score": 5,
    "creation_date": 1384476648,
    "view_count": 1901,
    "answer_count": 1,
    "tags": "python;nlp;wordnet"
  },
  {
    "question_id": 18837021,
    "title": "Best Open source / free NLP engine for the job",
    "body": "<p>Let's say that I have a pull (a list) of well known phrases, like: \n{ \"I love you\", \"Your mother is is a ...\", \"I think I am pregnant\" ... } Let's say about a 1000 like these. And now I want the users to enter free text into a text box, and put some kind of NLP engine to digest the text and find the 10 most relevant phrases from the pull that may be related in a way to the text.</p>\n\n<ol>\n<li>I thought that the simplest implementation could be looking by the words. Picking each time one word and looking for similarities in some way. Not sure which?</li>\n<li>What most frightens me is the size of a vocabulary that I must support. I am a single developer of some kind of a demo, and I don't like the idea of filling in words into a table... </li>\n<li>I am looking for a free NLP engine. I am agnostic about the language it's written in, but it must be free - NOT some kind of an online service that charges by API calls..</li>\n</ol>\n",
    "score": 5,
    "creation_date": 1379364418,
    "view_count": 3315,
    "answer_count": 2,
    "tags": "nlp"
  },
  {
    "question_id": 15869147,
    "title": "Converting natural language to a math equation",
    "body": "<p>I've got a home automation system working in Java, and I want to add simple math capabilities such as addition, subtraction, multiplication, division, roots, and powers.</p>\n\n<p>At the system current state, it can convert a phrase into tags, as shown in the following examples:</p>\n\n<p>example:</p>\n\n<pre><code>Phrase: \"what is one hundred twenty two to the power of seven\"\nTagged: {QUESTION/math} {NUMBER/122} {MATH/pwr} {NUMBER/7}\n</code></pre>\n\n<p>example:</p>\n\n<pre><code>Phrase: \"twenty seven plus pi 3 squared\"\nTagged: {NUMBER/27} {MATH/add} {NUMBER/3.14159} {MATH/multiply} {MATH/pwr} {NUMBER/2}\n</code></pre>\n\n<p>This example could be just as easily converted to something like this:</p>\n\n<pre><code>27 + 3.14159 * 3^2\n</code></pre>\n\n<p>Each tag is an object that can be queried for it value.</p>\n\n<p><strong>Edit: Specific question:</strong></p>\n\n<p>So now I need a way to read that group of tags as an equation, and return a numerical result. As a last resort I could use google or wolfram alpha, but that will be slower, and I'm trying to keep the automation system completely self contained.</p>\n\n<p>If you would like to see the entire source, <a href=\"https://github.com/Sprakle/HomeAutomation/tree/master/HomeAutomation\" rel=\"nofollow\">here it is in github.</a>\nNote that I have not committed the last few few changes, so some of the math related things I gave examples will not work.</p>\n",
    "score": 5,
    "creation_date": 1365377772,
    "view_count": 2926,
    "answer_count": 4,
    "tags": "java;math;nlp"
  },
  {
    "question_id": 15348351,
    "title": "Viterbi algorithm in java",
    "body": "<p>I'm doing the coursera NLP course and the first programming assignment is to build a Viterbi decoder. I think I'm really close to finishing it but there is some elusive bug which I cannot seem to be able to trace. Here is my code: </p>\n\n<p><a href=\"http://pastie.org/private/ksmbns3gjctedu1zxrehw\" rel=\"noreferrer\">http://pastie.org/private/ksmbns3gjctedu1zxrehw</a></p>\n\n<p><a href=\"http://pastie.org/private/ssv6tc8dwnamn2qegdvww\" rel=\"noreferrer\">http://pastie.org/private/ssv6tc8dwnamn2qegdvww</a></p>\n\n<p>So far I've debugged the \"teaching\" related functions so I can say that the parameters for the algorithms are being correctly estimated. Of particular interest is the viterbi() and findW() methods. The definition of the algorithm I'm using can be found here: <a href=\"http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf\" rel=\"noreferrer\">http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf</a> on page 18. </p>\n\n<p>One thing which I'm having hard time wrapping my head around is how am I supposed to update the backpointers for the special cases when K = {1, 2} (in my case this is 0 and 1, since I'm zero-indexing my array) respectively the parameters I'm using in those cases are q({TAGSET} | *, *) and q ({TAGSET} | *, {TAGSET}). </p>\n\n<p>Hints rather than spoon-fed answers will also be highly appreciated!</p>\n",
    "score": 5,
    "creation_date": 1363035055,
    "view_count": 7650,
    "answer_count": 2,
    "tags": "java;nlp;hidden-markov-models;viterbi"
  },
  {
    "question_id": 14088688,
    "title": "Can NLTK recognise initials followed by dot?",
    "body": "<p>I am trying to use NLTK to parse Russian text, but it does not work on abbreviations and initials like А. И. Манташева and Я. Вышинский.</p>\n<p>Instead, it breaks like below:</p>\n<blockquote>\n<p>организовывал забастовки и демонстрации, поднимал рабочих на бакинских предприятиях А.</p>\n<p>И.</p>\n<p>Манташева.</p>\n</blockquote>\n<p>It did the same when I used <code>russian.pickle</code> from <a href=\"https://github.com/mhq/train_punkt\" rel=\"nofollow noreferrer\">https://github.com/mhq/train_punkt</a> ,<br />\nIs this a general NLTK limitation or language-specific?</p>\n",
    "score": 5,
    "creation_date": 1356847368,
    "view_count": 2198,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 12166819,
    "title": "Use NLTK without installing",
    "body": "<p>Learning Python with the Natural Language Toolkit has been great fun, and they work fine on my local machine, though I had to install several packages in order to use it.  Exactly how the NLTK resources are now integrated on my system remains a mystery to me, though it seems clear that the NLTK source code is not simply sitting someplace where the Python interpreter knows to find it.  </p>\n\n<p>I would like to use the Toolkit on my website, which is hosted by another company.  Simply uploading the NLTK source code files to my server and telling scripts in the root directory to \"import nltk\" has not worked; I kind of doubted it would.  </p>\n\n<p>What, then, is the difference between whatever the NLTK install routine does and straightforward imports, and why should the toolkit be inaccessible to straightforward imports?  Is there a way to use the NLTK source files without essentially altering my host's Python?</p>\n\n<p>Many thanks for your thoughts and notes.\n-G</p>\n",
    "score": 5,
    "creation_date": 1346184666,
    "view_count": 2517,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 10488343,
    "title": "Removing an &quot;empty&quot; character item from a corpus of documents in R?",
    "body": "<p>I am using the <code>tm</code> and <code>lda</code> packages in R to topic model a corpus of news articles. However, I am getting a \"non-character\" problem represented as <code>\"\"</code> that is messing up my topics. Here is my workflow:</p>\n\n<pre><code>text &lt;- Corpus(VectorSource(d$text))\nnewtext &lt;- lapply(text, tolower)\nsw &lt;- c(stopwords(\"english\"), \"ahram\", \"online\", \"egypt\", \"egypts\", \"egyptian\")\nnewtext &lt;- lapply(newtext, function(x) removePunctuation(x))\nnewtext &lt;- lapply(newtext, function(x) removeWords(x, sw))\nnewtext &lt;- lapply(newtext, function(x) removeNumbers(x))\nnewtext &lt;- lapply(newtext, function(x) stripWhitespace(x))\nd$processed &lt;- unlist(newtext)\ncorpus &lt;- lexicalize(d$processed)\nk &lt;- 40\nresult &lt;-lda.collapsed.gibbs.sampler(corpus$documents, k, corpus$vocab, 500, .02, .05,\ncompute.log.likelihood = TRUE, trace = 2L)\n</code></pre>\n\n<p>Unfortunately, when I train the lda model, everything looks great except the most frequently occurring word is \"\". I try to remedy this by removing it from the vocab as given below and reestimating the model just as above: </p>\n\n<pre><code>newtext &lt;- lapply(newtext, function(x) removeWords(x, \"\"))\n</code></pre>\n\n<p>But, it's still there, as evidenced by:</p>\n\n<pre><code>str_split(newtext[[1]], \" \")\n\n[[1]]\n [1] \"\"              \"body\"          \"mohamed\"       \"hassan\"       \n [5] \"cook\"          \"found\"         \"turkish\"       \"search\"       \n [9] \"rescue\"        \"teams\"         \"rescued\"       \"hospital\"     \n[13] \"rescue\"        \"teams\"         \"continued\"     \"search\"       \n[17] \"missing\"       \"body\"          \"cook\"          \"crew\"         \n[21] \"wereegyptians\" \"sudanese\"      \"syrians\"       \"hassan\"       \n[25] \"cook\"          \"cargo\"         \"ship\"          \"sea\"          \n[29] \"bright\"        \"crashed\"       \"thursday\"      \"port\"         \n[33] \"antalya\"       \"southern\"      \"turkey\"        \"vessel\"       \n[37] \"collided\"      \"rocks\"         \"port\"          \"thursday\"     \n[41] \"night\"         \"result\"        \"heavy\"         \"winds\"        \n[45] \"waves\"         \"crew\"          \"\"             \n</code></pre>\n\n<p>Any suggestions on how to go about removing this? Adding <code>\"\"</code> to my list of stopwords doesn't help, either.</p>\n",
    "score": 5,
    "creation_date": 1336420973,
    "view_count": 7366,
    "answer_count": 2,
    "tags": "r;text-mining;text-analysis;lda;topic-modeling"
  },
  {
    "question_id": 8688271,
    "title": "Multitask learning",
    "body": "<p>Can anybody please explain multitask learning in simple and intuitive way? May be some real\nworld problem would be useful.Mostly, these days i am seeing many people are using it for natural language processing tasks.</p>\n",
    "score": 5,
    "creation_date": 1325337036,
    "view_count": 371,
    "answer_count": 1,
    "tags": "nlp;machine-learning;stanford-nlp"
  },
  {
    "question_id": 8351918,
    "title": "Could you recommend a NLP toolkit in Prolog?",
    "body": "<p>I need to parse or tokenize English sentences.\nIs there any NLP toolkit in Prolog?\nThanks.</p>\n",
    "score": 5,
    "creation_date": 1322801435,
    "view_count": 1373,
    "answer_count": 1,
    "tags": "prolog;nlp"
  },
  {
    "question_id": 4959723,
    "title": "Searching Natural Language Sentence Structure",
    "body": "<p>What's the best way to store and search a database of natural language sentence structure  trees?</p>\n\n<p>Using <a href=\"http://incubator.apache.org/opennlp/\" rel=\"noreferrer\">OpenNLP's</a> English Treebank Parser, I can get fairly reliable sentence structure parsings for arbitrary sentences. What I'd like to do is create a tool that can extract all the doc strings from my source code, generate these trees for all sentences in the doc strings, store these trees and their associated function name in a database, and then allow a user to search the database using natural language queries.</p>\n\n<p>So, given the sentence <code>\"This uploads files to a remote machine.\"</code> for the function <code>upload_files()</code>, I'd have the tree:</p>\n\n<pre><code>(TOP\n  (S\n    (NP (DT This))\n    (VP\n      (VBZ uploads)\n      (NP (NNS files))\n      (PP (TO to) (NP (DT a) (JJ remote) (NN machine))))\n    (. .)))\n</code></pre>\n\n<p>If someone entered the query \"How can I upload files?\", equating to the tree:</p>\n\n<pre><code>(TOP\n  (SBARQ\n    (WHADVP (WRB How))\n    (SQ (MD can) (NP (PRP I)) (VP (VB upload) (NP (NNS files))))\n    (. ?)))\n</code></pre>\n\n<p>how would I store and query these trees in a SQL database?</p>\n\n<p>I've written a simple proof-of-concept script that can perform this search using a mix of regular expressions and network graph parsing, but I'm not sure how I'd implement this in a  scalable way.</p>\n\n<p>And yes, I realize my example would be trivial to retrieve using a simple keyword search. The idea I'm trying to test is how I might take advantage of grammatical structure, so I can weed-out entries with similar keywords, but a different sentence structure. For example, with the above query, I wouldn't want to retrieve the entry associated with the sentence <code>\"Checks a remote machine to find a user that uploads files.\"</code> which has similar keywords, but is obviously describing a completely different behavior.</p>\n",
    "score": 5,
    "creation_date": 1297354774,
    "view_count": 2048,
    "answer_count": 3,
    "tags": "sql;artificial-intelligence;scalability;nlp;machine-learning"
  },
  {
    "question_id": 4633794,
    "title": "Understanding Relevance Score of OpenCalais",
    "body": "<p>I am trying to understand what is the relevance score that opencalais returns associated with each entity? What does it signify and how is it to be interpreted? I would be thankful for insights into this.</p>\n",
    "score": 5,
    "creation_date": 1294488907,
    "view_count": 720,
    "answer_count": 1,
    "tags": "nlp;information-extraction;named-entity-recognition;opencalais"
  },
  {
    "question_id": 3428131,
    "title": "Extracting a set of words with the Python/NLTK, then comparing it to a standard English dictionary",
    "body": "<p>I have:</p>\n\n<pre><code>from __future__ import division\nimport nltk, re, pprint\nf = open('/home/a/Desktop/Projects/FinnegansWake/JamesJoyce-FinnegansWake.txt')\nraw = f.read()\ntokens = nltk.wordpunct_tokenize(raw)\ntext = nltk.Text(tokens)\nwords = [w.lower() for w in text]\n\nf2 = open('/home/a/Desktop/Projects/FinnegansWake/catted-several-long-Russian-novels-and-the-NYT.txt')\nenglishraw = f2.read()\nenglishtokens = nltk.wordpunct_tokenize(englishraw)\nenglishtext = nltk.Text(englishtokens)\nenglishwords = [w.lower() for w in englishwords]\n</code></pre>\n\n<p>which is straight from the NLTK manual. What I want to do next is to compare <code>vocab</code> to an exhaustive set of English words, like the OED, and extract the difference -- the set of Finnegans Wake words that have not, and probably never will, be in the OED. I'm much more of a verbal person than a math-oriented person, so I haven't figured out how to do that yet, and the manual goes into way too much detail about stuff I don't actually want to do. I'm assuming it's just one or two more lines of code, though. </p>\n",
    "score": 5,
    "creation_date": 1281132240,
    "view_count": 3347,
    "answer_count": 1,
    "tags": "python;text;set;nlp;nltk"
  },
  {
    "question_id": 2844105,
    "title": "Java or Python distributed compute job (on a student budget)?",
    "body": "<p>I have a large dataset (c. 40G) that I want to use for some NLP (largely embarrassingly parallel) over a couple of computers in the lab, to which i do <em>not</em> have root access, and only 1G of user space.\nI experimented with hadoop, but of course this was dead in the water-- the data is stored on an external usb hard drive, and i cant load it on to the dfs because of the 1G user space cap. \nI have been looking into a couple of python based options (as I'd rather use NLTK instead of Java's lingpipe if I can help it), and it seems distributed compute options look like:</p>\n\n<ul>\n<li>Ipython</li>\n<li>DISCO</li>\n</ul>\n\n<p>After my hadoop experience, i am trying to make sure i try and make an informed choice -- any help on what might be more appropriate would be greatly appreciated.</p>\n\n<p>Amazon's EC2 etc not really an option, as i have next to no budget.</p>\n",
    "score": 5,
    "creation_date": 1274020114,
    "view_count": 347,
    "answer_count": 4,
    "tags": "java;python;nlp;hadoop;nltk"
  },
  {
    "question_id": 2014004,
    "title": "Matching substrings from a dictionary to other string: suggestions?",
    "body": "<p>Hellow Stack Overflow people. I'd like some suggestions regarding the following problem. I am using Java.</p>\n\n<p>I have an array #1 with a number of Strings. For example, two of the strings might be: \"An apple fell on Newton's head\" and \"Apples grow on trees\".</p>\n\n<p>On the other side, I have another array #2 with terms like (Fruits => Apple, Orange, Peach; Items => Pen, Book; ...). I'd call this array my \"dictionary\".</p>\n\n<p>By comparing items from one array to the other, I need to see in which \"category\" the items from #1 fall into from #2. E.g. Both from #1 would fall under \"Fruits\".</p>\n\n<p>My most important consideration is speed. I need to do those operations fast. A structure allowing constant time retrieval would be good.</p>\n\n<p>I considered a Hashset with the contains() method, but it doesn't allow substrings. I also tried running regex like (apple|orange|peach|...etc) with case insensitive flag on, but I read that it will not be fast when the terms increase in number (minimum 200 to be expected). Finally, I searched, and am considering using an ArrayList with indexOf() but I don't know about its performance. I also need to know which of the terms actually matched, so in this case, it would be \"Apple\".</p>\n\n<p>Please provide your views, ideas and suggestions on this problem.</p>\n\n<p>I saw Aho-Corasick algorithm, but the keywords/terms are very likely to change often. So I don't think I can use that. Oh, I'm no expert in text mining and maths, so please elaborate on complex concepts.</p>\n\n<p>Thank you, Stack Overflow people, for your time! :)</p>\n",
    "score": 5,
    "creation_date": 1262791825,
    "view_count": 767,
    "answer_count": 3,
    "tags": "java;nlp"
  },
  {
    "question_id": 76983305,
    "title": "Fine-tuning TheBloke/Llama-2-13B-chat-GPTQ model with Hugging Face Transformers library throws Exllama error",
    "body": "<p>I am trying to fine-tune the TheBloke/Llama-2-13B-chat-GPTQ model using the Hugging Face Transformers library. I am using a JSON file for the training and validation datasets. However, I am encountering an error related to Exllama backend when I try to run the script.</p>\n<p>Here is my code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom datasets import load_dataset\nimport torch\n\n# Check GPU availability\nprint(&quot;Available GPU devices:&quot;, torch.cuda.device_count())\nprint(&quot;Name of the first available GPU:&quot;, torch.cuda.get_device_name(0))\n\n# Load model and tokenizer\nmodel_name = &quot;TheBloke/Llama-2-13B-chat-GPTQ&quot;\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Move the model to GPU\nmodel.to('cuda')\n\n# Load training and validation data\ntrain_data = load_dataset('json', data_files='train_data.jsonl')\nval_data = load_dataset('json', data_files='val_data.jsonl')\n\n# Function to format the data\ndef formatting_func(example):\n    return tokenizer(example['input'], example.get('output', ''), truncation=True, padding='max_length')\n\n# Prepare training and validation data\ntrain_data = train_data.map(formatting_func)\nval_data = val_data.map(formatting_func)\n\n# Set training arguments\ntraining_args = TrainingArguments(\n    output_dir=&quot;./output&quot;,\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n)\n\n# Start training\ntrainer.train()\n\n# Save the model\nmodel.save_pretrained(&quot;./output&quot;)\n\n</code></pre>\n<p>The error message I get is:</p>\n<pre><code>ValueError: Found modules on cpu/disk. Using Exllama backend requires all the \nmodules to be on GPU. You can deactivate exllama backend by setting \n`disable_exllama=True` in the quantization config object.\n</code></pre>\n<p>I have already moved the model to GPU using model.to('cuda'), but the error persists. Any help would be greatly appreciated.</p>\n<p>I tried moving the model to the GPU using model.to('cuda') before initiating the training process, as suggested in the Hugging Face documentation. I also ensured that my environment has all the required packages and dependencies installed. I was expecting the model to fine-tune on my custom JSON dataset without any issues.</p>\n<p>However, despite moving the model to the GPU, I still encounter the Exllama backend error. I am not sure why this is happening, as the model should be on the GPU as per my code. I am looking for a way to resolve this error and successfully fine-tune the model on my custom dataset.</p>\n",
    "score": 5,
    "creation_date": 1693058250,
    "view_count": 8368,
    "answer_count": 2,
    "tags": "nlp;huggingface-transformers;huggingface;llama;fine-tuning"
  },
  {
    "question_id": 76627232,
    "title": "How to improve the results of this neural network of finetuned BERT model?",
    "body": "<p>I'm working on a NLP classification problem where I'm trying to classify training courses into 99 categories. I managed to make a few models including <a href=\"https://stackoverflow.com/questions/76490589/valueerror-when-using-model-fit-even-with-the-vectors-being-aligned\">the Bayesian classifier</a> but it had an accuracy of 55% (very bad).</p>\n<p>Given those results, I tried to fine-tune the camemBERT model (my data is in french) to improve the model results but I never used these methods before so I tried to follow this <a href=\"https://www.kaggle.com/code/houssemayed/camembert-for-french-tweets-classification/comments\" rel=\"nofollow noreferrer\">example</a> and adapt it to my code.</p>\n<p>In the example above, there are 2 labels while I have 99 labels.</p>\n<p>I left certain parts intact</p>\n<pre><code>epochs = 5\nMAX_LEN = 128\nbatch_size = 16\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)\n</code></pre>\n<p>I selected the same variable names, in text you have the feature column and in labels you have the labels</p>\n<pre><code>text = training['Intitulé (Ce champ doit respecter la nomenclature suivante : Code action – Libellé)_x']\nlabels = training['Domaine sou domaine ']\n</code></pre>\n<p>I tokenized and padded the sequences using the same values in the example because I didn't know which values are right for my data</p>\n<pre><code>#user tokenizer to convert sentences into tokenizer\ninput_ids = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN) for sent in text]\n\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=&quot;long&quot;, truncating=&quot;post&quot;, padding=&quot;post&quot;)\n\n# Create attention masks\nattention_masks = []\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n    seq_mask = [float(i &gt; 0) for i in seq]\n    attention_masks.append(seq_mask)\n</code></pre>\n<p>I noticed that the labels are numeric in the example above so I changed my labels to numeric using this code</p>\n<pre><code>label_map = {label: i for i, label in enumerate(set(labels))}\nnumeric_labels = [label_map[label] for label in labels]\nlabels = numeric_labels\n</code></pre>\n<p>I started building the model starting with the tensors</p>\n<pre><code># Use train_test_split to split our data into train and validation sets for training\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, labels, random_state=42, test_size=0.1\n)\n\ntrain_masks, validation_masks = train_test_split(\n    attention_masks, random_state=42, test_size=0.1\n)\n\n# Convert the data to torch tensors\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Create data loaders\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n# Define the model architecture\nmodel = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=99)\n\n# Move the model to the appropriate device\nmodel.to(device)                                                           \n</code></pre>\n<p>the output is:</p>\n<pre><code>CamembertForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=99, bias=True)\n  )\n)\n</code></pre>\n<p>Then I proceeded with creating the neural network</p>\n<pre><code>param_optimizer = list(model.named_parameters())\noptimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer], 'weight_decay_rate': 0.01}]\noptimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=10e-8)\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ntrain_loss_set = []\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=&quot;Epoch&quot;):  \n    # Tracking variables for training\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n  \n    # Train the model\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        # Add batch to device CPU or GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        # Get loss value\n        loss = outputs[0]\n        # Add it to train loss list\n        train_loss_set.append(loss.item())    \n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n    \n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n\n    print(&quot;Train loss: {}&quot;.format(tr_loss / nb_tr_steps))\n\n    # Tracking variables for validation\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    # Validation of the model\n    model.eval()\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        # Add batch to device CPU or GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n            loss, logits = outputs[:2]\n    \n        # Move logits and labels to CPU if GPU is used\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    \n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n\n    print(&quot;Validation Accuracy: {}&quot;.format(eval_accuracy / nb_eval_steps))\n</code></pre>\n<p>And the code worked, but the accuracy level was at 30%, which is way worse than a Bayesian classifier that uses a very simple algorithm and straightforward calculation. This made me realize that I must have fine-tuned the model wrongly, but I don't understand fine-tuning well enough to know where I went wrong.</p>\n",
    "score": 5,
    "creation_date": 1688634564,
    "view_count": 757,
    "answer_count": 2,
    "tags": "python;tensorflow;keras;deep-learning;nlp"
  },
  {
    "question_id": 74415426,
    "title": "How to stop data shuffling while training the HuggingFace BERT model?",
    "body": "<p>I want to train a BERT transformer model using the <code>HuggingFace</code> implementation/library. During training, <code>HuggingFace</code> shuffles the training data for each epoch, but I don't want to shuffle the data. For example, if I have 5 training data and the batch size = 2, then I want the training data to be presented as [1, 2], [2, 3], [3, 4] and [4, 5]. I cannot find any resources that show how to disable the default shuffling.</p>\n",
    "score": 5,
    "creation_date": 1668277013,
    "view_count": 3678,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;bert-language-model;transformer-model"
  },
  {
    "question_id": 72774975,
    "title": "Fine Tuning Blenderbot",
    "body": "<p>I have been trying to fine-tune a conversational model of HuggingFace: Blendebot. I have tried the conventional method given on the official hugging face website which asks us to do it using the trainer.train() method. I also tried it using the .compile() method. I have tried fine-tuning using PyTorch as well as TensorFlow on my dataset. Both methods seem to fail and give us an error saying that there is no method called compile or train for the Blenderbot model.\nI have also looked everywhere online to check how Blenderbot could be fine-tuned on my custom data and nowhere does it mention properly that runs without throwing an error. I have gone through Youtube tutorials, blogs, and StackOverflow posts but none answer this question. Hoping someone would respond here and help me out. I am open to using other HuggingFace Conversational Models as well for fine-tuning.</p>\n<p>Thank you! :)</p>\n",
    "score": 5,
    "creation_date": 1656345189,
    "view_count": 1765,
    "answer_count": 1,
    "tags": "tensorflow;nlp;pytorch;huggingface-transformers;blenderbot"
  },
  {
    "question_id": 71147799,
    "title": "Create new boolean fields based on specific bigrams appearing in a tokenized pandas dataframe",
    "body": "<p>Looping over a list of bigrams to search for, I need to create a boolean field for each bigram according to whether or not it is present in a tokenized pandas series. And I'd appreciate an upvote if you think this is a good question!</p>\n<p>List of bigrams:</p>\n<pre><code>bigrams = ['data science', 'computer science', 'bachelors degree']\n</code></pre>\n<p>Dataframe:</p>\n<pre><code>df = pd.DataFrame(data={'job_description': [['data', 'science', 'degree', 'expert'],\n                                            ['computer', 'science', 'degree', 'masters'],\n                                            ['bachelors', 'degree', 'computer', 'vision'],\n                                            ['data', 'processing', 'science']]})\n</code></pre>\n<p>Desired Output:</p>\n<pre><code>                         job_description  data science computer science bachelors degree\n0        [data, science, degree, expert]          True            False            False\n1   [computer, science, degree, masters]         False             True            False\n2  [bachelors, degree, computer, vision]         False            False             True\n3             [data, bachelors, science]         False            False            False\n</code></pre>\n<p>Criteria:</p>\n<ol>\n<li>Only exact matches should be replaced (for example, flagging for 'data science' should return True for 'data science' but False for 'science data' or 'data bachelors science')</li>\n<li>Each search term should get it's own field and be concatenated to the original df</li>\n</ol>\n<p>What I've tried:</p>\n<p>Failed:  <code>df = [x for x in df['job_description'] if x in bigrams]</code></p>\n<p>Failed: <code>df[bigrams] = [[any(w==term for w in lst) for term in bigrams] for lst in df['job_description']]</code></p>\n<p>Failed: Could not adapt the approach here -&gt; <a href=\"https://stackoverflow.com/questions/8304305/match-trigrams-bigrams-and-unigrams-to-a-text-if-unigram-or-bigram-a-substrin\">Match trigrams, bigrams, and unigrams to a text; if unigram or bigram a substring of already matched trigram, pass; python</a></p>\n<p>Failed: Could not get this one to adapt, either -&gt; <a href=\"https://stackoverflow.com/questions/61328486/compare-two-bigrams-lists-and-return-the-matching-bigram\">Compare two bigrams lists and return the matching bigram</a></p>\n<p>Failed: <em>This method is very close</em>, but couldn't adapt it to bigrams -&gt; <a href=\"https://stackoverflow.com/questions/71070447/create-new-boolean-fields-based-on-specific-terms-appearing-in-a-tokenized-panda\">Create new boolean fields based on specific terms appearing in a tokenized pandas dataframe</a></p>\n<p>Thanks for any help you can provide!</p>\n",
    "score": 5,
    "creation_date": 1645037988,
    "view_count": 133,
    "answer_count": 2,
    "tags": "python;pandas;dataframe;nlp;boolean"
  },
  {
    "question_id": 69104280,
    "title": "How can I use SMOTE in a Sklearn Pipeline for a NLP Classification problem?",
    "body": "<p>I'm dealing with a multiclass classification problem, in which some classes are very imbalanced. My data looks like this:</p>\n<pre><code>product_description                  class\n&quot;This should be used to clean...&quot;    1\n&quot;Beauty product, natural...&quot;         2\n&quot;Cleaning product, be careful...&quot;    2\n&quot;Food, prepared with fruits...&quot;      2\n&quot;T-shirt, sports, white, light...&quot;   3\n&quot;Cleaning product, used to ...&quot;      2\n&quot;Blue pants, two pockets, men...&quot;    3\n</code></pre>\n<p>So I needed to make a classification model. This is what my pipeline currently looks like:</p>\n<pre><code>X = df['product_description']\ny = df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, random_state=42\n)\n\ndef text_process(mess):\n\n    STOPWORDS = stopwords.words(&quot;english&quot;)\n\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = &quot;&quot;.join(nopunc)\n\n    # Now just remove any stopwords\n    return &quot; &quot;.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])\n\npipe = Pipeline(\nsteps=[\n    (&quot;vect&quot;, CountVectorizer(analyzer= text_process)),\n    (&quot;feature_selection&quot;, SelectKBest(chi2, k=20)),\n    (&quot;polynomial&quot;, PolynomialFeatures(2)),\n    (&quot;reg&quot;, LogisticRegression()),\n]\n)\n\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n</code></pre>\n<p>However, I have a very imbalanced dataset, with the following distribution: class 1 - 80%, class 2 - 10%, class 3 - 5%, class 4 - 4%, class 5 - 1%. So I'm trying to apply SMOTE. However, I still couldn't understand where should SMOTE be applied.</p>\n<p>At first, I thought about applying SMOTE before the Pipeline, but I got the following error:</p>\n<pre><code>ValueError: could not convert string to float: '...'\n</code></pre>\n<p>So I thought about using SMOTE with the Pipeline. But I also got an error. I tried using SMOTE() in the first step and also in the second step, after CountVectorizer - this is what seemed logical to me -, but both returned the same error:</p>\n<pre><code>TypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'SMOTE()' (type &lt;class 'imblearn.over_sampling._smote.base.SMOTE'&gt;) doesn't\n</code></pre>\n<p>Any idea on how to solve this issue? What am I missing in here?</p>\n<p>Thanks</p>\n",
    "score": 5,
    "creation_date": 1631108782,
    "view_count": 1980,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp;pipeline;smote"
  },
  {
    "question_id": 67570696,
    "title": "TypeError: Parameter to MergeFrom() must be instance of same class: expected TensorShapeProto got TensorShapeProto. in tf.keras.layers.Embedding",
    "body": "<p>I'm Trying to do text Classification with <code>tensorflow.keras.layers.Embedding</code>\nand Glove.\nwhen I run the code:</p>\n<pre><code>model.add(Embedding(len(word_index) + 1,\n 100,\n weights=[embedding_matrix],\n input_length=MAX_LENGTH,\n trainable=False))\n</code></pre>\n<p>I get the error :</p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected TensorShapeProto got TensorShapeProto.\n</code></pre>\n<p>My TensorFlow ver: 1.14.0\nI'm using Win-64</p>\n",
    "score": 5,
    "creation_date": 1621258228,
    "view_count": 7523,
    "answer_count": 1,
    "tags": "python;tensorflow;keras;nlp"
  },
  {
    "question_id": 67097467,
    "title": "What is &quot;language modeling head&quot; in BertForMaskedLM",
    "body": "<p>I have recently read about BERT and want to use BertForMaskedLM for fill_mask task. I know about BERT architecture. Also, as far as I know, BertForMaskedLM is built from BERT with a language modeling head on top, but I have no idea about what <em>language modeling head</em> means here. Can anyone give me a brief explanation.</p>\n",
    "score": 5,
    "creation_date": 1618426090,
    "view_count": 6832,
    "answer_count": 2,
    "tags": "nlp;bert-language-model;huggingface-transformers;language-model"
  },
  {
    "question_id": 66608244,
    "title": "What is the best approach to measure a similarity between texts in multiple languages in python?",
    "body": "<p>So, I have a task where I need to measure the similarity between two texts. These texts are short descriptions of products from a grocery store. They always include a name of a product (for example, milk), and they may include a producer and/or size, and maybe some other characteristics of a product.</p>\n<p>I have a whole set of such texts, and then, when a new one arrives, I need to determine whether there are similar products in my database and measure how similar they are (on a scale from 0 to 100%).</p>\n<p>The thing is: the texts may be in two different languages: Ukrainian and Russian. Also, if there is a foreign brand (like, <code>Coca Cola</code>), it will be written in English.</p>\n<p>My initial idea on solving this task was to get multilingual word embeddings (where similar words in different languages are located nearby) and find the distance between those texts. However, I am not sure how efficient this will be, and if it is ok, what to start with.</p>\n<p>Because each text I have is just a set of product characteristics, some word embeddings based on a context may not work (I'm not sure in this statement, it is just my assumption).</p>\n<p>So far, I have tried to get familiar with the <a href=\"https://github.com/facebookresearch/MUSE\" rel=\"noreferrer\">MUSE</a> framework, but I encountered an <a href=\"https://github.com/facebookresearch/faiss/issues/1755\" rel=\"noreferrer\">issue</a> with <code>faiss</code> installation.</p>\n<p>Hence, my questions are:</p>\n<ul>\n<li>Is my idea with word embeddings worth trying?</li>\n<li>Is there maybe a better approach?</li>\n<li>If the idea with word embeddings is okay, which ones should I use?</li>\n</ul>\n<p><em>Note:</em> I have Windows 10 (in case some libraries don't work on Windows), and I need the library to work with Ukrainian and Russian languages.</p>\n<p>Thanks in advance for any help! Any advice would be highly appreciated!</p>\n",
    "score": 5,
    "creation_date": 1615589369,
    "view_count": 2586,
    "answer_count": 4,
    "tags": "python;nlp;multilingual;similarity;word-embedding"
  },
  {
    "question_id": 64634027,
    "title": "How to verify if two text datasets are from different distribution?",
    "body": "<p>I have two text datasets. Each dataset consists of multiple sequences and each sequence can have more than one sentence.</p>\n<p>How do I measure if both datasets are from same distribution?</p>\n<p>The purpose is to verify transfer learning from one distribution to another only if the difference between the distributions is statistically significant.</p>\n<p>I am panning to use chi-square test but not sure if it will help for text data considering the high degrees of freedom.</p>\n<p>update:\nExample:\nSupppose I want to train a sentiment classification model. I train a model on IMDb dataset and evaluate on IMDb and Yelp datasets. I found that my model trained on IMDb still does well on Yelp. But the question is how different these datasets are?</p>\n<p>Train Dataset : <a href=\"https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format?select=Train.csv\" rel=\"noreferrer\">https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format?select=Train.csv</a></p>\n<p>Eval 1: <a href=\"https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format?select=Valid.csv\" rel=\"noreferrer\">https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format?select=Valid.csv</a></p>\n<p>Eval 2: <a href=\"https://www.kaggle.com/omkarsabnis/sentiment-analysis-on-the-yelp-reviews-dataset\" rel=\"noreferrer\">https://www.kaggle.com/omkarsabnis/sentiment-analysis-on-the-yelp-reviews-dataset</a></p>\n<p>Now,</p>\n<ol>\n<li>How different are train and eval 1?</li>\n<li>How different are train and eval 2?</li>\n<li>Is the dissimilarity between train and eval 2 by chance ? What is the statistical significance and p value?</li>\n</ol>\n",
    "score": 5,
    "creation_date": 1604247270,
    "view_count": 2634,
    "answer_count": 2,
    "tags": "machine-learning;nlp;statistics;data-analysis;chi-squared"
  },
  {
    "question_id": 64521775,
    "title": "Why is my attention model worse than non-attention model",
    "body": "<p>My task was to convert english sentence to German sentence. I first did this with normal encoder-decoder network, on which I got fairly good results. Then, I tried to solve the same task with the same exact model as before, but with <strong>Bahdanau Attention</strong> in it. And, the model without attention outperformed the one with the attention.</p>\n<p>The Model's loss without attention gone from approximately 8.0 to 1.4 in 5 epochs and gone to 1.0 in 10 epochs and the loss was still reducing but at a slower rate.</p>\n<p>The Model's loss with attention gone from approximately 8.0 to 2.6 in 5 epochs and was not learning much more.</p>\n<p>None of the models were overfitting as the validation loss was also decreasing in both the models.</p>\n<p>Each English sentence had 47 words in it (after padding), and each German sentence had 54 words in it (after padding). I had 7000 English and 7000 German sentence in the training set and 3000 in the validation set.</p>\n<p>I tried almost everything like: different learning rates, different optimizer, different batch sizes, different activation functions I used in the model, tried applying batch and layer normalization, and different number of LSTM units for the encoder and decoder, but nothing makes much difference, except the normalization and increasing the data, in which the loss goes down till approx 1.5 but then again stops learning!</p>\n<p>Why did this happened? Why did the Model with Bahdanau attention failed while the one without any kind of attention was performing well?</p>\n<p>Edit 1 - I tried applying LayerNormalization before the attention, after the attention and both before and after the attention. The results were approximately the same in each case. But, this time, the loss went from approx 8.0 to 2.1 in 5 epochs, and was again not learning much. But most of the learning was done in 1 epoch as at the end of 1 epoch it reached a loss of approx 2.6 and then reached 2.1 in the next epoch, and then again not learning much.</p>\n<p>Still, the model without any attention outperforms the one with both attention and LayerNormzalization. What could be the reason to this? Are the results that I got even <em>possible</em>? How can a normal encoder-decoder network without any kind of normalization, without any dropout layer perform better than the model with both attention and LayerNormalization?</p>\n<p>Edit 2 - I tried increasing the data (I did it 7 times more than the previous one), this time, both the models performance improved a lot. But still, the model without attention performed better than the model with attention. Why is this happening?</p>\n<p>Edit 3 - I tried to debug the model by first passing just one sample from the whole training dataset. The loss started at approx 9.0 and was reducing and converging at 0. Then, I tried by passing 2 samples, the loss again started at approx 9.0, but, this time, it was just wandering between 1.5 and 2.0 for the first 400 epochs and then reducing slowly. This is a plot of how the loss reduces when I trained it with just 2 samples:</p>\n<p><a href=\"https://i.sstatic.net/DEdVp.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/DEdVp.png\" alt=\"enter image description here\" /></a></p>\n<p>This is a plot of how the loss reduces when I trained it with just 1 sample:</p>\n<p><a href=\"https://i.sstatic.net/yzNj7.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/yzNj7.png\" alt=\"enter image description here\" /></a></p>\n",
    "score": 5,
    "creation_date": 1603614658,
    "view_count": 1496,
    "answer_count": 1,
    "tags": "machine-learning;deep-learning;nlp;attention-model;encoder-decoder"
  },
  {
    "question_id": 62703391,
    "title": "Estimate token probability/logits given a sentence without computing the entire sentence",
    "body": "<p>I have a sentence like:  <code>&quot;I like sitting in my new chair and _____ about life&quot;</code>.</p>\n<p>And I have a SPECIFIC set of tokens like <code>[&quot;watch&quot;, &quot;run&quot;, &quot;think&quot;, &quot;apple&quot;, &quot;light&quot;]</code></p>\n<p>I would like to calculate the probability of each of those tokens to appear as the next word in that incomplete sentence. Hopefully I should get that the probability of <code>&quot;think&quot;</code> is higher that <code>&quot;apple&quot;</code> for instance.</p>\n<p>I am working with pytorch-transformers (GPT2LMHeadModel specifically), and a possible solution is to evaluate the score of the full sentence with each of the tokens, but when number of tokens to evaluate is on the order of 100 or 1000 then the computation time starts to be too long.</p>\n<p>It must be possible to process the sentence only once and somehow use the hidden states to calculate the probabilities of the set of tokens, but I don't know how to do it.</p>\n<p>Any ideas? Thanks in advance</p>\n<hr />\n<p>EDIT:</p>\n<p>The actual code looks like the one below (estimating the probability for the full sentence every time). For every sentence it takes about 0.1 seconds to run the <code>score()</code> method, which turns into hours if I want to evaluate some thousands of words.</p>\n<pre><code>from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport pandas as pd\n\nmodel = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)\nmodel.eval()\ntokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n\n\ndef score(sentence):\n    tokenize_input = tokenizer.tokenize(sentence)\n    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n    loss = model(tensor_input, labels=tensor_input)\n    return -loss[0].item()\n\n\ncandidates = [&quot;watch&quot;, &quot;run&quot;, &quot;think&quot;, &quot;apple&quot;, &quot;light&quot;]\nsent_template = &quot;I like sitting in my new chair and {} about life&quot;\nprint({candidate: score(sent_template.format(candidate)) for candidate in candidates})\n</code></pre>\n",
    "score": 5,
    "creation_date": 1593716755,
    "view_count": 3325,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers"
  },
  {
    "question_id": 62627157,
    "title": "Failed to run the tflite model on Interpreter due to Internal Error",
    "body": "<p>I am trying to build an offline translator for android. My model is highly inspired from this guide: <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\" rel=\"noreferrer\">https://www.tensorflow.org/tutorials/text/nmt_with_attention</a>. I just did some modifications to make sure the model is serialisable. (You can find the code for the model at the end)</p>\n<p>The model works perfectly on my jupyter notebook. I am using Tensorflow version: 2.3.0-dev20200617, I also was able to generate the tflite file using the following snippet:</p>\n<pre><code>converter = tf.lite.TFLiteConverter.from_keras_model(partial_model)\ntflite_model = converter.convert()\n\nwith tf.io.gfile.GFile('goog_nmt_v2.tflite', 'wb') as f:\n  f.write(tflite_model)\n</code></pre>\n<p>However when I used the generated tflite model to get predictions on android, it throws the error <code>java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/concatenation.cc:73 t-&gt;dims-&gt;data[d] != t0-&gt;dims-&gt;data[d] (8 != 1) Node number 84 (CONCATENATION) failed to prepare.</code></p>\n<p>This is strange because I have provided the exact same input dimensions as I did in my jupyter notebook. Here is the java code that is used to test (dummy inputs) if model runs on android:</p>\n<pre><code> HashMap&lt;Integer, Object&gt; outputVal = new HashMap&lt;&gt;();\n        for(int i=0; i&lt;2; i++) outputVal.put(i, new float[1][5]);\n        float[][] inp_test = new float[1][8];\n        float[][] enc_hidden = new float[1][1024];\n        float[][] dec_input = new float[1][1];\n        float[][] dec_test = new float[1][8];\n\n        tfLite.runForMultipleInputsOutputs(new Object[] {inp_test,enc_hidden, dec_input, dec_test},outputVal);\n</code></pre>\n<p>And here are my gradle dependencies:</p>\n<pre><code>dependencies {\n    implementation fileTree(dir: 'libs', include: ['*.jar'])\n\n    implementation 'androidx.appcompat:appcompat:1.1.0'\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\n    // This dependency adds the necessary TF op support.\n    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'\n    testImplementation 'junit:junit:4.12'\n    androidTestImplementation 'androidx.test.ext:junit:1.1.1'\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'\n}\n</code></pre>\n<p>As the error pointed, there was something wrong with dimensions at node 84. So I went ahead and visualised the tflite file using Netron. I have zoomed the concatenation node, you can find the pic of the node along with input and output dimensions <a href=\"https://i.sstatic.net/3unpX.png\" rel=\"noreferrer\">here</a>. You can find the whole generated graph <a href=\"https://drive.google.com/file/d/1DXxuke12NwwFREaSJyaoXrSNJ_nmjZ4O/view?usp=sharing\" rel=\"noreferrer\">here</a>.</p>\n<p>As it turns out, the concatenation node at location 84 isn't actually concatenating, you can see this from the input and output dimensions. It just spits out a 1X1X1 matrix after processing 1X1X1 and 1X1X256 matrix. I know tflite graph isn't same as the original model graph since a lot of operations are replaced and even removed for optimisations but this seems a little odd.</p>\n<p>I can't relate this to the error. And if it runs prefectly on jupyter, is it a framework issue or am I missing something? Also, could anyone please explain me what does the error mean by <code>t-&gt;dims-&gt;data[d] != t0-&gt;dims-&gt;data[d]</code> what is d?</p>\n<p>Please if you have answers to even any one of the question, please write it. If you require any extra details please let me know.</p>\n<p>Here is the code for the model:</p>\n<pre><code>\nTx = 8\ndef Partial_model():\n    outputs = []\n    X = tf.keras.layers.Input(shape=(Tx,))\n    partial = tf.keras.layers.Input(shape=(Tx,))\n    enc_hidden = tf.keras.layers.Input(shape=(units,))\n    dec_input = tf.keras.layers.Input(shape=(1,))\n    \n    d_i = dec_input\n    e_h = enc_hidden\n    X_i = X\n    \n    enc_output, e_h = encoder(X, enc_hidden)\n    \n    \n    dec_hidden = enc_hidden\n    print(dec_input.shape, 'inp', dec_hidden.shape, 'dec_hidd')\n    for t in range(1, Tx):\n        print(t, 'tt')\n      # passing enc_output to the decoder\n        predictions, dec_hidden, _ = decoder(d_i, dec_hidden, enc_output)\n#         outputs.append(predictions)\n        print(predictions.shape, 'pred')\n        d_i = tf.reshape(partial[:, t], (-1, 1))\n        print(dec_input.shape, 'dec_input')\n    \n    predictions, dec_hidden, _ = decoder(d_i, dec_hidden, enc_output)\n    d_i = tf.squeeze(d_i)\n    \n    outputs.append(tf.math.top_k(predictions, 5))\n    \n    return tf.keras.Model(inputs = [X, enc_hidden, dec_input, partial], outputs = [outputs[0][0], outputs[0][1]])\n\n\n\n\nclass Encoder():\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n  def __call__(self, x, hidden):\n    x = self.embedding(x)\n    output, state = self.gru(x, initial_state = hidden)\n    print(output.shape, hidden.shape, &quot;out&quot;, &quot;hid&quot;)\n    return output, state\n\n\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))\n\n\n\nclass BahdanauAttention():\n  def __init__(self, units):\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def __call__(self, query, values):\n    # query hidden state shape == (batch_size, hidden size)\n    # query_with_time_axis shape == (batch_size, 1, hidden size)\n    # values shape == (batch_size, max_len, hidden size)\n    # we are doing this to broadcast addition along the time axis to calculate the score\n    print(query.shape, 'shape')\n    query_with_time_axis = tf.expand_dims(query, 1)\n    # score shape == (batch_size, max_length, 1)\n    # we get 1 at the last axis because we are applying score to self.V\n    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n    print(&quot;2&quot;)\n    score = self.V(tf.nn.tanh(\n        self.W1(query_with_time_axis) + self.W2(values)))\n    print(&quot;3&quot;)\n\n    # attention_weights shape == (batch_size, max_length, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n    \n    return context_vector, attention_weights\n\n\nclass Decoder():\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n\n    # used for attention\n    self.attention = BahdanauAttention(self.dec_units)\n\n  def __call__(self, x, hidden, enc_output):\n    # enc_output shape == (batch_size, max_length, hidden_size)\n    context_vector, attention_weights = self.attention(hidden, enc_output)\n    \n    print(context_vector.shape, 'c_v', attention_weights.shape, &quot;attention_w&quot;)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    print(x.shape, 'xshape', context_vector.shape, 'context')\n    expanded_dims = tf.expand_dims(context_vector, 1)\n    x = tf.concat([expanded_dims, x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # output shape == (batch_size * 1, hidden_size)\n    output = tf.reshape(output, (-1, output.shape[2]))\n\n    # output shape == (batch_size, vocab)\n    x = self.fc(output)\n\n    return x, state, attention_weights\n\n\n\n\n</code></pre>\n",
    "score": 5,
    "creation_date": 1593371686,
    "view_count": 10426,
    "answer_count": 1,
    "tags": "tensorflow;nlp;tensorflow2.0;tensorflow-lite"
  },
  {
    "question_id": 61446106,
    "title": "What is the difference between parsing and Part Of Speech Tagging?",
    "body": "<p>I know that POS tagging labels each and every word in a sentence with its appropriate Part Of Speech , But isn't that what a Parser does too ? i.e, break a sentence into its component parts? \nI've looked this up on the internet but couldn't find any satisfactory explanation . \nPlease clear my doubt.\nThanks in advance </p>\n",
    "score": 5,
    "creation_date": 1587926303,
    "view_count": 1551,
    "answer_count": 1,
    "tags": "parsing;nlp;stanford-nlp;part-of-speech"
  },
  {
    "question_id": 58832191,
    "title": "Understanding shapes of Keras layers",
    "body": "<p>I am going through this link to understand  Multi-channel CNN Model for Text Classification.</p>\n\n<p>The code is based on <a href=\"https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/\" rel=\"nofollow noreferrer\">this tutorial.</a></p>\n\n<p>I have understood most of the things, however I can't understand how Keras defines the output shapes of certain layers.</p>\n\n<p>Here is the code:</p>\n\n<p>define a model with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review text.</p>\n\n<pre><code>#Skipped keras imports\n\n# load a clean dataset\ndef load_dataset(filename):\n    return load(open(filename, 'rb'))\n\n# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n# calculate the maximum document length\ndef max_length(lines):\n    return max([len(s.split()) for s in lines])\n\n# encode a list of lines\ndef encode_text(tokenizer, lines, length):\n    # integer encode\n    encoded = tokenizer.texts_to_sequences(lines)\n    # pad encoded sequences\n    padded = pad_sequences(encoded, maxlen=length, padding='post')\n    return padded\n\n# define the model\ndef define_model(length, vocab_size):\n    # channel 1\n    inputs1 = Input(shape=(length,))\n    embedding1 = Embedding(vocab_size, 100)(inputs1)\n    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n    drop1 = Dropout(0.5)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    inputs2 = Input(shape=(length,))\n    embedding2 = Embedding(vocab_size, 100)(inputs2)\n    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n    drop2 = Dropout(0.5)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    inputs3 = Input(shape=(length,))\n    embedding3 = Embedding(vocab_size, 100)(inputs3)\n    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n    drop3 = Dropout(0.5)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    dense1 = Dense(10, activation='relu')(merged)\n    outputs = Dense(1, activation='sigmoid')(dense1)\n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n    # compile\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize\n    print(model.summary())\n    plot_model(model, show_shapes=True, to_file='multichannel.png')\n    return model\n\n# load training dataset\ntrainLines, trainLabels = load_dataset('train.pkl')\n# create tokenizer\ntokenizer = create_tokenizer(trainLines)\n# calculate max document length\nlength = max_length(trainLines)\n# calculate vocabulary size\nvocab_size = len(tokenizer.word_index) + 1\nprint('Max document length: %d' % length)\nprint('Vocabulary size: %d' % vocab_size)\n# encode data\ntrainX = encode_text(tokenizer, trainLines, length)\nprint(trainX.shape)\n\n# define model\nmodel = define_model(length, vocab_size)\n# fit model\nmodel.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\n# save the model\nmodel.save('model.h5')\n</code></pre>\n\n<p>Running the code:</p>\n\n<p>Running the example first prints a summary of the prepared training dataset.\nMax document length: 1380\nVocabulary size: 44277\n(1800, 1380)</p>\n\n<pre><code>____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to\n====================================================================================================\ninput_1 (InputLayer)             (None, 1380)          0\n____________________________________________________________________________________________________\ninput_2 (InputLayer)             (None, 1380)          0\n____________________________________________________________________________________________________\ninput_3 (InputLayer)             (None, 1380)          0\n____________________________________________________________________________________________________\nembedding_1 (Embedding)          (None, 1380, 100)     4427700     input_1[0][0]\n____________________________________________________________________________________________________\nembedding_2 (Embedding)          (None, 1380, 100)     4427700     input_2[0][0]\n____________________________________________________________________________________________________\nembedding_3 (Embedding)          (None, 1380, 100)     4427700     input_3[0][0]\n____________________________________________________________________________________________________\nconv1d_1 (Conv1D)                (None, 1377, 32)      12832       embedding_1[0][0]\n____________________________________________________________________________________________________\nconv1d_2 (Conv1D)                (None, 1375, 32)      19232       embedding_2[0][0]\n____________________________________________________________________________________________________\nconv1d_3 (Conv1D)                (None, 1373, 32)      25632       embedding_3[0][0]\n____________________________________________________________________________________________________\ndropout_1 (Dropout)              (None, 1377, 32)      0           conv1d_1[0][0]\n____________________________________________________________________________________________________\ndropout_2 (Dropout)              (None, 1375, 32)      0           conv1d_2[0][0]\n____________________________________________________________________________________________________\ndropout_3 (Dropout)              (None, 1373, 32)      0           conv1d_3[0][0]\n____________________________________________________________________________________________________\nmax_pooling1d_1 (MaxPooling1D)   (None, 688, 32)       0           dropout_1[0][0]\n____________________________________________________________________________________________________\nmax_pooling1d_2 (MaxPooling1D)   (None, 687, 32)       0           dropout_2[0][0]\n____________________________________________________________________________________________________\nmax_pooling1d_3 (MaxPooling1D)   (None, 686, 32)       0           dropout_3[0][0]\n____________________________________________________________________________________________________\nflatten_1 (Flatten)              (None, 22016)         0           max_pooling1d_1[0][0]\n____________________________________________________________________________________________________\nflatten_2 (Flatten)              (None, 21984)         0           max_pooling1d_2[0][0]\n____________________________________________________________________________________________________\nflatten_3 (Flatten)              (None, 21952)         0           max_pooling1d_3[0][0]\n____________________________________________________________________________________________________\nconcatenate_1 (Concatenate)      (None, 65952)         0           flatten_1[0][0]\n                                                                   flatten_2[0][0]\n                                                                   flatten_3[0][0]\n____________________________________________________________________________________________________\ndense_1 (Dense)                  (None, 10)            659530      concatenate_1[0][0]\n____________________________________________________________________________________________________\ndense_2 (Dense)                  (None, 1)             11          dense_1[0][0]\n====================================================================================================\nTotal params: 14,000,337\nTrainable params: 14,000,337\nNon-trainable params: 0\n____________________________________________________________________________________________________\n</code></pre>\n\n<p>And</p>\n\n<pre><code>Epoch 6/10\n1800/1800 [==============================] - 30s - loss: 9.9093e-04 - acc: 1.0000\nEpoch 7/10\n1800/1800 [==============================] - 29s - loss: 5.1899e-04 - acc: 1.0000\nEpoch 8/10\n1800/1800 [==============================] - 28s - loss: 3.7958e-04 - acc: 1.0000\nEpoch 9/10\n1800/1800 [==============================] - 29s - loss: 3.0534e-04 - acc: 1.0000\nEpoch 10/10\n1800/1800 [==============================] - 29s - loss: 2.6234e-04 - acc: 1.0000\n</code></pre>\n\n<p>My interpretation of the Layer and output shape are as follows:\nPlease help me understand if its correct as I am lost in multi-dimension.</p>\n\n<p><strong>input_1 (InputLayer)  (None, 1380)</strong> : ---> <code>1380</code> is the total number of features ( that is 1380 input neurons) per data point.  <code>1800</code> is the total number of documents or data points.</p>\n\n<p><strong>embedding_1 (Embedding) (None, 1380, 100)  4427700</strong> ----> Embedding layer is : 1380 as features(words) and each feature is a vector of dimension 100.</p>\n\n<p>How the number of parameters here is <code>4427700</code>??</p>\n\n<p><strong>conv1d_1 (Conv1D)  (None, 1377, 32)  12832</strong> ------> Conv1d is of <code>kernel size=4</code>. Is it <code>1*4</code>  filter which is used <code>32</code> times. Then how the dimension became  <code>(None, 1377, 32)</code> with <code>12832</code> parameters?</p>\n\n<p><strong>max_pooling1d_1 (MaxPooling1D)  (None, 688, 32)</strong> with MaxPooling1D(pool_size=2) how the dimension became <code>(None, 688, 32)</code>? \n<strong>flatten_1 (Flatten)  (None, 22016)</strong> This is just multiplication of 688, 32?</p>\n\n<p>** Does every epoch trains 1800 data points at once?**</p>\n\n<p>Please let me know how output dimensions is calculated. Any reference or help would be appreciated.</p>\n",
    "score": 5,
    "creation_date": 1573630040,
    "view_count": 640,
    "answer_count": 1,
    "tags": "python;machine-learning;deep-learning;nlp;conv-neural-network"
  },
  {
    "question_id": 56896753,
    "title": "Is there a way to get entire constituents using SpaCy?",
    "body": "<p>I guess I'm trying to navigate SpaCy's parse tree in a more blunt way than is provided.</p>\n\n<p>For instance, if I have sentences like: \"He was a genius\" or \"The dog was green,\" I want to be able to save the objects to variables (\"a genius\" and \"green\"). </p>\n\n<p>token.children provides the IMMEDIATE syntactic dependents, so, for the first example, the children of \"was\" are \"he\" and \"genius,\" and then \"a\" is a child of \"genius.\" This isn't so helpful if I just want the entire constituent \"a genius.\" I'm not sure how to reconstruct it from the token.children or if there's a better way.</p>\n\n<p>I can figure out how to match \"is\" and \"was\" using token.text (part of what I'm trying to do), but I can't figure out how to return the whole constituent \"a genius\" using the info provided about children.</p>\n\n<pre><code>import spacy\nnlp = spacy.load('en_core_web_sm')\n\nsent = nlp(\"He was a genius.\")\n\nfor token in sent:\n     print(token.text, token.tag_, token.dep_, [child for child in token.children])\n</code></pre>\n\n<h1>This is the output:</h1>\n\n<p>He PRP nsubj []</p>\n\n<p>was VBD ROOT [He, genius, .]</p>\n\n<p>a DT det []</p>\n\n<p>genius NN attr [a]</p>\n\n<p>. . punct []</p>\n",
    "score": 5,
    "creation_date": 1562302546,
    "view_count": 4663,
    "answer_count": 2,
    "tags": "python;nlp;tokenize;spacy"
  },
  {
    "question_id": 56602442,
    "title": "Pytorch: How to implement nested transformers: a character-level transformer for words and a word-level transformer for sentences?",
    "body": "<p>I have a model in mind, but I'm having a hard time figuring out how to actually implement it in Pytorch, especially when it comes to training the model (e.g. how to define mini-batches, etc.). First of all let me quickly introduce the context:</p>\n\n<p>I'm working on VQA (visual question answering), in which the task is to answer questions about images, for example:</p>\n\n<p><a href=\"https://i.sstatic.net/UW8hE.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/UW8hE.png\" alt=\"enter image description here\"></a></p>\n\n<p>So, letting aside many details, I just want to focus here on the NLP aspect/branch of the model. In order to process the natural language question, I want to  use <em>character-level</em> embeddings (instead of traditional <em>word-level</em> embeddings) because they are more robust in the sense that they can easily accommodate for morphological variations in words (e.g. prefixes, suffixes, plurals, verb conjugations, hyphens, etc.). But at the same time I don't want to lose the inductive bias of reasoning at the word level. Therefore, I came up with the following design:</p>\n\n<p><a href=\"https://i.sstatic.net/Tl7bu.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Tl7bu.png\" alt=\"enter image description here\"></a></p>\n\n<p>As you can see in the picture above, I want to use <a href=\"http://papers.nips.cc/paper/7181-attention-is-all-you-need\" rel=\"noreferrer\">transformers</a> (or even better, <a href=\"https://arxiv.org/abs/1807.03819\" rel=\"noreferrer\">universal transformers</a>), but with a little twist. I want to use 2 transformers: the first one will process each word characters in isolation (<em>character-level</em> transformer) to produce an initial word-level embedding for each word in the question. Once we have all these initial word-level embeddings, a second <em>word-level</em> transformer will refine these embeddings to enrich their representation with context, thus obtaining <em>context-aware word-level</em> embeddings.</p>\n\n<p>The full model for the whole VQA task obviously is more complex, but I just want to focus here on this NLP part. So my question is basically about which Pytorch functions should I pay attention to when implementing this. For example, since I'll be using <em>character-level</em> embeddings I have to define a <em>character-level</em> embedding matrix, but then I have to perform lookups on this matrix to generate the inputs for the <em>character-level</em> transformer, repeat this for each word in the question and then feed all these vectors into the <em>word-level</em> transformer. Moreover, words in a single question can have different lengths, and questions within a single mini-batch can have different lengths too. So in my code I have to somehow account for different lengths at the word and the question level simultaneously in a single mini-batch (during training), and I've got no idea how to do that in Pytorch or whether it's even possible at all.</p>\n\n<p>Any tips on how to go about implementing this in Pytorch that could lead me in the right direction will be deeply appreciated.</p>\n",
    "score": 5,
    "creation_date": 1560531469,
    "view_count": 1494,
    "answer_count": 1,
    "tags": "nlp;pytorch"
  },
  {
    "question_id": 56082191,
    "title": "Losses in NER training loop not decreasing in spacy",
    "body": "<p>I am trying to train a new entity type 'HE INST'--to recognize colleges.\nThat is the only new label. I have a long document as raw text. I ran NER on it and saved the entities to the TRAIN DATA and then added the new entity labels to the TRAIN_DATA( i replaced in places where there was overlap).</p>\n\n<p>The training loop is constant at a loss value(~4000 for all the 15 texts) and (~300) for a single data. Why does this happen, how do I train the model properly. I have around 18 texts with 40 annotated new entities.Even after all iterations, the model still doesn't predict the output correctly.</p>\n\n<p>I haven't changed the script much. Just added en_core_web_lg, the new label and my TRAIN_DATA</p>\n\n<p>I am trying to tag institutes from resume(C.V) data:</p>\n\n<p>This would be one of my text in TRAIN_DATA: (soory for the long text)\nI have around 18 such texts concantenated to form TRAIN_DATA</p>\n\n<pre><code>[(\"To perform better in my work each day. To increase my knowledge. To bring out my best by hardworking and improving my skills. To serve my parents and my family. To contribute my skills to my country. Marital ; Single Status Nationality \\xe2\\x80\\x94: Indian Known . Parr . English, Malayalam, Hindi, Tamil Languages Hobby Playing cricket and football, Listening to music, Movies, Games. Father's ; V.N. Balappan Nair Name Mother's ; Saraswathy B Nair Name Believers Church Caarmel Engineering College R-Perunad Btech Electronics and communication engineering 6.09(Upto S6) 2015 - 2019 Marthoma Senior Secondary School Kozhencherry All India Senior School Certificate Examination 75% 2014 - 2015 Marthoma Senior Secondary School Kozhencherry Secondary School Examination 8.2 2012 - 2013 s@ INTERESTS Electronics, Sports s@ PERSONAL STRENGTHS Hardworking Loyal Good Team Spirit Good in mathematics ees IAA eM LANL NUL e (2 Problem Solving Skills rg DUS \\\\ TRAININGS completed the Vocational Industrial Training on Long Distance Communication Systems conducted by Southern Telecom Region, Bharat Sanchar Nigam Limited. Completed the internship training in Power Electronics Group(PEG), Tool Room, Fabrication Shop, Transform Winding, Electro Plating, Security And Surveillance Group(SSG), Special Products Group(SPG), Search And Rescue Beacon(SRB), Intelligent Tracking and Communication Project and Technology Development Center of Keltron Equipment Complex, Thiruvananthapuram. PROJECTS Final Year Project: Life Detection Using Quadcopter This project is useful at the time of natural calamities like flood earthquake etc... And can also be used in military applications as this device detects life signals using a PIR sensor and a thermal sensor. The components used in this are: PIR sensor, Thermal sensor, Arduino Nano, BEC, ESC, Quadcopter. Design project: Wireless Power Bank Wireless Power Bank enables us to charge our phone wordlessly. It can charge a device which is kept 10m(maximum) away from the adaptor without any obstacles in between. It uses the IR technology for power transmission. ACHIEVEMENTS &amp; AWARDS Participated in Pecardio Debugging Conducted as a part of NAKSHATRA 2019, The Annual National Level Techno Cultural Fest held at Saingits College of Engineering, kottayam. Volunteered in Alexa One day workshop on Artificial intelligence. Completed a period of two year tenue with a total of 240 hours in the National Service Scheme activities and has attended NSS Annual Special Camp. Participant in Cricket and football at the Annual Sports Meets. DECLARATION do here by confirm that the information given in this form is true to the best of my knowledge and belief.\", {'entities': [(29, 37, 'DATE'), (210, 223, 'ORG'), (241, 247, 'NORP'), (256, 260, 'PERSON'), (263, 270, 'LANGUAGE'), (272, 281, 'PERSON'), (283, 288, 'PERSON'), (290, 295, 'NORP'), (362, 375, 'EVENT'), (388, 401, 'PERSON'), (402, 420, 'PERSON'), (423, 445, 'PERSON'), (446, 490, 'HE INST'), (563, 574, 'DATE'), (575, 620, 'ORG'), (625, 668, 'ORG'), (669, 672, 'PERCENT'), (673, 684, 'DATE'), (685, 717, 'ORG'), (764, 775, 'DATE'), (779, 800, 'ORG'), (890, 893, 'ORG'), (909, 910, 'CARDINAL'), (963, 997, 'ORG'), (1001, 1036, 'ORG'), (1050, 1073, 'ORG'), (1075, 1103, 'ORG'), (1142, 1169, 'ORG'), (1172, 1181, 'ORG'), (1183, 1199, 'ORG'), (1201, 1218, 'ORG'), (1220, 1235, 'ORG'), (1275, 1301, 'ORG'), (1304, 1332, 'ORG'), (1335, 1355, 'ORG'), (1360, 1415, 'ORG'), (1419, 1444, 'ORG'), (1446, 1464, 'LOC'), (1475, 1494, 'EVENT'), (1797, 1809, 'GPE'), (1811, 1814, 'GPE'), (1816, 1819, 'ORG'), (1821, 1831, 'ORG'), (1849, 1888, 'ORG'), (1969, 1980, 'CARDINAL'), (2050, 2052, 'ORG'), (2088, 2122, 'ORG'), (2126, 2154, 'ORG'), (2168, 2182, 'EVENT'), (2188, 2194, 'DATE'), (2239, 2270, 'HE INST'), (2297, 2302, 'GPE'), (2303, 2310, 'DATE'), (2358, 2369, 'DATE'), (2370, 2378, 'DATE'), (2401, 2410, 'TIME'), (2414, 2441, 'ORG'), (2470, 2493, 'ORG'), (2534, 2557, 'EVENT')]})]\n</code></pre>\n\n<p>The script is given below: (Note:- eval function is used to parse the TRAIN_DATA to list after reading it as string from text file-----you most probably know that but just in case)</p>\n\n<pre><code>from __future__ import unicode_literals, print_function\n\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nimport en_core_web_lg\nfrom spacy.util import minibatch, compounding\n\n\n# new entity label\nLABEL = \"HE INST\"\n\nwith open('train_dump-backup.txt', 'r') as i_file:\n    t_data = i_file.read()\nTRAIN_DATA=eval(t_data)\n\n@plac.annotations(\n    model=(\"en_core_web_lg\", \"option\", \"m\", str),\n    new_model_name=(\"NLP_INST\", \"option\", \"nm\", str),\n    output_dir=(\"/home/drbinu/Downloads/NLP_INST\", \"option\", \"o\", Path),\n    n_iter=(\"30\", \"option\", \"n\", int),\n)\n\ndef main(model=None, new_model_name=\"animal\", output_dir=None, n_iter=30):\n    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n    random.seed(0)\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    # Add entity recognizer to model if it's not in the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner)\n    # otherwise, get it, so we can add labels to it\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    ner.add_label(LABEL)  # add new entity label to entity recognizer\n    # Adding extraneous labels shouldn't mess anything up\n    ner.add_label(\"VEGETABLE\")\n    if model is None:\n        optimizer = nlp.begin_training()\n    else:\n        optimizer = nlp.resume_training()\n    move_names = list(ner.move_names)\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        for itn in range(n_iter):\n            random.shuffle(TRAIN_DATA)\n            batches = minibatch(TRAIN_DATA, size=sizes)\n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n            print(\"Losses\", losses)\n\n    # test the trained model\n    test_text = \"B.Tech from Believers Church Caarmel Engineering College CGPA of 8.9\"\n    doc = nlp(test_text)\n    print(\"Entities in '%s'\" % test_text)\n    for ent in doc.ents:\n        print(ent.label_, ent.text)\n\n    # save model to output directory\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        nlp.meta[\"name\"] = new_model_name  # rename model\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        # test the saved model\n        print(\"Loading from\", output_dir)\n        nlp2 = spacy.load(output_dir)\n        # Check the classes have loaded back consistently\n        assert nlp2.get_pipe(\"ner\").move_names == move_names\n        doc2 = nlp2(test_text)\n        for ent in doc2.ents:\n            print(ent.label_, ent.text)\n\n\nif __name__ == \"__main__\":\n    plac.call(main)\n</code></pre>\n",
    "score": 5,
    "creation_date": 1557508822,
    "view_count": 5109,
    "answer_count": 1,
    "tags": "python;deep-learning;nlp;spacy"
  },
  {
    "question_id": 54308997,
    "title": "Efficient Python for word pair co-occurrence counting?",
    "body": "<p>I would like an efficient Pythonic way to count neighbouring word pairs in text. Efficient because it needs to wok well with larger datasets.</p>\n\n<p>The way the count is done is important too.</p>\n\n<p>Consider this simplified example:</p>\n\n<pre><code>words_list = \"apple banana banana apple\".split()\n</code></pre>\n\n<p>I can create neighbouring pairs using:</p>\n\n<pre><code>word_pair_list = zip(words_list[:-1], words_list[1:])\n</code></pre>\n\n<p>I can then count them Pythonically using</p>\n\n<pre><code>word_pair_ctr = collections.Counter(word_pair_list)\n</code></pre>\n\n<p>This gives me</p>\n\n<pre><code>(('apple', 'banana'), 1)\n(('banana', 'banana'), 1)\n(('banana', 'apple'), 1)\n</code></pre>\n\n<p>Note that <code>'apple'</code> and <code>'apple'</code> are not a neighbouring pair.</p>\n\n<p>But I want the order of the pairs not to count. That means the <code>('apple', 'banana')</code> and <code>('banana', 'apple')</code> should be considered the same, and the counts should be</p>\n\n<pre><code>(('apple', 'banana'), 2)\n(('banana', 'banana'), 1)\n</code></pre>\n\n<p>I can't find a Pythonic way of doing this that doesn't need me to visit each item in the word list, which becomes inefficient for larger text.</p>\n\n<p>I'm happy to use the common scipy, numpy and pandas as libraries too.</p>\n",
    "score": 5,
    "creation_date": 1548162440,
    "view_count": 2482,
    "answer_count": 2,
    "tags": "python;pandas;nlp"
  },
  {
    "question_id": 53920770,
    "title": "How is the Tf-Idf value calculated with analyzer =&#39;char&#39;?",
    "body": "<p>I'm having a problem in understanding how we got the Tf-Idf in the following program:</p>\n\n<p>I have tried calculating the value of <code>a</code> in the document 2 (<code>'And_this_is_the_third_one.'</code>) using the concept given on the <a href=\"https://www.kaggle.com/divsinha/sentiment-analysis-countvectorizer-tf-idf\" rel=\"nofollow noreferrer\">site</a>, but my value of 'a' using the above concept is</p>\n\n<blockquote>\n  <p>1/26*log(4/1)</p>\n  \n  <p>((count of occurrence of 'a' character)/(no of characters in the given\n  document)*log( # Docs/ # Docs in which given character\n  occurred))</p>\n  \n  <p>= 0.023156</p>\n</blockquote>\n\n<p>But output is returned as 0.2203 as can be seen in the output.</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = ['This_is_the_first_document.', 'This_document_is_the_second_document.', 'And_this_is_the_third_one.', 'Is_this_the_first_document?', ]\nvectorizer = TfidfVectorizer(min_df=0.0, analyzer=\"char\")\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nm = X.todense()\nprint(m)\n</code></pre>\n\n<p>I expected the output to be 0.023156 using the concept explained above.</p>\n\n<p>The output is:</p>\n\n<pre><code>['.', '?', '_', 'a', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u']\n\n\n{'t': 15, 'h': 8, 'i': 9, 's': 14, '_': 2, 'e': 6, 'f': 7, 'r': 13, 'd': 5, 'o': 12, 'c': 4, 'u': 16, 'm': 10, 'n': 11, '.': 0, 'a': 3, '?': 1}\n\n\n[[0.14540332 0.         0.47550697 0.         0.14540332 0.11887674\n  0.23775349 0.17960203 0.23775349 0.35663023 0.14540332 0.11887674\n  0.11887674 0.14540332 0.35663023 0.47550697 0.14540332]\n\n\n [0.10814145 0.         0.44206359 0.         0.32442434 0.26523816\n  0.35365088 0.         0.17682544 0.17682544 0.21628289 0.26523816\n  0.26523816 0.         0.26523816 0.35365088 0.21628289]\n\n\n [0.14061506 0.         0.57481012 0.22030066 0.         0.22992405\n  0.22992405 0.         0.34488607 0.34488607 0.         0.22992405\n  0.11496202 0.14061506 0.22992405 0.34488607 0.        ]\n\n\n [0.         0.2243785  0.46836004 0.         0.14321789 0.11709001\n  0.23418002 0.17690259 0.23418002 0.35127003 0.14321789 0.11709001\n  0.11709001 0.14321789 0.35127003 0.46836004 0.14321789]]\n</code></pre>\n",
    "score": 5,
    "creation_date": 1545727786,
    "view_count": 5294,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp"
  },
  {
    "question_id": 51514208,
    "title": "Edit Vader_lexicon.txt in nltk for python to add words related to my domain",
    "body": "<p>I am using <code>vader</code> in <code>nltk</code> to find sentiments of each line in a file. I have 2 questions:</p>\n\n<ol>\n<li>I need to add words in <code>vader_lexicon.txt</code> however the syntax of which looks like :</li>\n</ol>\n\n<blockquote>\n  <p>assaults -2.5    0.92195 [-1, -3, -3, -3, -4, -3, -1, -2, -2, -3]</p>\n</blockquote>\n\n<p>What does <code>-2.5</code> and <code>0.92195 [-1, -3, -3, -3, -4, -3, -1, -2, -2, -3]</code> represent?</p>\n\n<p>How should i code it for a new word? Say i have to add something like <code>'100%'</code> , <code>'A1'</code>.</p>\n\n<ol start=\"2\">\n<li>I can also see positive and negative words txt in <code>nltk_data\\corpora\\opinion_lexicon</code> folder. How are these getting utilised? Can I add my words in these txt files too?</li>\n</ol>\n",
    "score": 5,
    "creation_date": 1532507173,
    "view_count": 3497,
    "answer_count": 1,
    "tags": "python;python-3.x;nlp;nltk;sentiment-analysis"
  },
  {
    "question_id": 49433627,
    "title": "Extending Lemma Lookup Table in Spacy",
    "body": "<p>I am currently processing texts with the NLP library Spacy. Spacy, however, does not lemmatize all words correctly, therefore I want to extend the lookup table. Currently I am merging Spacy's constant lookup table with my extension and subsequently overwrite Spacy's native lookup table. </p>\n\n<p>I have the feeling, however, that this approach may not be the best and most consistent one. </p>\n\n<p>Question: Is there another possibility to update the lookup table in Spacy, e.g. an update or extend function? I have read the Docs and could not find something like that. Or is this approach \"just fine\"?</p>\n\n<p>Working example of my current approach:</p>\n\n<pre><code>import spacy\nnlp = spacy.load('de')\nSpacy_lookup = spacy.lang.de.LOOKUP\nNew_lookup = {'AAA':'Anonyme Affen Allianz','BBB':'Berliner Bauern Bund','CCC':'Chaos Chaoten Club'}\nSpacy_lookup.update(New_lookup)\nspacy.lang.de.LOOKUP = Spacy_lookup\ntagged = nlp(\"Die AAA besiegt die BBB und den CCC unverdient.\")\n[ print(each.lemma_) for each in tagged]\n\nDie\nAnonyme Affen Allianz\nbesiegen\nder\nBerliner Bauern Bund\nund\nder\nChaos Chaoten Club\nunverdient\n.\n</code></pre>\n",
    "score": 5,
    "creation_date": 1521735616,
    "view_count": 2025,
    "answer_count": 1,
    "tags": "python;python-3.x;nlp;spacy"
  },
  {
    "question_id": 48259290,
    "title": "Character embeddings with Keras",
    "body": "<p>I am trying to implement the type of character level embeddings described in <a href=\"https://arxiv.org/pdf/1603.01360.pdf\" rel=\"noreferrer\">this paper</a> in Keras. The character embeddings are calculated using a bidirectional LSTM.</p>\n\n<p><a href=\"https://i.sstatic.net/PSvdx.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/PSvdx.png\" alt=\"enter image description here\"></a></p>\n\n<p>To recreate this, I've first created a matrix of containing, for each word, the indexes of the characters making up the word:</p>\n\n<pre><code>char2ind = {char: index for index, char in enumerate(chars)}\nmax_word_len = max([len(word) for sentence in sentences for word in sentence])\nX_char = []\nfor sentence in X:\n    for word in sentence:\n        word_chars = []\n        for character in word:\n            word_chars.append(char2ind[character])\n\n        X_char.append(word_chars)\nX_char = sequence.pad_sequences(X_char, maxlen = max_word_len)\n</code></pre>\n\n<p>I then define a BiLSTM model with an embedding layer for the word-character matrix. I assume the input_dimension will have to be equal to the number of characters. I want a size of 64 for my character embeddings, so I set the hidden size of the BiLSTM to 32:</p>\n\n<pre><code>char_lstm = Sequential()\nchar_lstm.add(Embedding(len(char2ind) + 1, 64))    \nchar_lstm.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))\n</code></pre>\n\n<p>And this is where I get confused. How can I retrieve the embeddings from the model? I'm guessing I would have to compile the model and fit it then retrieve the weights to get the embeddings, but what parameters should I use to fit it ?</p>\n\n<hr>\n\n<p>Additional details:</p>\n\n<p>This is for an NER task, so the dataset technically could be be anything in the word - label format, although I am specifically working with the WikiGold ConLL corpus available here: <a href=\"https://github.com/pritishuplavikar/Resume-NER/blob/master/wikigold.conll.txt\" rel=\"noreferrer\">https://github.com/pritishuplavikar/Resume-NER/blob/master/wikigold.conll.txt</a>\nThe expected output from the network are the labels (I-MISC, O, I-PER...)</p>\n\n<p>I expect the dataset to be large enough to be training character embeddings directly from it. All words are coded with the index of their constituting characters, alphabet size is roughly 200 characters. The words are padded / cut to 20 characters. There are around 30 000 different words in the dataset.</p>\n\n<p>I hope to be able learn embeddings for each characters based on the info from the different words. Then, as in the paper, I would concatenate the character embeddings with the word's glove embedding before feeding into a Bi-LSTM network with a final CRF layer.</p>\n\n<p>I would also like to be able to save the embeddings so I can reuse them for other similar NLP tasks.</p>\n",
    "score": 5,
    "creation_date": 1516005104,
    "view_count": 8304,
    "answer_count": 1,
    "tags": "python;nlp;keras;lstm;word-embedding"
  },
  {
    "question_id": 45915803,
    "title": "Identifying the subject of a sententce",
    "body": "<p>I have been exploring NLP techniques with the goal of identifying the subject of survey comments (which I then use in conjunction with sentiment analysis). I want to make high level statements such as \"10% of survey respondents made a positive comment (+ sentiment) about Account Managers\".</p>\n\n<p>My approach has used <a href=\"https://en.wikipedia.org/wiki/Named-entity_recognition\" rel=\"nofollow noreferrer\">Named Entity Recognition (NER)</a>. Now that I am working with real data, I am getting visibility of some of the complexities &amp; nuances associated with identifying the subject of a sentence.  Here are 5 examples of sentences where the subject is the Account Manager. I have put the named entity in bold for demonstration purposes. </p>\n\n<ol>\n<li>Our <strong>account manager</strong> is great, he always goes the extra mile!</li>\n<li><strong>Steve</strong> our <strong>account manager</strong> is great, he always goes the extra mile!</li>\n<li><strong>Steve</strong> our <strong>relationship manager</strong> is great, he always goes the extra \nmile!</li>\n<li><strong>Steven</strong> is great, he always goes the extra mile!</li>\n<li><strong>Steve Smith</strong> is great, he always goes the extra mile!</li>\n<li>Our <strong>business mgr</strong>. is great,he always goes the extra mile!</li>\n</ol>\n\n<p>I see three challenges that add complexity to my task</p>\n\n<ol>\n<li>Synonyms: Account manager vs relationship manager vs business mgr. This is somewhat domain specific and tends to vary with the survey target audience.</li>\n<li>Abbreviations:  Mgr. vs manager</li>\n<li>Ambiguity -  Whether “Steven” is “Steve Smith” &amp; therefore an\n“account manager”.</li>\n</ol>\n\n<p>Of these the synonym problem is the most frequent issue, followed by the ambiguity issues. Based on what I have seen, the abbreviation issue isn’t that frequent in my data. </p>\n\n<p>Are there any NLP techniques that can help deal with any of these issues to a relatively high degree of confidence?   </p>\n",
    "score": 5,
    "creation_date": 1503912447,
    "view_count": 1344,
    "answer_count": 3,
    "tags": "python;nlp;text-analysis"
  },
  {
    "question_id": 44895192,
    "title": "What algorithms can group characters into words?",
    "body": "<p>I have some text generated by some lousy OCR software.</p>\n\n<p>The output contains mixture of words and space-separated characters, which should have been grouped into words. For example,</p>\n\n<pre><code>Expr e s s i o n Syntax\nS u m m a r y o f T e r minology \n</code></pre>\n\n<p>should have been</p>\n\n<pre><code>Expression Syntax\nSummary of Terminology \n</code></pre>\n\n<p>What algorithms can group characters into words? </p>\n\n<p>If I program in Python, C#, Java, C or C++, what libraries provide the implementation of the algorithms?</p>\n\n<p>Thanks.</p>\n",
    "score": 5,
    "creation_date": 1499126577,
    "view_count": 332,
    "answer_count": 1,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 42982824,
    "title": "wordcloud for non-english corpus",
    "body": "<p><a href=\"https://i.sstatic.net/YdUCc.png\" rel=\"noreferrer\">wordcloud for non English text</a></p>\n\n<p>Dear friends\nI am facing problems in generating proper wordcloud for non english text. The cloud is generated but it gives un-satisfactroy results. It shows wordcloud with characters only while I require wordcloud with proper words.\nI processed following code to generate wordcloud.</p>\n\n<pre><code>from os import path\nfrom scipy.misc import imread\nimport matplotlib.pyplot as plt\nimport random\nimport unicodedata\nfrom wordcloud import WordCloud, STOPWORDS\ntext = scorpus\nwordcloud = WordCloud(font_path='MBKhursheed.ttf',\n                      relative_scaling = 1.0,\n                      stopwords = sw\n                      ).generate(text)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()\n</code></pre>\n",
    "score": 5,
    "creation_date": 1490289563,
    "view_count": 2224,
    "answer_count": 1,
    "tags": "python-3.x;utf-8;nlp;jupyter-notebook;word-cloud"
  },
  {
    "question_id": 42606961,
    "title": "Can I use next layer&#39;s output as current layer&#39;s input by Keras?",
    "body": "<p>In text generate mission, we usually use model's last output as current input to generate next word. More generalized, I want to achieve a neural network  that regards next layer's finally hidden state as current layer's input. Just like the following(what confuses me is the decoder part):</p>\n\n<p><a href=\"https://i.sstatic.net/ED0aA.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/ED0aA.png\" alt=\"encoder-decoder\"></a></p>\n\n<p>But I have read Keras document and haven't found any functions to achieve it.</p>\n\n<p>Can I achieve this structure by Keras? How?</p>\n",
    "score": 5,
    "creation_date": 1488708231,
    "view_count": 1190,
    "answer_count": 2,
    "tags": "neural-network;nlp;keras;lstm"
  },
  {
    "question_id": 42517201,
    "title": "Get gender from noun using NLTK with German corpora",
    "body": "<p>I'm experimenting with NTLK. My question is if the library can detect the gender of a noun in German. I want to receive this information in order to determine if a text is written gender neutral. See here for more information:\n<a href=\"https://en.wikipedia.org/wiki/Gender_neutrality_in_languages_with_grammatical_gender\" rel=\"nofollow noreferrer\">https://en.wikipedia.org/wiki/Gender_neutrality_in_languages_with_grammatical_gender</a></p>\n\n<p>The underlying code categorizes my sentence, but I can't see any information about the gender of <strong>\"Mitarbeiter\"</strong>. My code so far:</p>\n\n<pre><code>sentence = \"\"\"Der Mitarbeiter geht.\"\"\"\ntokens = nltk.word_tokenize(sentence)\ntagged = nltk.pos_tag(tokens)\n&gt;&gt;&gt; tagged[0:6]\n</code></pre>\n\n<p>I haven't found any tools or scripts which accomplish this so far. Maybe there's also a better solution for my task.</p>\n",
    "score": 5,
    "creation_date": 1488310132,
    "view_count": 2162,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 42334335,
    "title": "How to structure an LSTM neural network for classification",
    "body": "<p>I have data that has various conversations between two people. Each sentence has some type of classification. I am attempting to use an NLP net to classify each sentence of the conversation. I tried a convolution net and get decent results (not ground breaking tho). I figured that since this a back and forth conversation, and LSTM net may produce better results, because what was previously said may have a large impact on what follows.</p>\n\n<p><a href=\"https://i.sstatic.net/lpl08.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/lpl08.png\" alt=\"Type of RNN nets\"></a></p>\n\n<p>If I follow the structure above, I would assume that I am doing a many-to-many. My data looks like.</p>\n\n<pre><code>X_train = [[sentence 1],  \n           [sentence 2],\n           [sentence 3]]\nY_train = [[0],\n           [1],\n           [0]]\n</code></pre>\n\n<p>Data has been processed using word2vec. I then design my network as follows..</p>\n\n<pre><code>model = Sequential()      \nmodel.add(Embedding(len(vocabulary),embedding_dim,\n          input_length=X_train.shape[1]))\nmodel.add(LSTM(88))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',\n              metrics['accuracy'])\nmodel.fit(X_train,Y_train,verbose=2,nb_epoch=3,batch_size=15)\n</code></pre>\n\n<p>I assume that this setup will feed one batch of sentences in at a time. However, if in model.fit, shuffle is not equal to false its receiving shuffled batches, so why is an LSTM net even useful in this case? From research on the subject, to achieve a many-to-many structure one would need to change the LSTM layer too</p>\n\n<pre><code>model.add(LSTM(88,return_sequence=True))\n</code></pre>\n\n<p>and the output layer would need to be...</p>\n\n<pre><code>model.add(TimeDistributed(Dense(1,activation='sigmoid')))\n</code></pre>\n\n<p>When switching to this structure I get an error on the input size. I'm unsure of how to reformat the data to meet this requirement, and also how to edit the embedding layer to receive the new data format.</p>\n\n<p>Any input would be greatly appreciated. Or if you have any suggestions on a better method, I am more than happy to hear them!</p>\n",
    "score": 5,
    "creation_date": 1487547937,
    "view_count": 2332,
    "answer_count": 1,
    "tags": "python;neural-network;nlp;keras;lstm"
  },
  {
    "question_id": 42331225,
    "title": "fuzzy .substring text-matching function",
    "body": "<p>I am looking for a way to fuzzy <code>substring</code> function. What do I mean by that: </p>\n\n<ul>\n<li>Two strings are given. </li>\n<li>One is often longer than the other one. Let's call then \"short\" and \"long\" </li>\n<li>We want to score how much of the \"short\" has appeared in the \"long\". </li>\n<li>We want to take into account proximity and oder. Like if the elements of the \"short\" appear in the \"long\", they are preferred to appear in the same order and close to each other. </li>\n</ul>\n\n<p><strong>Example 1:</strong></p>\n\n<ul>\n<li>Short: \"weeds are destroyed\"</li>\n<li>Long: \"Crops engineered with a bacterial gene making the plants resistant to herbicides can grow while weeds are destroyed, and genetically engineered crops that can resist destructive insects reduce the need for chemical insecticides.\"</li>\n</ul>\n\n<p>This is an exact match and should have score 1.0. </p>\n\n<p><strong>Example 2:</strong></p>\n\n<ul>\n<li>Short: \"weeds will be destroyed\"</li>\n<li>Long: Same as above. </li>\n</ul>\n\n<p>This is a fuzzy match, as \"weed\" and \"destroyed\" appear in the text, but without \"will be\". Still it should get a high-score (say 0.8). </p>\n\n<p><strong>Example 3:</strong></p>\n\n<p>If we set the \"Short\" to \"destroyed will be weeds\", although \"destroyed\" and \"weeds\" both appear in the original text, the score should be very low, since their order has changed. </p>\n\n<p>Any suggested implementation on this? </p>\n\n<p>A final point is that, there is no unique way of doing this scoring. But I am looking for AN algorithm. The parameters of this algorithm can be tuned based on the needs and requirements. </p>\n",
    "score": 5,
    "creation_date": 1487529427,
    "view_count": 364,
    "answer_count": 2,
    "tags": "algorithm;nlp;substring"
  },
  {
    "question_id": 40143405,
    "title": "how to fine-tune word2vec when training our CNN for text classification?",
    "body": "<p>I have 3 Questions about fine-tuning word vectors. Please, help me out. I will really appreciate it! Many thanks in advance!</p>\n\n<ol>\n<li><p>When I train my own CNN for text classification, I use Word2vec to initialize the words, then I just employ these pre-trained vectors as my input features to train CNN, so if I never had a embedding layer, it surely can not do any fine-tunes through back-propagation. my question is if I want to do fine-tuning, does it means to create a Embedding layer?and how to create it? </p></li>\n<li><p>When we train Word2vec, we use unsupervised training right? as in my case, I use the skip-gram model to get my pre-trained word2vec; But when I had the vec.bin and use it in the text classification model (CNN) as my words initialiser, if I could fine-tune the word-to-vector map in vec.bin, does it means that I have to have a CNN net structure exactly same as the one when training my Word2vec? and does the fine-tunes stuff would change the vec.bin or just fine-tune in computer memory?</p></li>\n<li><p>Are the skip-gram model and CBOW model are only used for unsupervised Word2vec training? Or they could also apply for other general text classification tasks? and what's the different of the network between Word2vec unsupervised training supervised fine-tuning? </p></li>\n</ol>\n\n<p>@Franck Dernoncourt thank you for reminding me. I'm green here, and hope to learn something from the powerful community. Please have a look at my questions when you have time, thank you again!</p>\n",
    "score": 5,
    "creation_date": 1476921716,
    "view_count": 6071,
    "answer_count": 2,
    "tags": "machine-learning;nlp;artificial-intelligence;deep-learning"
  },
  {
    "question_id": 38932299,
    "title": "Entities on my gazette are not recognized",
    "body": "<p>I would like to create a custom NER model. That's what i did:</p>\n\n<p><strong>TRAINING DATA</strong> (stanford-ner.tsv):</p>\n\n<pre><code>Hello    O\n!    O\nMy    O\nname    O\nis    O\nDamiano    PERSON\n.    O\n</code></pre>\n\n<p><strong>PROPERTIES</strong> (stanford-ner.prop):</p>\n\n<pre><code>trainFile = stanford-ner.tsv\nserializeTo = ner-model.ser.gz\nmap = word=0,answer=1\nmaxLeft=1\nuseClassFeature=true\nuseWord=true\nuseNGrams=true\nnoMidNGrams=true\nmaxNGramLeng=6\nusePrev=true\nuseNext=true\nuseDisjunctive=true\nuseSequences=true\nusePrevSequences=true\nuseTypeSeqs=true\nuseTypeSeqs2=true\nuseTypeySequences=true\nwordShape=chris2useLC\nuseGazettes=true\ngazette=gazzetta.txt\ncleanGazette=true\n</code></pre>\n\n<p><strong>GAZZETTE</strong> gazzetta.txt):</p>\n\n<pre><code>PERSON John\nPERSON Andrea\n</code></pre>\n\n<p>I build the model via command line with:</p>\n\n<pre><code>java -classpath \"stanford-ner.jar:lib/*\" edu.stanford.nlp.ie.crf.CRFClassifier  -prop stanford-ner.prop\n</code></pre>\n\n<p>And test with:</p>\n\n<pre><code>java -classpath \"stanford-ner.jar:lib/*\" edu.stanford.nlp.ie.crf.CRFClassifier  -loadClassifier ner-model.ser.gz -textFile test.txt\n</code></pre>\n\n<p>I did two tests with the following texts:</p>\n\n<p><strong>>>> TEST 1 &lt;&lt;&lt;</strong></p>\n\n<ul>\n<li><p>TEXT:\nHello! My name is Damiano and this is a fake text to test.</p></li>\n<li><p>OUTPUT\n<em>Hello/O !/O\nMy/O name/O is/O Damiano/PERSON and/O this/O is/O a/O fake/O text/O to/O test/O ./O</em></p></li>\n</ul>\n\n<p><strong>>>> TEST 2 &lt;&lt;&lt;</strong></p>\n\n<ul>\n<li><p>TEXT:\nHello! My name is John and this is a fake text to test.</p></li>\n<li><p>OUTPUT\n<em>Hello/O !/O\nMy/O name/O is/O John/O and/O this/O is/O a/O fake/O text/O to/O test/O ./O</em></p></li>\n</ul>\n\n<p>As you can see only \"Damiano\" entity is found. This entity is in my training data but \"John\" (second test) is inside the gazzette. So the question is.</p>\n\n<p>Why does John entity is not recognized ?</p>\n\n<p>Thank you so much in advance.</p>\n",
    "score": 5,
    "creation_date": 1471088219,
    "view_count": 920,
    "answer_count": 3,
    "tags": "machine-learning;nlp;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 38182860,
    "title": "what is the best way to remove non-ASCII characters from a text Corpus when using Quanteda in R?",
    "body": "<p>I am in dire need. I have a corpus that I have converted into a common language, but some of the words were not properly converted into English. Therefore, my corpus has non-ASCII characters such as <code>U+00F8</code>. </p>\n\n<p>I am using Quanteda and I have imported my text using this code: </p>\n\n<pre><code> EUCorpus &lt;- corpus(textfile(file=\"/Users/RiohBurke/Documents/RStudio/PROJECT/*.txt\"), encodingFrom = \"UTF-8-BOM\")\n</code></pre>\n\n<p>My corpus consists of 166 documents. Having imported the documents into R, what would be the best way to get rid of these non-ASCII characters?</p>\n",
    "score": 5,
    "creation_date": 1467629334,
    "view_count": 3941,
    "answer_count": 1,
    "tags": "r;nlp;tm;corpus;quanteda"
  },
  {
    "question_id": 37641584,
    "title": "How to get Sense Key in WordNet for NLTK Python?",
    "body": "<p>Hi Stackoverflow Community</p>\n\n<p>I just started tinkering around with Python NLTK and have directed my attention to the Wordnet module.</p>\n\n<p>I am attempting to get the Sense Ky for a given lemma and found the following:</p>\n\n<pre><code>s = wn.synset('skill.n.01')\ns.lemmas # &gt;&gt;&gt; [Lemma('skill.n.01.skill'), ... ]\ns.lemmas[0].key # &gt;&gt;&gt; 'skill%1:09:01::'\n</code></pre>\n\n<p>However, this implementation doesn't seem to be supported anymore. </p>\n\n<pre><code>Traceback (most recent call last):\nFile \"C:/Users/Admin/PycharmProjects/momely/placementarchitect/testbench.py\", line 59, in &lt;module&gt;\ns.lemmas[0].key\nTypeError: 'method' object is not subscriptable\n</code></pre>\n\n<p>I am wondering whether anyone would be able to point me in the right direction as to how I might be able to get the sense key given a lemma or synset?</p>\n\n<p>Any advice would be highly appreciated!</p>\n",
    "score": 5,
    "creation_date": 1465127688,
    "view_count": 4494,
    "answer_count": 3,
    "tags": "python;nlp;nltk;wordnet;open-multilingual-wordnet"
  },
  {
    "question_id": 36831354,
    "title": "Absolute position of leaves in NLTK tree",
    "body": "<p>I am trying to find the span (start index, end index) of a noun phrase in a given sentence. The following is the code for extracting noun phrases</p>\n\n<pre><code>sent=nltk.word_tokenize(a)\nsent_pos=nltk.pos_tag(sent)\ngrammar = r\"\"\"\n    NBAR:\n        {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}  # Nouns and Adjectives, terminated with Nouns\n\n    NP:\n        {&lt;NBAR&gt;}\n        {&lt;NBAR&gt;&lt;IN&gt;&lt;NBAR&gt;}  # Above, connected with in/of/etc...\n    VP:\n        {&lt;VBD&gt;&lt;PP&gt;?}\n        {&lt;VBZ&gt;&lt;PP&gt;?}\n        {&lt;VB&gt;&lt;PP&gt;?}\n        {&lt;VBN&gt;&lt;PP&gt;?}\n        {&lt;VBG&gt;&lt;PP&gt;?}\n        {&lt;VBP&gt;&lt;PP&gt;?}\n\"\"\"\n\ncp = nltk.RegexpParser(grammar)\nresult = cp.parse(sent_pos)\nnounPhrases = []\nfor subtree in result.subtrees(filter=lambda t: t.label() == 'NP'):\n  np = ''\n  for x in subtree.leaves():\n    np = np + ' ' + x[0]\n  nounPhrases.append(np.strip())\n</code></pre>\n\n<p>For <em>a = \"The American Civil War, also known as the War between the States or simply the Civil War, was a civil war fought from 1861 to 1865 in the United States after several Southern slave states declared their secession and formed the Confederate States of America.</em>\", the noun phrases extracted are</p>\n\n<p><em>['American Civil War', 'War', 'States', 'Civil War', 'civil war fought', 'United States', 'several Southern', 'states', 'secession', 'Confederate States', 'America'].</em></p>\n\n<p>Now I need to find the span (start position and end position of the phrase) of noun phrases. For example, the span of above noun phrases will be </p>\n\n<p><em>[(1,3), (9,9), (12, 12), (16, 17), (21, 23), ....]</em>.</p>\n\n<p>I'm fairly new to NLTK and I've looked into <a href=\"http://www.nltk.org/_modules/nltk/tree.html\" rel=\"noreferrer\">http://www.nltk.org/_modules/nltk/tree.html</a>. I tried to use <em>Tree.treepositions()</em> but I couldn't manage to extract absolute positions using these indices. Any help would be greatly appreciated. Thank You!</p>\n",
    "score": 5,
    "creation_date": 1461553191,
    "view_count": 1979,
    "answer_count": 3,
    "tags": "python;tree;nlp;nltk;chunking"
  },
  {
    "question_id": 34330922,
    "title": "What&#39;s the difference between indicative summarization and informative summarization?",
    "body": "<p>I have trouble in distinguishing between indicative summarization and informative summarization. Can you give me a clear example to show the difference between them?</p>\n\n<p>Thanks in advance!</p>\n",
    "score": 5,
    "creation_date": 1450344286,
    "view_count": 3147,
    "answer_count": 1,
    "tags": "nlp;text-mining;text-processing;summarization"
  },
  {
    "question_id": 33098511,
    "title": "How to retrieve all kinds of dates and temporal values from text",
    "body": "<p>I wanted to retrieve dates and other temporal entities from a set of Strings. Can this be done without parsing the string for dates in JAVA as most parsers deal with a limited scope of input patterns. But input is a manual entry which here and hence ambiguous. </p>\n\n<p>Inputs can be like:</p>\n\n<blockquote>\n  <p>12th Sep |mid-March |12.September.2013</p>\n  \n  <p>Sep 12th |12th September| 2013</p>\n  \n  <p>Sept 13 |12th, September |12th,Feb,2013</p>\n</blockquote>\n\n<p>I've gone through many answers on finding date in Java but most of them don't deal with such a huge scope of input patterns.</p>\n\n<p>I've tried using <code>SimpleDateFormat</code> class and using some parse() functions to check if parse function breaks which mean its not a date. I've tried using <code>regex</code> but I'm not sure if it falls fit in this scenario. I've also used <a href=\"https://github.com/clir/clearnlp/tree/master/src\" rel=\"nofollow noreferrer\">ClearNLP</a> to annotate the dates but it doesn't give a reliable annotation set.</p>\n\n<p>The closest approach to getting these values could be using a <code>Chain of responsibility</code> as mentioned below. Is there a library that has a set of patterns for date. I can use that maybe?</p>\n",
    "score": 5,
    "creation_date": 1444727236,
    "view_count": 722,
    "answer_count": 5,
    "tags": "java;date;nlp;gate;temporal"
  },
  {
    "question_id": 31316274,
    "title": "Implementing n-grams for next word prediction",
    "body": "<p>I'm trying to utilize a trigram for next word prediction.</p>\n\n<p>I have been able to upload a corpus and identify the most common trigrams by their frequencies. I used the \"ngrams\", \"RWeka\" and \"tm\" packages in R. I followed this question for guidance:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/8161167/what-algorithm-i-need-to-find-n-grams\">What algorithm I need to find n-grams?</a></p>\n\n<pre><code>text1&lt;-readLines(\"MyText.txt\", encoding = \"UTF-8\")\ncorpus &lt;- Corpus(VectorSource(text1))\n\nBigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 3, max =       3))\ntdm &lt;- TermDocumentMatrix(corpus, control = list(tokenize =      BigramTokenizer)) \n</code></pre>\n\n<p>If a user were to input a set a words, how would I go about generating the next word? For example, if a user types \"can of\", how would would I retrieve the three most likely words (e.g. beer, soda, paint, etc..)?</p>\n",
    "score": 5,
    "creation_date": 1436441712,
    "view_count": 7422,
    "answer_count": 2,
    "tags": "r;text;nlp;n-gram"
  },
  {
    "question_id": 30450705,
    "title": "Identify prepositons and individual POS",
    "body": "<p>I am trying to find correct parts of speech for each word in paragraph. I am using Stanford POS Tagger. However, I am stuck at a point.</p>\n\n<p>I want to identify prepositions from the paragraph.</p>\n\n<p>Penn Treebank Tagset says that: </p>\n\n<pre><code>IN  Preposition or subordinating conjunction\n</code></pre>\n\n<p>how, can I be sure if current word is be <strong>preposition</strong> or <strong>subordinating conjunction</strong>. How can I extract only prepositions from paragraph in this case?</p>\n",
    "score": 5,
    "creation_date": 1432620556,
    "view_count": 633,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 28756796,
    "title": "Detecting which alphabet characters belong to in Python",
    "body": "<p>Is there a library or other simple way to detect which alphabet characters belong to in Python? I know I can use unicode code ranges for this, but if there's already a built-in way or a library or some such that provides the mappings, I'd rather not reinvent the wheel.</p>\n\n<p>Note: I'm asking about <strong>alphabet</strong> not <strong>language</strong>. Both \"hello\" and \"hola\" would map to Latin alphabet, whereas \"Поиск\" would map to Cyrillic. </p>\n",
    "score": 5,
    "creation_date": 1425006114,
    "view_count": 1710,
    "answer_count": 3,
    "tags": "python;nlp"
  },
  {
    "question_id": 17709940,
    "title": "tf-idf using data on unigram frequency from Google",
    "body": "<p>I'm trying to identify important terms in a set of government documents. Generating the term frequencies is no problem.</p>\n\n<p>For document frequency, I was hoping to use the <a href=\"http://norvig.com/ngrams/\" rel=\"noreferrer\">handy Python scripts and accompanying data</a> that Peter Norvig posted for his chapter in \"Beautiful Data\", which include the frequencies of unigrams in a huge corpus of data from the Web.</p>\n\n<p>My understanding of tf-idf, however, is that \"document frequency\" refers to the number of documents containing a term, not the number of total words that <em>are</em> this term, which is what we get from the Norvig script. Can I still use this data for a crude tf-idf operation?</p>\n\n<p>Here's some sample data:</p>\n\n<pre><code>word    tf       global frequency\nchina   1684     0.000121447\nthe     352385   0.022573582\neconomy 6602     0.0000451130774123\nand     160794   0.012681757\niran    2779     0.0000231482902018\nromney  1159     0.000000678497795593 \n</code></pre>\n\n<p>Simply dividing tf by gf gives \"the\" a higher score than \"economy,\" which can't be right. Is there some basic math I'm missing, perhaps? </p>\n",
    "score": 5,
    "creation_date": 1374094176,
    "view_count": 1000,
    "answer_count": 1,
    "tags": "nlp;tf-idf"
  },
  {
    "question_id": 13958659,
    "title": "How to find &#39;similar&#39; records in a MySQL table based on &#39;title&#39; and &#39;description&#39; columns?",
    "body": "<p>I have a MySQL table storing some user generated content. For each piece of content, I have a title (VARCHAR 255) and a description (TEXT) column.</p>\n\n<p>When a user is viewing a record, I want to find other records that are 'similar' to it, based on the title/description being similar.</p>\n\n<p>What's the best way to go about doing this? I'm using PHP and MySQL.</p>\n\n<p>My initial ideas are:</p>\n\n<p>1) Either to strip out common words from the title and description to be left with 'unique' keywords, and then find other records which share those keywords.</p>\n\n<p>E.g in the sentence: \"Bob woke up at 5 am and went to school\", the keywords would be: \"Bob, woke, 5, went, school\". Then if there's another record whose title talks about 'bob' and 'school', they would be considered 'similar'.</p>\n\n<p>2) Or to use MySQL's full text search, though I don't know if this would be any good for something like this?</p>\n\n<p>Which method would be better out of the two, or is there another method which is even better?</p>\n",
    "score": 5,
    "creation_date": 1355941389,
    "view_count": 5175,
    "answer_count": 3,
    "tags": "php;mysql;nlp;artificial-intelligence;text-analysis"
  },
  {
    "question_id": 13666142,
    "title": "Entity Extraction Library",
    "body": "<p>I’m looking for a library that does text analysis and extract entities.</p>\n\n<p>The type/classification of an entity is not critical, it’s the identification of something that’s worthwhile that is critical. The entities universe in this case is infinite, it’s not bounded by fixed dictionary.</p>\n\n<p>It seems that there are a couple of web services that do that (NERD let you compare the results of these web services: <a href=\"http://nerd.eurecom.fr/documentation\" rel=\"nofollow noreferrer\">http://nerd.eurecom.fr/documentation</a> which is pretty useful), but I’m looking for a local library and not a remotely hosted service. I’d prefer Java or .NET but if it’s a good library I’ll learn whatever language that it’s written in. </p>\n\n<p>There are few older threads on similar topic and I was hoping to find new development in this area, and/or libraries built on top of lower level NLP libraries:</p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/7455188/entity-extraction-recognition-with-free-tools-while-feeding-lucene-index\">Entity Extraction/Recognition with free tools while feeding Lucene Index</a></li>\n<li><a href=\"https://stackoverflow.com/questions/4199382/lucene-entity-extraction\">Lucene Entity Extraction</a></li>\n<li><a href=\"https://stackoverflow.com/questions/4308132/how-do-i-do-entity-extraction-in-lucene\">How do I do Entity Extraction in Lucene</a></li>\n<li><a href=\"https://stackoverflow.com/questions/tagged/named-entity-extraction\">https://stackoverflow.com/questions/tagged/named-entity-extraction</a></li>\n<li><a href=\"https://stackoverflow.com/questions/tagged/named-entity-recognition\">https://stackoverflow.com/questions/tagged/named-entity-recognition</a></li>\n</ul>\n\n<p>Does anyone know about a good library that does a decent job? </p>\n",
    "score": 5,
    "creation_date": 1354418939,
    "view_count": 2307,
    "answer_count": 3,
    "tags": "nlp;semantics;named-entity-recognition;named-entity-extraction"
  },
  {
    "question_id": 9970466,
    "title": "Find Synonyms for multi-word phrases",
    "body": "<p><strong>Is it possible for the python library NLTK to suggest/create synonyms for groups of words?</strong></p>\n\n<p>For example; for the word/group \"main course\" can I use NLTK to get the synonyms \"main dish\", \"main meal\", \"dinner\" etc.?</p>\n\n<p>Heres my code that works for single word synonyms but not multiwords:</p>\n\n<pre><code>from nltk.corpus import wordnet as wn\nprint wn.synset(\"eat.v.01\").lemma_names # prints synonyms of eat\nprint wn.synset(\"main course.n.01\").lemma_names # throws WordNetError\n</code></pre>\n",
    "score": 5,
    "creation_date": 1333336309,
    "view_count": 4355,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 8874938,
    "title": "Techniques for calculating adjective frequency",
    "body": "<p>I need to calculate word frequencies of a given set of adjectives in a large set of customer support reviews. However I don't want to include those that are negated. </p>\n\n<p>For example suppose my list of adjectives was: [helpful, knowledgeable, friendly]. I want to make sure \"friendly\" isn't counted in a sentence such as \"The representative was not very friendly.\" </p>\n\n<p>Do I need to do a full NLP parse of the text or is there an easier approach? I don't need super high accuracy. </p>\n\n<p>I'm not at all familiar with NLP. I'm hoping for something that doesn't have such a steep learning curve and isn't so processor intensive.</p>\n\n<p>Thanks</p>\n",
    "score": 5,
    "creation_date": 1326677468,
    "view_count": 1203,
    "answer_count": 2,
    "tags": "full-text-search;nlp;data-mining"
  },
  {
    "question_id": 6686758,
    "title": "Package to generate n-gram language models with smoothing? (Alternatives to NLTK)",
    "body": "<p>I'd like to find some type of package or module (preferably Python or Perl, but others would do) that automatically generate n-gram probabilities from an input text, and can automatically apply one or more smoothing algorithms as well.</p>\n\n<p>That is, I am looking for something like the NLTK <code>NgramModel</code> class. I can't use this for my purposes because there are some bugs with the smoothing functions which make it choke when you ask for the probability of a word it hasn't seen before. </p>\n\n<p>I've read through the dev forums for NLTK and as of now there seems to be no progress on this.  </p>\n\n<p>Any alternatives out there?</p>\n",
    "score": 5,
    "creation_date": 1310598594,
    "view_count": 2678,
    "answer_count": 3,
    "tags": "nlp;nltk;n-gram"
  },
  {
    "question_id": 4456181,
    "title": "What are some examples of Machine Translation applications/libraries currently being developed?",
    "body": "<p>I'm interested in learning more about Machine Translation.  While I have some very interesting books on the matter, I'd like to see some real world applications of MT's theories.</p>\n\n<p>I've found a couple open source projects just by searching around:</p>\n\n<p><a href=\"http://www.apertium.org/\">Apertium</a></p>\n\n<p><a href=\"http://www.statmt.org/moses/\">Moses</a></p>\n\n<p>So, does anyone have any other examples?  I'm looking for active projects; stuff which has not been abandoned.</p>\n",
    "score": 5,
    "creation_date": 1292457069,
    "view_count": 520,
    "answer_count": 2,
    "tags": "open-source;nlp;machine-translation"
  },
  {
    "question_id": 2836959,
    "title": "Adjective Nominalization in Python NLTK",
    "body": "<p>Is there a way to obtain Wordnet adjective nominalizations using NLTK?\nFor example, for <code>happy</code> the desired output would be <code>happiness</code>.</p>\n<p>I tried to dig around, but couldn't find anything.</p>\n",
    "score": 5,
    "creation_date": 1273864777,
    "view_count": 1438,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 2672267,
    "title": "Format relative dates",
    "body": "<p>Is there a ruby gem that will format dates relative to the current time? I want output like \"Tomorrow at 5pm\", \"Thursday next week at 5:15pm\", I'm not too concerned about the exact output, just as long as it's relative dates in natural language</p>\n",
    "score": 5,
    "creation_date": 1271732602,
    "view_count": 3647,
    "answer_count": 2,
    "tags": "ruby;datetime;rubygems;nlp"
  },
  {
    "question_id": 2399355,
    "title": "I have a list of names, some of them are fake, I need to use NLP and Python 3.1 to keep the real names and throw out the fake names",
    "body": "<p>I have no clue of where to start on this. I've never done any NLP and only programmed in Python 3.1, which I have to use. I'm looking at the site <a href=\"http://www.linkedin.com\" rel=\"noreferrer\">http://www.linkedin.com</a> and I have to gather all of the public profiles and some of them have very fake names, like 'aaaaaa k dudujjek' and I've been told I can use NLP to find the real names, where would I even start?</p>\n",
    "score": 5,
    "creation_date": 1268023790,
    "view_count": 2174,
    "answer_count": 3,
    "tags": "python-3.x;nlp"
  },
  {
    "question_id": 78862426,
    "title": "Unable to use nltk functions",
    "body": "<p>I was trying to run some nltk functions on the UCI spam message dataset but ran into this problem of word_tokenize not working even after downloading dependencies.</p>\n<pre><code>import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\ndf['text'].apply(lambda x: len(nltk.word_tokenize(x)))\n</code></pre>\n<p>following is the error:</p>\n<pre><code>{\n    &quot;name&quot;: &quot;LookupError&quot;,\n    &quot;message&quot;: &quot;\n**********************************************************************\n  Resource punkt_tab not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  &gt;&gt;&gt; import nltk\n  &gt;&gt;&gt; nltk.download('punkt_tab')\n  \n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load tokenizers/punkt_tab/english/\n\n  Searched in:\n    - 'C:\\\\\\\\Users\\\\\\\\user/nltk_data'\n    - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\nltk_data'\n    - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\\share\\\\\\\nltk_data'\n    - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\\lib\\\\\\\nltk_data'\n    - 'C:\\\\\\\\Users\\\\\\\\user\\\\\\AppData\\\\\\\\Roaming\\\\\\\nltk_data'\n    - 'C:\\\\\\\nltk_data'\n    - 'D:\\\\\\\nltk_data'\n    - 'E:\\\\\\\nltk_data'\n**********************************************************************\n&quot;,\n    &quot;stack&quot;: &quot;---------------------------------------------------------------------------\nLookupError                               Traceback (most recent call last)\nCell In[1024], line 3\n      1 #finding no. of words\n----&gt; 3 df['text'].apply(lambda x: len(nltk.word_tokenize(x)))\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\series.py:4915, in Series.apply(self, func, convert_dtype, args, by_row, **kwargs)\n   4780 def apply(\n   4781     self,\n   4782     func: AggFuncType,\n   (...)\n   4787     **kwargs,\n   4788 ) -&gt; DataFrame | Series:\n   4789     \\&quot;\\&quot;\\&quot;\n   4790     Invoke function on values of Series.\n   4791 \n   (...)\n   4906     dtype: float64\n   4907     \\&quot;\\&quot;\\&quot;\n   4908     return SeriesApply(\n   4909         self,\n   4910         func,\n   4911         convert_dtype=convert_dtype,\n   4912         by_row=by_row,\n   4913         args=args,\n   4914         kwargs=kwargs,\n-&gt; 4915     ).apply()\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\apply.py:1427, in SeriesApply.apply(self)\n   1424     return self.apply_compat()\n   1426 # self.func is Callable\n-&gt; 1427 return self.apply_standard()\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\apply.py:1507, in SeriesApply.apply_standard(self)\n   1501 # row-wise access\n   1502 # apply doesn't have a `na_action` keyword and for backward compat reasons\n   1503 # we need to give `na_action=\\&quot;ignore\\&quot;` for categorical data.\n   1504 # TODO: remove the `na_action=\\&quot;ignore\\&quot;` when that default has been changed in\n   1505 #  Categorical (GH51645).\n   1506 action = \\&quot;ignore\\&quot; if isinstance(obj.dtype, CategoricalDtype) else None\n-&gt; 1507 mapped = obj._map_values(\n   1508     mapper=curried, na_action=action, convert=self.convert_dtype\n   1509 )\n   1511 if len(mapped) and isinstance(mapped[0], ABCSeries):\n   1512     # GH#43986 Need to do list(mapped) in order to get treated as nested\n   1513     #  See also GH#25959 regarding EA support\n   1514     return obj._constructor_expanddim(list(mapped), index=obj.index)\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\base.py:921, in IndexOpsMixin._map_values(self, mapper, na_action, convert)\n    918 if isinstance(arr, ExtensionArray):\n    919     return arr.map(mapper, na_action=na_action)\n--&gt; 921 return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\algorithms.py:1743, in map_array(arr, mapper, na_action, convert)\n   1741 values = arr.astype(object, copy=False)\n   1742 if na_action is None:\n-&gt; 1743     return lib.map_infer(values, mapper, convert=convert)\n   1744 else:\n   1745     return lib.map_infer_mask(\n   1746         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n   1747     )\n\nFile lib.pyx:2972, in pandas._libs.lib.map_infer()\n\nCell In[1024], line 3, in &lt;lambda&gt;(x)\n      1 #finding no. of words\n----&gt; 3 df['text'].apply(lambda x: len(nltk.word_tokenize(x)))\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\nltk\\\\tokenize\\\\__init__.py:129, in word_tokenize(text, language, preserve_line)\n    114 def word_tokenize(text, language=\\&quot;english\\&quot;, preserve_line=False):\n    115     \\&quot;\\&quot;\\&quot;\n    116     Return a tokenized copy of *text*,\n    117     using NLTK's recommended word tokenizer\n   (...)\n    127     :type preserve_line: bool\n    128     \\&quot;\\&quot;\\&quot;\n--&gt; 129     sentences = [text] if preserve_line else sent_tokenize(text, language)\n    130     return [\n    131         token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n    132     ]\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\nltk\\\\tokenize\\\\__init__.py:106, in sent_tokenize(text, language)\n     96 def sent_tokenize(text, language=\\&quot;english\\&quot;):\n     97     \\&quot;\\&quot;\\&quot;\n     98     Return a sentence-tokenized copy of *text*,\n     99     using NLTK's recommended sentence tokenizer\n   (...)\n    104     :param language: the model name in the Punkt corpus\n    105     \\&quot;\\&quot;\\&quot;\n--&gt; 106     tokenizer = PunktTokenizer(language)\n    107     return tokenizer.tokenize(text)\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\nltk\\\\tokenize\\\\punkt.py:1744, in PunktTokenizer.__init__(self, lang)\n   1742 def __init__(self, lang=\\&quot;english\\&quot;):\n   1743     PunktSentenceTokenizer.__init__(self)\n-&gt; 1744     self.load_lang(lang)\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\nltk\\\\tokenize\\\\punkt.py:1749, in PunktTokenizer.load_lang(self, lang)\n   1746 def load_lang(self, lang=\\&quot;english\\&quot;):\n   1747     from nltk.data import find\n-&gt; 1749     lang_dir = find(f\\&quot;tokenizers/punkt_tab/{lang}/\\&quot;)\n   1750     self._params = load_punkt_params(lang_dir)\n   1751     self._lang = lang\n\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\nltk\\\\data.py:582, in find(resource_name, paths)\n    580 sep = \\&quot;*\\&quot; * 70\n    581 resource_not_found = f\\&quot;\\\n{sep}\\\n{msg}\\\n{sep}\\\n\\&quot;\n--&gt; 582 raise LookupError(resource_not_found)\n\nLookupError: \n**********************************************************************\n  Resource punkt_tab not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  &gt;&gt;&gt; import nltk\n  &gt;&gt;&gt; nltk.download('punkt_tab')\n  \n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load tokenizers/punkt_tab/english/\n\n  Searched in:\n    - 'C:\\\\\\\\Users\\\\\\\\user/nltk_data'\n    - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\nltk_data'\n    - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\\share\\\\\\\nltk_data'\n    - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\\lib\\\\\\\nltk_data'\n    - 'C:\\\\\\\\Users\\\\\\\\user\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\nltk_data'\n    - 'C:\\\\\\\nltk_data'\n    - 'D:\\\\\\\nltk_data'\n    - 'E:\\\\\\\nltk_data'\n**********************************************************************\n&quot;\n}\n</code></pre>\n<p>I tried re installing nltk and try and download a few other dependency files but nothing works. What am I doing wrong?</p>\n",
    "score": 5,
    "creation_date": 1723475849,
    "view_count": 9985,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;nltk"
  },
  {
    "question_id": 77386212,
    "title": "How to make named entity recognition provide better categorization of data",
    "body": "<p>Following is a default categorization of data from a news article.</p>\n<pre><code>Christiane Amanpour 268 287 PERSON\nHamas 155 160 ORG\nRania 6 11 PERSON\nWarner 0 6 ORG\n</code></pre>\n<p>But I would like to change the behavior as follows</p>\n<pre><code>I would want to categorize `Christiane Amanpour` as a journalist\nI would want to categorize `Rania` as a queen\nI would want to categorize `Warner` as a cricket player\n</code></pre>\n<p>How exactly I train the data to do this</p>\n",
    "score": 5,
    "creation_date": 1698640132,
    "view_count": 357,
    "answer_count": 1,
    "tags": "nlp;spacy;large-language-model"
  },
  {
    "question_id": 76195038,
    "title": "NLTagger tags every word as OtherWord and name scheme as Other",
    "body": "<p>I tried Apple's <a href=\"https://developer.apple.com/documentation/naturallanguage/identifying_people_places_and_organizations\" rel=\"noreferrer\">own example</a>:</p>\n<pre><code>import NaturalLanguage\n\nlet text = &quot;The American Red Cross was established in Washington, D.C., by Clara Barton.&quot;\n\nlet tagger = NLTagger(tagSchemes: [.nameType])\ntagger.string = text\n\nlet options: NLTagger.Options = [.omitPunctuation, .omitWhitespace, .joinNames]\nlet tags: [NLTag] = [.personalName, .placeName, .organizationName]\n\ntagger.enumerateTags(in: text.startIndex..&lt;text.endIndex, unit: .word, scheme: .nameType, options: options) { tag, tokenRange in \n    // Get the most likely tag, and print it if it's a named entity.\n    if let tag = tag, tags.contains(tag) {\n        print(&quot;\\(text[tokenRange]): \\(tag.rawValue)&quot;)\n    }\n        \n    // Get multiple possible tags with their associated confidence scores.\n    let (hypotheses, _) = tagger.tagHypotheses(at: tokenRange.lowerBound, unit: .word, scheme: .nameType, maximumCount: 1)\n    print(hypotheses)\n        \n   return true\n}\n</code></pre>\n<p>But it returns all name tags as <code>Other</code>. I also tried another example of tagging the sentence with lexical class, and it also tags every word as <code>OtherWord</code>:</p>\n<pre><code>var text = &quot;The American Red Cross was established in Washington, D.C., by Clara Barton.&quot;\n\nlet tagger = NLTagger(tagSchemes: [.lexicalClass])\ntagger.string = text\n\nlet options: NLTagger.Options = [.omitPunctuation, .omitWhitespace, .joinNames]\n\nprint(&quot;language&quot;, tagger.dominantLanguage)\n\ntagger.enumerateTags(in: text.startIndex..&lt;text.endIndex, unit: .word, scheme: .lexicalClass, options: options) { tag, tokenRange in\n    // Get the most likely tag, and print it if it's a named entity.\n    if let tag = tag {\n        print(&quot;\\(text[tokenRange]): \\(tag.rawValue)&quot;)\n    }\n\n   return true\n}\n</code></pre>\n<p>I tried the answer for <a href=\"https://stackoverflow.com/questions/29311279/linguistic-tagger-incorrectly-tagging-as-otherword\">this question</a> by setting language orthography but it didn't help:</p>\n<pre><code>//tagger.setOrthography(NSOrthography(dominantScript: &quot;Latn&quot;, languageMap: [&quot;Latn&quot;: [&quot;en&quot;]]), range: text.startIndex..&lt;text.endIndex)\ntagger.setOrthography(NSOrthography.defaultOrthography(forLanguage: &quot;en-US&quot;), range: text.startIndex..&lt;text.endIndex)\n</code></pre>\n<p>Anybody has a clue why is it like this?</p>\n<p>By the way, my Xcode version is the latest one as of today, 14.3.</p>\n",
    "score": 5,
    "creation_date": 1683476555,
    "view_count": 638,
    "answer_count": 1,
    "tags": "ios;swift;nlp"
  },
  {
    "question_id": 75655918,
    "title": "Sentence-Transformer Training and Validation Loss",
    "body": "<p>I am using the Sentence-Transformers model to Fine Tune(using PyTorch) it on a custom dataset which is the same as the Semantic Text Similarity (STS) Dataset.</p>\n<p>I am unable to get(or print) the training or validation error during training. I am trying to find how to monitor these errors during or after training, explored different documentation, and tried solutions but still unable to monitor those errors.</p>\n<p>Below is the training part of the code. How can one know the training and validation error during the training for SBERT?</p>\n<pre><code>train_loss = losses.MultipleNegativesRankingLoss(model)\nevaluator = EmbeddingSimilarityEvaluator.from_input_examples(val_set, name='sts-dev')\nnum_epochs = 20\nwarmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n\nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          evaluator=evaluator,\n          epochs=num_epochs,\n          evaluation_steps=1000,\n          warmup_steps=warmup_steps,\n          show_progress_bar=True)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/F7X2X.png\" rel=\"noreferrer\">You can see the progress bar of training for reference</a></p>\n",
    "score": 5,
    "creation_date": 1678137311,
    "view_count": 4090,
    "answer_count": 2,
    "tags": "deep-learning;pytorch;nlp;bert-language-model;sentence-transformers"
  },
  {
    "question_id": 74785255,
    "title": "Cast topic modeling outcome to dataframe",
    "body": "<p>I have used <code>BertTopic</code> with <code>KeyBERT</code> to extract some <code>topics</code> from some <code>docs</code></p>\n<pre><code>from bertopic import BERTopic\ntopic_model = BERTopic(nr_topics=&quot;auto&quot;, verbose=True, n_gram_range=(1, 4), calculate_probabilities=True, embedding_model='paraphrase-MiniLM-L3-v2', min_topic_size= 3)\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre>\n<p>Now I can access the <code>topic name</code></p>\n<pre><code>freq = topic_model.get_topic_info()\nprint(&quot;Number of topics: {}&quot;.format( len(freq)))\nfreq.head(30)\n\n   Topic    Count   Name\n0   -1       1     -1_default_greenbone_gmp_manager\n1    0      14      0_http_tls_ssl tls_ssl\n2    1      8       1_jboss_console_web_application\n</code></pre>\n<p>and inspect the topics</p>\n<pre><code>[('http', 0.0855701486234524),          \n ('tls', 0.061977919455444744),\n ('ssl tls', 0.061977919455444744),\n ('ssl', 0.061977919455444744),\n ('tcp', 0.04551718585531556),\n ('number', 0.04551718585531556)]\n\n[('jboss', 0.14014705432060262),\n ('console', 0.09285308122803233),\n ('web', 0.07323749337563096),\n ('application', 0.0622930523123512),\n ('management', 0.0622930523123512),\n ('apache', 0.05032395169459188)]\n</code></pre>\n<p>What I want is to have a final data<code>frame</code> that has in one <code>column</code> the <code>topic name</code> and in another <code>column</code> the elements of the <code>topic</code></p>\n<pre><code>expected outcome:\n\n  class                         entities\no http_tls_ssl tls_ssl           HTTP...etc\n1 jboss_console_web_application  JBoss, console, etc\n</code></pre>\n<p>and one dataframe with the topic name on different columns</p>\n<pre><code>  http_tls_ssl tls_ssl           jboss_console_web_application\no http                           JBoss\n1 tls                            console\n2 etc                            etc\n</code></pre>\n<p>I did not find out how to do this. Is there a way?</p>\n",
    "score": 5,
    "creation_date": 1670936291,
    "view_count": 709,
    "answer_count": 1,
    "tags": "python-3.x;pandas;nlp;bert-language-model;topic-modeling"
  },
  {
    "question_id": 72503309,
    "title": "Save a Bert model with custom forward function and heads on Hugginface",
    "body": "<p>I have created my own BertClassifier model, starting from a pretrained and then added my own classification heads composed by different layers. After the fine-tuning, I want to save the model using model.save_pretrained() but when I print it upload it from pretrained i don't see my classifier head.\nThe code is the following. How can I save the all structure on my model and make it full accessible with <code> AutoModel.from_preatrained('folder_path')</code> ?\n. Thanks!</p>\n<pre><code>class BertClassifier(PreTrainedModel):\n    &quot;&quot;&quot;Bert Model for Classification Tasks.&quot;&quot;&quot;\n    config_class = AutoConfig\n    def __init__(self,config, freeze_bert=True): #tuning only the head\n        &quot;&quot;&quot;\n         @param    bert: a BertModel object\n         @param    classifier: a torch.nn.Module classifier\n         @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        &quot;&quot;&quot;\n        #super(BertClassifier, self).__init__()\n        super().__init__(config)\n\n        # Instantiate BERT model\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        self.D_in = 1024 #hidden size of Bert\n        self.H = 512\n        self.D_out = 2\n \n        # Instantiate the classifier head with some one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(self.D_in, 512),\n            nn.Tanh(),\n            nn.Linear(512, self.D_out),\n            nn.Tanh()\n        )\n \n\n\n    def forward(self, input_ids, attention_mask):\n\n\n         # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                             attention_mask=attention_mask)\n         \n         # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n \n         # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n \n        return logits\n\n</code></pre>\n<pre><code>configuration=AutoConfig.from_pretrained('Rostlab/prot_bert_bfd')\nmodel = BertClassifier(config=configuration,freeze_bert=False)\n</code></pre>\n<p>Saving the model after fine-tuning</p>\n<pre><code>model.save_pretrained('path')\n</code></pre>\n<p>Loading the fine-tuned model</p>\n<pre><code>model = AutoModel.from_pretrained('path') \n</code></pre>\n<p>Printing the model after loading shows I have as the last layer the following and missing my 2 linear layer:</p>\n<pre><code> (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (adapters): ModuleDict()\n          (adapter_fusion_layer): ModuleDict()\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n  (prefix_tuning): PrefixTuningPool(\n    (prefix_tunings): ModuleDict()\n  )\n)\n</code></pre>\n",
    "score": 5,
    "creation_date": 1654378702,
    "view_count": 3696,
    "answer_count": 1,
    "tags": "python;nlp;pytorch;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 72454434,
    "title": "Getting similarity score with spacy and a transformer model",
    "body": "<p>I've been using the spacy <code>en_core_web_lg</code> and wanted to try out <code>en_core_web_trf</code> (transformer model) but having some trouble wrapping my head around the difference in the model/pipeline usage.</p>\n<p>My use case looks like the following:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy import displacy\nnlp = spacy.load(&quot;en_core_web_trf&quot;)\n\ns1 = nlp(&quot;Running for president is probably hard.&quot;)\ns2 = nlp(&quot;Space aliens lurk in the night time.&quot;)\ns1.similarity(s2)\n</code></pre>\n<p>Output:</p>\n<pre><code>The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements.\n(0.0, Space aliens lurk in the night time.)\n</code></pre>\n<p>Looking at <a href=\"https://github.com/explosion/spaCy/discussions/7643\" rel=\"noreferrer\">this post</a>, the transformer model does not have a word vector in the same way <code>en_core_web_lg</code> does, but you can get the embedding via <code>s1._.trf_data.tensors</code>. Which looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>sent1._.trf_data.tensors[0].shape\n(1, 9, 768)\nsent1._.trf_data.tensors[1].shape\n(1, 768)\n</code></pre>\n<p>So I tried to manually take the cosine similarity (<a href=\"https://github.com/explosion/spaCy/discussions/6511?sort=new#discussioncomment-971055\" rel=\"noreferrer\">using this post as ref</a>):</p>\n<pre class=\"lang-py prettyprint-override\"><code>def similarity(obj1, obj2):\n        (v1, t1), (v2, t2) = obj1._.trf_data.tensors, obj2._.trf_data.tensors\n        try:\n            return ((1 - cosine(v1, v2)) + (1 - cosine(t1, t2))) / 2\n        except:\n            return 0.0\n</code></pre>\n<p>But this does not work.</p>\n",
    "score": 5,
    "creation_date": 1654033743,
    "view_count": 2322,
    "answer_count": 1,
    "tags": "nlp;cosine-similarity;spacy-3;spacy-transformers"
  },
  {
    "question_id": 72397740,
    "title": "Issues with spacy model en_core_web_lg : how to prevent the package from downloading every time the code is run",
    "body": "<p>I am using spacy and its model en_core_web_lg, to perform summarisation in python. The code is running perfectly and there is no error at all. Except that, I am trying to find a way of making sure that the en_core_web_lg doesn't keep downloading in an environment if it already has it. I have googled a lot to find a perfect solution for this which I will list below but none has gelled with what I am trying to achieve.\nThis code will be packaged and will be used by multiple people and I want to make sure that if they run the code everytime, the en_core_web_lg doesn't download if it already exists. Below is the spacy excerpt of my code and the solutions I tried:</p>\n<pre><code>#Importing necessary Libraries\nfrom heapq import nlargest\nfrom string import punctuation\nimport nltk\nimport spacy\nfrom spacy.cli.download import download\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nnltk.download('punkt')\ndownload(model=&quot;en_core_web_lg&quot;)\nnlp_g = spacy.load('en_core_web_lg') #downloads everytime the code is run even if the model is present in the environment\n\ndef spacy_summarize(text):\n    &quot;&quot;&quot;\n    Returns the summary for an input string text\n\n            Parameters:\n                :param text: Input String\n                :type text: str\n\n            Returns:\n                :return: The summary for the input text\n                :rtype: String\n\n    &quot;&quot;&quot;\n    nlp = nlp_g\n    doc= nlp(text)\n    word_frequencies={}\n    for word in doc:\n        if word.text.lower() not in [list(STOP_WORDS), punctuation]:\n            if word.text not in word_frequencies:\n                word_frequencies[word.text] = 1\n            else:\n                word_frequencies[word.text] += 1\n    max_frequency=max(word_frequencies.values())\n    for word in word_frequencies:\n        word_frequencies.copy()[word]=word_frequencies[word]/max_frequency\n    sentence_tokens= [sent for sent in doc.sents]\n    sentence_scores = {}\n    spacy_frequencies(word_frequencies, sentence_tokens, sentence_scores)\n    select_length=max(1,int(len(sentence_tokens)*0.05))\n    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n    final_summary=[word.text for word in summary]\n    summary=''.join(final_summary)\n    return summary\n\ndef spacy_frequencies(word_frequencies, sentence_tokens, sentence_scores):\n    &quot;&quot;&quot;\n    Child function for spacy function for calculating sentence scores\n            Parameters:\n                    :param: word frequeny, sentence token and score which\n                        is provided through the parent function\n\n    &quot;&quot;&quot;\n    for sent in sentence_tokens:\n        for word in sent:\n            if word.text.lower() in word_frequencies:\n                if sent not in sentence_scores:\n                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n                else:\n                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n\n\n\n</code></pre>\n<p>Things Tried:</p>\n<pre><code>import sys\nimport subprocess\nimport pkg_resources\n\nrequired = {'en_core_web_lg'}\ninstalled = {pkg.key for pkg in pkg_resources.working_set}\nmissing = required - installed\n\nif missing:\n  python = sys.executable\n  subprocess.check_call([python, '-m', 'spacy', 'download', *missing], stdout=subprocess.DEVNULL)\n\n</code></pre>\n<pre><code>try:\n  nlp_lg = spacy.load(&quot;en_core_web_lg&quot;)\nexcept ModuleNotFoundError:\n  download(model=&quot;en_core_web_lg&quot;)\n  nlp_lg = spacy.load(&quot;en_core_web_lg&quot;)\n\n\n</code></pre>\n<p>Both solutions didn't give a satisfactory result and the package was downloaded again and I would appreciate if someone could help me with this?\nThank you so much!</p>\n",
    "score": 5,
    "creation_date": 1653597221,
    "view_count": 1576,
    "answer_count": 1,
    "tags": "python;module;nlp;operating-system;spacy"
  },
  {
    "question_id": 70979844,
    "title": "Using weights with transformers huggingface",
    "body": "<p>I came across this <a href=\"https://github.com/jlealtru/website_tutorials/blob/main/notebooks/Longformer%20with%20IMDB.ipynb\" rel=\"nofollow noreferrer\">tutorial</a> which performs Text classification with the Longformer. I came across this two links - <a href=\"https://discuss.huggingface.co/t/how-can-i-use-class-weights-when-training/1067\" rel=\"nofollow noreferrer\">one</a> and <a href=\"https://huggingface.co/docs/transformers/master/main_classes/trainer\" rel=\"nofollow noreferrer\">two</a> which talk about using class weights when the data is unbalanced.</p>\n<pre><code># instantiate the trainer class and check for available devices\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_data,\n    eval_dataset=test_data\n)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n</code></pre>\n<p>I am not sure how to modify the above piece of code to include class weights as shown below (code copied from the last link from above)</p>\n<pre><code>from torch import nn\nfrom transformers import Trainer\n\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(&quot;labels&quot;)\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(&quot;logits&quot;)\n        # compute custom loss (suppose one has 3 labels with different weights)\n        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n</code></pre>\n<p>Could someone clarify how to combine above two blocks?</p>\n<h1>update 1======================</h1>\n<p>as per the answer given below,\nif I modify code , then do I need to provide more arguments when I create <code>trainer </code>  object so that <code>compute_loss</code> method gets used? would that method be called automatically? for example for that function one of the inputs is <code>inputs</code> and we are not feeding it when we create the <code>trainer</code> object</p>\n<pre><code>class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(&quot;labels&quot;)\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(&quot;logits&quot;)\n        # compute custom loss (suppose one has 3 labels with different weights)\n        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n\n# instantiate the trainer class and check for available devices\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_data,\n    eval_dataset=test_data\n)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n</code></pre>\n",
    "score": 5,
    "creation_date": 1643931758,
    "view_count": 5327,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers"
  },
  {
    "question_id": 69374271,
    "title": "How to use a language model for prediction after fine-tuning?",
    "body": "<p>I've trained/fine-tuned a <a href=\"https://huggingface.co/BSC-TeMU/roberta-base-bne\" rel=\"nofollow noreferrer\">Spanish RoBERTa</a> model that has recently been pre-trained for a variety of NLP tasks except for text classification.</p>\n<p>Since the baseline model seems to be promising, I want to fine-tune it for a different task: text classification, more precisely, sentiment analysis of Spanish Tweets and use it to predict labels on scraped tweets I have.</p>\n<p>The preprocessing and the training seem to work correctly. However, I don't know how I can use this mode afterwards for prediction.</p>\n<p>I'll leave out the preprocessing part because I don't think there seems to be an issue.</p>\n<h3>Code:</h3>\n<pre><code># Training with native TensorFlow \nfrom transformers import TFAutoModelForSequenceClassification\n\n## Model Definition\nmodel = TFAutoModelForSequenceClassification.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;, from_pt=True, num_labels=3)\n\n## Model Compilation\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.metrics.SparseCategoricalAccuracy()\nmodel.compile(optimizer=optimizer, \n              loss=loss,\n              metrics=metric) \n\n## Fitting the data \nhistory = model.fit(train_dataset.shuffle(1000).batch(64), epochs=3, batch_size=64)\n</code></pre>\n<h3>Output:</h3>\n<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  &quot;Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 &quot;\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/5\n16/16 [==============================] - 35s 1s/step - loss: 1.0455 - sparse_categorical_accuracy: 0.4452\nEpoch 2/5\n16/16 [==============================] - 18s 1s/step - loss: 0.6923 - sparse_categorical_accuracy: 0.7206\nEpoch 3/5\n16/16 [==============================] - 18s 1s/step - loss: 0.3533 - sparse_categorical_accuracy: 0.8885\nEpoch 4/5\n16/16 [==============================] - 18s 1s/step - loss: 0.1871 - sparse_categorical_accuracy: 0.9477\nEpoch 5/5\n16/16 [==============================] - 18s 1s/step - loss: 0.1031 - sparse_categorical_accuracy: 0.9714\n</code></pre>\n<h3>Question:</h3>\n<p>How can I use the model after fine-tuning for text classification/sentiment analysis? (I want to create a predicted label for each tweet I scraped.)\n<br>What would be a good way of approaching this?</p>\n<p>I've tried to save the model, but I don't know where I can find it and use then:</p>\n<pre><code># Save the model\nmodel.save_pretrained('Twitter_Roberta_Model')\n</code></pre>\n<p>I've also tried to just add it to a HuggingFace pipeline like the following. But I'm not sure if this works correctly.</p>\n<pre><code>classifier = pipeline('sentiment-analysis', \nmodel=model, \ntokenizer=AutoTokenizer.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;))\n</code></pre>\n",
    "score": 5,
    "creation_date": 1632909864,
    "view_count": 5224,
    "answer_count": 1,
    "tags": "tensorflow;keras;nlp;huggingface-transformers;transfer-learning"
  },
  {
    "question_id": 68563581,
    "title": "Error while loading spacy [E002] Can&#39;t find factory for &#39;tok2vec&#39;",
    "body": "<p>I can't seem to find a solution for this problem anywhere:</p>\n<p>I'm trying to use spacy's medium english model: en_core_web_md</p>\n<p>As downloading spacy through my command line doesn't work at all, I downloaded the model to my directory and proceeded to call it out:</p>\n<pre><code>import spacy\nnlp = spacy.load('en_core_web_md-3.1.0')\n\n</code></pre>\n<p>However, I keep getting the following error:</p>\n<pre><code>KeyError: &quot;[E002] Can't find factory for 'tok2vec'. This usually happens when spaCy calls `nlp.create_pipe` with a component name that's not built in - for example, when constructing the pipeline from a model's meta.json. If you're using a custom component, you can write to `Language.factories['tok2vec']` or remove it from the model meta and add it \nvia `nlp.add_pipe` instead.&quot;\n</code></pre>\n<p>The small spacy model en_core_web_sm works perfectly fine but I need a larger model to perform the calculations I need.</p>\n<pre><code>pip 21.1.3 \nspaCy version    2.3.5       \nPlatform         Windows-10-10.0.19041-SP0\nPython version   3.9.5\n</code></pre>\n",
    "score": 5,
    "creation_date": 1627487416,
    "view_count": 10109,
    "answer_count": 2,
    "tags": "python;model;nlp;spacy"
  },
  {
    "question_id": 68062706,
    "title": "Update built-in NER model of Spacy instead of overwrite",
    "body": "<p>I am using an inbuilt model of Spacy that is <code>en_core_web_lg</code> and want to train it using my custom entities. While doing that, I am facing two issues,</p>\n<ol>\n<li><p>It overwrite the new trained data with the old one and results in not recognizing the other entities. for example,\nBefore training, it can recognize the PERSON and ORG but after training it doesn't recognize the PERSON and ORG.</p>\n</li>\n<li><p>During the training process, it is giving me the following error,</p>\n</li>\n</ol>\n<pre><code>UserWarning: [W030] Some entities could not be aligned in the text &quot;('I work in Google.',)&quot; with entities &quot;[(9, 15, 'ORG')]&quot;. Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n</code></pre>\n<p>Here is my whole code,</p>\n<pre><code>import spacy\nimport random\nfrom spacy.util import minibatch, compounding\nfrom pathlib import Path\nfrom spacy.training.example import Example\nsentence = &quot;&quot;\nbody1 = &quot;James work in Facebook and love to have tuna fishes in the breafast.&quot;\nnlp_lg = spacy.load(&quot;en_core_web_lg&quot;)\nprint(nlp_lg.pipe_names)\ndoc = nlp_lg(body1)\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n\n\ntrain = [\n    ('I had tuna fish in breakfast', {'entities': [(6,14,'FOOD')]}),\n    ('I love prawns the most', {'entities': [(6,12,'FOOD')]}),\n    ('fish is the rich source of protein', {'entities': [(0,4,'FOOD')]}),\n    ('I work in Google.', {'entities': [(9,15,'ORG')]})\n    ]\n\n\nner = nlp_lg.get_pipe(&quot;ner&quot;)\n\nfor _, annotations in train:\n    for ent in annotations.get(&quot;entities&quot;):\n        ner.add_label(ent[2])\n\ndisable_pipes = [pipe for pipe in nlp_lg.pipe_names if pipe != 'ner']\n\nwith nlp_lg.disable_pipes(*disable_pipes):\n    optimizer = nlp_lg.resume_training()\n    for interation in range(30):\n        random.shuffle(train)\n        losses = {}\n\n        batches = minibatch(train, size=compounding(1.0,4.0,1.001))\n        for batch in batches:\n            text, annotation = zip(*batch)\n            doc1 = nlp_lg.make_doc(str(text))\n            example = Example.from_dict(doc1, annotations)\n            nlp_lg.update(\n                [example],\n                drop = 0.5,\n                losses = losses,\n                sgd = optimizer\n                )\n            print(&quot;Losses&quot;,losses)\n\ndoc = nlp_lg(body1)\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n\n</code></pre>\n<p>Expected Output :</p>\n<pre><code>James 0 5 PERSON\nFacebook 14 22 ORG\ntuna fishes 40 51 FOOD\n\n</code></pre>\n<p>Currently recognizing no entities..</p>\n<p>Please let me know where I am doing it wrong. Thanks!</p>\n",
    "score": 5,
    "creation_date": 1624253233,
    "view_count": 1569,
    "answer_count": 2,
    "tags": "python;python-3.x;nlp;spacy;spacy-3"
  },
  {
    "question_id": 66743649,
    "title": "BERT model : &quot;enable_padding() got an unexpected keyword argument &#39;max_length&#39;&quot;",
    "body": "<p>I am trying to implement the BERT model architecture using Hugging Face and KERAS. I am learning this from the Kaggle (<a href=\"https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\" rel=\"noreferrer\">link</a>) and try to understand it. When I tokenized my data, I face some problems and get an error message. The error msg is:</p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-20-888a40c0160b&gt; in &lt;module&gt;\n----&gt; 1 x_train = fast_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n      2 x_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n      3 x_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=MAX_LEN )\n      4 y_train = train1.toxic.values\n      5 y_valid = valid.toxic.values\n\n&lt;ipython-input-8-de591bf0a0b9&gt; in fast_encode(texts, tokenizer, chunk_size, maxlen)\n      4     &quot;&quot;&quot;\n      5     tokenizer.enable_truncation(max_length=maxlen)\n----&gt; 6     tokenizer.enable_padding(max_length=maxlen)\n      7     all_ids = []\n      8 \n\nTypeError: enable_padding() got an unexpected keyword argument 'max_length'\n</code></pre>\n<p>and the code is:</p>\n<pre><code>x_train = fast_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=192)\nx_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=192)\nx_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=192 )\ny_train = train1.toxic.values\ny_valid = valid.toxic.values\n</code></pre>\n<p>and the function fast_encode is here:</p>\n<pre><code>def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    &quot;&quot;&quot;\n    Encoder for encoding the text into sequence of integers for BERT Input\n    &quot;&quot;&quot;\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)\n</code></pre>\n<p>What should I do now?</p>\n",
    "score": 5,
    "creation_date": 1616406476,
    "view_count": 4762,
    "answer_count": 2,
    "tags": "nlp;padding;tokenize;bert-language-model"
  },
  {
    "question_id": 60422206,
    "title": "Machine learning entity candidate scoring",
    "body": "<p>I am trying to understand the machine learning part behind Google's <a href=\"https://ai.googleblog.com/2018/08/the-machine-learning-behind-android.html\" rel=\"noreferrer\">Smart Linkify</a>. The article states the following regarding their <code>generate candidate entities</code> model.</p>\n\n<blockquote>\n  <p>A given input text is first split into words (based on space\n  separation), then all possible word subsequences of certain maximum\n  length (15 words in our case) are generated, and for each candidate\n  the scoring neural net assigns a value (between 0 and 1) based on\n  whether it represents a valid entity:</p>\n</blockquote>\n\n<p><a href=\"https://i.sstatic.net/OewSD.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/OewSD.png\" alt=\"enter image description here\"></a></p>\n\n<blockquote>\n  <p>Next, the generated entities that overlap are removed, favoring the\n  ones with the higher score over the conflicting ones with a lower\n  score.</p>\n</blockquote>\n\n<p>If I understand correctly the model tries every word in the sentence and a combination of that word up to 15 words total?</p>\n\n<p>How can you train such model? I assume it's supervised learning but don't understand how such data could be labeled. Is it similar to NER where the entity is specified by character position? And there are only 2 entities in the data <code>entity</code> and <code>non-entity</code>.</p>\n\n<p>And for the output of the model, the so called \"candidate score\", how can a a neural network return a single numerical value? (the score). Or is the output layer just a single node?</p>\n\n<p>A more detailed explanation on:</p>\n\n<ul>\n<li><code>Possible word subsequences of certain maximum length</code> means it considers every word with the 7 words before and 7 after the word?</li>\n<li>How can the neural net generate a score when its a binary classification <code>entity</code> and <code>non-entity</code>? Or do they mean the probability score for entity?</li>\n<li>How to train a binary NER? Like any other NER except replace all entities to type 'entity' and then generate negative samples for <code>non-entity?</code></li>\n<li>How can this model be fast, as they claim, when it processes every word in the text plus 7 words before and after said word?</li>\n</ul>\n\n<p>is what I'm looking for, to understand. </p>\n",
    "score": 5,
    "creation_date": 1582749685,
    "view_count": 273,
    "answer_count": 1,
    "tags": "python;tensorflow;machine-learning;neural-network;nlp"
  },
  {
    "question_id": 59245541,
    "title": "How to implement incremental learning in NLP",
    "body": "<p>We are building a system wherein, we would have a initial very small amount of trained data to start with.\nThe job is to Classify the incoming data(Document, for our case) into 2 categories: Category A &amp; B.\nData is document , so the user needs to classify the Document to belong to Category A or B.\nSo, with the limited amount of data, we create the trained data set and we  start predicting the Category of next Document using the trained data set. </p>\n\n<p>Now if the prediction is correct user moves to the next Document.\nBut if the prediction is incorrect , the user inputs the correct Category (Lets Say Category A was predicted by the system, wherein the correct assignment to the data should be Category B). So now the system should use this learning(Category B instead of Category A) to enrich(learn) itself in near real time.</p>\n\n<p>It should train only the added data and not the complete dataset , which is already trained. So it should be incremental learning.\nFor classification we would be applying Naive Bayes Classification.</p>\n\n<p>Now the question is :</p>\n\n<ul>\n<li>How do we implement the incremental training, without training the\nwhole data set every time? </li>\n<li>I know there are incremental learning libraries like Vowpal Wabbit &amp;\ncreme. Will it be a good solution using these library for my case?</li>\n</ul>\n",
    "score": 5,
    "creation_date": 1575882328,
    "view_count": 1142,
    "answer_count": 1,
    "tags": "machine-learning;scikit-learn;nlp;opennlp;creme"
  },
  {
    "question_id": 57479028,
    "title": "spaCy NLP custom rule matcher",
    "body": "<p>I am begginer with NLP. I am using spaCy python library for my NLP project. Here is my requirement,</p>\n\n<p>I have a JSON File with all country names. Now i need to parse and get goldmedal count for the each countries in the document. Given\nbelow the sample sentence,</p>\n\n<pre><code>\"Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics\"\n</code></pre>\n\n<p>I am able to fetch country names but not it medal count. Given below my code. Please help to proceed further.</p>\n\n<pre><code>import json\nfrom spacy.lang.en import English\nfrom spacy.matcher import PhraseMatcher\n\nwith open(\"C:\\Python36\\srclcl\\countries.json\") as f:\n    COUNTRIES = json.loads(f.read())\n\nnlp = English()\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\ndoc = nlp(\"Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics\")\nmatcher = PhraseMatcher(nlp.vocab)\npatterns = list(nlp.pipe(COUNTRIES))\n\nmatcher.add(\"COUNTRY\", None, *patterns)\n\n\nfor sent in doc.sents:\n    subdoc = nlp(sent.text)\n    matches = matcher(subdoc)\n    print (sent.text)\n    for match_id, start, end in matches:\n        print(subdoc[start:end].text)\n</code></pre>\n\n<p>Also, if the given text is like ,</p>\n\n<pre><code>\"Czech Republic won 5 gold medals at olympics in 1995. Slovakia won 0 medals olympics\"\n</code></pre>\n",
    "score": 5,
    "creation_date": 1565703626,
    "view_count": 726,
    "answer_count": 1,
    "tags": "python;nlp;nltk;spacy"
  },
  {
    "question_id": 55060888,
    "title": "Torchtext AttributeError: &#39;Example&#39; object has no attribute &#39;text_content&#39;",
    "body": "<p>I'm working with RNN and using Pytorch &amp; Torchtext. I've got a problem with building vocab in my RNN. My code is as follows:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>TEXT = Field(tokenize=tokenizer, lower=True)\nLABEL = LabelField(dtype=torch.float)\n\ntrainds = TabularDataset(\n    path='drive/{}'.format(TRAIN_PATH), format='tsv',\n    fields=[\n        ('label_start', LABEL),\n        ('label_end', None),\n        ('title', None),\n        ('symbol', None),\n        ('text_content', TEXT),\n    ])\n\ntestds = TabularDataset(\n    path='drive/{}'.format(TEST_PATH), format='tsv',\n    fields=[\n        ('text_content', TEXT),\n    ])\n\nTEXT.build_vocab(trainds, testds)\n</code></pre>\n\n<p>When I want to build vocab, I'm getting this annoying error:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>AttributeError: 'Example' object has no attribute 'text_content'\n</code></pre>\n\n<p>I'm sure, that there is no missing <code>text_content</code> attr. I made try-catch in order to display this specific case:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>try:\n    print(len(trainds[i]))\nexcept:\n    print(trainds[i].text_content)\n</code></pre>\n\n<p>Surprisingly, I don't get any error and this specific print command shows:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>['znana', 'okresie', 'masarni', 'walc', 'y', 'myśl', 'programie', 'sprawy', ...]\n</code></pre>\n\n<p>So it indicates, that <strong>there is</strong> <code>text_content</code> attr. When I perform this on a smaller dataset, it works like a charm. This problem occurs when I want to work with proper data. I ran out of ideas. Maybe someone had a similar case and can explain it.</p>\n\n<p>My full traceback:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-16-cf31866a07e7&gt; in &lt;module&gt;()\n    155 \n    156 if __name__ == \"__main__\":\n--&gt; 157     main()\n    158 \n\n&lt;ipython-input-16-cf31866a07e7&gt; in main()\n    117             break\n    118 \n--&gt; 119     TEXT.build_vocab(trainds, testds)\n    120     print('zbudowano dla text')\n    121     LABEL.build_vocab(trainds)\n\n/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py in build_vocab(self, *args, **kwargs)\n    260                 sources.append(arg)\n    261         for data in sources:\n--&gt; 262             for x in data:\n    263                 if not self.sequential:\n    264                     x = [x]\n\n/usr/local/lib/python3.6/dist-packages/torchtext/data/dataset.py in __getattr__(self, attr)\n    152         if attr in self.fields:\n    153             for x in self.examples:\n--&gt; 154                 yield getattr(x, attr)\n    155 \n    156     @classmethod\n\nAttributeError: 'Example' object has no attribute 'text_content'\n</code></pre>\n",
    "score": 5,
    "creation_date": 1552039608,
    "view_count": 9837,
    "answer_count": 2,
    "tags": "neural-network;nlp;pytorch;recurrent-neural-network;torchtext"
  },
  {
    "question_id": 54847574,
    "title": "Combining TF-IDF with pre-trained Word embeddings",
    "body": "<p>I have a list of website meta-description (128k descriptions; each with avg. 20-30 words), and am trying to build a similarity ranker (as in: show me the 5 most similar sites to this site meta description)</p>\n<p><strong>It worked AMAZINGLY well with TF-IDF uni- and bigram</strong>, and I thought that I could additionally improve it by adding pre-trained word embeddings (spacy &quot;en_core_web_lg&quot; to be exact). <strong>Plot twist: it does not work at all</strong>. Literally did not get one good guess, and its suddenly spits out completely random suggestions.</p>\n<p>Below is my code. Any thoughts on where I might have gone wrong? Am I overseeing something highly intuitive?</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport sys\nimport pickle\nimport spacy\nimport scipy.sparse\nfrom scipy.sparse import csr_matrix\nimport math\nfrom sklearn.metrics.pairwise import linear_kernel\nnlp=spacy.load('en_core_web_lg')\n\n\n&quot;&quot;&quot; Tokenizing&quot;&quot;&quot;\ndef _keep_token(t):\n    return (t.is_alpha and \n            not (t.is_space or t.is_punct or \n                 t.is_stop or t.like_num))\ndef _lemmatize_doc(doc):\n    return [ t.lemma_ for t in doc if _keep_token(t)]\n\ndef _preprocess(doc_list):     \n    return [_lemmatize_doc(nlp(doc)) for doc in doc_list]\ndef dummy_fun(doc):\n    return doc\n\n# Importing List of 128.000 Metadescriptions:\nWeb_data=open(&quot;./data/meta_descriptions&quot;,&quot;r&quot;, encoding=&quot;utf-8&quot;)\nAll_lines=Web_data.readlines()\n# outputs a list of meta-descriptions consisting of lists of preprocessed tokens:\ndata=_preprocess(All_lines) \n\n# TF-IDF Vectorizer:    \nvectorizer = TfidfVectorizer(min_df=10,tokenizer=dummy_fun,preprocessor=dummy_fun,)\n    tfidf = vectorizer.fit_transform(data)    \ndictionary = vectorizer.get_feature_names()\n\n# Retrieving Word embedding vectors:\ntemp_array=[nlp(dictionary[i]).vector for i in range(len(dictionary))]\n\n# I had to build the sparse array in several steps due to RAM constraints\n# (with bigrams the vocabulary gets as large as &gt;1m \ndict_emb_sparse=scipy.sparse.csr_matrix(temp_array[0])\nfor arr in range(1,len(temp_array),100000):\n    print(str(arr))        \n    dict_emb_sparse=scipy.sparse.vstack([dict_emb_sparse, scipy.sparse.csr_matrix(temp_array[arr:min(arr+100000,len(temp_array))])])\n\n# Multiplying the TF-IDF matrix with the Word embeddings: \ntfidf_emb_sparse=tfidf.dot(dict_emb_sparse)\n\n# Translating the Query into the TF-IDF matrix and multiplying with the same Word Embeddings:\nquery_doc= vectorizer.transform(_preprocess([&quot;World of Books is one of the largest online sellers of second-hand books in the world Our massive collection of over million cheap used books also comes with free delivery in the UK Whether it s the latest book release fiction or non-fiction we have what you are looking for&quot;]))\nquery_emb_sparse=query_doc.dot(dict_emb_sparse)\n\n# Calculating Cosine Similarities:\ncosine_similarities = linear_kernel(query_emb_sparse, tfidf_emb_sparse).flatten()\n\nrelated_docs_indices = cosine_similarities.argsort()[:-10:-1]\n\n# Printing the Site descriptions with the highest match:    \nfor ID in related_docs_indices:\n    print(All_lines[ID])\n</code></pre>\n<p>I stole parts of the code/logic from <a href=\"https://github.com/crownpku/text2vec\" rel=\"nofollow noreferrer\">this Github</a> Rep\nDoes anybody see any straightforward errors here?\nMany thanks!!</p>\n",
    "score": 5,
    "creation_date": 1550967691,
    "view_count": 6074,
    "answer_count": 1,
    "tags": "nlp;spacy;tf-idf;word-embedding;tfidfvectorizer"
  },
  {
    "question_id": 52148690,
    "title": "How to make a tree from the output of a dependency parser?",
    "body": "<p>I am trying to make a tree (nested dictionary) from the output of dependency parser. The sentence is \"I shot an elephant in my sleep\". I am able to get the output as described on the link:\n<a href=\"https://stackoverflow.com/questions/7443330/how-do-i-do-dependency-parsing-in-nltk\">How do I do dependency parsing in NLTK?</a></p>\n\n<pre><code>nsubj(shot-2, I-1)\ndet(elephant-4, an-3)\ndobj(shot-2, elephant-4)\nprep(shot-2, in-5)\nposs(sleep-7, my-6)\npobj(in-5, sleep-7)\n</code></pre>\n\n<p>To convert this list of tuples into nested dictionary, I used the following link:\n<a href=\"https://stackoverflow.com/questions/39495924/how-to-convert-python-list-of-tuples-into-tree\">How to convert python list of tuples into tree?</a></p>\n\n<pre><code>def build_tree(list_of_tuples):\n    all_nodes = {n[2]:((n[0], n[1]),{}) for n in list_of_tuples}\n    root = {}    \n    print all_nodes\n    for item in list_of_tuples:\n        rel, gov,dep = item\n        if gov is not 'ROOT':\n            all_nodes[gov][1][dep] = all_nodes[dep]\n        else:\n            root[dep] = all_nodes[dep]\n    return root\n</code></pre>\n\n<p>This gives the output as follows:</p>\n\n<pre><code>{'shot': (('ROOT', 'ROOT'),\n  {'I': (('nsubj', 'shot'), {}),\n   'elephant': (('dobj', 'shot'), {'an': (('det', 'elephant'), {})}),\n   'sleep': (('nmod', 'shot'),\n    {'in': (('case', 'sleep'), {}), 'my': (('nmod:poss', 'sleep'), {})})})}\n</code></pre>\n\n<p>To find the root to leaf path, I used the following link: <a href=\"https://stackoverflow.com/questions/47302382/return-root-to-specific-leaf-from-a-nested-dictionary-tree\">Return root to specific leaf from a nested dictionary tree</a></p>\n\n<p>[Making the tree and finding the path are two separate things]The second objective is to find the root to leaf node path like done <a href=\"https://stackoverflow.com/questions/47302382/return-root-to-specific-leaf-from-a-nested-dictionary-tree]\">Return root to specific leaf from a nested dictionary tree</a>. \nBut I want to get the root-to-leaf (dependency relationship path)\nSo, for instance, when I will call recurse_category(categories, 'an') where categories is the nested tree structure and 'an' is the word in the tree, I should get <code>ROOT-nsubj-dobj</code> (dependency relationship till root) as output.</p>\n",
    "score": 5,
    "creation_date": 1535973426,
    "view_count": 4397,
    "answer_count": 2,
    "tags": "python;dictionary;nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 51722351,
    "title": "Get all possibles pos tags from a single word",
    "body": "<p>I'm currently trying to get all possible pos tags of a single word using Python. \nFrom traditional pos taggers you get back only one tag, if you enter the single word.\nIs there a way to get all possiblities?\nIs it possible to search in a corpora(e.g. brown) for a specific word and not just for a category? </p>\n\n<p>Kind regards &amp; thanks for help</p>\n",
    "score": 5,
    "creation_date": 1533630941,
    "view_count": 926,
    "answer_count": 1,
    "tags": "python-3.x;nlp;nltk"
  },
  {
    "question_id": 51571488,
    "title": "How to calculate key terms from a document with chi-squared test?",
    "body": "<p>I would like to extract key terms from documents with chi-squared test, thus I tried the following:</p>\n\n<pre><code>from sklearn.feature_extraction.text  import CountVectorizer\nfrom sklearn.feature_selection import  SelectKBest, chi2\n\nTexts=[\"should schools have uniform\",\"schools discipline\",\"legalize marriage\",\"marriage culture\"]\nvectorizer = TfidfVectorizer()\nterm_doc=vectorizer.fit_transform(Texts)\nch2 = SelectKBest(chi2, \"all\")\nX_train = ch2.fit_transform(term_doc)\nprint (ch2.scores_)\nvectorizer.get_feature_names()\n</code></pre>\n\n<p>However, I do not have labels and when I run the above code I got:</p>\n\n<pre><code>TypeError: fit() missing 1 required positional argument: 'y'\n</code></pre>\n\n<p>Is there any way of using chi-squared test to extract most important words without having any labels?</p>\n",
    "score": 5,
    "creation_date": 1532781513,
    "view_count": 1378,
    "answer_count": 1,
    "tags": "python;python-3.x;machine-learning;scikit-learn;nlp"
  },
  {
    "question_id": 47564903,
    "title": "How to transform multiple features in a PipeLine using FeatureUnion?",
    "body": "<p>I have a pandas data frame that contains information about messages sent by user.\nFor my model, I'm interested in predicting missing recipients of a message i,e given recipients A,B,C of a message I want to predict who else should have been part of the recipients.</p>\n\n<p>I'm doing multi-label classification using OneVsRestClassifier and LinearSVC.\nFor features, I want to use the recipients of the message. subject and body.</p>\n\n<p>Since recipients is a list of users, I want to transform that column using MultiLabelBinarizer. For Subject and Body, I want to use TFIDF</p>\n\n<p>My input pickle file has data as follows: All values are strings except recipients which is a set()</p>\n\n<pre><code>[[message_id,sent_time,subject,body,set(recipients),message_type, is_sender]]\n</code></pre>\n\n<p>I'm using feature union with custom transformers in the pipeline to achieve this as follows. </p>\n\n<pre><code>from sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nimport pickle\nimport pandas as pd\nimport numpy as np\n\nif __name__ == \"__main__\":\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, column):\n        self.column = column\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X[self.column]\n\nclass MultiLabelTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, column):\n        self.column = column\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        mlb = MultiLabelBinarizer()\n        return mlb.fit_transform(X[self.column])\n\npipeline = Pipeline([\n    ('features', FeatureUnion([\n        ('subject_tfidf', Pipeline([\n                    ('selector', ColumnSelector(column='Subject')),\n                    ('tfidf', TfidfVectorizer(min_df=0.0025, ngram_range=(1, 4)))\n                    ])),\n        ('body_tfidf', Pipeline([\n            ('selector', ColumnSelector(column='Body')),\n            ('tfidf', TfidfVectorizer(min_df=0.0025, ngram_range=(1, 4)))\n        ])),\n        ('recipients_binarizer', Pipeline([\n            ('multi_label', MultiLabelTransformer(column='CoRecipients'))\n        ])),\n    ])),\n    ('classifier', OneVsRestClassifier(LinearSVC(), n_jobs=-1))\n])\n\ntop_recips = ['A', 'B', 'C, 'D]\ncorpus_data = pickle.load(\n    open(\"E:\\\\Data\\\\messages_items.pkl\", \"rb\"))\ndf = pd.DataFrame(corpus_data, columns=[\n    'MessageId', 'SentTime', 'Subject', 'Body', 'Recipients', 'MessageType', 'IsSender'])\n\ndf = df.dropna()\n\n# add co recipients and top recipients columns\ndf['CoRecipients'] = df['Recipients'].apply(\n    lambda r: [x for x in r if x not in top_recips])\ndf['TopRecipients'] = df['Recipients'].apply(\n    lambda r: [x for x in top_recips if x in r])\n\n# drop rows where top recipients = 0\ndf = df.loc[df['TopRecipients'].str.len() &gt; 0]\ndf_train = df.loc[df['SentTime'] &lt;= '2017-10-15']\ndf_test = df.loc[(df['SentTime'] &gt; '2017-10-15') &amp; (df['MessageType'] == 'Meeting')]\n\nmlb = MultiLabelBinarizer(classes=top_recips)\n\ntrain_x = df_train[['Subject', 'Body', 'CoRecipients']]\ntrain_y = mlb.fit_transform(df_train['TopRecipients'])\n\ntest_x = df_train[['Subject', 'Body', 'CoRecipients']]\ntest_y = mlb.fit_transform(df_train['TopRecipients'])\n\nprint \"train\"\npipeline.fit(train_x, train_y)\n\nprint \"predict\"\npredictions = pipeline.predict(test_x)\n\nprint \"done\"\n</code></pre>\n\n<p>I'm not sure if I'm doing the featurization of the CoRecipients column correctly. As the results dont look right. Any clue?</p>\n\n<blockquote>\n  <p><strong>UPDATE 1</strong></p>\n</blockquote>\n\n<p>Changed the code of MLB transformer as follows:</p>\n\n<pre><code>class MultiLabelTransformer(BaseEstimator, TransformerMixin):\n        def __init__(self, column):\n            self.column = column\n\n        def fit(self, X, y=None):\n            self.mlb = MultiLabelBinarizer()\n            self.mlb.fit(X[self.column])\n            return self\n\n        def transform(self, X):\n            return self.mlb.transform(X[self.column])\n</code></pre>\n\n<p>And fixed the test set to use df_test </p>\n\n<pre><code>mlb = MultiLabelBinarizer(classes=top_recips)\n\ntrain_x = df_train[['Subject', 'Body', 'CoRecipients']]\ntrain_y = mlb.fit_transform(df_train['TopRecipients'])\n\ntest_x = df_test[['Subject', 'Body', 'CoRecipients']]\ntest_y = mlb.transform(df_test['TopRecipients'])\n</code></pre>\n\n<p>Seeing the below KeyError</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"E:\\Projects\\NLP\\FeatureUnion.py\", line 99, in &lt;module&gt;\n    predictions = pipeline.predict(test_x)\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 115, in &lt;lambda&gt;\n    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\pipeline.py\", line 306, in predict\n    Xt = transform.transform(Xt)\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\pipeline.py\", line 768, in transform\n    for name, trans, weight in self._iter())\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 779, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 625, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 588, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 111, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 332, in __init__\n    self.results = batch()\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\pipeline.py\", line 571, in _transform_one\n    res = transformer.transform(X)\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\pipeline.py\", line 426, in _transform\n    Xt = transform.transform(Xt)\n  File \"E:\\Projects\\NLP\\FeatureUnion.py\", line 37, in transform\n    return self.mlb.transform(X[self.column])\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\label.py\", line 765, in transform\n    yt = self._transform(y, class_to_index)\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\label.py\", line 789, in _transform\n    indices.extend(set(class_mapping[label] for label in labels))\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\label.py\", line 789, in &lt;genexpr&gt;\n    indices.extend(set(class_mapping[label] for label in labels))\nKeyError: u'cf3024@gmail.com'\n</code></pre>\n\n<p><strong>> UPDATE 2</strong></p>\n\n<p>Working code</p>\n\n<pre><code> class MultiLabelTransformer(BaseEstimator, TransformerMixin):\n        def __init__(self, column, classes):\n            self.column = column\n            self.classes = classes\n    def fit(self, X, y=None):\n        self.mlb = MultiLabelBinarizer(classes=self.classes)\n        self.mlb.fit(X[self.column])\n        return self\n\n    def transform(self, X):\n        return self.mlb.transform(X[self.column])\n\n# drop rows where top recipients = 0\ndf = df.loc[df['TopRecipients'].str.len() &gt; 0]\ndf_train = df.loc[df['SentTime'] &lt;= '2017-10-15']\ndf_test = df.loc[(df['SentTime'] &gt; '2017-10-15') &amp;\n                 (df['MessageType'] == 'Meeting')]\n\nmlb = MultiLabelBinarizer(classes=top_recips)\n\ntrain_x = df_train[['Subject', 'Body', 'CoRecipients']]\ntrain_y = mlb.fit_transform(df_train['TopRecipients'])\n\ntest_x = df_test[['Subject', 'Body', 'CoRecipients']]\ntest_y = mlb.transform(df_test['TopRecipients'])\n\n# get all unique co-recipients\nco_recips = list(set([a for b in df.CoRecipients.tolist() for a in b]))\n\n# create pipeline\npipeline = Pipeline([\n    ('features', FeatureUnion(\n        # list of features\n        transformer_list=[\n            ('subject_tfidf', Pipeline([\n                    ('selector', ColumnSelector(column='Subject')),\n                    ('tfidf', TfidfVectorizer(min_df=0.0025, ngram_range=(1, 4)))\n                    ])),\n            ('body_tfidf', Pipeline([\n                ('selector', ColumnSelector(column='Body')),\n                ('tfidf', TfidfVectorizer(min_df=0.0025, ngram_range=(1, 4)))\n            ])),\n            ('recipients_binarizer', Pipeline([\n                ('multi_label', MultiLabelTransformer(column='CoRecipients', classes=co_recips))\n            ]))\n        ],\n        # weight components in FeatureUnion\n        transformer_weights={\n            'subject_tfidf': 3.0,\n            'body_tfidf': 1.0,\n            'recipients_binarizer': 1.0,\n        }\n    )),\n    ('classifier', OneVsRestClassifier(LinearSVC(), n_jobs=-1))\n])\n\nprint \"train\"\npipeline.fit(train_x, train_y)\n\nprint \"predict\"\npredictions = pipeline.predict(test_x)\n</code></pre>\n",
    "score": 5,
    "creation_date": 1512008695,
    "view_count": 1884,
    "answer_count": 1,
    "tags": "python-2.7;scikit-learn;nlp;data-science;multilabel-classification"
  },
  {
    "question_id": 45861220,
    "title": "Extract most important keywords from a set of documents",
    "body": "<p>I have a set of 3000 text documents and I want to extract top 300 keywords (could be single word or multiple words).</p>\n\n<p>I have tried the below approaches - </p>\n\n<p><a href=\"https://www.airpair.com/nlp/keyword-extraction-tutorial\" rel=\"nofollow noreferrer\">RAKE</a>: It is a Python based keyword extraction library and it failed miserably.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" rel=\"nofollow noreferrer\">Tf-Idf</a>: It has given me good keywords per document, but it is not able to aggregate them and find keywords that represent the whole group of documents.\nAlso, just selecting top k words from each document based on Tf-Idf score won't help, right?</p>\n\n<p><a href=\"https://deeplearning4j.org/word2vec\" rel=\"nofollow noreferrer\">Word2vec</a>: I was able to do some cool stuff like find similar words but not sure how to find important keywords using it.</p>\n\n<p>Can you please suggest some good approach (or elaborate on how to improve any of the above 3) to solve this problem? Thanks :)</p>\n",
    "score": 5,
    "creation_date": 1503576473,
    "view_count": 5646,
    "answer_count": 4,
    "tags": "nlp;rake;feature-extraction;word2vec;tf-idf"
  },
  {
    "question_id": 44159982,
    "title": "Natural Language Generation - how to go beyond templates",
    "body": "<p>We've build a system that analyzes some data and outputs some results in plain English (i.e. no charts etc.). The current implementation relies on lots of templates and some randomization in order to give as much diversity to the text as possible.</p>\n\n<p>We'd like to switch to something more advanced with the hope that the produced text is less repetitive and sounds less robotic. I've searched a lot on google but I cannot find something concrete to start from. Any ideas?</p>\n\n<p>EDIT: The data fed to the NLG mechanism are in JSON format. Here is an example about web analytics data. The json file may contain for example a metric (e.g. visits), it's value in the last X days, whether the last value is expected or not and which dimensions (e.g. countries or marketing channels) affected its change.</p>\n\n<p>The current implementation could give something like this:</p>\n\n<blockquote>\n  <p>Overall visits in the UK mainly from ABC email campaign reached 10K (+20% DoD) and were above the expected value by 10%. Users were mainly landing on XXX page while the increase was consistent across devices.</p>\n</blockquote>\n\n<p>We're looking to finding a way to depend less on templates, sound even more natural and increase the vocabulary.</p>\n",
    "score": 5,
    "creation_date": 1495632932,
    "view_count": 1407,
    "answer_count": 2,
    "tags": "text;nlp;nlg"
  },
  {
    "question_id": 43721175,
    "title": "Python NLTK Shakespeare corpus",
    "body": "<p>I am trying to import sentences from Shakespeare's NLTK corpus – following <a href=\"http://www.nltk.org/howto/corpus.html#shakespeare\" rel=\"nofollow noreferrer\">this</a> help site – but I am having trouble getting access to the sentences (in order to train a word2vec model) :</p>\n\n<pre><code>from nltk.corpus import shakespeare #XMLCorpusreader\nshakespeare.fileids()\n['a_and_c.xml', 'dream.xml', 'hamlet.xml', 'j_caesar.xml', ...]\n\nplay = shakespeare.xml('dream.xml') #ElementTree object\nprint(play)\n&lt;Element 'PLAY' at ...&gt;\n\nfor i in range(9):\n    print('%s: %s' % (play[i].tag, play[i].text))\n</code></pre>\n\n<p>Returns the following :</p>\n\n<pre><code>TITLE: A Midsummer Night's Dream\nPERSONAE: \n\nSCNDESCR: SCENE  Athens, and a wood near it.\nPLAYSUBT: A MIDSUMMER NIGHT'S DREAM\nACT: None\nACT: None\nACT: None\nACT: None\nACT: None\n</code></pre>\n\n<p>Why are all the acts None ?</p>\n\n<p>None of the methods defined here (<a href=\"http://www.nltk.org/howto/corpus.html#data-access-methods\" rel=\"nofollow noreferrer\">http://www.nltk.org/howto/corpus.html#data-access-methods</a>) (.sents(), tagged_sents(), chunked_sents(), parsed_sents()) seem to work when applied to the shakespeare XMLCorpusReader</p>\n\n<p>I'd like to understand :<br>\n1/ how to get the sentences </p>\n\n<p>2/ how to know how to look for them in an ElementTree object</p>\n",
    "score": 5,
    "creation_date": 1493650535,
    "view_count": 2914,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 43382857,
    "title": "How to calculate the distance in meaning of two words in Python",
    "body": "<p>I am wondering if it's possible to calculate the distance/similarity between two related words in Python (like \"fraud\" and \"steal\"). These two words are not synonymous per se but they are clearly related. Are there any concepts/algorithms in NLP that can show this relationship numerically? Maybe via NLTK?</p>\n\n<p>I'm not looking for the Levenshtein distance as that relates to the individual characters that make up a word. I'm looking for how the meaning relates. </p>\n\n<p>Would appreciate any help provided.</p>\n",
    "score": 5,
    "creation_date": 1492052690,
    "view_count": 2971,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 41196081,
    "title": "spaCy Alternatives in Java",
    "body": "<p>I currently use spaCy to traverse the dependency tree, and generate entities. </p>\n\n<pre><code>nlp = get_spacy_model(detect_lang(unicode_text))\ndoc = nlp(unicode_text)\n\nentities = set()\nfor sentence in doc.sents:\n\n  # traverse tree picking up entities\n  for token in sentence.subtree:\n    ## pick entitites using some pre-defined rules\n\nentities.discard('')\nreturn entities\n</code></pre>\n\n<p>Are there any good Java alternatives for spaCy? </p>\n\n<p>I am looking for libs which generate the Dependency Tree as is done by spaCy.</p>\n\n<p>EDIT:</p>\n\n<p>I looked into Stanford Parser. However, it generated the following parse tree:</p>\n\n<pre><code>                     ROOT\n                      |\n                      NP\n       _______________|_________\n      |                         NP\n      |                _________|___\n      |               |             PP\n      |               |     ________|___\n      NP              NP   |            NP\n  ____|__________     |    |     _______|____\n DT   JJ    JJ   NN  NNS   IN   DT      JJ   NN\n |    |     |    |    |    |    |       |    |\nthe quick brown fox jumps over the     lazy dog\n</code></pre>\n\n<p>However, I am looking for a tree structure like spaCy does:</p>\n\n<pre><code>                             jumps_VBZ\n   __________________________|___________________\n  |       |        |         |      |         over_IN\n  |       |        |         |      |            |\n  |       |        |         |      |          dog_NN\n  |       |        |         |      |     _______|_______\nThe_DT quick_JJ brown_JJ   fox_NN  ._. the_DT         lazy_JJ\n</code></pre>\n",
    "score": 5,
    "creation_date": 1481957008,
    "view_count": 11855,
    "answer_count": 4,
    "tags": "nlp;stanford-nlp;pos-tagger;spacy"
  },
  {
    "question_id": 41085755,
    "title": "Word2Vec - adding constraint to vector representation",
    "body": "<p>I am trying to adapt the pre-trained Google News word2vec model to my specific domain. For the domain I am looking at, certain words are known to be similar to each other so in an ideal world, the Word2Vec representation of those words should represent that. I understand that I can train the pre-trained model on a corpus of domain-specific data to update the vectors. </p>\n\n<p>However, if I know for certain that certain words are highly similar and should be together, is there a way for me to incorporate that constraint into the word2vec model? Mathematically, I would like to add a term to the loss function of word2vec that provides a penalty if two that I know to be similar are not positioned close to each other in the vector space. Does anyone have advice on how to implement this? Will this require me to unpack the word2vec model or is there a way for me to potentially add that additional term to the loss function?</p>\n",
    "score": 5,
    "creation_date": 1481456511,
    "view_count": 615,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;word2vec"
  },
  {
    "question_id": 40640242,
    "title": "pyparsing.ParseException when using parseString (searchString works)",
    "body": "<p>I'm trying to parse some Traffic Violation sentences using pyparsing, when I use <code>grammar.searchString(sentence)</code> it is ok, but when I use <code>parseString</code> a <code>ParseException</code>   is thrown. Can anybody help me please saying what is wrong with my code?</p>\n\n<pre><code>from pyparsing import Or, Literal, oneOf, OneOrMore, nums, alphas, Regex, Word, \\\n    SkipTo, LineEnd, originalTextFor, Optional, ZeroOrMore, Keyword, Group\nimport pyparsing as pp\n\nfrom nltk.tag import pos_tag\n\nsentences = ['Failure to control vehicle speed on highway to avoid collision','Failure to stop at stop sign', 'Introducing additives into special fuel by unauthorized person and contrary to regulations', 'driver fail to stop at yield sign at nearest pointf approaching traffic view when req. for safety', 'Operating unregistered motor vehicle on highway', 'Exceeding maximum speed: 39 MPH in a posted 30 MPH zone']\n\n\nfor sentence in sentences:\n    words = pos_tag(sentence.split())\n    #print words\n    verbs = [word for word, pos in words if pos in ['VB','VBD','VBG']]\n    nouns = [word for word, pos in words if pos == 'NN']\n    adjectives = [word for word, pos in words if pos == 'JJ']\n\n    adjectives.append('great')  # initializing  \n    verbs.append('get') # initializing \n\n\n    object_generator = oneOf('for to')\n    location_generator = oneOf('at in into on onto over within')\n    speed_generator = oneOf('MPH KM/H')\n\n    noun = oneOf(nouns)\n    adjective = oneOf(adjectives)\n\n    location = location_generator + pp.Group(Optional(adjective) + noun)\n\n    action = oneOf(verbs)\n    speed = Word(nums) + speed_generator\n\n    grammar =  action | location | speed\n\n    parsed = grammar.parseString(sentence)\n\n    print parsed\n</code></pre>\n\n<p>Error traceback</p>\n\n<blockquote>\n  <p>Traceback (most recent call last): File \"script3.py\", line 35, in  parsed = grammar.parseString(sentence) File \"/Users/alana/anaconda/lib/python2.7/site-packages/pyparsing‌​.py\", line 1032, in parseString raise exc pyparsing.ParseException: Expected Re:('control|avoid|get') (at char 0), (line:1, col:1)</p>\n</blockquote>\n",
    "score": 5,
    "creation_date": 1479322991,
    "view_count": 1619,
    "answer_count": 1,
    "tags": "python;nlp;nltk;grammar;pyparsing"
  },
  {
    "question_id": 36746071,
    "title": "Building your own NLP API",
    "body": "<p>I'm building a chatbot and I'm new to NLP.</p>\n\n<p>(api.ai &amp; AlchemyAPI are too expensive for my use case. And wit.ai seems to be buggy and constantly changing at the moment.)</p>\n\n<p>For the NLP experts, how easily can I replicate their services locally?</p>\n\n<p>My vision so far (with node, but open to Python):</p>\n\n<ul>\n<li>entity extraction via StanfordNER</li>\n<li>intent via NodeNatural's LogisticRegressionClassifier</li>\n<li>training UI with text and validate/invalidate buttons (any prebuilt tools for this?)</li>\n</ul>\n\n<p>Are entities and intents all I'll need for a chatbot? How good will NodeNatural/StanfordNER be compared to NLP-as-a-service? What headaches am I not seeing?</p>\n",
    "score": 5,
    "creation_date": 1461160446,
    "view_count": 2801,
    "answer_count": 3,
    "tags": "python;node.js;nlp;chatbot"
  },
  {
    "question_id": 36027477,
    "title": "Extracting Product Attribute/Features from text",
    "body": "<p>I've been assigned a task to extract features/attributes from product description. </p>\n\n<pre><code>Levi Strauss slim fit jeans\nBig shopping bag in pink and gold\n</code></pre>\n\n<p>I need to be able to extract out attributes such as \"Jeans\" and \"slim fit\" or \"shopping bag\" and \"pink\" and \"gold\".\nThe product description listings are not just for clothes, they can basically be anything.</p>\n\n<p>I am not sure how to approach this problem. I tried implementing a Named Entity Recognizer solution and also a POS implementations, The NER implementation fails to recognize any token and most of the tokens show up as NNP(Proper Nouns) in he POS solution, which doesn't help me out a lot. I need a way to be able to distinguish between the brand name and the features of the Product(like if it is a t-shirt, the color or design(round neck, v-neck) etc). </p>\n\n<p>I did implement a KMean solution which did cluster like products together, but then again it is not the result I am looking for.</p>\n\n<p>Just looking for someone to direct me in the correct direction.</p>\n",
    "score": 5,
    "creation_date": 1458106299,
    "view_count": 2190,
    "answer_count": 2,
    "tags": "nlp;feature-extraction;named-entity-recognition;named-entity-extraction"
  },
  {
    "question_id": 34455749,
    "title": "nltk : How to prevent stemming of proper nouns",
    "body": "<p>I am trying to wrote a keyword extraction program using Stanford POS taggers and NER. For keyword extraction, i am only interested in proper nouns. Here is the basic approach</p>\n\n<ol>\n<li>Clean up the data by removing anything but alphabets</li>\n<li>Remove stopwords</li>\n<li>Stem each word</li>\n<li>Determine POS tag of each word</li>\n<li>If the POS tag is a noun then feed it to the NER</li>\n<li>The NER will then determine if the word is a person, organization or location</li>\n</ol>\n\n<p>sample code</p>\n\n<pre><code>docText=\"'Jack Frost works for Boeing Company. He manages 5 aircraft and their crew in London\"\n\nwords = re.split(\"\\W+\",docText) \n\nstops = set(stopwords.words(\"english\"))\n\n#remove stop words from the list\nwords = [w for w in words if w not in stops and len(w) &gt; 2]\n\n# Stemming\npstem = PorterStemmer()\n\nwords = [pstem.stem(w) for w in words]    \n\nnounsWeWant = set(['NN' ,'NNS', 'NNP', 'NNPS'])\n\nfinalWords = []\n\nstn = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \nstp = StanfordPOSTagger('english-bidirectional-distsim.tagger') \n\nfor w in words:\n    if stp.tag([w.lower()])[0][1] not in nounsWeWant:\n        finalWords.append(w.lower())\n    else:\n        finalWords.append(w)\n\nfinalString = \" \".join(finalWords)\nprint finalString\n\ntagged = stn.tag(finalWords)\nprint tagged\n</code></pre>\n\n<p>which gives me</p>\n\n<pre><code>Jack Frost work Boe Compani manag aircraft crew London\n[(u'Jack', u'PERSON'), (u'Frost', u'PERSON'), (u'work', u'O'), (u'Boe', u'O'), (u'Compani', u'O'), (u'manag', u'O'), (u'aircraft', u'O'), (u'crew', u'O'), (u'London', u'LOCATION')]\n</code></pre>\n\n<p>so clearly, i did not want Boeing to be stemmed. nor Company. I need to stem the words as my input might contain terms like <code>Performing</code>. I have seen that a word like <code>Performing</code> will be picked up by the NER as a proper noun and hence could be categorized as  <code>Organization</code>. Hence, first i stem all the words and convert to lower case. Then i check to see if the POS tag of the word is a noun. If so, i keep it as is. If not, i convert the word to lower case and add it to the final word list that will be passed to the NER.</p>\n\n<p>Any idea on how to avoid stemming proper nouns?</p>\n",
    "score": 5,
    "creation_date": 1450975928,
    "view_count": 2683,
    "answer_count": 1,
    "tags": "python;nlp;nltk;stanford-nlp;stemming"
  },
  {
    "question_id": 32973119,
    "title": "Python - NLP - convert iter(iter(tree)) to list(tree)",
    "body": "<p>I have a parser function which returns <code>iter(iter(tree))</code>. </p>\n\n<pre><code>parsedSentence = parser.raw_parse_sents([sentence],False)  \n</code></pre>\n\n<p>How can I convert the parsedSentence type to list(tree)  and access 1st element of that list. </p>\n\n<p><em>I've already tried <code>list(parser.raw_parse_sents([sentence],False))</code> but it's not converting the result to list.</em> </p>\n\n<p><strong>Edited:</strong>  </p>\n\n<pre><code>s1 = parsedSentence[0]\nt1 = Tree.convert(s1)\npositions = t1.treepositions()\n</code></pre>\n\n<p>Here it throws an error:</p>\n\n<pre><code>'listiterator' object has no attribute 'treepositions'\n</code></pre>\n\n<p>Thank You.</p>\n",
    "score": 5,
    "creation_date": 1444143240,
    "view_count": 586,
    "answer_count": 2,
    "tags": "python;list;parsing;iterator;nlp"
  },
  {
    "question_id": 32794464,
    "title": "How to parse temporal expressions (esp. time ranges), Python?",
    "body": "<p>I have an NLP task which has 3 components. I tried few methods (mentioned in the end) but I am not able to get good results.</p>\n\n<ol>\n<li>Detecting temporal expressions in a statement</li>\n<li>Classifying then as either <strong>time stamp</strong>, <strong>time trigger</strong> or <strong>time period</strong>.</li>\n<li>Equate each expression to its DateTime equivalent.</li>\n</ol>\n\n<p>Example:</p>\n\n<p>Taking reference time as <strong>2000 hrs, Thursday, July 20th, 2015</strong></p>\n\n<ol>\n<li><p><strong>time stamp</strong> :</p>\n\n<p>I want to book a cab 20 minutes from now</p>\n\n<pre><code>Answer: [tStamp]2020 hrs, Thursday, July 20th\n</code></pre></li>\n<li><p><strong>time trigger</strong> : </p>\n\n<p>Any timer after 2 is fine</p>\n\n<pre><code>Answer:  [tTrigger] - start - 0200 hrs,July 21st 2015\n</code></pre>\n\n<p>Before 5 is good</p>\n\n<pre><code>[tTrigger] - start - now, 2000hrs, July 20th, 2015 : end - 0500 hrs, July 21st, 2015\n</code></pre></li>\n<li><p><strong>time period</strong>:</p>\n\n<p>I was working in san francisco for last two years</p>\n\n<pre><code>[tPeriod] -  2013-2015\n</code></pre></li>\n</ol>\n\n<p>I tried to do this with regex gives very generic results. Second option I read was to try to make the model learn from Naive Bays classifier but naive bays learns exact words and not phrases.</p>\n\n<p>I came across <a href=\"https://pypi.python.org/pypi/parsedatetime/\" rel=\"nofollow\">parsedatetime 1.5</a> package in python which is awesome to some extent in converting phrases to timestamps which solves 3. of mentioned problem but still I am not able to solve detention and classification.</p>\n",
    "score": 5,
    "creation_date": 1443250129,
    "view_count": 1480,
    "answer_count": 1,
    "tags": "python;nlp;stanford-nlp;jnlp;opennlp"
  },
  {
    "question_id": 30550315,
    "title": "Handling count of characters with diacritics in R",
    "body": "<p>I'm trying to get the number of characters in strings with characters with diacritics, but I can't manage to get the right result.</p>\n\n<pre><code>&gt; x &lt;- \"n̥ala\"\n&gt; nchar(x)\n[1] 5\n</code></pre>\n\n<p>What I want to get is is <code>4</code>, since <code>n̥</code> should be considered one character (i.e. diacritics shouldn't be considered characters on their own, even with more than one diacritic stacked on a base character).</p>\n\n<p>How can I get this kind of result?</p>\n",
    "score": 5,
    "creation_date": 1433013291,
    "view_count": 504,
    "answer_count": 3,
    "tags": "r;unicode;character-encoding;nlp;linguistics"
  },
  {
    "question_id": 29676112,
    "title": "Python: Goslate translation request returns &quot;503: Service Unavailable&quot;",
    "body": "<p>A few months ago, I used Python's <code>goslate</code> package to translate a bunch of French text to English. When I tried to do so this morning, though, the service returned an error:</p>\n\n<pre><code>import goslate\ngs = goslate.Goslate()\nprint gs.translate('hello world', 'de')\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"c:\\Python27\\lib\\site-packages\\goslate.py\", line 389, in translate\n    return _unwrapper_single_element(self._translate_single_text(text, target_language, source_language))\n  File \"c:\\Python27\\lib\\site-packages\\goslate.py\", line 317, in _translate_single_text\n    results = list(self._execute(make_task(i) for i in split_text(text)))\n  File \"c:\\Python27\\lib\\site-packages\\goslate.py\", line 200, in _execute\n    yield each()\n  File \"c:\\Python27\\lib\\site-packages\\goslate.py\", line 315, in &lt;lambda&gt;\n    return lambda: self._basic_translate(text, target_language, source_lauguage)[0]\n  File \"c:\\Python27\\lib\\site-packages\\goslate.py\", line 241, in _basic_translate\n    response_content = self._open_url(url)\n  File \"c:\\Python27\\lib\\site-packages\\goslate.py\", line 178, in _open_url\n    response = self._opener.open(request, timeout=self._TIMEOUT)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 437, in open\n    response = meth(req, response)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 550, in http_response\n    'http', request, response, code, msg, hdrs)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 469, in error\n    result = self._call_chain(*args)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 409, in _call_chain\n    result = func(*args)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 656, in http_error_302\n    return self.parent.open(new, timeout=req.timeout)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 437, in open\n    response = meth(req, response)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 550, in http_response\n    'http', request, response, code, msg, hdrs)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 475, in error\n    return self._call_chain(*args)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 409, in _call_chain\n    result = func(*args)\n  File \"c:\\Python27\\lib\\urllib2.py\", line 558, in http_error_default\n    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)\nurllib2.HTTPError: HTTP Error 503: Service Unavailable\n</code></pre>\n\n<p>Does anyone know what happened to <code>goslate</code>? If it's gone for good, are there decent alternatives to the <code>goslate</code> package for translating French to English via an API call?</p>\n",
    "score": 5,
    "creation_date": 1429190690,
    "view_count": 3900,
    "answer_count": 1,
    "tags": "python;nlp;translation;machine-translation"
  },
  {
    "question_id": 25566426,
    "title": "Correcting repeated letters in user messages",
    "body": "<p>I try to analyze messages from social media or chats. A common issue is this special kind of misspelling where people use repeated characters to express their emotions or whatever e.g.</p>\n\n<pre><code>\"wowwwwww!\"\n\"Daaaaaaamn!\"\n\"I'm soooooo pisssssed\" \n</code></pre>\n\n<p>I wonder if there is a more or less robust way to correct those cases.</p>\n\n<p>To replace repeated characters (3 repetitions or more) with only two characters, I already found the suitable regex:</p>\n\n<pre><code>s = re.sub(r'(.)\\1+', r'\\1\\1', s)\n</code></pre>\n\n<p>But given the example above this would result in </p>\n\n<pre><code>\"woww!\"\n\"Daamn!\"\n\"I'm soo pissed\"\n</code></pre>\n\n<p>This looks better, but not yet perfect. How could I decide best when to replace it with one character instead of two? </p>\n\n<p>A (probably naive) approach would be to use a dictionary to check if one version or the other is in the dictionary. Or perhaps are are some rather simple linguistic rules to check when double letters are needed. Or maybe there is a much better way to do this altogether.</p>\n\n<p>EDIT: Based on all the answer I came up with a solution that seems to work not too badly</p>\n\n<ul>\n<li>Used Ubuntu's <code>/usr/share/dict/words</code> as dictionary</li>\n<li>Indexed all dictionary words using <code>Soundex</code> (and other similar algorithms)</li>\n<li>For each input word I (a) check if it's in the index, and if not (b) all similar-sounding words and select the one with the highest Levenshtein similarity -- note that, as a preliminary step, replace all 2+ repeated letters with 2 letters</li>\n<li>In the rare case I get multiple top results (e.g., <code>wooww</code> results in <code>wow</code> and <code>woo</code> as equally similar), I exploit the notion of <a href=\"http://en.wikipedia.org/wiki/Typoglycemia\" rel=\"nofollow\">Typoglycemia</a> checking the similarity based on the first and last letter</li>\n</ul>\n\n<p>With this solution I not only cover the issue of repeated letters pretty well but also typos in general. It's probably far from perfect, but on the other hand it's a very simple solution.</p>\n",
    "score": 5,
    "creation_date": 1409308052,
    "view_count": 1156,
    "answer_count": 3,
    "tags": "python;regex;nlp"
  },
  {
    "question_id": 15045190,
    "title": "Named Entity Recognition Data and Features",
    "body": "<p>I am building a Named Entity Recognizer with a Conditional Random Field and am looking for two things:</p>\n\n<p>A) An open source, English NER dataset for Person, Location, and Organization entities</p>\n\n<p>B) A list of English NER features</p>\n\n<p>I have already looked at the CoNLL-2003 corpus and found this is exactly what I want but it is not readily available. I have been unsuccessful in finding a list of NER features; I am trying to avoid having to hand design these features. </p>\n\n<p>Thanks</p>\n",
    "score": 5,
    "creation_date": 1361650809,
    "view_count": 3949,
    "answer_count": 2,
    "tags": "nlp;named-entity-recognition"
  },
  {
    "question_id": 10941370,
    "title": "Search string for numbers",
    "body": "<p>I have a javascript chat bot where a person can type into an input box any question they like and hope to get an accurate answer. I can do this but I know I'm going about this all wrong because I don't know what position the number will appear in the sentence. If a person types in exactly:</p>\n\n<p>what's the square root of 5 this works fine. </p>\n\n<p>If he types in things like this it doesn't. </p>\n\n<p>what is the square root of the 5</p>\n\n<p>the square root of 5 is what</p>\n\n<p>do you know what the square root of 5 is</p>\n\n<p>etc</p>\n\n<p>I need to be able to determine where the number appears in the sentence then do the calculation from there. Note the line below is part of a bigger working chatbot. In the line below I'm just trying to be able to answer any square root question regardless of where the number appears in the sentence. I also know there are many pitfalls with an open ended input box where a person can type anything such as spelling errors etc. This is just for entertainment not a serious scientific project. :)   </p>\n\n<pre><code>if(\n    (word[0]==\"what's\") &amp;&amp;\n    (word[1]==\"the\") &amp;&amp;\n    (word[2]==\"square\") &amp;&amp;\n    (word[3]==\"root\") &amp;&amp;\n    (word [4]==\"of\") &amp;&amp;\n    (input.search(/\\d{1,10}/)!=-1) &amp;&amp;\n    (num_of_words==6)\n){        \n    var root= word[5];\n    if(root&lt;0){ \n        document.result.result.value = \"The square root of a negative number is not possible.\";\n    }else{\n         word[5] = Math.sqrt(root);\n         word[5] = Math.round(word[5]*100)/100 \n         document.result.result.value = \"The square root of \"+ root +\" is \"+ word[5] +\".\"; \n    }\n    return true;\n}\n</code></pre>\n\n<p>Just to be clear the bot is written using \"If statemments\" for a reason. If the input in this case doesn't include the words \"what\" and \"square root\" and \"some number\" the line doesn't trigger and is answered further down by the bot with a generic \"I don't know type of response\". So I'm hoping any answer will fit the format I am using. Be kind, I'm new here. I like making bots but I'm not much of a programmer. Thanks.</p>\n",
    "score": 5,
    "creation_date": 1339112717,
    "view_count": 763,
    "answer_count": 2,
    "tags": "javascript;math;nlp;string-parsing;square-root"
  },
  {
    "question_id": 10708852,
    "title": "How to calculate probabilities from confusion matrices? need denominator, chars matrices",
    "body": "<p><a href=\"http://acl.ldc.upenn.edu/C/C90/C90-2036.pdf\" rel=\"noreferrer\">This paper</a> contains confusion matrices for spelling errors in a noisy channel. It describes how to correct the errors based on conditional properties.</p>\n\n<p>The conditional probability computation is on page 2, left column. In footnote 4, page 2, left column, the authors say: \"The chars matrices  can  be   easily  replicated, and are therefore omitted from the appendix.\" I cannot figure out how can they be replicated!</p>\n\n<p><strong>How to replicate them? Do I need the original corpus? or, did the authors mean they could be recomputed from the material in the paper itself?</strong></p>\n",
    "score": 5,
    "creation_date": 1337715196,
    "view_count": 2014,
    "answer_count": 1,
    "tags": "nlp;machine-learning;stanford-nlp;opennlp;confusion-matrix"
  },
  {
    "question_id": 9757904,
    "title": "Discover user behind multiple different user accounts according to words he uses",
    "body": "<p>I would like to create algorithm to distinguish the persons writing on forum under different nicknames.</p>\n\n<p>The goal is to discover people registring new account to flame forum anonymously, not under their main account.</p>\n\n<p>Basicaly I was thinking about stemming words they use and compare users according to similarities or these words.</p>\n\n<p><img src=\"https://i.sstatic.net/ggqW0.png\" alt=\"Users using words\"></p>\n\n<p>As shown on the picture there is user3 and user4 who uses same words. It means there is probably one person behind the computer.</p>\n\n<p>Its clear that there are lot of common words which are being used by all users. So I should focus on \"user specific\" words.</p>\n\n<p>Input is (related to the image above):</p>\n\n<pre><code>&lt;word1, user1&gt;\n&lt;word2, user1&gt;\n&lt;word2, user2&gt;\n&lt;word3, user2&gt;\n&lt;word4, user2&gt;\n&lt;word5, user3&gt;\n&lt;word5, user4&gt;\n... etc. The order doesnt matter\n</code></pre>\n\n<p>Output should be:</p>\n\n<pre><code>user1\nuser2\nuser3 = user4\n</code></pre>\n\n<p>I am doing this in Java but I want this question to be language independent.</p>\n\n<p><strong>Any ideas how to do it?</strong></p>\n\n<p>1) how to store words/users? What data structures?</p>\n\n<p>2) how to get rid of common words everybody use? I have to somehow ignore them among user specific words. Maybe I could just ignore them because they get lost. I am afraid that they will hide significant difference of \"user specific words\"</p>\n\n<p>3) how to recognize same users? - somehow count same words between each user?</p>\n\n<p>I am very thankful for every advice in advance.</p>\n",
    "score": 5,
    "creation_date": 1332069961,
    "view_count": 512,
    "answer_count": 3,
    "tags": "algorithm;language-agnostic;nlp"
  },
  {
    "question_id": 8370366,
    "title": "Running CRFSuite examples",
    "body": "<p>I'm trying to use CRFSuite but I can't figure out how to use the example/ner.py and pos.py</p>\n\n<p>Precisely, how do I make an input of the form:</p>\n\n<pre><code># Ner.py\nfields = 'y w pos chk'\n</code></pre>\n\n<p>or</p>\n\n<pre><code># Pos.py\nfields = 'w num cap sym p1 p2 p3 p4 s1 s2 s3 s4 y'\n</code></pre>\n\n<p>The \"y w pos\" I can get from a CoNNL model, for example, but the \"chk\" part and all those fields in pos.py I don't really get.</p>\n\n<p>Also, is there a way to process a raw text (without all those tags) with CRFSuite given that I have a trained model?</p>\n",
    "score": 5,
    "creation_date": 1322940825,
    "view_count": 3544,
    "answer_count": 3,
    "tags": "python;machine-learning;nlp;crfsuite"
  },
  {
    "question_id": 7377197,
    "title": "How to Join Arabic letters to form words",
    "body": "<p>I have to read arabic letters from xml file and display them as a word </p>\n\n<p>input :س ع ا د ة\noutput :سعادة look like that ..</p>\n\n<p>I dont know how do that in any language , what algorithm to read, I need some start point to acomplish this task </p>\n\n<p>I am also not sure if i have added the right tags, please free to make changes.</p>\n",
    "score": 5,
    "creation_date": 1315730432,
    "view_count": 2385,
    "answer_count": 2,
    "tags": "algorithm;nlp;arabic"
  },
  {
    "question_id": 7113008,
    "title": "Fuzzy sentence search algorithms",
    "body": "<p>Suppose I have a set of phrases - about 10 000 - of average length - 7-20 words in which I want to find some given phrase. The phrase I am looking for could have some errors - for example miss one or two words, have some words misplaced, or some random words - for example my database contains \"As I was riding my red bike, I saw Christine\", and I want it to much \"As I was riding my blue bike, saw Christine\", or \"I was riding my bike, I saw Christine and Marion\". What could be some good approach to this problem? I know about Levenhstein's distance, and I also suppose that this problem may have no easy, good solution.</p>\n",
    "score": 5,
    "creation_date": 1313695772,
    "view_count": 1183,
    "answer_count": 2,
    "tags": "nlp;fuzzy-search"
  },
  {
    "question_id": 6003246,
    "title": "USE (NLP) GATE TOOL FOR NAMED-ENTITY",
    "body": "<p>Can I use GATE <a href=\"http://gate.ac.uk/\" rel=\"noreferrer\">http://gate.ac.uk/</a> within my java program to extract named-entity. If yes, could you give any examples or guide me to some sources. Thank you </p>\n",
    "score": 5,
    "creation_date": 1305391201,
    "view_count": 4211,
    "answer_count": 2,
    "tags": "nlp;gate"
  },
  {
    "question_id": 5919123,
    "title": "java keyword extraction",
    "body": "<p>Is there a simple to use Java library that can take a String and return a set of Strings which are the keywords/keyphrases.</p>\n\n<p>It doesn't have to be particularly clever, just use stop words and stemming to match keywords.</p>\n\n<p>I am looking at the KEA package <a href=\"http://code.google.com/p/kea-algorithm/\" rel=\"nofollow\">http://code.google.com/p/kea-algorithm/</a> but I can't figure out how to use their code.</p>\n\n<p>Ideally something simple which has a little example documentation would be good. In the meantime I will set about writing this myself!</p>\n\n<p>EDIT: When I say I can't see how to figure out how to use their code, I mean I can't see a simple way. The individiual classes by themselves have useful methods that will do much of the work.</p>\n",
    "score": 5,
    "creation_date": 1304742377,
    "view_count": 6165,
    "answer_count": 2,
    "tags": "java;nlp;keyword"
  },
  {
    "question_id": 5616271,
    "title": "POS Pattern Filter?",
    "body": "<p>I'm writing some code that iterates a set of POS tags (generated by pos_tag in NLTK) to search for POS patterns. Matching sets of POS tags are stored in a list for later processing. Surely a regex-style pattern filter already exists for a task like this, but a couple of initial google searches didn't give me anything.</p>\n\n<p>Are there any code snippets out there that can do my POS pattern filtering for me?</p>\n\n<p>Thanks,\nDave</p>\n\n<p>EDIT: Complete solution (using RegexParser, and where messages is any string)</p>\n\n<pre><code>text = nltk.word_tokenize(message)\ntags = nltk.pos_tag(text)\ngrammar = r\"\"\"\n    RULE_1: {&lt;JJ&gt;+&lt;NNP&gt;*&lt;NN&gt;*}\n    \"\"\"\nchunker = nltk.RegexpParser(grammar)\nchunked = chunker.parse(tags)\ndef filter(tree):\n    return (tree.node == \"RULE_1\")\nfor s in chunked.subtrees(filter):\n    print s\n</code></pre>\n\n<p>Check out <a href=\"http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html\" rel=\"nofollow\">http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html</a> and <a href=\"http://www.regular-expressions.info/reference.html\" rel=\"nofollow\">http://www.regular-expressions.info/reference.html</a> for more on creating the rules.</p>\n",
    "score": 5,
    "creation_date": 1302489583,
    "view_count": 1853,
    "answer_count": 1,
    "tags": "nlp;nltk"
  },
  {
    "question_id": 4989381,
    "title": "Nullpointer Exception with OpenNLP in NameFinderME class",
    "body": "<p>I am using <strong><a href=\"https://opennlp.apache.org/\" rel=\"nofollow noreferrer\">OpenNLP</a></strong> to extract named entities from a given text.\nIt gives me the following error while running the code on large data. When I run it on small data it works fine.</p>\n\n<pre><code>java.lang.NullPointerException\n    at opennlp.tools.util.Cache.put(Cache.java:134)\n    at opennlp.tools.util.featuregen.CachedFeatureGenerator.createFeatures(CachedFeatureGenerator.java:71)\n    at opennlp.tools.namefind.DefaultNameContextGenerator.getContext(DefaultNameContextGenerator.java:116)\n    at opennlp.tools.namefind.DefaultNameContextGenerator.getContext(DefaultNameContextGenerator.java:39)\n    at opennlp.tools.util.BeamSearch.bestSequences(BeamSearch.java:125)\n    at opennlp.tools.util.BeamSearch.bestSequence(BeamSearch.java:198)\n    at opennlp.tools.namefind.NameFinderME.find(NameFinderME.java:214)\n    at opennlp.tools.namefind.NameFinderME.find(NameFinderME.java:198)\n</code></pre>\n\n<p>Please help me out with this.</p>\n",
    "score": 5,
    "creation_date": 1297664442,
    "view_count": 1475,
    "answer_count": 1,
    "tags": "nlp;opennlp"
  },
  {
    "question_id": 4604399,
    "title": "Earley recognizer to Earley parser",
    "body": "<p>I managed to create Earley recognizer, everything works fine. I have all proper sets of situation. But I only can use it to decide if word is accepted by grammar. How to make it to parse? I need some article or explanation, it seems that I need to create associations to situations that made new situations. Any help would be appreciated.</p>\n\n<p>My implementation it's based exactly on: <a href=\"http://www.cs.uvic.ca/~nigelh/Publications/PracticalEarleyParsing.pdf\" rel=\"nofollow\">http://www.cs.uvic.ca/~nigelh/Publications/PracticalEarleyParsing.pdf</a></p>\n",
    "score": 5,
    "creation_date": 1294233320,
    "view_count": 1027,
    "answer_count": 2,
    "tags": "algorithm;parsing;nlp;earley-parser"
  },
  {
    "question_id": 68009571,
    "title": "Naive Gaussian predict probability only returns 0 or 1",
    "body": "<p>I trained the GaussianNB model from scikit sklearn. When I call the method  <code>classifier.predict_proba</code> it only returns 1 or 0 on new data. It is expected to return a percentage of confidence that the prediction is correct or not. I doubt it can have 100% confidence on new data it has never seen before. I have tested it on multiple different inputs. I use CountVectorizer and TfidfTransformer for the text encoding.</p>\n<p>The encoding:</p>\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ncount_vect = CountVectorizer()\ntfidf_transformer = TfidfTransformer()\n\nX_train_counts = count_vect.fit_transform(X_train_word)\nX_train = tfidf_transformer.fit_transform(X_train_counts).toarray()\nprint(X_train)\n\nX_test_counts = count_vect.transform(X_test_word)\nX_test = tfidf_transformer.transform(X_test_counts).toarray()\nprint(X_test)\n</code></pre>\n<p>The model: (I am getting an accuracy of 91%)</p>\n<pre><code>from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predict Class\ny_pred = classifier.predict(X_test)\n\n# Accuracy \nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_pred)\nprint(accuracy)\n</code></pre>\n<p>And finally, when I use the predict_proba method:</p>\n<pre><code>y_pred = classifier.predict_proba(X_test)\nprint(y_pred)\n</code></pre>\n<p>I am getting an output like:</p>\n<pre><code>[[0. 1.]\n [1. 0.]\n [0. 1.]\n ...\n [1. 0.]\n [1. 0.]\n [1. 0.]]\n</code></pre>\n<p>It doesn't make much sense to have 100% accuracy on new data. Other than on <code>y_test</code> I have tested it on other inputs and it still returns the same. Any help would be appreciated!</p>\n<p>Edit for the comments:\nThe response of <code>.predict_log_proba()</code> is even more strange:</p>\n<pre><code>[[ 0.00000000e+00 -6.95947375e+09]\n [-4.83948755e+09  0.00000000e+00]\n [ 0.00000000e+00 -1.26497690e+10]\n ...\n [ 0.00000000e+00 -6.97191054e+09]\n [ 0.00000000e+00 -2.25589894e+09]\n [ 0.00000000e+00 -2.93089863e+09]]\n</code></pre>\n",
    "score": 5,
    "creation_date": 1623874970,
    "view_count": 1242,
    "answer_count": 1,
    "tags": "python;machine-learning;scikit-learn;nlp;data-science"
  },
  {
    "question_id": 67136740,
    "title": "Fine-tune a BERT model for context specific embeddigns",
    "body": "<p>I'm trying to find information on how to train a BERT model, possibly from the <a href=\"https://huggingface.co/transformers/index.html\" rel=\"noreferrer\">Huggingface Transformers</a> library, so that the embedding it outputs are more closely related to the context o the text I'm using.</p>\n<p>However, all the examples that I'm able to find, are about fine-tuning the model for another task, such as <a href=\"https://huggingface.co/transformers/training.html\" rel=\"noreferrer\">classification</a>.</p>\n<p>Would anyone happen to have an example of a BERT fine-tuning model for masked tokens or next sentence prediction, that outputs another raw BERT model that is fine-tuned to the context?</p>\n<p>Thanks!</p>\n",
    "score": 5,
    "creation_date": 1618652514,
    "view_count": 2203,
    "answer_count": 1,
    "tags": "python;nlp;bert-language-model"
  },
  {
    "question_id": 66979328,
    "title": "PyTorch ValueError: Target size (torch.Size([64])) must be the same as input size (torch.Size([15]))",
    "body": "<p>I'm currently using <a href=\"https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\" rel=\"noreferrer\">this repo</a> to perform NLP and learn more about CNN's using my own dataset, and I keep running into an error regarding a shape mismatch:</p>\n<pre><code>ValueError: Target size (torch.Size([64])) must be the same as input size (torch.Size([15]))\n\n     10 }\n     11 for epoch in tqdm(range(params['epochs'])):\n---&gt; 12     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n     13     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n     14     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n     57         print(&quot;PredictionShapeAfter:&quot;)\n     58         print(predictions.shape)\n---&gt; 59         loss = criterion(predictions, batch.l)\n     60 \n     61         acc = binary_accuracy(predictions, batch.l)\n</code></pre>\n<p>Doing some digging, I found that my CNN's prediction is a different size compared to the training data truth it's being compared to:</p>\n<pre><code>  Input Shape:\n    torch.Size([15, 64])\n    Truth Shape:\n    torch.Size([64])\n    embedded unsqueezed: torch.Size([15, 1, 64, 100])\n    cat shape: torch.Size([15, 300])\n    Prediction Shape Before Squeeze:\n    torch.Size([15, 1])\n    PredictionShapeAfter:\n    torch.Size([15])\n</code></pre>\n<p>The model is making the prediction shape (the last value in this list) as the first dimension of the inputs. Is this a common problem and is there a way to rectify this issue?</p>\n<p>My Model:</p>\n<pre><code>class CNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n                 dropout, pad_idx):\n        super().__init__()\n                \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        \n        self.convs = nn.ModuleList([\n                                    nn.Conv2d(in_channels = 1, \n                                              out_channels = n_filters, \n                                              kernel_size = (fs, embedding_dim)) \n                                    for fs in filter_sizes\n                                    ])\n        \n        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text): \n        embedded = self.embedding(text)\n        embedded = embedded.unsqueeze(1)\n        print(f&quot;embedded unsqueezed: {embedded.shape}&quot;)\n        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]          \n        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n        cat = self.dropout(torch.cat(pooled, dim = 1))   \n        print(f&quot;cat shape: {cat.shape}&quot;)       \n        return self.fc(cat)\n</code></pre>\n<p>My Training function:</p>\n<pre><code>def train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()\n\n        print(&quot;InputShape:&quot;)\n        print(batch.t.shape)\n        print(&quot;Truth Shape:&quot;)\n        print(batch.l.shape)\n\n        predictions = model(batch.t)\n        print(&quot;Prediction Shape Before Squeeze:&quot;)\n        print(predictions.shape)\n\n        predictions = predictions.squeeze(1)\n        print(&quot;PredictionShapeAfter:&quot;)\n        print(predictions.shape)\n        loss = criterion(predictions, batch.l)\n        \n        acc = binary_accuracy(predictions, batch.l)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n</code></pre>\n<p>My full code can be found at <a href=\"https://colab.research.google.com/drive/1aY7XlgUClEe5Ldeu8DjSsxuj9sXTtRw4?usp=sharing\" rel=\"noreferrer\">this link.</a></p>\n",
    "score": 5,
    "creation_date": 1617767398,
    "view_count": 1724,
    "answer_count": 1,
    "tags": "python;nlp;pytorch;conv-neural-network"
  },
  {
    "question_id": 66473771,
    "title": "Wordcloud for only emojis",
    "body": "<p>I have a text full of emojis.</p>\n<pre><code>import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nwordcloud = WordCloud().generate(text)\nplt.imshow(wordcloud)\nplt.axis(&quot;off&quot;)\nplt.show()\n</code></pre>\n<p>when i try to get a wordcloud for those emojis like the code above, it returns an error like this:</p>\n<pre><code>ValueError: We need at least 1 word to plot a word cloud, got 0.\n</code></pre>\n<p>I think the wordcloud library is unable to read emojis.\nI want to get an output like this(with emojis of course):</p>\n<p><a href=\"https://i.sstatic.net/1egcj.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/1egcj.jpg\" alt=\"wordcloud\" /></a></p>\n<p>Anyone knows how to solve this problem?</p>\n",
    "score": 5,
    "creation_date": 1614855994,
    "view_count": 2273,
    "answer_count": 1,
    "tags": "python;matplotlib;nlp;word-cloud"
  },
  {
    "question_id": 65151225,
    "title": "Error using class_weights parameter with Keras in Multi-Class Classification problem",
    "body": "<p>This problem has been asked in other forums and I have tried their variations with no avail: <a href=\"https://stackoverflow.com/questions/49806437/class-weight-for-imbalanced-data-keras\">class_weight for imbalanced data - Keras</a></p>\n<p><a href=\"https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\">how to set class-weights for imbalanced classes in keras</a></p>\n<p>However it seems stale as no one has answered the question. Does anyone know how to implement the <code>class_weight</code> parameter in Keras when using <code>categorical_crossentropy</code>?\nI have been trying to use the <code>class_weight</code> parameter in Keras but keep getting this error:</p>\n<blockquote>\n<p>ValueError: Expected <code>class_weight</code> to be a dict with keys from 0 to one less than the number of classes, found {'prediction': {0: 1.217169570760731, 1: 5.323420074349443, 2: 0.5023680056130504}</p>\n</blockquote>\n<p>Each sample will be classified either 0, 1, or 2 (softmax). The bias in this dataset is significant. My model using the Keras functional API.</p>\n<p>The class_weights were calculated using Sklearn:</p>\n<pre><code>class_weights = class_weight.compute_class_weight('balanced', np.unique(np.array(y_trn_labels_HB_2_pd['labels'])), y_trn_labels_HB_2_pd['labels'])\nclass_weight_dict = dict(enumerate(class_weights))\nclass_weight_dict\n</code></pre>\n<p>Here is my last layer:</p>\n<pre><code>prediction = Dense(3, activation=&quot;softmax&quot;, name = 'prediction')(x)\n</code></pre>\n<p>Here is my model:</p>\n<pre><code>tf.__version__ = 2.3.0\n\nmodel = Model(inputs = [sequence_input_head, sequence_input_body, semantic_feat,\n             wordOL_feat, avg_subj_feat], outputs = [prediction])\nmodel.compile(loss = 'categorical_crossentropy',\n             optimizer='adam',\n             metrics = ['accuracy'])\nmodel.summary()\n</code></pre>\n<p>Here is my class_weight parameter:</p>\n<pre><code>class_weight= {'prediction': {0:1.217169570760731, 1:5.323420074349443, 2:0.5023680056130504} })\n</code></pre>\n<p>Here is the full error:</p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-271-e3bb78b84171&gt; in &lt;module&gt;()\n     26                                        y_val_2_cat),\n     27                     callbacks = [es],\n---&gt; 28                     class_weight= {'prediction': class_weights})\n     29 \n     30 modeled = model.save(os.path.join(save_path, path_model))\n\n3 frames\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\n    106   def _method_wrapper(self, *args, **kwargs):\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\n--&gt; 108       return method(self, *args, **kwargs)\n    109 \n    110     # Running inside `run_distribute_coordinator` already.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n   1061           use_multiprocessing=use_multiprocessing,\n   1062           model=self,\n-&gt; 1063           steps_per_execution=self._steps_per_execution)\n   1064 \n   1065       # Container that configures and calls `tf.keras.Callback`s.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\n   1120     dataset = self._adapter.get_dataset()\n   1121     if class_weight:\n-&gt; 1122       dataset = dataset.map(_make_class_weight_map_fn(class_weight))\n   1123     self._inferred_steps = self._infer_steps(steps_per_epoch, dataset)\n   1124 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _make_class_weight_map_fn(class_weight)\n   1299         &quot;Expected `class_weight` to be a dict with keys from 0 to one less &quot;\n   1300         &quot;than the number of classes, found {}&quot;).format(class_weight)\n-&gt; 1301     raise ValueError(error_msg)\n   1302 \n   1303   class_weight_tensor = ops.convert_to_tensor_v2(\n\nValueError: Expected `class_weight` to be a dict with keys from 0 to one less than the number of classes, found {'prediction': {0: 1.217169570760731, 1: 5.323420074349443}}\n</code></pre>\n<p>EDIT1:</p>\n<p>I tried your suggestion @Prateek Bhatt</p>\n<pre><code>history = model.fit({'headline': hl_pd_tr, 'articleBody':bd_pd_train, 'semantic': semantic_sim_180_train_x, 'wordOverlap': wrd_OvLp_train_x, 'avg_subjectivity': avg_subj_hb_train_x}, #@param [&quot;model.fit({'headline': hl_pd_tr, 'articleBody':bd_pd_train},&quot;, &quot;model.fit({'headline': hl_pd_tr, 'articleBody':bd_pd_train, 'semantic': semantic_x_tr},&quot;, &quot;model.fit({'headline': hl_pd_tr, 'articleBody':bd_pd_train, 'semantic': semantic_x_tr, 'wordOverlap': wrd_OvLp_x_tr},&quot;, &quot;model.fit({'headline': hl_pd_tr, 'articleBody':bd_pd_train, 'semantic': semantic_x_tr, 'wordOverlap': wrd_OvLp_x_tr, 'avgsubj': avg_subj_x_tr},&quot;] {type:&quot;raw&quot;, allow-input: true}\n                    {'prediction':y_train_2_cat},\n                    epochs=100,\n                    batch_size= BATCH__SIZE,\n                    shuffle= True,\n                    validation_data = ([hl_pd_val, bd_pd_val, semantic_sim_180_val_x, wrd_OvLp_val_x, avg_subj_hb_val_x], y_val_2_cat),\n                    callbacks = [es],\n                    class_weight= {0:1.217169570760731, 1:5.323420074349443, 2:0.5023680056130504})\n</code></pre>\n<p>However, I get this error:</p>\n<pre><code>ValueError: `class_weight` is only supported for Models with a single output.\n</code></pre>\n<p>Full error:</p>\n<pre><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-272-bfbab936a723&gt; in &lt;module&gt;()\n     26                                        y_val_2_cat),\n     27                     callbacks = [es],\n---&gt; 28                     class_weight= {0:1.217169570760731, 1:5.323420074349443, 2:0.5023680056130504})\n     29 \n     30 modeled = model.save(os.path.join(save_path, path_model))\n\n16 frames\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\n    106   def _method_wrapper(self, *args, **kwargs):\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\n--&gt; 108       return method(self, *args, **kwargs)\n    109 \n    110     # Running inside `run_distribute_coordinator` already.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n   1061           use_multiprocessing=use_multiprocessing,\n   1062           model=self,\n-&gt; 1063           steps_per_execution=self._steps_per_execution)\n   1064 \n   1065       # Container that configures and calls `tf.keras.Callback`s.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\n   1120     dataset = self._adapter.get_dataset()\n   1121     if class_weight:\n-&gt; 1122       dataset = dataset.map(_make_class_weight_map_fn(class_weight))\n   1123     self._inferred_steps = self._infer_steps(steps_per_epoch, dataset)\n   1124 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic)\n   1693     &quot;&quot;&quot;\n   1694     if num_parallel_calls is None:\n-&gt; 1695       return MapDataset(self, map_func, preserve_cardinality=True)\n   1696     else:\n   1697       return ParallelMapDataset(\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\n   4043         self._transformation_name(),\n   4044         dataset=input_dataset,\n-&gt; 4045         use_legacy_function=use_legacy_function)\n   4046     variant_tensor = gen_dataset_ops.map_dataset(\n   4047         input_dataset._variant_tensor,  # pylint: disable=protected-access\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\n   3369       with tracking.resource_tracker_scope(resource_tracker):\n   3370         # TODO(b/141462134): Switch to using garbage collection.\n-&gt; 3371         self._function = wrapper_fn.get_concrete_function()\n   3372         if add_to_graph:\n   3373           self._function.add_to_graph(ops.get_default_graph())\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)\n   2937     &quot;&quot;&quot;\n   2938     graph_function = self._get_concrete_function_garbage_collected(\n-&gt; 2939         *args, **kwargs)\n   2940     graph_function._garbage_collector.release()  # pylint: disable=protected-access\n   2941     return graph_function\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\n   2904       args, kwargs = None, None\n   2905     with self._lock:\n-&gt; 2906       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n   2907       seen_names = set()\n   2908       captured = object_identity.ObjectIdentitySet(\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\n   3211 \n   3212       self._function_cache.missed.add(call_context_key)\n-&gt; 3213       graph_function = self._create_graph_function(args, kwargs)\n   3214       self._function_cache.primary[cache_key] = graph_function\n   3215       return graph_function, args, kwargs\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n   3073             arg_names=arg_names,\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\n-&gt; 3075             capture_by_value=self._capture_by_value),\n   3076         self._function_attributes,\n   3077         function_spec=self.function_spec,\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n    984         _, original_func = tf_decorator.unwrap(python_func)\n    985 \n--&gt; 986       func_outputs = python_func(*func_args, **func_kwargs)\n    987 \n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_fn(*args)\n   3362           attributes=defun_kwargs)\n   3363       def wrapper_fn(*args):  # pylint: disable=missing-docstring\n-&gt; 3364         ret = _wrapper_helper(*args)\n   3365         ret = structure.to_tensor_list(self._output_structure, ret)\n   3366         return [ops.convert_to_tensor(t) for t in ret]\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in _wrapper_helper(*args)\n   3297         nested_args = (nested_args,)\n   3298 \n-&gt; 3299       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\n   3300       # If `func` returns a list of tensors, `nest.flatten()` and\n   3301       # `ops.convert_to_tensor()` would conspire to attempt to stack\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\n    253       try:\n    254         with conversion_ctx:\n--&gt; 255           return converted_call(f, args, kwargs, options=options)\n    256       except Exception as e:  # pylint:disable=broad-except\n    257         if hasattr(e, 'ag_error_metadata'):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)\n    530 \n    531   if not options.user_requested and conversion.is_whitelisted(f):\n--&gt; 532     return _call_unconverted(f, args, kwargs, options)\n    533 \n    534   # internal_convert_user_code is for example turned off when issuing a dynamic\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)\n    337 \n    338   if kwargs is not None:\n--&gt; 339     return f(*args, **kwargs)\n    340   return f(*args)\n    341 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _class_weights_map_fn(*data)\n   1310     if nest.is_sequence(y):\n   1311       raise ValueError(\n-&gt; 1312           &quot;`class_weight` is only supported for Models with a single output.&quot;)\n   1313 \n   1314     if y.shape.rank &gt; 2:\n\nValueError: `class_weight` is only supported for Models with a single output.\n</code></pre>\n",
    "score": 5,
    "creation_date": 1607118647,
    "view_count": 8606,
    "answer_count": 4,
    "tags": "python;tensorflow;keras;nlp"
  },
  {
    "question_id": 64190127,
    "title": "Changing a Noun to its Pronoun in a sentence",
    "body": "<p>I want to replace a Noun in a sentence with its pronoun. I will be using this to create a dataset for a NLP task. for example if my sentences are --&gt;</p>\n<blockquote>\n<p>&quot;Jack and Ryan are friends. <em><strong>Jack</strong></em> is also friends with Michelle.&quot;</p>\n</blockquote>\n<p>Then I want to replace the second Jack(in italics and bold ) with &quot;He&quot;.\nI have done the POS tagging to find the Nouns in my sentences. But I do not know how to proceed from here.\nIf I have a list of all possible pronouns that can be used, Is there a corpus or system that can tell me the most appropriate pronoun for the word?</p>\n",
    "score": 5,
    "creation_date": 1601770873,
    "view_count": 793,
    "answer_count": 1,
    "tags": "python;python-3.x;nlp;stanford-nlp;coreference-resolution"
  },
  {
    "question_id": 56451249,
    "title": "Computing Skip-gram Frequency with countVectorizer",
    "body": "<p>I'm trying to compute the most frequent skip-grams in a text file.\nI'm using nltk's skipgram and scikit-learn's countVectorizer, but it gives me a list of <em>distinct</em> skip-grams. Hence, when I put them in a dictionary to count them, I get frequency = 1 for every skip-gram.</p>\n\n<p>I believe this is because I'm using the vectorizer.vocabulary_ method, which skips the repeating skip-grams.</p>\n\n<p>I'm using this code <a href=\"https://github.com/nltk/nltk/issues/1428#issuecomment-231647710\" rel=\"noreferrer\">https://github.com/nltk/nltk/issues/1428#issuecomment-231647710</a></p>\n\n<p>In this original code they weren't trying to compute the frequency, so distinct skip-grams (vocabulary) was fine. In my case, how can I change the code so that I get the comprehensive list of all the skip-grams generated by countVectorizer?</p>\n\n<pre><code>import functools\nfrom nltk.util import skipgrams\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntext = [word_tokenize(line.strip()) for line in open('test.txt', 'r')]\nskipper = functools.partial(skipgrams, n=2, k=2)\nvectorizer = CountVectorizer(analyzer=skipper)\nvectorizer.fit(text)\nvectorizer.vocabulary_\n\ndict = {}\ndict = vectorizer.vocabulary_\n\ndef getList(dict): \n    return dict.keys() #get all the skip-grams\n\n#store all skip-grams in a list to count their frequencies\nnewlist = []\nfor key in getList(dict):\n  newlist.append(key) \n\n#count frequency of items in list\ndef count(listOfTuple):       \n    count_map = {} \n    for i in listOfTuple: \n        count_map[i] = count_map.get(i, 0) +1\n    return count_map \n\nd = count(newlist)\nprint(d)\n</code></pre>\n\n<p>For example, if I have a text consisting of two strings \"i love apple\" and \"i love watermelon\"\nprint(d) should give:</p>\n\n<pre><code>('i', 'love'):2\n('i', 'apple'):1\n('i', 'watermelon'):1\n</code></pre>\n\n<p>However right now I am getting 1 everywhere.</p>\n\n<p>Any help would be greatly appreciated!!</p>\n",
    "score": 5,
    "creation_date": 1559681802,
    "view_count": 745,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp;nltk;n-gram"
  },
  {
    "question_id": 56370298,
    "title": "How do I write a Tensorflow custom op containing a persistent C++ object?",
    "body": "<p>I'm developing a Tensorflow sequence model that uses a beam search through an OpenFST decoding graph (loaded from a binary file) over the logits output from a Tensorflow sequence model.</p>\n\n<p>I've written a custom op that allows me to perform decoding over the logits, but each time, I'm having the op call fst::Read(BINARY_FILE) before performing the decoding. This might be fine as long as it stays small but I'd like to avoid the I/O overhead.</p>\n\n<p>I've read through the Tensorflow custom op and tried to find similar examples but I'm still lost. Basically, what I want to do in the graph is:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>FstDecodingOp.Initialize('BINARY_FILE.bin') #loads the BINARY_FILE.bin into memory\n...\nfor o in output:\n    FstDecodingOp.decode(o) # uses BINARY_FILE.bin to decode\n\n</code></pre>\n\n<p>This would of course be straightforward in Python outside of the tensorflow graph, but I need to eventually move this into a vanilla TF-Serving environment, so it needs to get frozen into an export graph. Has anyone encountered a similar problem before? </p>\n\n<p><strong>Solution:</strong></p>\n\n<p>Didn't realize that you could set private attributes using the \"OpKernel(context)\". Just initialized it using that function.</p>\n\n<p><strong>Edit: more detail on how I did it. Have yet to try serving.</strong></p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>REGISTER_OP(\"FstDecoder\")\n    .Input(\"log_likelihoods: float\")\n    .Attr(\"fst_decoder_path: string\")\n    ....\n\n...\n\ntemplate &lt;typename Device, typename T&gt;\nclass FstDecoderOp : public OpKernel {\n\nprivate:\n   fst::Fst&lt;fst::StdArc&gt;* fst_;\n   float beam_;\n\npublic:\n  explicit FstDecoderOp(OpKernelConstruction* context) : OpKernel(context) {\n    OP_REQUIRES_OK(context, context-&gt;GetAttr(\"beam\", &amp;beam_));\n\n    std::string fst_path;\n    OP_REQUIRES_OK(context, context-&gt;GetAttr(\"fst_decoder_path\", &amp;fst_path));\n\n    fst_ = fst::Fst&lt;fst::StdArc&gt;::Read(fst_path);\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // do some compute \n    const Tensor* log_likelihoods;\n\n    OP_REQUIRES_OK(context, context-&gt;input(\"log_likelihoods\", \n     &amp;log_likelihoods));\n\n    // simplified \n    compute_op(_fst, log_likelihoods);\n\n  }\n};\n</code></pre>\n\n<p>In python:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>\nsess = tf.Session()\nmat = tf.placeholder(tf.float32, shape=test_npy.shape)\nres_ = decoder_op.fst_decoder(beam=30, fst_decoder_path=\"decoder_path.fst\", log_likelihoods=mat)\nres = sess.run(res_, {mat : test_npy} )\n\n</code></pre>\n",
    "score": 5,
    "creation_date": 1559177613,
    "view_count": 1366,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;tensorflow-serving;openfst"
  },
  {
    "question_id": 54966341,
    "title": "Extract relationship concepts from sentences",
    "body": "<p>Is there a current model or how could I train a model that takes a sentence involving two subjects like: </p>\n\n<blockquote>\n  <p>[Meiosis] is a type of [cell division]...</p>\n</blockquote>\n\n<p>and decides if one is the child or parent concept of the other? In this case, cell division is the parent of meiosis.</p>\n",
    "score": 5,
    "creation_date": 1551596031,
    "view_count": 305,
    "answer_count": 1,
    "tags": "nlp;word2vec;word-embedding;information-extraction;relation-extraction"
  },
  {
    "question_id": 53327804,
    "title": "Any efficient way to find surrounding ADJ respect to target phrase in python?",
    "body": "<p>I am doing sentiment analysis on given documents, my goal is I want to find out the closest or surrounding adjective words respect to target phrase in my sentences. I do have an idea how to extract surrounding words respect to target phrases, but How do I find out relatively close or closest adjective or <code>NNP</code> or <code>VBN</code> or other POS tag respect to target phrase.</p>\n\n<p>Here is the sketch idea of how I may get surrounding words to respect to my target phrase.</p>\n\n<pre><code>sentence_List= {\"Obviously one of the most important features of any computer is the human interface.\", \"Good for everyday computing and web browsing.\",\n\"My problem was with DELL Customer Service\", \"I play a lot of casual games online[comma] and the touchpad is very responsive\"}\n\ntarget_phraseList={\"human interface\",\"everyday computing\",\"DELL Customer Service\",\"touchpad\"}\n</code></pre>\n\n<p>Note that my original dataset was given as dataframe where the list of the sentence and respective target phrases were given. Here I just simulated data as follows:</p>\n\n<pre><code>import pandas as pd\ndf=pd.Series(sentence_List, target_phraseList)\ndf=pd.DataFrame(df)\n</code></pre>\n\n<p>Here I tokenize the sentence as follow:</p>\n\n<pre><code>from nltk.tokenize import word_tokenize\ntokenized_sents = [word_tokenize(i) for i in sentence_List]\ntokenized=[i for i in tokenized_sents]\n</code></pre>\n\n<p>then I try to find out surrounding words respect to my target phrases by using this <a href=\"https://stackoverflow.com/questions/17645701/extract-words-surrounding-a-search-word\">loot at here</a>. However, I want to find out relatively closer or closet <code>adjective</code>, or <code>verbs</code> or <code>VBN</code> respect to my target phrase. How can I make this happen? Any idea to get this done? Thanks</p>\n",
    "score": 5,
    "creation_date": 1542315481,
    "view_count": 681,
    "answer_count": 1,
    "tags": "python;parsing;nlp;sentiment-analysis"
  },
  {
    "question_id": 52834152,
    "title": "Use the google transliterate api in python",
    "body": "<p>I am trying to use google transliterate<a href=\"https://developers.google.com/transliterate/\" rel=\"noreferrer\"> [1] </a>  to convert hindi words written in english to hindi. e.g-<br>\nInput text- Main sahi hun.<br>\nRequired text -मैं सही हूँ  </p>\n\n<p>I want to pass the input string to api and wants a required text in hindi language.\nI am using google transliterate but as it was deprecated long time ago so can't find a suitable way to do it on python as currently the example they are providing is in javascript and not very beginner friendly. \nHow to do it?</p>\n",
    "score": 5,
    "creation_date": 1539688354,
    "view_count": 1438,
    "answer_count": 2,
    "tags": "python-3.x;google-api;nlp;machine-translation"
  },
  {
    "question_id": 52686366,
    "title": "Can&#39;t get attribute &#39;Word2VecKeyedVectors&#39; on &lt;module &#39;gensim.models.keyedvectors&#39; &gt;",
    "body": "<p>I train and save a gensim word2vec model:</p>\n\n<pre><code>W2V_MODEL_FN = r\"C:\\Users\\models\\w2v.model\"\n\nmodel = Word2Vec(X, size=150, window=3, min_count=2, workers=10)\nmodel.train(X, total_examples=len(X), epochs=50)\nmodel.save(W2V_MODEL_FN)\n</code></pre>\n\n<p>And then:</p>\n\n<pre><code>w2v_model = Word2Vec.load(W2V_MODEL_FN)\n</code></pre>\n\n<p>On one enviroment it works perfectly but in another I get the error:</p>\n\n<blockquote>\n  <p>{AttributeError}Can't get attribute 'Word2VecKeyedVectors' on  module\n  'gensim.models.keyedvectors' from\n  'C:\\Users\\Anaconda3_New\\envs\\ISP_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py'></p>\n</blockquote>\n\n<p>So I guess it might be a package version issue?</p>\n\n<p>But I couldn't figure what it is.\nAny ideas?</p>\n\n<p>Thanks!</p>\n",
    "score": 5,
    "creation_date": 1538897320,
    "view_count": 6226,
    "answer_count": 2,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 52473653,
    "title": "Better named-entity recognition and similarity using spaCy",
    "body": "<p>I've been trying out spaCy for a small side-project, and had a few questions &amp; concerns.</p>\n\n<p>I noticed that spaCy's named-entity recognition results (with its largest <code>en_vectors_web_lg</code> model) don't seem to be as accurate as that of Google Cloud Natural Language API [1]. Google's API is able to extract more entities, more accurately, most likely because their model is even larger. So, is there a way to improve spaCy's NER results using a different model if possible, or through some other technique?</p>\n\n<p>Secondly, Google's API also returns Wikipedia article links for relevant entities. Is this possible with spaCy too, or using some other technique on top of spaCy's NER results?</p>\n\n<p>Thirdly, I noticed that spaCy has a <code>similarity()</code> method [2] that uses GloVe word vectors. But being new to it, I'm not sure what's the best way to frequently perform similarity comparison between each document in a set of documents (say 5000-10000 text documents of under 500 characters each) to generate buckets of similar documents?</p>\n\n<p>Hoping for someone to have any suggestions or tips.</p>\n\n<p>Many thanks!</p>\n\n<hr>\n\n<p>[1] <a href=\"https://cloud.google.com/natural-language/\" rel=\"noreferrer\">https://cloud.google.com/natural-language/</a></p>\n\n<p>[2] <a href=\"https://spacy.io/usage/vectors-similarity\" rel=\"noreferrer\">https://spacy.io/usage/vectors-similarity</a></p>\n",
    "score": 5,
    "creation_date": 1537770115,
    "view_count": 2048,
    "answer_count": 1,
    "tags": "python;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 49804717,
    "title": "How to generate GloVe embeddings for POS tags? Python",
    "body": "<p>For a sentence analysis task, I would like to take the sequence of POS tags associated with the sentence and feed it to my model as if the POS tags are words. </p>\n\n<p>I am using GloVe to make representations of each word in the sentence and SpaCy to generate POS tags. However, GloVe embeddings do not make much sense for POS tags. So I will have to somehow create embeddings for each POS tag. What is the best way to do create embeddings for POS tags, so that I can feed POS sequences into my model in the same way I would feed sentences? Could anyone point to code examples of how to do this with GloVe in Python?</p>\n\n<p><strong>Added context</strong></p>\n\n<p>My task is a binary classification of sentence pairs, based on their resemblance (similar meaning vs different meaning). </p>\n\n<p>I would like to use POS tags as words, so that the POS tags serve as an additional bit of information to compare the sentences. My current model does not use an LSTM as a way to predict sequences. </p>\n",
    "score": 5,
    "creation_date": 1523562288,
    "view_count": 2921,
    "answer_count": 1,
    "tags": "python-3.x;machine-learning;nlp;spacy;word-embedding"
  },
  {
    "question_id": 49017069,
    "title": "When the stop word removal process is executed in sklearn TfidfVectorizer?",
    "body": "<p>If I pass a list of custom stopwords to <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"nofollow noreferrer\"><code>TfidfVectorizer</code></a>, when will the stopwords be removed exactly? According to <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"nofollow noreferrer\">the documentation</a>:</p>\n\n<blockquote>\n  <p><strong>stop_words</strong> : <code>string</code> {‘english’}, <code>list</code>, or <code>None</code> (default)</p>\n  \n  <p>...</p>\n  \n  <p>If a list, that list is assumed to contain stop words, all of which\n  will be removed from the resulting tokens. Only applies if <code>analyzer == 'word'</code>.</p>\n</blockquote>\n\n<p>so it seems that the process happens <em>after</em> the tokenization, am I right? The doubt arises because if the tokenization also involves stemming, I think there is the risk to erroneously skip (not remove) a stopword because, after stemming, it is not recognized anymore.</p>\n",
    "score": 5,
    "creation_date": 1519761917,
    "view_count": 2778,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp;stop-words;tfidfvectorizer"
  },
  {
    "question_id": 48913352,
    "title": "How to split sentences into correlated words (term extraction)?",
    "body": "<p>Is there any NLP python library that split sentence or joins words into related pairs of words? For example:</p>\n\n<blockquote>\n  <p>That is not bad example -> \"That\" \"is\" \"not bad\" \"example\"</p>\n</blockquote>\n\n<p>\"Not bad\" means the same as good so it would be useless to process it as \"not\" and \"bad\" in machine learning.\nI dont even know how to call these pairs of words that are correlated. (term extraction? phases extraction?)\nOr would be even better to split into adjectives with nouns for example:</p>\n\n<blockquote>\n  <p>dishonest media relating about tax cuts -> \"dishonest media\", \"relating\", \"about\", \"tax cuts\"</p>\n</blockquote>\n\n<p>I found topia.termextract but it does not work with python3.</p>\n",
    "score": 5,
    "creation_date": 1519239070,
    "view_count": 1250,
    "answer_count": 3,
    "tags": "python;nlp;nltk;sentiment-analysis;text-extraction"
  },
  {
    "question_id": 48233179,
    "title": "Convert adjective to adverb",
    "body": "<p>Does anyone know <strong>how to convert an english adjective to its respective adverb</strong>?  Python would be ideal, but really any programmatic approach would be great.</p>\n\n<p>I've tried <a href=\"https://www.clips.uantwerpen.be/pages/pattern-en\" rel=\"noreferrer\">pattern.en</a>, <a href=\"http://www.nltk.org/howto/wordnet.html\" rel=\"noreferrer\">nltk wordnet</a>, and <a href=\"http://spacy.io/\" rel=\"noreferrer\">spacy</a> to no avail.</p>\n\n<p>Converting adverbs to their root adjective form is no problem.  I'm using the SO solution <a href=\"https://stackoverflow.com/questions/17245123/getting-adjective-from-an-adverb-in-nltk-or-other-nlp-library\">here</a>.</p>\n\n<p>What I want is to go the other way.  From adjective to adverb.</p>\n\n<p><a href=\"https://stackoverflow.com/a/48218093/8870055\">Here is nltk wordnet code</a> that kind of converts words between different word forms, but fails for adjective &lt;--> adverb conversions. </p>\n\n<p>Specifically, I'd like a function <code>getAdverb</code> like this:</p>\n\n<pre><code>getAdverb('quick')\n&gt;&gt;&gt; quickly\ngetAdverb('noteable')\n&gt;&gt;&gt; notably\ngetAdverb('happy')\n&gt;&gt;&gt; happily\n</code></pre>\n\n<p>Any code, resources, or suggestions would be greatly appreciated!</p>\n",
    "score": 5,
    "creation_date": 1515786918,
    "view_count": 1255,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet;spacy"
  },
  {
    "question_id": 46188027,
    "title": "Ensemble of CNN and RNN model in keras",
    "body": "<p>trying to implement the model from paper <a href=\"http://sentic.net/convolutional-and-recurrent-neural-networks-for-text-categorization.pdf\" rel=\"nofollow noreferrer\">Ensemble Application of Convolutional and Recurrent Neural Networks for Multi-label Text Categorization</a> in keras</p>\n\n<p>The model looks like the following (taken from the paper)\n<a href=\"https://i.sstatic.net/cuqUL.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/cuqUL.png\" alt=\"enter image description here\"></a></p>\n\n<p>I have the code as</p>\n\n<pre><code>document_input = Input(shape=(None,), dtype='int32')\nembedding_layer = Embedding(vocab_size, WORD_EMB_SIZE, weights=[initial_embeddings], \n                                input_length=DOC_SEQ_LEN, trainable=True)\nconvs = []\nfilter_sizes = [2,3,4,5]\n\ndoc_embedding = embedding_layer(document_input)\nfor filter_size in filter_sizes:\n    l_conv = Conv1D(filters=256, kernel_size=filter_size, padding='same', activation='relu')(doc_embedding)\n    l_pool = MaxPooling1D(filter_size)(l_conv)\n    convs.append(l_pool)\n\nl_merge = Concatenate(axis=1)(convs)\nl_flat = Flatten()(l_merge)\nl_dense = Dense(100, activation='relu')(l_flat)\nl_dense_3d = Reshape((1,int(l_dense.shape[1])))(l_dense)\n\ngene_variation_input = Input(shape=(None,), dtype='int32')\ngene_variation_embedding = embedding_layer(gene_variation_input)\nrnn_layer = LSTM(100, return_sequences=False, stateful=True)(gene_variation_embedding,initial_state=[l_dense_3d])\n\nl_flat = Flatten()(rnn_layer)\noutput_layer = Dense(9, activation='softmax')(l_flat)\nmodel = Model(inputs=[document_input,gene_variation_input], outputs=[output_layer])\n</code></pre>\n\n<p>I dont know whether I am setting up the <strong>Text feature vector</strong> right in the above diagram right ! I tried and I get the error as</p>\n\n<pre><code>ValueError: Layer lstm_9 expects 3 inputs, but it received 2 input tensors. Input received: [&lt;tf.Tensor 'embedding_10_1/Gather:0' shape=(?, ?, 200) dtype=float32&gt;, &lt;tf.Tensor 'reshape_9/Reshape:0' shape=(?, 1, 100) dtype=float32&gt;]\n</code></pre>\n\n<p>I did follow the section on <strong>Note on specifying the initial state of RNNs</strong> in <a href=\"https://keras.io/layers/recurrent/#recurrent\" rel=\"nofollow noreferrer\">keras documentation</a> and <a href=\"https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L62\" rel=\"nofollow noreferrer\">code</a></p>\n\n<p>Any help appreciated.</p>\n\n<p><strong>update:</strong>\nThe suggestion and some more reading into the code the model looks like this</p>\n\n<pre><code>embedding_layer = Embedding(vocab_size, WORD_EMB_SIZE, weights=[initial_embeddings], trainable=True)\n\ndocument_input = Input(shape=(DOC_SEQ_LEN,), batch_shape=(BATCH_SIZE, DOC_SEQ_LEN),dtype='int32')\ndoc_embedding = embedding_layer(document_input)\n\nconvs = []\nfilter_sizes = [2,3,4,5]\n\nfor filter_size in filter_sizes:\n    l_conv = Conv1D(filters=256, kernel_size=filter_size, padding='same', activation='relu')(doc_embedding)\n    l_pool = MaxPooling1D(filter_size)(l_conv)\n    convs.append(l_pool)\n\nl_merge = Concatenate(axis=1)(convs)\nl_flat = Flatten()(l_merge)\nl_dense = Dense(100, activation='relu')(l_flat)\n\ngene_variation_input = Input(shape=(GENE_VARIATION_SEQ_LEN,), batch_shape=(BATCH_SIZE, GENE_VARIATION_SEQ_LEN),dtype='int32')\ngene_variation_embedding = embedding_layer(gene_variation_input)\n\nrnn_layer = LSTM(100, return_sequences=False, \n                 batch_input_shape=(BATCH_SIZE, GENE_VARIATION_SEQ_LEN, WORD_EMB_SIZE),\n                 stateful=False)(gene_variation_embedding, initial_state=[l_dense, l_dense])\n\noutput_layer = Dense(9, activation='softmax')(rnn_layer)\n\nmodel = Model(inputs=[document_input,gene_variation_input], outputs=[output_layer])\n</code></pre>\n\n<p>model summary</p>\n\n<pre><code>____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ninput_8 (InputLayer)             (32, 9)               0                                            \n____________________________________________________________________________________________________\ninput_7 (InputLayer)             (32, 4000)            0                                            \n____________________________________________________________________________________________________\nembedding_6 (Embedding)          multiple              73764400    input_7[0][0]                    \n                                                                   input_8[0][0]                    \n____________________________________________________________________________________________________\nconv1d_13 (Conv1D)               (32, 4000, 256)       102656      embedding_6[0][0]                \n____________________________________________________________________________________________________\nconv1d_14 (Conv1D)               (32, 4000, 256)       153856      embedding_6[0][0]                \n____________________________________________________________________________________________________\nconv1d_15 (Conv1D)               (32, 4000, 256)       205056      embedding_6[0][0]                \n____________________________________________________________________________________________________\nconv1d_16 (Conv1D)               (32, 4000, 256)       256256      embedding_6[0][0]                \n____________________________________________________________________________________________________\nmax_pooling1d_13 (MaxPooling1D)  (32, 2000, 256)       0           conv1d_13[0][0]                  \n____________________________________________________________________________________________________\nmax_pooling1d_14 (MaxPooling1D)  (32, 1333, 256)       0           conv1d_14[0][0]                  \n____________________________________________________________________________________________________\nmax_pooling1d_15 (MaxPooling1D)  (32, 1000, 256)       0           conv1d_15[0][0]                  \n____________________________________________________________________________________________________\nmax_pooling1d_16 (MaxPooling1D)  (32, 800, 256)        0           conv1d_16[0][0]                  \n____________________________________________________________________________________________________\nconcatenate_4 (Concatenate)      (32, 5133, 256)       0           max_pooling1d_13[0][0]           \n                                                                   max_pooling1d_14[0][0]           \n                                                                   max_pooling1d_15[0][0]           \n                                                                   max_pooling1d_16[0][0]           \n____________________________________________________________________________________________________\nflatten_4 (Flatten)              (32, 1314048)         0           concatenate_4[0][0]              \n____________________________________________________________________________________________________\ndense_6 (Dense)                  (32, 100)             131404900   flatten_4[0][0]                  \n____________________________________________________________________________________________________\nlstm_4 (LSTM)                    (32, 100)             120400      embedding_6[1][0]                \n                                                                   dense_6[0][0]                    \n                                                                   dense_6[0][0]                    \n____________________________________________________________________________________________________\ndense_7 (Dense)                  (32, 9)               909         lstm_4[0][0]                     \n====================================================================================================\nTotal params: 206,008,433\nTrainable params: 206,008,433\nNon-trainable params: 0\n____________________________________________________________________________________________________\n</code></pre>\n",
    "score": 5,
    "creation_date": 1505270685,
    "view_count": 4329,
    "answer_count": 1,
    "tags": "nlp;deep-learning;keras;recurrent-neural-network"
  },
  {
    "question_id": 46001910,
    "title": "What&#39;s the best way to compare several corpora in natural language?",
    "body": "<p>I've been doing LDA topic models of narrative reports in natural language for a research project (using Gensim with python). I have several smallish corpora (from 1400 to 200 docs each – I know, that's tiny!) that I'd like to compare, but I don't know how to do that beyond looking at each LDA model (for instance with pyLDAviz). My academic background is not in CS, and I'm still a bit new to NLP.</p>\n\n<p>What are some good ways to compare topics across corpora/topic models? For instance, is it possible to estimate how much two LDA models overlap? Or are there other ways to assess the topic similarity of several corpora?</p>\n\n<p>Thanks in advance for your help!</p>\n",
    "score": 5,
    "creation_date": 1504274490,
    "view_count": 1216,
    "answer_count": 1,
    "tags": "python;nlp;nltk;lda;topic-modeling"
  },
  {
    "question_id": 43500996,
    "title": "Disabling Gensim&#39;s removal of punctuation etc. when parsing a wiki corpus",
    "body": "<p>I want to train a word2vec model on the english wikipedia using python with gensim. I closely followed <a href=\"https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw\" rel=\"noreferrer\">https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw</a> for that.</p>\n\n<p>It works for me but what I don't like about the resulting word2vec model is that named entities are split which makes the model unusable for my specific application. The model I need has to represent named entities as a single vector. </p>\n\n<p>Thats why I planned to parse the wikipedia articles with spacy and merge entities like \"north carolina\" into \"north_carolina\", so that word2vec would represent them as a single vector. So far so good.</p>\n\n<p>The spacy parsing has to be part of the preprocessing, which I originally did as recommended in the linked discussion using:</p>\n\n<pre><code>...\nwiki = WikiCorpus(wiki_bz2_file, dictionary={})\nfor text in wiki.get_texts():\n    article = \" \".join(text) + \"\\n\"\n    output.write(article)\n...\n</code></pre>\n\n<p>This removes punctuation, stop words, numbers and capitalization and saves  each article in a separate line in the resulting output file. The problem is that spacy's NER doesn't really work on this preprocessed text, since I  guess it relies on punctuation and capitalization for NER (?).</p>\n\n<p><strong>Does anyone know if I can \"disable\" gensim's preprocessing so that it doesn't remove punctuation etc. but still parses the wikipedia articles to text directly from the compressed wikipedia dump? Or does someone know a better way to accomplish this? Thanks in advance!</strong></p>\n",
    "score": 5,
    "creation_date": 1492618494,
    "view_count": 1894,
    "answer_count": 2,
    "tags": "python;nlp;gensim;word2vec;spacy"
  },
  {
    "question_id": 42760602,
    "title": "Stanford NER lowercase entities",
    "body": "<p>I am facing problem to detect named entities which starts with lowercase letter. If I train the model with only lowercase words, then the accuracy is reasonable; however, when the model is trained with fully uppercase tokens or even mix of lowercase and uppercase, the result is very bad. I tried some features which presented by the Stanford NLP Group <a href=\"http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/NERFeatureFactory.html\" rel=\"noreferrer\">Class NERFeatureFactory</a> as well as variety of sentences, but I could not get the results that I expected.\nAn example for the problem I am facing is as follow:</p>\n\n<p>\"ali studied at university of michigan and now he works for us navy.\"</p>\n\n<p>I expected the model to recognize entities as follow:</p>\n\n<ul>\n<li>\"university\" : \"FACILITY\",</li>\n<li>\"of michigan\" : \"FACILITY\",</li>\n<li>\"ali\" : \"PERSON\"</li>\n<li>\"us\" : \"ORGANIZATION\"</li>\n<li>\"navy\" : \"ORGANIZATION\"</li>\n</ul>\n\n<p>If the .TSV file, which used as training data, contains ONLY lowercase letters, then I can get the above result otherwise the result is surprising.</p>\n\n<p>Any help is highly appreciated a head. </p>\n",
    "score": 5,
    "creation_date": 1489398439,
    "view_count": 3171,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 41073166,
    "title": "Reply after a given delay",
    "body": "<p>I would like to implement an simple timer within a communication.</p>\n\n<p>My scenario is a small math trainer where you train 5 minutes, after the 5 minutes of normal interaction I would like to inform the user that the time is now up. I don't want to wait until the user has finished his next input/answer (optional just if there is currently no input).</p>\n\n<p>Is there any way to \"push\" an answer time based?</p>\n",
    "score": 5,
    "creation_date": 1481354395,
    "view_count": 3163,
    "answer_count": 3,
    "tags": "nlp;dialogflow-es;actions-on-google"
  },
  {
    "question_id": 38325438,
    "title": "Where do dimensions in Word2Vec come from?",
    "body": "<p>I am using word2vec model for training a neural network and building a neural embedding for finding the similar words on the vector space. But my question is about dimensions in the word and context embeddings (matrices), which we initialise them by random numbers(vectors) at the beginning of the training, like this <a href=\"https://iksinc.wordpress.com/2015/04/13/words-as-vectors/\" rel=\"nofollow\">https://iksinc.wordpress.com/2015/04/13/words-as-vectors/</a></p>\n\n<p>Lets say we want to display {book,paper,notebook,novel} words on a graph, first of all we should build a matrix with this dimensions 4x2 or 4x3 or 4x4 etc, I know the first dimension of the matrix its the size of our vocabulary |v|. But the second dimension of the matrix (number of vector's dimensions), for example this is a vector for word “book\"  [0.3,0.01,0.04], what are these numbers? do they have any meaning? for example the 0.3 number related to the relation between word “book\" and “paper” in the vocabulary, the 0.01 is the relation between book and notebook, etc.\nJust like TF-IDF, or Co-Occurence matrices that each dimension (column) Y has a meaning - its a word or document related to the word in row X.</p>\n",
    "score": 5,
    "creation_date": 1468317081,
    "view_count": 3331,
    "answer_count": 2,
    "tags": "machine-learning;neural-network;nlp;word2vec;word-embedding"
  },
  {
    "question_id": 36607956,
    "title": "Verb tense conversion in Python",
    "body": "<p>I'm trying to convert certain verbs to other tenses for some NLP task.</p>\n\n<p>I'm trying to use the NodeBox::Linguistics library as suggested here:\n<a href=\"https://stackoverflow.com/questions/3753021/using-nltk-and-wordnet-how-do-i-convert-simple-tense-verb-into-its-present-pas\">Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?</a></p>\n\n<p>But I find that this code does not print the correct form of the word:</p>\n\n<pre><code>print en.verb.present(\"found\")\nprint en.verb.infinitive(\"found\")\n</code></pre>\n\n<p>I expect it to print 'find' but it actually just prints 'found'. </p>\n\n<ol>\n<li>Is this a bug in the library or am I missing something?</li>\n<li>Would you recommend using any other library for any other reason?</li>\n</ol>\n",
    "score": 5,
    "creation_date": 1460576123,
    "view_count": 3215,
    "answer_count": 1,
    "tags": "python;nlp;linguistics;nodebox-linguistics"
  },
  {
    "question_id": 34351978,
    "title": "NLTK RegEx Chunker not capturing defined grammar patterns with wildcards",
    "body": "<p>I am trying to chunk a sentence using NLTK's POS tags as regular expressions. 2 rules are defined to identify phrases, based on the tags of words in the sentence.</p>\n<p>Mainly, I wanted to capture the chunk of <strong>one or more verbs followed by an optional determiner and then one or more nouns at the end</strong>. This is the first rule in definition. But it is not getting captured as Phrase Chunk.</p>\n<pre><code>import nltk\n\n## Defining the POS tagger \ntagger = nltk.data.load(nltk.tag._POS_TAGGER)\n\n\n## A Single sentence - input text value\ntextv=&quot;This has allowed the device to start, and I then see glitches which is not nice.&quot;\ntagged_text = tagger.tag(textv.split())\n\n## Defining Grammar rules for  Phrases\nactphgrammar = r&quot;&quot;&quot;\n     Ph: {&lt;VB*&gt;+&lt;DT&gt;?&lt;NN*&gt;+}  # verbal phrase - one or more verbs followed by optional determiner, and one or more nouns at the end\n     {&lt;RB*&gt;&lt;VB*|JJ*|NN*\\$&gt;} # Adverbial phrase - Adverb followed by adjective / Noun or Verb\n     &quot;&quot;&quot;\n\n### Parsing the defined grammar for  phrases\nactp = nltk.RegexpParser(actphgrammar)\n\nactphrases = actp.parse(tagged_text)\n</code></pre>\n<p>The input to the chunker, tagged_text is as below.</p>\n<blockquote>\n<p>tagged_text\nOut[7]:\n[('This', 'DT'),\n('has', 'VBZ'),\n('allowed', 'VBN'),\n('the', 'DT'),\n('device', 'NN'),\n('to', 'TO'),\n('start,', 'NNP'),\n('and', 'CC'),\n('I', 'PRP'),\n('then', 'RB'),\n('see', 'VB'),\n('glitches', 'NNS'),\n('which', 'WDT'),\n('is', 'VBZ'),\n('not', 'RB'),\n('nice.', 'NNP')]</p>\n</blockquote>\n<p>In the final output, only the adverbial phrase ('<strong>then see</strong>'), that is matching the second rule is being captured.\nI expected the verbal phrase ('<strong>allowed the device</strong>') to match with the first rule and get captured as well, but its not.</p>\n<blockquote>\n<p>actphrases Out[8]: Tree('S', [('This', 'DT'), ('has', 'VBZ'),\n('allowed', 'VBN'), ('the', 'DT'), ('device', 'NN'), ('to', 'TO'),\n('start,', 'NNP'), ('and', 'CC'), ('I', 'PRP'), <em><strong>Tree('Ph', [('then',\n'RB'), ('see', 'VB')])</strong></em>, ('glitches', 'NNS'), ('which', 'WDT'), ('is',\n'VBZ'), ('not', 'RB'), ('nice.', 'NNP')])</p>\n</blockquote>\n<p>NLTK version used is 2.0.5 (Python 2.7)\nAny help or suggestion would be greatly appreciated.</p>\n",
    "score": 5,
    "creation_date": 1450429675,
    "view_count": 2240,
    "answer_count": 1,
    "tags": "python;regex;nlp;nltk;text-chunking"
  },
  {
    "question_id": 33641898,
    "title": "NLTK Document clustering: no terms remain after pruning?",
    "body": "<p>I have 900 different text files loaded into my console, totaling about 3.5 million words. I'm running the document clustering algorithms seen <a href=\"http://brandonrose.org/clustering\" rel=\"nofollow\">here</a>, and am running into issues with the <code>TfidfVectorizer</code> function. Here's what I'm looking at:</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=0.4, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n\nstore_matrix = {}\nfor key,value in speech_dict.items():\n    tfidf_matrix = tfidf_vectorizer.fit_transform(value) #fit the vectorizer to synopses\n    store_matrix[key] = tfidf_matrix\n</code></pre>\n\n<p>This code runs until <code>ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.</code> pops up. However, the code won't quit on error unless I riase <code>max_df</code> to <code>0.99</code> and lower <code>min_df</code> to <code>0.01</code>. Then, it runs seemingly forever, as it's including basically all 3.5 million terms.</p>\n\n<p>How can I get around this?</p>\n\n<p>My text files are stored in <code>speech_dict</code>, the keys of which are the filenames, and the values of which is the text.</p>\n",
    "score": 5,
    "creation_date": 1447199647,
    "view_count": 3678,
    "answer_count": 1,
    "tags": "python;dictionary;nlp"
  },
  {
    "question_id": 23374694,
    "title": "n-gram markov chain transition table",
    "body": "<p>I'm trying to build an n-gram markov model from a given piece of text, and then access the transition table for it so I can calculate the conditional entropy for each sequence of words of length n (the grams). \nFor example, in a 2-gram model, after reading in a corpus of text</p>\n\n<p>\"dogs chase cats dogs chase cats dogs chase cats\n    dogs chase cats dogs chase cats dogs chase cats\n    dogs chase cats dogs chase cats dogs chase cats\n    dogs chase people\"</p>\n\n<p>and building an internal transition table, the state \"dogs chase\" may transition to the state \"chase cats\" with probability 0.9, and to state \"chase people\" with probability 0.1. If I know of the possible transitions, I can calculate the conditional entropy.</p>\n\n<p>Are there any good python libraries for doing this? I've checked NLTK, SRILM, and others but haven't found much.</p>\n",
    "score": 5,
    "creation_date": 1398803530,
    "view_count": 1302,
    "answer_count": 1,
    "tags": "python;nlp;nltk;entropy;markov-models"
  },
  {
    "question_id": 22774913,
    "title": "Estimating document polarity using R&#39;s qdap package without sentSplit",
    "body": "<p>I'd like to apply <code>qdap</code>'s <code>polarity</code> function to a vector of documents, each of which could contain multiple sentences, and obtain the corresponding polarity for each document. For example:</p>\n\n<pre><code>library(qdap)\npolarity(DATA$state)$all$polarity\n# Results:\n [1] -0.8165 -0.4082  0.0000 -0.8944  0.0000  0.0000  0.0000 -0.5774  0.0000\n[10]  0.4082  0.0000\nWarning message:\nIn polarity(DATA$state) :\n  Some rows contain double punctuation.  Suggested use of `sentSplit` function.\n</code></pre>\n\n<p>This warning can't be ignored, as it seems to add the polarity scores of each sentence in the document. This can result in document-level polarity scores outside the [-1, 1] bounds.</p>\n\n<p>I'm aware of the option to first run <code>sentSplit</code> and then average across the sentences, perhaps weighting polarity by word count, but this is (1) inefficient (takes roughly 4x as long as running on the full documents with the warning), and (2) unclear how to weight sentences. This option would look something like this:</p>\n\n<pre><code>DATA$id &lt;- seq(nrow(DATA)) # For identifying and aggregating documents \nsentences &lt;- sentSplit(DATA, \"state\")\nlibrary(data.table) # For aggregation\npol.dt &lt;- data.table(polarity(sentences$state)$all)\npol.dt[, id := sentences$id]\ndocument.polarity &lt;- pol.dt[, sum(polarity * wc) / sum(wc), \"id\"]\n</code></pre>\n\n<p>I was hoping I could run <code>polarity</code> on a version of the vector with periods removed, but it seems that <code>sentSplit</code> does more than that. This works on <code>DATA</code> but not on other sets of text (I'm unsure of the full set of breaks other than periods).</p>\n\n<p>So, I suspect the best way of approaching this is to make each element of the document vector look like one long sentence. How would I do this, or is there another way?</p>\n",
    "score": 5,
    "creation_date": 1396313689,
    "view_count": 3953,
    "answer_count": 2,
    "tags": "r;nlp;sentiment-analysis;qdap"
  },
  {
    "question_id": 19580566,
    "title": "How to do a Tree Transfer in prolog for MT",
    "body": "<p>I need to find a way to Transfer a parse tree in to another with different order.\nIt is for machine translation project with two languages with SVO and SOV architecture.</p>\n\n<pre><code>t1 = s(np(n(he)), vp( v(went), np(n(home))))\n</code></pre>\n\n<p>and I want it to be</p>\n\n<pre><code>t2 = s(np(n(he)), vp( np(n(home)), v(went)))\n</code></pre>\n\n<p>according to a rule that represent t1 represent the SVO language and t2 represent the SOV language architecture.</p>\n\n<p>And the rule set should be applicable for complex sentences with adjectives and adverbs.</p>\n\n<pre><code>t1 = s(np(n(he)), vp( v(went), np(adj(his), n(home))))\n\nt2 = s(np(n(he)), vp( np(adj(his), n(home)), v(went)))\n</code></pre>\n\n<p>Any comment would be useful</p>\n\n<p>thanks Mathee</p>\n",
    "score": 5,
    "creation_date": 1382671513,
    "view_count": 136,
    "answer_count": 1,
    "tags": "prolog;nlp;dcg;machine-translation"
  },
  {
    "question_id": 18502361,
    "title": "where to get News summarization corpus?",
    "body": "<p>Is there any publicly available news+summary corpus for automatic summarization. If yes, can you please provide way to get it ?</p>\n",
    "score": 5,
    "creation_date": 1377750932,
    "view_count": 1187,
    "answer_count": 2,
    "tags": "nlp;text-mining"
  },
  {
    "question_id": 17454291,
    "title": "What text processing tool is recommended for parsing screenplays?",
    "body": "<p>I have some plain-text kinda-structured screenplays, formatted like the example at the end of this post. I would like to parse each into some format where: </p>\n\n<ul>\n<li>It will be easy to pull up just stage directions that deal with a specific place.</li>\n<li>It will be easy to pull up just dialogue belonging to a particular character.</li>\n</ul>\n\n<p>The most obvious approach I can think of is using <code>sed</code> or <code>perl</code> or <code>php</code> to put div tags around each block, with classes representing character, location, and whether it's stage directions or dialogue. Then, open it up as a web-page and use jQuery to pull out whatever I'm interested in. But this sounds like a roundabout way to do it and maybe it only seems like a good idea because these are tools I'm accustomed to. But I'm sure this is a recurring problem that's been solved before, so can anybody recommend a more efficient workflow that can be used on a Linux box? Thanks.</p>\n\n<p>Here is some sample input:</p>\n\n<pre><code>      SOMEWHERE CORPORATION - OPTIONAL COMMENT\n      A guy named BOB is sitting at his computer.\n\n                             BOB\n                Mmmm. Stackoverflow. I like.\n\n      Footsteps are heard approaching.\n\n                             ALICE\n                Where's that report you said you'd have for me?\n\n      Closeup of clock ticking.\n\n                             BOB (looking up)\n                Huh? What?\n\n                             ALICE\n                Some more dialogue.\n\n      Some more stage directions.\n</code></pre>\n\n<p>Here is what sample output might look like:</p>\n\n<pre><code>      &lt;div class='scene somewhere_corporation'&gt;\n       &lt;div class='comment'&gt;OPTIONAL COMMENT&lt;/div&gt;\n       &lt;div class='direction'&gt;A guy named BOB is sitting at his computer.&lt;/div&gt;\n       &lt;div class='dialogue bob'&gt;Mmmm. Stackoverflow. I like.&lt;/div&gt;\n       &lt;div class='direction'&gt;Footsteps are heard approaching.&lt;/div&gt;\n       &lt;div class='dialogue alice'&gt;Where's that report you said you'd have for me?&lt;/div&gt;\n       &lt;div class='direction'&gt;Closeup of clock ticking.&lt;/div&gt;\n       &lt;div class='comment bob'&gt;looking up&lt;/div&gt;\n       &lt;div class='dialogue bob'&gt;Huh? What?&lt;/div&gt;\n       &lt;div class='dialogue alice'&gt;Some more dialogue.&lt;/div&gt;\n       &lt;div class='direction'&gt;Some more stage directions.&lt;/div&gt;\n      &lt;/div&gt;\n</code></pre>\n\n<p>I'm using DOM as an example, but again, only because that's something I understand. I'm open to whatever is considered a best practice for this type of text-processing task if, as I suspect, roll-your-own regexps and jQuery is not the best practice. Thanks.</p>\n",
    "score": 5,
    "creation_date": 1372871710,
    "view_count": 1234,
    "answer_count": 2,
    "tags": "text;nlp;semantic-markup;searchable"
  },
  {
    "question_id": 15710601,
    "title": "Converting an array of morphemes to a sentence in Ruby",
    "body": "<p>I want to convert an array of morphemes produced by a <a href=\"http://www.cis.upenn.edu/~treebank/tokenization.html\" rel=\"nofollow\">PTB-style</a> tokenizer:</p>\n\n<pre><code>[\"The\", \"house\", \"is\", \"n't\", \"on\", \"fire\", \".\"]\n</code></pre>\n\n<p>To a sentence:</p>\n\n<pre><code>\"The house isn't on fire.\"\n</code></pre>\n\n<p>What is a sensible way to accomplish this?</p>\n",
    "score": 5,
    "creation_date": 1364587293,
    "view_count": 225,
    "answer_count": 1,
    "tags": "ruby;nlp"
  },
  {
    "question_id": 12467817,
    "title": "Is Mark V. Shaney still the best way to generate text?",
    "body": "<p>I want to take large documents and generate text that resembles them. I know that Markov Chains were used to do this with <a href=\"http://en.wikipedia.org/wiki/Mark_V_Shaney\" rel=\"noreferrer\">Mark V Shaney</a>. Is there a better way to do it now, or is this approach still basically the best one available?</p>\n",
    "score": 5,
    "creation_date": 1347919991,
    "view_count": 798,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 11606493,
    "title": "How to Extract subject Verb Object using nlp java",
    "body": "<p>How to Extract SVO using NLP in java, i am new in nlp.i am currently using opennlp. but how to do in java with a perticular in java sentence.</p>\n\n<pre><code>LexicalizedParser lp = **new LexicalizedParser(\"englishPCFG.ser.gz\");**\nString[] sent = { \"This\", \"is\", \"an\", \"easy\", \"sentence\", \".\" };\nTree parse = (Tree) lp.apply(Arrays.asList(sent));\nparse.pennPrint();\nSystem.out.println();\nTreePrint tp = new TreePrint(\"penn,typedDependenciesCollapsed\");\ntp.print(parse);\n</code></pre>\n\n<p>getting an compilation error at \nnew LexicalizedParser(\"englishPCFG.ser.gz\");**\nThe constructor LexicalizedParser(String) is undefined</p>\n",
    "score": 5,
    "creation_date": 1343017612,
    "view_count": 5692,
    "answer_count": 4,
    "tags": "nlp"
  },
  {
    "question_id": 11229732,
    "title": "Natural Language Generation in PHP",
    "body": "<p>I woke up last night with a thought in my head: Can PHP be used to generate random words that sound natural? (Like the Lorem ipsum verses).</p>\n\n<ol>\n<li>Words being single letter: 'a,e,i,o,u'</li>\n<li>Words being double letter: any combination of vowel and consonant.</li>\n<li>Maximum word length would be I think six letters.</li>\n</ol>\n\n<p>The purpose would be to fill space on website templates with this instead of 'Lorem ipsum', or send test emails for certain PHP scripts to make sure mail() works.</p>\n\n<p>But my thoughts on how it would work are that PHP would generate random length words, 1-6 letters each, with a few \"don't do this\" rules like \"no two single-letter words next to each other\" or \"no three-vowels in a row\" or \"no three-consonants in a row\" and automatically add punctuation and capitalization after between 4 and 8 words to a sentence.</p>\n\n<p>Would this be at all possible, and if so, are there any pre-existing classes or functions I could implement?</p>\n",
    "score": 5,
    "creation_date": 1340810699,
    "view_count": 1084,
    "answer_count": 1,
    "tags": "php;nlp"
  },
  {
    "question_id": 10846075,
    "title": "Python NLTK: How to retrieve percentage confidence in classifier prediction",
    "body": "<p>I am currently training an NLTK classifier to recognize motion commands.  These commands can include \"move left\", \"please move forward\", \"halt!\", \"move towards the right\", etc.</p>\n\n<p>I am currently using the classifier based on a few key features (such as the existence of \"halt\" and \"left\") to classify information, and it works fine.</p>\n\n<p>However, let us presume the following text, \"move to the left right\", is given.  In this case, both keywords are conflicting each other, and presumably, the classifier should have a low confidence level when offering its resultant prediction.</p>\n\n<p>As such, is there any way to retrieve the confidence of its predicted \"direction\", after using <code>&lt;CLASSIFIER&gt;.classify()</code> ?</p>\n\n<p>NOTE: I have tried to use <code>nltk.classify.accuracy()</code> but it is only for use on a test data-set, not a single query.</p>\n",
    "score": 5,
    "creation_date": 1338535278,
    "view_count": 3015,
    "answer_count": 1,
    "tags": "python;nlp;machine-learning;classification;nltk"
  },
  {
    "question_id": 9255768,
    "title": "Python vs Java for natural language processing",
    "body": "<p>I have been working on java to find the similarity between two documents. I prefer finding semantic similarity , but havent made efforts to find it yet . I am using the following approach . </p>\n\n<ol>\n<li>Extract terms / tokens (I am using JAWS with wordnet to remove synonyms thus improves the similarities )</li>\n<li>make a term document matrix </li>\n<li>LSA </li>\n<li>Cosine similarity </li>\n</ol>\n\n<p>When i was looking at few stackoverflow pages , i got quite a few links to python implementations. </p>\n\n<p>I would like to know if python is a better language to find the text similarity and would also like to know if i can find semantic similairty between two documents in python  </p>\n",
    "score": 5,
    "creation_date": 1329108837,
    "view_count": 2776,
    "answer_count": 1,
    "tags": "java;python;text;nlp;similarity"
  },
  {
    "question_id": 8947701,
    "title": "Using context to improve part-of-speech tagging",
    "body": "<p>Are there some common or recommended techniques for using the context of word to improve the accuracy of part-of-speech tagging?</p>\n\n<p>For example if I had the sentence:</p>\n\n<blockquote>\n  <p>I played golf on a links.</p>\n</blockquote>\n\n<p>The word \"links\" could be either singular (a golf course) or plural. I tried this sentence in several grammar checkers and they all correctly recognized the sentence as valid.</p>\n\n<p>The problem is they also thought that this sentence was valid:</p>\n\n<blockquote>\n  <p>I clicked on a links.</p>\n</blockquote>\n\n<p>Is there a good way to use the context (clicked vs played golf) to infer the correct part-of-speech?</p>\n\n<p>Thanks!</p>\n",
    "score": 5,
    "creation_date": 1327092606,
    "view_count": 802,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 7813683,
    "title": "weka - how to print incorrectly classified instances",
    "body": "<p>my weka output shows:</p>\n\n<pre><code>Correctly Classified Instances       32083               94.0244 %\nIncorrectly Classified Instances      2039                5.9756 %\n</code></pre>\n\n<p>I want to be able to print out what the incorrect instances were so i can make adjustments and understand why they were misclassified.</p>\n\n<p>my print method is below.<br>\ni am attempting to find instances whose predicted class value were not equal to the actual class value and then print its attributes.<br>\nbut when i do this the attribute enumeration is not printing anything. </p>\n\n<p>Does anyone have a suggestion for how to print out the misclassified instances? </p>\n\n<p>thanks much. </p>\n\n<pre><code>private void printSummary(Classifier base, Evaluation eval, Instances data) throws Exception\n{\n    // output evaluation\n    System.out.println();\n    System.out.println(\"=== Setup ===\");\n    System.out.println(\"Classifier: \" + classifierName.getClass().getName() + \" \" + Utils.joinOptions(base.getOptions()));\n    System.out.println(\"Dataset: \" + data.relationName());\n    System.out.println();\n\n    // output predictions\n    System.out.println(\"# - actual - predicted - error - distribution - token\");\n    for (int i = 0; i &lt; data.numInstances(); i++) \n    {\n        double pred = base.classifyInstance(data.instance(i));\n        double actual = data.instance(i).classValue();\n        double[] dist = base.distributionForInstance(data.instance(i));\n\n        if (pred != actual)\n        {\n            System.out.print((i+1));\n            System.out.print(\" - \");\n            System.out.print(data.instance(i).toString(data.classIndex()));\n            System.out.print(\" - \");\n            System.out.print(data.classAttribute().value((int) pred));\n            System.out.print(\" - \");\n            if (pred != data.instance(i).classValue())\n                System.out.print(\"yes\");\n            else\n                System.out.print(\"no\");\n            System.out.print(\" - \");\n            System.out.print(Utils.arrayToString(dist));\n            System.out.print(\" - \");\n            data.instance(i).enumerateAttributes().toString();\n            System.out.println();\n        }\n    }\n\n    System.out.println(eval.toSummaryString());\n    System.out.println(eval.toClassDetailsString());\n    System.out.println(eval.toMatrixString());\n}\n</code></pre>\n",
    "score": 5,
    "creation_date": 1318971777,
    "view_count": 5312,
    "answer_count": 2,
    "tags": "java;nlp;classification;weka"
  },
  {
    "question_id": 4561612,
    "title": "Open source options for non-English term extraction?",
    "body": "<p>I am looking for a open source project that does term extraction with multiple languages.</p>\n<p>I have already found <a href=\"http://developer.yahoo.com/search/content/V1/termExtraction.html\" rel=\"nofollow noreferrer\">Yahoo BOSS Term Extraction Web Service</a>, and it is good. However, it does not handle languages other than English.</p>\n<p>Are there any open source term extraction projects that support more languages?</p>\n<p>Thanks!</p>\n",
    "score": 5,
    "creation_date": 1293702632,
    "view_count": 1215,
    "answer_count": 3,
    "tags": "nlp;information-extraction"
  },
  {
    "question_id": 244913,
    "title": "Split string into sentences using regular expression",
    "body": "<p>I need to match a string like \"one. two.    three. four. five.  six. seven. eight. nine. ten. eleven\" into groups of four sentences.  I need a regular expression to break the string into a group after every fourth period. Something like: </p>\n\n<pre><code>  string regex = @\"(.*.\\s){4}\";\n\n  System.Text.RegularExpressions.Regex exp = new System.Text.RegularExpressions.Regex(regex);\n\n  string result = exp.Replace(toTest, \".\\n\");\n</code></pre>\n\n<p>doesn't work because it will replace the text before the periods, not just the periods themselves.  How can I count just the periods and replace them with a period and new line character?</p>\n",
    "score": 5,
    "creation_date": 1225228953,
    "view_count": 10234,
    "answer_count": 6,
    "tags": "c#;regex;nlp"
  },
  {
    "question_id": 77879969,
    "title": "Is there a way to save a pre-compiled AutoTokenizer?",
    "body": "<p>Sometimes, we'll have to do something like this to extend a pre-trained tokenizer:</p>\n<pre><code>from transformers import AutoTokenizer\n\nfrom datasets import load_dataset\n\n\nds_de = load_dataset(&quot;mc4&quot;, 'de')\nds_fr = load_dataset(&quot;mc4&quot;, 'fr')\n\nde_tokenizer = tokenizer.train_new_from_iterator(\n    ds_de['text'],vocab_size=50_000\n)\n\nfr_tokenizer = tokenizer.train_new_from_iterator(\n    ds_fr['text'],vocab_size=50_000\n)\n\nnew_tokens_de = set(de_tokenizer.vocab).difference(tokenizer.vocab)\nnew_tokens_fr = set(fr_tokenizer.vocab).difference(tokenizer.vocab)\nnew_tokens = set(new_tokens_de).union(new_tokens_fr)\n\n\ntokenizer = AutoTokenizer.from_pretrained(\n    'moussaKam/frugalscore_tiny_bert-base_bert-score'\n)\n\ntokenizer.add_tokens(list(new_tokens))\n\ntokenizer.save_pretrained('frugalscore_tiny_bert-de-fr')\n</code></pre>\n<p>And then when loading the tokenizer,</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(\n  'frugalscore_tiny_bert-de-fr', local_files_only=True\n)\n</code></pre>\n<p>It takes pretty long to load from <code>%%time</code> in a Jupyter cell:</p>\n<pre><code>CPU times: user 34min 20s\nWall time: 34min 22s\n</code></pre>\n<p>I guess this is due to regex compilation for the added tokens which was also raised in <a href=\"https://github.com/huggingface/tokenizers/issues/914\" rel=\"noreferrer\">https://github.com/huggingface/tokenizers/issues/914</a></p>\n<p>I think it's okay since it'll load once and the work can be done without redoing the regex compiles.</p>\n<h3>But, is there a way to just save the tokenizer in binary form and avoid the whole regex compilation the next time?</h3>\n",
    "score": 5,
    "creation_date": 1706185565,
    "view_count": 547,
    "answer_count": 1,
    "tags": "python;nlp;tokenize;huggingface;huggingface-tokenizers"
  },
  {
    "question_id": 67337774,
    "title": "Loss function for comparing two vectors for categorization",
    "body": "<p>I am performing a NLP task where I analyze a document and classify it into one of six categories. However, I do this operation at three different time periods. So the final output is an array of three integers (sparse), where each integer is the category 0-5. So a label looks like this: <code>[1, 4, 5]</code>.</p>\n<p>I am using BERT and am trying to decide what type of head I should attach to it, as well as what type of loss function I should use. Would it make sense to use BERT's output of size <code>1024</code> and run it through a <code>Dense</code> layer with 18 neurons, then reshape into something of size <code>(3,6)</code>?</p>\n<p>Finally, I assume I would use Sparse Categorical Cross-Entropy as my loss function?</p>\n",
    "score": 5,
    "creation_date": 1619802035,
    "view_count": 1527,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;bert-language-model"
  },
  {
    "question_id": 65906965,
    "title": "PyTorch GPU memory leak during inference",
    "body": "<p>I am trying to encode documents sentence-wise with a huggingface transformer module. I'm using the very small <code>google/bert_uncased_L-2_H-128_A-2</code> pretrained model with the following code:</p>\n<pre><code>def pre_encode_wikipedia(model, tokenizer, device, save_path):\n  \n  document_data_list = []\n\n  for iteration, document in enumerate(wikipedia_small['text']):\n    torch.cuda.empty_cache()\n\n    sentence_embeds_per_doc = [torch.randn(128)]\n    attention_mask_per_doc = [1]\n    special_tokens_per_doc = [1]\n\n    doc_split = nltk.sent_tokenize(document)\n    doc_tokenized = tokenizer.batch_encode_plus(doc_split, padding='longest', truncation=True, max_length=512, return_tensors='pt')\n\n    for key, value in doc_tokenized.items():\n      doc_tokenized[key] = doc_tokenized[key].to(device)\n\n    with torch.no_grad():  \n      doc_encoded = model(**doc_tokenized)\n\n    for sentence in doc_encoded['last_hidden_state']:\n      sentence[0].to('cpu')\n      sentence_embeds_per_doc.append(sentence[0])\n      attention_mask_per_doc.append(1)\n      special_tokens_per_doc.append(0)\n\n    sentence_embeds = torch.stack(sentence_embeds_per_doc)\n    attention_mask = torch.FloatTensor(attention_mask_per_doc)\n    special_tokens_mask = torch.FloatTensor(special_tokens_per_doc)\n\n    document_data = torch.utils.data.TensorDataset(*[sentence_embeds, attention_mask, special_tokens_mask])\n    torch.save(document_data, f'{save_path}{time.strftime(&quot;%Y%m%d-%H%M%S&quot;)}{iteration}.pt')\n    print(f&quot;Document at {iteration} encoded and saved.&quot;)\n</code></pre>\n<p>After about 200-300 iterations on my local GTX 1060 3GB I get an error saying that my <strong>CUDA memory is full</strong>. Running this code on Colab with more GPU RAM gives me a few thousand iterations.</p>\n<p>Things I've tried:</p>\n<ul>\n<li>Adding <code>torch.cuda.empty_cache()</code> to the start of every iteration to clear out previously held tensors</li>\n<li>Wrapping the model in <code>torch.no_grad()</code> to disable the computation graph</li>\n<li>Setting <code>model.eval()</code> to disable any stochastic properties that might take up memory</li>\n<li>Sending the output straight to CPU in hopes to free up memory</li>\n</ul>\n<p>I'm baffled as to why my memory keeps overflowing. I've trained several models of bigger sizes, applying all the standard practices of a training loop (<code>optimizer.zero_grad()</code>, etc.) I've never had this problem. Why does it appear during this seemingly trivial task?</p>\n<p><strong>Edit #1</strong>\nChanging <code>sentence[0].to('cpu')</code> to <code>cpu_sentence = sentence[0].to('cpu')</code> gave me a few thousand iterations before VRAM usage suddenly spiked, causing the run to crash:<a href=\"https://i.sstatic.net/ahUZb.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/ahUZb.png\" alt=\"enter image description here\" /></a></p>\n",
    "score": 5,
    "creation_date": 1611685019,
    "view_count": 4997,
    "answer_count": 1,
    "tags": "nlp;pytorch;gpu;bert-language-model;huggingface-transformers"
  },
  {
    "question_id": 65632248,
    "title": "Same sentences produces a different vector in XLNet",
    "body": "<p>I have computed the vectors for two same sentences using <a href=\"https://github.com/amansrivastava17/embedding-as-service\" rel=\"nofollow noreferrer\">XLNet embedding-as-service</a>. But the model produces different vector embeddings for both the two same sentences hence the cosine similarity is not 1 and the Euclidean distances also not 0. in case of BERT its works fine.\nfor example; if</p>\n<pre><code>vec1 = en.encode(texts=['he is anger'],pooling='reduce_mean')\nvec2 = en.encode(texts=['he is anger'],pooling='reduce_mean')\n</code></pre>\n<p>the model (XLNet) is saying that these two sentences are dissimilar.</p>\n",
    "score": 5,
    "creation_date": 1610121225,
    "view_count": 272,
    "answer_count": 2,
    "tags": "python;nlp;huggingface-transformers;bert-language-model;sentence-transformers"
  },
  {
    "question_id": 55993817,
    "title": "How to properly update a model in spaCy?",
    "body": "<p>I want to update a model with new entities. I'm loading the \"pt\" NER model, and trying to update it.\nBefore doing anything, I tried this phrase: \"meu nome é Mário e hoje eu vou para academia\". (in English this phrase is \"my name is Mário and today I'm going to go to gym).\nBefore the whole process I got this: </p>\n\n<pre><code>Entities [('Mário', 'PER')]\nTokens [('meu', '', 2), ('nome', '', 2), ('é', '', 2), ('Mário', 'PER', 3), ('e', '', 2), ('hoje', '', 2), ('eu', '', 2), ('vou', '', 2), ('pra', '', 2), ('academia', '', 2)]\n</code></pre>\n\n<p>Ok, Mário is a name and it's correct. \nBut I want the model recognize \"hoje (today)\" as DATE, then I ran the script below.</p>\n\n<p>After I ran the script, I've tried the same setence and got this:</p>\n\n<pre><code>Entities [('hoje', 'DATE')]\nTokens [('meu', '', 2), ('nome', '', 2), ('é', '', 2), ('Mário', '', 2), ('e', '', 2), ('hoje', 'DATE', 3), ('eu', '', 2), ('vou', '', 2), ('pra', '', 2), ('academia', '', 2)]\n</code></pre>\n\n<p>The model is recognizing \"hoje\" as DATE, but totally forgot about Mário as Person.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from __future__ import unicode_literals, print_function\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\n\n# training data\nTRAIN_DATA = [\n    (\"Infelizmente não, eu briguei com meus amigos hoje\", {\"entities\": [(45, 49, \"DATE\")]}),\n    (\"hoje foi um bom dia.\", {\"entities\": [(0, 4, \"DATE\")]}),\n    (\"ah não sei, hoje foi horrível\", {\"entities\": [(12, 16, \"DATE\")]}),\n    (\"hoje eu briguei com o Mário\", {\"entities\": [(0, 4, \"DATE\")]})\n]\n\n\n@plac.annotations(\n    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n)\n\ndef main(model=\"pt\", output_dir=\"/model\", n_iter=100):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"pt\")  # create blank Language class\n            print(\"Created blank 'en' model\")\n\n    doc = nlp(\"meu nome é Mário e hoje eu vou pra academia\")\n    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    # add labels\n    for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # reset and initialize the weights randomly – but only if we're\n        # training a new model\n        if model is None:\n            nlp.begin_training()\n        for itn in range(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n            print(\"Losses\", losses)\n\n    # test the trained model\n   # for text, _ in TRAIN_DATA:\n    doc = nlp(\"meu nome é Mário e hoje eu vou pra academia\")\n    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n\n    # save model to output directory\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        # test the saved model\n        print(\"Loading from\", output_dir)\n        nlp2 = spacy.load(output_dir)\n        # for text, _ in TRAIN_DATA:\n        #     doc = nlp2(text)\n        #     print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n        #     print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n</code></pre>\n",
    "score": 5,
    "creation_date": 1557071810,
    "view_count": 2445,
    "answer_count": 1,
    "tags": "python;machine-learning;spacy;nlp"
  },
  {
    "question_id": 55108636,
    "title": "Document similarity with Word Mover Distance and Bert-Embedding",
    "body": "<p>I am trying to calculate the document similarity (nearest neighbor) for two arbitrary documents using word embeddings based on <a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\" rel=\"noreferrer\">Google's BERT</a>.\nIn order to obtain word embeddings from Bert, I use <a href=\"https://github.com/hanxiao/bert-as-service\" rel=\"noreferrer\">bert-as-a-service</a>.\nDocument similarity should be based on Word-Mover-Distance with the python <a href=\"https://github.com/src-d/wmd-relax\" rel=\"noreferrer\">wmd-relax</a> package.</p>\n\n<p>My previous tries are orientated along this tutorial from the <code>wmd-relax</code> github repo: <a href=\"https://github.com/src-d/wmd-relax/blob/master/spacy_example.py\" rel=\"noreferrer\">https://github.com/src-d/wmd-relax/blob/master/spacy_example.py</a></p>\n\n<pre><code>import numpy as np\nimport spacy\nimport requests\nfrom wmd import WMD\nfrom collections import Counter\nfrom bert_serving.client import BertClient\n\n# Wikipedia titles\ntitles = [\"Germany\", \"Spain\", \"Google\", \"Apple\"]\n\n# Standard model from spacy\nnlp = spacy.load(\"en_vectors_web_lg\")\n\n# Fetch wiki articles and prepare as specy document\ndocuments_spacy = {}\nprint('Create spacy document')\nfor title in titles:\n    print(\"... fetching\", title)\n    pages = requests.get(\n        \"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=%s\"\n        \"&amp;prop=extracts&amp;explaintext\" % title).json()[\"query\"][\"pages\"]\n    text = nlp(next(iter(pages.values()))[\"extract\"])\n    tokens = [t for t in text if t.is_alpha and not t.is_stop]\n    words = Counter(t.text for t in tokens)\n    orths = {t.text: t.orth for t in tokens}\n    sorted_words = sorted(words)\n    documents_spacy[title] = (title, [orths[t] for t in sorted_words],\n                              np.array([words[t] for t in sorted_words],\n                                       dtype=np.float32))\n\n\n# This is the original embedding class with the model from spacy\nclass SpacyEmbeddings(object):\n    def __getitem__(self, item):\n        return nlp.vocab[item].vector\n\n\n# Bert Embeddings using bert-as-as-service\nclass BertEmbeddings:\n    def __init__(self, ip='localhost', port=5555, port_out=5556):\n        self.server = BertClient(ip=ip, port=port, port_out=port_out)\n\n    def __getitem__(self, item):\n        text = nlp.vocab[item].text\n        emb = self.server.encode([text])\n        return emb\n\n\n# Get the nearest neighbor of one of the atricles\ncalc_bert = WMD(BertEmbeddings(), documents_spacy)\ncalc_bert.nearest_neighbors(titles[0])\n</code></pre>\n\n<p>Unfortunately, the calculations fails with a dimensions mismatch in the distance calculation:\n<code>ValueError: shapes (812,1,768) and (768,1,812) not aligned: 768 (dim 2) != 1 (dim 1)</code></p>\n",
    "score": 5,
    "creation_date": 1552330831,
    "view_count": 3716,
    "answer_count": 1,
    "tags": "python;nlp;similarity;word-embedding"
  },
  {
    "question_id": 51798593,
    "title": "Python Spacy NLP - TypeError: Argument &#39;string&#39; has incorrect type (expected unicode, got str)",
    "body": "<p>I was getting the following error while i was trying to read a txt file in spacy.</p>\n\n<blockquote>\n  <p>TypeError: Argument 'string' has incorrect type (expected unicode, got str) </p>\n</blockquote>\n\n<p>Here is the code below</p>\n\n<pre><code>from __future__  import unicode_literals\nimport spacy\nnlp= spacy.load('en')\ndoc_file = nlp(open(\"example.txt\").read())\n</code></pre>\n",
    "score": 5,
    "creation_date": 1533982087,
    "view_count": 4890,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 49733653,
    "title": "Tokenize TEI-like text",
    "body": "<p>I'm trying to use spaCy to tokenize a text document, where named entities are wrapped in XML tags. E.g. <a href=\"https://en.wikipedia.org/wiki/Text_Encoding_Initiative\" rel=\"nofollow noreferrer\">TEI</a>-like <code>&lt;personName&gt;Harry&lt;/personName&gt; goes to &lt;orgName&gt;Hogwarts&lt;/orgName&gt;</code>.</p>\n\n<pre><code>import spacy\n\nnlp = spacy.load('en')\ntxt = '&lt;personName&gt;Harry&lt;/personName&gt; goes to &lt;orgName&gt;Hogwarts&lt;/orgName&gt;. &lt;personName&gt;Sally&lt;/personName&gt; lives in &lt;locationName&gt;London&lt;/locationName&gt;.'\ndoc = nlp(txt)\nsents = list(doc.sents)\nfor i, s in enumerate(doc.sents):\n    print(\"{}: {}\".format(i, s))\n</code></pre>\n\n<p>However, the XML tags cause a sentence split:</p>\n\n<pre><code>0: &lt;personName&gt;\n1: Harry&lt;/personName&gt; goes to &lt;orgName&gt;\n2: Hogwarts&lt;/orgName&gt;.\n3: &lt;personName&gt;\n4: Sally&lt;/personName&gt; lives in &lt;\n5: locationName&gt;\n6: London&lt;/locationName&gt;.\n</code></pre>\n\n<p>How can I get only 2 sentences?\nI know that spaCy has a support for a <a href=\"https://spacy.io/usage/linguistic-features#section-tokenization\" rel=\"nofollow noreferrer\">custom tokenizer</a> but since the rest of the text is standard, I'd like to keep using the built-in one or perhaps build on top of it to recognize the XML annotations.</p>\n",
    "score": 5,
    "creation_date": 1523278975,
    "view_count": 458,
    "answer_count": 1,
    "tags": "python;nlp;tokenize;spacy;named-entity-recognition"
  },
  {
    "question_id": 49147890,
    "title": "How to check if a string contains roman numerals in R?",
    "body": "<p>I have a column for residential adresses in my dataset 'ad'. I want to check for addresses which has no numbers(including roman numerals) present. \nI'm using </p>\n\n<pre><code>ad$check &lt;- grepl(\"[[:digit:]]\",ad$address)\n</code></pre>\n\n<p>to flag out addresses with no digits present. How do I do the same with addresses that contain roman numerals?</p>\n\n<p>Eg: \"floor X, DLF Building- III, ABC City\"</p>\n",
    "score": 5,
    "creation_date": 1520413557,
    "view_count": 1399,
    "answer_count": 1,
    "tags": "r;regex;text-analysis;roman-numerals"
  },
  {
    "question_id": 44763499,
    "title": "extending NLP entity extraction",
    "body": "<p>We would like to identify from a simple search neighborhood and streets in various cities. We don't only use English but also various other Cyrillic languages. We need to be able to identify spelling mistakes of locations. When looking at python libraries, I found this one: \n<a href=\"http://polyglot.readthedocs.io/en/latest/NamedEntityRecognition.html\" rel=\"noreferrer\">http://polyglot.readthedocs.io/en/latest/NamedEntityRecognition.html</a></p>\n\n<p>We tried to play around with it, but cannot find a way to extend the entity recognition database. How can that be done?<br>\nIf not is there any other suggestion for a multi lingual nlp that can help spell check and also extract various entities matching a custom database? </p>\n",
    "score": 5,
    "creation_date": 1498491868,
    "view_count": 1062,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;polyglot;named-entity-extraction"
  },
  {
    "question_id": 43068076,
    "title": "Arabic text not showing in R-",
    "body": "<p>Just started working with R in Arabic as I plan to do text analysis and text mining with Hadith corpus. I have been reading threads related to my question but nevertheless, still can't manage to get the REAL basics here (sorry, absolute beginner).</p>\n\n<p>So, I entered:\ntextarabic.v &lt;- scan(\"data/arabic-text.txt\", encoding=\"UTF-8\", what= \"character\",sep=\"\\n\")</p>\n\n<p>And what comes out <code>textarabic.v</code> is of course, symbols (pic). Prior to this, I saved my text in utf-8 as I read in a thread but still nothing shows in Arabic.</p>\n\n<p>I can type in Arabic R but scan brings the text in symbols.</p>\n\n<p><a href=\"https://i.sstatic.net/vwwv5.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/vwwv5.png\" alt=\"enter image description here\"></a></p>\n\n<p>Also read and tried to implement other user's are codes to make Arabic text function but I don't even know how and where to implement them.\nI added to R, tm and NLP packages. </p>\n\n<p>What do you suggest for me to do next?\nThanks in advance,</p>\n",
    "score": 5,
    "creation_date": 1490699653,
    "view_count": 884,
    "answer_count": 1,
    "tags": "r;nlp;arabic;tm"
  },
  {
    "question_id": 37248553,
    "title": "What are the common methods to determine intent",
    "body": "<p>Many NLP APIs offer intent extraction like API.ai and wit.ai. However I'm unclear about their details. Do they do dependency parsing then extract relations, or simply taking out keywords from a sentence? How to parse \"check if tomorrow is going to rain\"?</p>\n",
    "score": 5,
    "creation_date": 1463382789,
    "view_count": 1625,
    "answer_count": 1,
    "tags": "nlp;chatbot"
  },
  {
    "question_id": 34376532,
    "title": "How to save sklearn pipeline/feature-transformer",
    "body": "<p>I have a pipeline contains only a feature union that has three different sets of feature, including tfidf:</p>\n\n<pre><code>A_vec = AVectorizer()\nB_vec = BVectorizer()\ntfidf_vec = TfidfVectorizer(ngram_range=(1,2), analyzer='word', binary=False, stop_words=stopWords, min_df=0.01, use_idf=True)\nall_features = FeatureUnion([('A_feature', A_vec), ('V_feature', B_vec), ('tfidf_feature', tfidf_vec)])\npipeline = Pipeline([('all_feature', all_features)])\n</code></pre>\n\n<p>I want to save this pipelined feature transformer for my test data (I am using LibSVM for classification), and this is what I have tried:</p>\n\n<ul>\n<li><p>I have used joblib.dump to save this pipeline but it generated toooo many .npy files so I had to stop the writing process. It was a rather stupid attempt!</p></li>\n<li><p>I have saved tfidf_vec.vocabulary_ and thus</p>\n\n<p>tfidf_vec2 = TfidfVectorizer(ngram_range=(1,3), analyzer='word', binary=False, stop_words=stopWords, min_df=0.01, use_idf=True,vocabulary=pickle.load(open(\"../vocab.pkl\", \"rb\"))</p>\n\n<p>... ...</p>\n\n<p>feat_test = pipeline2.transform(X_test)</p></li>\n</ul>\n\n<p>It says \"NotFittedError: idf vector is not fitted\". I then used fit_transform rather than transform but it generates a feature vector that contains different values (comparing to the correct feature vector). Then I followed <a href=\"http://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/\" rel=\"nofollow\">http://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/</a> and still struggling to get it work. </p>\n\n<p>Is there a simpler way to achieve this? Thanks!</p>\n",
    "score": 5,
    "creation_date": 1450568139,
    "view_count": 4080,
    "answer_count": 2,
    "tags": "python;nlp;scikit-learn"
  },
  {
    "question_id": 31824260,
    "title": "sentiment analysis of Non-English tweets in python",
    "body": "<p>Objective: To classify each tweet as positive or negative and write it to an output file which will contain the username, original tweet and the sentiment of the tweet.</p>\n\n<p>Code: </p>\n\n<pre><code>import re,math\ninput_file=\"raw_data.csv\"\nfileout=open(\"Output.txt\",\"w\")\nwordFile=open(\"words.txt\",\"w\")\nexpression=r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\"\n\nfileAFINN = 'AFINN-111.txt'\nafinn = dict(map(lambda (w, s): (w, int(s)), [ws.strip().split('\\t') for ws in open(fileAFINN)]))\n\npattern=re.compile(r'\\w+')\npattern_split = re.compile(r\"\\W+\")\nwords = pattern_split.split(input_file.lower())\nprint \"File processing started\"\nwith open(input_file,'r') as myfile:\nfor line in myfile:\n    line = line.lower()\n\n    line=re.sub(expression,\" \",line)\n    words = pattern_split.split(line.lower())\n    sentiments = map(lambda word: afinn.get(word, 0), words)\n    #print sentiments\n    # How should you weight the individual word sentiments?\n    # You could do N, sqrt(N) or 1 for example. Here I use sqrt(N)\n    \"\"\"\n    Returns a float for sentiment strength based on the input text.\n    Positive values are positive valence, negative value are negative valence.\n    \"\"\"\n    if sentiments:\n        sentiment = float(sum(sentiments))/math.sqrt(len(sentiments))\n        #wordFile.write(sentiments)\n    else:\n        sentiment = 0\n    wordFile.write(line+','+str(sentiment)+'\\n')\nfileout.write(line+'\\n')\nprint \"File processing completed\"\n\nfileout.close()\nmyfile.close()\nwordFile.close()\n</code></pre>\n\n<p>Issue: Apparently the output.txt file is </p>\n\n<pre><code>abc some tweet text 0\nbcd some more tweets 1\nefg some more tweet 0\n</code></pre>\n\n<p>Question 1: How do I add a comma between the userid tweet-text sentiment? The output should be like;</p>\n\n<pre><code> abc,some tweet text,0\n bcd,some other tweet,1\n efg,more tweets,0\n</code></pre>\n\n<p>Question 2: The tweets are in Bahasa Melayu (BM) and the AFINN dictionary that I am using is of English words. So the classification is wrong. Do you know any BM dictionary that I can use?</p>\n\n<p>Question 3: How do I pack this code in a JAR file?</p>\n\n<p>Thank you.</p>\n",
    "score": 5,
    "creation_date": 1438753432,
    "view_count": 1071,
    "answer_count": 1,
    "tags": "python;python-2.7;twitter;nlp;sentiment-analysis"
  },
  {
    "question_id": 30005043,
    "title": "How to See Original Words that Mapped to a Particular Stem Word",
    "body": "<p>I'm doing some text analysis using tm_map in R. I run the following code (no errors) to produce a Document Term Matrix of (stemmed and otherwise pre-processed) words.</p>\n\n<pre><code>  corpus = Corpus(VectorSource(textVector))\n  corpus = tm_map(corpus, tolower)\n  corpus = tm_map(corpus, PlainTextDocument) \n  corpus = tm_map(corpus, removePunctuation)\n  corpus = tm_map(corpus, removeWords, c(stopwords(\"english\")))\n  corpus = tm_map(corpus, stemDocument, language=\"english\")\n\n  dtm = DocumentTermMatrix(corpus)\n  mostFreqTerms = findFreqTerms(dtm, lowfreq=125) \n</code></pre>\n\n<p>But when I look at my (stemmed) mostFreqTerms, I see a couple that make me think, \"hm, what words were stemmed to produce that?\" Also, there may be stem words that make sense to me at first glance, but maybe I'm missing the fact that they actually contain words with different meanings.</p>\n\n<p>I'd like to apply the strategy/technique described in this SO answer on retaining specific terms during stemming (e.g. keeping \"natural\" and \"naturalized\" from becoming the same stemmed term.\n<a href=\"https://stackoverflow.com/questions/16069406/text-mining-with-the-tm-package-word-stemming\">Text-mining with the tm-package - word stemming</a></p>\n\n<p>But to do so most comprehensively, I'd like to see a list of all the separate words that mapped to my most frequent stem words. Is there a way to find the words that, when stemmed, produced my list of mostFreqTerms?</p>\n\n<p>EDIT: REPRODUCIBLE EXAMPLE</p>\n\n<pre><code>textVector = c(\"Trisha Takinawa: Here comes Mayor Adam West \n               himself. Mr. West do you have any words \n               for our viewers?Mayor Adam West: Box toaster\n               aluminum maple syrup... no I take that one \n               back. Im gonna hold onto that one. \n               Now MaxPower is adding adamant\n               so this example works\")\n\n      corpus = Corpus(VectorSource(textVector))\n      corpus = tm_map(corpus, tolower)\n      corpus = tm_map(corpus, PlainTextDocument) \n      corpus = tm_map(corpus, removePunctuation)\n      corpus = tm_map(corpus, removeWords, c(stopwords(\"english\")))\n      corpus = tm_map(corpus, stemDocument, language=\"english\")\n\n      dtm = DocumentTermMatrix(corpus)\n      mostFreqTerms = findFreqTerms(dtm, lowfreq=2) \n      mostFreqTerms\n</code></pre>\n\n<p>...The above mostFreqTerms outputs</p>\n\n<blockquote>\n  <p>[1] \"adam\" \"one\"   \"west\"</p>\n</blockquote>\n\n<p>I'm looking for a programmatic way to determine that the stem word \"adam\" came from original words \"adam\" and \"adamant\".</p>\n",
    "score": 5,
    "creation_date": 1430585998,
    "view_count": 1300,
    "answer_count": 1,
    "tags": "r;nlp;tm"
  },
  {
    "question_id": 29869607,
    "title": "How to tune a Machine Translation model with huge language model?",
    "body": "<p><code>Moses</code> is a software to build machine translation models. And <code>KenLM</code> is the defacto language model software that moses uses.</p>\n\n<p>I have a textfile with 16GB of text and i use it to build a language model as such:</p>\n\n<pre><code>bin/lmplz -o 5 &lt;text &gt; text.arpa\n</code></pre>\n\n<p>The resulting file (<code>text.arpa</code>) is 38GB. Then I binarized the language model as such:</p>\n\n<pre><code>bin/build_binary text.arpa text.binary\n</code></pre>\n\n<p>And the binarized language model (<code>text.binary</code>) grows to 71GB.</p>\n\n<p>In <code>moses</code>, after training the translation model, you should tune the weights of the model by using <code>MERT</code> algorithm. And this can simply be done with <a href=\"https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/mert-moses.pl\">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/mert-moses.pl</a>. </p>\n\n<p>MERT works fine with small language model but with the big language model, it takes quite some days to finish. </p>\n\n<p>I did a google search and found KenLM's filter, which promises to filter the language model to a smaller size: <a href=\"https://kheafield.com/code/kenlm/filter/\">https://kheafield.com/code/kenlm/filter/</a></p>\n\n<p>But i'm clueless as to how to make it work. The command help gives:</p>\n\n<pre><code>$ ~/moses/bin/filter\nUsage: /home/alvas/moses/bin/filter mode [context] [phrase] [raw|arpa] [threads:m] [batch_size:m] (vocab|model):input_file output_file\n\ncopy mode just copies, but makes the format nicer for e.g. irstlm's broken\n    parser.\nsingle mode treats the entire input as a single sentence.\nmultiple mode filters to multiple sentences in parallel.  Each sentence is on\n    a separate line.  A separate file is created for each sentence by appending\n    the 0-indexed line number to the output file name.\nunion mode produces one filtered model that is the union of models created by\n    multiple mode.\n\ncontext means only the context (all but last word) has to pass the filter, but\n    the entire n-gram is output.\n\nphrase means that the vocabulary is actually tab-delimited phrases and that the\n    phrases can generate the n-gram when assembled in arbitrary order and\n    clipped.  Currently works with multiple or union mode.\n\nThe file format is set by [raw|arpa] with default arpa:\nraw means space-separated tokens, optionally followed by a tab and arbitrary\n    text.  This is useful for ngram count files.\narpa means the ARPA file format for n-gram language models.\n\nthreads:m sets m threads (default: conccurrency detected by boost)\nbatch_size:m sets the batch size for threading.  Expect memory usage from this\n    of 2*threads*batch_size n-grams.\n\nThere are two inputs: vocabulary and model.  Either may be given as a file\n    while the other is on stdin.  Specify the type given as a file using\n    vocab: or model: before the file name.  \n\nFor ARPA format, the output must be seekable.  For raw format, it can be a\n    stream i.e. /dev/stdout\n</code></pre>\n\n<p>But when I tried the following, it gets stuck and does nothing:</p>\n\n<pre><code>$ ~/moses/bin/filter union lm.en.binary lm.filter.binary\nAssuming that lm.en.binary is a model file\nReading lm.en.binary\n----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n</code></pre>\n\n<p><strong>What should one do to the Language Model after binarization? Is there any other steps to manipulate large language models to reduce the\ncomputing load when tuning?</strong></p>\n\n<p><strong>What is the usual way to tune on a large LM file?</strong></p>\n\n<p><strong>How to use KenLM's filter?</strong></p>\n\n<p>(more details on <a href=\"https://www.mail-archive.com/moses-support@mit.edu/msg12089.html\">https://www.mail-archive.com/moses-support@mit.edu/msg12089.html</a>)</p>\n",
    "score": 5,
    "creation_date": 1429989628,
    "view_count": 1531,
    "answer_count": 1,
    "tags": "nlp;n-gram;machine-translation;moses;language-model"
  },
  {
    "question_id": 29864751,
    "title": "Sentence Similarity using WS4J",
    "body": "<p>I want to use ws4j to calculate similarity between two sentence.\nI am using the Online Demo of WS4J @ <a href=\"http://ws4jdemo.appspot.com/&lt;br/\" rel=\"nofollow noreferrer\">WS4J Online demo</a></p>\n\n<p>I am using the default example sentences given by WS4J. \nAfter entering the sentence and hitting on calculate similarity button, i am getting the following output:<img src=\"https://i.sstatic.net/kftrl.png\" alt=\"enter image description here\"></p>\n\n<p>Here i am getting the similarity between individual tokens of the sentence.<br/>\nHow do i proceed further from here.I want to get a single value (say 0.5 or 0.8) which denotes the similarity of these 2 sentences.<br/>\nIs there a standard way of proceeding from here or will i have to write my own algorithm?</p>\n",
    "score": 5,
    "creation_date": 1429962530,
    "view_count": 2930,
    "answer_count": 1,
    "tags": "java;nlp;wordnet;jaws-wordnet;ws4j"
  },
  {
    "question_id": 28399340,
    "title": "install issue with python - spacy package in anaconda environment",
    "body": "<p>I'm attempting to follow <a href=\"http://honnibal.github.io/spaCy/quickstart.html\" rel=\"nofollow\">this tutorial</a> to install the natural language processing package spaCy into a python 3 anaconda environment, windows 8</p>\n\n<p>I opened console, cd-ed to my site-packages folder, activated environment, pip-ed for install, everything seemed fine except I couldn't run the second command here</p>\n\n<pre><code>$ pip install spacy\n$ python -m spacy.en.download\n</code></pre>\n\n<p>Now I can successfully load the package but when I run the second line below, I get the following error</p>\n\n<pre><code>&gt;&gt;&gt; from spacy.en import English   #this works\n&gt;&gt;&gt; nlp = English()                #this doesn't\n\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"C:\\Users\\garrett\\Anaconda\\envs\\py3k\\lib\\site-packages\\spacy\\en\\__init__.py\", line 64, in __init__\n    get_lex_props=get_lex_props)\n  File \"spacy/vocab.pyx\", line 42, in spacy.vocab.Vocab.__init__ (spacy/vocab.cpp:2216)\nOSError: Directory C:\\Users\\garrett\\Anaconda\\envs\\py3k\\lib\\site-packages\\spacy\\en\\data\\vocab not found -- cannot load Vocab.\n</code></pre>\n\n<p>I think that it is due to the fact that I couldn't run <code>python -m spacy.en.download</code></p>\n\n<p><strong>Can anyone give me an idea of what <code>python -m spacy.en.download</code> is supposed to be doing?</strong></p>\n\n<p><strong>Can anyone provide a walkthrough for how to get spaCy installed in an anaconda environment?</strong></p>\n\n<p>here's the error I get after setting the directory, activating python env, running command. The first several times I tried, my spyder editor went unresponsive and I killed the console, the most recent time I got this error </p>\n\n<pre><code>$ cd C:\\Users\\garrett\\Anaconda\\envs\\py3k\\Lib\\site-packages\n$ C:\\Users\\garrett\\Anaconda\\envs\\py3k\\Lib\\site-packages&gt;activate py3k\n$ [py3k] C:\\Users\\garrett\\Anaconda\\envs\\py3k\\Lib\\site-packages&gt;python -m spacy.en.download\n\nMoving existing dir C:\\Users\\garrett\\Anaconda\\envs\\py3k\\Lib\\site-packages\\spacy\\en\\data to /tmp\nTraceback (most recent call last):\n  File \"C:\\Users\\garrett\\Anaconda\\envs\\py3k\\lib\\runpy.py\", line 160, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"C:\\Users\\garrett\\Anaconda\\envs\\py3k\\lib\\runpy.py\", line 73, in _run_code\n    exec(code, run_globals)\n  File \".\\spacy\\en\\download.py\", line 56, in &lt;module&gt;\n    plac.call(main)\n  File \".\\plac_core.py\", line 309, in call\n    cmd, result = parser_from(obj).consume(arglist)\n  File \".\\plac_core.py\", line 195, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \".\\spacy\\en\\download.py\", line 51, in main\n    shutil.move(DEST_DIR, '/tmp')\n  File \"C:\\Users\\garrett\\Anaconda\\envs\\py3k\\lib\\shutil.py\", line 521, in move\n    raise Error(\"Destination path '%s' already exists\" % real_dst)\nshutil.Error: Destination path '/tmp\\data' already exists\n</code></pre>\n\n<p>appreciate any help or advice you can provide</p>\n",
    "score": 5,
    "creation_date": 1423429444,
    "view_count": 5019,
    "answer_count": 1,
    "tags": "python-3.x;installation;nlp;anaconda;spacy"
  },
  {
    "question_id": 23178275,
    "title": "Text classification in python - (NLTK Sentence based)",
    "body": "<p>I need to classify text and i am using Text blob python module to achieve it.I can use either Naive Bayes classifier/Decision tree. I am concern about the below mentioned points.</p>\n<ol>\n<li><p>I Need to classify <strong>sentences</strong> as argument/ Not an argument. I am using two classifiers and training the model using apt data sets. My question is all about do i need to train the model with only keywords? or i can train the data set with all possible argument and non argument <strong>sample sentences</strong>? Which would be the best approach in terms of text classification accuracy and time to retrieve?</p>\n</li>\n<li><p>Since the classification would be either argument/not an argument, which classifier would fetch exact results? It is Naive Bayes /Decision tree/Positive Naive bayes?</p>\n</li>\n</ol>\n",
    "score": 5,
    "creation_date": 1397966487,
    "view_count": 1322,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;text-classification"
  },
  {
    "question_id": 23055468,
    "title": "spell checker uses language model",
    "body": "<p>I look for spell checker that could use language model.</p>\n\n<p>I know there is a lot of good spell checkers such as <a href=\"http://hunspell.sourceforge.net/\" rel=\"noreferrer\">Hunspell</a>, however as I see it doesn't relate to context, so it only token-based spell checker.</p>\n\n<p>for example,</p>\n\n<p><code>I lick eating banana</code></p>\n\n<p>so here at token-based level no misspellings at all, all words are correct, but there is no meaning in the sentence. However \"smart\" spell checker would recognize that \"lick\" is actually correctly written word, but may be the author meant \"like\" and then there is a meaning in the sentence.</p>\n\n<p>I have a bunch of correctly written sentences in the specific domain, I want to train \"smart\" spell checker to recognize misspelling and to learn language model, such that it would recognize that even thought \"lick\" is written correctly, however the author meant \"like\".</p>\n\n<p>I don't see that Hunspell has such feature, can you suggest any other spell checker, that could do so.</p>\n",
    "score": 5,
    "creation_date": 1397464397,
    "view_count": 1684,
    "answer_count": 2,
    "tags": "machine-learning;nlp;spell-checking;hunspell"
  },
  {
    "question_id": 18681052,
    "title": "Semantic parsing with NLTK",
    "body": "<p>I am trying to use NLTK for semantic parsing of spoken navigation commands such as\n\"go to San Francisco\", \"give me directions to 123 Main Street\", etc. </p>\n\n<p>This could be done with a fairly simple CFG grammar such as</p>\n\n<pre><code>S -&gt; COMMAND LOCATION\nCOMMAND -&gt; \"go to\" | \"give me directions to\" | ...\nLOCATION -&gt; CITY | STREET | ...\n</code></pre>\n\n<p>The problem is that this involves non-atomic (more than one word-long) literals such as \"go to\", which NLTK doesn't seem to be set up for (correct me if I am wrong). The parsing task has tagging as a prerequisite, and all taggers seem to always tag individual words. So, my options seem to be: </p>\n\n<p>a) Define a custom tagger that can assign non-syntactic tags to word sequences rather than individual words (e.g., \"go to\" : \"COMMAND\"). \nb) Use features to augment the grammar, e.g., something like: </p>\n\n<pre><code>COMMAND -&gt; VB[sem='go'] P[sem='to'] | ...\n</code></pre>\n\n<p>c) Use a chunker to extract sub-structures like COMMAND, then apply a parser to the result. Does NLTK allow chunker->parser cascading? </p>\n\n<p>Some of these options seem convoluted (hacks). Is there a good way? </p>\n",
    "score": 5,
    "creation_date": 1378621144,
    "view_count": 4874,
    "answer_count": 3,
    "tags": "python;parsing;nlp;nltk;chunking"
  },
  {
    "question_id": 18286106,
    "title": "How do I get the context of a sentence?",
    "body": "<p>There is a questionnaire that we use to evaluate the student knowledge level (we do this manually, as in a test paper). It consists of the following parts:</p>\n\n<ol>\n<li>Multiple choice</li>\n<li>Comprehension Questions (I.e: Is a spider an insect?)</li>\n</ol>\n\n<p>Now I have been given a task to make an expert system that will automate this. So basically we have a proper answer for this. But my problem is the \"comprehension questions\". I need to compare the context of their answer to the context of the correct answer.</p>\n\n<p>I already initially searched for the answer, but it seems like it's really a big task to do. What I have search so far is I can do this through NLP which is really new to me. Also, if I'm not mistaken, it seems like that I have to find a dictionary of all words that is possible for the examiner to answer.</p>\n\n<p>Am I on the right track? If no, please suggest of what should I do (study what?) or give me some links to the materials that I need. Also, should I make my own dictionary? Because the words that I will be using are in the Filipino language.</p>\n\n<p><b> Update: Comprehension question</b></p>\n\n<p>The comprehension section of the questionnaire contains one paragraph explaining a certain scenario. The questions are fairly simple. Here is an example:</p>\n\n<p>Bonnie's uncle told her to pick apples from the tree. Picking up a stick, she poked the fruits so they would fall. In the middle of doing this, a strong gust of wind blew. Due to her fear of the fruits falling on top of her head, she stopped what she was doing. After this, though, she noticed that the wind had caused apples to fall from the tree. These fallen apples were what she brought home to her uncle.</p>\n\n<p>The questions are:</p>\n\n<ol>\n<li>What did Bonnie's uncle tell her to do?</li>\n<li>What caused Bonnie to stop picking apples from the tree?</li>\n<li>Is Bonnie a good fruit picker? Please explain your answer.</li>\n</ol>\n\n<p>The possible answers that the answer key states are:</p>\n\n<p>For number 1: <br>\n1.1 Bonnie's uncle told her to pick apples from the tree <br>\n1.2 Get apples <br></p>\n\n<p>For number 2: <br>\n2.1 A strong gust of wind blew <br>\n2.2 She might get hit in the head by the fruits <br></p>\n\n<p>For number 3: <br>\n3.1 No, because the apples she got were already on the ground <br>\n3.2 No, because the wind was what caused the fruits to fall <br>\n3.3 Yes, because it is difficult to pick fruits when it's windy. <br>\n3.4 Yes, because at least she tried</p>\n\n<p>Now there are answers that were given to me. The job that the system shall be able to do is to compare the context of the student's answer to the context of the right answer in order for the system to successfully be able to grade the student's answer.</p>\n",
    "score": 5,
    "creation_date": 1376723063,
    "view_count": 4073,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 16927288,
    "title": "How to add compound words to the tagger in NLTK?",
    "body": "<p>So, I was wondering if anyone had any idea how to combine multiple terms to create a single term in the taggers in <a href=\"http://nltk.org/\" rel=\"nofollow\">NLTK.</a>.</p>\n\n<p>For example, when I do: </p>\n\n<pre><code>nltk.pos_tag(nltk.word_tokenize('Apple Incorporated is the largest company'))\n</code></pre>\n\n<p>It gives me:</p>\n\n<pre><code>[('Apple', 'NNP'), ('Incorporated', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('largest', 'JJS'), ('company', 'NN')]\n</code></pre>\n\n<p>How do I make it put 'Apple' and 'Incorporated' Together to be <code>('Apple Incorporated','NNP')</code></p>\n",
    "score": 5,
    "creation_date": 1370379240,
    "view_count": 1808,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 15760440,
    "title": "Anaphor resolution in Python",
    "body": "<p>I want to use Anaphor resolution in my project related to text mining. I am doing the project on the Linux platform and I am using Python. I have searched on net but there is no appropriate toolkit which can perform anaphora resolution in Python. Please suggest some toolkit which can do so.</p>\n\n<p>I want to find the noun of the pronoun. for e.g </p>\n\n<blockquote>\n  <p>Joey was having lunch. He was too hungry.</p>\n</blockquote>\n\n<p>I want the \"He\" in next sentence to be replaced by\"Joey\". </p>\n",
    "score": 5,
    "creation_date": 1364894478,
    "view_count": 860,
    "answer_count": 1,
    "tags": "python;linux;nlp"
  },
  {
    "question_id": 13387042,
    "title": "Are there any ML/NLP works/papers on parsing/solving math word problems?",
    "body": "<p>As the title says, any pointers are much appreciated.</p>\n\n<p>I am exploring, where do we stand in terms of ML/NLP efforts, in context of solving (to begin with - parsing) Math Word Problems.</p>\n\n<p>We have decent enough softwares in likes of Mathematica which can solve well formulated math equations.</p>\n\n<p>But when it comes to solving math problems expressed in natural languages, I could not find anything substantial.\nWhen I think about how to approach this, I see it as a sort of Machine Translation problem (translating from English to Math-equations), but there is hardly any 'labeled' data for that. Other approach can be semi (or un) sypervised Relation Extraction.</p>\n\n<p>Since these are just random thoughts, I want to start with some existing work/papers in this direction. My otherwise decent googling skills, didn't help much.</p>\n",
    "score": 5,
    "creation_date": 1352926035,
    "view_count": 1178,
    "answer_count": 2,
    "tags": "machine-learning;nlp"
  },
  {
    "question_id": 12376982,
    "title": "Check if string is a grammatically valid sentence?",
    "body": "<p>Is there a way to check if a string represents an English sentence? I am currently looking at java packages but I cannot find anything that does it yet.</p>\n\n<p>i.e.</p>\n\n<pre><code>the weather is good (valid)\nthe good is weather (invalid) \n</code></pre>\n\n<p>Any ideas?</p>\n\n<p>Thanks</p>\n",
    "score": 5,
    "creation_date": 1347393438,
    "view_count": 3296,
    "answer_count": 2,
    "tags": "java;nlp"
  },
  {
    "question_id": 11371476,
    "title": "Sentence segmentation tools to use when input sentence has no punctuation (is normalized)",
    "body": "<p>Suppose there is a sentence like \"find me some jazz music and play it\", where all the text is normalized and there are no punctuation marks (output of a speech recognition library). </p>\n\n<p>What online/offline tools can be used to do \"sentence segmentation\" other than the naive approach of splitting on conjunctions ?</p>\n\n<p>Input:</p>\n\n<blockquote>\n  <p>find me some jazz music and play it  </p>\n</blockquote>\n\n<p>Output:</p>\n\n<blockquote>\n  <p>find me some jazz music<br>\n  play it</p>\n</blockquote>\n",
    "score": 5,
    "creation_date": 1341623387,
    "view_count": 1291,
    "answer_count": 2,
    "tags": "nlp;text-segmentation"
  },
  {
    "question_id": 9710975,
    "title": "CJK Languages Pronunciation APIs",
    "body": "<p>Are there any good (preferably open) APIs or databases of pronunciation audio files for Chinese/Japanese/Korean languages? I’ve been looking around, but somehow couldn’t find anything other than <a href=\"http://api.forvo.com/\" rel=\"nofollow\">Forvo</a> or Google Translate. Both are an overkill for me, since I only need data for those languages, and only pronunciations, no translations.</p>\n",
    "score": 5,
    "creation_date": 1331762554,
    "view_count": 706,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 8470563,
    "title": "Efficient Lemmatizer that avoids dictionary lookup",
    "body": "<p>I want to convert string like 'eat' to 'eating', 'eats'. I searched and found the lemmatization as the solution, but all the lemmatizer tools that I have come across uses wordlist or dictionary-lookup. Is there any lemmatizer which avoids dictionary lookup and gives high efficiency, may be a lemmatizer that is based on rules. Yes and I am not looking for \"stemmer\".</p>\n",
    "score": 5,
    "creation_date": 1323670472,
    "view_count": 914,
    "answer_count": 1,
    "tags": "java;relevance;text-analysis;lemmatization"
  },
  {
    "question_id": 7490028,
    "title": "Finding words from Wordnet separated by a fixed Edit Distance from a given word",
    "body": "<p>I am writing a spell checker using nltk and wordnet, I have a few wrongly spelt words say \"belive\". What I want to do is find all words from wordnet that are separated by a leveshtein's edit distance of 1 or 2 from this given word. \nDoes nltk provide any methods to accomplish this? How to do this?</p>\n\n<hr>\n\n<p>May be, I put it wrongly. the <code>edit_distance</code> method takes 2 arguments like <code>edit_distance(word1,word2)</code> returns the levenshtein's distance between word1 and word2.\nWhat I want is to find edit distance between the word I give with every other word in wordnet.</p>\n",
    "score": 5,
    "creation_date": 1316543794,
    "view_count": 1503,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 5443553,
    "title": "Translation of Single Words, Taking into Account Context, using Computer Language Processing Tools",
    "body": "<p>I would like to automatically annotate texts for learners of foreign languages with translations of difficult words.</p>\n\n<p>For instance, if the original text is:</p>\n\n<blockquote>\n  <p>El gato esta en la casa de mis vecinos</p>\n</blockquote>\n\n<p>Becomes</p>\n\n<blockquote>\n  <p>El gato esta en la casa de mis <strong>vecinos</strong> (<em>neighbours</em>)</p>\n</blockquote>\n\n<p>The first step is to identify which words are the difficult ones. This could be done by lemmatization of the words in the original text and comparing them with a list of 'easy words' (a basic vocabulary of 1500-2000 words). Those not found in this list will be designated as 'hard words.' This process seems straightforward enough using the Natural Language Tool Kit (NLTK) for Python.</p>\n\n<p>There is some difficulty in words that must be translated as a pair, such as 'newly weds,' or phrasal verbs 'he <strong>called</strong> me <strong>up</strong>' or the German 'er <strong>ruft</strong> mich <strong>an</strong>' (anrufen). Here words can't be treated individually. For phrasal verbs and the like perhaps some understanding of grammer is needed.</p>\n\n<p>The second step involves obtaining a correct translation of the difficult words according to context in which they appear. As I understand, this is effectively applying the first half of a statistical machine translation system like google translate. I believe this problem could solved using the Google Translate Research API, that lets you send text to be translated, and the response includes information about which word in the translation corresponds to which word in the original text. So you could feed in the whole sentence and then fish out the word you wanted from the response. You have to apply to use this API however, and they have usage limits, which would likely be a problem for my application. I would rather find another solution. I expect no solution will give 100% correct translations and they will have to be checked by hand, but this should still speed things up.</p>\n\n<p>Thanks for your comments.</p>\n\n<p>David</p>\n",
    "score": 5,
    "creation_date": 1301156047,
    "view_count": 946,
    "answer_count": 1,
    "tags": "python;text;nltk;google-translate;nlp"
  },
  {
    "question_id": 4580877,
    "title": "Text segmentation: dictionary-based word splitting",
    "body": "<h2>Background</h2>\n\n<p>Split database column names into equivalent English text to seed a data dictionary. The English dictionary is created from a corpus of corporate documents, wikis, and email. The dictionary (<code>lexicon.csv</code>) is a CSV file with words and probabilities. Thus, the more often someone writes the word \"therapist\" (in email or on a wiki page) the higher the chance of \"therapistname\" splits to \"therapist name\" as opposed to something else. (The lexicon probably won't even include the word rapist.)</p>\n\n<h2>Source Code</h2>\n\n<ul>\n<li>TextSegmenter.java @ <a href=\"http://pastebin.com/taXyE03L\" rel=\"nofollow noreferrer\">http://pastebin.com/taXyE03L</a></li>\n<li>SortableValueMap.java @ <a href=\"http://pastebin.com/v3hRXYan\" rel=\"nofollow noreferrer\">http://pastebin.com/v3hRXYan</a></li>\n</ul>\n\n<h2>Data Files</h2>\n\n<ul>\n<li>lexicon.csv - <a href=\"http://pastebin.com/0crECtXY\" rel=\"nofollow noreferrer\">http://pastebin.com/0crECtXY</a></li>\n<li>columns.txt - <a href=\"http://pastebin.com/EtN9Qesr\" rel=\"nofollow noreferrer\">http://pastebin.com/EtN9Qesr</a></li>\n</ul>\n\n<h2>Problem (updated 2011-01-03)</h2>\n\n<p>When the following problem is encountered:</p>\n\n<pre><code>dependentrelationship::end depend ent dependent relationship\nend=0.86\nent=0.001\ndependent=0.8\nrelationship=0.9\n</code></pre>\n\n<p>These possible solutions exist:</p>\n\n<pre><code>dependentrelationship::dependent relationship\ndependentrelationship::dep end ent relationship\ndependentrelationship::depend ent relationship\n</code></pre>\n\n<p>The lexicon contains words with their relative probabilities (based on word frequency): <code>dependent 0.8</code>, <code>end 0.86</code>, <code>relationship 0.9</code>, <code>depend 0.3</code>, and <code>ent 0.001</code>.</p>\n\n<p>Eliminate the solution of <code>dep end ent relationship</code> because <code>dep</code> is not in the lexicon (i.e., 75% word usage), whereas the other two solutions cover 100% of words in the lexicon. Of the remaining solutions, the probability of <code>dependent relationship</code> is <em>0.72</em> whereas <code>depend ent relationship</code> is <em>0.00027</em>. We can therefore select <code>dependent relationship</code> as the correct solution.</p>\n\n<h2>Related</h2>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/3856630/how-to-separate-words-in-a-sentence-with-spaces\">How to separate words in a &quot;sentence&quot; with spaces?</a></li>\n<li><a href=\"https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjMzMTEzYTA5NTk3NjFjOTc\" rel=\"nofollow noreferrer\">Top Coder - Text Segmentation Presentation 1/2</a></li>\n<li><a href=\"https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjQ1YmFiZTNhODVjMzY2MmY\" rel=\"nofollow noreferrer\">Top Coder - Text Segmentation Presentation 2/2</a></li>\n<li><a href=\"https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjYyN2I4NTA0OTIxZDNlMGE\" rel=\"nofollow noreferrer\">Linear Text Segmentation using Dynamic Programming Algorithm</a></li>\n<li><a href=\"https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjUxZmFkNzc0MmMzOTc5ZmI\" rel=\"nofollow noreferrer\">Dynamic Programming: Segmentation</a></li>\n<li><a href=\"https://sites.google.com/site/djpdfstore/text-segmentation-02.pdf?attredirects=0&amp;d=1\" rel=\"nofollow noreferrer\">Dynamic Programming: A Computational Tool</a></li>\n</ul>\n\n<h2>Question</h2>\n\n<p>Given:</p>\n\n<pre><code>// The concatenated phrase or database column (e.g., dependentrelationship).\nString concat;\n\n// All words (String) in the lexicon within concat, in left-to-right order; and\n// the ranked probability of those words (Double). (E.g., {end, 0.97}\n// {dependent, 0.86}, {relationship, 0.95}.)\nMap.Entry&lt;String, Double&gt; word;\n</code></pre>\n\n<p>How would you implement a routine that generates the most likely solution based on lexicon coverage and probabilities? For example:</p>\n\n<pre><code>for( Map.Entry&lt;String, Double&gt; word : words ) {\n  result.append( word.getKey() ).append( ' ' );\n\n  // What goes here?\n\n  System.out.printf( \"%s=%f\\n\", word.getKey(), word.getValue() );\n}\n</code></pre>\n\n<p>Thank you!</p>\n",
    "score": 5,
    "creation_date": 1294007545,
    "view_count": 7579,
    "answer_count": 3,
    "tags": "java;nlp;data-dictionary;text-segmentation"
  },
  {
    "question_id": 79344668,
    "title": "word reduction of a list of words",
    "body": "<p>Besides using a chatbot like o1 mini, is there more local way to reduce a list of words from 10 words down to 3 words? I feel that due to context, word association, semantic meanings of the words, a there has to be some form of extrapolation to reduce the list but so far my attempts have not been nearly as good.</p>\n<p>The ultimate goal is to reduce a list to something that can be searchable.</p>\n<p>example using chatgpt 1o mini</p>\n<p>reduce the list of words to 3 terms able to used in search. Combine words if they make sense in context to all other words in the list. Each term should combine words meaningfully, and no word should be repeated across the terms. return in list form</p>\n<pre><code>[&quot;rpg&quot;, &quot;role playing&quot;, &quot;fantasy&quot;, &quot;monster&quot;, &quot;Dungeons&quot;, &quot;dragons&quot;, &quot;master&quot;, &quot;monster&quot;, &quot;job&quot;, &quot;class&quot;]\n</code></pre>\n<blockquote>\n<p>result: Dungeons Dragons, Role Playing, Monster Class</p>\n</blockquote>\n<p>here are the attempts I tried using python</p>\n<p>attempt 1: UMAP, Cosine similarity</p>\n<pre><code>from sklearn.metrics.pairwise import cosine_similarity\n\ndef concatenate_overlap(list_of_list):\n    pooled = [set(subList) for subList in list_of_list]\n    merging = True\n    while merging:\n        merging = False\n        for i, group in enumerate(pooled):\n            merged = next((g for g in pooled[i + 1:] if g.intersection(group)), None)\n            if not merged: \n                continue\n            group.update(merged)\n            pooled.remove(merged)\n            merging = True\n    return [list(x) for x in pooled]\n\n\ntext_list = [&quot;rpg&quot;, &quot;role playing&quot;, &quot;fantasy&quot;, &quot;monster&quot;, &quot;Dungeons&quot;, &quot;dragons&quot;, &quot;master&quot;, &quot;monster&quot;, &quot;job&quot;, &quot;class&quot;]\n\nMODEL_NAME = 'Alibaba-NLP/gte-multilingual-base'\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\nbatch_dict = tokenizer(text_list, max_length=8192, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**batch_dict)\nembeddings = outputs.last_hidden_state[:, 0][:768]\nsimilarity_matrix = cosine_similarity(embeddings.detach())\nnp.fill_diagonal(similarity_matrix, 0)\nto_merge_list: List[List[int]] = []\nfor idx, topic_similarity_scores in enumerate(similarity_matrix):\n    similar_words = list(np.where(0.8&gt; min_similarity)[0])\n    similar_words.append(idx)\n    to_merge_list.append(similar_words)\n\nto_concat = concatenate_overlap(to_merge_list)\nwords = [text_list[a[0]] for a in to_concat]\n\n</code></pre>\n<blockquote>\n<p>result: ['rpg', 'monster', 'Dungeons', 'dragons', 'master', 'monster', 'job', 'class']</p>\n</blockquote>\n<p>attempt 2: summurization model</p>\n<pre><code>summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)\n\nword_list = [&quot;rpg&quot;, &quot;role playing&quot;, &quot;fantasy&quot;, &quot;monster&quot;, &quot;Dungeons&quot;, &quot;dragons&quot;, &quot;master&quot;, &quot;monster&quot;, &quot;job&quot;, &quot;class&quot;]\ntext = &quot;, &quot;.join(word_list)\nsummarizer(text, max_length=50, do_sample=False)\n</code></pre>\n<blockquote>\n<p>result: rpgs, role playing, fantasy, monster, Dungeons, dragons, master, monster. job, class. rpg, roleplaying, fantasy,. monster, dungeons, dragons,. master, monsters, master. job,. class</p>\n</blockquote>\n<p>edit: the criteria is simple, shrink the word list to a 3 or 4 word phrase that can be searchable. In the case of chatgpt 01, it gave me 3 multi word expressions for searching. For the two examples. Ideally it would give me something also as small.</p>\n<p>I plan to take word lists and boil them down to searchable prompts</p>\n<p>edit 2:\nto provide more context to what the expected outcome should be here is a unit test and explanation.</p>\n<pre><code>input: [&quot;bad&quot;, &quot;horrible&quot;, &quot;unfun&quot;, &quot;waste&quot;, &quot;big&quot;, &quot;mutha&quot;, &quot;truckers&quot;, &quot;racing&quot;, &quot;incomplete&quot;, &quot;big mutha&quot;]\n\noutput: [&quot;big mutha truckers&quot;, &quot;unfun&quot;, &quot;incomplete&quot;]\n</code></pre>\n<p>contextually, the list of 10 words is focusing on a game called &quot;big mutha truckers&quot; and as such this would get pulled from the word list. The second would be all the synonyms which refer to the game being not good, so &quot;unfun&quot; or &quot;bad&quot; or even &quot;horrible racing&quot;. Lastly &quot;incomplete&quot; would get pulled as it isn't a close synonym of bad and is is descriptive</p>\n<p>with these 3 words, I can throw it into google and ideally search results of negative reviews for big mutha truckers should appear.</p>\n",
    "score": 5,
    "creation_date": 1736485950,
    "view_count": 79,
    "answer_count": 0,
    "tags": "python;nlp"
  },
  {
    "question_id": 77701433,
    "title": "ValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead",
    "body": "<p>I am trying to run a GitHub project on my computer.<br>\n<a href=\"https://github.com/suryanshgupta9933/Law-GPT\" rel=\"noreferrer\">GitHub Repo that I am trying to run</a>\n<br>This is the code snippet that is causing errors.\n<br>Steps I took for replicating the project are:</p>\n<blockquote>\n<ol>\n<li>Cloned the repository.</li>\n<li>Generated the Hugging Face access token</li>\n<li>Added <code>offload_folder</code> and <code>offload_dict_state</code> after reading the Hugging Face guide to load huge models.</li>\n</ol>\n</blockquote>\n<pre><code>def load_llm():\n    &quot;&quot;&quot;\n    Load the LLM\n    &quot;&quot;&quot;\n    # Model ID\n    repo_id = 'meta-llama/Llama-2-7b-chat-hf'\n    login(token=&quot;hf_xxxxxxxx&quot;)\n    # Load the model\n    model = AutoModelForCausalLM.from_pretrained(\n        repo_id,\n        device_map='auto',\n        load_in_4bit=False,\n        token = True,\n        offload_folder = r&quot;C:\\Users\\DHRUV\\Desktop\\New folder\\Law-GPT&quot;,\n        offload_state_dict = True\n    )\n\n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        repo_id,\n        use_fast=True\n    )\n\n    # Create pipeline\n    pipe = pipeline(\n        'text-generation',\n        model=model,\n        tokenizer=tokenizer,\n        max_length=512\n    )\n\n    # Load the LLM\n    llm = HuggingFacePipeline(pipeline=pipe)\n\n    return llm\n</code></pre>\n<p>The Error I am facing, Please help:</p>\n<pre><code>Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to C:\\Users\\DHRUV\\.cache\\huggingface\\token\nLogin successful\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;?, ?it/s]\nTraceback (most recent call last):\n  File &quot;C:\\Users\\DHRUV\\Desktop\\New folder\\Law-GPT\\app.py&quot;, line 5, in &lt;module&gt;\n    chain = qa_pipeline()\n  File &quot;C:\\Users\\DHRUV\\Desktop\\New folder\\Law-GPT\\utils.py&quot;, line 100, in qa_pipeline\n    llm = load_llm()\n  File &quot;C:\\Users\\DHRUV\\Desktop\\New folder\\Law-GPT\\utils.py&quot;, line 44, in load_llm\n    model = AutoModelForCausalLM.from_pretrained(\n  File &quot;C:\\Users\\DHRUV\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py&quot;, line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File &quot;C:\\Users\\DHRUV\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\modeling_utils.py&quot;, line 3773, in from_pretrained\n    dispatch_model(model, **device_map_kwargs)\n  File &quot;C:\\Users\\DHRUV\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\accelerate\\big_modeling.py&quot;, line 438, in dispatch_model\n    raise ValueError(\nValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\n</code></pre>\n",
    "score": 5,
    "creation_date": 1703211929,
    "view_count": 3362,
    "answer_count": 1,
    "tags": "python;nlp;large-language-model;pre-trained-model;llama"
  },
  {
    "question_id": 76886176,
    "title": "Langchain AgentExecutor Won&#39;t Use Any Tools",
    "body": "<p>I'm using the tiiuae/falcon-40b-instruct off HF, and I am trying to incorporate it with LangChain ReAct. I'm using a regular LLMChain with a StringPromptTemplate that's just the standard Thought/Action/Action Input/Observation prompt with some few shot examples incorporated (I've tried with and without the examples, it doesn't seem to work either way). The LLMChain is part of an LLMSingleActionAgent, which is then run by an AgentExecutor.</p>\n<p>The problem is, the executor doesn't seem to be capable of using any tools. Most of the time, it is capable of choosing a correct tool, but will just hallucinate the output of the tool. With debug mode on, it is very clear it most of the time doesn't actually enter the tool/start or tool/end chains. Some prior posts about similar issues recommended raising the model's temperature. With the higher temperature, it maybe 5%-10% of the time will enter the tool chains, but the only thing that's returned is <code>Invalid or incomplete response.</code></p>\n<p>I don't have a good idea exactly where something could be going wrong, but here's how my chain and agents are being defined, as a starting point for anyone interested, and a Github repo with the full code <a href=\"https://github.com/kennlw/langchain-help/tree/main\" rel=\"noreferrer\"><code>here</code></a>:</p>\n<pre><code># LLM chain consisting of the LLM and a prompt\nllm_chain = LLMChain(llm=local_llm, prompt=prompt)\n\ntool_names = [tool.name for tool in tools]\n\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain, \n    output_parser=CustomOutputParser(),\n    stop=[&quot;\\nObservation:&quot;], \n    allowed_tools=tool_names\n)\n# handle_parsing_errors=&quot;Check your output and make sure it conforms!&quot;\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, \n                                                    tools=tools, \n                                                    verbose=True)\n</code></pre>\n<p>I've tried using structured tools and adjusting the prompt. However, the same error occurs with the executor just not using the tools available. I've attached an image of the error here.<a href=\"https://i.sstatic.net/WcWw3.png\" rel=\"noreferrer\">Model hallucinating and not using tools.</a></p>\n",
    "score": 5,
    "creation_date": 1691780803,
    "view_count": 3744,
    "answer_count": 0,
    "tags": "python;machine-learning;nlp;langchain;py-langchain"
  },
  {
    "question_id": 71865293,
    "title": "Extracting full-text research papers",
    "body": "<p>I am working on an NLP project which deals with full-text research papers. I have a list of DOIs and I want to store all the text of these research papers in one .txt file. Currently, I am downloading the pdfs from scihub, and then extracting text from these pdfs. But, this is very slow, especially when I have a lot of papers. Are there better alternatives?</p>\n<p>At a high-level, this is how I get the text for one paper:</p>\n<pre><code>!python -m PyPaperBot --doi=&quot;10.2196/29324&quot; --dwn-dir=&quot;C:\\&quot;\n</code></pre>\n<p>and then</p>\n<pre><code>with fitz.open(&quot;sample.pdf&quot;) as doc:\n    text = &quot;&quot;\n    for page in doc:\n        text += page.get_text()\n</code></pre>\n<p>I took a look at some related questions (<a href=\"https://stackoverflow.com/questions/30904755/extract-abstract-full-text-from-scientific-literature-given-doi-or-title\">Extract abstract / full text from scientific literature given DOI or Title</a>), but they seem very outdated. I've also looked into PubMedCentral, but it has a smaller database of research papers than sci-hub.</p>\n",
    "score": 5,
    "creation_date": 1649897760,
    "view_count": 1691,
    "answer_count": 1,
    "tags": "python;web-scraping;nlp"
  },
  {
    "question_id": 71327693,
    "title": "How to disable seqeval label formatting for POS-tagging",
    "body": "\n<p>I am trying to evaluate my POS-tagger using huggingface's implementation of the <code>seqeval</code> metric but, since my tags are not made for NER, they are not formatted the way the library expects them. Consequently, when I try to read the results of my classification report, the labels for class-specific results consistently lack the first character (the last if I pass <code>suffix=True</code>).</p>\n<p>Is there a way to disable entity recognition in labels or do I have to pass all my labels with a starting space to solve this issue? (Given that the library is supposed to be suitable for POS-tagging, I hope there is a built-in solution)</p>\n<h3>SSCCE:</h3>\n<pre class=\"lang-py prettyprint-override\"><code>from seqeval.metrics import accuracy_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score\n\ny_true = [['INT', 'PRO', 'PRO', 'VER:pres'], ['ADV', 'PRP', 'PRP', 'ADV']]\ny_pred = [['INT', 'PRO', 'PRO', 'VER:pres'], ['ADV', 'PRP', 'PRP', 'ADV']]\n\nprint(classification_report(y_true, y_pred))\n</code></pre>\n<h3>Output:</h3>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: right;\"></th>\n<th style=\"text-align: right;\">precision</th>\n<th style=\"text-align: right;\">recall</th>\n<th style=\"text-align: right;\">f1-score</th>\n<th style=\"text-align: right;\">support</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: right;\">DV</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">2</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">ER:pres</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">NT</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">RO</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">RP</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">micro avg</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">6</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">macro avg</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">6</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">weighted avg</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">6</td>\n</tr>\n</tbody>\n</table>\n</div>",
    "score": 5,
    "creation_date": 1646246634,
    "view_count": 1322,
    "answer_count": 0,
    "tags": "python;nlp;huggingface-transformers;pos-tagger;huggingface-datasets"
  },
  {
    "question_id": 70592181,
    "title": "How to prevent NER model to overfit entity position with Spacy",
    "body": "<p>I'm building a custom NER model to detect brands in product titles using a well sized dataset of 98k products with their corresponding title, the train contains around 84k records, validation split 10k and test split 3k.\nThe only problem with the dataset is that 89% of all product titles have the brand as their first words.</p>\n<p>When training the NER model from scratch, it gives a good F1 score of 85% after just few epochs (batch size =32), however when testing the model I noticed the follwoign :</p>\n<ul>\n<li>The model is strongly biased toward predicting the first word of the title as a brand</li>\n<li>The model is very good at detecting brands when they occur as first words, but is quite weak for titles having brands in their middle or end.</li>\n</ul>\n<p>I had the idea to solve this by resampling the dataset and removing some brands from some titles as their first word and put it at the end or the middle.</p>\n<p>However, I would like to know if there is a technique in NLP that allow the model to not give a large importance to the entity position in the text ? I used dropout of 0.6 but with no success</p>\n",
    "score": 5,
    "creation_date": 1641382538,
    "view_count": 675,
    "answer_count": 0,
    "tags": "nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 70367348,
    "title": "Spacy alignment differences when training with DocBin vs custom data reading and batching",
    "body": "<p>I am just getting started with training Spacy named entity recognition models, and following the basic example described <a href=\"https://spacy.io/usage/training#training-data\" rel=\"nofollow noreferrer\">here</a>, where you create training examples by instantiating <code>Doc</code> objects and serializing those with <code>DocBin</code>.</p>\n<p>My custom <code>preprocess.py</code> file looks like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>\nif __name__ == '__main__':\n    nlp = spacy.blank(&quot;en&quot;)\n    counter = 0\n\n    db = DocBin()\n\n    with open(sys.argv[1], 'r') as fp:\n        line = fp.readline()\n        while line:\n\n            record = MyRecord.build(json.loads(line))\n\n            doc = record.to_spacy_doc(nlp=nlp)\n            # internally, something like:\n            # # char-level indices\n            # ent = doc.char_span(0, 5, label='SOMETHING') \n            # doc.set_ents([ent])\n\n            db.add(doc)\n\n            counter += 1\n            # hacky way to save 1000 docs in each DocBin\n            if counter == 1000:\n                db.to_disk(&quot;./train.spacy&quot;)\n                db = DocBin()\n\n            if counter == 2000:\n                db.to_disk(&quot;./dev.spacy&quot;)\n                break\n            line = fp.readline()\n</code></pre>\n<p>Then run the training script with a command like this:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>python -m spacy train config.cfg --output ./output --paths.train train.spacy --paths.dev dev.spacy\n</code></pre>\n<p>This seems to work well enough. However, I then read that you can write custom data loader functions by writing and registering a generator that yields instances of <code>Example</code> in a process described <a href=\"https://spacy.io/usage/training#custom-code-readers-batchers\" rel=\"nofollow noreferrer\">here</a>. This interests me, as in theory you could read from larger-than-RAM files during the training loop.</p>\n<p>I wrote such a generator myself in <code>functions.py</code>, that yields <code>Example</code> instances from an external data source (which happens to be custom JSONL from disk) where I have some known entity labels (with char-level indices):</p>\n<pre class=\"lang-py prettyprint-override\"><code>@spacy.registry.readers(&quot;corpus_variants.v1&quot;)\ndef stream_data(source: str) -&gt; Callable[[Language], Iterator[Example]]:\n    def generate_stream(nlp: Language):\n        counter = 0\n        with open(source, 'r') as fp:\n            line = fp.readline()\n            while line:\n\n                record = MyRecord.build(json.loads(line))\n\n                #doc = nlp(record.text)\n                doc = nlp.make_doc(record.doc_with_annotations.text)\n\n                entities = [\n                    (start, end, label)     # char-level offets (not token-level)\n                    for start, end, label, _\n                    in record.get_entity_tuples()\n                ]\n\n                gold_dict = dict(\n                    entities=entities\n                )\n\n                example = Example.from_dict(doc, gold_dict)\n                yield example\n\n                counter += 1\n                # arbitrarily stop at 20 for debugging purposes, but ideally stream the very large file\n                if counter &gt; 20:\n                    break\n                line = fp.readline()\n    return generate_stream\n</code></pre>\n<p>I also modified <code>config.cfg</code> to contain the following:</p>\n<pre><code>[corpora.dev]\n@readers = &quot;corpus_variants.v1&quot;\nsource = &quot;dev.jsonl&quot;\n\n[corpora.train]\n@readers = &quot;corpus_variants.v1&quot;\nsource = &quot;train.jsonl&quot;\n</code></pre>\n<p>When I run the training command:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>python -m spacy train config.cfg --output ./output --code functions.py\n</code></pre>\n<p>I get many <code>UserWarning: [W030] Some entities could not be aligned in the text</code> warnings. I've read a few posts on the <a href=\"https://github.com/explosion/spaCy/issues/5727\" rel=\"nofollow noreferrer\">theory behind these warnings</a>, but I am curious as to why this behavior does not occur when I am saving <code>DocBin</code>s? Is the alignment actually different or do the warnings occur only when explicitly creating <code>Example</code> instances?</p>\n<p>I am interested in getting the custom data loader working with this data, but also interested in alternative approaches that essentially allow me to stream arbitrary lines from a (larger-than-RAM) file as training examples.</p>\n<p>Finally, what might also be relevant to answering this question is understanding the differences between training a fresh (NER) model from scratch vs. updating an existing one. If I understand Spacy pipelines correctly, there might be some alignment advantages to updating an existing model since the same tokenizer can be used when assembling training examples and during inference.</p>\n",
    "score": 5,
    "creation_date": 1639586818,
    "view_count": 796,
    "answer_count": 0,
    "tags": "python;nlp;spacy;named-entity-recognition;spacy-3"
  },
  {
    "question_id": 67638993,
    "title": "Retrieving attention weights for sentences? Most attentive sentences are zero vectors",
    "body": "<p>I have a document classification task, that classifies documents as good (1) or bad (0), and I use some sentence embeddings for each document to classify the documents accordingly.</p>\n<p><strong>What I like to do is retrieving the attention scores for each document, to obtain the most &quot;relevant&quot; sentences (i.e., those with high attention scores)</strong></p>\n<p><em>I padded each document to the same length</em> (i.e., 1000 sentences per document). So my tensor for 5000 documents looks like this <code>X = np.ones(shape=(5000, 1000, 200))</code> (5000 documents with each having a 1000 sequence of sentence vectors and each sentence vector consisting of 200 features).</p>\n<p>My network looks like this:</p>\n<pre><code>no_sentences_per_doc = 1000\nsentence_embedding = 200\n\nsequence_input  = Input(shape=(no_sentences_per_doc, sentence_embedding))\ngru_layer = Bidirectional(GRU(50,\n                          return_sequences=True\n                          ))(sequence_input)\nsent_dense = Dense(100, activation='relu', name='sent_dense')(gru_layer)  \nsent_att,sent_coeffs = AttentionLayer(100,return_coefficients=True, name='sent_attention')(sent_dense)\npreds = Dense(1, activation='sigmoid',name='output')(sent_att)  \nmodel = Model(sequence_input, preds)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=[TruePositives(name='true_positives'),\n                      TrueNegatives(name='true_negatives'),\n                      FalseNegatives(name='false_negatives'),\n                      FalsePositives(name='false_positives')\n                      ])\n\nhistory = model.fit(X, y, validation_data=(x_val, y_val), epochs=10, batch_size=32)\n</code></pre>\n<p>After training I retrieved the attention scores as follows:</p>\n<pre><code>sent_att_weights = Model(inputs=sequence_input,outputs=sent_coeffs)\n\n## load a single sample\n## from file with 150 sentences (one sentence per line)\n## each sentence consisting of 200 features\nx_sample = np.load(x_sample)\n## and reshape to (1, 1000, 200)\nx_sample = x_sample.reshape(1,1000,200) \n\noutput_array = sent_att_weights.predict(x_sample)\n</code></pre>\n<p><em>However,</em> if I show the top 3 attention scores for the sentences, I also obtain sentence indices that are, for example, <code>[432, 434, 999]</code> for a document that has only 150 sentences (the rest is padded, i.e., just zeros).</p>\n<p><strong>Does that make sense or am I doing something wrong here?</strong> (is there a mistake in my attention layer? Or is due to a low F-score?)</p>\n<p>The attention layer I use is the following:</p>\n<pre><code>class AttentionLayer(Layer):\n    &quot;&quot;&quot;\n    https://humboldt-wi.github.io/blog/research/information_systems_1819/group5_han/\n    &quot;&quot;&quot;\n    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n        # Initializer \n        self.supports_masking = True\n        self.return_coefficients = return_coefficients\n        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n        self.attention_dim = attention_dim\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        # Builds all weights\n        # W = Weight matrix, b = bias vector, u = context vector\n        assert len(input_shape) == 3\n        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n        self.trainable_weights = [self.W, self.b, self.u]\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, hit, mask=None):\n        # Here, the actual calculation is done\n        uit = K.bias_add(K.dot(hit, self.W),self.b)\n        uit = K.tanh(uit)\n        \n        ait = K.dot(uit, self.u)\n        ait = K.squeeze(ait, -1)\n        ait = K.exp(ait)\n        \n        if mask is not None:\n            ait *= K.cast(mask, K.floatx())\n\n        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        ait = K.expand_dims(ait)\n        weighted_input = hit * ait\n        \n        if self.return_coefficients:\n            return [K.sum(weighted_input, axis=1), ait]\n        else:\n            return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        if self.return_coefficients:\n            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n        else:\n            return input_shape[0], input_shape[-1]\n</code></pre>\n<p>Note that I use <code>keras</code> with <code>tensorflow</code> backend version 2.1.; the attention layer was originally written for theano, but I use <code>import tensorflow.keras.backend as K</code></p>\n",
    "score": 5,
    "creation_date": 1621607838,
    "view_count": 457,
    "answer_count": 0,
    "tags": "python;tensorflow;keras;nlp;attention-model"
  },
  {
    "question_id": 67440324,
    "title": "Pytorch TypeError: only integer tensors of a single element can be converted to an index",
    "body": "<p>I am trying to use pytorch in a case for nlp binary classification and i need help to finish the neural network training and validate. I am newbie and this is my first time using pytorch, see the code below and the error...</p>\n<pre><code>X_train_tensor = torch.from_numpy(np.asarray(X_train)).type(torch.FloatTensor).to(device)\ny_train_tensor = torch.from_numpy(np.asarray(y_train)).type(torch.FloatTensor).unsqueeze(1).to(device)\n\nX_valid_tensor = torch.from_numpy(np.asarray(X_valid)).type(torch.FloatTensor).to(device)\ny_valid_tensor = torch.from_numpy(np.asarray(y_valid)).type(torch.FloatTensor).unsqueeze(1).to(device)\n\n\n\nX_train_tensor.size()\n</code></pre>\n<p>out: torch.Size([5438, 768])</p>\n<pre><code>y_train_tensor.size()\n</code></pre>\n<p>out: torch.Size([5438, 1])</p>\n<pre><code>criterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n\n\ndef binary_acc(preds, y_valid):\n    \n    y_valid_tag = torch.round(preds)\n\n    correct_results = (y_valid_tag == y_valid).float()\n    acc = correct_results.sum() / len(correct_results)\n    \n    return acc\n\n\ndef train(model, *var):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for x in range(X_train_tensor):\n    \n        optimizer.zero_grad() \n    \n        predictions = model(X_train_tensor)\n        loss = criterion(predictions, y_train_tensor) \n        acc = binary_acc(predictions, y_valid_tensor)\n    \n        loss.backward()\n        optimizer.step()\n    \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n    \n    return epoch_loss / len(X_train_tensor), epoch_acc / len(X_train_tensor)\n\n\n\n\ndef evaluate(model, *var):\n\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for X in range(X_valid_tensor):\n            predictions = model(X_train_tensor)\n            acc = binary_acc(predictions, y_valid_tensor)\n        \n            epoch_acc += acc.item()\n        \n    return epoch_acc / len(X_valid_tensor)\n\n\n\n\nloss=[]\nacc=[]\nval_acc=[]\n\nfor epoch in range(10):\n    \n    train_loss, train_acc = train(model, X_train_tensor, y_train_tensor, y_valid_tensor)\n    valid_acc = evaluate(model, X_valid_tensor, y_valid_tensor)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n    \n    loss.append(train_loss)\n    acc.append(train_acc)\n    val_acc.append(valid_acc)\n</code></pre>\n<p>THE OUTPUT ERROR: <strong>TypeError: only integer tensors of a single element can be converted to an index</strong></p>\n<p><strong>Please help me to fix this</strong></p>\n<pre><code>TypeError                                 Traceback (most recent call last)\n&lt;ipython-input-46-bfd7a45f13aa&gt; in &lt;module&gt;\n      5 for epoch in range(10):\n      6 \n----&gt; 7     train_loss, train_acc = train(model, X_train_tensor, y_train_tensor, y_valid_tensor)\n      8     valid_acc = evaluate(model, X_valid_tensor, y_valid_tensor)\n      9 \n\n&lt;ipython-input-44-689c66e0e9ed&gt; in train(model, *var)\n      6     model.train()\n      7 \n----&gt; 8     for x in range(X_train_tensor):\n      9 \n     10         optimizer.zero_grad()\n\nTypeError: only integer tensors of a single element can be converted to an index\n</code></pre>\n",
    "score": 5,
    "creation_date": 1620413660,
    "view_count": 10155,
    "answer_count": 1,
    "tags": "python;neural-network;nlp;pytorch;tensor"
  },
  {
    "question_id": 67283520,
    "title": "Using a TPus with SpaCy",
    "body": "<p>Is it possible to use a tpu in spacy... I know that you can use a gpu with <code>spacy.prefer_gpu()</code>. Is there something similar to this for tpu? Thanks in advance!</p>\n",
    "score": 5,
    "creation_date": 1619528222,
    "view_count": 436,
    "answer_count": 0,
    "tags": "nlp;spacy;tpu"
  },
  {
    "question_id": 66516359,
    "title": "How to train Spacy3 project with FP16 mixed precision",
    "body": "<p><strong>The goal is to run <code>python -m spacy train</code> with FP16 mixed precision</strong> to enable the use of large transformers (<code>roberta-large</code>, <code>albert-large</code>, etc.) in limited VRAM (RTX 2080ti 11 GB).</p>\n<p>The new Spacy3 <a href=\"https://spacy.io/usage/projects#project-yml\" rel=\"nofollow noreferrer\">project.yml approach</a> to training directly uses <a href=\"https://huggingface.co/models?filter=pytorch\" rel=\"nofollow noreferrer\">Huggingface-transformers models</a> loaded via <a href=\"https://github.com/explosion/spacy-transformers\" rel=\"nofollow noreferrer\">Spacy-transformers v1.0</a>. Huggingface models can be run with mixed precision just by adding the <code>--fp16</code> flag (<a href=\"https://huggingface.co/transformers/examples.html#distributed-training-and-mixed-precision\" rel=\"nofollow noreferrer\">as described here</a>).</p>\n<p>The spacy config was generated using <code>python -m spacy init config --lang en --pipeline ner --optimize efficiency --gpu -F default.cfg</code>, and checked to be complete by <code>python -m spacy init fill-config default.cfg config.cfg --diff</code>. Yet no FP16 / mixed-precision is to be found.</p>\n<h3>To reproduce</h3>\n<p>Use the <a href=\"https://github.com/explosion/projects/tree/v3/pipelines/ner_wikiner\" rel=\"nofollow noreferrer\">spaCy Project: Named Entity Recognition (WikiNER)</a> with changed <code>init-config</code> in <code>project.yml</code> to use a GPU and a transformer (<code>roberta-base</code> by default):</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>commands:\n  -\n    name: init-config\n    help: &quot;Generate a transformer English NER config&quot;\n    script:\n      - &quot;python -m spacy init config --lang en --pipeline ner --gpu -F --optimize efficiency -C configs/${vars.config}.cfg&quot;\n</code></pre>\n<h3>What was tested</h3>\n<ul>\n<li>Added <code>--fp16</code> to <code>python -m spacy project run</code></li>\n<li>Added <code>--fp16</code> to <code>python -m spacy train</code></li>\n<li>Added <code>fp16 = true</code> to <code>default.cfg</code> in various sections (<code>[components.transformer], [components.transformer.model], [training], [initialize]</code>)</li>\n</ul>\n<p>The logic was <code>transformers</code> are run in FP16 as such:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import TrainingArguments\nTrainingArguments(..., fp16=True, ...)\n</code></pre>\n<h3>SW stack specifics</h3>\n<pre class=\"lang-sh prettyprint-override\"><code> - spacy              3.0.3\n - spacy-transformers 1.0.1\n - transformers       4.2.2\n - torch              1.6.0+cu101\n</code></pre>\n",
    "score": 5,
    "creation_date": 1615118965,
    "view_count": 362,
    "answer_count": 0,
    "tags": "nlp;pytorch;spacy;huggingface-transformers;spacy-3"
  },
  {
    "question_id": 65591064,
    "title": "How to prepare custom training data for LayoutLM",
    "body": "<p>I want to train a LayoutLM through huggingface transformer, however I need help in creating the training data for LayoutLM from my pdf documents.</p>\n",
    "score": 5,
    "creation_date": 1609913883,
    "view_count": 2464,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers"
  },
  {
    "question_id": 64064193,
    "title": "Giving higher weight to a labeling function in Snorkel",
    "body": "<p>I am using snorkel to create labels for my training data. I currently have five labeling functions for the task which I have stored in a list. I am using the following code to apply the labeling function:</p>\n<pre><code>lfs = [lf_a, lf_b, lf_c, lf_d, lf_e]\napplier = PandasLFApplier(lfs)\nL_train = applier.apply(df_data_sample)\n\n# Train the label model and compute the training labels\nlabel_model = LabelModel(cardinality=2, verbose=True)\nlabel_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n</code></pre>\n<p>going into the task I want to give a higher weight to <em>lf_e</em> labeling function as I my testing shows it has a higher accuracy than others. Not being able to do that leads to outputs from other lfs dominating output from <em>lf_e</em>. And I also don't want to remove any of the functions as I reduce my coverage if I do that.</p>\n<p>Is there a way to do that in Snorkel?</p>\n",
    "score": 5,
    "creation_date": 1601037031,
    "view_count": 222,
    "answer_count": 0,
    "tags": "python;nlp;labeling;snorkel;semisupervised-learning"
  },
  {
    "question_id": 62526917,
    "title": "Is there a particular range for good perplexity value in NLP?",
    "body": "<p>I'm fine-tuning a language model and am calculating training and validation losses along with the training and validation perplexities. It s calculated by taking the exponential of the loss, in my program. I'm aware that lower perplexities represent better language models and is wondering what the range of values are for a good model. Any help is appreciated. Thank you.</p>\n",
    "score": 5,
    "creation_date": 1592883360,
    "view_count": 695,
    "answer_count": 0,
    "tags": "deep-learning;neural-network;nlp;language-model;perplexity"
  },
  {
    "question_id": 62119250,
    "title": "What does merge.txt file mean in BERT-based models in HuggingFace library?",
    "body": "<p>I am trying to understand what merge.txt file infers in tokenizers for RoBERTa model in HuggingFace library. However, nothing is said about it on their website. Any help is appreciated.</p>\n",
    "score": 5,
    "creation_date": 1590942618,
    "view_count": 2662,
    "answer_count": 1,
    "tags": "nlp;tokenize;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 61641257,
    "title": "How to get document embeddings using GPT-2?",
    "body": "<p>I'm curious if using GPT-2 might yield a higher accuracy for document vectors (with greatly varying length) or not (would it surpass the state of the art?)</p>\n\n<p>Really I'm most interested in document embeddings that are as accurate as possible. I'm wondering if using GPT-2 will get results that are more accurate than Paragraph Vectors for example. </p>\n\n<p>I heard that in order to get vectors from GPT-2 \"you can use a weighted sum and/or concatenation of vector outputs at its hidden layers (typically the last few hidden layers) as a representation of its corresponding words or even \"meaning\" of the entire text, although for this role BERT is used more often as it is bi-directional and takes into account of both forward and backward contexts.\"</p>\n\n<p>As a machine learning and NLP beginner, I'd love to know how to go about this, or to be pointed in the right direction to learn more about how to attempt this in Python.</p>\n\n<p>I've tried fine-tuning GPT-2 before but I have no idea how to extract vectors from it for text.</p>\n",
    "score": 5,
    "creation_date": 1588785588,
    "view_count": 1618,
    "answer_count": 0,
    "tags": "python;machine-learning;nlp;artificial-intelligence;doc2vec"
  },
  {
    "question_id": 60803757,
    "title": "How to do Sentence Similarity with XLNet?",
    "body": "<p>I want to perform a sentence similarity task and tried the following:</p>\n\n<pre><code>from transformers import XLNetTokenizer, XLNetModel\nimport torch\nimport scipy\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\nmodel = XLNetModel.from_pretrained('xlnet-large-cased')\n\ninput_ids = torch.tensor(tokenizer.encode(\"Hello, my animal is cute\", add_special_tokens=False)).unsqueeze(0)\noutputs = model(input_ids)\nlast_hidden_states = outputs[0]\n\ninput_ids = torch.tensor(tokenizer.encode(\"I like your cat\", add_special_tokens=False)).unsqueeze(0) \n\noutputs1 = model(input_ids)\nlast_hidden_states1 = outputs1[0]\n\ncos = nn.CosineSimilarity(dim=1, eps=1e-6)\noutput = cos(last_hidden_states, last_hidden_states1)\n</code></pre>\n\n<p>However, I get the following error:</p>\n\n<pre><code>RuntimeError: The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 1\n</code></pre>\n\n<p>Can anybody tell me, what I am doing wrong? Is there a better way to do it?</p>\n",
    "score": 5,
    "creation_date": 1584904100,
    "view_count": 1410,
    "answer_count": 1,
    "tags": "python;nlp;embedding;cosine-similarity;transformer-model"
  },
  {
    "question_id": 59813763,
    "title": "Is there a way to create comparison and commonality wordclouds in python (as in r)",
    "body": "<p>i'm trying to compare 3 sets of documents using a wordcloud with python.\nin R there is a simple comparison wordcloud which shows the common words (or weighted by tfidf) per category with different color in the same wordcloud.\nalso there is a commonality wordcloud that shows the most common words from all categories.</p>\n\n<p>I didnt find a way to do that in python. the closest i got was Venn diagram but i couldn't make the word-sizes relate to the tfidf score.</p>\n\n<p>is there a way to create those wordclouds in python?</p>\n",
    "score": 5,
    "creation_date": 1579462998,
    "view_count": 1431,
    "answer_count": 0,
    "tags": "python;text;nlp;data-visualization;word-cloud"
  },
  {
    "question_id": 57581587,
    "title": "huggingface pytorch-transformers: how to initialize embeddings with certain values?",
    "body": "<p>I am finetuning the bert model from <a href=\"https://github.com/huggingface/pytorch-transformers\" rel=\"nofollow noreferrer\">huggingface</a>. Is there a way to manually set the initial embedding of a certain word piece? e.g. having the initial embedding of the word \"dog\" equal to <code>torch.ones(768)</code>. Thanks!</p>\n",
    "score": 5,
    "creation_date": 1566335518,
    "view_count": 1293,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers"
  },
  {
    "question_id": 56782842,
    "title": "Why does pandas dataframe memory usage more than double when the number of rows in the dataframe is doubled?",
    "body": "<p>I'm testing pandas memory usage with text corpora of different lengths. Memory usage is inconsistent and I'm trying to understand why.</p>\n\n<p>I have two text corpora of radically different length (a corpus of yelp reviews: ~ 6 million words - and the British National Corpus ~ 100 million words). I'm using two pandas dataframes to store and manipulate these data. The shape and datatypes of the two corpora are identical. In each dataframe, each row is one word and each column is a piece of metadata about that word (e.g., part-of-speech tag, lemma, the word's text's id). The datatype of all columns in both dataframes is category as all values are strings and most are repeated. Using </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dataframe.memory_usage(deep=True, index=True).sum()\n</code></pre>\n\n<p>pandas tells me the Yelp corpus takes up about 0.2 GBs and the BNC takes up about 1.8 GB. Even though the BNC is about 17 times larger than the yelp corpus, it takes up nine times the memory. OK, fair, the corpora are different. They're not directly comparable. If I concatenate the yelp corpus to the end of the BNC and relevel the categorical variables using </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>pandas.concat([bnc, yelp]).astype('category')\n</code></pre>\n\n<p>I expect the resulting dataframe to take up somewhat less than 2 GBs of memory (1.8 + 0.2 - overhead and duplicated category levels in the two corpora), but it actually takes up 3.2 GBs. Additionally, if I double the yelp corpus using pandas.concat([yelp, yelp]), the memory usage increases to about 0.4 GB. On the other hand, if I double the BNC using the same code, the memory usages increases to 5.2 GBs, nearly three times the memory usage of the original corpus.</p>\n\n<p>I've read up on how pandas represents series/dataframes in memory, but nothing I can find explains this. I'm also sorry to say I can't provide links to the corpora as I don't have permission to distribute them. I'm hoping someone can provide insight rather than code.</p>\n",
    "score": 5,
    "creation_date": 1561599733,
    "view_count": 471,
    "answer_count": 0,
    "tags": "python;pandas;nlp"
  },
  {
    "question_id": 53473260,
    "title": "Google NLP api gives Could not find TLS ALPN provider; no working netty-tcnative, Conscrypt, or Jetty NPN/ALPN available",
    "body": "<p>I am trying to use google natural language processing api. I add libraries using Maven and add the <code>GOOGLE_APPLICATION_CREDENTIALS</code>as environment variable which is having the path to a JSON file that contains my service account key. </p>\n\n<p>It is giving me an error;   <code>Could not find TLS ALPN provider; no working netty-tcnative, Conscrypt, or Jetty NPN/ALPN available</code></p>\n\n<pre><code>try (LanguageServiceClient language = LanguageServiceClient.create()) {\n\n            // The text to analyze\n            String text = \"Hello, world!\";\n            Document doc = Document.newBuilder()\n                    .setContent(text).setType(Type.PLAIN_TEXT).build();\n\n            // Detects the sentiment of the text\n            Sentiment sentiment = language.analyzeSentiment(doc).getDocumentSentiment();\n\n            System.out.printf(\"Text: %s%n\", text);\n            System.out.printf(\"Sentiment: %s, %s%n\", sentiment.getScore(), sentiment.getMagnitude());\n        }catch(Exception e){\n            System.out.println(\"Gevindu Error \"+ e.getMessage());\n        }\n</code></pre>\n",
    "score": 5,
    "creation_date": 1543190741,
    "view_count": 1490,
    "answer_count": 1,
    "tags": "java;maven;nlp;google-natural-language"
  },
  {
    "question_id": 44124251,
    "title": "Initializing Decoder States in Sequence To Sequence Models",
    "body": "<p>I'm writing my first neural machine translator in tensorflow. I am using an encoder/decoder mechanism with attention. My encoder and decoder are lstm stacks with residual connections, but the encoder has an initial bidirectional layer. The decoder does not.</p>\n\n<p>It is common practice in the code that I have seen to initialize the state of the decoder cells with the last state of the encoder cells. However, this is only a clean solution if your encoder and decoder architectures are the same, as is the case in many of the seq2seq tutorials. In many other systems, such as <a href=\"https://arxiv.org/pdf/1609.08144.pdf\" rel=\"noreferrer\">this model</a>\nby Google, the architectures differ on the encoder and decoder. </p>\n\n<p>What are some of the alternative strategies used for initializing decoder state in these circumstances?</p>\n\n<p>I have seen cases where the encoder's last hidden state is passed through a trained weight vector to create the initial decoder state, for all decoder layers. I have also seen more inventive ideas such as the one presented <a href=\"https://arxiv.org/pdf/1703.04357.pdf\" rel=\"noreferrer\">here</a>, but I would like to develop an intuition as to why people pick certain strategies.</p>\n",
    "score": 5,
    "creation_date": 1495501691,
    "view_count": 802,
    "answer_count": 0,
    "tags": "machine-learning;tensorflow;nlp;deep-learning;recurrent-neural-network"
  },
  {
    "question_id": 43322300,
    "title": "Python NLTK - Create own synset",
    "body": "<p>What is the best way to create its own corpus of synonyms/thesaurus/synsets using python NLTK?\nFor instance - in the context of finance:</p>\n\n<pre><code>US\nsynonyms: United States, Washington, \nhyponyms: Wall Street\nmeronym: dollar\n</code></pre>\n\n<p>etc.</p>\n\n<p>Simply creating a synset similar to wordnet but more suited to my purpose.</p>\n\n<p>I am planning to start creating it by hand, and then implement an algorihtm to do it once the creation steps are clear to me.</p>\n\n<p>But first i need to understand the principle of such a corpus. looking at the files in the Wordnet corpus i find the following list of files:</p>\n\n<pre><code>adj.exc  citation.bib  data.adj  data.noun  index.adj  index.noun   index.verb  LICENSE   README\nadv.exc  cntlist.rev   data.adv  data.verb  index.adv  index.sense  lexnames    noun.exc  verb.exc\n</code></pre>\n\n<p>The content of those files is quite obscure, and really isn't helpful to understand how it works. All insights appreciated.\nThank you</p>\n",
    "score": 5,
    "creation_date": 1491824702,
    "view_count": 1307,
    "answer_count": 0,
    "tags": "nlp;nltk"
  },
  {
    "question_id": 41906887,
    "title": "CoreNLP: Cannot find node in dependency for word said",
    "body": "<p>Frequently when running CoreNLP I am seeing this in stderr:</p>\n\n<pre><code>Cannot find node in dependency for word said\n</code></pre>\n\n<p>This seems to happen for the verb that connects a quote to its speaker, which is pretty concerning for my application, quote extraction / characterization.  </p>\n\n<p>(<strong>Edit</strong>) What does this warning mean in practice?  Is this a known problem, and is there a fix?</p>\n",
    "score": 5,
    "creation_date": 1485583592,
    "view_count": 121,
    "answer_count": 0,
    "tags": "java;nlp;stanford-nlp;dependency-parsing"
  },
  {
    "question_id": 41537302,
    "title": "How to predict next word in sentence using ngram model in R",
    "body": "<p>I have pre-processed text data into a corpus I would now  like to build a prediction model based on the previous 2 words (so I think a 3-gram model?). Based on my understanding of the articles I have read, here is how I am thinking of doing it:</p>\n\n<p>step 1: enter two word phrase we wish to predict the next word for</p>\n\n<pre><code># phrase our word prediction will be based on\nphrase &lt;- \"I love\"\n</code></pre>\n\n<p>step 2: calculate 3 gram frequencies</p>\n\n<pre><code>library(RWeka)\n\nthreegramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))\n\ndtm_threegram &lt;- DocumentTermMatrix(corpus, control=list(tokenize=threegramTokenizer))\n\nthreegram_freq &lt;- sort(colSums(as.matrix(dtm_threegram)), decreasing = TRUE)\n</code></pre>\n\n<p>The next step is where I am getting stuck. Conceptually, I think I should subset my 3-gram to only include three word combinations that start with \"I love\". Then, I should only keep the highest frequency 3-gram. For instance, if \"I love you\" appeared 12 times in my corpus and \"I love beer\" appeared 15 times, then the probability of \"beer\" being the next word is higher than \"love\" hence the model should return the former. Is this the correct approach and if so, how do I create something like this programmatically? My <code>threegram_freq</code> object appears to be of numeric class with a character attribute which I don't fully understand what that is. Is it possible to use a regular expression to only include elements starting with \"I love\" and then extract the 3rd word of the 3-gram with the highest frequency?</p>\n\n<p>Thank you!</p>\n",
    "score": 5,
    "creation_date": 1483906054,
    "view_count": 1932,
    "answer_count": 0,
    "tags": "r;nlp;prediction;text-processing;n-gram"
  },
  {
    "question_id": 41136505,
    "title": "How to use keras mask layer correctly?",
    "body": "<p>The Keras mask layer can be used to deal with variable-length sequence training of RNNs. When I use them I get lower accuracy with the mask layer than single batch training. I suspect that I'm not using mask layer correctly.</p>\n\n<p>My goal is to train an LSTM to learn how to spell words. The sequences, which are different English words, are encoded with one-hot representation. Below is code of data encoding part: <code>chars</code> are the set of all letters that make up the sequences, <code>mylist</code> is list of the sequences, <code>MAXLEN</code> is the max length of sequences.</p>\n\n<pre><code>char_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))\n\nX = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)\ny = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)\n\nfor i, sentence in enumerate(mylist):\n    for t in range(len(sentence)-Data_end):\n        X[i, t, char_indices[sentence[t]]] = 1\n        y[i, t, char_indices[sentence[t+1]]] = 1\n</code></pre>\n\n<p>My network is defined as:</p>\n\n<pre><code>model = Sequential()\nmodel.add(Masking(mask_value=0., input_shape=(None, len(chars))))\nmodel.add(LSTM(2000, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(2000, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(TimeDistributed(Dense(len(chars))))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=lr_init, decay=decay_init, momentum=momentum_init, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd)\nearly_stopping = EarlyStopping(patience=2,verbose=1)\n</code></pre>\n\n<p>To train:</p>\n\n<pre><code>model.fit(X, y, callbacks=[early_stopping],batch_size=32, nb_epoch=1)\n</code></pre>\n\n<p>Am I using the mask layer correctly?</p>\n",
    "score": 5,
    "creation_date": 1481698490,
    "view_count": 3137,
    "answer_count": 0,
    "tags": "python;nlp;keras;recurrent-neural-network"
  },
  {
    "question_id": 38688600,
    "title": "Case insensitive POS (Part of Speech) Tagger for SyntaxNet",
    "body": "<p>I have tried, <code>Parsey McParseface</code>, the pre-trained POS tagger that comes with Syntax Net and it does a good job at tagging sentences that have proper capitalization.</p>\n\n<p>I would like to tag sentences that are all <strong>lower case</strong>, like: <code>i grew up in toronto</code> and then parse it to identify <strong>named entities</strong> such as cities, in this case, <code>toronto</code>.</p>\n\n<p>I have a couple of questions:</p>\n\n<ul>\n<li>Is there a pre-trained <strong>case insensitive</strong> POS tagger for SyntaxNet that I can use? </li>\n<li>How should I go about training my own <strong>case insensitive</strong> POS tagger for SyntaxNet? </li>\n<li>Does training the SyntaxNet POS tagger require substantial amount of CPU/GPU power or it can be done on regular servers I could rent on Amazon or similar services? </li>\n<li>Is the data-set that google used to train <code>Parsey McParseface</code> available for public use?</li>\n</ul>\n",
    "score": 5,
    "creation_date": 1470000923,
    "view_count": 756,
    "answer_count": 0,
    "tags": "parsing;nlp;tensorflow;part-of-speech;syntaxnet"
  },
  {
    "question_id": 36306815,
    "title": "Extracting products models for products description text (not reviews, unstructured text)",
    "body": "<p>I'm looking for a method that will help me with this task. I'm familiar with NER but in this case it wont help me, the sentences are short and NER doesn't work well with it.</p>\n\n<p>The input is products description (from Amazon for example, see the marked line in the image below), not something that POS tagging might help with.</p>\n\n<p><a href=\"https://i.sstatic.net/eygX7.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/eygX7.png\" alt=\"enter image description here\"></a></p>\n\n<p>An example:</p>\n\n<pre><code>1. Apple iPhone 6 64GB White\n2. iRobot Scooba 390 Floor Scrubbing Robot\n3. Samsung UN60J6200 60-Inch TV with HW-J450 Soundbar\n</code></pre>\n\n<p>And the results I'm looking for:</p>\n\n<pre><code>1. Brand: Apple, Model: iPhone 6, Features: 64GB, White\n2. Brand: iRobot, Model: Scooba 390, Features: Floor Scrubbing Robot\n3. Brand: Samsung , Model: UN60J6200 , Features: 60-Inch, HW-J450 Soundbar\n</code></pre>\n\n<p>This is a really difficult problem for me and till now Iv'e solved it in a non-generic way and just by using NLP methods. So I wonder if there's a way of training a model for such task.</p>\n\n<p>Some points I want to add:</p>\n\n<ul>\n<li>The brand name is not always at the beginning of the sentence</li>\n<li>Some times the entire sentence contains only small letters.</li>\n<li>In many cases different brands use really similar models for their products (iPhone 6s and Galaxy S6 for example).</li>\n</ul>\n",
    "score": 5,
    "creation_date": 1459335524,
    "view_count": 771,
    "answer_count": 0,
    "tags": "python;machine-learning;nlp;named-entity-recognition"
  },
  {
    "question_id": 35403495,
    "title": "Client Side JavaScript Code Analyser",
    "body": "<p>Is there a JavaScript Code analyser which can be used on the client side to analyze code patterns? \nI've found the following but it seems that this is just for regular text and gives you the <code>=</code> sign etc. I need some code analysis which can run on the client side (JS code), is there any which can be used? </p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>function parseData() {\r\n  var rawData = document.getElementById('data').value.trim(),  \r\n      result,\r\n      output = $('#output'),\r\n      table = $('table').remove(),\r\n      header,\r\n      row,\r\n      cell,\r\n      ul,\r\n      slice,\r\n      wpm = [],        \r\n      wpmAvg = [];\r\n\r\n  output.empty();\r\n  table.find('thead, tbody').empty();\r\n\r\n  if ($('[name=\"format\"]:checked').val() === 'text') {\r\n    // Simple text        \r\n    result = analyzeText(rawData);\r\n    output.append('Word count: ' + result.count + '&lt;br&gt;&lt;br&gt;Frequent words:&lt;br&gt;');\r\n    ul = $('&lt;ul&gt;');\r\n    _.forEach(result.frequentWords, function(value, key) {\r\n      ul.append('&lt;li&gt;' + value.word + ': ' + value.count + '&lt;/li&gt;');\r\n    });\r\n    output.append(ul);       \r\n  }\r\n  else {\r\n    // JSON\r\n    try {\r\n      data = JSON.parse(rawData);\r\n    }\r\n    catch(e) {\r\n      console.log('Error parsing JSON', e);\r\n    }\r\n    header = table.find('thead');\r\n    body = table.find('tbody');\r\n    row = $('&lt;tr&gt;');\r\n    body.append(row);\r\n    // Loop over slices\r\n    _.forEach(data, function(value, key) {\r\n      slice = '';\r\n      // Loop over statements\r\n      _.forEach(value, function(value, key) {\r\n        slice += value.words + ' ';\r\n      });\r\n\r\n      result = analyzeText(slice);\r\n\r\n      addCell(slice, key);\r\n\r\n    });\r\n    $.plot('#wpm', [wpm], {\r\n      xaxes: [{\r\n        axisLabel: 'Time index (1-minute increments)',\r\n      }],\r\n      yaxes: [{\r\n        position: 'left',\r\n        axisLabel: 'Words per minute',\r\n      }]\r\n    });\r\n    output.append(table);\r\n  }\r\n\r\n  function addCell(data, index) {\r\n    var cell1, cell2, ul1, ul2, result;\r\n    cell1 = $('&lt;td&gt;');\r\n    cell2 = $('&lt;td&gt;');\r\n    ul1 = $('&lt;ul&gt;');\r\n    ul2 = $('&lt;ul&gt;');\r\n    cell1.append(ul1);\r\n    cell2.append(ul2);\r\n    row.append(cell1, cell2);\r\n    result = analyzeText(data);\r\n    header.append('&lt;th&gt;' + index + '&lt;/th&gt;&lt;th class=\"subText\"&gt;(' + result.count + ')&lt;/th&gt;');\r\n    wpm.push([index, result.count]);\r\n    _.forEach(result.frequentWords, function(value, key) {\r\n      ul1.append('&lt;li&gt;' + value.word + '&lt;/li&gt;');\r\n      ul2.append('&lt;li&gt;' + value.count + '&lt;/li&gt;');\r\n    });\r\n  }\r\n}\r\n\r\nfunction analyzeText(rawData) {\r\n  var result = {\r\n    count: 0,\r\n    frequentWords: []\r\n  },\r\n      data = rawData.split(/[\\s.,]+/g)\r\n  counts = {},\r\n    countsArray = [],\r\n    commonWords = [ \r\n    0,1,2,3,4,5,6,7,8,9,            \r\n    '-',\r\n    'a',\r\n    'about',\r\n    'function',\r\n    'object'\r\n  ];\r\n\r\n  if (!data[data.length]) {\r\n    data.splice(-1, 1);\r\n  }\r\n\r\n  // Word count\r\n  result.count = data.length;\r\n\r\n  // Word frequency (filtered for common words, sorted descending by count)\r\n  for (var i = 0; i &lt; data.length; i++) {\r\n    if (!counts.hasOwnProperty(data[i].toLowerCase())) {\r\n      counts[data[i].toLowerCase()] = 1;\r\n    }\r\n    else {\r\n      counts[data[i].toLowerCase()] += 1;\r\n    }\r\n  }    \r\n  _.forEach(counts, function(value, key) {\r\n    if (commonWords.indexOf(key.toLowerCase()) === -1) {\r\n      countsArray.push({\r\n        word: key.toLowerCase(),\r\n        count: value\r\n      });\r\n    }\r\n  });\r\n  countsArray = _.sortBy(countsArray, 'count').reverse();    \r\n  _.forEach(countsArray, function(value, index) {\r\n    if (value.count &gt; 1) {\r\n      result.frequentWords.push(value);            \r\n    }\r\n  });\r\n\r\n  return result;\r\n}</code></pre>\r\n<pre class=\"snippet-code-css lang-css prettyprint-override\"><code>body {\r\n  font-family: arial;\r\n}\r\ntable, tr, td, th {\r\n  border-collapse: collapse;\r\n  border: solid 1px #ddd;\r\n}\r\nth, td {\r\n  padding: 4px 8px;    \r\n}\r\n.subText {\r\n  color:#999;\r\n  font-style: italic;\r\n}\r\n#wpm {\r\n  width:600px;\r\n  height: 400px;\r\n}</code></pre>\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.8.3/underscore-min.js\"&gt;&lt;/script&gt;\r\n&lt;script src=\"https://code.jquery.com/jquery-2.1.4.min.js\"&gt;&lt;/script&gt;\r\n&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/flot/0.8.3/jquery.flot.js\"&gt;&lt;/script&gt;\r\n\r\n&lt;textarea id=\"data\" cols=\"80\" rows=\"20\" placeholder=\"Paste text or JSON here\"&gt;&lt;/textarea&gt;&lt;br /&gt;\r\n&lt;label for=\"text\"&gt;&lt;input type=\"radio\" name=\"format\" checked value=\"text\" id=\"text\"&gt; Simple text&lt;/label&gt;\r\n&lt;button type=\"button\" onclick=\"parseData()\"&gt;Analyze text&lt;/button&gt;\r\n&lt;br&gt;&lt;br&gt;\r\n&lt;div id=\"output\"&gt;&lt;/div&gt;&lt;br&gt;&lt;br&gt;\r\n&lt;div id=\"wpm\"&gt;&lt;/div&gt;\r\n&lt;table&gt;\r\n  &lt;thead&gt;\r\n  &lt;/thead&gt;\r\n  &lt;tbody&gt;\r\n  &lt;/tbody&gt;\r\n&lt;/table&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p><a href=\"https://jsfiddle.net/fxn5q8y0/6/\" rel=\"nofollow\">https://jsfiddle.net/fxn5q8y0/6/</a></p>\n",
    "score": 5,
    "creation_date": 1455520573,
    "view_count": 429,
    "answer_count": 1,
    "tags": "javascript;jquery;text-analysis"
  },
  {
    "question_id": 35288965,
    "title": "Using data set for training and testing in NLTK",
    "body": "<p>I am trying to use Naive Bayes algorithm to do sentimental analysis and was going through a few articles. As mentioned in almost every article I need to train my Naive Bayes algorithm with some pre-computed sentiments.</p>\n\n<p>Now, I have a piece of code using movie_review module provided with NLTK. The code is :</p>\n\n<pre><code>import nltk\nimport random\nfrom nltk.corpus import movie_reviews\n\ndocuments = [(list(movie_reviews.words(fileid)), category)\n             for category in movie_reviews.categories()\n             for fileid in movie_reviews.fileids(category)]\n\nrandom.shuffle(documents)\n\nall_words = []\nfor w in movie_reviews.words():\n    all_words.append(w.lower())\nall_words = nltk.FreqDist(all_words)\nword_features = list(all_words.keys())[:3000]\n\ndef find_features(document):\n    words = set(document)\n    features = {}\n    for w in word_features:\n        features[w] = (w in words)\n\n    return features\n\nfeaturesets = [(find_features(rev), category) for (rev, category) in documents]\n\n\ntraining_set = featuresets[:1900]\ntesting_set = featuresets[1900:]\n\nclassifier = nltk.NaiveBayesClassifier.train(training_set)\nprint(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n</code></pre>\n\n<p>So, in the above code I have a training_set and a testing_set.\nI checked the movie_review module and inside the movie review module we have many small text files containing reviews.</p>\n\n<ul>\n<li>So, my question is here we had the movie review module and we imported it and trained and tested using the module but how can we do when I am using an external training data set and external testing data set.</li>\n<li>Also, how is NLTK parsing movie_review directory which has so many text files inside it. As I will be using  <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\" rel=\"noreferrer\">http://ai.stanford.edu/~amaas/data/sentiment/</a> this as my training data set, so I need to understand how its done. </li>\n</ul>\n",
    "score": 5,
    "creation_date": 1455012013,
    "view_count": 2238,
    "answer_count": 0,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 34911264,
    "title": "Is there a way to tell NLTK that a certain word isn&#39;t a proper noun but a noun?",
    "body": "<p>I'm doing some NLP where I'm finding out when patients were diagnosed with multiple sclerosis.</p>\n\n<p>I'd like to use nltk to tell me that the noun of a sentence was multiple sclerosis. Problem is, doctors frequently refer to multiple sclerosis as MS which nltk picks up as a proper noun.</p>\n\n<p>For example, this sentence, \"His MS was diagnosed in 1999.\" Is tagged as: <code>[('His', 'PRP$'), ('MS', 'NNP'), ('was', 'VBD'), ('diagnosed', 'VBN'), ('in', 'IN'), ('1999', 'CD'), ('.', '.')]</code></p>\n\n<p>MS should be a noun here. Any suggestions?</p>\n",
    "score": 5,
    "creation_date": 1453326509,
    "view_count": 551,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 33007246,
    "title": "How to automatically transcribe a Skype meeting, correctly attributed to each participant?",
    "body": "<p>Assuming each participant agrees to the recording and transcription of the Skype call, is there a way to transcribe the meeting (either live or offline or both) such that it produces a text transcript where each spoken text is correctly attributed to the speaker.  The transcript could then be input to any variety of search or NLP algorithms.  </p>\n\n<p>The top 3 Google search hits of \"automatically transcribe Skype\" refer to apps which make <strong>manual</strong> transcription easier:</p>\n\n<p>(1) <a href=\"http://www.dummies.com/how-to/content/how-to-convert-skype-audio-to-text-with-transcribe.html\" rel=\"noreferrer\">http://www.dummies.com/how-to/content/how-to-convert-skype-audio-to-text-with-transcribe.html</a></p>\n\n<p>(2) <a href=\"http://ask.metafilter.com/231400/How-to-record-and-transcribe-Skype-conversation\" rel=\"noreferrer\">http://ask.metafilter.com/231400/How-to-record-and-transcribe-Skype-conversation</a></p>\n\n<p>(3) <a href=\"https://www.ttetranscripts.com/blog/how-to-record-and-transcribe-your-skype-conversations\" rel=\"noreferrer\">https://www.ttetranscripts.com/blog/how-to-record-and-transcribe-your-skype-conversations</a></p>\n\n<p>While it would be trivial to record the audio and send it to a speech-to-text engine, I doubt it would be very high quality because the best results are usually speaker dependent models (else we wouldn't have to take time to train Dragon Naturally Speaking).  </p>\n\n<p>But, before we can choose speaker dependent transcription models, we need to know which segment of the audio belongs to which speaker.  There's 2 ways that this is solved:</p>\n\n<ol>\n<li><p>There is an easy way to retrieve all the audio that came from each participant, e.g. you just record all the audio from each speaker's microphone during the call, and you don't have to do any segmentation.  </p></li>\n<li><p>In case the first option isn't feasible or prohibitive in some way, we have to use a Speaker Diarization algorithm, which segments the audio into N clusters/speakers (most algorithms allow for being told how many speakers in the audio, but some can figure this out on their own).  For real-time transcript as the call goes on, I imagine we'd need some fancy Real Time Speaker Diarization algorithm.  </p></li>\n</ol>\n\n<p>In any case, once the segmentation is solved, each participant has their trained speaker model, which is then applied to their portions of the audio.  At the end of the day, everyone gets a nice conversation transcript and later one we can do fancy things like topic analysis or maybe Big Brother wants to sift over everyone's project meetings without having to listen to hours of audio.  </p>\n\n<p>My question is, what would be a way to implement this in practice?  </p>\n",
    "score": 5,
    "creation_date": 1444281768,
    "view_count": 4492,
    "answer_count": 0,
    "tags": "nlp;audio-recording;skype;speech-to-text;transcription"
  },
  {
    "question_id": 31397628,
    "title": "Corenlp document level multithreading",
    "body": "<p>I have 8 million wikipedia articles to parse. I want to run 7 operations: tokenize,ssplit,pos,lemma,ner,parse,dcoref. Each document is taking approx 20 secs. In this rate it will take months to parse the whole data set in single thread. There is a 'nthreads' option for simultaneously parsing successive sentences. But co-reference analyzer cannot work on single sentence level. I can split my documents in multiple buckets and run corenlp on each of them simultaneously but that is resource hungry. Is there any simpler way to run multi-threaded corenlp at document level (not sentence) ? (I have  100 GB ram and 50 cores). </p>\n",
    "score": 5,
    "creation_date": 1436847940,
    "view_count": 331,
    "answer_count": 0,
    "tags": "multithreading;nlp;stanford-nlp;multicore;wikipedia"
  },
  {
    "question_id": 30712526,
    "title": "Detect (predefined) topics in natural text",
    "body": "<p>Is there a library or database out there that can detect the topics of natural text?</p>\n\n<p>I'm not talking about generating topics from extracted keywords, but about analysing the used vocabulary and matching it with predefined topics. Like searching for words used in cooking or certain sports (like names of football clubs or technical terms).</p>\n\n<p><strong>Update with clarification:</strong></p>\n\n<p>Example text snippet: A sentence about football, then another sentence talking about catering at the event.</p>\n\n<p>Library could assign categories \"sports\", \"football\", \"cooking\". </p>\n\n<p>I'm looking for something that can assign these categories (or \"topics of interest\" maybe) without me having to train thousands of models with terabytes of manually classified documents. This could for example work by matching keywords instead of statistical analysis (that's why I mentioned database earlier).</p>\n\n<p>I'm searching this because I don't have the manpower to build such a big database myself.</p>\n",
    "score": 5,
    "creation_date": 1433775442,
    "view_count": 1686,
    "answer_count": 2,
    "tags": "nlp;text-classification;information-extraction"
  },
  {
    "question_id": 19967883,
    "title": "Multiple word spelling correction",
    "body": "<p>Correcting one word spelling mistakes (both non-word &amp; real-word mistakes) is easy:</p>\n\n<pre><code>P(w|c) P(c)\n</code></pre>\n\n<p>Where <code>w</code> is the incorrectly spelled word and <code>c</code> is the candidate we're trying to match, such that the candidate is a one word token.</p>\n\n<p>But in Google, when you enter something like <code>spelligncheck</code>, it corrects the word into two different words. Now, <code>P(w|c)</code> is easy here, if i use levenshtein distance. But that means i can't have one word (one token, rather) candidates anymore. So this will increase the size of my dictionary exponentially.</p>\n\n<p>Moreover when I enter <code>app le</code> Google corrects it to <code>apple</code>...</p>\n\n<p>So what is the best way of doing multiple word spelling correction, given a one-token dictionary?</p>\n",
    "score": 5,
    "creation_date": 1384392847,
    "view_count": 1219,
    "answer_count": 1,
    "tags": "python;nlp;probability;spell-checking"
  },
  {
    "question_id": 19748887,
    "title": "Does Lua have an NLP tool with the capabilities of the NLTK?",
    "body": "<p>The NLTK is a powerful NLP tool with much documentation, but most of my projects use Lua, not Python. Is there a Lua equivalent of the NLTK that uses native Lua code?</p>\n",
    "score": 5,
    "creation_date": 1383441885,
    "view_count": 1332,
    "answer_count": 0,
    "tags": "lua;nlp;nltk"
  },
  {
    "question_id": 16309798,
    "title": "Calculating TF-IDF Similarity Between 2 Documents Using Gensim",
    "body": "<p>I'm using Gensim to calculate the similarity between 2 documents. For some reason the line tfidf[corpus] returns an empty list. I'm not sure why though</p>\n\n<pre><code>    articles = []\n#make a corpus by adding each of the top 25 documents to a list\nfor x in range(0,25):\n    articles.append(str(WikiDoc(sorted_links[0]).jsonify()['text']))\n#puts all of the top 25 documents into a list\ntexts = [[word for word in document.lower().split()] for document in articles]\nprint texts\n#load precomputed dictionary\narticles_dict = corpora.Dictionary(texts)\narticles_dict.save('./articles.dict')\narticles_dict = Dictionary.load('./articles.dict')\n#articles_corpus = [articles_dict.doc2bow(text) for text in texts]\n#corpora.MmCorpus.serialize('./articles.mm', articles_corpus)\ncorpus = [articles_dict.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize('./articles.mm', corpus)\ncorpus = corpora.MmCorpus('./articles.mm')\n#build the tfidf model based on the 25 documents so that we can find similarities \n#with respect to each of these documents\ntfidf = models.TfidfModel(corpus)\n#get the other document and process to produce dictionary representation\none_doc_bow = WikiDoc('SpongeBob')\none_doc_bow = articles_dict.doc2bow(one_doc_bow.jsonify()['text'].lower().split())\nprint tfidf[one_doc_bow]\ntop = tfidf[one_doc_bow]\ncorpus_tfidf = tfidf[corpus]\n</code></pre>\n\n<p>When I print the dictionary I get: Dictionary(2204 unique tokens)\nWhen I print the MmCorpus I get: MmCorpus(25 documents, 2204 features, 55100 non-zero entries)\ntfidf[corpus] yield [].\nCan anyone diagnose my problem? Thanks a lot!</p>\n",
    "score": 5,
    "creation_date": 1367359804,
    "view_count": 2027,
    "answer_count": 0,
    "tags": "python;nlp;similarity;gensim"
  },
  {
    "question_id": 13795019,
    "title": "Up-to-date sentence readability algorithm",
    "body": "<p>I'm working on an algorithm for estimating <a href=\"https://en.wikipedia.org/wiki/Readability_test\" rel=\"nofollow noreferrer\">sentence difficulty</a>, but the methods that I have found seem to be too old to take advantage of what modern computers can do.</p>\n\n<p>The algorithms in use today were mostly developed around 40 to 60 years ago. <a href=\"https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\" rel=\"nofollow noreferrer\">Flesch-Kincaid</a> is the most popular and is still used as the standard for documents by the Department of Defense and many states and businesses.  I have looked at Flesch-Kincaid Grade Level, Gunning Fog Index, SMOG Index, Fry Readability Formula, and Coleman-Liau Index.</p>\n\n<p>I have decided to use the Automated Readability Index:</p>\n\n<pre><code>ARI = 4.71 * (characters / words) + .5 * (words / sentences) - 21.43;\n</code></pre>\n\n<p>It seems to me that it would not be difficult to assign a value to each word based on Corpus-based word frequency lists and then work these values into the old readability formula.<br>\nThis could be done for the first 1000 to 5000 most frequent words.\nAlso it would probably be effective to make separate lists for some different kinds of words and parts of speech.  The presence of conjunctions would definitely be a sign of sentence complexity.   </p>\n\n<p>Are there any formulas for doing this?</p>\n",
    "score": 5,
    "creation_date": 1355113695,
    "view_count": 585,
    "answer_count": 1,
    "tags": "machine-learning;nlp;text-mining"
  },
  {
    "question_id": 9257057,
    "title": "How to implement LSA (Latent semantic analysis) in Python?",
    "body": "<p>How to implement Latent semantic analysis in Python and compare corps of text against query using Cosine similarity ?</p>\n",
    "score": 5,
    "creation_date": 1329119046,
    "view_count": 7911,
    "answer_count": 0,
    "tags": "python;math;nlp"
  },
  {
    "question_id": 32748859,
    "title": "Accurately splitting sentences",
    "body": "<p>My program takes a text file and splits each sentence into a list using <code>split('.')</code> meaning that it will split when it registers a full stop however it can be inaccurate.</p>\n\n<h1>For Example</h1>\n\n<pre><code>str='i love carpets. In fact i own 2.4 km of the stuff.'\n</code></pre>\n\n<h2>Output</h2>\n\n<p><code>listOfSentences = ['i love carpets', 'in fact i own 2', '4 km of the stuff']</code></p>\n\n<h2>Desired Output</h2>\n\n<pre><code> listOfSentences = ['i love carpets', 'in fact i own 2.4 km of the stuff']\n</code></pre>\n\n<p>My question is: <strong>How do I split the end of sentences and not at every full stop.</strong></p>\n",
    "score": 5,
    "creation_date": 1443040029,
    "view_count": 2927,
    "answer_count": 5,
    "tags": "python;parsing;nlp"
  },
  {
    "question_id": 16766698,
    "title": "Python Context Free Grammar and PCFG generation benchmarks?",
    "body": "<p>I know there are various functions to use to general CFGs and PCFGs in Python; however they all seem to differ in speed.</p>\n<p>E.g.: NLTK, PyParsing.</p>\n<h3>Are there any recent benchmarks comparing various attributes related to speed and memory usage?</h3>\n",
    "score": 5,
    "creation_date": 1369631893,
    "view_count": 727,
    "answer_count": 1,
    "tags": "python;nlp;nltk;context-free-grammar;text-analysis"
  },
  {
    "question_id": 22844284,
    "title": "detecting emotions in sentiment analysis python",
    "body": "<p>I was curious if anyone had any thoughts about how one might detect major emotions displayed in a text? Are there any python packages or examples that do this?</p>\n\n<p>TO clarify:</p>\n\n<p>I know that there is already something called sentiment analysis. However, I'm noticing this only looks at positive/negative sentiment.\nI'm wondering if it's actually possible to find emotions (like sadness, joy, despair, etc) linked to certain texts. </p>\n",
    "score": 5,
    "creation_date": 1396544506,
    "view_count": 7001,
    "answer_count": 1,
    "tags": "python;nlp"
  },
  {
    "question_id": 285848,
    "title": "Is functional programming the next step towards natural-language programming?",
    "body": "<p>This is my very first question so I am a bit nervous about it because I am not sure whether I get the meaning across well enough. Anyhow, here we go....</p>\n\n<p>Whenever new milestones in programming have been reached it seems they always have had one goal in common: to make it easier for programmers, well, to program.</p>\n\n<p>Machine language, opcodes/mnemonics, procedures/functions, structs, classes (OOP) etc. always helped, in their time, to plan, structure and code programs in a more natural,  understandable and better maintainable way.</p>\n\n<p>Of course functional programming is by no means a novelty but it seems that it has experienced a sort of renaissance in recent years. I also believe that FP will get an enormous boost when Microsoft will add F# to their mainstream programming languages.</p>\n\n<p>Returning to my original question, I believe that ultimately programming will be done in a natural language (English) with very few restrictions or rules. The compiler will be part of an AI/NLP system that extracts information from the code or should I say text and transforms it into an intermediate language which the compiler can compile.</p>\n\n<p>So, does FP take programming closer to natural-language programming or is it rather an obstacle and mainstream OOP will lead us faster to natural-language programming?</p>\n\n<p>This question should not be used to discuss the useability or feasability of natural-language programming because only the future will tell. </p>\n",
    "score": 4,
    "creation_date": 1226533385,
    "view_count": 1727,
    "answer_count": 9,
    "tags": "functional-programming;nlp"
  },
  {
    "question_id": 3298129,
    "title": "Is similarity to &quot;natural language&quot; a convincing selling point for a programming language?",
    "body": "<p>Look, for example at AppleScript (and there are plenty of others, some admittedly quite good) which advertise their use of the natural language metaphor. Code is apparently more readable because it can be/is intended to be constructed in English-like sentences, says they. I'm sure there are people who would like nothing better than to program using only English sentences. However, I have doubts about the viability of a language that takes that paradigm too far (excepting niche cases).</p>\n\n<p>So, after a certain reasonable point, is natural-languaginess a benefit or a misfeature? What if the concept is carried to an extreme -- will code necessarily be more readable? Or might it be unnecessarily long, difficult to work with, and just as capable of producing hilarity on the scale of obfuscated Perl, obfuscated C, and eye-twisting Bash script logorrhea? </p>\n\n<p>I am aware of some specialty cases like \"Inform\" that are almost pure English, but these have a niche that they're not likely to venture out from. I hear and read about how great it would be for code to read more like English sentences, but are there discussions of the possible disadvantages? If everyday language is so clear, simple, clean, lovely, concise, understandable, why did we invent mathematical notation in the first place?</p>\n\n<p>Is it really easier to describe complex instructions <em>accurately and precisely</em> to a machine in natural language, or isn't something closer to mathematical markup a much better choice? Where should that line be drawn? And finally, are you attracted to languages that are touted as resembling English sentences? Should this whole question have just been a one liner:</p>\n\n<pre><code>naturalLanguage &gt; computerishLanguage ? booAndHiss : cheerLoudly;\n</code></pre>\n",
    "score": 4,
    "creation_date": 1279706951,
    "view_count": 701,
    "answer_count": 13,
    "tags": "programming-languages;nlp"
  },
  {
    "question_id": 53294482,
    "title": "How to get TF-IDF scores for the words?",
    "body": "<p>I have a large corpus (around 400k unique sentences). I just want to get TF-IDF score for each word. I tried to calculate the score for each word by scanning each word and calculating the frequency but it's taking too long.</p>\n\n<p>I used :</p>\n\n<pre><code>  X= tfidfVectorizer(corpus)\n</code></pre>\n\n<p>from sklearn but it directly gives back the vector representation of the sentence. Is there any way I can get the TF-IDF scores for each word in the corpus?</p>\n",
    "score": 4,
    "creation_date": 1542177845,
    "view_count": 18421,
    "answer_count": 2,
    "tags": "python;nlp;tf-idf;tfidfvectorizer"
  },
  {
    "question_id": 30085694,
    "title": "Python convert list of multiple words to single words",
    "body": "<p>I have a list of words for example:</p>\n\n<p><code>words = ['one','two','three four','five','six seven']</code> # quote was missing</p>\n\n<p>And I am trying to create a new list where each item in the list is just one word so I would have:</p>\n\n<p><code>words = ['one','two','three','four','five','six','seven']</code></p>\n\n<p>Would the best thing to do be join the entire list into a string and then tokenize the string? Something like this:</p>\n\n<p><code>word_string = ' '.join(words)\ntokenize_list = nltk.tokenize(word_string)</code></p>\n\n<p>Or is there a better option?</p>\n",
    "score": 4,
    "creation_date": 1430939878,
    "view_count": 3756,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 170452,
    "title": "Theory: &quot;Lexical Encoding&quot;",
    "body": "<p><strong>I am using the term \"Lexical Encoding\" for my lack of a better one.</strong></p>\n\n<p>A Word is arguably the fundamental unit of communication as opposed to a Letter.  Unicode tries to assign a numeric value to each Letter of all known Alphabets.  What is a Letter to one language, is a Glyph to another.  Unicode 5.1 assigns more than 100,000 values to these Glyphs currently.  Out of the approximately 180,000 Words being used in Modern English, it is said that with a vocabulary of about 2,000 Words, you should be able to converse in general terms. A \"Lexical Encoding\" would encode each Word not each Letter, and encapsulate them within a Sentence.</p>\n\n<pre><code>// An simplified example of a \"Lexical Encoding\"\nString sentence = \"How are you today?\";\nint[] sentence = { 93, 22, 14, 330, QUERY };\n</code></pre>\n\n<p>In this example each Token in the String was encoded as an Integer. The Encoding Scheme here simply assigned an int value based on generalised statistical ranking of word usage, and assigned a constant to the question mark.</p>\n\n<p>Ultimately, a Word has both a Spelling &amp; Meaning though.  Any \"Lexical Encoding\" would preserve the meaning and intent of the Sentence as a whole, and not be language specific.  An English sentence would be encoded into <a href=\"https://stackoverflow.com/questions/170452/linguistics-lexical-encoding#174249\">\"...language-neutral atomic elements of meaning ...\"</a> which could then be reconstituted into any language with a structured Syntactic Form and Grammatical Structure.</p>\n\n<p>What are other examples of \"Lexical Encoding\" techniques?</p>\n\n<hr>\n\n<p>If you were interested in where the word-usage statistics come from :<br>\n<a href=\"http://www.wordcount.org\" rel=\"nofollow noreferrer\">http://www.wordcount.org</a></p>\n",
    "score": 4,
    "creation_date": 1223131686,
    "view_count": 2497,
    "answer_count": 8,
    "tags": "encoding;theory;nlp;linguistics"
  },
  {
    "question_id": 76342339,
    "title": "How can I handle overflowing tokens in Huggingface Transformer model?",
    "body": "<p>I am training a XLM-RoBERTa model for token classification using Huggingface Transformers. My maximum token length of the already fine-tuned model is 166. I truncated longer and padded shorter sequences in the training data. Now, during inference/prediction time I would like to predict all tokens, even in sequences longer than 166. But if I read the documentation correctly, overflowing tokens get thrown away. Is that correct? I am not completely sure what the &quot;return_overflowing_tokens&quot; and stride parameters do. Could they be used to split sequences that are too long into two or more shorter sequences?</p>\n<p>I have already tried to split my text data into sentences to have smaller chunks, but some of them still exceed the max token length. It would be ideal, if overflowing tokens would be automatically added to an additional sequence.</p>\n",
    "score": 4,
    "creation_date": 1685115801,
    "view_count": 5017,
    "answer_count": 1,
    "tags": "nlp;tokenize;huggingface-transformers"
  },
  {
    "question_id": 32411594,
    "title": "Identify the word as a noun, verb or adjective",
    "body": "<p>Given a single word such as \"table\", I want to identify what it is most commonly used as, whether its most common usage is noun, verb or adjective. I want to do this in python. Is there anything else besides wordnet too? I don't prefer wordnet. Or, if I use wordnet, how would I do it exactly with it? </p>\n",
    "score": 4,
    "creation_date": 1441445797,
    "view_count": 17275,
    "answer_count": 2,
    "tags": "python;nlp;wordnet;word-sense-disambiguation"
  },
  {
    "question_id": 24409642,
    "title": "How to extract nouns using NLTK pos_tag()?",
    "body": "<p>I am fairly new to python. I am not able to figure out the bug. I want to extract nouns using NLTK.</p>\n\n<p>I have written the following code:</p>\n\n<pre><code>import nltk\n\nsentence = \"At eight o'clock on Thursday film morning word line test best beautiful Ram Aaron design\"\n\ntokens = nltk.word_tokenize(sentence)\n\ntagged = nltk.pos_tag(tokens)\n\n\nlength = len(tagged) - 1\n\na = list()\n\nfor i in (0,length):\n    log = (tagged[i][1][0] == 'N')\n    if log == True:\n      a.append(tagged[i][0])\n</code></pre>\n\n<p>When I run this, 'a' only has one element </p>\n\n<pre><code>a\n['detail']\n</code></pre>\n\n<p>I do not understand why?</p>\n\n<p>When I do it without for loop, that is running</p>\n\n<pre><code>log = (tagged[i][1][0] == 'N')\n    if log == True:\n      a.append(tagged[i][0])\n</code></pre>\n\n<p>by change value of 'i' manually from 0 to 'length', i get the output perfectly, but with for loop it only returns the end element. Can someone tell me what is wrong happening with for loop. </p>\n\n<p>'a' should be as follows after the code</p>\n\n<pre><code>['Thursday', 'film', 'morning', 'word', 'line', 'test', 'Ram' 'Aaron', 'design']\n</code></pre>\n",
    "score": 4,
    "creation_date": 1403701807,
    "view_count": 13543,
    "answer_count": 4,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 18871706,
    "title": "check if two words are related to each other",
    "body": "<p>I have two lists: one, the interests of the user; and second, the keywords about a book. I want to recommend the book to the user based on his given interests list. I am using the <code>SequenceMatcher</code> class of Python library <code>difflib</code> to match similar words like \"game\", \"games\", \"gaming\", \"gamer\", etc. The <code>ratio</code> function gives me a number between [0,1] stating how similar the 2 strings are. But I got stuck at one example where I calculated the similarity between \"looping\" and \"shooting\". It comes out to be <code>0.6667</code>.</p>\n\n<pre><code>for interest in self.interests:\n    for keyword in keywords:\n       s = SequenceMatcher(None,interest,keyword)\n       match_freq = s.ratio()\n       if match_freq &gt;= self.limit:\n            #print interest, keyword, match_freq\n            final_score += 1\n            break \n</code></pre>\n\n<p>Is there any other way to perform this kind of matching in Python?</p>\n",
    "score": 4,
    "creation_date": 1379505639,
    "view_count": 12056,
    "answer_count": 3,
    "tags": "python;python-2.7;nlp;nltk"
  },
  {
    "question_id": 7218310,
    "title": "How to intelligently parse last name",
    "body": "<p>Assuming western naming convention of <code>FirstName MiddleName(s) LastName</code>, </p>\n\n<p>What would be the best way to correctly parse out the last name from a full name?</p>\n\n<p>For example:</p>\n\n<pre><code>John Smith --&gt; 'Smith'\nJohn Maxwell Smith --&gt; 'Smith'\nJohn Smith Jr --&gt; 'Smith Jr'\nJohn van Damme --&gt; 'van Damme'\nJohn Smith, IV --&gt; 'Smith, IV'\nJohn Mark Del La Hoya --&gt; 'Del La Hoya'\n</code></pre>\n\n<p>...and the countless other permutations from this.</p>\n",
    "score": 4,
    "creation_date": 1314491962,
    "view_count": 1851,
    "answer_count": 3,
    "tags": "python;regex;parsing;nlp"
  },
  {
    "question_id": 7026620,
    "title": "Is there a library for splitting sentence into a list of words in it?",
    "body": "<p>I'm looking at nltk for python, but it splits(tokenize) <code>won't</code> as <code>['wo',\"n't\"]</code>. Are there libraries that do this more robustly? </p>\n\n<p>I know i can build a regex of some sort to solve this problem, but I'm looking for a library/tool because it would be a more directed approach. For example, after a basic regex with periods and commas, I realized words like 'Mr. ' will break the system. </p>\n\n<p>(@artsiom)</p>\n\n<p>If the sentence was \"you won't?\", split() will give me [\"you\", \"won't?\"]. So there's an extra '?' that I have to deal with.\nI'm looking for a tried and tested method which do away with the kinks like the above mentioned and also the lot many exceptions that I'm sure exist. Of course, I'll resort to a split(regex) if I don't find any.</p>\n",
    "score": 4,
    "creation_date": 1313068969,
    "view_count": 4314,
    "answer_count": 5,
    "tags": "python;regex;nlp"
  },
  {
    "question_id": 23704361,
    "title": "How to use the confusion matrix module in NLTK?",
    "body": "<p>I followed the NLTK book in using the confusion matrix but the confusionmatrix looks very odd.</p>\n\n<pre><code>#empirically exam where tagger is making mistakes\ntest_tags = [tag for sent in brown.sents(categories='editorial')\n    for (word, tag) in t2.tag(sent)]\ngold_tags = [tag for (word, tag) in brown.tagged_words(categories='editorial')]\nprint nltk.ConfusionMatrix(gold_tags, test_tags)\n</code></pre>\n\n<p>Can anyone explain how to use the confusion matrix?</p>\n",
    "score": 4,
    "creation_date": 1400273534,
    "view_count": 7861,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 28389564,
    "title": "Stanford CoreNLP Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl",
    "body": "<p>I am trying to learn the Stanford CoreNLP library. I am using C# with the posted example (<a href=\"https://sergeytihon.wordpress.com/2013/10/26/stanford-corenlp-is-available-on-nuget-for-fc-devs/\" rel=\"nofollow\">https://sergeytihon.wordpress.com/2013/10/26/stanford-corenlp-is-available-on-nuget-for-fc-devs/</a>).  I loaded the package “Stanford.NLP.CoreNLP” (it added IKVM.NET) via nuget and downloaded the code. Unzipped the .jar models. My directory is correct.  I get the following error:</p>\n\n<pre><code>&gt; edu.stanford.nlp.util.ReflectionLoading.ReflectionLoadingException was\n&gt; unhandled HResult=-2146233088 Message=Error creating\n&gt; edu.stanford.nlp.time.TimeExpressionExtractorImpl\n&gt; Source=stanford-corenlp-3.5.0 StackTrace: at\n&gt; edu.stanford.nlp.util.ReflectionLoading.loadByReflection(String\n&gt; className, Object[] arguments) at\n&gt; edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(String\n&gt; className, String name, Properties props) at\n&gt; edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(String\n&gt; name, Properties props) at\n&gt; edu.stanford.nlp.ie.regexp.NumberSequenceClassifier..ctor(Properties\n&gt; props, Boolean useSUTime, Properties sutimeProps) at\n&gt; edu.stanford.nlp.ie.NERClassifierCombiner..ctor(Boolean\n&gt; applyNumericClassifiers, Boolean useSUTime, Properties nscProps,\n&gt; String[] loadPaths) at\n&gt; edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(Properties\n&gt; properties) at edu.stanford.nlp.pipeline.AnnotatorFactories.6.create()\n&gt; at edu.stanford.nlp.pipeline.AnnotatorPool.get(String name) at\n&gt; edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(Properties A_1,\n&gt; Boolean A_2, AnnotatorImplementations A_3) at\n&gt; edu.stanford.nlp.pipeline.StanfordCoreNLP..ctor(Properties props,\n&gt; Boolean enforceRequirements) at\n&gt; edu.stanford.nlp.pipeline.StanfordCoreNLP..ctor(Properties props) at\n&gt; ConsoleApplication1.Program.Main(String[] args) in\n&gt; d:\\Programming_Code\\VisualStudio\\visual studio\n&gt; 2013\\Projects\\AutoWikify\\ConsoleApplication1\\ConsoleApplication1\\Program.cs:line\n&gt; 30 at System.AppDomain._nExecuteAssembly(RuntimeAssembly assembly,\n&gt; String[] args) at\n&gt; Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly() at\n&gt; System.Threading.ExecutionContext.RunInternal(ExecutionContext\n&gt; executionContext, ContextCallback callback, Object state, Boolean\n&gt; preserveSyncCtx) at\n&gt; System.Threading.ExecutionContext.Run(ExecutionContext\n&gt; executionContext, ContextCallback callback, Object state, Boolean\n&gt; preserveSyncCtx) at\n&gt; System.Threading.ExecutionContext.Run(ExecutionContext\n&gt; executionContext, ContextCallback callback, Object state) at\n&gt; System.Threading.ThreadHelper.ThreadStart() InnerException:\n&gt; edu.stanford.nlp.util.MetaClass.ClassCreationException\n&gt; HResult=-2146233088 Message=MetaClass couldn’t create public\n&gt; edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties)\n&gt; with args [sutime, {sutime.binders=0, annotators=tokenize, ssplit,\n&gt; pos, lemma, ner, parse, dcoref}] Source=stanford-corenlp-3.5.0\n&gt; StackTrace: at\n&gt; edu.stanford.nlp.util.MetaClass.ClassFactory.createInstance(Object[]\n&gt; params) at edu.stanford.nlp.util.MetaClass.createInstance(Object[]\n&gt; objects) at\n&gt; edu.stanford.nlp.util.ReflectionLoading.loadByReflection(String\n&gt; className, Object[] arguments) InnerException:\n&gt; java.lang.reflect.InvocationTargetException HResult=-2146233088\n&gt; Message=”” Source=stanford-corenlp-3.5.0 StackTrace: at __(Object[] )\n&gt; at\n&gt; Java_sun_reflect_ReflectionFactory.FastConstructorAccessorImpl.newInstance(Object[]\n&gt; args) at java.lang.reflect.Constructor.newInstance(Object[] initargs,\n&gt; CallerID ) at\n&gt; edu.stanford.nlp.util.MetaClass.ClassFactory.createInstance(Object[]\n&gt; params) InnerException:\n</code></pre>\n\n<p>Here is my code:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing java.util;\nusing java.io;\nusing edu.stanford.nlp.pipeline;\nusing Console = System.Console;\n\nnamespace ConsoleApplication1\n{\nclass Program\n{\nstatic void Main(string[] args)\n{\n// Path to the folder with models extracted from `stanford-corenlp-3.4-models.jar`\nvar jarRoot = @”D:\\Programming_SDKs\\stanford-corenlp-full-2015-01-30\\stanford-corenlp-3.5.1-models\\”;\n\n// Text for processing\nvar text = “Kosgi Santosh sent an email to Stanford University. He didn't get a reply.”;\n\n// Annotation pipeline configuration\nvar props = new Properties();\nprops.setProperty(“annotators”, “tokenize, ssplit, pos, lemma, ner, parse, dcoref”);\nprops.setProperty(“sutime.binders”, “0”);\n\n// We should change current directory, so StanfordCoreNLP could find all the model files automatically\nvar curDir = Environment.CurrentDirectory;\nSystem.IO.Directory.SetCurrentDirectory(jarRoot);\nvar pipeline = new StanfordCoreNLP(props);\nSystem.IO.Directory.SetCurrentDirectory(curDir);\n\n// Annotation\nvar annotation = new Annotation(text);\npipeline.annotate(annotation);\n\n// Result – Pretty Print\nusing (var stream = new ByteArrayOutputStream())\n{\npipeline.prettyPrint(annotation, new PrintWriter(stream));\nConsole.WriteLine(stream.toString());\nstream.close();\n}\n}\n}\n}\n</code></pre>\n",
    "score": 4,
    "creation_date": 1423358758,
    "view_count": 3275,
    "answer_count": 2,
    "tags": "c#;nlp;stanford-nlp"
  },
  {
    "question_id": 23117979,
    "title": "The distance between the meaning of two sentences",
    "body": "<p>I am looking for a way to measure the semantic distance between two sentences. Suppose we have the following sentences:</p>\n\n<pre><code>(S1) The beautiful cherry blossoms in Japan. \n(S2) The beautiful Japan.\n</code></pre>\n\n<p>S2 is created from S1 by eliminating the words \"cherry\", \"blossoms\" and \"in\". I want to define a function that gives a high distance between S1 and S2. The reason for this is that they do have significantly different meaning, since beautiful modifies cherry blossoms and not Japan.</p>\n",
    "score": 4,
    "creation_date": 1397674924,
    "view_count": 5318,
    "answer_count": 3,
    "tags": "nlp;semantics;linguistics;semantic-analysis"
  },
  {
    "question_id": 621880,
    "title": "Natural language parsing, practical example",
    "body": "<p>I am looking to use a natural language parsing library for a simple chat bot.  I can get the Parts of Speech tags, but I always wonder.  What do you do with the POS.  If I know the parts of the speech, what then?</p>\n\n<p>I guess it would help with the responses.  But what data structures and architecture could I use.</p>\n",
    "score": 4,
    "creation_date": 1236434819,
    "view_count": 3591,
    "answer_count": 4,
    "tags": "java;nlp"
  },
  {
    "question_id": 11340963,
    "title": "Natural language time parser",
    "body": "<p>I'm trying to parse strings containing (natural language) times to <em>hh:mm</em> time objects? For example:</p>\n\n<pre><code>\"ten past five\"\n\"quarter to three\"\n\"half past noon\"\n\"15 past 3\"\n\"13:35\"\n\"ten fourteen am\"\n</code></pre>\n\n<p>I've looked into <a href=\"http://chronic.rubyforge.org/\" rel=\"nofollow\">Chronic</a> for Ruby and <a href=\"http://natty.joestelmach.com/\" rel=\"nofollow\">Natty</a> for Java (as well as some other libraries) but both seem to focus on parsing dates. Strings like \"ten past five\" are not parsed correctly by either.</p>\n\n<p>Does anyone know of a library which suit my needs? Or should I maybe start working on my own parser?</p>\n",
    "score": 4,
    "creation_date": 1341478616,
    "view_count": 4133,
    "answer_count": 2,
    "tags": "python;parsing;time;nlp"
  },
  {
    "question_id": 4304938,
    "title": "How to recognize names from a text using php",
    "body": "<p>I want to extract name(firstnames and lastnames) from a text using php.\nExample:\nFrom text below I want to extract names(in this case Aline Wright and Jesse Wright)</p>\n\n<blockquote>\n  <p>Aline Wright is a cancer survivor,\n  amputee and a newlywed.  Wednesday\n  night she began to show signs she was\n  having a stroke.</p>\n  \n  <p>\"I started feeling some left arm\n  numbness and a facial droop,\" said\n  Aline.</p>\n  \n  <p>\"It appeared to me that I was probably\n  having a stroke.\"</p>\n  \n  <p>That's when her husband of four days,\n  Jesse Wright, put her in the car and\n  rushed her to the Erlanger Medical\n  Center. Wright knows an emergency. He\n  is a nurse technician at Erlanger.</p>\n</blockquote>\n",
    "score": 4,
    "creation_date": 1291042889,
    "view_count": 6184,
    "answer_count": 3,
    "tags": "php;nlp;named-entity-recognition"
  },
  {
    "question_id": 67328345,
    "title": "How to use forward() method instead of model.generate() for T5 model",
    "body": "<p>For my use case, I need to use the model.forward() instead of the model.generate() method\ni.e instead of the below code</p>\n<pre><code>outs = model.model.generate(input_ids=batch['source_ids'],\n                                 attention_mask=batch['source_mask'],\n                                 output_scores=True,\n                                 max_length=model.model_arguments.max_output_seq_length)\n\npreds_cleaned = [model.tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True) for ids in outs]\n</code></pre>\n<p>I need to use</p>\n<pre><code>model_outputs = model.model(\n            input_ids=batch[&quot;source_ids&quot;],\n            attention_mask=batch[&quot;source_mask&quot;],\n            labels=lm_labels.to(device),\n            decoder_attention_mask=batch['target_mask']\n        )\nlogits = model_outputs.logits\nsoftmax_logits = m(logits)\nmax_logits = torch.max(softmax_logits, dim=2)\n\n    \n</code></pre>\n<p>decoding these logits gives unprocessed text that has many issues like repetition of words at the end etc.\nWhat do I need to do to get the same result as model.generate() ?</p>\n",
    "score": 4,
    "creation_date": 1619758184,
    "view_count": 5155,
    "answer_count": 2,
    "tags": "nlp;huggingface-transformers"
  },
  {
    "question_id": 62999811,
    "title": "How to get an old release of a spacy model?",
    "body": "<p>I manage to install an old version of spacy with <code>pip3 install spacy==2.2.4</code>.</p>\n<p>However, when I follow this up with\n<code>python3 -m spacy download en_core_web_sm</code>, it downloads <code>en_core_web_sm-2.2.5.tar.gz</code>.</p>\n",
    "score": 4,
    "creation_date": 1595262462,
    "view_count": 13259,
    "answer_count": 3,
    "tags": "python-3.x;linux;nlp;spacy"
  },
  {
    "question_id": 42360957,
    "title": "Generate ngrams with Julia",
    "body": "<p>To generate word bigrams in Julia, I could simply zip through the original list and a list that drops the first element, e.g.:</p>\n\n<pre><code>julia&gt; s = split(\"the lazy fox jumps over the brown dog\")\n8-element Array{SubString{String},1}:\n \"the\"  \n \"lazy\" \n \"fox\"  \n \"jumps\"\n \"over\" \n \"the\"  \n \"brown\"\n \"dog\"  \n\njulia&gt; collect(zip(s, drop(s,1)))\n7-element Array{Tuple{SubString{String},SubString{String}},1}:\n (\"the\",\"lazy\")  \n (\"lazy\",\"fox\")  \n (\"fox\",\"jumps\") \n (\"jumps\",\"over\")\n (\"over\",\"the\")  \n (\"the\",\"brown\") \n (\"brown\",\"dog\") \n</code></pre>\n\n<p>To generate a trigram I could use the same <code>collect(zip(...))</code> idiom to get:</p>\n\n<pre><code>julia&gt; collect(zip(s, drop(s,1), drop(s,2)))\n6-element Array{Tuple{SubString{String},SubString{String},SubString{String}},1}:\n (\"the\",\"lazy\",\"fox\")  \n (\"lazy\",\"fox\",\"jumps\")\n (\"fox\",\"jumps\",\"over\")\n (\"jumps\",\"over\",\"the\")\n (\"over\",\"the\",\"brown\")\n (\"the\",\"brown\",\"dog\") \n</code></pre>\n\n<p>But I have to manually add in the 3rd list to zip through, <strong>is there an idiomatic way such that I can do any order of <em>n</em>-gram?</strong>  </p>\n\n<p>e.g. I'll like to avoid doing this to extract 5-gram:</p>\n\n<pre><code>julia&gt; collect(zip(s, drop(s,1), drop(s,2), drop(s,3), drop(s,4)))\n4-element Array{Tuple{SubString{String},SubString{String},SubString{String},SubString{String},SubString{String}},1}:\n (\"the\",\"lazy\",\"fox\",\"jumps\",\"over\") \n (\"lazy\",\"fox\",\"jumps\",\"over\",\"the\") \n (\"fox\",\"jumps\",\"over\",\"the\",\"brown\")\n (\"jumps\",\"over\",\"the\",\"brown\",\"dog\")\n</code></pre>\n",
    "score": 4,
    "creation_date": 1487661604,
    "view_count": 824,
    "answer_count": 3,
    "tags": "nlp;zip;julia;n-gram"
  },
  {
    "question_id": 31526644,
    "title": "What is the difference between corpus and lexicon in NLTK (python)",
    "body": "<p>Can someone tell me the difference between a <strong>Corpora</strong> ,<strong>corpus</strong> and <strong>lexicon</strong> in NLTK ?</p>\n\n<p>What is the <strong>movie data set</strong> ?</p>\n\n<p>what is <strong>Wordnet</strong> ?</p>\n",
    "score": 4,
    "creation_date": 1437427628,
    "view_count": 12925,
    "answer_count": 1,
    "tags": "machine-learning;nlp;nltk;corpus;lexical"
  },
  {
    "question_id": 2292681,
    "title": "Generate a list of English words containing consecutive consonant sounds",
    "body": "<p>Start with this:</p>\n\n<pre><code>[G|C] * [T] *\n</code></pre>\n\n<p>Write a program that generates this:</p>\n\n<pre><code>Cat\nCut\nCute\nCity &lt;-- NOTE: this one is wrong, because City has an \"ESS\" sound at the start.\nCaught\n...\nGate\nGotti\nGut\n...\nKit\nKite\nKate\nKata\nKatie\n</code></pre>\n\n<p>Another Example, This:</p>\n\n<p>[C] * [T] * [N]</p>\n\n<p>Should produce this:</p>\n\n<p>Cotton\n   Kitten </p>\n\n<p>Where should I start my research as I figure out how to write a program/script that does this?</p>\n",
    "score": 4,
    "creation_date": 1266531700,
    "view_count": 1858,
    "answer_count": 6,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 1468636,
    "title": "How to automatically excerpt user generated content?",
    "body": "<p>I run a website that allows users to write blog-post, I would really like to summarize the written content and use it to fill the <code>&lt;meta name=\"description\".../&gt;</code>-tag for example.</p>\n\n<p><em>What methods can I employ to automatically summarize/describe the contents of user generated content?<br>\nAre there any (preferably free) methods out there that have solved this problem?</em></p>\n\n<p>(I've seen other websites just copy the first 100 or so words but this strikes me as a sub-optimal solution.)</p>\n",
    "score": 4,
    "creation_date": 1253741713,
    "view_count": 603,
    "answer_count": 10,
    "tags": "artificial-intelligence;nlp;user-generated-content"
  },
  {
    "question_id": 57679668,
    "title": "Tokenizing emojis contiguous to words",
    "body": "<p>I am trying to tokenize strings that have the two following patterns:</p>\n\n<ul>\n<li>contiguous emojis, for instance \"Hey, 😍🔥\"</li>\n<li>emojis contiguous to words, for instance \"surprise💥 !!\"</li>\n</ul>\n\n<p>To do this, I have tried the <code>word_tokenize()</code> function from <code>nltk</code> (<a href=\"https://kite.com/python/docs/nltk.tokenize.word_tokenize\" rel=\"nofollow noreferrer\">doc</a>). However, it does not split the contiguous entities when emojis are involved.</p>\n\n<p>For instance,</p>\n\n<pre><code>from nltk.tokenize import word_tokenize\nword_tokenize(\"Hey, 😍🔥\")\n</code></pre>\n\n<p>output: <code>['Hey', ',', '😍🔥']</code></p>\n\n<p>I'd like to get: <code>['Hey', ',', '😍', '🔥']</code></p>\n\n<p>and</p>\n\n<pre><code>word_tokenize(\"surprise💥 !!\")\n</code></pre>\n\n<p>output: <code>['surprise💥', '!', '!']</code></p>\n\n<p>I'd like to get <code>['surprise', '💥', '!', '!']</code></p>\n\n<p>Therefore, I was thinking maybe using specific regex pattern could solve the issue but I don't know what pattern to use.</p>\n",
    "score": 4,
    "creation_date": 1566927135,
    "view_count": 3215,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 50555303,
    "title": "How to install gensim from anaconda prompt?",
    "body": "<p>When I put the following command in anaconda prompt</p>\n\n<pre><code>conda install -c anaconda gensim\n</code></pre>\n\n<p>Python stops working and shows the following error message:</p>\n\n<p><a href=\"https://i.sstatic.net/HsTdz.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/HsTdz.png\" alt=\"enter image description here\"></a></p>\n\n<p>How do I deal with this problem?</p>\n",
    "score": 4,
    "creation_date": 1527446154,
    "view_count": 31873,
    "answer_count": 5,
    "tags": "python;machine-learning;nlp;anaconda;conda"
  },
  {
    "question_id": 49097804,
    "title": "Spacy Entity from PhraseMatcher only",
    "body": "<p>I'm using <a href=\"http://spacy.io/\" rel=\"nofollow noreferrer\">Spacy</a> for a NLP project. I have a list of phrases I'd like to mark as a new entity type. I originally tried training a NER model but since there's a finite terminology list, I think simply using a Matcher should be easier. I see in the <a href=\"https://spacy.io/usage/linguistic-features#on_match\" rel=\"nofollow noreferrer\">documentation</a> that you can add entities to a document based on a Matcher. My question is: how do I do this for a <strong>new</strong> entity and not have the NER pipe label any other tokens as this entity? Ideally only tokens found via my matcher should be marked as the entity but I need to add it as a label to the NER model which then ends up labeling some as the entity.</p>\n\n<p>Any suggestions on how to best accomplish this? Thanks!</p>\n",
    "score": 4,
    "creation_date": 1520182109,
    "view_count": 3837,
    "answer_count": 2,
    "tags": "nlp;spacy"
  },
  {
    "question_id": 37972152,
    "title": "Predicting Missing Word in Sentence",
    "body": "<p>How can I predict a word that's missing from a sentence?</p>\n\n<p>I've seen many papers on predicting the <em>next word</em> in a sentence using an n-grams language model with frequency distributions from a set of training data. But instead I want to predict a missing word that's not necessarily at the end of the sentence. For example:</p>\n\n<blockquote>\n  <p>I took my ___ for a walk.</p>\n</blockquote>\n\n<p>I can't seem to find any algorithms that take advantage of the words after the blank; I guess I could ignore them, but they must add some value. And of course, a bi/trigram model doesn't work for predicting the first two words. </p>\n\n<p>What algorithm/pattern should I use? Or is there no advantage to using the words after the blank?</p>\n",
    "score": 4,
    "creation_date": 1466608674,
    "view_count": 3395,
    "answer_count": 2,
    "tags": "algorithm;machine-learning;nlp"
  },
  {
    "question_id": 26152381,
    "title": "can I use numerical features in crf model",
    "body": "<p>Is it possible/good to add numerical features in crf models? e.g. position in the sequence. </p>\n\n<p>I'm using <a href=\"http://www.chokkan.org/software/crfsuite/manual.html\" rel=\"nofollow\">CRFsuite</a>. It seems all the features will be converted to string, e.g. 'pos=0', 'pos=1', which then lose it's meaning as euclidean distance. </p>\n\n<p>Or should I use them to train another model, e.g. svm, then ensemble with crf models? </p>\n",
    "score": 4,
    "creation_date": 1412206844,
    "view_count": 3604,
    "answer_count": 3,
    "tags": "machine-learning;nlp;data-mining;data-modeling;crf"
  },
  {
    "question_id": 2659881,
    "title": "Does knowing a Natural Language well help with Programming?",
    "body": "<p>We all hear that math at least helps a little bit with programming. My question though, does English or other natural language skills help with programming? I know it has to help with technical documentation, but what about actual programming? Are certain constructs in a programming language also there in natural languages? Does knowing how to write a 20 page research paper help with writing a 20k loc programming project? </p>\n",
    "score": 4,
    "creation_date": 1271535287,
    "view_count": 323,
    "answer_count": 5,
    "tags": "language-agnostic;nlp"
  },
  {
    "question_id": 64120659,
    "title": "How to store Bag of Words or Embeddings in a Database",
    "body": "<p>I would like to store vector features, like Bag-of-Words or Word-Embedding vectors of a large number of texts, in a dataset, stored in a SQL Database.\nWhat're the data structures and the best practices to save and retrieve these features?</p>\n",
    "score": 4,
    "creation_date": 1601386170,
    "view_count": 6595,
    "answer_count": 5,
    "tags": "python;database;nlp;dataset;word-embedding"
  },
  {
    "question_id": 60083593,
    "title": "How to retrieve the main intent of a sentence using spacy or nltk?",
    "body": "<p><strong>I have a use case where I want to extract main meaningful part of the sentence using spacy or nltk or any NLP libraries.</strong></p>\n\n<p><strong>Example sentence1:</strong> \"How Can I raise my voice against harassment\"\n<strong>Intent would be:</strong> \"raise voice against harassment\"</p>\n\n<p><strong>Example sentence2:</strong> \"Donald Duck is created by which cartoonist/which man/whom ?\"\n<strong>Intent would be:</strong> \"Donald duck is created by\"</p>\n\n<p><strong>Example sentence3:</strong> \"How to retrieve the main intent of a sentence using spacy or nltk\" ?\n<strong>Intent:</strong> \"retrieve main intent of sentence using spacy nltk\"</p>\n\n<p>I am new to dependency parsing and don't exactly know how to do this. Please help me.</p>\n",
    "score": 4,
    "creation_date": 1580934342,
    "view_count": 5858,
    "answer_count": 1,
    "tags": "nlp;nltk;spacy;pos-tagger;dependency-parsing"
  },
  {
    "question_id": 52891639,
    "title": "Spacy NLP with data from a Pandas DataFrame",
    "body": "<p>I have a large pandas data frame of survey string responses, and we would like to trial some features of Spacy's NLP. We are just exploring the capabilities at the moment, but struggling with how to format the data into a format that works with the nlp function of spacy.</p>\n\n<p>Eventually we would like to be able to look at popular topics in the string responses against their user data. </p>\n\n<p>How do I run the nlp pipeline on a column of a dataframe? Or am I going around this the wrong way?</p>\n",
    "score": 4,
    "creation_date": 1539949185,
    "view_count": 9018,
    "answer_count": 3,
    "tags": "python;pandas;dataframe;nlp;spacy"
  },
  {
    "question_id": 28365626,
    "title": "How to output NLTK chunks to file?",
    "body": "<p>I have this python script where I am using nltk library to parse,tokenize,tag and chunk some lets say random text from the web.</p>\n\n<p>I need to format and write in a file the output of <code>chunked1</code>,<code>chunked2</code>,<code>chunked3</code>. These have type <code>class 'nltk.tree.Tree'</code></p>\n\n<p>More specifically I need to write only the lines that match the regular expressions <code>chunkGram1</code>, <code>chunkGram2</code>, <code>chunkGram3</code>.</p>\n\n<p>How can i do that?</p>\n\n<pre><code>#! /usr/bin/python2.7\n\nimport nltk\nimport re\nimport codecs\n\nxstring = [\"An electronic library (also referred to as digital library or digital repository) is a focused collection of digital objects that can include text, visual material, audio material, video material, stored as electronic media formats (as opposed to print, micro form, or other media), along with means for organizing, storing, and retrieving the files and media contained in the library collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals, organizations, or affiliated with established physical library buildings or institutions, or with academic institutions.[1] The electronic content may be stored locally, or accessed remotely via computer networks. An electronic library is a type of information retrieval system.\"]\n\n\ndef processLanguage():\n    for item in xstring:\n        tokenized = nltk.word_tokenize(item)\n        tagged = nltk.pos_tag(tokenized)\n        #print tokenized\n        #print tagged\n\n        chunkGram1 = r\"\"\"Chunk: {&lt;JJ\\w?&gt;*&lt;NN&gt;}\"\"\"\n        chunkGram2 = r\"\"\"Chunk: {&lt;JJ\\w?&gt;*&lt;NNS&gt;}\"\"\"\n        chunkGram3 = r\"\"\"Chunk: {&lt;NNP\\w?&gt;*&lt;NNS&gt;}\"\"\"\n\n        chunkParser1 = nltk.RegexpParser(chunkGram1)\n        chunked1 = chunkParser1.parse(tagged)\n\n        chunkParser2 = nltk.RegexpParser(chunkGram2)\n        chunked2 = chunkParser2.parse(tagged)\n\n        chunkParser3 = nltk.RegexpParser(chunkGram3)\n        chunked3 = chunkParser2.parse(tagged)\n\n        #print chunked1\n        #print chunked2\n        #print chunked3\n\n        # with codecs.open('path\\to\\file\\output.txt', 'w', encoding='utf8') as outfile:\n\n            # for i,line in enumerate(chunked1):\n                # if \"JJ\" in line:\n                    # outfile.write(line)\n                # elif \"NNP\" in line:\n                    # outfile.write(line)\n\n\n\nprocessLanguage()\n</code></pre>\n\n<p>For the time being when I am trying to run it I get error:</p>\n\n<pre><code>`Traceback (most recent call last):\n  File \"sentdex.py\", line 47, in &lt;module&gt;\n    processLanguage()\n  File \"sentdex.py\", line 40, in processLanguage\n    outfile.write(line)\n  File \"C:\\Python27\\lib\\codecs.py\", line 688, in write\n    return self.writer.write(data)\n  File \"C:\\Python27\\lib\\codecs.py\", line 351, in write\n    data, consumed = self.encode(object, self.errors)\nTypeError: coercing to Unicode: need string or buffer, tuple found`\n</code></pre>\n\n<p><strong>edit:</strong> After @Alvas answer I managed to do what I wanted. However now, I would like to know how I could strip all non-ascii characters from a text <em>corpus</em>. example:</p>\n\n<pre><code>#store cleaned file into variable\nwith open('path\\to\\file.txt', 'r') as infile:\n    xstring = infile.readlines()\ninfile.close\n\n    def remove_non_ascii(line):\n        return ''.join([i if ord(i) &lt; 128 else ' ' for i in line])\n\n    for i, line in enumerate(xstring):\n        line = remove_non_ascii(line)\n\n#tokenize and tag text\ndef processLanguage():\n    for item in xstring:\n        tokenized = nltk.word_tokenize(item)\n        tagged = nltk.pos_tag(tokenized)\n        print tokenized\n        print tagged\nprocessLanguage()\n</code></pre>\n\n<p>This above is taken from another answer here in S/O. However it doesn't seem to work. What might be wrong? The error I am getting is:</p>\n\n<pre><code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position\nnot in range(128)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1423224961,
    "view_count": 1946,
    "answer_count": 2,
    "tags": "python;regex;file-io;nlp;nltk"
  },
  {
    "question_id": 11897501,
    "title": "Caching of the data of a big file in memory in java",
    "body": "<p>Hi I am working on Spelling Corrector project of Natural Language processing and I am supposed read data from a file whose size is <strike>6.2 MB</strike> 1 GB. While it is working fine, the problem that I am facing is that every time I run the java program I have to load the data in to the memory and it is taking same amount of time every time it is run. </p>\n\n<p>Is there any way this data can cached in to the memory in java?Can any one suggest me some work around of it? </p>\n\n<p>Basically what I want to know is that What is procedure of storing content of a large file in memory so that I dont have to read it again? lets say file is of GB. </p>\n",
    "score": 4,
    "creation_date": 1344585061,
    "view_count": 5988,
    "answer_count": 4,
    "tags": "java;algorithm;memory-management;garbage-collection;nlp"
  },
  {
    "question_id": 6837566,
    "title": "Can NLTK&#39;s XMLCorpusReader be used on a multi-file corpus?",
    "body": "<p>I'm trying to use NLTK to do some work on the <a href=\"http://nytnlp.googlegroups.com/web/new_york_times_annotated_corpus.pdf?gda=YSD3XlUAAADIiIQXTaZc7t_Z2e7P-nlf8Jldl4UE6-KChg2Y0XdPoEnDJZHGdnBmEWV8W-XIw5P180hAuqa8U_xChid0w715HJ_FiG1oec6ngyrQwZquuxrtYix3qocOGWUY90Yyf_g\" rel=\"nofollow\">New York Times Annotated Corpus</a> which contains an XML file for each article (in the News Industry Text Format NITF).</p>\n\n<p>I can parse individual documents with no problem like so:</p>\n\n<pre><code>from nltk.corpus.reader import XMLCorpusReader\nreader = XMLCorpusReader('nltk_data/corpora/nytimes/1987/01/01', r'0000000.xml')\n</code></pre>\n\n<p>I need to work on the whole corpus though.\nI tried doing this:</p>\n\n<pre><code>reader = XMLCorpusReader('corpora/nytimes', r'.*')\n</code></pre>\n\n<p>but this doesn't create a useable reader object. For instance</p>\n\n<pre><code>len(reader.words())\n</code></pre>\n\n<p>returns</p>\n\n<pre><code>raise TypeError('Expected a single file identifier string')\nTypeError: Expected a single file identifier string\n</code></pre>\n\n<p>How do I read this corpus into NLTK?</p>\n\n<p>I'm new to NLTK so any help is greatly appreciated.</p>\n",
    "score": 4,
    "creation_date": 1311720745,
    "view_count": 3883,
    "answer_count": 3,
    "tags": "python;xml;nltk;nlp"
  },
  {
    "question_id": 3882921,
    "title": "Stop-word elimination and stemmer in python",
    "body": "<p>I have a somewhat large document and want to do stop-word elimination and stemming on the words of this document with Python. Does anyone know an of the shelf package for these?\nIf not a code which is fast enough for large documents is also welcome.\nThanks</p>\n",
    "score": 4,
    "creation_date": 1286463190,
    "view_count": 2311,
    "answer_count": 2,
    "tags": "python;nlp;stemming;stop-words"
  },
  {
    "question_id": 1224316,
    "title": "identify tense in php",
    "body": "<p>I'm looking for a way to analyze a string of text and find out in which tense it was written, for example : \"I'm going to the store\" == current, \"I bought a car\" == past ect..</p>\n\n<p>Any tips on how I could this done?</p>\n",
    "score": 4,
    "creation_date": 1249329547,
    "view_count": 2264,
    "answer_count": 6,
    "tags": "php;regex;nlp;linguistics"
  },
  {
    "question_id": 68127754,
    "title": "Removal of Stop Words and Stemming/Lemmatization for BERTopic",
    "body": "<p>For Topic Modelling, I'm trying out the BERTopic: <a href=\"https://maartengr.github.io/BERTopic/index.html\" rel=\"nofollow noreferrer\">Link</a></p>\n<p>I'm little confused here, I am trying out the BERTopic on my custom Dataset. <br />\nSince BERT was trained in such a way that it holds the semantic meaning of the text/document,\nShould I be removing the stop words and stem/lemmatize my documents before passing it onto BERTopic?\nBecause I'm afraid if these stopwords might land into my topics as salient terms which they are not</p>\n<p>Suggestions and Advices please!</p>\n",
    "score": 4,
    "creation_date": 1624609221,
    "view_count": 9364,
    "answer_count": 3,
    "tags": "python;nlp;bert-language-model;topic-modeling"
  },
  {
    "question_id": 68003864,
    "title": "How can I make spaCy matches case Insensitive",
    "body": "<p>How can I make spaCy case insensitive?</p>\n<p>Is there any code snippet that i should add or something because I couldn't get entities that are not in uppercase?</p>\n<pre><code>import spacy\nimport pandas as pd\n\nfrom spacy.pipeline import EntityRuler\nnlp = spacy.load('en_core_web_sm', disable = ['ner'])\nruler = nlp.add_pipe(&quot;entity_ruler&quot;)\n\n\nflowers = [&quot;rose&quot;, &quot;tulip&quot;, &quot;african daisy&quot;]\nfor f in flowers:\n    ruler.add_patterns([{&quot;label&quot;: &quot;flower&quot;, &quot;pattern&quot;: f}])\nanimals = [&quot;cat&quot;, &quot;dog&quot;, &quot;artic fox&quot;]\nfor a in animals:\n    ruler.add_patterns([{&quot;label&quot;: &quot;animal&quot;, &quot;pattern&quot;: a}])\n\n\n\nresult={}\ndoc = nlp(&quot;CAT and Artic fox, plant african daisy&quot;)\nfor ent in doc.ents:\n        result[ent.label_]=ent.text\ndf = pd.DataFrame([result])\nprint(df)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1623850843,
    "view_count": 2797,
    "answer_count": 2,
    "tags": "python;pandas;nlp;spacy"
  },
  {
    "question_id": 64685243,
    "title": "Getting sentence embedding from huggingface Feature Extraction Pipeline",
    "body": "<p>How do i get an embedding for the whole sentence from huggingface's feature extraction pipeline?</p>\n<p>I understand how to get the features for each token (below) but how do i get the overall features for the sentence as a whole?</p>\n<pre><code>feature_extraction = pipeline('feature-extraction', model=&quot;distilroberta-base&quot;, tokenizer=&quot;distilroberta-base&quot;)\nfeatures = feature_extraction(&quot;i am sentence&quot;)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1604512342,
    "view_count": 16286,
    "answer_count": 4,
    "tags": "machine-learning;nlp;huggingface-transformers;spacy-transformers"
  },
  {
    "question_id": 48980120,
    "title": "Is it possible to parse emojis using spaCy?",
    "body": "<p>Is it possible to tokenize emojis like <code>:)</code>, <code>:(</code>, <code>;~(</code> properly using the spaCy Python library? e.g. If I run the following code:</p>\n\n<pre><code>import spacy\n\nnlp = spacy.load('en')\ndoc = nlp(\"Hello bright world :)\")\n</code></pre>\n\n<p>And then visualize the doc with <a href=\"https://spacy.io/usage/visualizers\" rel=\"nofollow noreferrer\">displaCy</a>:</p>\n\n<p><a href=\"https://i.sstatic.net/ZQjYk.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/ZQjYk.png\" alt=\"enter image description here\"></a></p>\n\n<p>It incorrectly parses <code>world :)</code> as one token. How can I modify spaCy so it recognizes these additional symbols? Thanks.</p>\n\n<p><strong>edit:</strong> Found the following: <a href=\"https://github.com/ines/spacymoji\" rel=\"nofollow noreferrer\">https://github.com/ines/spacymoji</a> but I think it only supports Unicode emojis like ✨ and not ASCII ones like <code>:)</code>?</p>\n",
    "score": 4,
    "creation_date": 1519606888,
    "view_count": 3488,
    "answer_count": 2,
    "tags": "python;nlp;emoji;spacy"
  },
  {
    "question_id": 42422902,
    "title": "How can I create a string in english letters from another language word?",
    "body": "<p>I need to find a way to rewrite words(translit) from some languages into English language. For example <code>привет</code> (in Russian) sounds like <code>privet</code> (in English).</p>\n\n<p>Meaning and grammar don't matter, but I'd like it to have a more similar sounding. Everything should be in Python, I have diligently looked up on the internet and haven't found a good approach.</p>\n\n<p>For example, something similar to this:</p>\n\n<pre><code>translit(\"юу со беутифул\", \"ru\") = juu so beutiful\n\ntranslit(\"кар\", \"ru\") = kar\n</code></pre>\n",
    "score": 4,
    "creation_date": 1487872512,
    "view_count": 4401,
    "answer_count": 4,
    "tags": "python;nlp;nltk;translation"
  },
  {
    "question_id": 39301481,
    "title": "Is natural language Turing complete?",
    "body": "<p>I'm pretty sure a human language (e.g. English) is powerful enough to simulate a Turing machine, which would make it Turing complete. However, that would imply natural languages are no more or less expressive than programming languages, which seems questionable.</p>\n\n<p>Is natural language Turing complete?</p>\n",
    "score": 4,
    "creation_date": 1472855485,
    "view_count": 4715,
    "answer_count": 2,
    "tags": "nlp;programming-languages;theory;turing-complete"
  },
  {
    "question_id": 38986863,
    "title": "What is the difference between a trait, freetext and keyword and which should I use in wit.ai",
    "body": "<p>I was wondering if somebody could elaborate on the difference between trait, freetext and keywords as search strategies in wit.ai entities? I don't see that much about it on the docs and haven't been able to find anything about it elsewhere. Could somebody please give a few use cases of when each would be the best choice? </p>\n",
    "score": 4,
    "creation_date": 1471397841,
    "view_count": 1130,
    "answer_count": 1,
    "tags": "nlp;wit.ai"
  },
  {
    "question_id": 29253913,
    "title": "Error while instaling Open GRM thrax",
    "body": "<p>I have already installed Open Fst in Ubuntu and its working fine. Now i'm trying to install Open GRM thrax. I have tried installing with 2 different versions of thrax.</p>\n\n<p>Thrax version 1.1.0:</p>\n\n<pre><code>thraxOpenGrm/thrax-1.1.0$ ./configure\n</code></pre>\n\n<p>below is the error that i get.</p>\n\n<pre><code>checking how to hardcode library paths into programs... immediate\nchecking for bison... no\nchecking for byacc... no\nchecking for std::tr1::hash&lt;long long unsigned&gt;... yes\nchecking for __gnu_cxx::slist&lt;int&gt;... yes\nchecking fst/fst.h usability... yes\nchecking fst/fst.h presence... no\nconfigure: WARNING: fst/fst.h: accepted by the compiler, rejected by the preprocessor!\nconfigure: WARNING: fst/fst.h: proceeding with the compiler's result\nchecking for fst/fst.h... yes\nchecking fst/extensions/far/far.h usability... yes\nchecking fst/extensions/far/far.h presence... no\nconfigure: WARNING: fst/extensions/far/far.h: accepted by the compiler, rejected by the preprocessor!\nconfigure: WARNING: fst/extensions/far/far.h: proceeding with the compiler's result\nchecking for fst/extensions/far/far.h... yes\nchecking fst/extensions/pdt/pdt.h usability... no\nchecking fst/extensions/pdt/pdt.h presence... no\nchecking for fst/extensions/pdt/pdt.h... no\nconfigure: error: fst/extensions/pdt/pdt.h header not found\n</code></pre>\n\n<p>Thrax version 0.1.0:</p>\n\n<pre><code>thraxOpenGrm/thrax-0.1.0$ ./configure\n</code></pre>\n\n<p>below is the error that i get.</p>\n\n<pre><code>checking how to hardcode library paths into programs... immediate\nchecking for bison... no\nchecking for byacc... no\nchecking for std::tr1::hash&lt;long long unsigned&gt;... yes\nchecking for __gnu_cxx::slist&lt;int&gt;... yes\nchecking fst/fst.h usability... no\nchecking fst/fst.h presence... no\nchecking for fst/fst.h... no\nconfigure: error: fst/fst.h header not found\n</code></pre>\n\n<p>It throws different errors with different thrax versions. I read a solution in this forum.</p>\n\n<p><a href=\"http://www.openfst.org/twiki/bin/view/Forum/GrmThraxForum\" rel=\"nofollow\">http://www.openfst.org/twiki/bin/view/Forum/GrmThraxForum</a></p>\n\n<p>It says openfst must be 'built' with <code>./configure --enable-far=true</code> . i uninstalled openfst and installed it using <code>./configure --enable-far=true</code> and also with <code>./configure --enable-far</code>. The error still persists. </p>\n",
    "score": 4,
    "creation_date": 1427281538,
    "view_count": 2924,
    "answer_count": 4,
    "tags": "c++;ubuntu-14.04;text-mining;text-analysis;openfst"
  },
  {
    "question_id": 15916143,
    "title": "Checking English Grammar with NLTK",
    "body": "<p>I'm starting to use the <a href=\"http://nltk.org/\" rel=\"nofollow noreferrer\">NLTK library</a>, and I want to check whether a sentence in English is correct or not.</p>\n\n<p>Example:</p>\n\n<p>\"He see Bob\" - not correct</p>\n\n<p>\"He sees Bob\" - correct</p>\n\n<p>I read <a href=\"http://www.ling.helsinki.fi/kit/2008s/clt231/nltk-0.9.5/doc/en/ch08.html\" rel=\"nofollow noreferrer\">this</a>, but it's quite hard for me. \nI need an easier example.</p>\n",
    "score": 4,
    "creation_date": 1365562682,
    "view_count": 4671,
    "answer_count": 1,
    "tags": "nlp;grammar;nltk;context-free-grammar"
  },
  {
    "question_id": 6122545,
    "title": "Stop words and stemmer in java",
    "body": "<p>I'm thinking of putting a stop words in my similarity program and then a stemmer (going for porters 1 or 2 depends on what easiest to implement)</p>\n\n<p>I was wondering that since I read my text from files as whole lines and save them as a long string, so if I got two strings ex.</p>\n\n<pre><code>String one = \"I decided buy something from the shop.\";\nString two = \"Nevertheless I decidedly bought something from a shop.\";\n</code></pre>\n\n<p>Now that I got those strings </p>\n\n<p>Stemming:\nCan I just use the stemmer algoritmen directly on it, save it as a String and then continue working on the similarity like I did before implementing the stemmer in the program, like running one.stem(); kind of thing? </p>\n\n<p>Stop word:\nHow does this work out? O.o\nDo I just use; one.replaceall(\"I\", \"\"); or is there some specific way to use for this proces? I want to keep working with the string and get a string before using the similarity algorithms on it to get the similarity. Wiki doesn't say a lot. </p>\n\n<p>Hope you can help me out! Thanks. </p>\n\n<p>Edit: It is for a school-related project where I'm writing a paper on similarity between different algorithms so I don't think I'm allowed to use lucene or other libraries that does the work for me. Plus I would like to try and understand how it works before I start using the libraries like Lucene and co. Hope it's not too much a bother ^^</p>\n",
    "score": 4,
    "creation_date": 1306317214,
    "view_count": 18627,
    "answer_count": 3,
    "tags": "java;nlp;stop-words;porter-stemmer"
  },
  {
    "question_id": 4166041,
    "title": "How to make use of USE SharpNlp in my C# application",
    "body": "<p>I require POS tagging for my files in the corpus. \nI have successfully followed the installation instructions of <a href=\"http://sharpnlp.codeplex.com/wikipage?title=Installation%20Instructions\" rel=\"nofollow noreferrer\"><strong>SharpNlp</strong></a><br>\nI am using the binary version</p>\n\n<pre><code>I created a new c# project in:       E:\\sharp\\sharpapp\nlocation of Models Folder is:        E:\\sharp\\sharpapp\\bin\\Models\nlocation of my SharpNlp Binary is:   E:\\sharp\\SharpNLP-1.0.2529-Bin\n</code></pre>\n\n<p>I have also followed the instructions to modify both .config files \"ParseTree.Exe\" and \"ToolsExamples.Exe\"</p>\n\n<p>Now in my c# project I have a class called tagging.cs where I have to access my corpus text files and do POS tagging for those files. Can anybody help me how can I make use of SharpNlp to do so</p>\n\n<p>Please provide steps to do so.</p>\n",
    "score": 4,
    "creation_date": 1289574982,
    "view_count": 12082,
    "answer_count": 2,
    "tags": "c#;nlp"
  },
  {
    "question_id": 2749150,
    "title": "How to estimate the quality of a web page?",
    "body": "<p>I'm doing a university project, that must gather and combine data on a user provided topic. The problem I've encountered is that Google search results for many terms are polluted with low quality autogenerated pages and if I use them, I can end up with wrong facts. How is it possible to estimate the quality/trustworthiness of a page? </p>\n\n<p>You may think \"nah, Google engineers are working on the problem for 10 years and he's asking for a solution\", but if you think about it, SE must provide up-to-date content and if it marks a good page as a bad one, users will be dissatisfied. I don't have such limitations, so if the algorithm accidentally marks as bad some good pages, that wouldn't be a problem.</p>\n\n<p>Here's an example:\nSay the input is <code>buy aspirin in south la</code>. Try to Google search it. The first 3 results are already deleted from the sites, but the fourth one is interesting: <code>radioteleginen.ning.com/profile/BuyASAAspirin</code> (I don't want to make an active link)</p>\n\n<p>Here's the first paragraph of the text:</p>\n\n<blockquote>\n  <p>The bare of purchasing prescription drugs from Canada is big\n  in the U.S. at this moment. This is\n  because in the U.S. prescription drug\n  prices bang skyrocketed making it\n  arduous for those who bang limited or\n  concentrated incomes to buy their much\n  needed medications. Americans pay more\n  for their drugs than anyone in the\n  class.</p>\n</blockquote>\n\n<p>The rest of the text is similar and then the list of related keywords follows. This is what I think is a low quality page. While this particular text seems to make sense (except it's horrible), the other examples I've seen (yet can't find now) are just some rubbish, whose purpose is to get some users from Google and get banned 1 day after creation.</p>\n",
    "score": 4,
    "creation_date": 1272697271,
    "view_count": 397,
    "answer_count": 5,
    "tags": "machine-learning;nlp;spam;information-retrieval"
  },
  {
    "question_id": 970487,
    "title": "Parsing text into sentences?",
    "body": "<p>I am trying to parse text off of a PDF page into sentences but it is much more difficult than I had anticipated.  There are a whole lot of special cases to consider such as initials, decimals, quotations, etc which contain periods but do not necessarily end the sentence.</p>\n\n<p>I was curious if anyone here was familiar with an NLP library for C or C++ that could help me out with this task or just offer any advice?</p>\n\n<p>Thank you for any help.</p>\n",
    "score": 4,
    "creation_date": 1244558739,
    "view_count": 3601,
    "answer_count": 3,
    "tags": "c++;c;parsing;nlp"
  },
  {
    "question_id": 723397,
    "title": "Compare many text files that contain duplicate &quot;stubs&quot; from the previous and next file and remove duplicate text automatically",
    "body": "<p>I have a large number of text files (1000+) each containing an article from an academic journal. Unfortunately each article's file also contains a \"stub\" from the end of the previous article (at the beginning) and from the beginning of the next article (at the end). </p>\n\n<p>I need to remove these stubs in preparation for running a frequency analysis on the articles because the stubs constitute duplicate data.</p>\n\n<p>There is no simple field that marks the beginning and end of each article in all cases. However, the duplicate text does seem to formatted the same and on the same line in both cases.</p>\n\n<p>A script that compared each file to the next file and then removed 1 copy of the duplicate text would be perfect. This seems like it would be a pretty common issue when programming so I am surprised that I haven't been able to find anything that does this.</p>\n\n<p>The file names sort in order, so a script that compares each file to the next sequentially should work. E.G.</p>\n\n<pre>\nbul_9_5_181.txt\nbul_9_5_186.txt\n</pre>\n\n<p>are two articles, one starting on page 181 and the other on page 186. Both of these articles are included bellow. </p>\n\n<p>There is two volumes of test data located at [<a href=\"http://drop.io/fdsayre][1]\" rel=\"nofollow noreferrer\">http://drop.io/fdsayre][1]</a></p>\n\n<p>Note: I am an academic doing content analysis of old journal articles for a project in the history of psychology. I am no programmer, but I do have 10+ years experience with linux and can usually figure things out as I go. </p>\n\n<p>Thanks for your help</p>\n\n<p><b>FILENAME: bul_9_5_181.txt</b></p>\n\n<p>\nSYN&amp;STHESIA</p>\n\n<p>ISI</p>\n\n<p>the majority of Portugese words signifying black objects or ideas relating to black. This association is, admittedly, no true synsesthesia, but the author believes that it is only a matter of degree between these logical and spontaneous associations and genuine cases of colored audition.\nREFERENCES</p>\n\n<p>DOWNEY, JUNE E. A Case of Colored Gustation. Amer. J. of Psycho!., 1911, 22, S28-539MEDEIROS-E-ALBUQUERQUE. Sur un phenomene de synopsie presente par des millions de sujets. / . de psychol. norm, et path., 1911, 8, 147-151. MYERS, C. S. A Case of Synassthesia. Brit. J. of Psychol., 1911, 4, 228-238.</p>\n\n<p>AFFECTIVE PHENOMENA — EXPERIMENTAL\nBY PROFESSOR JOHN F. .SHEPARD\nUniversity of Michigan</p>\n\n<p>Three articles have appeared from the Leipzig laboratory during the year. Drozynski (2) objects to the use of gustatory and olfactory stimuli in the study of organic reactions with feelings, because of the disturbance of breathing that may be involved. He uses rhythmical auditory stimuli, and finds that when given at different rates and in various groupings, they are accompanied by characteristic feelings in each subject. He records the chest breathing, and curves from a sphygmograph and a water plethysmograph. Each experiment began with a normal record, then the stimulus was given, and this was followed by a contrast stimulus; lastly, another normal was taken. The length and depth of breathing were measured (no time line was recorded), and the relation of length of inspiration to length of expiration was determined. The length and height of the pulsebeats were also measured. Tabular summaries are given of the number of times the author finds each quantity to have been increased or decreased during a reaction period with each type of feeling. The feeling state accompanying a given rhythm is always complex, but the result is referred to that dimension which seemed to be dominant. Only a few disconnected extracts from normal and reaction periods are reproduced from the records. The author states that excitement gives increase in the rate and depth of breathing, in the inspiration-expiration ratio, and in the rate and size of pulse. There are undulations in the arm volume. In so far as the effect is quieting, it causes decrease in rate and depth of</p>\n\n<p>182</p>\n\n<p>JOHN F. SHEPARD</p>\n\n<p>breathing, in the inspiration-expiration ratio, and in the pulse rate and size. The arm volume shows a tendency to rise with respiratory waves. Agreeableness shows</p>\n",
    "score": 4,
    "creation_date": 1239054827,
    "view_count": 1229,
    "answer_count": 7,
    "tags": "text;scripting;nlp;duplicate-data"
  },
  {
    "question_id": 69091576,
    "title": "String comparison with BERT seems to ignore &quot;not&quot; in sentence",
    "body": "<p>I implemented a string comparison method using SentenceTransformers and BERT like following</p>\n<pre><code>from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n\nsentences = [\n    &quot;I'm a good person&quot;,\n    &quot;I'm not a good person&quot;\n]\n\nsentence_embeddings = model.encode(sentences)\n\ncosine_similarity(\n    [sentence_embeddings[0]],\n    sentence_embeddings[1:]\n)\n</code></pre>\n<p>Notice how my sentence examples are very similar but with the opposite meaning. The problem is the cosine similarity returns 0.9, indicating that these two strings are very similar in context when I expected it to return something closer to zero, as they have the opposite meanings.</p>\n<p>How can I adapt my code to return a more accurate result?</p>\n",
    "score": 4,
    "creation_date": 1631031485,
    "view_count": 1403,
    "answer_count": 3,
    "tags": "nlp;bert-language-model;transformer-model;sentence-similarity;sentence-transformers"
  },
  {
    "question_id": 64823090,
    "title": "How to search for (separable) phrases in text",
    "body": "<p>I'm looking for a way to search for a phrase or an idiomatic expression in a text, regardless of tense or possible prepositions / adverbs, e.g. if I'm looking for <pre>call off</pre> I would also like to find usages like <pre>My boss <strong>call</strong>ed the meeting <strong>off</strong>.</pre></p>\n<p>Is this possible (using spacy)? If so, what feature or ability of NLP am I looking for?</p>\n",
    "score": 4,
    "creation_date": 1605279349,
    "view_count": 650,
    "answer_count": 2,
    "tags": "nlp;spacy"
  },
  {
    "question_id": 62386631,
    "title": "Cannot import BertModel from transformers",
    "body": "<p>I am trying to import BertModel from transformers, but it fails. This is code I am using</p>\n\n<pre><code>from transformers import BertModel, BertForMaskedLM\n</code></pre>\n\n<p>This is the error I get</p>\n\n<pre><code>ImportError: cannot import name 'BertModel' from 'transformers'\n</code></pre>\n\n<p>Can anyone help me fix this?</p>\n",
    "score": 4,
    "creation_date": 1592218034,
    "view_count": 22561,
    "answer_count": 4,
    "tags": "python;nlp;pytorch;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 51537441,
    "title": "How to combine both word embeddings and pos embedding together to build the classifier",
    "body": "<p>You known POS is like 'NP', 'VERB'. How can I combine these features to word2vec?</p>\n\n<p>Just like the follow vectors?</p>\n\n<pre><code>keyword    V1         V2          V3         V4            V5         V6   \ncorruption 0.07397  0.290874    -0.170812   0.085428     'VERB'    'NP' \npeople      ..............................................................\nbudget      ...........................................................\n</code></pre>\n",
    "score": 4,
    "creation_date": 1532603427,
    "view_count": 7642,
    "answer_count": 2,
    "tags": "nlp;word2vec;word-embedding;part-of-speech"
  },
  {
    "question_id": 51233632,
    "title": "word2vec gensim multiple languages",
    "body": "<p>This problem is going completely over my head. I am training a Word2Vec model using gensim. I have provided data in multiple languages i.e. English and Hindi. When I am trying to find the words closest to 'man', this is what I am getting:</p>\n\n<pre><code>model.wv.most_similar(positive = ['man'])\nOut[14]: \n[('woman', 0.7380284070968628),\n ('lady', 0.6933152675628662),\n ('monk', 0.6662989258766174),\n ('guy', 0.6513140201568604),\n ('soldier', 0.6491742134094238),\n ('priest', 0.6440571546554565),\n ('farmer', 0.6366012692451477),\n ('sailor', 0.6297377943992615),\n ('knight', 0.6290514469146729),\n ('person', 0.6288090944290161)]\n--------------------------------------------\n</code></pre>\n\n<p>Problem is, these are all English words. Then I tried to find similarity between same meaning Hindi and English words, </p>\n\n<pre><code>model.similarity('man', 'आदमी')\n__main__:1: DeprecationWarning: Call to deprecated `similarity` (Method will \nbe removed in 4.0.0, use self.wv.similarity() instead).\nOut[13]: 0.078265618974427215\n</code></pre>\n\n<p>This accuracy should have been better than all the other accuracies. The Hindi corpus I have has been made by translating the English one. Hence the words appear in similar contexts. Hence they should be close.</p>\n\n<p>This is what I am doing here:</p>\n\n<pre><code>#Combining all the words together.\nall_reviews=HindiWordsList + EnglishWordsList\n\n#Training FastText model\ncpu_count=multiprocessing.cpu_count()\nmodel=Word2Vec(size=300,window=5,min_count=1,alpha=0.025,workers=cpu_count,max_vocab_size=None,negative=10)\nmodel.build_vocab(all_reviews)\nmodel.train(all_reviews,total_examples=model.corpus_count,epochs=model.iter)\nmodel.save(\"word2vec_combined_50.bin\")\n</code></pre>\n",
    "score": 4,
    "creation_date": 1531064760,
    "view_count": 7592,
    "answer_count": 3,
    "tags": "python;nlp;artificial-intelligence;word2vec;gensim"
  },
  {
    "question_id": 48166721,
    "title": "Is tensorflow embedding_lookup differentiable?",
    "body": "<p>Some of the tutorials I came across, described using a randomly initialized embedding matrix and then using the <code>tf.nn.embedding_lookup</code> function to obtain the embeddings for the integer sequences. I am under the impression that since the <code>embedding_matrix</code> is obtained through <code>tf.get_variable</code>, the optimizer would add appropriate <strong>ops</strong> for updating it. </p>\n\n<p>What I don't understand is how backpropagation happens through the lookup function which seems to be hard rather than being soft. What is the gradient of the this operation wrt. one of it's input ids?</p>\n",
    "score": 4,
    "creation_date": 1515495687,
    "view_count": 1571,
    "answer_count": 1,
    "tags": "tensorflow;nlp;deep-learning;word-embedding;sequence-to-sequence"
  },
  {
    "question_id": 47932025,
    "title": "Fastest way to check if word is in NLTK synsets?",
    "body": "<p>I want to check if certain words is present in NLTK's synsets. The following code does that,</p>\n\n<pre><code>from nltk.corpus import wordnet\nif wordnet.synsets(word): \n    ... do something ...\n</code></pre>\n\n<p>but it is rather slow if you have a lot of words to check. Is there a faster way? <strong>I don't need the actual synset object, just a yes/no</strong> if there's anything there. I don't have the list of words ahead of time, so I can't precompute the answers.</p>\n",
    "score": 4,
    "creation_date": 1513886725,
    "view_count": 3055,
    "answer_count": 1,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 44579161,
    "title": "Why do we do padding in NLP tasks?",
    "body": "<p>In NLP tasks, it's very common that people annotate a sentence with SOC (start of a sentence) and EOC(end of a sentence). Why do they do that? </p>\n\n<p>Is it a task dependent performance? For instance, the reason you do padding in NER problems is different from the reason you do padding for translation problems? As in the NER problem you do padding as to extract more useful features from the context, however in a translation problem, you do padding to identify the end of a sentence because the decoder is trained sentence-by-sentence.</p>\n",
    "score": 4,
    "creation_date": 1497574363,
    "view_count": 5714,
    "answer_count": 1,
    "tags": "nlp;deep-learning"
  },
  {
    "question_id": 33705555,
    "title": "How can I remove POS tags before slashes in nltk?",
    "body": "<p>This is part of my project where I need to represent the output after phrase detection like this - (a,x,b) where a, x, b are phrases. I constructed the code and got the output like this:</p>\n\n<pre><code>(CLAUSE (NP Jack/NNP) (VP loved/VBD) (NP Peter/NNP))\n(CLAUSE (NP Jack/NNP) (VP stayed/VBD) (NP in/IN London/NNP))\n(CLAUSE (NP Tom/NNP) (VP is/VBZ) (NP in/IN Kolkata/NNP))\n</code></pre>\n\n<p>I want to make it just like the previous representation which means I have to remove 'CLAUSE', 'NP', 'VP', 'VBD', 'NNP' etc tags.</p>\n\n<p>How to do that? </p>\n\n<h2>What I tried</h2>\n\n<p>First wrote this in a text file, tokenize and used <code>list.remove('word')</code>. But that is not at all helpful.\nI am clarifying a bit more.</p>\n\n<h2>My Input</h2>\n\n<p><code>(CLAUSE (NP Jack/NNP) (VP loved/VBD) (NP Peter/NNP))\n(CLAUSE (NP Jack/NNP) (VP stayed/VBD) (NP in/IN London/NNP))</code></p>\n\n<h2>Output will be</h2>\n\n<p>[Jack,loved,Peter], [Jack,stayed,in London] \nThe output is just according to the braces and without the tags.</p>\n",
    "score": 4,
    "creation_date": 1447480183,
    "view_count": 2481,
    "answer_count": 4,
    "tags": "python;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 30748791,
    "title": "Amazon Machine Learning for sentiment analysis",
    "body": "<p>How flexible or supportive is the Amazon Machine Learning platform for sentiment analysis and text analytics?</p>\n",
    "score": 4,
    "creation_date": 1433918188,
    "view_count": 3854,
    "answer_count": 2,
    "tags": "amazon-web-services;machine-learning;nlp;sentiment-analysis;amazon-machine-learning"
  },
  {
    "question_id": 30685404,
    "title": "How Can I Access the Brown Corpus in Java (aka outside of NLTK)",
    "body": "<p>I'm trying to write a program that makes use of natural language parts-of-speech in Java. I've been searching on Google and haven't found the entire Brown Corpus (or another corpus of tagged words). I keep finding NLTK information, which I'm not interested in. I want to be able to load data into a Java program and sum up occurrences of words (and what % likelihood they are to be what part-of-speech).</p>\n\n<p>I <strong>do not</strong> want to use a Java library like the Stanford one, I want to play with the corpus data myself.</p>\n",
    "score": 4,
    "creation_date": 1433610202,
    "view_count": 1109,
    "answer_count": 3,
    "tags": "java;nlp;nltk;corpus;tagged-corpus"
  },
  {
    "question_id": 22118136,
    "title": "NLTK: Find contexts of size 2k for a word",
    "body": "<p>I have a corpus and I have a word. For each occurrence of the word in the corpus I want to get a list containing the k words before and the k words after the word. I am doing this algorithmically OK (see below) but I wondered whether NLTK is providing some functionality for my needs that I missed?</p>\n\n<pre><code>def sized_context(word_index, window_radius, corpus):\n    \"\"\" Returns a list containing the window_size amount of words to the left\n    and to the right of word_index, not including the word at word_index.\n    \"\"\"\n\n    max_length = len(corpus)\n\n    left_border = word_index - window_radius\n    left_border = 0 if word_index - window_radius &lt; 0 else left_border\n\n    right_border = word_index + 1 + window_radius\n    right_border = max_length if right_border &gt; max_length else right_border\n\n    return corpus[left_border:word_index] + corpus[word_index+1: right_border]\n</code></pre>\n",
    "score": 4,
    "creation_date": 1393696916,
    "view_count": 2111,
    "answer_count": 2,
    "tags": "python;nlp;nltk;collocation"
  },
  {
    "question_id": 18946017,
    "title": "is there a way to install the nodebox English linguistics library through pip install?",
    "body": "<p>The <a href=\"http://nodebox.net/code/index.php/Linguistics\" rel=\"nofollow\">NodeBox English linguistic library</a> for Python has some nice features, like conjugation, that could be very useful for a project.</p>\n\n<p>I tried installing through pip in a particular virtualenv, but <code>pip search nodebox</code> only brings up:</p>\n\n<pre><code>NodeBox                   - Simple application for creating 2-dimensional\n                            graphics and animation using Python code\nnodebox-color             - Color classes for python\nNodeBox-for-OpenGL        - 2D animation with Python code\nnodebox-opengl            - NodeBox for OpenGL is a free, cross-platform\n                            library for generating 2D animations with Python\n                            programming code.\n</code></pre>\n\n<p>Is it pip-installable (in a virtualenv) by another name maybe? Or is the only way to install to </p>\n\n<blockquote>\n  <p>Put the en library folder in the same folder as your script so NodeBox\n  can find the library. You can also put it in <code>~/Library/Application\n  Support/NodeBox/</code>. It takes some time to load all the data the first\n  time.</p>\n</blockquote>\n\n<p>as stated on their website?</p>\n",
    "score": 4,
    "creation_date": 1379867044,
    "view_count": 4677,
    "answer_count": 1,
    "tags": "nlp;pip;nodebox"
  },
  {
    "question_id": 10775097,
    "title": "Hobbs&#39; algorithm for Coref Resolution",
    "body": "<p>I have implemented Hobbs' algorithm for anaphora resolution together with Lappin &amp; Leass ranking for alternatives.</p>\n\n<p>What bugs me is that the description of the algorithm is completely informal, and since there are sentences that are not correctly resolved by my implementation I am not sure whether the limit is on my implementation or on the actual algorithm.</p>\n\n<p>Here is the version I have worked with, found in the Jurafsky&amp;Martin:</p>\n\n<blockquote>\n  <ol>\n  <li>Begin at the noun phrase (NP) node immediately dominating the pronoun.</li>\n  <li>Go up the tree to the first NP or sentence (S) node encountered. Call this node X, and call the path used to reach it p.</li>\n  <li>Traverse all branches below node X to the left of path p in a left-to-right, breadth-first fashion. Propose as the antecedent any NP node that is\n  encountered which has an NP or S node between it and X.</li>\n  <li>If node X is the highest S node in the sentence, traverse the surface parse trees of previous sentences in the text in order of\n  recency, the most recent first; each tree is traversed in a\n  left-to-right, breadth-first manner, and when an NP node is\n  encountered, it is proposed as antecedent. If X is not the highest S\n  node in the sentence, continue to step 5.</li>\n  <li>From node X, go up the tree to the first NP or S node encountered. Call this new node X, and call the path traversed to reach it p.</li>\n  <li>If X is an NP node and if the path p to X did not pass through the Nominal node that X immediately dominates, propose X as the\n  antecedent.</li>\n  <li>Traverse all branches below node X to the left of path p in a left-to-right, breadth-\n  first manner. Propose any NP node encountered as the antecedent.</li>\n  <li>If X is an S node, traverse all branches of node X to the right of path p in a left-to- right, breadth-first manner, but do not go below any NP or S node\n  encountered. Propose any NP node encountered as the antecedent.</li>\n  <li>Go to Step4</li>\n  </ol>\n</blockquote>\n\n<p>Look at step 3: \"to the left of path p\". The way I interpreted it is to iterate through the subtrees left-to-right, until I find the branch that contains the pronoun (hence part of the path from the pronoun to X). In Java:</p>\n\n<pre><code>for (Tree relative : X.children()) {\n            for (Tree candidate : relative) {\n                if (candidate.contains(pronoun)) break; // I am looking to all the nodes to the LEFT (i.e. coming before) the path leading to X. contain &lt;-&gt; in the path\n...\n</code></pre>\n\n<p>However, doing it this way does not process sentences like \"The house is of King Arthur himself\". This is due to the fact that \"King Arthur\" contains \"himself\" and is therefore not taken into account.</p>\n\n<p>Is this a limit of the Hobbs algorithm or am I mistaking something here?</p>\n\n<p>For reference, the full code in Java (using the Stanford Parser) is here:</p>\n\n<pre><code>import java.io.BufferedReader;\nimport java.io.BufferedWriter;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.io.OutputStreamWriter;\nimport java.io.PrintWriter;\nimport java.io.Reader;\nimport java.io.StringReader;\nimport java.io.StringWriter;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.StringTokenizer;\n\nimport javax.xml.parsers.DocumentBuilder;\nimport javax.xml.parsers.DocumentBuilderFactory;\nimport javax.xml.parsers.ParserConfigurationException;\nimport javax.xml.transform.Transformer;\nimport javax.xml.transform.TransformerConfigurationException;\nimport javax.xml.transform.TransformerException;\nimport javax.xml.transform.TransformerFactory;\nimport javax.xml.transform.dom.DOMSource;\nimport javax.xml.transform.stream.StreamResult;\n\nimport org.apache.commons.lang3.ArrayUtils;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.commons.lang3.StringEscapeUtils;\n\nimport org.w3c.dom.Document;\nimport org.w3c.dom.Element;\nimport org.w3c.dom.NamedNodeMap;\nimport org.w3c.dom.Node;\nimport org.w3c.dom.NodeList;\nimport org.xml.sax.SAXException;\n\nimport edu.stanford.nlp.ling.HasWord;\nimport edu.stanford.nlp.ling.Word;\nimport edu.stanford.nlp.ling.Sentence;\nimport edu.stanford.nlp.process.DocumentPreprocessor;\nimport edu.stanford.nlp.process.Tokenizer;\nimport edu.stanford.nlp.trees.*;\nimport edu.stanford.nlp.parser.lexparser.LexicalizedParser;\n\nclass ParseAllXMLDocuments {\n    /** \n     * @throws ParserConfigurationException \n     * @throws SAXException \n     * @throws TransformerException \n     * @throws ModifyException \n     * @throws NavException \n     * @throws TranscodeException \n     * @throws ParseException \n     * @throws EntityException \n     * @throws EOFException \n     * @throws EncodingException */\n    static final int MAXPREVSENTENCES = 4;\n    public static void main(String[] args) throws IOException, SAXException, ParserConfigurationException, TransformerException  {\n        //      File dataFolder = new File(\"DataToPort\");\n        //      File[] documents;\n        String grammar = \"grammar/englishPCFG.ser.gz\";\n        String[] options = { \"-maxLength\", \"100\", \"-retainTmpSubcategories\" };\n        LexicalizedParser lp = \n                new LexicalizedParser(grammar, options);\n        //\n        //      if (dataFolder.isDirectory()) {\n        //          documents = dataFolder.listFiles();\n        //      } else {\n        //          documents = new File[] {dataFolder};\n        //      }\n        //      int currfile = 0;\n        //      int totfiles = documents.length;\n        //      for (File paper : documents) {\n        //          currfile++;\n        //          if (paper.getName().equals(\".DS_Store\")||paper.getName().equals(\".xml\")) {\n        //              currfile--;\n        //              totfiles--;\n        //              continue;\n        //          }\n        //          System.out.println(\"Working on \"+paper.getName()+\" (file \"+currfile+\" out of \"+totfiles+\").\");\n        //\n        //          DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance(); // This is for XML\n        //          DocumentBuilder docBuilder = docFactory.newDocumentBuilder();\n        //          Document doc = docBuilder.parse(paper.getAbsolutePath());\n        //\n        //          NodeList textlist = doc.getElementsByTagName(\"text\");\n        //          for(int i=0; i &lt; textlist.getLength(); i++) {\n        //              Node currentnode = textlist.item(i);\n        //              String wholetext = textlist.item(i).getTextContent();\n        String wholetext = \"The house of King Arthur himself. You live in it all the day.\";\n        //System.out.println(wholetext);\n        //Iterable&lt;List&lt;? extends HasWord&gt;&gt; sentences;\n        System.out.println(wholetext);\n        ArrayList&lt;Tree&gt; parseTrees = new ArrayList&lt;Tree&gt;();\n        String asd = \"\";\n        int j = 0;\n        StringReader stringreader = new StringReader(wholetext);\n        DocumentPreprocessor dp = new DocumentPreprocessor(stringreader);\n        @SuppressWarnings(\"rawtypes\")\n        ArrayList&lt;List&gt; sentences = preprocess(dp);\n        for (List sentence : sentences) {\n            parseTrees.add( lp.apply(sentence) ); // Parsing a new sentence and adding it to the parsed tree\n            ArrayList&lt;Tree&gt; PronounsList = findPronouns(parseTrees.get(j)); // Locating all pronouns to resolve in the sentence\n            Tree corefedTree;\n            for (Tree pronounTree : PronounsList) { \n                parseTrees.set(parseTrees.size()-1, HobbsResolve(pronounTree, parseTrees)); // Resolving the coref and modifying the tree for each pronoun\n            }\n            StringWriter strwr = new StringWriter();\n            PrintWriter prwr = new PrintWriter(strwr);\n            TreePrint tp = new TreePrint(\"penn\");\n            tp.printTree(parseTrees.get(j), prwr);\n            prwr.flush();   \n            asd += strwr.toString();\n            j++;\n        }\n        String armando = \"\";\n        for (Tree sentence : parseTrees) {\n            for (Tree leaf : Trees.leaves(sentence))\n                armando += leaf + \" \";\n        }\n        System.out.println(armando);\n        System.out.println(\"All done.\");\n        //              currentnode.setTextContent(asd);\n        //          }\n        //          TransformerFactory transformerFactory = TransformerFactory.newInstance();\n        //          Transformer transformer = transformerFactory.newTransformer();\n        //          DOMSource source = new DOMSource(doc);\n        //          StreamResult result = new StreamResult(paper);\n        //          transformer.transform(source, result);\n        //\n        //          System.out.println(\"Done\");\n        //      }\n    }\n\n    public static Tree HobbsResolve(Tree pronoun, ArrayList&lt;Tree&gt; forest) {\n        Tree wholetree = forest.get(forest.size()-1); // The last one is the one I am going to start from\n        ArrayList&lt;Tree&gt; candidates = new ArrayList&lt;Tree&gt;();\n        List&lt;Tree&gt; path = wholetree.pathNodeToNode(wholetree, pronoun);\n        System.out.println(path);\n        // Step 1\n        Tree ancestor = pronoun.parent(wholetree); // This one locates the NP the pronoun is in, therefore we need one more \"parenting\" !\n        // Step 2\n        ancestor = ancestor.parent(wholetree);\n        //System.out.println(\"LABEL: \"+pronoun.label().value() + \"\\n\\tVALUE: \"+pronoun.firstChild());\n        while ( !ancestor.label().value().equals(\"NP\") &amp;&amp; !ancestor.label().value().equals(\"S\") )\n            ancestor = ancestor.parent(wholetree);\n        Tree X = ancestor;\n        path = X.pathNodeToNode(wholetree, pronoun);\n        System.out.println(path);\n        // Step 3\n        for (Tree relative : X.children()) {\n            for (Tree candidate : relative) {\n                if (candidate.contains(pronoun)) break; // I am looking to all the nodes to the LEFT (i.e. coming before) the path leading to X. contain &lt;-&gt; in the path\n                //System.out.println(\"LABEL: \"+relative.label().value() + \"\\n\\tVALUE: \"+relative.firstChild());\n                if ( (candidate.parent(wholetree) != X) &amp;&amp; (candidate.parent(wholetree).label().value().equals(\"NP\") || candidate.parent(wholetree).label().value().equals(\"S\")) )\n                    if (candidate.label().value().equals(\"NP\")) // \"Propose as the antecedent any NP node that is encountered which has an NP or S node between it and X\"\n                        candidates.add(candidate);\n            }\n        }\n        // Step 9 is a GOTO step 4, hence I will envelope steps 4 to 8 inside a while statement.\n        while (true) { // It is NOT an infinite loop. \n            // Step 4\n            if (X.parent(wholetree) == wholetree) {\n                for (int q=1 ; q &lt; MAXPREVSENTENCES; ++q) {// I am looking for the prev sentence (hence we start with 1)\n                    if (forest.size()-1 &lt; q) break; // If I don't have it, break\n                    Tree prevTree = forest.get(forest.size()-1-q); // go to previous tree\n                    // Now we look for each S subtree, in order of recency (hence right-to-left, hence opposite order of that of .children() ).\n                    ArrayList&lt;Tree&gt; backlist = new ArrayList&lt;Tree&gt;();\n                    for (Tree child : prevTree.children()) {\n                        for (Tree subtree : child) {\n                            if (subtree.label().value().equals(\"S\")) {\n                                backlist.add(child);\n                                break;\n                            }\n                        }\n                    }\n                    for (int i = backlist.size()-1 ; i &gt;=0 ; --i) {\n                        Tree Treetovisit = backlist.get(i);\n                        for (Tree relative : Treetovisit.children()) {\n                            for (Tree candidate : relative) {\n                                if (candidate.contains(pronoun)) continue; // I am looking to all the nodes to the LEFT (i.e. coming before) the path leading to X. contain &lt;-&gt; in the path\n                                //System.out.println(\"LABEL: \"+relative.label().value() + \"\\n\\tVALUE: \"+relative.firstChild());\n                                if (candidate.label().value().equals(\"NP\")) { // \"Propose as the antecedent any NP node that you find\"\n                                    if (!candidates.contains(candidate)) candidates.add(candidate);\n                                }\n                            }\n                        }\n                    }\n                }\n                break; // It will always come here eventually\n            }\n            // Step 5\n            ancestor = X.parent(wholetree);\n            //System.out.println(\"LABEL: \"+pronoun.label().value() + \"\\n\\tVALUE: \"+pronoun.firstChild());\n            while ( !ancestor.label().value().equals(\"NP\") &amp;&amp; !ancestor.label().value().equals(\"S\") )\n                ancestor = ancestor.parent(wholetree);\n            X = ancestor;\n            // Step 6\n            if (X.label().value().equals(\"NP\")) { // If X is an NP\n                for (Tree child : X.children()) { // Find the nominal nodes that X directly dominates\n                    if (child.label().value().equals(\"NN\") || child.label().value().equals(\"NNS\") || child.label().value().equals(\"NNP\") || child.label().value().equals(\"NNPS\") )\n                        if (! child.contains(pronoun)) candidates.add(X); // If one of them is not in the path between X and the pronoun, add X to the antecedents\n                }\n            }\n            // Step SETTE\n            for (Tree relative : X.children()) {\n                for (Tree candidate : relative) {\n                    if (candidate.contains(pronoun)) continue; // I am looking to all the nodes to the LEFT (i.e. coming before) the path leading to X. contain &lt;-&gt; in the path\n                    //System.out.println(\"LABEL: \"+relative.label().value() + \"\\n\\tVALUE: \"+relative.firstChild());\n                    if (candidate.label().value().equals(\"NP\")) { // \"Propose as the antecedent any NP node that you find\"\n                        boolean contains = false;\n                        for (Tree oldercandidate : candidates) {\n                            if (oldercandidate.contains(candidate)) { \n                                contains=true;\n                                break;\n                            }\n                        }\n                        if (!contains) candidates.add(candidate);\n                    }\n                }\n            }\n            // Step 8\n            if (X.label().value().equals(\"S\")) {\n                boolean right = false;\n                // Now we want all branches to the RIGHT of the path pronoun -&gt; X.\n                for (Tree relative : X.children()) {\n                    if (relative.contains(pronoun)) {\n                        right = true;\n                        continue;\n                    }\n                    if (!right) continue;\n                    for (Tree child : relative) { // Go in but do not go below any NP or S node. Go below the rest\n                        if (child.label().value().equals(\"NP\")) {\n                            candidates.add(child);\n                            break; // not sure if this means avoid going below NP but continuing with the rest of non-NP children. Should be since its DFS.\n                        }\n                        if (child.label().value().equals(\"S\")) break; // Same\n                    }\n                }\n            }\n        }\n\n        // Step 9 is a GOTO, so we use a while.\n\n        System.out.println(pronoun + \": CHAIN IS \" + candidates.toString());\n        ArrayList&lt;Integer&gt; scores = new ArrayList&lt;Integer&gt;();\n\n        for (int j=0; j &lt; candidates.size(); ++j) {\n            Tree candidate = candidates.get(j);\n            Tree parent = null;\n            int parent_index = 0;\n            for (Tree tree : forest) {\n                if (tree.contains(candidate)) { \n                    parent = tree;\n                    break;\n                }\n                ++parent_index;\n            }\n            scores.add(0);\n            if (parent_index == 0) \n                scores.set(j, scores.get(j)+100); // If in the last sentence, +100 points\n            scores.set(j, scores.get(j) + syntacticScore(candidate, parent));\n\n            if (existentialEmphasis(candidate)) // Example: \"There was a dog standing outside\"\n                scores.set(j, scores.get(j)+70);\n            if (!adverbialEmphasis(candidate, parent))\n                scores.set(j, scores.get(j)+50);\n            if (headNounEmphasis(candidate, parent))\n                scores.set(j, scores.get(j)+80);\n\n            int sz = forest.size()-1;\n//          System.out.println(\"pronoun in sentence \" + sz + \"(sz). Candidate in sentence \"+parent_index+\" (parent_index)\");\n            int dividend = 1;\n            for (int u=0; u &lt; sz - parent_index; ++u)\n                dividend *= 2;\n            //System.out.println(\"\\t\"+dividend);\n            scores.set(j, scores.get(j)/dividend);\n            System.out.println(candidate + \" -&gt; \" + scores.get(j) );\n        }\n        int max = -1;\n        int max_index = -1;\n        for (int i=0; i &lt; scores.size(); ++i) {\n            if (scores.get(i) &gt; max) {\n                max_index = i;\n                max = scores.get(i);\n            }\n        }\n        Tree final_candidate = candidates.get(max_index);\n        System.out.println(\"My decision for \" + pronoun + \" is: \" + final_candidate);\n        // Decide what candidate, with both gender resolution and Lappin and Leass ranking.\n\n        Tree pronounparent = pronoun.parent(wholetree).parent(wholetree); // 1 parent gives me the NP of the pronoun\n        int pos = 0;\n        for (Tree sibling : pronounparent.children()) {\n            System.out.println(\"Sibling \"+pos+\": \" + sibling);\n            if (sibling.contains(pronoun)) break;\n            ++pos;\n        }\n        System.out.println(\"Before setchild: \" + pronounparent);\n        @SuppressWarnings(\"unused\")\n        Tree returnval = pronounparent.setChild(pos, final_candidate);\n        System.out.println(\"After setchild: \" + pronounparent);\n\n        return wholetree; // wholetree is already modified, since it contains pronounparent\n    }\n\n    private static int syntacticScore(Tree candidate, Tree root) {\n        // We will check whether the NP is inside an S (hence it would be a subject)\n        // a VP (direct object)\n        // a PP inside a VP (an indirect obj)\n        Tree parent = candidate;\n        while (! parent.label().value().equals(\"S\")) {\n            if (parent.label().value().equals(\"VP\")) return 50; // direct obj\n            if (parent.label().value().equals(\"PP\")) {\n                Tree grandparent = parent.parent(root);\n                while (! grandparent.label().value().equals(\"S\")) {\n                    if (parent.label().value().equals(\"VP\")) // indirect obj is a PP inside a VP\n                        return 40;\n                    parent = grandparent;\n                    grandparent = grandparent.parent(root);\n                } \n            }\n            parent = parent.parent(root);\n        }\n        return 80; // If nothing remains, it must be the subject\n    }\n\n    private static boolean existentialEmphasis(Tree candidate) {\n        // We want to check whether our NP's Dets are \"a\" or \"an\".\n        for (Tree child : candidate) {\n            if (child.label().value().equals(\"DT\")) {\n                for (Tree leaf : child) {\n                    if (leaf.value().equals(\"a\")||leaf.value().equals(\"an\")\n                            ||leaf.value().equals(\"A\")||leaf.value().equals(\"An\") ) {\n                        //System.out.println(\"Existential emphasis!\");\n                        return true;\n                    }\n                }\n            }\n        }\n        return false;\n    }\n\n    private static boolean headNounEmphasis(Tree candidate, Tree root) {\n        Tree parent = candidate.parent(root);\n        while (! parent.label().value().equals(\"S\")) { // If it is the head NP, it is not contained in another NP (that's exactly how the original algorithm does it)\n            if (parent.label().value().equals(\"NP\")) return false;\n            parent = parent.parent(root);\n        }\n        return true;\n    }\n\n    private static boolean adverbialEmphasis(Tree candidate, Tree root) { // Like in \"Inside the castle, King Arthur was invincible\". \"Castle\" has the adv emph.\n        Tree parent = candidate;\n        while (! parent.label().value().equals(\"S\")) {\n            if (parent.label().value().equals(\"PP\")) {\n                for (Tree sibling : parent.siblings(root)) {\n                    if ( (sibling.label().value().equals(\",\"))) {\n                        //System.out.println(\"adv Emph!\");\n                        return true;\n                    }\n                }\n            }\n            parent = parent.parent(root);\n        }\n        return false;\n    }\n\n    public static ArrayList&lt;Tree&gt; findPronouns(Tree t) {\n        ArrayList&lt;Tree&gt; pronouns = new ArrayList&lt;Tree&gt;();\n        if (t.label().value().equals(\"PRP\") &amp;&amp; !t.children()[0].label().value().equals(\"I\") &amp;&amp; !t.children()[0].label().value().equals(\"you\") &amp;&amp; !t.children()[0].label().value().equals(\"You\")) {\n            pronouns.add(t);\n        }\n        else\n            for (Tree child : t.children())\n                pronouns.addAll(findPronouns(child));\n                    return pronouns;\n    }\n\n    @SuppressWarnings(\"rawtypes\")\n    public static ArrayList&lt;List&gt; preprocess(DocumentPreprocessor strarray) {\n        ArrayList&lt;List&gt; Result = new ArrayList&lt;List&gt;();\n        for (List&lt;HasWord&gt; sentence : strarray) {\n            if (!StringUtils.isAsciiPrintable(sentence.toString())) {\n                continue; // Removing non ASCII printable sentences\n            }\n            //string = StringEscapeUtils.escapeJava(string);\n            //string = string.replaceAll(\"([^A-Za-z0-9])\", \"\\\\s$1\");\n            int nonwords_chars = 0;\n            int words_chars = 0;\n            for (HasWord hasword : sentence ) {\n                String next = hasword.toString();\n                if ((next.length() &gt; 30)||(next.matches(\"[^A-Za-z]\"))) nonwords_chars += next.length(); // Words too long or non alphabetical will be junk\n                else words_chars += next.length();\n            }\n            if ( (nonwords_chars / (nonwords_chars+words_chars)) &gt; 0.5) // If more than 50% of the string is non-alphabetical, it is going to be junk\n                continue;   // Working on a character-basis because some sentences may contain a single, very long word\n            if (sentence.size() &gt; 100) {\n                System.out.println(\"\\tString longer than 100 words!\\t\" + sentence.toString());\n                continue;\n            }\n            Result.add(sentence);\n        }\n        return Result;\n    }\n}\n</code></pre>\n",
    "score": 4,
    "creation_date": 1338132400,
    "view_count": 5876,
    "answer_count": 1,
    "tags": "java;language-agnostic;nlp"
  },
  {
    "question_id": 10585864,
    "title": "NER naive algorithm",
    "body": "<p>I never really dealt with NLP but had an idea about NER which should NOT have worked and somehow DOES exceptionally well in one case. I do not understand why it works, why doesn't it work or weather it can be extended. </p>\n\n<p>The idea was to extract names of the main characters in a story through:</p>\n\n<ol>\n<li>Building a dictionary for each word</li>\n<li>Filling for each word a list with the words that appear right next to it in the text</li>\n<li>Finding for each word a word with the max correlation of lists (meaning that the words are used similarly in the text)</li>\n<li>Given that one name of a character in the story, the words that are used like it, should be as well (Bogus, that is what should not work but since I never dealt with NLP until this morning I started the day naive)  </li>\n</ol>\n\n<p>I ran the overly simple code (attached below) on <a href=\"http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt\" rel=\"nofollow\">Alice in Wonderland</a>, which for \"Alice\" returns:</p>\n\n<blockquote>\n  <p>21 ['Mouse', 'Latitude', 'William', 'Rabbit', 'Dodo', 'Gryphon', 'Crab', 'Queen', 'Duchess', 'Footman', 'Panther', 'Caterpillar', 'Hearts', 'King', 'Bill', 'Pigeon', 'Cat', 'Hatter', 'Hare', 'Turtle', 'Dormouse']</p>\n</blockquote>\n\n<p>Though it filters for upper case words (and receives \"Alice\" as the word to cluster around), originally there are ~500 upper case words, and it's still pretty spot on as far as <a href=\"http://en.wikipedia.org/wiki/Alice%27s_Adventures_in_Wonderland#Characters\" rel=\"nofollow\">main characters</a> goes.</p>\n\n<p>It does not work that well with other characters and in other stories, though gives interesting results. </p>\n\n<p>Any idea if this idea is usable, extendable or why does it work at all in this story for \"Alice\" ?</p>\n\n<p>Thanks!</p>\n\n<pre><code>#English Name recognition\nimport re\nimport sys\nimport random\nfrom string import upper\n\ndef mimic_dict(filename):\n  dict = {}\n  f = open(filename)\n  text = f.read()\n  f.close()\n  prev = \"\"\n  words = text.split()\n  for word in words:\n    m = re.search(\"\\w+\",word)\n    if m == None:\n      continue\n    word = m.group()\n    if not prev in dict:\n      dict[prev] = [word]\n    else :\n      dict[prev] = dict[prev] + [word] \n    prev = word\n  return dict\n\ndef main():\n  if len(sys.argv) != 2:\n    print 'usage: ./main.py file-to-read'\n    sys.exit(1)\n\n  dict = mimic_dict(sys.argv[1])\n  upper = []\n  for e in dict.keys():\n    if len(e) &gt; 1 and  e[0].isupper():\n      upper.append(e)\n  print len(upper),upper\n\n  exclude = [\"ME\",\"Yes\",\"English\",\"Which\",\"When\",\"WOULD\",\"ONE\",\"THAT\",\"That\",\"Here\",\"and\",\"And\",\"it\",\"It\",\"me\"]\n  exclude = [ x  for x in exclude if dict.has_key(x)] \n  for s in exclude :\n    del dict[s]\n\n  scores = {}\n  for key1 in dict.keys():\n    max = 0\n    for key2 in dict.keys():\n      if key1 == key2 : continue\n      a =  dict[key1]\n      k =  dict[key2]\n      diff = []\n      for ia in a:\n        if ia in k and ia not in diff:\n          diff.append( ia)\n      if len(diff) &gt; max:\n        max = len(diff)\n        scores[key1]=(key2,max)\n  dictscores = {}\n  names = []\n  for e in scores.keys():\n    if scores[e][0]==\"Alice\" and e[0].isupper():\n      names.append(e)\n  print len(names), names     \n\n\nif __name__ == '__main__':\n  main()\n</code></pre>\n",
    "score": 4,
    "creation_date": 1337007576,
    "view_count": 423,
    "answer_count": 2,
    "tags": "python;nlp"
  },
  {
    "question_id": 5846574,
    "title": "Find sentences with similar relative meaning from a list of sentences against an example one",
    "body": "<p>I want to be able to find sentences with the same meaning. I have a query sentence, and a long list of millions of other sentences. Sentences are words, or a special type of word called a symbol which is just a type of word symbolizing some object being talked about.</p>\n\n<p>For example, my query sentence is:</p>\n\n<p>Example: add (x) to (y) giving (z)</p>\n\n<p>There may be a list of sentences already existing in my database such as: 1. the sum of (x) and (y) is (z) 2. (x) plus (y) equals (z) 3. (x) multiplied by (y) does not equal (z) 4. (z) is the sum of (x) and (y)</p>\n\n<p>The example should match the sentences in my database 1, 2, 4 but not 3. Also there should be some weight for the sentence matching.</p>\n\n<p>Its not just math sentences, its any sentence which can be compared to any other sentence based upon the meaning of the words. I need some way to have a comparison between a sentence and many other sentences to find the ones with the closes relative meaning. I.e. mapping between sentences based upon their meaning.</p>\n\n<p>Thanks! (the tag is language-design as I couldn't create any new tag)</p>\n",
    "score": 4,
    "creation_date": 1304226196,
    "view_count": 5354,
    "answer_count": 4,
    "tags": "nlp;google-natural-language"
  },
  {
    "question_id": 3809985,
    "title": "How to find dates in the sentence using NLP, RegEx in Python",
    "body": "<p>Can anyone suggest me some way of finding and parsing dates (in any format, \"Aug06\", \"Aug2006\", \"August 2 2008\", \"19th August 2006\", \"08-06\", \"01-08-06\") in the python.</p>\n\n<p>I came across this question, but it is in perl...\n<a href=\"https://stackoverflow.com/questions/3445358/extract-inconsistently-formatted-date-from-string-date-parsing-nlp\">Extract inconsistently formatted date from string (date parsing, NLP)</a></p>\n\n<p>Any suggestion would be helpful.</p>\n",
    "score": 4,
    "creation_date": 1285652434,
    "view_count": 4722,
    "answer_count": 3,
    "tags": "python;regex;parsing;nlp"
  },
  {
    "question_id": 75127920,
    "title": "How do I combine lists in column of dataframe to a single list",
    "body": "<p>Some context, I have some data that I'm doing some text analysis on, I have just tokenized them and I want to combine all the lists in the dataframe column for some further processing.</p>\n<p>My df is as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.DataFrame({'title': ['issue regarding app', 'graphics should be better'], 'text': [[&quot;'app'&quot;, &quot;'load'&quot;, &quot;'slowly'&quot;], [&quot;'interface'&quot;, &quot;'need'&quot;, &quot;'to'&quot;, &quot;'look'&quot;, &quot;'nicer'&quot;]]})`\n</code></pre>\n<p>I want to merge all the lists in the 'text' column into one list, and also remove the open/close inverted commas.</p>\n<p>Something like this:</p>\n<pre><code>lst = ['app', 'load', 'slowly', 'interface', 'need', 'to', 'look', 'nicer']`\n</code></pre>\n<p>Thank you for all your help!</p>\n",
    "score": 4,
    "creation_date": 1673811691,
    "view_count": 1733,
    "answer_count": 4,
    "tags": "python;pandas;list;dataframe;nlp"
  },
  {
    "question_id": 72854302,
    "title": "Are the pre-trained layers of the Huggingface BERT models frozen?",
    "body": "<p>I use the following classification model from Huggingface:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;dbmdz/bert-base-german-cased&quot;, num_labels=2).to(device)\n</code></pre>\n<p>As I understand, this adds a dense layer at the end of the pre-trained model which has 2 output nodes. But are all the pre-trained layers before that frozen? Or are they also updated when fine-tuning? I can't find information about that in the docs...</p>\n<p>So do I still have to do something like this?:</p>\n<pre class=\"lang-py prettyprint-override\"><code>for param in model.bert.parameters():\n    param.requires_grad = False\n</code></pre>\n",
    "score": 4,
    "creation_date": 1656925799,
    "view_count": 2107,
    "answer_count": 2,
    "tags": "nlp;pytorch;huggingface-transformers;bert-language-model"
  },
  {
    "question_id": 69216523,
    "title": "Spacy download en_core_web_lg manually",
    "body": "<p>I am trying to find a way to download the  model <code>en_core_web_lg ==2.3.1</code> for <code>Spacy == 2.3.2</code>.</p>\n<p>Currently using</p>\n<pre><code>python -m spacy download en_core_web_lg\nimport spacy\nnlp = spacy.load (&quot;en_core_web_lg&quot;)\n</code></pre>\n<p>Is it possible to download the <code>model file or directory</code> directly and <code>load the model</code> from that downloaded folder.</p>\n",
    "score": 4,
    "creation_date": 1631838768,
    "view_count": 15893,
    "answer_count": 2,
    "tags": "nlp;spacy;language-model;spacy-3"
  },
  {
    "question_id": 64546786,
    "title": "ValueError: Buffer dtype mismatch, expected &#39;double&#39; but got &#39;float&#39;",
    "body": "<pre><code>def cast_vector(row):\n    return np.array(list(map(lambda x: x.astype('float32'), row)))\n\nwords = pd.DataFrame(word_vectors.vocab.keys())\nwords.columns = ['words']\nwords['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])\nwords['vectors_typed'] = words.vectors.apply(cast_vector)\nwords['cluster'] = words.vectors_typed.apply(lambda x: model.predict([np.array(x)]))\n#words.cluster = words.cluster.apply(lambda x: x[0])\n</code></pre>\n<p>Why is there an error although it's float32?</p>\n<p><img src=\"https://i.sstatic.net/wd1hA.jpg\" alt=\"enter image description here\" /></p>\n",
    "score": 4,
    "creation_date": 1603758643,
    "view_count": 14144,
    "answer_count": 1,
    "tags": "python;pandas;nlp"
  },
  {
    "question_id": 64284835,
    "title": "Replace personal pronoun with previous person mentioned (noisy coref)",
    "body": "<p>I want to do a noisy resolution such that given a personal prounoun, that pronoun is replace by the previous(nearest) person.</p>\n<p>For example:</p>\n<p><code>Alex is looking at buying a U.K. startup for $1 billion. He is very confident that this is going to happen. Sussan is also in the same situation. However, she has lost hope.</code></p>\n<p>the output is:</p>\n<p><code>Alex is looking at buying a U.K. startup for $1 billion. Alex is very confident that this is going to happen. Sussan is also in the same situation. However, Susan has lost hope.</code></p>\n<p>Another example,</p>\n<p><code>Peter is a friend of Gates. But Gates does not like him. </code></p>\n<p>In this case, the output would be :</p>\n<p><code>Peter is a friend of Gates. But Gates does not like Gates.</code></p>\n<p>Yes! This is super noisy.</p>\n<p>Using spacy:\nI have extracted the <code>Person</code> using NER, but how can I replace pronouns appropriately?</p>\n<p>Code:</p>\n<pre><code>import spacy\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\nfor ent in doc.ents:\n  if ent.label_ == 'PERSON':\n    print(ent.text, ent.label_)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1602265244,
    "view_count": 1776,
    "answer_count": 3,
    "tags": "python;python-3.x;nlp;spacy;coreference-resolution"
  },
  {
    "question_id": 63343563,
    "title": "Can&#39;t set the attribute &quot;trainable_weights&quot;, likely because it conflicts with an existing read-only",
    "body": "<p>My code was running perfectly in colab. But today it's not running. It says\nCan't set the attribute &quot;trainable_weights&quot;, likely because it conflicts with an existing read-only @property of the object. Please choose a different name.</p>\n<p>I am using LSTM with the attention layer.</p>\n<p>class Attention(Layer):</p>\n<pre><code>def __init__(self, **kwargs):\n    self.init = initializers.get('normal')\n    #self.input_spec = [InputSpec(ndim=3)]\n    super(Attention, self).__init__(**kwargs)\n\ndef build(self, input_shape):\n    assert len(input_shape)==3\n    #self.W = self.init((input_shape[-1],1))\n    self.W = self.init((input_shape[-1],))\n    #self.input_spec = [InputSpec(shape=input_shape)]\n    self.trainable_weights = [self.W]\n    super(Attention, self).build(input_shape)  # be sure you call this somewhere!\n\ndef call(self, x, mask=None):\n    eij = K.tanh(K.dot(x, self.W))\n    \n    ai = K.exp(eij)\n    weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n\n    weighted_input = x*weights.dimshuffle(0,1,'x')\n    return weighted_input.sum(axis=1)\n\ndef get_output_shape_for(self, input_shape):\n    return (input_shape[0], input_shape[-1])\n</code></pre>\n<p>I am not sure what happened suddenly. Anyone encounter similar problem?</p>\n",
    "score": 4,
    "creation_date": 1597074614,
    "view_count": 6647,
    "answer_count": 3,
    "tags": "nlp;lstm;attention-model"
  },
  {
    "question_id": 52467936,
    "title": "read corpus of text files in spacy",
    "body": "<p>All the examples that I see for using spacy just read in a single text file (that is small in size).\nHow does one load a corpus of text files into spacy?</p>\n\n<p>I can do this with textacy by pickling all the text in the corpus:</p>\n\n<pre><code>docs =  textacy.io.spacy.read_spacy_docs('E:/spacy/DICKENS/dick.pkl', lang='en')\n\nfor doc in docs:\n    print(doc)\n</code></pre>\n\n<p>But I am not clear as to how to use this generator object (docs) for further analysis.</p>\n\n<p>Also, I would rather use spacy, not textacy.</p>\n\n<p>spacy also fails to read in a single file that is large (~ 2000000 characters).</p>\n\n<p>Any help is appreciated...</p>\n\n<p>Ravi</p>\n",
    "score": 4,
    "creation_date": 1537719106,
    "view_count": 7800,
    "answer_count": 3,
    "tags": "nlp;multiprocessing;generator;pipeline;spacy"
  },
  {
    "question_id": 44136757,
    "title": "Quanteda package, Naive Bayes: How can I predict on different-featured test data?",
    "body": "<p>I used <code>quanteda::textmodel_NB</code> to create a model that categorizes text into one of two categories. I fit the model on a training data set of data from last summer.</p>\n\n<p>Now, I am trying to use it this summer to categorize new text we get here at work. I tried doing this and got the following error:</p>\n\n<pre><code>Error in predict.textmodel_NB_fitted(model, test_dfm) : \nfeature set in newdata different from that in training set\n</code></pre>\n\n<p>The code in the function that generates the error <a href=\"https://github.com/kbenoit/quanteda/blob/5f7e3d26dac85c20b53950faef3e90c4b77d745a/R/textmodel_NB.R\" rel=\"nofollow noreferrer\">can be found here at lines 157 to 165.</a></p>\n\n<p>I assume this occurs because the words in the training data set do not <em>exactly match</em> the words used in the test data set. But why does this error occur? I feel as if—to be useful in real-world examples—the model should be able to handle data sets that contain different features, as this is what will probably always happen in applied use.</p>\n\n<p>So my first question is:</p>\n\n<p><strong>1. Is this error a property of the naive Bayes algorithm? Or was it a choice made by the author of the function to do this?</strong></p>\n\n<p>Which then leads me to my second question:</p>\n\n<p><strong>2. How can I remedy this issue?</strong></p>\n\n<p>To get at this second question, I provide reproducible code (the last line generates the error above):</p>\n\n<pre><code>library(quanteda)\nlibrary(magrittr)\nlibrary(data.table)\n\ntrain_text &lt;- c(\"Can random effects apply only to categorical variables?\",\n                \"ANOVA expectation identity\",\n                \"Statistical test for significance in ranking positions\",\n                \"Is Fisher Sharp Null Hypothesis testable?\",\n                \"List major reasons for different results from survival analysis among different studies\",\n                \"How do the tenses and aspects in English correspond temporally to one another?\",\n                \"Is there a correct gender-neutral singular pronoun (“his” vs. “her” vs. “their”)?\",\n                \"Are collective nouns always plural, or are certain ones singular?\",\n                \"What’s the rule for using “who” and “whom” correctly?\",\n                \"When is a gerund supposed to be preceded by a possessive adjective/determiner?\")\n\ntrain_class &lt;- factor(c(rep(0,5), rep(1,5)))\n\ntrain_dfm &lt;- train_text %&gt;% \n  dfm(tolower=TRUE, stem=TRUE, remove=stopwords(\"english\"))\n\nmodel &lt;- textmodel_NB(train_dfm, train_class)\n\ntest_text &lt;- c(\"Weighted Linear Regression with Proportional Standard Deviations in R\",\n               \"What do significance tests for adjusted means tell us?\",\n               \"How should I punctuate around quotes?\",\n               \"Should I put a comma before the last item in a list?\")\n\ntest_dfm &lt;- test_text %&gt;% \n  dfm(tolower=TRUE, stem=TRUE, remove=stopwords(\"english\"))\n\npredict(model, test_dfm)\n</code></pre>\n\n<p>The only thing I have thought to do was to manually make the features the same (I assumed that this would fill in <code>0</code> for features that are not present in the object), but this generated a new error. The code for the example above is:</p>\n\n<pre><code>model_features &lt;- model$data$x@Dimnames$features # gets the features of the training data\n\ntest_features &lt;- test_dfm@Dimnames$features # gets the features of the test data\n\nall_features &lt;- c(model_features, test_features) %&gt;% # combining the two sets of features...\n  subset(!duplicated(.)) # ...and getting rid of duplicate features\n\nmodel$data$x@Dimnames$features &lt;- test_dfm@Dimnames$features &lt;- all_features # replacing features of model and test_dfm with all_features\n\npredict(model, dfm) # new error?\n</code></pre>\n\n<p>However, this code generates a <em>new</em> error:</p>\n\n<pre><code>Error in if (ncol(object$PcGw) != ncol(newdata)) stop(\"feature set in newdata different from that in training set\") : \n  argument is of length zero\n</code></pre>\n\n<p><strong>How do I apply this naive Bayes model to a new data set with different features?</strong></p>\n",
    "score": 4,
    "creation_date": 1495547480,
    "view_count": 1605,
    "answer_count": 2,
    "tags": "r;naivebayes;text-analysis;quanteda"
  },
  {
    "question_id": 42939370,
    "title": "How to not match whole word &quot;king&quot; to &quot;king?&quot;?",
    "body": "<p>How do I verify an exact word occurs in a string? </p>\n\n<p>I need to account for cases when a word such as \"king\" has a question mark immediately following as in the example below.</p>\n\n<p><strong>unigrams</strong> this should be <strong>False</strong></p>\n\n<pre><code>In [1]: answer = \"king\"\nIn [2]: context = \"we run with the king? on sunday\"\n</code></pre>\n\n<p><strong>n_grams</strong> this should be <strong>False</strong></p>\n\n<pre><code>In [1]: answer = \"king tut\"\nIn [2]: context = \"we run with the king tut? on sunday\"\n</code></pre>\n\n<p><strong>unigrams</strong> this should be <strong>True</strong></p>\n\n<pre><code>In [1]: answer = \"king\"\nIn [2]: context = \"we run with the king on sunday\"\n</code></pre>\n\n<p><strong>n_grams</strong> this should be <strong>True</strong></p>\n\n<pre><code>In [1]: answer = \"king tut\"\nIn [2]: context = \"we run with the king tut on sunday\"\n</code></pre>\n\n<p>As people mentioned, for the unigram case we can handle it by splitting the string into a list, but that doesn't work for n_grams.</p>\n\n<p>After reading some posts, I think I should attempt to handle using a look behind, but I'm not sure.</p>\n",
    "score": 4,
    "creation_date": 1490135343,
    "view_count": 153,
    "answer_count": 3,
    "tags": "python;regex;nlp"
  },
  {
    "question_id": 39650749,
    "title": "Group by sparse matrix in scipy and return a matrix",
    "body": "<p>There are a few questions on SO dealing with using <code>groupby</code> with sparse matrices. However the output seem to be lists, <a href=\"https://stackoverflow.com/questions/35410839/group-by-on-scipy-sparse-matrix\">dictionaries</a>, <a href=\"http://stackoverflow.duapp.com/questions/30295570/groupby-sum-sparse-matrix-in-pandas-or-scipy-looking-for-performance?rq=1\" rel=\"nofollow noreferrer\">dataframes</a> and other objects. </p>\n\n<p>I'm working on an NLP problem and would like to keep all the data in sparse scipy matrices during processing to prevent memory errors.</p>\n\n<p>Here's the context:</p>\n\n<p>I have vectorized some documents (<a href=\"http://andrewbrown.ca/data/groupbysparsematrix%20.csv\" rel=\"nofollow noreferrer\">sample data here</a>): </p>\n\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndf = pd.read_csv('groupbysparsematrix.csv')\ndocs = df['Text'].tolist()\n\nvectorizer = CountVectorizer()\ntrain_X = vectorizer.fit_transform(docs)\n\nprint(\"Dimensions of training set: {0}\".format(train_X.shape))\nprint type(train_X)\n\nDimensions of training set: (8, 180)\n&lt;class 'scipy.sparse.csr.csr_matrix'&gt;\n</code></pre>\n\n<p>From the original dataframe I use the date, in a day of the year format, to create the groups I would like to sum over:</p>\n\n<pre><code>from scipy import sparse, hstack    \n\ndf['Date'] = pd.to_datetime(df['Date'])\ngroups = df['Date'].apply(lambda x: x.strftime('%j'))\ngroups_X = sparse.csr_matrix(groups.astype(float)).T\ntrain_X_all = sparse.hstack((train_X, groups_X))\n\nprint(\"Dimensions of concatenated set: {0}\".format(train_X_all.shape))\n\nDimensions of concatenated set: (8, 181)\n</code></pre>\n\n<p>Now I'd like to apply <code>groupby</code> (or a similar function) to find the sum of each token per day. I would like the output to be another sparse scipy matrix.   </p>\n\n<p>The output matrix would be 3 x 181 and look something like this:</p>\n\n<pre><code> 1, 1, 1, ..., 2, 1, 3\n 2, 1, 3, ..., 1, 1, 4\n 0, 0, 0, ..., 1, 2, 5\n</code></pre>\n\n<p>Where the columns 1 to 180 represent the tokens and column 181 represents the day of the year.</p>\n",
    "score": 4,
    "creation_date": 1474590038,
    "view_count": 2878,
    "answer_count": 2,
    "tags": "python;matrix;scipy;nlp"
  },
  {
    "question_id": 38987138,
    "title": "How to extract the grammar productions rules given bracketed parses?",
    "body": "<p>I have a sample sentence. \"Open the door.\" that I parsed a sentence to get the bracketed parse output as below.</p>\n\n<blockquote>\n  <p>(S (VP (VB open) (NP (DT the) (NN door))) (. .))</p>\n</blockquote>\n\n<p>I need to extract the CFG grammar rules that produce the parsed output.\nI can manually write them out as such:</p>\n\n<pre><code>grammar = CFG.fromstring(\"\"\"   \nS -&gt; VP NP   \nNP -&gt; Det N   \nVP -&gt; V   \nDet -&gt;'the '   \nN -&gt; 'door'   \nV -&gt; 'Open'   \n\"\"\")  \n</code></pre>\n\n<p>But it's time consuming, how do I produce the grammar rules given the bracketed parsed automatically?</p>\n",
    "score": 4,
    "creation_date": 1471400409,
    "view_count": 3068,
    "answer_count": 2,
    "tags": "python;parsing;nlp;nltk;context-free-grammar"
  },
  {
    "question_id": 37403563,
    "title": "Python: Newspaper Module - Any way to pool getting articles straight from URLs?",
    "body": "<p>I'm using the Newspaper module for python found <a href=\"http://newspaper.readthedocs.io/en/latest/user_guide/advanced.html\" rel=\"nofollow\">here</a>.</p>\n\n<p>In the tutorials, it describes how you can pool the building of different newspapers s.t. it generates them at the same time. (see the \"Multi-threading article downloads\" in the link above)</p>\n\n<p>Is there any way to do this for pulling articles straight from a LIST of urls? That is, is there any way I can pump in multiple urls into the following set-up and have it download and parse them concurrently?</p>\n\n<pre><code>from newspaper import Article\nurl = 'http://www.bbc.co.uk/zhongwen/simp/chinese_news/2012/12/121210_hongkong_politics.shtml'\na = Article(url, language='zh') # Chinese\na.download()\na.parse()\nprint(a.text[:150])\n</code></pre>\n",
    "score": 4,
    "creation_date": 1464058020,
    "view_count": 3016,
    "answer_count": 4,
    "tags": "python;multithreading;parsing;nlp;python-newspaper"
  },
  {
    "question_id": 28054337,
    "title": "Python regex: tokenizing English contractions",
    "body": "<p>I am trying to parse strings in such a way as to separate out all word components, even those that have been contracted.  For example the tokenization of  \"shouldn't\" would be [\"should\", \"n't\"].</p>\n\n<p>The nltk module does not seem to be up to the task however as:</p>\n\n<blockquote>\n  <p>\"I wouldn't've done that.\"</p>\n</blockquote>\n\n<p>tokenizes as:</p>\n\n<blockquote>\n  <p>['I', \"wouldn't\", \"'ve\", 'done', 'that', '.']</p>\n</blockquote>\n\n<p>where the desired tokenization of \"wouldn't've\" was: ['would', \"n't\", \"'ve\"]</p>\n\n<p>After examining common English contractions, I am trying to write a regex to do the job but I am having a hard time figuring out how to match \"'ve\" only once.  For example, the following tokens can all terminate a contraction:</p>\n\n<blockquote>\n  <p>n't, 've, 'd, 'll, 's, 'm, 're</p>\n</blockquote>\n\n<p>But the token \"'ve\" can also follow other contractions such as:</p>\n\n<blockquote>\n  <p>'d've, n't've, and (conceivably) 'll've</p>\n</blockquote>\n\n<p>At the moment, I am trying to wrangle this regex:</p>\n\n<blockquote>\n  <p>\\b[a-zA-Z]+(?:('d|'ll|n't)('ve)?)|('s|'m|'re|'ve)\\b</p>\n</blockquote>\n\n<p>However, this pattern also matches the badly formed:</p>\n\n<blockquote>\n  <p>\"wouldn't've've\"</p>\n</blockquote>\n\n<p>It seems the problem is that the third apostrophe qualifies as a word boundary so that the final \"'ve\" token matches the whole regex.</p>\n\n<p>I have been unable to think of a way to differentiate a word boundary from an apostrophe and, failing that, I am open to advice for alternative strategies.</p>\n\n<p>Also, I am curious if there is any way to include the word boundary special character in a character class.  According to the Python documentation, \\b in a character class matches a backspace and there doesn't seem to be a way around this.</p>\n\n<p>EDIT:</p>\n\n<p>Here's the output:</p>\n\n<pre><code>&gt;&gt;&gt;pattern = re.compile(r\"\\b[a-zA-Z]+(?:('d|'ll|n't)('ve)?)|('s|'m|'re|'ve)\\b\")\n&gt;&gt;&gt;matches = pattern.findall(\"She'll wish she hadn't've done that.\")\n&gt;&gt;&gt;print matches\n[(\"'ll\", '', ''), (\"n't\", \"'ve\", ''), ('', '', \"'ve\")]\n</code></pre>\n\n<p>I can't figure out the third match.  In particular, I just realized that if the third apostrophe were matching the leading \\b, then I don't know what would be matching the character class [a-zA-Z]+.</p>\n",
    "score": 4,
    "creation_date": 1421784829,
    "view_count": 3451,
    "answer_count": 5,
    "tags": "python;regex;pattern-matching;nlp"
  },
  {
    "question_id": 27695995,
    "title": "Decompose compound sentence to simple sentences",
    "body": "<p>I am looking for a way to decompose the compound sentence to simple sentences in stanford nlp.<br>\nFor ex: Input: The manager went home and committed suicide.<br>\nOutput: The manager went home. He committed suicide.</p>\n",
    "score": 4,
    "creation_date": 1419887129,
    "view_count": 4006,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 26602794,
    "title": "How to vectorize bigrams with the hashing-trick in scikit-learn?",
    "body": "<p>I have some bigrams, lets say: <code>[('word','word'),('word','word'),...,('word','word')]</code>. How can i use scikit's <code>HashingVectorizer</code> to create a feature vector that subsequently will be presented to some classification algorithm like e.g. <code>SVC</code> or Naive Bayes or any type of classification algorithm?</p>\n",
    "score": 4,
    "creation_date": 1414480803,
    "view_count": 4876,
    "answer_count": 2,
    "tags": "python;machine-learning;scipy;nlp;scikit-learn"
  },
  {
    "question_id": 17724164,
    "title": "Stanford NER prop file meaning of DistSim",
    "body": "<p>In one of the example .prop files coming with the Stanford NER software there are two options I do not understand:</p>\n\n<pre><code>useDistSim = true\ndistSimLexicon = /u/nlp/data/pos_tags_are_useless/egw4-reut.512.clusters\n</code></pre>\n\n<p>Does anyone have a hint what DistSim stands for and where I can find any more documentation on how to use these options?</p>\n\n<p>UPDATE: I just found out that DistSim means distributional similarity. I still wonder what that means in this context.</p>\n",
    "score": 4,
    "creation_date": 1374152343,
    "view_count": 1689,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 14966285,
    "title": "NLP/Quest. Answering - Retrieving information from DB",
    "body": "<p>I've been doing a bit of reading up on NLP recently, and so far I've got a <em>(very)</em> basic idea of how everything works, ranging from sentence splitting to POS-tagging, and also knowledge representation.</p>\n\n<p>I understand that there's a wide diversity of NLP libraries out there (mostly in Java or Python) and have found a .NET implementation (<a href=\"http://sharpnlp.codeplex.com/\" rel=\"nofollow\">SharpNLP</a>). It's been excellent actually. No need to write any custom processing logic; just use their functions and <em>voila!</em> user input is well-separated and POS-tagged.</p>\n\n<p>What I don't understand is where to go from here, if my main motivation is to build a Question Answering system (something like a chatterbot). What libraries (preferably .NET) are available for me to use? If I wish to construct my own KB, how should I represent my knowledge? Do I need to parse the POS-tagged input into something else that my DB can understand? And if I'm using MS SQL, is there any library that helps map POS-tagged input to database queries? Or do I need to write my own database querying logic, according to <em>procedural semantics</em> (I've read)?</p>\n\n<p>The next step, of course, is to formulate a well-constructed reply, but I think I can leave that for later. Right now what is bugging me is the lack of resources in this area (<strong>knowledge representation</strong>, <strong>NLP to KB/DB-retrieval</strong>), and I'd really appreciate it if anyone of you there could offer me your expertise :)</p>\n",
    "score": 4,
    "creation_date": 1361304048,
    "view_count": 949,
    "answer_count": 1,
    "tags": ".net;sql-server;nlp;artificial-intelligence;nlp-question-answering"
  },
  {
    "question_id": 14819050,
    "title": "Capitalize each word except selected words in an array",
    "body": "<p>Right now I have </p>\n\n<pre><code>value = \"United states of america\"\nwords_to_ignore = [\"the\",\"of\"]\nnew_string = value.split(' ').map {|w| w.capitalize }.join(' ')\n</code></pre>\n\n<p>What I am trying to do here is except the word <code>of</code>, I want the rest capitalized. So the output would be <code>United States of America</code>. Now I am not sure, how exactly to do this.</p>\n",
    "score": 4,
    "creation_date": 1360609435,
    "view_count": 1586,
    "answer_count": 5,
    "tags": "ruby;arrays;nlp"
  },
  {
    "question_id": 12613294,
    "title": "Part of speech tagging : tagging unknown words",
    "body": "<p>In the part of speech tagger, the best probable tags for the given sentence is determined using HMM by</p>\n\n<pre><code>    P(T*) = argmax P(Word/Tag)*P(Tag/TagPrev)\n              T\n</code></pre>\n\n<p>But when 'Word' did not appear in the training corpus, P(Word/Tag) produces ZERO for given all possible tags, this leaves no room for choosing the best. </p>\n\n<p>I have tried few ways, </p>\n\n<p>1) Assigning small amount of probability for all unknown words, P(UnknownWord/AnyTag)~Epsilon... means this completely ignores the P(Word/Tag) for unknowns word by assigning the constant probability.. So decision making on unknown word is by prior probabilities.. As expected it is not producing good result. </p>\n\n<p>2) Laplace Smoothing \nI confused with this. I don't know what is difference between (1) and this.   My way of understanding Laplace Smoothing adds the constant probability(lambda) to all unknown &amp; Known words.. So the All Unknown words will get constant probability(fraction of lambda) and Known words probabilities will be the same relatively since all word's prob increased by Lambda.\nIs the Laplace Smoothing same as the previous one ?</p>\n\n<p>*)Is there any better way of dealing with unknown words ?</p>\n",
    "score": 4,
    "creation_date": 1348713463,
    "view_count": 8337,
    "answer_count": 2,
    "tags": "nlp;pos-tagger;oov"
  },
  {
    "question_id": 12439320,
    "title": "Profanities in Django comments",
    "body": "<p>Since Django doesn't handle filtering profanities - does anyone have any suggestions on an easy way to implement some sort of natural language processing / filtering of profanities in django?</p>\n",
    "score": 4,
    "creation_date": 1347728160,
    "view_count": 1820,
    "answer_count": 2,
    "tags": "python;django;nlp"
  },
  {
    "question_id": 10768038,
    "title": "Is there any Phrase head finder?",
    "body": "<p>I have some sentences that I want to parse. Here is what I have and what I need: I have sentences like these:</p>\n\n<blockquote>\n  <p>I was in the hospital. </p>\n  \n  <p>I was going from home to Canada.</p>\n</blockquote>\n\n<p>What I want is to know the head of \"in the hospital\", \"from home\", and \"to Canada\" phrases. </p>\n\n<p>I am using Berkley parser, but what it gives me is the parsing result of all the sentence, and if I want to extract the head of phrases manually, I should develop another parser! The file that I want to parse is a very big file, so if I develop a parser myself, it may have many errors. Is there any parser that can give me the result I am looking for?</p>\n\n<p>By the way, as parsing the phrases separately, may result in a different parsing compared with sentence parsing, I insist on parsing the sentences and then extract the phrase heads. </p>\n",
    "score": 4,
    "creation_date": 1338050239,
    "view_count": 2988,
    "answer_count": 2,
    "tags": "java;nlp"
  },
  {
    "question_id": 9015389,
    "title": "Package tm stop-word parameter",
    "body": "<p>I am trying to filter stop-words from the following documents using package <code>tm</code>.</p>\n\n<pre><code>library(tm)\ndocuments &lt;- c(\"the quick brown fox jumps over the lazy dog\", \"i am the walrus\")\ncorpus &lt;- Corpus(VectorSource(documents))\nmatrix &lt;- DocumentTermMatrix(corpus,control=list(stopwords=TRUE))\n</code></pre>\n\n<p>However, when I run this code I still get the following in the <code>DocumentTermMatrix</code>.</p>\n\n<pre><code>colnames(matrix)\n[1] \"brown\"  \"dog\"    \"fox\"    \"jumps\"  \"lazy\"   \"over\"   \"quick\"  \"the\"    \"walrus\"\n</code></pre>\n\n<p>\"The\" is listed as a stop-word in the list that package <code>tm</code> uses. Am I doing something wrong regarding the <code>stopwords</code> parameter, or is this a bug in the <code>tm</code> package?</p>\n\n<p><strong>EDIT:</strong> I contacted Ingo Feinerer and he noted that it is technically not a bug:</p>\n\n<blockquote>\n  <p>User-provided options are processed first, and then all remaining\n  options. Hence stopword removal is done before tokenization (as\n  already written by Vincent Zoonekynd on stackoverflow.com) which gives\n  exactly your result.</p>\n</blockquote>\n\n<p>Therefore, the solution is to explicitly list the default tokenizing option prior to the <code>stopwords</code> parameter, for example:</p>\n\n<pre><code>library(tm)\ndocuments &lt;- c(\"the quick brown fox jumps over the lazy dog\", \"i am the walrus\")\ncorpus &lt;- Corpus(VectorSource(documents))\nmatrix &lt;- DocumentTermMatrix(corpus,control=list(tokenize=scan_tokenizer,stopwords=TRUE))\ncolnames(matrix)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1327565924,
    "view_count": 5110,
    "answer_count": 3,
    "tags": "r;nlp"
  },
  {
    "question_id": 8822746,
    "title": "Wordnet (Word Sense Annotated) Corpus",
    "body": "<p>I've been utilizing lots of different corpora for natural language processing, and I've been looking for a corpus that has been annotated with Wordnet Word Senses.</p>\n\n<p>I understand that there probably is not a big corpus with this information, since the corpus needs to be built up manually, but there has to be something to go off of.</p>\n\n<p>Also if there isn't a corpus in existence, is there at least a sense annotated ngram database (with what percentage of the time a word is each of its definitions, or a numerical count of each wordnet definition depending on how common the word sense is)?</p>\n",
    "score": 4,
    "creation_date": 1326298764,
    "view_count": 1779,
    "answer_count": 3,
    "tags": "nlp;wordnet;corpus;tagged-corpus"
  },
  {
    "question_id": 8631199,
    "title": "Grouping Similar Strings",
    "body": "<p>I'm trying to analyze a bunch of search terms, so many that individually they don't tell much.  That said, I'd like to group the terms because I think similar terms should have similar effectiveness.  For example,</p>\n\n<pre><code>Term               Group\nNBA Basketball     1\nBasketball NBA     1\nBasketball         1\nBaseball           2\n</code></pre>\n\n<p>It's a contrived example, but hopefully it explains what I'm trying to do.  So then, what is the best way to do what I've described?  I thought the <code>nltk</code> may have something along those lines, but I'm only barely familiar with it.</p>\n\n<p>Thanks </p>\n",
    "score": 4,
    "creation_date": 1324841813,
    "view_count": 3294,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 5866710,
    "title": "Text classification using Java",
    "body": "<p>I need to categorize a text or word to a particular category. For example, the text 'Pink Floyd' should be categorized as 'music' or 'Wikimedia' as 'technology' or 'Einstein' as 'science'.  </p>\n\n<p>How can this be done? Is there a way I can use the <a href=\"http://dbpedia.org/\" rel=\"nofollow\">DBpedia</a> for the same? If not, the database has to be trained from time to time, right?</p>\n",
    "score": 4,
    "creation_date": 1304409578,
    "view_count": 2256,
    "answer_count": 5,
    "tags": "nlp;machine-learning;ontology;dbpedia"
  },
  {
    "question_id": 5668089,
    "title": "Comparing overlapping ranges",
    "body": "<p>I'm going to ask this question using Scala syntax, even though the question is really language independent.</p>\n\n<p>Suppose I have two lists</p>\n\n<pre><code>val groundtruth:List[Range]\nval testresult:List[Range]\n</code></pre>\n\n<p>And I want to find all of the elements of <code>testresult</code> that overlap some element in <code>groundtruth</code>.</p>\n\n<p>I can do this as follows:</p>\n\n<pre><code>def overlaps(x:Range,y:Range) = (x contains y.start) || (y contains x.start)\nval result = testresult.filter{ tr =&gt; groundtruth.exists{gt =&gt; overlaps(gt,tr)}}\n</code></pre>\n\n<p>But this takes <code>O(testresult.size * groundtruth.size)</code> time to run.</p>\n\n<p>Is there a faster algorithm for computing this result, or a data structure that can make the <code>exists</code> test more efficient?</p>\n\n<hr>\n\n<p>P.S. The algorithm should work on <code>groundtruth</code> and <code>testresult</code> generated with an expression like the following. In other words, there are no guarantees about the relationships between the ranges within a list, the <code>Range</code>s have an average size of 100 or larger. </p>\n\n<pre><code>(1 to 1000).map{x =&gt;\n   val midPt = r.nextInt(100000);\n   ((midPt - r.nextInt(100)) to (midPt + r.nextInt(100)));\n}.toList\n</code></pre>\n",
    "score": 4,
    "creation_date": 1302806856,
    "view_count": 2081,
    "answer_count": 3,
    "tags": "algorithm;data-structures;nlp;overlapping-matches"
  },
  {
    "question_id": 5579974,
    "title": "Language detection for very short text",
    "body": "<p>I'm creating an application for detecting the language of short texts, with an average of &lt; 100 characters and contains slang (e.g tweets, user queries, sms).</p>\n\n<p>All the libraries I tested work well for normal web pages but not for very short text. The library that's giving the best results so far is Chrome's Language Detection (CLD) library which I had to build as a shared library.</p>\n\n<p>CLD fails when the text is made of very short words. After looking at the source code of CLD, I see that it uses 4-grams so that could be the reason.</p>\n\n<p>The approach I'm thinking of right now to improve the accuracy is:</p>\n\n<ul>\n<li>Remove brand names, numbers, urls and words like \"software\", \"download\", \"internet\"</li>\n<li>Use a dictionary When the text contains a number of short words above a threashold or when it contains too few words.</li>\n<li>The dictionary is created from wikipedia news articles + hunspell dictionaries.</li>\n</ul>\n\n<p>What dataset is most suitable for this task? And how can I improve this approach? </p>\n\n<p>So far I'm using EUROPARL and Wikipedia articles. I'm using NLTK for most of the work.</p>\n",
    "score": 4,
    "creation_date": 1302173780,
    "view_count": 4668,
    "answer_count": 3,
    "tags": "nlp;nltk;language-detection"
  },
  {
    "question_id": 5282657,
    "title": "Converting Sentences into first Order logic",
    "body": "<p>in first order logic, i know the rules. However, whenever i convert some sentences into FOL, i get errors, I read many books and tutorials, do u have any tricks that can help me out,</p>\n\n<p>some examples where i makes errors</p>\n\n<p>Some children will eat any food</p>\n\n<pre><code>C(x) means “x is a child.”\nF(x) means “x is food.”\nEat(x,y) x eats y\nI would have written like this:\n\n(∃x)(∀y) C(x) ∧ Eat(x,y)\n\nedit:  (∃x)(∀y) C(x) ∧  F(y) ∧ Eat(x,y)\n\nBut the book write it like this\n\n(∃x)(C(x) ∧ (∀y)(F(y)→Eat(x,y)))\n</code></pre>\n\n<p>Edit No2:\n   2nd Type of error i'm making:\n   Turtles outlast Rabbits.</p>\n\n<pre><code>i'm writing it like this: ∀x,y Turtle(x)  ∧  Rabbit(y)  ∧ Outlast(x,y)\n\n but according to the book  ∀x,y Turtle(x)  ∧  Rabbit(y)  --&gt; Outlast(x,y)\n</code></pre>\n\n<p>Of course, I agree with the book, but is there any problem with my version\n!!</p>\n",
    "score": 4,
    "creation_date": 1299935128,
    "view_count": 10644,
    "answer_count": 3,
    "tags": "nlp;first-order-logic"
  },
  {
    "question_id": 2679733,
    "title": "details on the following Natural Language Processing terms?",
    "body": "<pre><code>Named Entity Extraction (extract ppl, cities, organizations)\nContent Tagging (extract topic tags by scanning doc)\nStructured Data Extraction\nTopic Categorization (taxonomy classification by scanning doc....bayesian )\nText extraction (HTML page cleaning)\n</code></pre>\n\n<p>are there libraries that i can use to do any of the above functions of NLP ?</p>\n\n<p>dont really feel like forking out cash to AlchemyAPI</p>\n",
    "score": 4,
    "creation_date": 1271812946,
    "view_count": 477,
    "answer_count": 2,
    "tags": "nlp;libraries;text-processing"
  },
  {
    "question_id": 78905620,
    "title": "TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: &#39;recursive_guard&#39;",
    "body": "<p>Pls help. I need help setting up spacy inside jupyter enviornment</p>\n<p>I am trying to use spacy to summarize youtube transcripts but finding lots of problems with spacy and python 3.12.4/3.12.3.<br />\nI started with 3.12.4, then installed python 3.12.3 onto conda environment <code>py3.12.3</code>.</p>\n<p>I install spacy 3.7.6 into env with <code>pip install stacy</code></p>\n<pre><code>Here are the `conda env list: \nbase                     /opt/anaconda3\npy3.11                *  /opt/anaconda3/envs/py3.11\npy3.12.3                 /opt/anaconda3/envs/py3.12.3\n</code></pre>\n<p>I added python kernels via cmd below</p>\n<pre><code>jupyter kernelspec install py3.12.3\npython -m ipykernel install --user --name=py3.12.3\n</code></pre>\n<p>On command line, import spacy is ok:</p>\n<pre><code>(py3.12.3) UID ~ % python           \nPython 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 14:43:12) [Clang 14.0.6 ] on darwin\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import spacy\n&gt;&gt;&gt; quit()\n(py3.12.3) UID ~ % pip freeze | grep spacy\nspacy==3.7.6\nspacy-legacy==3.0.12\nspacy-loggers==1.0.5\n\n</code></pre>\n<p>Inside jupyter (run from base, restarted after installing py3.12.3), <code>import spacy</code> gives error below.  Any help is welcomed???</p>\n<pre><code>1. (base) UID ~ % python -m ipykernel install --user --name=py3.12.3\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\nInstalled kernelspec py3.12.3 in /Users/UID/Library/Jupyter/kernels/py3.12.3\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 import spacy # module will be used  to build NLP model\n      2 from spacy.lang.en.stop_words import STOP_WORDS # module will be used  to build NLP model\n      3 from string import punctuation\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/__init__.py:13\n     10 # These are imported as part of the API\n     11 from thinc.api import Config, prefer_gpu, require_cpu, require_gpu  # noqa: F401\n---&gt; 13 from . import pipeline  # noqa: F401\n     14 from . import util\n     15 from .about import __version__  # noqa: F401\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/__init__.py:1\n----&gt; 1 from .attributeruler import AttributeRuler\n      2 from .dep_parser import DependencyParser\n      3 from .edit_tree_lemmatizer import EditTreeLemmatizer\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/attributeruler.py:8\n      6 from .. import util\n      7 from ..errors import Errors\n----&gt; 8 from ..language import Language\n      9 from ..matcher import Matcher\n     10 from ..scorer import Scorer\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/language.py:43\n     41 from .lang.tokenizer_exceptions import BASE_EXCEPTIONS, URL_MATCH\n     42 from .lookups import load_lookups\n---&gt; 43 from .pipe_analysis import analyze_pipes, print_pipe_analysis, validate_attrs\n     44 from .schemas import (\n     45     ConfigSchema,\n     46     ConfigSchemaInit,\n   (...)\n     49     validate_init_settings,\n     50 )\n     51 from .scorer import Scorer\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/pipe_analysis.py:6\n      3 from wasabi import msg\n      5 from .errors import Errors\n----&gt; 6 from .tokens import Doc, Span, Token\n      7 from .util import dot_to_dict\n      9 if TYPE_CHECKING:\n     10     # This lets us add type hints for mypy etc. without causing circular imports\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/__init__.py:1\n----&gt; 1 from ._serialize import DocBin\n      2 from .doc import Doc\n      3 from .morphanalysis import MorphAnalysis\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/_serialize.py:14\n     12 from ..errors import Errors\n     13 from ..util import SimpleFrozenList, ensure_path\n---&gt; 14 from ..vocab import Vocab\n     15 from ._dict_proxies import SpanGroups\n     16 from .doc import DOCBIN_ALL_ATTRS as ALL_ATTRS\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/vocab.pyx:1, in init spacy.vocab()\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/doc.pyx:49, in init spacy.tokens.doc()\n\nFile /opt/anaconda3/lib/python3.12/site-packages/spacy/schemas.py:195\n    191         obj = converted\n    192     return validate(TokenPatternSchema, {&quot;pattern&quot;: obj})\n--&gt; 195 class TokenPatternString(BaseModel):\n    196     REGEX: Optional[Union[StrictStr, &quot;TokenPatternString&quot;]] = Field(None, alias=&quot;regex&quot;)\n    197     IN: Optional[List[StrictStr]] = Field(None, alias=&quot;in&quot;)\n\nFile /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/main.py:286, in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)\n    284 cls.__signature__ = ClassAttribute('__signature__', generate_model_signature(cls.__init__, fields, config))\n    285 if resolve_forward_refs:\n--&gt; 286     cls.__try_update_forward_refs__()\n    288 # preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\n    289 # for attributes not in `new_namespace` (e.g. private attributes)\n    290 for name, obj in namespace.items():\n\nFile /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/main.py:808, in BaseModel.__try_update_forward_refs__(cls, **localns)\n    802 @classmethod\n    803 def __try_update_forward_refs__(cls, **localns: Any) -&gt; None:\n    804     &quot;&quot;&quot;\n    805     Same as update_forward_refs but will not raise exception\n    806     when forward references are not defined.\n    807     &quot;&quot;&quot;\n--&gt; 808     update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))\n\nFile /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:554, in update_model_forward_refs(model, fields, json_encoders, localns, exc_to_suppress)\n    552 for f in fields:\n    553     try:\n--&gt; 554         update_field_forward_refs(f, globalns=globalns, localns=localns)\n    555     except exc_to_suppress:\n    556         pass\n\nFile /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:529, in update_field_forward_refs(field, globalns, localns)\n    527 if field.sub_fields:\n    528     for sub_f in field.sub_fields:\n--&gt; 529         update_field_forward_refs(sub_f, globalns=globalns, localns=localns)\n    531 if field.discriminator_key is not None:\n    532     field.prepare_discriminated_union_sub_fields()\n\nFile /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:520, in update_field_forward_refs(field, globalns, localns)\n    518 if field.type_.__class__ == ForwardRef:\n    519     prepare = True\n--&gt; 520     field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\n    521 if field.outer_type_.__class__ == ForwardRef:\n    522     prepare = True\n\nFile /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:66, in evaluate_forwardref(type_, globalns, localns)\n     63 def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -&gt; Any:\n     64     # Even though it is the right signature for python 3.9, mypy complains with\n     65     # `error: Too many arguments for &quot;_evaluate&quot; of &quot;ForwardRef&quot;` hence the cast...\n---&gt; 66     return cast(Any, type_)._evaluate(globalns, localns, set())\n\nTypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\n</code></pre>\n",
    "score": 4,
    "creation_date": 1724411655,
    "view_count": 4019,
    "answer_count": 1,
    "tags": "python;python-3.x;nlp;spacy;sentence"
  },
  {
    "question_id": 74451907,
    "title": "Import Spacy Error &quot;cannot import name dataclass_transform&quot;",
    "body": "<p>I am working on a jupyter notebook project which should use spacy. I already used pip install to install spacy in anaconda prompt.</p>\n<p>However, when I tried to import spacy, it gives me the follwing error.</p>\n<p>I wonder what the problem is and what I can do to solve that.</p>\n<pre><code>---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-96-3173a3034708&gt; in &lt;module&gt;\n      9 #nltk.download()\n     10 from nltk.corpus import stopwords\n---&gt; 11 import spacy\n     12 \n     13 #path where we store the txt files\n\nD:\\Python\\lib\\site-packages\\spacy\\__init__.py in &lt;module&gt;\n      4 \n      5 # set library-specific custom warning handling before doing anything else\n----&gt; 6 from .errors import setup_default_warnings\n      7 \n      8 setup_default_warnings()  # noqa: E402\n\nD:\\Python\\lib\\site-packages\\spacy\\errors.py in &lt;module&gt;\n      1 import warnings\n----&gt; 2 from .compat import Literal\n      3 \n      4 \n      5 class ErrorsWithCodes(type):\n\nD:\\Python\\lib\\site-packages\\spacy\\compat.py in &lt;module&gt;\n      1 &quot;&quot;&quot;Helpers for Python and platform compatibility.&quot;&quot;&quot;\n      2 import sys\n----&gt; 3 from thinc.util import copy_array\n      4 \n      5 try:\n\nD:\\Python\\lib\\site-packages\\thinc\\util.py in &lt;module&gt;\n      6 import functools\n      7 from wasabi import table\n----&gt; 8 from pydantic import create_model, ValidationError\n      9 import inspect\n     10 import os\n\nD:\\Python\\lib\\site-packages\\pydantic\\__init__.cp38-win_amd64.pyd in init pydantic.__init__()\n\nD:\\Python\\lib\\site-packages\\pydantic\\dataclasses.cp38-win_amd64.pyd in init pydantic.dataclasses()\n\nImportError: cannot import name dataclass_transform\n</code></pre>\n",
    "score": 4,
    "creation_date": 1668544772,
    "view_count": 21765,
    "answer_count": 4,
    "tags": "python;import;nlp;spacy;python-packaging"
  },
  {
    "question_id": 68686272,
    "title": "How to increase dimension-vector size of BERT sentence-transformers embedding",
    "body": "<p>I am using sentence-transformers for semantic search but sometimes it does not understand the contextual meaning and returns wrong result\neg. <a href=\"https://stackoverflow.com/questions/68627093/bert-problem-with-context-semantic-search-in-italian-language\">BERT problem with context/semantic search in italian language</a></p>\n<p>by default the vector side of embedding of the sentence is 78 columns, so how do I increase that dimension so that it can understand the contextual meaning in deep.</p>\n<p><strong>code:</strong></p>\n<pre><code># Load the BERT Model\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Setup a Corpus\n# A corpus is a list with documents split by sentences.\n\nsentences = ['Absence of sanity', \n             'Lack of saneness',\n             'A man is eating food.',\n             'A man is eating a piece of bread.',\n             'The girl is carrying a baby.',\n             'A man is riding a horse.',\n             'A woman is playing violin.',\n             'Two men pushed carts through the woods.',\n             'A man is riding a white horse on an enclosed ground.',\n             'A monkey is playing drums.',\n             'A cheetah is running behind its prey.']\n\n# Each sentence is encoded as a 1-D vector with 78 columns \nsentence_embeddings = model.encode(sentences) ### how to increase vector dimention \n\nprint('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n\nprint('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])\n</code></pre>\n",
    "score": 4,
    "creation_date": 1628275555,
    "view_count": 8129,
    "answer_count": 2,
    "tags": "machine-learning;nlp;artificial-intelligence;bert-language-model"
  },
  {
    "question_id": 68471586,
    "title": "Training epochs interpretation during spaCy NER training",
    "body": "<p>I Was training my NER model with transformers, and am not really sure why the training stopped at some point, or why did it even go with so many batches. This is how my configuration file looks like (relevant part):</p>\n<pre><code>[training]\ntrain_corpus = &quot;corpora.train&quot;\ndev_corpus = &quot;corpora.dev&quot;\nseed = ${system.seed}\ngpu_allocator = ${system.gpu_allocator}\ndropout = 0.1\naccumulate_gradient = 1\npatience = 1600\nmax_epochs = 2\nmax_steps = 0\neval_frequency = 200\nfrozen_components = []\nbefore_to_disk = null\n\n[training.batcher]\n@batchers = &quot;spacy.batch_by_words.v1&quot;\ndiscard_oversize = false\ntolerance = 0.2\nget_length = null\n\n[training.batcher.size]\n@schedules = &quot;compounding.v1&quot;\nstart = 100\nstop = 1000\ncompound = 1.001\nt = 0.0\n\n[training.optimizer]\n@optimizers = &quot;Adam.v1&quot;\nbeta1 = 0.9\nbeta2 = 0.999\nL2_is_weight_decay = true\nL2 = 0.01\ngrad_clip = 1.0\nuse_averages = false\neps = 0.00000001\nlearn_rate = 0.00005\n</code></pre>\n<p>And this is the training log:</p>\n<pre><code>============================= Training pipeline =============================\nℹ Pipeline: ['transformer', 'ner']\nℹ Initial learn rate: 5e-05\nE    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n---  ------  -------------  --------  ------  ------  ------  ------\n  0       0         398.75     40.97    2.84    3.36    2.46    0.03\n  0     200         906.30   1861.38   94.51   94.00   95.03    0.95\n  0     400         230.06   1028.51   98.10   97.32   98.89    0.98\n  0     600          90.22   1013.38   98.99   98.40   99.58    0.99\n  0     800          80.64   1131.73   99.02   98.25   99.81    0.99\n  0    1000          98.50   1260.47   99.50   99.16   99.85    1.00\n  0    1200          73.32   1414.91   99.49   99.25   99.73    0.99\n  0    1400          84.94   1529.75   99.70   99.56   99.85    1.00\n  0    1600          55.61   1697.55   99.75   99.63   99.87    1.00\n  0    1800          80.41   1936.64   99.75   99.63   99.87    1.00\n  0    2000         115.39   2125.54   99.78   99.69   99.87    1.00\n  0    2200          63.06   2395.48   99.80   99.75   99.85    1.00\n  0    2400         104.14   2574.36   99.87   99.79   99.96    1.00\n  0    2600          86.07   2308.35   99.88   99.79   99.97    1.00\n  0    2800          81.05   1853.15   99.90   99.87   99.93    1.00\n  0    3000          52.67   1462.61   99.96   99.93   99.99    1.00\n  0    3200          57.99   1154.62   99.94   99.91   99.97    1.00\n  0    3400         110.74    847.50   99.90   99.85   99.96    1.00\n  0    3600          90.49    621.99   99.90   99.91   99.90    1.00\n  0    3800          51.03    378.93   99.87   99.78   99.97    1.00\n  0    4000          93.40    274.80   99.95   99.93   99.97    1.00\n  0    4200         138.98    203.28   99.91   99.87   99.96    1.00\n  0    4400         106.16    127.60   99.70   99.75   99.64    1.00\n  0    4600          70.28     87.25   99.95   99.94   99.96    1.00\n✔ Saved pipeline to output directory\ntraining/model-last\n\n</code></pre>\n<p>I was trying to train my model for 2 epochs (<code>max_epochs=2</code>), and my train file has around 123591 Examples, and dev file has 2522 Examples.</p>\n<p>My question is:</p>\n<ul>\n<li><p>Since my minimum batch size is 100, I expect my training to end before the 2400th eval batch, right? Because 2400th batch evaluated implies I have a <em>minimum</em> of 2400*100 = 240000, and it would actually be even more than that, since my batch size is increasing. So why did it go all the way to # 4600?</p>\n</li>\n<li><p>The training ended automatically, but the E still reads the 0th epoch. Why is that?</p>\n</li>\n</ul>\n<p>Edit: In continuation to my 2nd bullet point, I'm curious to know why did the training went all the way upto 4600 batches, because 4600 batches at minimum means 4600*100 = 460000 examples, and I gave 123591  examples for train, so I'm clearly well above and over the 1st epoch, but E still reads as 0.</p>\n",
    "score": 4,
    "creation_date": 1626878529,
    "view_count": 5760,
    "answer_count": 3,
    "tags": "nlp;spacy;named-entity-recognition;spacy-3"
  },
  {
    "question_id": 67789544,
    "title": "Given a word can we get all possible lemmas for it using Spacy?",
    "body": "<p>The input word is standalone and not part of a sentence but I would like to get all of its possible lemmas as if the input word were in different sentences with all possible POS tags. I would also like to get the lookup version of the word's lemma.</p>\n<p>Why am I doing this?</p>\n<p>I have extracted lemmas from all the documents and I have also calculated the number of dependency links between lemmas. Both of which I have done using <code>en_core_web_sm</code>. Now, given an input word, I would like to return the lemmas that are linked most frequently to all the possible lemmas of the input word.</p>\n<p>So in short, I would like to replicate the behaviour of <code>token._lemma</code> for the input word with all possible POS tags to maintain consistency with the lemma links I have counted.</p>\n",
    "score": 4,
    "creation_date": 1622553215,
    "view_count": 2401,
    "answer_count": 3,
    "tags": "python;nlp;spacy;lemmatization;spacy-3"
  },
  {
    "question_id": 66877053,
    "title": "SpaCy NLP- Detect the verb form",
    "body": "<p>As far as I know that we can get the v1 form of a verb using</p>\n<pre><code>word.lemma_\n</code></pre>\n<p>I wanted to know is their a way in which we can get the form of the verb like:</p>\n<p><em>swims</em> it should output v4</p>\n<p>Is their way to do that using SpaCy or any other lib and if there is then please give a link to that command</p>\n",
    "score": 4,
    "creation_date": 1617131265,
    "view_count": 2831,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 66715423,
    "title": "Distance between strings by similarity of sound",
    "body": "<p>Is the a quantitative descriptor of similarity between two words based on how they sound/are pronounced, analogous to Levenshtein distance?</p>\n<p>I know soundex gives same id to <a href=\"https://stackoverflow.com/questions/55331723/how-to-get-the-similar-sounding-words-together\">similar sounding</a> words, but as far as I undestood it is not a quantitative descriptor of difference between the words.</p>\n<pre><code>from jellyfish import soundex\n\nprint(soundex(&quot;two&quot;))\nprint(soundex(&quot;to&quot;))\n</code></pre>\n",
    "score": 4,
    "creation_date": 1616187048,
    "view_count": 2648,
    "answer_count": 1,
    "tags": "python;audio;nlp;linguistics"
  },
  {
    "question_id": 58701337,
    "title": "How to construct PPMI matrix from a text corpus?",
    "body": "<p>I am trying to use an SVD model for word embedding on the Brown corpus. For this, I want to first generate a word-word co-occurence matrix and then convert to PPMI matrix for the SVD matrix multiplication process.</p>\n\n<p>I have tried to create a co-occurence using SkLearn CountVectorizer </p>\n\n<pre><code>count_model = CountVectorizer(ngram_range=(1,1))\n\nX = count_model.fit_transform(corpus)\nX[X &gt; 0] = 1\nXc = (X.T * X)\nXc.setdiag(0)\nprint(Xc.todense())\n</code></pre>\n\n<p>But:</p>\n\n<p>(1) Am not sure how I can control the context window with this method? I want to experiment with various context sizes and see how the impact the process.</p>\n\n<p>(2) How do I then compute the PPMI properly assuming that \nPMI(a, b) = log p(a, b)/p(a)p(b)</p>\n\n<p>Any help on the thought process and implementation would be greatly appreciated!</p>\n\n<p>Thanks (-:</p>\n",
    "score": 4,
    "creation_date": 1572902931,
    "view_count": 6239,
    "answer_count": 1,
    "tags": "python;nlp;word-embedding"
  },
  {
    "question_id": 57546530,
    "title": "Get all pairs of right-branching words from a sentence",
    "body": "<p>Given that I have a string like:</p>\n\n<pre><code> 'velvet evening purse bags'\n</code></pre>\n\n<p>how can I get all word pairs of this? In other words, all 2-word combinations of this:</p>\n\n<pre><code>'velvet evening'\n'velvet purse'\n'velvet bags'\n'evening purse'\n'evening bags'\n'purse bags'\n</code></pre>\n\n<p>I know python's <code>nltk</code> package can give the bigrams but I'm looking for something beyond that functionality. Or do I have to write my own custom function in Python?</p>\n",
    "score": 4,
    "creation_date": 1566146719,
    "view_count": 509,
    "answer_count": 3,
    "tags": "python;nlp;nltk;python-itertools"
  },
  {
    "question_id": 56575579,
    "title": "How to increase accuracy of lstm training",
    "body": "<p>I trained quora question pair detection with LSTM but training accuracy is very low and always changes when i train. I dont understand what mistake i did.</p>\n\n<p>I tried changing loss and optimiser and with increased epoch.</p>\n\n<pre><code>import numpy as np\nfrom numpy import array\nfrom keras.callbacks import ModelCheckpoint\nimport keras\nfrom keras.optimizers import SGD\nimport tensorflow as tf\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom keras import backend as K\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom keras.preprocessing.text import Tokenizer , text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import Sequential, model_from_json, load_model\nfrom keras.layers import LSTM, Dense, Input, concatenate, Concatenate,             Activation, Flatten\n from keras.models import Model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import     TfidfVectorizer,CountVectorizer\nimport nltk\n\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport scipy\nimport matplotlib.pyplot as plt\nimport pickle\n\ndf = pd.read_csv(\"questions.csv\")\ndf.drop(['id','qid1', 'qid2'], axis=1, inplace=True)\n\ndf2 = pd.read_csv(\"testmenew.csv\") \n</code></pre>\n\n## TO filter the datset\n\n<pre><code> SPECIAL_TOKENS = {\n    'quoted': 'quoted_item',\n    'non-ascii': 'non_ascii_word',\n    'undefined': 'something'\n}\n\ndef clean(text, stem_words=True):\n    import re\n    from string import punctuation\n    from nltk.stem import SnowballStemmer\n    from nltk.corpus import stopwords\n\n    def pad_str(s):\n        return ' '+s+' '\n\n    if pd.isnull(text):\n        return ''\n\n    if type(text) != str or text=='':\n        return ''\n\n    text = re.sub(\"\\'s\", \" \", text) \n    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n    text = re.sub(\"\\'ve\", \" have \", text)\n    text = re.sub(\"can't\", \"can not\", text)\n    text = re.sub(\"n't\", \" not \", text)\n    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n    text = re.sub(\"\\'re\", \" are \", text)\n    text = re.sub(\"\\'d\", \" would \", text)\n    text = re.sub(\"\\'ll\", \" will \", text)\n    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n    text = re.sub(\"(\\d+)(kK)\", \" \\g&lt;1&gt;000 \", text)\n    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text,    flags=re.IGNORECASE)\n    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \",  text, flags=re.IGNORECASE)\n     text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n\n    text = re.sub('(?&lt;=[0-9])\\,(?=[0-9])', \"\", text)\n    text = re.sub('\\$', \" dollar \", text)\n    text = re.sub('\\%', \" percent \", text)\n    text = re.sub('\\&amp;', \" and \", text)     \n    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text)  \n    text = re.sub(\"(?&lt;=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" india \", \" India \", text)\n    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n    text = re.sub(r\" china \", \" China \", text)\n    text = re.sub(r\" chinese \", \" Chinese \", text) \n    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" dms \", \" direct messages \", text,   flags=re.IGNORECASE)  \n    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n     text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n     text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n     text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" III \", \" 3 \", text)\n    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n    text = ''.join([c for c in text if c not in punctuation]).lower()\n    return text\n\n    text = re.sub('(?&lt;=[0-9])\\,(?=[0-9])', \"\", text)\n\n df['question1'] = df['question1'].apply(clean)\n df['question2'] = df['question2'].apply(clean)\n\ndf2['q1'] = df2['q1'].apply(clean)\ndf2['q2'] = df2['q2'].apply(clean)\n\nmain =df['is_duplicate'].values\n\nmain.shape\n(404351,)\n\n\nvocabularySize = 20000\n lstm_out = 200\nembed_dim = 128\n\nRawdata=df['question1'].apply(word_tokenize)\nRawdata2=df['question2'].apply(word_tokenize)\n\ntestme = df2['q1'].apply(word_tokenize)\ntestme2=df2['q2'].apply(word_tokenize)\n\ntokenizer2 = Tokenizer(num_words = vocabularySize )\n\ntokenizer2.fit_on_texts(testme)\ntokenizer2.fit_on_texts(testme2)\n\ntokenizer = Tokenizer(num_words = vocabularySize )\n\ntokenizer.fit_on_texts(Rawdata)\ntokenizer.fit_on_texts(Rawdata2)\n\n sequences = tokenizer.texts_to_sequences(Rawdata)\nsequences2 = tokenizer.texts_to_sequences(Rawdata2)\n\nsequences3 = tokenizer2.texts_to_sequences(testme)\nsequences4 = tokenizer2.texts_to_sequences(testme2)\n\ndata = pad_sequences(sequences, maxlen=2)\ndata2 = pad_sequences(sequences2, maxlen=2)\n\ndata3 = pad_sequences(sequences3, maxlen=2)\ndata4 = pad_sequences(sequences4, maxlen=2)\n\nTestInput = np.array([data3,data4])\nTestInput = TestInput.reshape(1,2,2)\nInput = np.array([data,data2])\nInput =  Input.reshape(404351,2,2)\n\n#opt = SGD(lr = 0.001, momentum = 0.60)\n\nmodel = Sequential()\n#model.add(Embedding(1, 4,input_length = 2 , dropout = 0.4))\nmodel.add(LSTM((1), input_shape = (2,2), return_sequences=False))\nmodel.add(Activation ('sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])\nX_train,X_test,y_train,y_test = train_test_split(Input,main,test_size = 0.2,random_state = 4)\n\nInput.shape\n(404351, 2, 2)\n\nhistory = model.fit(X_train,y_train,epochs = 10,validation_data=   (X_test,y_test) )\nmodel.save_weights('newoutput2.h5') \n</code></pre>\n\n<p>Train on 323480 samples, validate on 80871 samples\nEpoch 1/10\n323480/323480 [==============================] - 27s 83us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\nEpoch 2/10\n323480/323480 [==============================] - 24s 73us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\nEpoch 3/10\n323480/323480 [==============================] - 23s 71us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\nEpoch 4/10\n323480/323480 [==============================] - 23s 71us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\nEpoch 5/10\n323480/323480 [==============================] - 23s 72us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\nEpoch 6/10\n323480/323480 [==============================] - 23s 71us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\nEpoch 7/10\n323480/323480 [==============================] - 23s 71us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\nEpoch 8/10\n323480/323480 [==============================] - 25s 76us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\nEpoch 9/10\n323480/323480 [==============================] - 25s 78us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\nEpoch 10/10\n323480/323480 [==============================] - 25s 78us/step - loss: 0.6931 - acc: 0.6304 - val_loss: 0.6931 - val_acc: 0.6323\n​</p>\n\n<pre><code>filename = 'newoutput2.h5'\nmodel.load_weights(filename)\nnew = model.predict(TestInput)\nif new &gt; 0.6:\n    print(\"Duplication detected\")\nelse:\n    print(\"No duplicate\")\nnew \n\ngiving output around 0.6567 but not atall increasing, Please help !!\n</code></pre>\n\n<p>I need to Increase accuracy of training</p>\n",
    "score": 4,
    "creation_date": 1560411550,
    "view_count": 19312,
    "answer_count": 2,
    "tags": "python;deep-learning;nlp;lstm"
  },
  {
    "question_id": 56308612,
    "title": "What exactly does target_vocab_size mean in the method tfds.features.text.SubwordTextEncoder.build_from_corpus?",
    "body": "<p>According to <a href=\"https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder#build_from_corpus\" rel=\"nofollow noreferrer\">this link</a>, <strong><code>target_vocab_size:</code> int, approximate size of the vocabulary to create.</strong> The statement is pretty ambiguous for me. As far as I can understand, the encoder will map each vocabulary to a unique ID. What will happen if the corpus has <code>vocab_size</code> larger than the <code>target_vocab_size</code>?</p>\n",
    "score": 4,
    "creation_date": 1558817519,
    "view_count": 2840,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp"
  },
  {
    "question_id": 56279948,
    "title": "Remove special characters but not accented letters",
    "body": "<p>I do the following:</p>\n\n<pre><code>re.sub(r'[^ \\nA-Za-z0-9/]+', '', document)\n</code></pre>\n\n<p>to remove every character which is not alphanumeric, space, newline, or forward slash.</p>\n\n<p>So I basically I want to remove all special characters except for the newline and the forward slash.</p>\n\n<p>However, I do not want to remove the accented letters which various languages have such as in French, German etc.</p>\n\n<p>But if I run the code above then for example the word </p>\n\n<p><code>Motörhead</code> </p>\n\n<p>becomes </p>\n\n<p><code>Motrhead</code> </p>\n\n<p>and I do not want to do this.</p>\n\n<p>So how do I run the code above but without removing the accented letters?</p>\n\n<p><strong>UPDATE:</strong></p>\n\n<p>@MattM below has suggested a solution which does work for languages such as English, French, German etc but it certainly does not work for languages such as Polish where all the accented letters were still removed.</p>\n",
    "score": 4,
    "creation_date": 1558630946,
    "view_count": 4688,
    "answer_count": 2,
    "tags": "python;nlp;diacritics"
  },
  {
    "question_id": 56016682,
    "title": "Generating n-grams from a string",
    "body": "<p>I need to make a list of all  𝑛 -grams beginning at the head of string for each integer  𝑛  from 1 to M. Then return a tuple of M such lists. </p>\n\n<pre><code>    def letter_n_gram_tuple(s, M):\n        s = list(s)\n        output = []\n    for i in range(0, M+1):\n\n        output.append(s[i:])\n\n    return(tuple(output))\n</code></pre>\n\n<p>From <code>letter_n_gram_tuple(\"abcd\", 3)</code> output should be:</p>\n\n<pre><code>(['a', 'b', 'c', 'd'], ['ab', 'bc', 'cd'], ['abc', 'bcd']))\n</code></pre>\n\n<p>However, my output is:</p>\n\n<pre><code>(['a', 'b', 'c', 'd'], ['b', 'c', 'd'], ['c', 'd'], ['d']).\n</code></pre>\n\n<p>Should I use string slicing and then saving slices into the list?</p>\n",
    "score": 4,
    "creation_date": 1557210374,
    "view_count": 139,
    "answer_count": 4,
    "tags": "python;string;nlp;n-gram"
  },
  {
    "question_id": 52561244,
    "title": "NLP in Python: Obtain word names from SelectKBest after vectorizing",
    "body": "<p>I can't seem to find an answer to my exact problem. Can anyone help?</p>\n\n<p>A simplified description of my dataframe (\"df\"): It has 2 columns: one is a bunch of text (\"Notes\"), and the other is a binary variable indicating if the resolution time was above average or not (\"y\"). </p>\n\n<p>I did bag-of-words on the text:</p>\n\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(lowercase=True, stop_words=\"english\")\nmatrix = vectorizer.fit_transform(df[\"Notes\"])\n</code></pre>\n\n<p>My matrix is 6290 x 4650. No problem getting the word names (i.e. feature names) :</p>\n\n<pre><code>feature_names = vectorizer.get_feature_names()\nfeature_names\n</code></pre>\n\n<p>Next, I want to know which of these 4650 are most associated with above average resolution times; and reduce the matrix I may want to use in a predictive model. I do a chi-square test to find the top 20 most important words.</p>\n\n<pre><code>from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nselector = SelectKBest(chi2, k=20)\nselector.fit(matrix, y)\ntop_words = selector.get_support().nonzero()\n\n# Pick only the most informative columns in the data.\nchi_matrix = matrix[:,top_words[0]]\n</code></pre>\n\n<p>Now I'm stuck. How do I get the words from this reduced matrix (\"chi_matrix\")? What are my feature names? I was trying this:</p>\n\n<pre><code>chi_matrix.feature_names[selector.get_support(indices=True)].tolist()\n</code></pre>\n\n<p>Or</p>\n\n<pre><code>chi_matrix.feature_names[features.get_support()]\n</code></pre>\n\n<p>These gives me an error: feature_names not found. What am I missing?</p>\n\n<p>A</p>\n",
    "score": 4,
    "creation_date": 1538161140,
    "view_count": 2501,
    "answer_count": 2,
    "tags": "python;nlp;vectorization"
  },
  {
    "question_id": 48474442,
    "title": "Python - From list of list of tokens to bag of words",
    "body": "<p>I am struggling with computing bag of words. I have a pandas dataframe with a textual column, that I properly tokenize, remove stop words, and stem.\nIn the end, for each document, I have a list of strings.</p>\n\n<p>My ultimate goal is to compute bag of words for this column, I've seen that scikit-learn has a function to do that but it works on string, not on a list of string.</p>\n\n<p>I am doing the preprocessing myself with NLTK and would like to keep it that way...</p>\n\n<p>Is there a way to compute bag of words based on a list of list of tokens ? e.g., something like that:</p>\n\n<pre><code>[\"hello\", \"world\"]\n[\"hello\", \"stackoverflow\", \"hello\"]\n</code></pre>\n\n<p>should be converted into</p>\n\n<pre><code>[1, 1, 0]\n[2, 0, 1]\n</code></pre>\n\n<p>with vocabulary:</p>\n\n<pre><code>[\"hello\", \"world\", \"stackoverflow\"]\n</code></pre>\n",
    "score": 4,
    "creation_date": 1517045652,
    "view_count": 5757,
    "answer_count": 3,
    "tags": "python;pandas;scikit-learn;nlp;nltk"
  },
  {
    "question_id": 47614742,
    "title": "Python Bag of Words NameError: name &#39;unicode&#39; is not defined",
    "body": "<p>I have been following this site, <a href=\"https://radimrehurek.com/data_science_python/\" rel=\"nofollow noreferrer\">https://radimrehurek.com/data_science_python/</a>, to apply bag of words on a list of tweets.</p>\n\n<pre><code>import csv\nfrom textblob import TextBlob\nimport pandas\n\nmessages = pandas.read_csv('C:/Users/Suki/Project/Project12/newData1.csv', sep='\\t', quoting=csv.QUOTE_NONE,\n                               names=[\"label\", \"message\"])\n\ndef split_into_tokens(message):\n    message = unicode(message, encoding=\"utf8\")  # convert bytes into proper unicode\n    return TextBlob(message).words\n\nmessages.message.head().apply(split_into_tokens)\n\nprint (messages)\n</code></pre>\n\n<p>However I keep getting this error. I've checked and I following the code on the site but the error keeps arising.</p>\n\n<p>Error</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:/Users/Suki/Project/Project12/projectBagofWords.py\", line 34, in &lt;module&gt;\n    messages.message.head().apply(split_into_tokens)\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\pandas\\core\\series.py\", line 2510, in apply\n    mapped = lib.map_infer(values, f, convert=convert_dtype)\n  File \"pandas/_libs/src\\inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n  File \"C:/Users/Suki/Project/Project12/projectBagofWords.py\", line 31, in split_into_tokens\n    message = unicode(message, encoding=\"utf8\")  # convert bytes into proper unicode\nNameError: name 'unicode' is not defined\n</code></pre>\n\n<p>Can someone offer advice on how I could rectify this?</p>\n\n<p>Thanks</p>\n",
    "score": 4,
    "creation_date": 1512268645,
    "view_count": 5536,
    "answer_count": 2,
    "tags": "python;twitter;nlp"
  },
  {
    "question_id": 45876711,
    "title": "Gensim word2vec WMD similarity dictionary",
    "body": "<p>I'm using word2vec on a 1 million abstracts dataset (2 billion words). To find most similar documents, I use the <code>gensim.similarities.WmdSimilarity</code> class. When trying to retrieve the best match using <code>wmd_similarity_index[query]</code>, the calculation spends most of its time building a dictionary. Here is a piece of log:</p>\n\n<pre><code>2017-08-25 09:45:39,441 : INFO : built Dictionary(127 unique tokens: ['empirical', 'model', 'estimating', 'vertical', 'concentration']...) from 2 documents (total 175 corpus positions)                                                        \n2017-08-25 09:45:39,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])          \n</code></pre>\n\n<p>What does this part ? Is it dependent on the query ? Is there a way to do these calculations once for all ?</p>\n\n<p><strong>EDIT:</strong> training and scoring phases in my code:</p>\n\n<p>Training and saving to disk:</p>\n\n<pre><code>w2v_size = 300\nword2vec = gensim.models.Word2Vec(texts, size=w2v_size, window=9, min_count=5, workers=1, sg=1, hs=1, iter=20) # sg=1 means skip gram is used \nword2vec.save(utils.paths.PATH_DATA_GENSIM_WORD2VEC)\ncorpus_w2v_wmd_index = gensim.similarities.WmdSimilarity(texts, word2vec.wv)\ncorpus_w2v_wmd_index.save(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)\n</code></pre>\n\n<p>Loading and scoring:</p>\n\n<pre><code>w2v = gensim.models.Word2Vec.load(utils.paths.PATH_DATA_GENSIM_WORD2VEC)\nwords = [t for t in proc_text if t in w2v.wv]\ncorpus_w2v_wmd_index = gensim.similarities.docsim.Similarity.load(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)\nscores_w2v = np.array(corpus_w2v_wmd_index[words])  \n</code></pre>\n",
    "score": 4,
    "creation_date": 1503647469,
    "view_count": 4038,
    "answer_count": 1,
    "tags": "python;nlp;cpu-word;gensim;word2vec"
  },
  {
    "question_id": 42718792,
    "title": "Reading Bengali with python Natural Language Toolkit",
    "body": "<p>I want to read Bengali texts in NLTK's CategorizedPlainCorpusReader. For this Snapshot of my Bengali text file in gedit text editor:</p>\n\n<p><a href=\"https://i.sstatic.net/tXERR.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/tXERR.png\" alt=\"enter image description here\"></a></p>\n\n<p>Snapshot of file in sublime text editor:</p>\n\n<p><a href=\"https://i.sstatic.net/qYlyK.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/qYlyK.png\" alt=\"enter image description here\"></a></p>\n\n<p>From the snapshots you can see the problem. The problem is Unicode composition problem (the dotted ring is a dead giveaway). And here is the code segment for reading texts:</p>\n\n<pre><code>&gt;&gt;&gt; path = os.path.expanduser('~/nltk_data/corpora/Bangla')\n&gt;&gt;&gt; from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n&gt;&gt;&gt; from nltk import RegexpTokenizer\n&gt;&gt;&gt; word_tokenize = RegexpTokenizer(\"[\\w']+\")\n&gt;&gt;&gt; reader = CategorizedPlaintextCorpusReader(path,r'.*\\.txt',cat_pattern=r'(.*)_.*',word_tokenizer=word_tokenize)\n&gt;&gt;&gt; reader.sents(categories='pos')\n</code></pre>\n\n<p>The output is:</p>\n\n<p><a href=\"https://i.sstatic.net/knvjX.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/knvjX.png\" alt=\"enter image description here\"></a></p>\n\n<p>The output should be 'একবার' rather than 'একব' 'র'. What can be done?? Thanks in advance.</p>\n",
    "score": 4,
    "creation_date": 1489149984,
    "view_count": 2877,
    "answer_count": 3,
    "tags": "python;nlp;text-processing"
  },
  {
    "question_id": 37157202,
    "title": "Encoding Column variable with string values by integer values in Python or Sklearn",
    "body": "<p>How can I encode the column values of string types in the data table by integer values. For example I have two feature variables:  color (possible string values R, G and B) and skills ( with possible string values  C++ , Java, SQL and Python). Given Data-table has two columns- </p>\n\n<pre><code>Color' -&gt; R G B B G R B G G R G  ;\nSkills' -&gt; Java , C++, SQL, Java, Python, Python, SQL, C++, Java, SQL, Java.\n</code></pre>\n\n<p>I want to know which sklearn function/method will  transform  above two columns as with R=0, G=1 and B=2 and  with C++ =0, Java=1, SQL=2 and Python=3 : </p>\n\n<pre><code>Color: 0, 1, 2, 2, 1, 0, 2, 1, 1, 0, 1\nSkills:  1, 0, 2, 1, 3, 3, 2, 0, 1, 2, 1\n</code></pre>\n\n<p>Kindly, let me know how to do this ??</p>\n",
    "score": 4,
    "creation_date": 1462955769,
    "view_count": 5081,
    "answer_count": 1,
    "tags": "python;nlp;scikit-learn"
  },
  {
    "question_id": 36780358,
    "title": "Preventing Stanford Core NLP Server from outputting the text it receives",
    "body": "<p>I am running a <a href=\"http://stanfordnlp.github.io/CoreNLP/\" rel=\"nofollow\">Stanford CoreNLP</a> server:</p>\n\n<pre><code>java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 50000\n</code></pre>\n\n<p>Whenever it receives some text, it outputs it in the shell it is running it. How to prevent this from happening?</p>\n\n<hr>\n\n<p>It that matters, here is the code I use to pass data to Stanford Core NLP Server:</p>\n\n<pre><code>'''\nFrom https://github.com/smilli/py-corenlp/blob/master/example.py\n'''\nfrom pycorenlp import StanfordCoreNLP\nimport pprint\n\nif __name__ == '__main__':\n    nlp = StanfordCoreNLP('http://localhost:9000')\n    fp = open(\"long_text.txt\")\n    text = fp.read()\n    output = nlp.annotate(text, properties={\n        'annotators': 'tokenize,ssplit,pos,depparse,parse',\n        'outputFormat': 'json'\n    })\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(output)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1461272380,
    "view_count": 1273,
    "answer_count": 3,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 36057715,
    "title": "TensorFlow with a NER-Tagger",
    "body": "<p>I was wondering if there is any possibility to use Named-Entity-Recognition with a self trained model in tensorflow.</p>\n\n<p>There is a word2vec implementation, but I could not find the 'classic' POS or NER tagger.</p>\n\n<p>Thanks for your help!</p>\n",
    "score": 4,
    "creation_date": 1458210273,
    "view_count": 4937,
    "answer_count": 1,
    "tags": "nlp;tensorflow"
  },
  {
    "question_id": 34395127,
    "title": "Stanford NLP parse tree format",
    "body": "<p>This may be a silly question, but how does one iterate through a parse tree as an output of an NLP parser (like Stanford NLP)? It's all nested brackets, which is neither an <code>array</code> nor a <code>dictionary</code> or any other collection type I've used.</p>\n\n<pre><code>(ROOT\\n  (S\\n    (PP (IN As)\\n      (NP (DT an) (NN accountant)))\\n    (NP (PRP I))\\n    (VP (VBP want)\\n      (S\\n        (VP (TO to)\\n          (VP (VB make)\\n            (NP (DT a) (NN payment))))))))\n</code></pre>\n",
    "score": 4,
    "creation_date": 1450698945,
    "view_count": 5056,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp;parse-tree"
  },
  {
    "question_id": 33326704,
    "title": "scikit-learn calculate F1 in multilabel classification",
    "body": "<p>I am trying to calculate macro-F1 with scikit in <a href=\"https://en.wikipedia.org/wiki/Multi-label_classification\" rel=\"nofollow\">multi-label classification</a></p>\n\n<pre><code>from sklearn.metrics import f1_score\n\ny_true = [[1,2,3]]\ny_pred = [[1,2,3]]\n\nprint f1_score(y_true, y_pred, average='macro')\n</code></pre>\n\n<p>However it fails with error message</p>\n\n<pre><code>ValueError: multiclass-multioutput is not supported\n</code></pre>\n\n<p>How I can calculate macro-F1 with multi-label classification?</p>\n",
    "score": 4,
    "creation_date": 1445752750,
    "view_count": 4939,
    "answer_count": 1,
    "tags": "machine-learning;nlp;scikit-learn;precision-recall"
  },
  {
    "question_id": 29591485,
    "title": "Where to find a state of art relation extraction dataset",
    "body": "<p>I am looking for a dataset which contains large quantities of relation tuples. For example, the search of \"people\" and \"location\" yields \"lives in\", \"worked in\", etc. University of Washington's OpenIE <a href=\"http://OpenIE.cs.washington.edu\" rel=\"nofollow\">http://OpenIE.cs.washington.edu</a> is a good tool but their dataset is only accessible through web. Where can I download a database or library like this?</p>\n",
    "score": 4,
    "creation_date": 1428854738,
    "view_count": 2341,
    "answer_count": 2,
    "tags": "nlp"
  },
  {
    "question_id": 26505638,
    "title": "Simple grammar give ValueError in Python",
    "body": "<p>I'm new to Python, nltk and nlp. I have written simple grammar. But when running the program it gives below error. Please help me to solve this error</p>\n\n<p>Grammar:-</p>\n\n<pre><code>S -&gt; NP\nNP -&gt; PN|PRO|D[NUM=?n] N[NUM=?n]|D[NUM=?n] A N[NUM=?n]|D[NUM=?n] N[NUM=?n] PP|QP N[NUM=?n]|A N[NUM=?n]|D[NUM=?n] NOM PP|D[NUM=?n] NOM\nPP -&gt; P NP\nD[NUM=sg] -&gt; 'a'\nD -&gt; 'the'\nN[NUM=sg] -&gt; 'boy'|'girl'|'room'|'garden'|'hair'\nN[NUM=pl] -&gt; 'dogs'|'cats'\nPN -&gt; 'saumya'|'dinesh'\nPRO -&gt; 'she'|'he'|'we'\nA -&gt; 'tall'|'naughty'|'long'|'three'|'black'\nP -&gt; 'with'|'in'|'from'|'at'\nQP -&gt; 'some'\nNOM -&gt; A NOM|N[NUM=?n]\n</code></pre>\n\n<p>Code:-</p>\n\n<pre><code>import nltk\n\ngrammar = nltk.data.load('file:english_grammer.cfg')\nrdparser = nltk.RecursiveDescentParser(grammar)\nsent = \"a dogs\".split()\ntrees = rdparser.parse(sent)\n\nfor tree in trees: print (tree)\n</code></pre>\n\n<p>Error:-</p>\n\n<p>ValueError: Expected a nonterminal, found: [NUM=?n] N[NUM=?n]|D[NUM=?n] A N[NUM=?n]|D[NUM=?n] N[NUM=?n] PP|QP N[NUM=?n]|A N[NUM=?n]|D[NUM=?n] NOM PP|D[NUM=?n] NOM</p>\n",
    "score": 4,
    "creation_date": 1413974530,
    "view_count": 3109,
    "answer_count": 3,
    "tags": "python-3.x;nlp;nltk"
  },
  {
    "question_id": 25330079,
    "title": "where can I get training data of part-of-speech tagger?",
    "body": "<p>I want to implement a part-of-speech tagger,but I don't know where I can get a lot of training data?\nThanks!</p>\n",
    "score": 4,
    "creation_date": 1408119149,
    "view_count": 6803,
    "answer_count": 2,
    "tags": "machine-learning;nlp;part-of-speech"
  },
  {
    "question_id": 17684186,
    "title": "NLTK words lemmatizing",
    "body": "<p>I am trying to do lemmatization on words with <code>NLTK</code>.  </p>\n\n<p>What I can find now is that I can use the <code>stem</code> package to get some results like transform \"cars\" to \"car\" and \"women\" to \"woman\", however I cannot do lemmatization on some words with affixes like \"acknowledgement\".  </p>\n\n<p>When using <code>WordNetLemmatizer()</code> on \"acknowledgement\", it returns \"acknowledgement\" and using <code>.PorterStemmer()</code>, it returns \"acknowledg\" rather than \"acknowledge\".  </p>\n\n<p>Can anyone tell me how to eliminate the affixes of words?<br>\nSay, when input is \"acknowledgement\", the output to be \"acknowledge\"</p>\n",
    "score": 4,
    "creation_date": 1373998907,
    "view_count": 2408,
    "answer_count": 1,
    "tags": "python;nlp;nltk;stemming;lemmatization"
  },
  {
    "question_id": 17325554,
    "title": "Difference between IOB Accuracy and Precision",
    "body": "<p>I'm doing some works on NLTK with named entity recognition and chunkers. I retrained a classifier using <code>nltk/chunk/named_entity.py</code> for that and I got the following mesures:</p>\n\n<pre><code>ChunkParse score:\n    IOB Accuracy:  96.5%\n    Precision:     78.0%\n    Recall:        91.9%\n    F-Measure:     84.4%\n</code></pre>\n\n<p>But I don't understand what is the exact difference between IOB Accuracy and Precision in this case. Actually, I found on the docs (<a href=\"http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html\" rel=\"nofollow\">here</a>) the following for an specific example:</p>\n\n<blockquote>\n  <p>The IOB tag accuracy indicates that more than a third of the words are\n  tagged with O, i.e. not in an NP chunk. However, since our tagger did\n  not find any chunks, its precision, recall, and f-measure are all\n  zero.</p>\n</blockquote>\n\n<p>So, if IOB accuracy is just the number of O labels, how come we don't have chunks and IOB accuracy is not 100% at the same time, in that example?</p>\n\n<p>Thank you in advance</p>\n",
    "score": 4,
    "creation_date": 1372264069,
    "view_count": 3253,
    "answer_count": 1,
    "tags": "python;nlp;nltk;precision;named-entity-recognition"
  },
  {
    "question_id": 17076635,
    "title": "How to extract lines numbers that match a regular expression in a text file",
    "body": "<p>I'm doing a project on statistical machine translation in which I need to extract line numbers from a POS-tagged text file that match a regular expression (any non-separated phrasal verb with the particle 'out'), and write the line numbers to a file (in python).</p>\n\n<p>I have this regular expression: '\\w*_VB.?\\sout_RP' and my POS-tagged text file: 'Corpus.txt'.\nI would like to get an output file with the line numbers that match the above-mentioned regular expression, and the output file should just have one line number per line (no empty lines), e.g.:</p>\n\n<p>2</p>\n\n<p>5</p>\n\n<p>44</p>\n\n<p>So far all I have in my script is the following:</p>\n\n<pre><code>OutputLineNumbers = open('OutputLineNumbers', 'w')\nwith open('Corpus.txt', 'r') as textfile:\n    phrase='\\w*_VB.?\\sout_RP'\n    for phrase in textfile: \n\nOutputLineNumbers.close()\n</code></pre>\n\n<p>Any idea how to solve this problem?</p>\n\n<p>In advance, thanks for your help!</p>\n",
    "score": 4,
    "creation_date": 1371077082,
    "view_count": 11727,
    "answer_count": 2,
    "tags": "python;regex;nlp;part-of-speech"
  },
  {
    "question_id": 16323078,
    "title": "gender identification in natural language processing",
    "body": "<p>I have written below code using stanford nlp packages.</p>\n\n<pre><code>GenderAnnotator myGenderAnnotation = new GenderAnnotator();\nmyGenderAnnotation.annotate(annotation);\n</code></pre>\n\n<p>But for the sentence \"Annie goes to school\", it is not able to identify the gender of Annie. </p>\n\n<p>The output of application is:</p>\n\n<pre><code>     [Text=Annie CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNP Lemma=Annie NamedEntityTag=PERSON] \n     [Text=goes CharacterOffsetBegin=6 CharacterOffsetEnd=10 PartOfSpeech=VBZ Lemma=go NamedEntityTag=O] \n     [Text=to CharacterOffsetBegin=11 CharacterOffsetEnd=13 PartOfSpeech=TO Lemma=to NamedEntityTag=O] \n     [Text=school CharacterOffsetBegin=14 CharacterOffsetEnd=20 PartOfSpeech=NN Lemma=school NamedEntityTag=O] \n     [Text=. CharacterOffsetBegin=20 CharacterOffsetEnd=21 PartOfSpeech=. Lemma=. NamedEntityTag=O]\n</code></pre>\n\n<p>What is the correct approach to get the gender?</p>\n",
    "score": 4,
    "creation_date": 1367428927,
    "view_count": 9078,
    "answer_count": 5,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 11116508,
    "title": "which parser is most suitable for [biomedical] relation extraction?",
    "body": "<p>I have read about continuency parser and dependency parser. but confused which could be the best choice.</p>\n\n<p>my task is to extract relationship from english wikipedia text(other source may also be included later). What I need is an semantic path(with only most important information) between the two entities interesting. for instance,</p>\n\n<p>form text:\n<em><strong>\"In America, diabetes is, as everybody knows, a common disease.\"</strong></p>\n\n<p>I need the information: \n<strong>\"diabetes is disease\"</em></strong></p>\n\n<p>which implementation of parser would you suggest? Stanford? Maltparser? or other?</p>\n\n<p>any clue is appreciated.</p>\n",
    "score": 4,
    "creation_date": 1340184319,
    "view_count": 807,
    "answer_count": 2,
    "tags": "parsing;nlp;information-extraction"
  },
  {
    "question_id": 9941961,
    "title": "Algorithm for Natural-Looking Sentence in English Language",
    "body": "<p>I'm building an application that does sentence checking. Do you know are there any DLLs out there that recognize sentences and their logic and organize sentences correctly? Like put words in a sentence into a correct sentence.</p>\n\n<p>If it's not available, maybe you can suggest search terms that I can research.</p>\n",
    "score": 4,
    "creation_date": 1333106480,
    "view_count": 2963,
    "answer_count": 3,
    "tags": "c#;.net;nlp"
  },
  {
    "question_id": 9422073,
    "title": "Part-of-speech tagger in PHP?",
    "body": "<p>I am looking for a simple part-of-speech library or code that I can download. My criteria is that it must be simple to use and free is possible.</p>\n\n<p>Do you know such a library ?</p>\n",
    "score": 4,
    "creation_date": 1330034556,
    "view_count": 3183,
    "answer_count": 1,
    "tags": "php;nlp;part-of-speech"
  },
  {
    "question_id": 7894079,
    "title": "What language can be recommended for text mining/parsing?",
    "body": "<p>I'm doing some text mining in web pages. Currently I'm working with Java, but maybe there is more appropriate languages to do what I want.</p>\n\n<p>Example of some things I want to do:</p>\n\n<p>Determine the char type of a word based on it parts (letter, digit, symbols, etc.) as Alphabetic, Number, Alphanumeric, Symbol, etc.(there is more types).</p>\n\n<p>Discover stop words based on statistics.</p>\n\n<p>Discover some gramatical class (verb, noun, preposition, conjuntion) based on statistics and some logics.</p>\n\n<p>I was thinking about using Prolog and R (I don't know much about these languages), but I don't know if they are good for this or maybe, another language more appropriate.</p>\n\n<p>Which can I use? Good libs for Java are welcome too.</p>\n",
    "score": 4,
    "creation_date": 1319567529,
    "view_count": 1350,
    "answer_count": 4,
    "tags": "java;r;prolog;nlp;text-mining"
  },
  {
    "question_id": 7697034,
    "title": "What does discriminative reranking do in NLP tasks?",
    "body": "<p>Recently,i have read about the \"discriminative reranking for natural language processing\" by Collins.\nI'm confused what does the reranking actually do?\nAdd more global features to the rerank model? or something else?</p>\n",
    "score": 4,
    "creation_date": 1318079541,
    "view_count": 1528,
    "answer_count": 1,
    "tags": "nlp;machine-learning"
  },
  {
    "question_id": 6493956,
    "title": "Least used unicode delimiter",
    "body": "<p>I'm trying to tag my text with a delimiter at specific places that will be used later for parsing. I want to use a delimiter character that is least frequently used. I'm currently looking at the \"\\2\" or the U+0002 character. Is that safe enough to use? What other suggestions are there? The text is unicode and will have both english and non-english characters.</p>\n\n<p>A want to use a character that can still be \"exploded()\" by PHP.</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>Also I want to be able to display this piece of text on screen (to the browser) and the delimiter will be \"invisible\" to the user. I can definitely use a str_replace() to get rid of visible delimiters, but if there are good invisible delimiters, then no such processing is needed.</p>\n",
    "score": 4,
    "creation_date": 1309182870,
    "view_count": 3114,
    "answer_count": 1,
    "tags": "php;parsing;unicode;nlp;unicode-string"
  },
  {
    "question_id": 4099042,
    "title": "Sequence of vowels count",
    "body": "<p>This is not a homework question, it is an exam preparation question.</p>\n\n<p>I should deﬁne a function <code>syllables(word)</code> that counts the number of syllables in\nA word in the following way:</p>\n\n<p>• a maximal sequence of vowels is a syllable;</p>\n\n<p>• a ﬁnal <strong>e</strong> in a word is not a syllable (or the vowel sequence it is a part\nOf).</p>\n\n<p>I do not have to deal with any special cases, such as a ﬁnal <strong>e</strong> in a\nOne-syllable word (e.g., ’be’ or ’bee’).</p>\n\n<pre><code>&gt;&gt;&gt; syllables(’honour’)\n2\n&gt;&gt;&gt; syllables(’decode’)\n2\n&gt;&gt;&gt; syllables(’oiseau’)\n2\n</code></pre>\n\n<p>Should I use regular expression here or just list comprehension ?</p>\n",
    "score": 4,
    "creation_date": 1288889213,
    "view_count": 3021,
    "answer_count": 9,
    "tags": "python;regex;nlp"
  },
  {
    "question_id": 4051572,
    "title": "SQL word root matching",
    "body": "<p>I'm wondering whether major SQL engines out there (MS SQL, Oracle, MySQL) have the ability to understand that 2 words are related because they share the same root.</p>\n\n<p>We know it's easy to match \"networking\" when searching for \"network\" because the latter is a substring of the former.</p>\n\n<p>But do SQL engines have functions that can match \"network\" when searching for \"networking\"?</p>\n\n<p>Thanks a lot.</p>\n",
    "score": 4,
    "creation_date": 1288353304,
    "view_count": 2245,
    "answer_count": 4,
    "tags": "sql;nlp;stemming;lemmatization"
  },
  {
    "question_id": 951764,
    "title": "English translation of the STTS tagset",
    "body": "<p>The most common part-of-speech tagset for German is the <a href=\"http://www.ims.uni-stuttgart.de/projekte/corplex/TagSets/stts-table.html\" rel=\"nofollow noreferrer\">STTS tagset</a>. I need an English translation of the explanations for each tag. Not being a linguist I don't feel comfortable (let alone qualified) for translating this myself.</p>\n\n<p>Google turned up nothing, so any help is appreciated.</p>\n",
    "score": 4,
    "creation_date": 1244134155,
    "view_count": 645,
    "answer_count": 2,
    "tags": "nlp"
  },
  {
    "question_id": 660683,
    "title": "Non regular context-free language and infinite regular sublanguages",
    "body": "<p>I had a work for the university which basically said:  </p>\n\n<p>\"Demonstrates that the non-regular language L={0^n 1^n : n natural} had no infinite regular sublanguages.\"</p>\n\n<p>I demonstrated this by contradiction.  I basically said that there is a language S which is a sublanguage of L and it is a regular language.  Since the possible Regular expressions for S are 0*, 1*, (1+0)* and (0o1)*.  I check each grammar and demonstrate that none of them are part of the language L.  </p>\n\n<p>However, how I could prove that ANY non regular context free language could not contain any regular infinite sublanguages?</p>\n\n<p>I don't want the prove per se, I just want to be pointed in the right direction.</p>\n",
    "score": 4,
    "creation_date": 1237423888,
    "view_count": 4387,
    "answer_count": 5,
    "tags": "regex;nlp;context-free-grammar"
  },
  {
    "question_id": 283305,
    "title": "CORPUS resource",
    "body": "<p>I am designing an <strong>Automatic text summarizer</strong>. One of the major modules in this project requires <strong>TRAINING CORPUS</strong>. Can someone please help me out by providing <strong>TRAINING CORPUS</strong> or <strong>referring some link to download</strong> it. Thanks in anticipation</p>\n",
    "score": 4,
    "creation_date": 1226478317,
    "view_count": 4564,
    "answer_count": 4,
    "tags": "nlp;corpus"
  },
  {
    "question_id": 75195026,
    "title": "Transformers gets killed for no reason on linux",
    "body": "<p>So i'm trying to run inference on a Huggingface model, the model is 6.18gb.\nThis morning I was on Windows and it was possible to load the model, but inference was very slow so I took a look at DeepSpeed but only available on linux so I switched to Zorin OS.\nNow the exact same script gets killed when running</p>\n<pre><code>from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(&quot;Cedille/fr-boris&quot;, device_map = &quot;auto&quot;)\n</code></pre>\n<p>What is going on ?</p>\n",
    "score": 4,
    "creation_date": 1674320061,
    "view_count": 3088,
    "answer_count": 1,
    "tags": "memory-management;nlp;out-of-memory;huggingface-transformers;huggingface"
  },
  {
    "question_id": 74175424,
    "title": "Is Spacy lemmatization not working properly or does it not lemmatize all words ending with &quot;-ing&quot;?",
    "body": "<p>When I run the spacy lemmatizer, it does not lemmatize the word &quot;consulting&quot; and therefore I suspect it is failing.</p>\n<p>Here is my code:</p>\n<pre><code>nlp = spacy.load('en_core_web_trf', disable=['parser', 'ner'])\nlemmatizer = nlp.get_pipe('lemmatizer')\ndoc = nlp('consulting')\nprint([token.lemma_ for token in doc])\n</code></pre>\n<p>And my output:</p>\n<pre><code>['consulting']\n</code></pre>\n",
    "score": 4,
    "creation_date": 1666566641,
    "view_count": 3752,
    "answer_count": 2,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 69304467,
    "title": "How to download &quot;en_core_web_sm&quot; model at runtime in spacy?",
    "body": "<p>I am working on writing a lambda function which depends on Spacy's &quot;en_core_web_sm&quot; model. Due to limitation of AWS Lambda, I need to find a mechanism which allows me to download the model at runtime (I can afford to increase the lambda timeout).</p>\n<p><strong>Note:</strong> I can't use EFS at the moment due to some requirement restriction.</p>\n",
    "score": 4,
    "creation_date": 1632417838,
    "view_count": 9134,
    "answer_count": 1,
    "tags": "amazon-web-services;aws-lambda;nlp;spacy;serverless-framework"
  },
  {
    "question_id": 69270987,
    "title": "How to resolve the error related to frame used in zstandard which requires too much memory for decoding",
    "body": "<p>To download the data related to questions and answers, I am following the script on <a href=\"https://github.com/facebookresearch/ELI5\" rel=\"nofollow noreferrer\">facebook/ELI5</a>.</p>\n<p>There it says to run the command: <code>python download_reddit_qalist.py -Q</code>. On running this command, I get an error on line number 70 in python file 'download_reddit_qalist.py', where the zstandardDecompressor object is enumerated. The error log says that:</p>\n<blockquote>\n<p>zstd.ZstdError: Zstd decompress error: Frame requires too much memory\nfor decoding</p>\n</blockquote>\n<p>Thinking the memory issue, I allocated 32 gb memory to the container along with 8 CPUs. But the error stays.</p>\n<p>When I replaced the enumerate function with ElementTree.iterparse(), then along with this error, another message adds up:</p>\n<blockquote>\n<p>for i, l in ET.iterparse(f):</p>\n</blockquote>\n<blockquote>\n<p>File &quot;/anaconda3/lib/python3.8/xml/etree/ElementTree.py&quot;, line 1229, in iterator</p>\n</blockquote>\n<blockquote>\n<p>data = source.read(100 * 2048)</p>\n</blockquote>\n<blockquote>\n<p>zstd.ZstdError: zstd decompress error: Frame requires too much memory for decoding</p>\n</blockquote>\n<p>Does anyone face the similar error? I have the docker container running on the slurm cluster. If you need more information let me know.</p>\n",
    "score": 4,
    "creation_date": 1632235548,
    "view_count": 4614,
    "answer_count": 1,
    "tags": "nlp;reddit;nlp-question-answering"
  },
  {
    "question_id": 67909030,
    "title": "How to prepare text for BERT - getting error",
    "body": "<p>I am trying to learn BERT for text classification. I am finding some problem in preparing data for using BERT.</p>\n<p>From my Dataset, I am segregating the sentiments and reviews as:</p>\n<pre><code>X = df['sentiments']\ny = df['reviews'] #it contains four different class of reviews\n</code></pre>\n<p>Next,</p>\n<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\ntrain_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512) \n</code></pre>\n<p>Here is where I get error:</p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-70-22714fcf7991&gt; in &lt;module&gt;()\n----&gt; 1 train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n      2 #valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)\n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\n   2261         if not _is_valid_text_input(text):\n   2262             raise ValueError(\n-&gt; 2263                 \"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\n   2264                 \"or `List[List[str]]` (batch of pretokenized examples).\"\n   2265             )\n\nValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).</code></pre>\r\n</div>\r\n</div>\r\n</p>\n<p>When I am trying to convert X to list and use it, I get another error:</p>\n<pre><code>TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n</code></pre>\n<p>Can someone please explain where the problem is? Previously I followed a tutorial on 20 news dataset and it worked. But now when I am using it for another project, it doesn't work and I feel sad.</p>\n<p>Thanks.</p>\n",
    "score": 4,
    "creation_date": 1623259659,
    "view_count": 7511,
    "answer_count": 1,
    "tags": "python-3.x;nlp;bert-language-model;transfer-learning"
  },
  {
    "question_id": 55241927,
    "title": "SpaCy -- intra-word hyphens. How to treat them one word?",
    "body": "<p>Following is the code provided as answer to the <a href=\"https://stackoverflow.com/questions/52293874/why-does-spacy-not-preserve-intra-word-hyphens-during-tokenization-like-stanford/52380286#comment97213137_52380286\">question</a>;</p>\n\n<pre><code>import spacy\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\nimport re\n\nnlp = spacy.load('en')\n\ninfixes = nlp.Defaults.prefixes + (r\"[./]\", r\"[-]~\", r\"(.'.)\")\n\ninfix_re = spacy.util.compile_infix_regex(infixes)\n\ndef custom_tokenizer(nlp):\n    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n\nnlp.tokenizer = custom_tokenizer(nlp)\n\ns1 = \"Marketing-Representative- won't die in car accident.\"\ns2 = \"Out-of-box implementation\"\n\nfor s in s1,s2:\n    doc = nlp(\"{}\".format(s))\n    print([token.text for token in doc])\n</code></pre>\n\n<p>Result  </p>\n\n<pre><code>$python3 /tmp/nlp.py  \n['Marketing-Representative-', 'wo', \"n't\", 'die', 'in', 'car', 'accident', '.']  \n['Out-of-box', 'implementation']  \n</code></pre>\n\n<p>What are the first (r\"[./]\") and the last (r\"(.'.)\") patterns used for in the following?</p>\n\n<pre><code>infixes = nlp.Defaults.prefixes + (r\"[./]\", r\"[-]~\", r\"(.'.)\")\n</code></pre>\n\n<p>Edit: I expect the splits to be as follows;</p>\n\n<p>That</p>\n\n<p>is</p>\n\n<p>Yahya</p>\n\n<p>'s</p>\n\n<p>laptop-cover</p>\n\n<p>.</p>\n\n<hr>\n\n<p>I want spacy to treat intra-hyphen word as one token without impacting negatively on other split rules.</p>\n\n<p>\"That is Yahya's laptop-cover. 3.14!\"</p>\n\n<p>[\"That\", \"is\", \"Yahya\", \"'s\", \"laptop-cover\", \".\", \"3.14\", \"!\"] (<strong><em>EXPECTED</em></strong>)</p>\n\n<p>By default,</p>\n\n<pre><code>import spacy\nnlp = spacy.load('en_core_web_md')\nfor token in nlp(\"That is Yahya's laptop-cover. 3.14!\"):\n    print (token.text)\n</code></pre>\n\n<p>SpaCy gives;</p>\n\n<pre><code>[\"That\", \"is\", \"Yahya\", \"'s\", \"laptop\", \"-\", \"cover\", \".\", \"3.14\", \"!\"]\n</code></pre>\n\n<p>However,</p>\n\n<pre><code>from spacy.util import compile_infix_regex\ninfixes = nlp.Defaults.prefixes + tuple([r\"[-]~\"])\ninfix_re = spacy.util.compile_infix_regex(infixes)\nnlp.tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\nfor token in nlp(\"That is Yahya's laptop-cover. 3.14!\"):\n    print (token.text)\n</code></pre>\n\n<p>gives;</p>\n\n<pre><code>[\"That\", \"is\", \"Yahya\", \"'\", \"s\", \"laptop-cover.\", \"3.14\", \"!\"]\n</code></pre>\n",
    "score": 4,
    "creation_date": 1553001151,
    "view_count": 3220,
    "answer_count": 1,
    "tags": "nlp;tokenize;spacy"
  },
  {
    "question_id": 53971240,
    "title": "Normalize vectors in gensim model",
    "body": "<p>I have a pre-trained word embedding with vectors of different norms, and I want to normalize all vectors in the model. I am doing it with a for loop that iterates each word and normalizes its vector, but the model us huge and takes too much time. Does <code>gensim</code> include any way to do this faster? I cannot find it.</p>\n<p>Thanks!!</p>\n",
    "score": 4,
    "creation_date": 1546100520,
    "view_count": 6032,
    "answer_count": 1,
    "tags": "python;nlp;gensim;word-embedding"
  },
  {
    "question_id": 51406066,
    "title": "Abbreviation Detection for Python",
    "body": "<p>I am trying to measure the similarity of company names, however I am having difficulties while I'm trying to match the abbreviations for those names. For example:</p>\n\n<pre><code>IBM\nThe International Business Machines Corporation\n</code></pre>\n\n<p>I have tried using <code>fuzzywuzzy</code> to measure the similarity:</p>\n\n<pre><code>&gt;&gt;&gt; fuzz.partial_ratio(\"IBM\",\"The International Business Machines Corporation\")\n33\n&gt;&gt;&gt; fuzz.partial_ratio(\"General Electric\",\"GE Company\")\n20\n&gt;&gt;&gt; fuzz.partial_ratio(\"LTCG Holdings Corp\",\"Long Term Care Group Inc\")\n39\n&gt;&gt;&gt; fuzz.partial_ratio(\"Young Innovations Inc\",\"YI LLC\")\n33\n</code></pre>\n\n<p>Do you know any techniques to measure a higher similarity for such abbreviations?</p>\n",
    "score": 4,
    "creation_date": 1531928527,
    "view_count": 7781,
    "answer_count": 1,
    "tags": "python;string;nlp;similarity;fuzzy-comparison"
  },
  {
    "question_id": 47011991,
    "title": "Apache Open NLP vs NLTK",
    "body": "<p>We have a spring boot application integrated with Node.js and socket.io chat application , to which we want to integrate Natural language processing. Not getting any direction on which of these two <strong><em><code>Apache-OpenNlp or NLTK</code></em></strong> would be a better choice for us as both of the frameworks offer the kind of processing we need. </p>\n\n<p>Wrt to the features provided by the frameworks , they both are good. Both have features that we are looking for. More than how to choose between features , what would suit our architecture better is a perspective I would like.. </p>\n\n<p>Any suggestions ? </p>\n",
    "score": 4,
    "creation_date": 1509356326,
    "view_count": 3915,
    "answer_count": 1,
    "tags": "architecture;nlp;nltk;opennlp"
  },
  {
    "question_id": 46083322,
    "title": "Multiple tags for single document in doc2vec. TaggedDocument",
    "body": "<p>Is it possible to to train a doc2vec model where a single document has multiple tags?\nFor example, in movie reviews,</p>\n\n<pre><code>doc0 = doc2vec.TaggedDocument(words=review0,tags=['UID_0','horror','action'])\ndoc1 = doc2vec.TaggedDocument(words=review1,tags=['UID_1','drama','action','romance'])\n</code></pre>\n\n<p>In such case where each document has a unique tag (UID) and multiple categorical tags, how do I access the vector after the training? For example, what would be the most proper syntax to call</p>\n\n<pre><code>model['UID_1']\n</code></pre>\n",
    "score": 4,
    "creation_date": 1504727537,
    "view_count": 3437,
    "answer_count": 1,
    "tags": "python;nlp;gensim;word2vec;doc2vec"
  },
  {
    "question_id": 44254493,
    "title": "tidytext read files from folder",
    "body": "<p>I'm trying to read a folder of pdf files into a dataframe in R.  I'm able to read individual pdf files in using the <code>pdftools</code> library and <code>pdf_text(filepath)</code>.  </p>\n\n<p>Ideally, I could grab the author and title of a series of pdf's that are then pushed into a dataframe that has a column with these so that I can then use basic <code>tidytext</code> functions on the text.</p>\n\n<p>For a single file right now, I can just use:</p>\n\n<pre><code>library(pdftools)\nlibrary(tidytext)\nlibrary(dplyr)\ntxt &lt;- pdf_text(\"filpath\")\ntxt &lt;- data_frame(txt)\ntxt %&gt;%\n     unnest_tokens(word, txt)\n</code></pre>\n\n<p>Here I have a dataframe with single words.  I'd like to get to a dataframe where I have articles unpacked including a title and author column.</p>\n",
    "score": 4,
    "creation_date": 1496123722,
    "view_count": 2051,
    "answer_count": 2,
    "tags": "r;nlp;tidytext"
  },
  {
    "question_id": 42137959,
    "title": "Detect conditional tense via spacy?",
    "body": "<p>is there any function/attribute built in spacy to uncover tokens in conditional tense? Or some possible detour to get to those?</p>\n",
    "score": 4,
    "creation_date": 1486646755,
    "view_count": 4400,
    "answer_count": 2,
    "tags": "nlp;conditional-statements;spacy"
  },
  {
    "question_id": 39060283,
    "title": "Using Keras for text classification",
    "body": "<p>I am struggling to approach the bag of words / vocabulary method for representing my input data as one hot vectors for my neural net model in keras. </p>\n\n<p>I would like to build a simple 3 layer network but I need help in understanding and developing an approach to transform my labelled data in the form of text,sentinment which is has 7 labels, in the range of 0 - 1 in steps of 0.2. </p>\n\n<p>I have tried to use scikit's vectorisers but they are too rigid i.e they either tokenise words or characters, whereas I need a sentence to be compared to the vocabulary which includes words, characters, punctuation and emojis. When i use tfid on a test sentence it only counts the words and ignores everything else. I also need guidance on taking this one hot approach and how it will be implemented in keras.</p>\n",
    "score": 4,
    "creation_date": 1471745630,
    "view_count": 9689,
    "answer_count": 1,
    "tags": "python;nlp;keras;text-classification"
  },
  {
    "question_id": 38241134,
    "title": "Input shape for Keras LSTM/GRU language model",
    "body": "<p>I am trying to train a language model on word level in Keras. </p>\n\n<p>I have my X and Y, both with the shape (90582L, 517L)</p>\n\n<p>When I try fit this model:</p>\n\n<pre><code>print('Build model...')\nmodel = Sequential()\nmodel.add(GRU(512, return_sequences=True, input_shape=(90582, 517)))\nmodel.add(Dropout(0.2))\nmodel.add(GRU(512, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(TimeDistributedDense(1))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\nmodel.fit(x_pad, y_pad, batch_size=128, nb_epoch=2)\n</code></pre>\n\n<p>I get the error:</p>\n\n<pre><code>Exception: Error when checking model input: \nexpected gru_input_7 to have 3 dimensions, but got array with shape (90582L, 517L)\n</code></pre>\n\n<p>I need some guidance as to what the input shape should be? I've done trial and error on all sorts of combinations but it seems I am misunderstanding something fundamental.</p>\n\n<p>In the Keras text generation example, the X matrix had 3 dimensions. I have no idea what the third dimension is supposed to be though.  </p>\n",
    "score": 4,
    "creation_date": 1467881281,
    "view_count": 14009,
    "answer_count": 1,
    "tags": "python;nlp;keras;lstm;language-model"
  },
  {
    "question_id": 37782569,
    "title": "How to find most frequent noun following the word &#39;the&#39;?",
    "body": "<pre><code>from nltk.corpus import brown\n\ntagged = brown.tagged_words(tagset='universal')\n</code></pre>\n\n<p>I understand that to find the most frequent word following 'the' is done like so </p>\n\n<pre><code>cfd3 = nltk.ConditionalFreqDist(nltk.bigrams(brown.words())\n\ncfd3['the'].max()\n</code></pre>\n\n<p>however, how would one go about finding the most frequent noun following the word 'the' </p>\n",
    "score": 4,
    "creation_date": 1465796817,
    "view_count": 2063,
    "answer_count": 2,
    "tags": "python-3.x;nlp;nltk;corpus;pos-tagger"
  },
  {
    "question_id": 37391443,
    "title": "Annotated Training data for NER corpus",
    "body": "<p>It is mentioned in the documentation of opennlp that we've to train our model with 15000 line for a good performance. \nnow, I've to extract different entities from the document which means I've to add different tags for many tokens in the training data(15000 lines) which will take a lot of time. Is there any other way to do this? which will reduce the time or any other method which I can proceed.</p>\n\n<p>Thanks.</p>\n",
    "score": 4,
    "creation_date": 1464007680,
    "view_count": 3390,
    "answer_count": 4,
    "tags": "nlp;opennlp;corpus;training-data;named-entity-recognition"
  },
  {
    "question_id": 36181361,
    "title": "Convert dfmSparse from Quanteda package to Data Frame or Data Table in R",
    "body": "<p>I have a dfmSparse object (large, with 2.1GB) which is tokenized and with ngrams (unigrams, bigrams, trigrams and fourgrams), and <strong>I want to convert it to a data frame or a data table object with the columns: Content and Frequency</strong>.</p>\n\n<p>I tried to unlist... but didn't work. I'm new in NLP, and I don't know with method to use, I'm without ideas and didn't found a solution here or with Google.</p>\n\n<p>Some info about the data:</p>\n\n<pre><code>&gt;str(tokfreq)\nFormal class 'dfmSparse' [package \"quanteda\"] with 11 slots\n  ..@ settings    :List of 1\n  .. ..$ : NULL\n  ..@ weighting   : chr \"frequency\"\n  ..@ smooth      : num 0\n  ..@ ngrams      : int [1:4] 1 2 3 4\n  ..@ concatenator: chr \"_\"\n  ..@ Dim         : int [1:2] 167500 19765478\n  ..@ Dimnames    :List of 2\n  .. ..$ docs    : chr [1:167500] \"character(0).content\" \"character(0).content\" \"character(0).content\" \"character(0).content\" ...\n  .. ..$ features: chr [1:19765478] \"add\" \"lime\" \"juice\" \"tequila\" ...\n  ..@ i           : int [1:54488417] 0 75 91 178 247 258 272 327 371 391 ...\n  ..@ p           : int [1:19765479] 0 3218 3453 4015 4146 4427 4637 140665 140736 142771 ...\n  ..@ x           : num [1:54488417] 1 1 1 1 5 1 1 1 1 1 ...\n  ..@ factors     : list()\n\n&gt;summary(tokfreq)\n       Length         Class          Mode \n3310717565000     dfmSparse            S4\n</code></pre>\n\n<p>Thanks!</p>\n\n<p>EDITED:\nThis is how I created the dataset from a corpus:</p>\n\n<pre><code># tokenize\ntokenized &lt;- tokenize(x = teste, ngrams = 1:4)\n# Creating the dfm\ntokfreq &lt;- dfm(x = tokenized)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1458744439,
    "view_count": 3035,
    "answer_count": 2,
    "tags": "r;dataframe;nlp;data.table;quanteda"
  },
  {
    "question_id": 27454767,
    "title": "How to vectorize labeled bigrams with scikit learn?",
    "body": "<p>I'm self studying how to use scikit-learn and i decided to start the <a href=\"http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#where-to-from-here\" rel=\"nofollow\">second task</a> but with my own corpus. I obtained some bigrams by hand, let's say:</p>\n\n<pre><code>training_data = [[('this', 'is'), ('is', 'a'),('a', 'text'), 'POS'],\n[('and', 'one'), ('one', 'more'), 'NEG']\n[('and', 'other'), ('one', 'more'), 'NEU']]\n</code></pre>\n\n<p>I would like to vectorize them in a format that nicely can be filled in some classification algorithm provided by scikit-learn (svc, multnomial naive bayes, etc). This is what i tried:</p>\n\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer(analyzer='word')\n\nX = count_vect.transform(((' '.join(x) for x in sample)\n                  for sample in training_data))\n\nprint X.toarray()\n</code></pre>\n\n<p>The problem with this is that i dont know how to treat the label (i.e. <code>'POS', 'NEG', 'NEU'</code>), do i need to \"vectorize\" the label too in order to pass the <code>training_data</code> to a classification algorithm or i just could let it like 'POS' or any other kind of string?. Another problem is that I'm getting this:</p>\n\n<pre><code>raise ValueError(\"Vocabulary wasn't fitted or is empty!\")\nValueError: Vocabulary wasn't fitted or is empty!\n</code></pre>\n\n<p>So, how can i vectorize bigrams like <code>training_data</code>. I also was reading about <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\" rel=\"nofollow\">dictvectorizer</a> and <a href=\"https://github.com/paulgb/sklearn-pandas\" rel=\"nofollow\">Sklearn-pandas</a>, do you guys think using them could be a better aproach for this task?</p>\n",
    "score": 4,
    "creation_date": 1418436045,
    "view_count": 2690,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;scikit-learn;nltk"
  },
  {
    "question_id": 23258832,
    "title": "How to check for unreadable OCRed text with NLTK",
    "body": "<p>I am using NLTK to analyze a corpus that has been OCRed. I'm new to NLTK. Most of the OCR is good -- but sometimes I come across lines that are plainly junk. For instance: <code>oomfi ow Ba wmnondmam BE wBwHo&lt;oBoBm. Bowman as: Ham: 8 ooww om $5</code> </p>\n\n<p>I want to identify (and filter out) such lines from my analysis. </p>\n\n<p>How do NLP practitioners handle this situation? Something like: if 70 % of the words in the sentence are not in wordnet, discard. Or if NLTK can't identify the part of speech for 80% of the word, then discard? What algorithms work for this? Is there a \"gold standard\" way to do this?</p>\n",
    "score": 4,
    "creation_date": 1398307177,
    "view_count": 465,
    "answer_count": 1,
    "tags": "nlp;nltk"
  },
  {
    "question_id": 22031968,
    "title": "How to find distance between two synset using python nltk in wordnet hierarchy?",
    "body": "<p>Suppose I have two synsets synset(car.n.01') and synset('bank.n.01') and If I want to find the distance between these two synset in wordnet hierarchy then How can I do it using nltk?<br>\nI searched on internet but I am getting similarity algorithms like lin,resnik,jcn etc which are not solution for my question.<br>\nPlease help me to solve this problem.</p>\n",
    "score": 4,
    "creation_date": 1393389914,
    "view_count": 9727,
    "answer_count": 2,
    "tags": "python;nlp;nltk;wordnet"
  },
  {
    "question_id": 20015715,
    "title": "Paragraph breaks using Stanford CoreNLP",
    "body": "<p>Is there a way to extract paragraph information from Stanford CoreNLP?  I'm currently using it to extract sentences from a document, but am also interested in identifying the paragraph structure of the document, which I'd ideally like CoreNLP to do for me.  I have paragraph breaks as double line breaks in my source document.  I've looked through CoreNLP's javadoc, and it seems there is a <a href=\"http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/CoreAnnotations.ParagraphsAnnotation.html\" rel=\"nofollow\"><code>ParagraphAnnotation</code></a> class, but the documentation doesn't seem to specify what it contains, and I see no example anywhere of how to use it.  Can anyone point me in the right direction?</p>\n\n<p>For reference, my current code does something like this:</p>\n\n<pre><code>    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);\n    List&lt;Sentence&gt; convertedSentences = new ArrayList&lt;&gt; ();\n    for (CoreMap sentence : sentences)\n    {\n        convertedSentences.add (new Sentence (sentence));\n    }\n</code></pre>\n\n<p>where Sentence's constructor extracts the words from the sentence.  How would I extend this so that I get an extra level of data, that is my currently document-wide 'convertedSentences' list is supplemented by a 'convertedParagraphs' list, each entry of which contains a 'convertedSentences' list?</p>\n\n<p>I tried the approach that seemed most obvious to me:</p>\n\n<pre><code>List&lt;CoreMap&gt; paragraphs = document.get(ParagraphsAnnotation.class);\nfor (CoreMap paragraph : paragraphs)\n{\n        List&lt;CoreMap&gt; sentences = paragraph.get(SentencesAnnotation.class);\n        List&lt;Sentence&gt; convertedSentences = new ArrayList&lt;&gt; ();\n        for (CoreMap sentence : sentences)\n        {\n            convertedSentences.add (new Sentence (sentence));\n        }\n\n        convertedParagraphs.add (new Paragraph (convertedSentences));\n}\n</code></pre>\n\n<p>but this didn't work, so I guess I misunderstand something about how this is supposed to work.</p>\n",
    "score": 4,
    "creation_date": 1384584922,
    "view_count": 1730,
    "answer_count": 2,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 18174646,
    "title": "Split multi-paragraph documents into paragraph-numbered sentences",
    "body": "<p>I have a list of well-parsed, multi-paragraph documents (all paragraphs separated by <em>\\n\\n</em> and sentences separated by \".\") that I'd like to split into sentences, together with a number indicating the paragraph number within the document. For example, the (two paragraph) input is:</p>\n\n<pre><code>First sentence of the 1st paragraph. Second sentence of the 1st paragraph. \\n\\n \n\nFirst sentence of the 2nd paragraph. Second sentence of the 2nd paragraph. \\n\\n\n</code></pre>\n\n<p>Ideally the output should be:</p>\n\n<pre><code>1 First sentence of the 1st paragraph. \n\n1 Second sentence of the 1st paragraph. \n\n2 First sentence of the 2nd paragraph.\n\n2 Second sentence of the 2nd paragraph.\n</code></pre>\n\n<p>I'm familiar with the Lingua::Sentences package in Perl that can split documents into sentences. However it is not compatible with paragraph numbering. As such I'm wondering if there's an alternative way to achieve the above (the documents contains no abbreviations). Any help is greatly appreciated. Thanks!</p>\n",
    "score": 4,
    "creation_date": 1376241879,
    "view_count": 662,
    "answer_count": 2,
    "tags": "regex;perl;nlp;text-segmentation"
  },
  {
    "question_id": 18127183,
    "title": "Java Algorithm To Extract Information From a String",
    "body": "<p>I'm trying to implement a smart search feature in my application.\nUsecase: The user enters the search term in a textbox</p>\n\n<p>Eg: <em>Find me a christian male 28 years old from Brazil.</em> </p>\n\n<p>I need to be parse the input into a map as follows:</p>\n\n<p><strong>Gender:</strong> male\n<strong>Age:</strong> 38\n<strong>Location:</strong> Brazil\n<strong>Relegion:</strong> Christian</p>\n\n<p>Already had a glance on : OpenNLP, Cross Validate, Java Pattern Matching and Regex, Information Extraction. I'm confused which one I need to look deeper into.</p>\n\n<p>Is there any <strong>java</strong> lib already available for this specific domain? </p>\n",
    "score": 4,
    "creation_date": 1375968173,
    "view_count": 3900,
    "answer_count": 3,
    "tags": "java;machine-learning;nlp;opennlp;information-extraction"
  },
  {
    "question_id": 14692489,
    "title": "Chunking with nltk",
    "body": "<p>How can I obtain all the chunk from a sentence given a pattern.\nExemple</p>\n\n<pre><code>NP:{&lt;NN&gt;&lt;NN&gt;}\n</code></pre>\n\n<p>Sentence tagged:</p>\n\n<pre><code>[(\"money\", \"NN\"), (\"market\", \"NN\") (\"fund\", \"NN\")]\n</code></pre>\n\n<p>If I parse I obtain</p>\n\n<pre><code>(S (NP money/NN market/NN) fund/NN)\n</code></pre>\n\n<p>I would like to have also the other alternative that is </p>\n\n<pre><code>(S money/NN (NP market/NN fund/NN))\n</code></pre>\n",
    "score": 4,
    "creation_date": 1360000300,
    "view_count": 3836,
    "answer_count": 2,
    "tags": "python;nlp;nltk;chunking"
  },
  {
    "question_id": 13477357,
    "title": "calculating TF-IDF for words in documents in solr and java",
    "body": "<p>I can easily get the TF by counting the number of Term in a document \nand i want to know how to calculate document frequency,\ni.e. the number of documents that contain this term</p>\n\n<p>What I've reached so far is querying solr with a large number of rows and counting the results  back , but this is very Time and memory expensive . I want to count the terms only </p>\n\n<pre><code>    SolrQuery q = new SolrQuery();\n    q.setQuery(\"tweet_text:\"+kw);\n    q.addField(\"tweet_text\");\n    q.setRows(40000000);        \n    SolrDocumentList results = null ;\n\n    try {\n        QueryResponse rsp = solrServer.query(q);\n        results = rsp.getResults();\n    } catch (SolrServerException e) {\n        e.printStackTrace();\n    }\n\n    ArrayList&lt;String&gt; tweets = new ArrayList&lt;String&gt;();\n    for (SolrDocument doc : results)\n    {\n        tweets.add(doc.getFieldValue(\"tweet_text\").toString());\n    }\n</code></pre>\n",
    "score": 4,
    "creation_date": 1353428042,
    "view_count": 5995,
    "answer_count": 1,
    "tags": "java;solr;nlp;tf-idf"
  },
  {
    "question_id": 10668992,
    "title": "NLP library for determining if a sentence is equivalent to another sentence?",
    "body": "<p>I would like to find an NLP library in Python, PHP or even JavaScript for determining whether a sentence in a string is equivalent to a differently structured sentence?</p>\n\n<p>For example, the library would need to be able to determine whether these two sentences are equivalent:</p>\n\n<p>\"Would you like the order for here or to go?\"\n\"Do you want the order for here or to go?\"</p>\n\n<p>Is there such a thing?  Or would it actually be easier for me to build something like this myself for the specific application I need it for?</p>\n",
    "score": 4,
    "creation_date": 1337463021,
    "view_count": 811,
    "answer_count": 2,
    "tags": "php;javascript;python;nlp"
  },
  {
    "question_id": 8119295,
    "title": "SMS Text normalization",
    "body": "<p>I am looking for a good library or some project that has been done in the area of SMS text normalization. I have found some good research projects like <a href=\"http://www-nlp.stanford.edu/sms/\" rel=\"nofollow\">this</a> one.</p>\n\n<p>I am using Java as the programming language.</p>\n\n<p>The concept in a nutshell is to handle SMS based text like \"<strong>tel him 2 go home nw</strong>\" and convert it to normal english language text \"<strong>tell him to go home now</strong>\".</p>\n",
    "score": 4,
    "creation_date": 1321261643,
    "view_count": 1000,
    "answer_count": 2,
    "tags": "java;sms;nlp"
  },
  {
    "question_id": 7967419,
    "title": "Is (a^p )(b^q) a regular language",
    "body": "<p>I read somewhere that {(a^p)(b^q):p,Q belong to N} is a regular language. However, i dont think this is correct. This can be proved using pumping lemma. Just want to verify if my solution is correct</p>\n\n<p>Let y be ab . Thus, x(y^n)z does not belong to L as there will be some b's before a's for n>=1. However, expression does not allow this. Thus, (a^p)(b^q) is not a RL</p>\n",
    "score": 4,
    "creation_date": 1320155213,
    "view_count": 375,
    "answer_count": 2,
    "tags": "nlp;regular-language"
  },
  {
    "question_id": 7310030,
    "title": "Autocorrect spelling mistakes in text input",
    "body": "<p>I am writing a natural language processor in C# that extracts the sentiment (positive/negative) of a sentence.  There is something of an issue, though, in being able to discern the sentiment of a misspelled word - if it's not in the dictionary, I can neither tag it nor rate it!</p>\n\n<p>I know there has to be a way to handle this.  Google gives accurate suggestions all the time, I simply need to take the top suggestion from a similar algorithm and hit the database with it.  The problem is, I'm not sure where to start with algorithm names and so forth.  I need help figuring that out.</p>\n\n<p>I checked around on the site for similar questions, and found some concepts that seemed useful, but the basic way of handling the distance between a misspelling and a real word basically relied on hitting every word in your data set, which seems horribly inefficient.  Some help with ideas to make the algorithm run quickly would also be much appreciated; this analysis engine is supposed to be able to handle multiple thousands of items a day.</p>\n\n<p>Thanks in advance.</p>\n",
    "score": 4,
    "creation_date": 1315236395,
    "view_count": 2832,
    "answer_count": 3,
    "tags": "c#;algorithm;nlp;autocorrect"
  },
  {
    "question_id": 1916892,
    "title": "Training Hidden Markov Models without Tagged Corpus Data",
    "body": "<p>For a linguistics course we implemented Part of Speech (POS) tagging using a hidden markov model, where the hidden variables were the parts of speech. We trained the system on some tagged data, and then tested it and compared our results with the gold data.</p>\n\n<p>Would it have been possible to train the HMM without the tagged training set? </p>\n",
    "score": 4,
    "creation_date": 1260990119,
    "view_count": 1259,
    "answer_count": 2,
    "tags": "artificial-intelligence;machine-learning;nlp;linguistics;markov-models"
  },
  {
    "question_id": 75673222,
    "title": "Semantic searching using Google flan-t5",
    "body": "<p>I'm trying to use google flan t5-large to create embeddings for a simple semantic search engine. However, the generated embeddings cosine similarity with my query is very off. Is there something I'm doing wrong?</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.spatial.distance import euclidean\n\ntokenizer = AutoTokenizer.from_pretrained('google/flan-t5-large')\nmodel = AutoModel.from_pretrained('google/flan-t5-large')\n\n# Set the text to encode\n\ndef emebddings_generate(text):\n  all_embeddings = []\n  for i in text:\n    input_ids = tokenizer.encode(i, return_tensors='pt')\n    with torch.no_grad():\n      embeddings = model(input_ids, decoder_input_ids=input_ids).last_hidden_state.mean(dim=1)\n      all_embeddings.append((embeddings,i))\n  return all_embeddings\n\ndef run_query(query,corpus):\n  input_ids = tokenizer.encode(query, return_tensors='pt')\n  with torch.no_grad():\n        quer_emebedding=model(input_ids,decoder_input_ids=input_ids).last_hidden_state.mean(dim=1)\n\n  similairtiy = []\n\n  for embeds in corpus:\n    sim = euclidean(embeds[0].flatten(),quer_emebedding.flatten())\n    similairtiy.append((embeds[1],float(sim)))\n  return similairtiy\n\n\ntext = ['some sad song', ' a very happy song']\ncorpus = emebddings_generate(text)\n\nquery = &quot;I'm feeling so sad rn&quot;\nsimilairtiy = run_query( query,corpus)\nfor i in similairtiy:\n  print(i)\n  print(i[1],i[0])\n\n</code></pre>\n<p>I've tried different pooling techniques as well as using other distance metrics.</p>\n",
    "score": 4,
    "creation_date": 1678279623,
    "view_count": 3845,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;word-embedding;semantic-search"
  },
  {
    "question_id": 74528441,
    "title": "Detect passive or active sentence from text",
    "body": "<p>Using the Python package <a href=\"https://spacy.io/\" rel=\"nofollow noreferrer\">spaCy</a>, how can one detect whether a sentence uses a passive or active voice? For example, the following sentences should be detected as using a passive and active voice respectively:</p>\n<pre class=\"lang-none prettyprint-override\"><code>passive_sentence = &quot;John was accused of committing crimes by David&quot;\n# passive voice &quot;John was accused&quot;\n\nactive_sentence = &quot;David accused John of committing crimes&quot;\n# active voice &quot;David accused John&quot;\n</code></pre>\n",
    "score": 4,
    "creation_date": 1669099459,
    "view_count": 2574,
    "answer_count": 2,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 72477087,
    "title": "How to select only rows containing emojis and emoticons in Python?",
    "body": "<p>I have DataFrame in Python Pandas like below:</p>\n<pre><code>sentence\n------------\n😎🤘🏾\nI like it\n+1😍😘\nOne :-) :)\nhah\n</code></pre>\n<p>I need to select only rows containing emoticons or emojis, so as a result I need something like below:</p>\n<pre><code>sentence\n------------\n😎🤘🏾\n+1😍😘\nOne :-) :)\n</code></pre>\n<p>How can I do that in Python ?</p>\n",
    "score": 4,
    "creation_date": 1654176442,
    "view_count": 1009,
    "answer_count": 1,
    "tags": "python;nlp;emoji;sentiment-analysis;emoticons"
  },
  {
    "question_id": 72037888,
    "title": "R: Correct Way to Calculate Cosine Similarity?",
    "body": "<p>I am working with the R programming language.</p>\n<p>I have the following data:</p>\n<pre><code>text = structure(list(id = 1:8, reviews = c(&quot;I guess the employee decided to buy their lunch with my card my card hoping I wouldn't notice but since it took so long to run my car I want to head and check my bank account and sure enough they had bought food on my card that I did not receive leave. Had to demand for and for a refund because they acted like it was my fault and told me the charges are still pending even though they are for 2 different amounts.&quot;, \n&quot;I went to McDonald's and they charge me 50 for Big Mac when I only came with 49. The casher told me that I can't read correctly and told me to get glasses. I am file a report on your casher and now I'm mad.&quot;, \n&quot;I really think that if you can buy breakfast anytime then I should be able to get a cheeseburger anytime especially since I really don't care for breakfast food. I really like McDonald's food but I preferred tree lunch rather than breakfast. Thank you thank you thank you.&quot;, \n&quot;I guess the employee decided to buy their lunch with my card my card hoping I wouldn't notice but since it took so long to run my car I want to head and check my bank account and sure enough they had bought food on my card that I did not receive leave. Had to demand for and for a refund because they acted like it was my fault and told me the charges are still pending even though they are for 2 different amounts.&quot;, \n&quot;Never order McDonald's from Uber or Skip or any delivery service for that matter, most particularly one on Elgin Street and Rideau Street, they never get the order right. Workers at either of these locations don't know how to follow simple instructions. Don't waste your money at these two locations.&quot;, \n&quot;Employees left me out in the snow and wouldn’t answer the drive through. They locked the doors and it was freezing. I asked the employee a simple question and they were so stupid they answered a completely different question. Dumb employees and bad food.&quot;, \n&quot;McDonalds food was always so good but ever since they add new/more crispy chicken sandwiches it has come out bad. At first I thought oh they must haven't had a good day but every time I go there now it's always soggy, and has no flavor. They need to fix this!!!&quot;, \n&quot;I just ordered the new crispy chicken sandwich and I'm very disappointed. Not only did it taste horrible, but it was more bun than chicken. Not at all like the commercial shows. I hate sweet pickles and there were two slices on my sandwich. I wish I could add a photo to show the huge bun and tiny chicken.&quot;\n)), class = &quot;data.frame&quot;, row.names = c(NA, -8L))\n</code></pre>\n<p>I would like to calculate a matrix of cosine similarities between each pair of elements:</p>\n<pre><code>library(lsa)\nlibrary(proxy)\nlibrary(tm)\n\ntext = text[,2]\n\ncorpus &lt;- VCorpus(VectorSource(text))\ntdm &lt;- TermDocumentMatrix(corpus, \n    control = list(wordLengths = c(1, Inf)))\noccurrence &lt;- apply(X = tdm, \n    MARGIN = 1, \n    FUN = function(x) sum(x &gt; 0) / ncol(tdm))\n\ntdm_mat &lt;- as.matrix(tdm[names(occurrence)[occurrence &gt;= 0.5], ])\n\nlsaSpace &lt;- lsa(tdm_mat)\n\n# lsaMatrix now is a k x (num doc) matrix, in k-dimensional LSA space\nlsaMatrix &lt;- diag(lsaSpace$sk) %*% t(lsaSpace$dk)\n\n# Use the `cosine` function in `lsa` package to get cosine similarities matrix\n# (subtract from 1 to get dissimilarity matrix)\ndistMatrix &lt;- 1 - cosine(lsaMatrix)\n</code></pre>\n<p>When looking at the resulting matrix:</p>\n<pre><code> distMatrix\n             1           2         3            4          5          6            7           8\n1 0.000000e+00 0.006362649 0.2616818 0.000000e+00 0.06794855 0.25138506 3.107289e-05 0.003658840\n2 6.362649e-03 0.000000000 0.1904180 6.362649e-03 0.11468650 0.33082042 5.505664e-03 0.019623883\n3 2.616818e-01 0.190417963 0.0000000 2.616818e-01 0.55622109 0.89444938 2.563879e-01 0.322025370\n4 0.000000e+00 0.006362649 0.2616818 0.000000e+00 0.06794855 0.25138506 3.107289e-05 0.003658840\n5 6.794855e-02 0.114686503 0.5562211 6.794855e-02 0.00000000 0.06202843 7.083380e-02 0.040392530\n6 2.513851e-01 0.330820421 0.8944494 2.513851e-01 0.06202843 0.00000000 2.566349e-01 0.197460291\n7 3.107289e-05 0.005505664 0.2563879 3.107289e-05 0.07083380 0.25663492 0.000000e+00 0.004363538\n8 3.658840e-03 0.019623883 0.3220254 3.658840e-03 0.04039253 0.19746029 4.363538e-03 0.000000000\n</code></pre>\n<p><strong>My Question:</strong> Have I calculated the cosine similarity correctly? Is there another way to do this?</p>\n<p>Thank you!</p>\n<p><strong>References:</strong></p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/15229584/compute-cosine-similarities-between-documents-in-semantic-space-using-r-lsa-pac\">Compute cosine similarities between documents in semantic space, using R-lsa package</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"nofollow noreferrer\">https://en.wikipedia.org/wiki/Cosine_similarity</a></li>\n</ul>\n",
    "score": 4,
    "creation_date": 1651118484,
    "view_count": 2608,
    "answer_count": 3,
    "tags": "r;text;nlp"
  },
  {
    "question_id": 69118249,
    "title": "Is there a maximum sequence length for the output of a transformer?",
    "body": "<p>There's just one thing that I can't find an answer to :\nWhen putting the ouput back in the transformer, we compute it similarly to the inputs (with added masks), so is there also a sequence size limit ?</p>\n<p>Even BERT has an input size limit of 512 tokens, so transformers are limited in how much they can take in.\nSo is there something to make the output length as big as wanted or is there a fixed max length ?</p>\n<p>If I wasn't clear enough, does the network generate words infinitely until the &lt; end &gt; token or is there a token limit for the outputs?</p>\n",
    "score": 4,
    "creation_date": 1631190784,
    "view_count": 5408,
    "answer_count": 3,
    "tags": "nlp;artificial-intelligence;transformer-model"
  },
  {
    "question_id": 68694759,
    "title": "Python Regex: How to select lines between two patterns",
    "body": "<p>Consider a typical live chat data as follows:</p>\n<pre><code>Peter (08:16): \nHi \nWhat's up? \n;-D\n\nAnji Juo (09:13): \nHey, I'm using WhatsApp!\n\nPeter (11:17):\nCould you please tell me where is the feedback?\n\nAnji Juo (19:13): \nI don't know where it is. \n\nAnji Juo (19:14): \nDo you by any chance know where I can catch a taxi ?\n🙏🙏🙏\n</code></pre>\n<p>To convert this raw text file to a DataFrame,  I need to write some regex to identify column names and then extract corresponding values.</p>\n<p>Please see <a href=\"https://regex101.com/r/X3ubqF/1\" rel=\"nofollow noreferrer\">https://regex101.com/r/X3ubqF/1</a></p>\n<pre><code>Index(time)     Name        Message\n08:16           Peter       Hi \n                            What's up? \n                            ;-D\n09:13           Anji Juo    Hey, I'm using WhatsApp!\n11:17           Peter       Could you please tell me where is the feedback?\n19:13           Anji Juo    I don't know where it is. \n19:14           Anji Juo    Do you by any chance know where I can catch a taxi ?\n                            🙏🙏🙏\n</code></pre>\n<p>The regex <code>r&quot;(?P&lt;Name&gt;.*?)\\s*\\((?P&lt;Index&gt;(?:\\d|[01]\\d|2[0-3]):[0-5]\\d)\\)&quot;</code> can extract the values of the time and name columns perfectly, but I have no idea how to highlight and extract messages from a specific sender for each time index.</p>\n",
    "score": 4,
    "creation_date": 1628357963,
    "view_count": 1128,
    "answer_count": 2,
    "tags": "python;regex;pandas;dataframe;nlp"
  },
  {
    "question_id": 67147261,
    "title": "How to Find Top N Similar Words in a Dictionary of Words / Things?",
    "body": "<p>I have a list of <code>str</code> that I want to map against. The words could be &quot;metal&quot; or &quot;st. patrick&quot;. The goal is to map a new string against this list and find Top N Similar items. For example, if I pass through &quot;St. Patrick&quot;, I want to capture &quot;st patrick&quot; or &quot;saint patrick&quot;.</p>\n<p>I know there's gensim and fastText, and I have an intuition that I should go for cosine similarity (or I'm all ears if there's other suggestions). I work primarily with time series, and gensim model training doesn't seem to like a list of words.</p>\n<p>What should I aim for next?</p>\n",
    "score": 4,
    "creation_date": 1618739773,
    "view_count": 1921,
    "answer_count": 1,
    "tags": "python;nlp;fasttext"
  },
  {
    "question_id": 67043468,
    "title": "UnparsedFlagAccessError: Trying to access flag --preserve_unused_tokens before flags were parsed. BERT",
    "body": "<p>I want to use Bert language model for training a multi class text classification task.\nPreviously I trained using LSTM without any Error but Bert gives me this Error.\nI get the following Error and I really don't know how to solve it, can anyone help me please?</p>\n<p>Unfortunately there is very little documentation using BERT in keras library.</p>\n<pre><code>!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n\nimport tensorflow_hub as hub\nfrom bert import tokenization\nmodule_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\n\n\n\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [&quot;[CLS]&quot;] + text + [&quot;[SEP]&quot;]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)\n    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;segment_ids&quot;)\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    net = tf.keras.layers.Dense(64, activation='softmax')(clf_output)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    net = tf.keras.layers.Dense(32, activation='softmax')(net)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    out = tf.keras.layers.Dense(3, activation='softmax')(net)\n    \n    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n\n\nmax_len = 150\ntrain_input = bert_encode(data.text_cleaned, tokenizer, max_len=max_len)\n\n</code></pre>\n<p>Error as following :</p>\n<pre><code>\nUnparsedFlagAccessError                   Traceback (most recent call last)\n&lt;ipython-input-175-fd64df42591d&gt; in &lt;module&gt;()\n      1 import sys\n      2 max_len = 150\n----&gt; 3 train_input = bert_encode(o.text_cleaned, tokenizer, max_len=max_len)\n\n4 frames\n/usr/local/lib/python3.7/dist-packages/absl/flags/_flagvalues.py in __getattr__(self, name)\n    496         # get too much noise.\n    497         logging.error(error_message)\n--&gt; 498       raise _exceptions.UnparsedFlagAccessError(error_message)\n    499 \n    500   def __setattr__(self, name, value):\n\nUnparsedFlagAccessError: Trying to access flag --preserve_unused_tokens before flags were parsed.\n\n</code></pre>\n",
    "score": 4,
    "creation_date": 1618133978,
    "view_count": 4573,
    "answer_count": 2,
    "tags": "python;nlp;bert-language-model"
  },
  {
    "question_id": 66596142,
    "title": "BertModel or BertForPreTraining",
    "body": "<p>I want to use Bert only for embedding and use the Bert output as an input for a classification net that I will build from scratch.</p>\n<p>I am not sure if I want to do finetuning for the model.</p>\n<p>I think the relevant classes are BertModel or BertForPreTraining.</p>\n<p><a href=\"https://dejanbatanjac.github.io/bert-word-predicting/\" rel=\"nofollow noreferrer\">BertForPreTraining</a>  head contains two &quot;actions&quot;:\nself.predictions is MLM (Masked Language Modeling) head is what gives BERT the power to fix the grammar errors, and self.seq_relationship is NSP (Next Sentence Prediction); usually refereed as the classification head.</p>\n<pre><code>class BertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n</code></pre>\n<p>I think the NSP isn't relevant for my task so I can &quot;override&quot; it.\nwhat does the MLM do and is it relevant for my goal or should I use the BertModel?</p>\n",
    "score": 4,
    "creation_date": 1615535592,
    "view_count": 5269,
    "answer_count": 1,
    "tags": "deep-learning;nlp;bert-language-model;huggingface-transformers;transformer-model"
  },
  {
    "question_id": 66518375,
    "title": "How is transformers loss calculated for blank token predictions?",
    "body": "<p>I'm currently trying to implement a transformer and have trouble understanding its loss calculation.</p>\n<p>My encoders input looks for batch_size=1  and max_sentence_length=8 like:</p>\n<pre><code>[[Das, Wetter, ist, gut, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]\n</code></pre>\n<p>My decoders input looks like (german to english):</p>\n<pre><code>[[&lt;start&gt;, The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;]]\n</code></pre>\n<p>Let's say my transformer predicted those class probabilities (only showing the word for the class with the highest class probability):</p>\n<pre><code>[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]\n</code></pre>\n<p>Now I calculate the loss using:</p>\n<pre><code>loss = categorical_crossentropy(\n   [[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]],\n   [[The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]\n)\n</code></pre>\n<p>Is this the correct way to calculate the loss? My transformer always predicts the blank token for the next word and I thought that's because I have a mistake in my loss calculation and have to do something with the blank tokens before calculating the loss.</p>\n",
    "score": 4,
    "creation_date": 1615132276,
    "view_count": 1971,
    "answer_count": 2,
    "tags": "machine-learning;nlp;transformer-model;language-model"
  },
  {
    "question_id": 66342359,
    "title": "nlp.update issue with Spacy 3.0: TypeError: [E978] The Language.update method takes a list of Example objects, but got: {&lt;class &#39;tuple&#39;&gt;}",
    "body": "<p>With Spacy version 3.0 there seem to be some changes with nlp.update.\nI am utterly confused with this simple code:</p>\n<pre><code>examples = TRAIN_DATA\nrandom.shuffle(examples)\nlosses = {}\n    \nfor batch in minibatch(examples, size=8):\n    nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)\n</code></pre>\n<p>When I do type(batch) it indicates that batch is of type list. But the error message says it is a tuple. I also tried to convert it to a list without success. What am I doing wrong?</p>\n<p>The exact error is:</p>\n<hr />\n<p>TypeError                                 Traceback (most recent call last)\n in \n22\n23         for batch in minibatch(examples, size=8):\n---&gt; 24             nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)\n25\n26         print(&quot;Losses ({}/{})&quot;.format(epoch + 1, epochs), losses)</p>\n<p>~/nlp_learn/statbot/.statbot/lib/python3.8/site-packages/spacy/language.py in update(self, examples, _, drop, sgd, losses, component_cfg, exclude)\n1090         if len(examples) == 0:\n1091             return losses\n-&gt; 1092         validate_examples(examples, &quot;Language.update&quot;)\n1093         examples = _copy_examples(examples)\n1094         if sgd is None:</p>\n<p>~/nlp_learn/statbot/.statbot/lib/python3.8/site-packages/spacy/training/example.pyx in spacy.training.example.validate_examples()</p>\n<p>TypeError: [E978] The Language.update method takes a list of Example objects, but got: {&lt;class 'tuple'&gt;}</p>\n<p>Here the first line of TRAIN_DATA as an example:\n('Auf Bauer Lehmanns Hof wird an beiden Pfingsttagen Brot im Backofen gebacken.',\n{'entities': [(10, 18, 'PER')]})</p>\n",
    "score": 4,
    "creation_date": 1614121417,
    "view_count": 3859,
    "answer_count": 1,
    "tags": "python;nlp;spacy-3"
  },
  {
    "question_id": 65113929,
    "title": "How Sklearn Latent Dirichlet Allocation really Works?",
    "body": "<p>I have some texts and I'm using sklearn <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\" rel=\"nofollow noreferrer\">LatentDirichletAllocation</a> algorithm to extract the topics from the texts.</p>\n<p>I already have the texts converted into sequences using Keras and I'm doing this:</p>\n<pre><code>from sklearn.decomposition import LatentDirichletAllocation\n\nlda = LatentDirichletAllocation()\nX_topics = lda.fit_transform(X)\n</code></pre>\n<p><code>X</code>:</p>\n<pre><code>print(X)\n#  array([[0, 988, 233, 21, 42, 5436, ...],\n   [0, 43, 6526, 21, 566, 762, 12, ...]])\n</code></pre>\n<p><code>X_topics</code>:</p>\n<pre><code>print(X_topics)\n#  array([[1.24143852e-05, 1.23983890e-05, 1.24238815e-05, 2.08399432e-01,\n    7.91563331e-01],\n   [5.64976371e-01, 1.33304549e-05, 5.60003133e-03, 1.06638803e-01,\n    3.22771464e-01]])\n</code></pre>\n<p>My question is, what is exactly what's being returned from <code>fit_transform</code>, I know that should be the main topics detected from the texts but I cannot map those numbers to an index so I'm not able to see what those sequences means, I failed at searching for an explanation of what is actually happening, so any suggestion will be much appreciated.</p>\n",
    "score": 4,
    "creation_date": 1606933414,
    "view_count": 2490,
    "answer_count": 1,
    "tags": "python-3.x;scikit-learn;nlp;latent-semantic-analysis"
  },
  {
    "question_id": 65059959,
    "title": "gensim most_similar with positive and negative, how does it work?",
    "body": "<p>I was reading <a href=\"https://stackoverflow.com/a/54581599/7339624\">this answer</a> That says about Gensim <code>most_similar</code>:</p>\n<blockquote>\n<p>it performs vector arithmetic: adding the positive vectors,\nsubtracting the negative, then from that resulting position, listing\nthe known-vectors closest to that angle.</p>\n</blockquote>\n<p>But when I tested it, that is not the case. I trained a Word2Vec with Gensim <code>&quot;text8&quot;</code> dataset and tested these two:</p>\n<pre><code>model.most_similar(positive=['woman', 'king'], negative=['man'])\n\n&gt;&gt;&gt; [('queen', 0.7131118178367615), ('prince', 0.6359186768531799),...]\n</code></pre>\n<hr />\n<pre><code>model.wv.most_similar([model[&quot;king&quot;] + model[&quot;woman&quot;] - model[&quot;man&quot;]])\n\n&gt;&gt;&gt; [('king', 0.84305739402771), ('queen', 0.7326322793960571),...]\n</code></pre>\n<p>They are clearly not the same. even the queen score in the first is <code>0.713</code> and on the second <code>0.732</code> which are not the same.</p>\n<p><strong>So</strong> I ask the question again, How does Gensim <code>most_similar</code> work? why the result of the two above are different?</p>\n",
    "score": 4,
    "creation_date": 1606652178,
    "view_count": 3418,
    "answer_count": 1,
    "tags": "python;nlp;gensim;word2vec"
  },
  {
    "question_id": 61756189,
    "title": "How to use NLTK Regex patterns to annotate financial news with UP/DOWN indicator?",
    "body": "<p>I'm working on replicating an algorithm describe in this paper: <a href=\"https://arxiv.org/pdf/1811.11008.pdf\" rel=\"nofollow noreferrer\">https://arxiv.org/pdf/1811.11008.pdf</a></p>\n\n<p>On the last page it describes extracting a leaf defined in the grammar labelled 'NP JJ' using the following example: Operating profit margin was 8.3%, compared to 11.8% a year earlier.</p>\n\n<p>I'm expecting to see a leaf labelled 'NP JJ' but I'm not. I'm tearing my hair out as to why (relatively new to regular expressions.)  </p>\n\n<pre><code>def split_sentence(sentence_as_string):\n    ''' function to split sentence into list of words\n    '''\n    words = word_tokenize(sentence_as_string)\n\n    return words\n\ndef pos_tagging(sentence_as_list):\n\n    words = nltk.pos_tag(sentence_as_list)\n\n    return words\n\ndef get_regex(sentence, grammar):\n\n    sentence = pos_tagging(split_sentence(sentence));\n\n    cp = nltk.RegexpParser(grammar) \n\n    result = cp.parse(sentence) \n\n    return result\n\n\nexample_sentence = \"Operating profit margin was 8.3%, compared to 11.8% a year earlier.\"\n\ngrammar = \"\"\"JJ : {&lt; JJ.∗ &gt; ∗}\n            V B : {&lt; V B.∗ &gt;}\n            NP : {(&lt; NNS|NN &gt;)∗}\n            NP P : {&lt; NNP|NNP S &gt;}\n            RB : {&lt; RB.∗ &gt;}\n            CD : {&lt; CD &gt;}\n            NP JJ : : {&lt; NP|NP P &gt; +(&lt; (&gt;&lt; .∗ &gt; ∗ &lt;) &gt;) ∗ (&lt; IN &gt;&lt; DT &gt; ∗ &lt; RB &gt; ∗ &lt; JJ &gt; ∗ &lt; NP|NP P &gt;) ∗ &lt; RB &gt; ∗(&lt; V B &gt;&lt; JJ &gt;&lt; NP &gt;)∗ &lt; V B &gt; (&lt; DT &gt;&lt; CD &gt;&lt; NP &gt;) ∗ &lt; NP|NP P &gt; ∗ &lt; CD &gt; ∗ &lt; .∗ &gt; ∗ &lt; CD &gt; ∗| &lt; NP|NP P &gt;&lt; IN &gt;&lt; NP|NP P &gt;&lt; CD &gt;&lt; .∗ &gt; ∗ &lt;, &gt;&lt; V B &gt; &lt; IN &gt;&lt; NP|NP P &gt;&lt; CD &gt;}\"\"\"\n\ngrammar = grammar.replace('∗','*')\n\ntree = get_regex(example_sentence, grammar)\n\nprint(tree)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1589298983,
    "view_count": 255,
    "answer_count": 1,
    "tags": "python;regex;nlp;nltk"
  },
  {
    "question_id": 57370524,
    "title": "Meaning of &quot;drop&quot; in SpaCy custom NER model training?",
    "body": "<p>Below code is an example training loop for SpaCy's named entity recognition(<code>NER</code>). </p>\n\n<pre><code>for itn in range(100):\n    random.shuffle(train_data)\n    for raw_text, entity_offsets in train_data:\n        doc = nlp.make_doc(raw_text)\n        gold = GoldParse(doc, entities=entity_offsets)\n        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)\nnlp.to_disk(\"/model\")\n</code></pre>\n\n<p><code>drop</code>  as per <code>spacy</code> is the drop out rate. Can somebody explain the meaning of the same in detail?</p>\n",
    "score": 4,
    "creation_date": 1565074615,
    "view_count": 2295,
    "answer_count": 1,
    "tags": "python;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 56543866,
    "title": "Count the occurrences of words in a string row wise based on existing words in other columns",
    "body": "<p>I have a data frame that has rows of strings. I want to count the occurrence of words in the rows based on what words appear in the column. How can I achieve this with the code below? <strong><em>Can the below code be modified somehow to achieve this or can anyone suggest another piece of code that doesn't require loops</em></strong>? Thanks so much in advance!</p>\n\n<pre><code>df &lt;- data.frame(\n  words = c(\"I want want to compare each \",\n            \"column to the values in\",\n            \"If any word from the list any\",\n            \"replace the word in the respective the word want\"),\n  want= c(\"want\", \"want\", \"want\", \"want\"),\n  word= c(\"word\", \"word\", \"word\", \"word\"),\n  any= c(\"any\", \"any\", \"any\", \"any\"))\n\n#add 1 for match and 0 for no match\nfor (i in 2:ncol(df))\n{\n  for (j in 1:nrow(df))\n  {                 \n    df[j,i] &lt;- ifelse (grepl (df[j,i] , df$words[j]) %in% \"TRUE\", 1, 0)\n  }\n  print(i)\n}\n\n*'data.frame':  4 obs. of  4 variables:\n $ words: chr  \"I want want to compare each \" \"column to the values in \" \"If any word from the words any\" \"replace the word in the respective the word\"\n $ want : chr  \"want\" \"want\" \"want\" \"want\"\n $ word : chr  \"word\" \"word\" \"word\" \"word\"\n $ any  : chr  \"any\" \"any\" \"any\" \"any\"*\n</code></pre>\n\n<p><strong>The output should look like below:</strong></p>\n\n<pre><code>    words                                                 want word any\n1   I want want to compare each                            2    0   0\n2   column to the values in                                0    0   0\n3   If any word from the list any                          0    1   2\n4   replace the word in the respective the word want       1    2   0\n</code></pre>\n\n<p><strong>Current output with existing code looks like this:</strong> </p>\n\n<pre><code>    words                                                 want word any\n1   I want want to compare each                            1    0   0\n2   column to the values in                                0    0   0\n3   If any word from the list any                          0    1   1\n4   replace the word in the respective the word want       1    1   0\n</code></pre>\n",
    "score": 4,
    "creation_date": 1560256473,
    "view_count": 437,
    "answer_count": 2,
    "tags": "r;nlp;text-mining"
  },
  {
    "question_id": 56416641,
    "title": "separate texts into sentences NLTK vs spaCy",
    "body": "<p>I want to separate texts into sentences.</p>\n\n<p>looking in stack overflow I found:</p>\n\n<p>WITH NLTK</p>\n\n<pre><code>from nltk.tokenize import sent_tokenize\ntext=\"\"\"Hello Mr. Smith, how are you doing today? The weathe is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\ntokenized_text=sent_tokenize(text)\nprint(tokenized_text)\n</code></pre>\n\n<p>WITH SPACY</p>\n\n<pre><code>from spacy.lang.en import English # updated\n\nraw_text = 'Hello, world. Here are two sentences.'\nnlp = English()\nnlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\ndoc = nlp(raw_text)\nsentences = [sent.string.strip() for sent in doc.sents]\n</code></pre>\n\n<p>The question is what in the background for spacy having to do it differently with a so called create_pipe.\nSentences are important for training your own word embedings for NLP. There should be a reason why spaCy does not include directly out of the box a sentence tokenizer.</p>\n\n<p>Thanks.</p>\n\n<p>NOTE: Be aware that a simply .split(.) does not work, there are several decimal numbers in the text and other kind of tokens containing '.'</p>\n",
    "score": 4,
    "creation_date": 1559490936,
    "view_count": 7906,
    "answer_count": 2,
    "tags": "python;nlp;nltk;spacy;sentence"
  },
  {
    "question_id": 55766558,
    "title": "Recursion in nltk&#39;s RegexpParser",
    "body": "<p>Based on the <a href=\"http://www.nltk.org/book/ch07.html#code-cascaded-chunker\" rel=\"nofollow noreferrer\">grammar in the chapter 7 of the NLTK Book</a>:</p>\n\n<pre><code>grammar = r\"\"\"\n      NP: {&lt;DT|JJ|NN.*&gt;+} # ...\n\"\"\"\n</code></pre>\n\n<p>I want to expand <strong>NP</strong> (noun phrase) to include multiple <strong>NP</strong> joined by <strong>CC</strong> (coordinating conjunctions: <em>and</em>) or <strong>,</strong> (commas) to capture noun phrases like:</p>\n\n<ul>\n<li><em>The house and tree</em></li>\n<li><em>The apple, orange and mango</em></li>\n<li><em>Car, house, and plane</em></li>\n</ul>\n\n<p>I cannot get my modified grammar to capture those as a single <strong>NP</strong>:</p>\n\n<pre><code>import nltk\n\ngrammar = r\"\"\"\n  NP: {&lt;DT|JJ|NN.*&gt;+(&lt;CC|,&gt;+&lt;NP&gt;)?}\n\"\"\"\n\nsentence = 'The house and tree'\nchunkParser = nltk.RegexpParser(grammar)\nwords = nltk.word_tokenize(sentence)\ntagged = nltk.pos_tag(words)\nprint(chunkParser.parse(tagged))\n</code></pre>\n\n<p>Results in:</p>\n\n<pre><code>(S (NP The/DT house/NN) and/CC (NP tree/NN))\n</code></pre>\n\n<p>I've tried moving the <strong>NP</strong> to the beginning: <code>NP: {(&lt;NP&gt;&lt;CC|,&gt;+)?&lt;DT|JJ|NN.*&gt;+}</code> but I get the same result</p>\n\n<pre><code>(S (NP The/DT house/NN) and/CC (NP tree/NN))\n</code></pre>\n",
    "score": 4,
    "creation_date": 1555700349,
    "view_count": 838,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 50985619,
    "title": "How to read PDF files which are in asian languages (Chinese, Japanese, Thai, etc.) and store in a string in python",
    "body": "<p>I am using PyPDF2 to read PDF files in python. While it works well for languages in English and European languages (with alphabets in english), the library fails to read Asian languages like Japanese and Chinese. I tried <code>encode('utf-8')</code>, <code>decode('utf-8')</code> but nothing seems to work. It just prints a blank string on extraction of the text.</p>\n\n<p>I have tried other libraries like textract and PDFMiner but no success yet.</p>\n\n<p>When I copy the text from PDF and paste it on a notebook, the characters turn into some random format text (probably in a different encoding). </p>\n\n<pre><code>def convert_pdf_to_text(filename):\n    text = ''\n    pdf = PyPDF2.PdfFileReader(open(filename, \"rb\"))\n    if pdf.isEncrypted:\n        pdf.decrypt('')\n    for page in pdf.pages:\n        text = text + page.extractText()\n    return text\n</code></pre>\n\n<p>Can anyone point me in the right direction?</p>\n",
    "score": 4,
    "creation_date": 1529662090,
    "view_count": 9551,
    "answer_count": 1,
    "tags": "python;unicode;nlp;text-extraction;pdf-reader"
  },
  {
    "question_id": 50910287,
    "title": "loading of fasttext pre trained german word embedding&#39;s .vec file throwing out of memory error",
    "body": "<p>I am using gensim to load the fasttext's pre-trained word embedding</p>\n\n<p><code>de_model = KeyedVectors.load_word2vec_format('wiki.de\\wiki.de.vec')</code></p>\n\n<p>But this gives me a memory error.</p>\n\n<p>Is there any way I can load it?</p>\n",
    "score": 4,
    "creation_date": 1529327337,
    "view_count": 2774,
    "answer_count": 1,
    "tags": "nlp;gensim;word-embedding;fasttext"
  },
  {
    "question_id": 48003907,
    "title": "How to train Naive Bayes Classifier for n-gram (movie_reviews)",
    "body": "<p>Below is the code of training <code>Naive Bayes Classifier</code> on <code>movie_reviews</code> dataset for <code>unigram</code> model. I want to train and analyze its performance by considering <code>bigram</code>, <code>trigram</code> model. How can we do it.</p>\n\n<pre><code>import nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import movie_reviews\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndef create_word_features(words):\n    useful_words = [word for word in words if word not in stopwords.words(\"english\")] \n    my_dict = dict([(word, True) for word in useful_words])\n    return my_dict\n\npos_data = []\nfor fileid in movie_reviews.fileids('pos'):\n    words = movie_reviews.words(fileid)\n    pos_data.append((create_word_features(words), \"positive\"))    \n\nneg_data = []\nfor fileid in movie_reviews.fileids('neg'):\n    words = movie_reviews.words(fileid)\n    neg_data.append((create_word_features(words), \"negative\")) \n\ntrain_set = pos_data[:800] + neg_data[:800]\ntest_set =  pos_data[800:] + neg_data[800:]\n\nclassifier = NaiveBayesClassifier.train(train_set)\n\naccuracy = nltk.classify.util.accuracy(classifier, test_set)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1514448623,
    "view_count": 6303,
    "answer_count": 2,
    "tags": "python;nlp;classification;nltk"
  },
  {
    "question_id": 47239639,
    "title": "Difference between fine-grained and coarse-grained score for WSD tasks?",
    "body": "<p>In all Senseval and SemEval Tasks, two scores are reported - fine-grained and coarse-grained. What do they mean in the context of sense disambiguation?</p>\n",
    "score": 4,
    "creation_date": 1510414605,
    "view_count": 1161,
    "answer_count": 1,
    "tags": "machine-learning;nlp;word-sense-disambiguation"
  },
  {
    "question_id": 45886128,
    "title": "Unable to set up my own Stanford CoreNLP server with error &quot;Could not delete shutdown key file&quot;",
    "body": "<p>I try to set up my own Stanford CoreNLP server following the <a href=\"https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\" rel=\"nofollow noreferrer\">official guide</a>. However, I am not able to start the server using the following command:</p>\n\n<pre><code>java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n</code></pre>\n\n<p>I paste the error messages below:</p>\n\n<pre><code>my_server_name$ java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---\n[main] INFO CoreNLP - setting default constituency parser\n[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz\n[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead\n[main] INFO CoreNLP - to use shift reduce parser download English models jar from:\n[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html\nException in thread \"main\" java.lang.IllegalStateException: Could not delete shutdown key file\nat edu.stanford.nlp.pipeline.StanfordCoreNLPServer.&lt;init&gt;(StanfordCoreNLPServer.java:195)\nat edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1323)\n[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.\n</code></pre>\n\n<p>The main problem is the IllegalSstateException: Could not delete shutdown key file. I just wonder whether the cause of this problem is the sudo access. The official guide doesn't explicitly state this command needs sudo access. </p>\n\n<p>I want to ask 1) whether the above command requires the sudo access and 2) if that command doesn't need sudo access, what could be the potential error for my IllegalSstateException. </p>\n\n<p>Thanks.</p>\n\n<p>PS: I am running on a server with Ubuntu 16.04.3 LTS. </p>\n",
    "score": 4,
    "creation_date": 1503679745,
    "view_count": 2300,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 45340148,
    "title": "python memory usage: txt file much smaller than python list containing file text",
    "body": "<p>I have a 543 MB txt file containing a single line of space separated, utf-8 tokens:</p>\n\n<pre><code>aaa algeria americansamoa appliedethics accessiblecomputing ada anarchism ...\n</code></pre>\n\n<p>But, when I load this text data into a python list, it uses ~8 GB of memory (~900 MB for the list and ~8 GB for the tokens):</p>\n\n<pre><code>with open('tokens.txt', 'r') as f:\n    tokens = f.read().decode('utf-8').split()\n\nimport sys\n\nprint sys.getsizeof(tokens)\n# 917450944 bytes for the list\nprint sum(sys.getsizeof(t) for t in tokens)\n# 7067732908 bytes for the actual tokens\n</code></pre>\n\n<p>I expected the memory usage to be approximately file size + list overhead = 1.5 GB. Why do the tokens consume so much more memory when loaded into a list?</p>\n",
    "score": 4,
    "creation_date": 1501121132,
    "view_count": 537,
    "answer_count": 1,
    "tags": "python;python-2.7;memory;utf-8;nlp"
  },
  {
    "question_id": 43936198,
    "title": "How to find strings from one list based on list of indexes from other list with some conditions in Python?",
    "body": "<p>I'm new to python and continuously learning to build better codes in python. I have two lists; one with indexes stored in x variable, where the indexes in the x represents index of tuples in list named bb with string ('IN') and surrounded on both sides at least with one tuple containing 'NN'. </p>\n\n<p>What i'm trying to get from the below code is, From every index mentioned in x in bb, how many continuous strings starting with 'NN' are present on both sides of the string tuple in bb list. </p>\n\n<p>I tried the below code, but the code isn't efficient enough. Anybody please help me in making the code efficient.</p>\n\n<pre><code>     bb = [('The', 'RB'),\n     ('company', 'NN'),\n     ('whose', 'NNS'),\n     ('stock', 'IN'),\n     ('has', 'NNP'),\n     ('been', 'NNS'),\n     ('on', 'NNP'),\n     ('tear', 'VBJ'),\n     ('this', 'VB'),\n     ('week', 'NNS'),\n     ('already', 'NN'),\n     ('sells', 'IN'),\n     ('its', 'NNP'),\n     ('graphics', 'NNS'),\n     ('processing', 'VB'),\n     ('units', 'VBJ'),\n     ('biggest', 'NNS'),\n     ('cloud', 'NN'),\n     ('companies', 'IN'),\n     ('just', 'NNP'),\n     ('that', 'IN')]\n\ndef solvr(bb):\n    x = []\n    for i in range(len(bb)-1):\n        if bb[i][1] == 'IN':\n            if 'NN' in (bb[i-1][1]) and 'NN' in (bb[i+1][1]):\n                x.append(i)\n    #===============================        \n\n    for i in range(len(bb)-1):\n        if i in x:\n            k=[]\n            front = bb[i+1:]\n            v = 0-i\n            back = bb[:-v]\n    #======================\n\n    for i in back:\n        if 'NN' in i[1]:\n            k.append(i[0])\n            [[] for i in k] \n    #================================\n\n\n    for i, j in enumerate(front):\n        if front[i][1][:2] == 'NN':\n            k.append(front[i][0])\n        else:\n            break\n    return(k)\n\n&gt;&gt; solvr(bb)\n\noutput:\n\n['company',\n 'whose',\n 'has',\n 'been',\n 'on',\n 'week',\n 'already',\n 'its',\n 'graphics',\n 'biggest',\n 'cloud',\n 'just']\n</code></pre>\n\n<p>My expectation from code is to get each iteration result in new list with also 'IN' string included in every list.</p>\n\n<pre><code> [['company', 'whose', 'stock', 'has', 'been', 'on'],\n ['week', 'already', 'sells', 'its', 'graphics'],\n ['biggest', 'cloud', 'companies', 'just']]\n</code></pre>\n\n<p>Would be thankful if someone provides any changes to my code. </p>\n",
    "score": 4,
    "creation_date": 1494586901,
    "view_count": 171,
    "answer_count": 4,
    "tags": "python;python-3.x;nlp"
  },
  {
    "question_id": 42848438,
    "title": "How to use Stanford Open IE with nltk",
    "body": "<p>I am on an NLP project right now and I need to use Stanford Open information extraction tool with python (nltk if possible). I found <a href=\"https://github.com/philipperemy/Stanford-OpenIE-Python\" rel=\"nofollow noreferrer\">a python wrapper</a></p>\n\n<p>but it's poorly documented and does not give full functionality interface to Stanford Open IE. Any suggestion?</p>\n",
    "score": 4,
    "creation_date": 1489719511,
    "view_count": 4326,
    "answer_count": 2,
    "tags": "nlp;nltk;stanford-nlp"
  },
  {
    "question_id": 41699913,
    "title": "Apache OpenNLP: java.io.FileInputStream cannot be cast to opennlp.tools.util.InputStreamFactory",
    "body": "<p>I am trying to build a custom NER using Apache OpenNLP 1.7. From the documentation available <a href=\"https://opennlp.apache.org/documentation/1.7.0/manual/opennlp.html#tools.namefind.training.featuregen\" rel=\"nofollow noreferrer\">Here</a>, I have developed the following code</p>\n\n<pre><code>import java.io.BufferedOutputStream;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.nio.charset.Charset;\n\nimport opennlp.tools.namefind.NameFinderME;\nimport opennlp.tools.namefind.NameSample;\nimport opennlp.tools.namefind.NameSampleDataStream;\nimport opennlp.tools.namefind.TokenNameFinderFactory;\nimport opennlp.tools.namefind.TokenNameFinderModel;\nimport opennlp.tools.util.ObjectStream;\nimport opennlp.tools.util.PlainTextByLineStream;\nimport opennlp.tools.util.TrainingParameters;\n\npublic class PersonClassifierTrainer {\n\n        static String modelFile = \"/opt/NLP/data/en-ner-customperson.bin\";\n\n        public static void main(String[] args) throws IOException {\n\n            Charset charset = Charset.forName(\"UTF-8\");\n            **ObjectStream&lt;String&gt; lineStream = new PlainTextByLineStream(new FileInputStream(\"/opt/NLP/data/person.train\"), charset);**\n            ObjectStream&lt;NameSample&gt; sampleStream = new NameSampleDataStream(lineStream);\n\n            TokenNameFinderModel model;\n            TokenNameFinderFactory nameFinderFactory = null;\n\n            try {\n                model = NameFinderME.train(\"en\", \"person\", sampleStream, TrainingParameters.defaultParams(),\n                        nameFinderFactory);\n            } finally {\n                sampleStream.close();\n            }\n\n            BufferedOutputStream modelOut = null;\n\n            try {\n                modelOut = new BufferedOutputStream(new FileOutputStream(modelFile));\n                model.serialize(modelOut);\n            } finally {\n                if (modelOut != null)\n                    modelOut.close();\n            }\n        }\n    }\n</code></pre>\n\n<p>The code highlighted above, shows - <strong>'Cast argument 'file' to 'insputstreamfactory'</strong></p>\n\n<p>I am forced to cast this, because it shows error otherwise.</p>\n\n<p>Now when I run my code, I get the following error</p>\n\n<pre><code>java.io.FileInputStream cannot be cast to opennlp.tools.util.InputStreamFactory\n</code></pre>\n\n<p>Is there anything missing here?</p>\n\n<p>Edit 1: Person.train file has this data</p>\n\n<pre><code>&lt;START:person&gt; Hardik &lt;END&gt; is a software Professional.&lt;START:person&gt; Hardik works at company&lt;END&gt; and &lt;START:person&gt; is part of development team&lt;END&gt;. &lt;START:person&gt; Hardik&lt;END&gt; lives in New York\n&lt;START:person&gt; Hardik&lt;END&gt; loves R statistical software\n&lt;START:person&gt; Hardik&lt;END&gt; is a student at ISB\n&lt;START:person&gt; Hardik&lt;END&gt; loves nature\n</code></pre>\n\n<p>Edit2: I am now getting null pointer exception, any help?</p>\n",
    "score": 4,
    "creation_date": 1484663929,
    "view_count": 1710,
    "answer_count": 1,
    "tags": "java;nlp;opennlp"
  },
  {
    "question_id": 41412532,
    "title": "How should I understand the .transform method in python-sklearn?",
    "body": "<h1> Introduction </h1>\n\n<p>I am following a tutorial, and I'm fairly new to Python, as well as Machine Learning.  (So, apologies if I seem like a noobie... it's because I am).  The tutorial can be found here: <a href=\"http://radimrehurek.com/data_science_python/\" rel=\"nofollow noreferrer\">Data Science Python Tutorial</a></p>\n\n<h2> Lines of Code in Question </h2>\n\n<p>I see that a <code>bow_transformer</code> variable has been created early on:</p>\n\n<pre><code>bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\n</code></pre>\n\n<p>My understanding of 'Vectorizing' isn't really clear here... are we saying that each word per SMS message in the corpus get's it's own row?</p>\n\n<p>Then, this variable is transformed (I assume <code>transform</code> here is to create some sort of (x,y) representation of a word so that a machine can read + count their occurrences.:</p>\n\n<pre><code>bow4 = bow_transformer.transform(messages['message'])\nmessages_bow = bow_transformer.transform(messages['message'])\n</code></pre>\n\n<p>Up to this point, I'm generally confused... and I think I've rationalized everything right in my head (correct me if I've made some logical errors above, which will help aid my understanding of Machine Learning + Python <strong>tremendously.</strong></p>\n\n<h2> Now, the main Question </h2>\n\n<p>My confusion was magnified by the following block:</p>\n\n<pre><code>tfidf_transformer = TfidfTransformer().fit(messages_bow)\ntfidf4 = tfidf_transformer.transform(bow4)\n</code></pre>\n\n<h2> My Interpretation </h2>\n\n<p><code>messages_bow</code> is bow transformed (whatever that means), and then <code>messages_bow</code> is then fitted onto the <code>tfidfTransformer</code>, and this is assigned the <code>tfidf_transformer</code> variable.  This newly created <code>tfidf_transformer</code> variable is now doesn't look like a variable anymore, because the next line is creating a new variable (<code>tfidf4</code>) using the aforementioned procedure/object?</p>\n\n<h1> Conclusion </h1>\n\n<p>I hope you guys can understand my confusion- I didn't know how to search for my question, because I simply don't know what I don't know.  My question is screaming \"Noobie\" and I hope this doesn't deter anyone from taking my question seriously. </p>\n",
    "score": 4,
    "creation_date": 1483230064,
    "view_count": 2991,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp"
  },
  {
    "question_id": 40040453,
    "title": "Using Python to create a (random) sample of n words from text files",
    "body": "<p>For my PhD project I am evaluating all existing Named Entity Recogition Taggers for Dutch. In order to check the precision and recall for those taggers I want to manually annotate all Named Entities in a random sample from my corpus. That manually annotated sample will function as the 'gold standard' to which I will compare the results of the different taggers. </p>\n\n<p>My corpus consists of 170 Dutch novels. I am writing a Python script to generate a random sample of a specific amount of words for each novel (which I will use to annotate afterwards). All novels will be stored in the same directory. The following script is meant to generate for each novel in that directory a random sample of n-lines:</p>\n\n<pre><code>import random\nimport os\nimport glob\nimport sys\nimport errno\n\npath = '/Users/roelsmeets/Desktop/libris_corpus_clean/*.txt'\nfiles = glob.glob(path)  \n\nfor text in files:\n    try:\n        with open(text, 'rt', encoding='utf-8') as f:\n             # number of lines from txt file\n             random_sample_input = random.sample(f.readlines(),100) \n\n    except IOError as exc:\n    # Do not fail if a directory is found, just ignore it.\n        if exc.errno != errno.EISDIR: \n            raise \n\n\n# This block of code writes the result of the previous to a new file\nrandom_sample_output = open(\"randomsample\", \"w\", encoding='utf-8') \nrandom_sample_input = map(lambda x: x+\"\\n\", random_sample_input)\nrandom_sample_output.writelines(random_sample_input)\nrandom_sample_output.close()\n</code></pre>\n\n<p>There are two problems with this code:</p>\n\n<ol>\n<li><p>Currently, I have put two novels (.txt files) in the directory. But the code only outputs a random sample for one of each novels. </p></li>\n<li><p>Currently, the code samples a random amount of LINES from each .txt file, but I prefer to generate a random amount of WORDS for each .txt file. Ideally, I would like to generate a sample of, say, the first or last 100 words of each of the 170 .txt-files. In that case, the sample won't be random at all; but thus far, I couldn't find a way to create a sample without using the random library.</p></li>\n</ol>\n\n<p>Could anyone give a suggestion how to solve both problems? I am still new to Python and programming in general (I am a literary scholar), so I would be pleased to learn different approaches. Many thanks in advance!</p>\n",
    "score": 4,
    "creation_date": 1476439502,
    "view_count": 2727,
    "answer_count": 3,
    "tags": "python;text;random;nlp;named-entity-recognition"
  },
  {
    "question_id": 37273126,
    "title": "How to get Named Entity Extraction using GATE Annie in Java?",
    "body": "<p>I am newbie in <strong>GATE ANNIE</strong>. I tried <strong>GATE GUI interface</strong> and got experience to do task on it. I wanted to know how can I implement <strong>Named Entity Extraction</strong> in Java? </p>\n\n<p>I did R&amp;D but unable to find any tutorial regarding <strong>Named Entity Extraction</strong>. </p>\n\n<p>Is there any code available to find out <strong>Named Entity Extraction</strong> in <strong>GATE ANNIE</strong> in <strong>Java</strong>?</p>\n",
    "score": 4,
    "creation_date": 1463479760,
    "view_count": 2833,
    "answer_count": 2,
    "tags": "java;nlp;named-entity-recognition;gate"
  },
  {
    "question_id": 36901948,
    "title": "Does CKY really require CNF?",
    "body": "<p>I've read a number of places that the CYK/CKY algorithms require the grammar to be in Chomsky Normal Form (CNF), e.g.</p>\n\n<blockquote>\n  <p>The standard version of CYK operates only on context-free grammars\n  given in Chomsky normal form (CNF) ~<a href=\"https://en.wikipedia.org/wiki/CYK_algorithm\" rel=\"nofollow\">Wikipedia</a></p>\n</blockquote>\n\n<p>However, I've also seen a number of examples of CKY algorithms where the grammar was not in CNF.  A common example that Christopher Manning uses is \"fish people fish tanks\" (ref: <a href=\"http://nlp.stanford.edu/courses/lsa354/SLoSP-2007-4.ppt\" rel=\"nofollow\">PPT slide #19</a>) which contains unary rules:</p>\n\n<pre><code>S -&gt; NP VP [0.9]\nS -&gt; VP [0.1]\nVP -&gt; V NP [0.4]\nVp -&gt; V [0.6]\n...\n</code></pre>\n\n<p>I've also seen other examples demonstrating CKY that use three non-terminals in the RHS of the production (e.g. <code>VP -&gt; Verb NP NP</code> <a href=\"http://web.engr.illinois.edu/~juliahmr/cs598/Slides/Lecture3.pdf#page=43\" rel=\"nofollow\">reference</a>).  Why the discrepancy?</p>\n",
    "score": 4,
    "creation_date": 1461795543,
    "view_count": 1595,
    "answer_count": 1,
    "tags": "nlp;grammar;context-free-grammar;chomsky-normal-form;cyk"
  },
  {
    "question_id": 36425313,
    "title": "Proper way of eliminating letter repetitions from English words?",
    "body": "<p>As the title clearly describes, I wonder what is the right way to eliminate character repetitions in English that are commonly used in social media to exaggerate the feeling. Since I am developing a software solution to correct mistyped words, I need a global algorithm that can be applied to most majority of English words. So, I am asking experts to learn the right way to eliminate additional letters in English words without using learning-based approachs?</p>\n\n<p>ps. (1) I check programmatically if the word is valid or not using the <a href=\"https://wordnet.princeton.edu\" rel=\"nofollow\">WordNet 3.0 database</a>. So far so good except some examples such as the word <code>veery</code> which is defined as <code>tawny brown North American trush noted for its song</code> in WordNet 3.0. I interrupt letter elimination process when the word is found in WordNet. So are there any other knowledge bases that can be used instead of WordNet?</p>\n\n<p>ps. (2) Actually I asked this question at <a href=\"http://english.stackexchange.com\">English Language &amp; Usage community</a>. But they guided me to ask it here.</p>\n\n<p><strong>Some examples:</strong></p>\n\n<pre><code>haappyy --&gt; happy\namaaazzinng --&gt; amazing\nveeerry --&gt; very\n</code></pre>\n\n<p>As you see in the examples, the place of letter repetition various through the word.</p>\n",
    "score": 4,
    "creation_date": 1459856237,
    "view_count": 299,
    "answer_count": 2,
    "tags": "nlp;sentiment-analysis;linguistics"
  },
  {
    "question_id": 33195322,
    "title": "how to represent gazetteers or dictionaries as features in crf++?",
    "body": "<p>how to use gazetteers or dictionaries as features in <a href=\"https://taku910.github.io/crfpp/\" rel=\"nofollow\">CRF++</a>?</p>\n\n<p>To elaborate: suppose I want to do NER on person names, and I am having a gazetteer (or dictionary) containing commonly seen person names, I want to use this gazetteer as an input to crf++, how can I do that? </p>\n\n<p>I am using the conditional random field package crf++ to perform named entity recognition tasks.\nI know how to represent some commonly used features in crf++. For example, if we want to use Capitalization as a feature, we can add one separate column in the feature template of crf indicating if a word is capitalized or not.   </p>\n",
    "score": 4,
    "creation_date": 1445152521,
    "view_count": 1605,
    "answer_count": 1,
    "tags": "nlp;named-entity-recognition;crf;crf++"
  },
  {
    "question_id": 31507528,
    "title": "integrating Elasticsearch &amp; Stanford NLP without re-indexing",
    "body": "<p>We've been using Elasticsearch in the system. Although i used its analyzers and queries. I didn't do deep into its indexing. as of now, i don't know how far ES lets us work the Lucene (inverted-)indexes it has in its shards. </p>\n\n<p>We're now looking at a range of NLP features-- NER for one thing\nand Stanford NLP appealed. </p>\n\n<p>There's no plug-in to work these 2 packages together(?)</p>\n\n<p>I haven't had a deep look into Stanford NLP. however - as far as i saw, \nit's working \nit all on its own indexes. whichever object or type passed to it, \nStanford NLP is indexing it itself and going from there. </p>\n\n<p>This would make the system work 2 different indexes for the same set of documents-- \nthose of ES &amp; StanfordNLP, and this would be costly. </p>\n\n<p>Is there a way to get around this? </p>\n\n<p>One scenario i have is: make StanfordNLP work on Lucene segments-- the inverted indexes that ES already built. \nIn this case: </p>\n\n<p>1.) does StanfordNLP use Lucene indexes without re-indexing anything for itself? i don't know StanfordNLP's indexing structure-- or even how far it uses/doesn't use Lucene. </p>\n\n<p>2.) are there any restrictions on using the Lucene indexes in ES shards? would we hit a rock bottom in using these Lucene segments directly as is, bypassing ES in between? </p>\n\n<p>I'm trying to put things together-- all in the air for now. sorry for the naive Q. </p>\n\n<p>I'm aware of OpenNLP and its plug-in. i haven't checked - i'm guessing it wouldn't be \"double-indexing\" and using ES's indexes(?)\nHowever, it's StanfordNLP we're after. </p>\n\n<p>TIA.  </p>\n",
    "score": 4,
    "creation_date": 1437353586,
    "view_count": 2460,
    "answer_count": 2,
    "tags": "elasticsearch;lucene;nlp;stanford-nlp;opennlp"
  },
  {
    "question_id": 24756738,
    "title": "Build and run your Apache Stanbol instance - fails",
    "body": "<p>Following the instructions on:\n<a href=\"https://stanbol.apache.org/docs/trunk/tutorial.html\" rel=\"nofollow\">stanbol build and run stanbol instance</a></p>\n\n<p>We always get the following error while executing:</p>\n\n<pre><code>% mvn clean install\n</code></pre>\n\n<p>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test (default-test) on project org.apache.stanbol.commons.owl: There are test failures.</p>\n\n<p>We assume that it has something to do with this:</p>\n\n<p>SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See <a href=\"http://www.slf4j.org/codes.html#StaticLoggerBinder\" rel=\"nofollow\">http://www.slf4j.org/codes.html#StaticLoggerBinder</a>  for further details.</p>\n\n<p>The full log can be found: <a href=\"https://docs.google.com/file/d/0B4juZt9g6iPFMDh1cDNaMWlTeUk/edit\" rel=\"nofollow\">here</a></p>\n\n<p>As we do not know what exactly went wrong we hope someone can give us some hints to find a solution.</p>\n\n<p>If the tests are skipped with -DskipTests </p>\n\n<p>When then lauchning the server following error appears:</p>\n\n<pre><code>16.07.2014 12:30:10.136 *ERROR* [FelixStartLevel] ERROR: Error starting \n</code></pre>\n\n<p>Full server log can be accessed <a href=\"https://docs.google.com/file/d/0B4juZt9g6iPFVHNQeEtjWTZ6eFk/edit\" rel=\"nofollow\">here</a></p>\n\n<p>The result I get when using stanbol 0.12 <a href=\"https://docs.google.com/file/d/0B4juZt9g6iPFQjdwZE1mMmFhNzA/edit\" rel=\"nofollow\">here</a> is still an error but most of the tests succeed...the version on github has a similar error <a href=\"https://docs.google.com/file/d/0B4juZt9g6iPFWVE3aGs4cHVGb0k/edit\" rel=\"nofollow\">here</a></p>\n\n<p>And launching after skipping the tests throws this Errors showed <a href=\"https://docs.google.com/file/d/0B4juZt9g6iPFdmRhd0pJdnJ0aGs/edit\" rel=\"nofollow\">here</a></p>\n",
    "score": 4,
    "creation_date": 1405422812,
    "view_count": 1430,
    "answer_count": 5,
    "tags": "java;apache;maven;nlp;rdf"
  },
  {
    "question_id": 19574549,
    "title": "extracting relations from text",
    "body": "<p>I want to extract relations from unstructured text in the form of (SUBJECT,OBJECT,ACTION) relations,</p>\n\n<p>for instance,</p>\n\n<p>\"The boy is sitting on the table eating the chicken\"</p>\n\n<p>would give me, <br/> <br/>\n(boy,chicken,eat)  <br/>\n(boy,table,LOCATION)</p>\n\n<p>etc..</p>\n\n<p>although a python program + NLTK could process such a simple sentence as above.</p>\n\n<p>I'd like to know if any of you have used tools or libraries preferably opensource to extract relations from a much wider domain such as a large collection of text documents or the web.</p>\n",
    "score": 4,
    "creation_date": 1382641861,
    "view_count": 2316,
    "answer_count": 1,
    "tags": "python;nlp;data-mining;nltk"
  },
  {
    "question_id": 18999952,
    "title": "Part-of-speech tag without context using nltk",
    "body": "<p>Is there an easy way to determine the most likely part of speech tag for a given word <strong>without context</strong> using nltk. Or if not using any other tool / dataset.</p>\n\n<p>I tried to use wordnet, but it seems that the sysnets are not ordered by likelihood. </p>\n\n<pre><code>&gt;&gt;&gt; wn.synsets('says')\n\n[Synset('say.n.01'), Synset('state.v.01'), ...]\n</code></pre>\n",
    "score": 4,
    "creation_date": 1380097973,
    "view_count": 1337,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 16422399,
    "title": "How do I convert words to their equivalent number?",
    "body": "<p>If I have a positive integer written as a string, such as <code>\"three\"</code> or <code>\"forty nine\"</code>, is there a simple way to convert this to an integer?</p>\n\n<p>I'm happy using Linguistics to convert the other way, but I've never tried this way!</p>\n",
    "score": 4,
    "creation_date": 1367939061,
    "view_count": 223,
    "answer_count": 1,
    "tags": "ruby;nlp;linguistics"
  },
  {
    "question_id": 12249120,
    "title": "Build a dictionary from two sentenced-aligned texts?",
    "body": "<p>I have a text corpus which is already aligned at sentence level by construction - it is a list of pairs of English strings and their translation in another language. I have about 10 000 strings of 5 - 20 words each and their translations. My goal is to try to build a metric of the quality of the translation - automatically of course, because I'm dealing with languages I know nothing about :)</p>\n\n<p>I'd like to build a dictionary from this list of translations that would give me the (most probable) translation of each word in the source English strings into the other language. I know the dictionary will be far from perfect but I'm hoping I can have something good enough to flag when a word is not consistently translated, for example, if my dictionary says \"Store\" is to be tranlated into French by \"Magasin\" then if I spot some place where \"Store\" is translated as \"Boutique\" I can suspect that something is wrong.</p>\n\n<p>So I'd need to:</p>\n\n<ol>\n<li>build a dictionary from my corpus</li>\n<li>align the words inside the string/translation pairs</li>\n</ol>\n\n<p>Do you have good references on how to do this? Known algorithms? I found many links about text alignment but they seem to be more at the sentence level than at the word level...</p>\n\n<p>Any other suggestion on how to automatically check whether a translation is consistent would be greatly appreciated!</p>\n\n<p>Thanks in advance.</p>\n",
    "score": 4,
    "creation_date": 1346680017,
    "view_count": 1311,
    "answer_count": 3,
    "tags": "nlp;machine-translation"
  },
  {
    "question_id": 6783664,
    "title": "getting the lemma of a word using wordnet",
    "body": "<p>How can I get the lemma for a given word using Wordnet. I couldn't seem to find in the wordnet documentation what i want.  <a href=\"http://wordnet.princeton.edu/wordnet/man/wn.1WN.html\" rel=\"nofollow\">http://wordnet.princeton.edu/wordnet/man/wn.1WN.html</a></p>\n\n<p>For example for the word \"books\" i want to get \"book\" , ashes => ash , booking => book, apples => apple .... etc. </p>\n\n<p>i want to achieve this using wordnet in command line and I cant find exact options to retrieve such case. </p>\n\n<p>A php solution would also be of great help because I originally intend to use the wordnet php API but it seems the current one in their website isn't working. </p>\n",
    "score": 4,
    "creation_date": 1311287990,
    "view_count": 4291,
    "answer_count": 4,
    "tags": "php;nlp;wordnet;lemmatization;morphological-analysis"
  },
  {
    "question_id": 6418785,
    "title": "Scraping English Words using Python",
    "body": "<p>I would like to scrape all English words from, say, New York Times front page. I wrote something like this in Python:</p>\n\n<pre><code>import re\nfrom urllib import FancyURLopener\n\nclass MyOpener(FancyURLopener):\n    version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'            \n\nopener = MyOpener()\nurl = \"http://www.nytimes.com\"\nh = opener.open(url)\ncontent = h.read()\ntokens = re.findall(\"\\s*(\\w*)\\s*\", content, re.UNICODE) \nprint tokens\n</code></pre>\n\n<p>This works okay, but I get HTML keywords such as \"img\", \"src\" as well as English words. Is there a simple way to get only English words from Web scaping / HTML ? </p>\n\n<p>I saw <a href=\"https://stackoverflow.com/questions/5635400/scraping-with-python\">this</a> post, it only seems to talk about the mechanics of scraping, none of the tools mentioned talk about how to filter out non-language elements. I am not interested in links, formatting, etc. Just plain words. Any help would be appreciated. </p>\n",
    "score": 4,
    "creation_date": 1308613194,
    "view_count": 1858,
    "answer_count": 5,
    "tags": "python;nlp;urllib2;web-scraping;urllib"
  },
  {
    "question_id": 6381825,
    "title": "What&#39;s the best way to detect garbled text in an OCR-ed document",
    "body": "<p>Are there any good NLP or statistical techniques for detecting garbled characters in OCR-ed text? Off the top of my head I was thinking that looking at the distribution of n-grams in text might be a good starting point but I'm pretty new to the whole NLP domain.</p>\n\n<p>Here is what I've looked at so far:</p>\n\n<ul>\n<li><a href=\"http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/33035.pdf\" rel=\"nofollow\">N-gram Statistics in English and Chinese: Similarities and Differences</a></li>\n<li><a href=\"http://www.data-compression.com/english.html\" rel=\"nofollow\">Statistical Distributions of English Text</a></li>\n</ul>\n\n<p>The text will mostly be in english but a general solution would be nice. The text is currently indexed in Lucene so any ideas on a term based approach would be useful too.</p>\n\n<p><br/></p>\n\n<p>Any suggestions would be great! Thanks!</p>\n",
    "score": 4,
    "creation_date": 1308289681,
    "view_count": 1038,
    "answer_count": 2,
    "tags": "text;statistics;nlp;ocr"
  },
  {
    "question_id": 5485072,
    "title": "Book translation data format",
    "body": "<p>I'm thinking of translating a book from English to my native language. I can translate just fine, and I'm happy with <code>vim</code> as a text editor. My problem is that I'd like to somehow preserve the semantics, i.e. which parts of my translation correspond to the original.</p>\n\n<p>I could basically create a simple XML-based markup language, that'd look something like</p>\n\n<pre><code>&lt;book&gt;\n  &lt;chapter&gt;\n    &lt;paragraph&gt;\n      &lt;sentence&gt;\n        &lt;original&gt;This is an example sentence.&lt;/original&gt;\n        &lt;translation lang=\"fi\"&gt;Tämä on esimerkkilause.&lt;/translation&gt;\n      &lt;/sentence&gt;\n    &lt;/paragraph&gt;\n  &lt;/chapter&gt;\n&lt;/book&gt;\n</code></pre>\n\n<p>Now, that would probably have its benefits but I don't think editing that would be very fun.</p>\n\n<p>Another possibility that I can think of would be to keep the original and translation in separate files. If I add a newline after each translation chunk and keep line numbering consistent, editing would be easy and I'd be able to programmatically match the original and translation.</p>\n\n<pre><code>original.txt:\n  This is an example sentence.\n  In this format editing is easy.\n\ntranslation-fi.txt:\n  Tämä on esimerkkilause.\n  Tässä muodossa muokkaaminen on helppoa.\n</code></pre>\n\n<p>However, this doesn't seem very robust. It would be easy to mess up. Probably someone has better ideas. Thus the question:</p>\n\n<p><strong>What would be the best data format for making a book translation with a text editor?</strong></p>\n\n<p>EDIT: added tag <code>vim</code>, since I'd prefer to do this with vim and believe that some vim guru might have ideas.</p>\n\n<p>EDIT2: started a bounty on this. I'm currently leaning to the second idea I describe, but I hope to get something about as easy to edit (and quite easy to implement) but more robust.</p>\n",
    "score": 4,
    "creation_date": 1301480713,
    "view_count": 329,
    "answer_count": 3,
    "tags": "vim;nlp;translation;file-format"
  },
  {
    "question_id": 5235537,
    "title": "What is the easiest way to compare two web pages using python?",
    "body": "<p>Hello I want to Compare two webpages using python script.\nhow can i achieve it? thanks in advance!</p>\n",
    "score": 4,
    "creation_date": 1299602873,
    "view_count": 9156,
    "answer_count": 2,
    "tags": "python;comparison;nlp"
  },
  {
    "question_id": 4454029,
    "title": "Is NER necessary for Coreference resolution?",
    "body": "<p>... or is gender information enough?\nMore specifically, I'm interested in knowing if I can reduce the number of models loaded by the Stanford Core NLP to extract coreferences. I am not interested in actual named entity recognition.</p>\n\n<p>Thank you</p>\n",
    "score": 4,
    "creation_date": 1292441588,
    "view_count": 1510,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp;named-entity-recognition"
  },
  {
    "question_id": 2509631,
    "title": "Hierarchy of meaning",
    "body": "<p>I am looking for a method to build a hierarchy of words. </p>\n\n<p>Background: I am a \"amateur\" natural language processing enthusiast and right now one of the problems that I am interested in is determining the hierarchy of word semantics from a group of words.</p>\n\n<p>For example, if I have the set which contains a \"super\" representation of others, i.e.</p>\n\n<pre><code>[cat, dog, monkey, animal, bird, ... ]\n</code></pre>\n\n<p>I am interested to use any technique which would allow me to extract the word 'animal' which has the most meaningful and accurate representation of the other words inside this set.</p>\n\n<p>Note: they are NOT the same in meaning. cat != dog != monkey != animal\nBUT cat is a subset of animal and dog is a subset of animal.</p>\n\n<p>I know by now a lot of you will be telling me to use wordnet. Well, I will try to but I am actually interested in doing a very domain specific area which WordNet doesn't apply because:\n1) Most words are not found in Wordnet\n2) All the words are in another language; translation is possible but is to limited effect.</p>\n\n<p>another example would be:</p>\n\n<pre><code>[ noise reduction, focal length, flash, functionality, .. ]\n</code></pre>\n\n<p>so functionality includes everything in this set. </p>\n\n<p>I have also tried crawling wikipedia pages and applying some techniques on td-idf etc but wikipedia pages doesn't really do much either.</p>\n\n<p>Can someone possibly enlighten me as to what direction my research should go towards? (I could use anything)</p>\n",
    "score": 4,
    "creation_date": 1269449367,
    "view_count": 622,
    "answer_count": 2,
    "tags": "machine-learning;nlp;wordnet"
  },
  {
    "question_id": 767444,
    "title": "TDD and the Bayesian Spam Filter problem",
    "body": "<p>It's well known that Bayesian classifiers are an effective way to filter spam. These can be fairly concise (our one is only a few hundred LoC) but all core code needs to be written up-front before you get any results at all.</p>\n\n<p>However, the TDD approach mandates that only the minimum amount of code to pass a test can be written, so given the following method signature:</p>\n\n<pre><code>bool IsSpam(string text)\n</code></pre>\n\n<p>And the following string of text, which is clearly spam:</p>\n\n<pre><code>\"Cheap generic viagra\"\n</code></pre>\n\n<p>The minimum amount of code I could write is:</p>\n\n<pre><code>bool IsSpam(string text)\n{\n    return text == \"Cheap generic viagra\"\n}\n</code></pre>\n\n<p>Now maybe I add another test message, e.g.</p>\n\n<pre><code>\"Online viagra pharmacy\"\n</code></pre>\n\n<p>I could change the code to:</p>\n\n<pre><code>bool IsSpam(string text)\n{\n    return text.Contains(\"viagra\");\n}\n</code></pre>\n\n<p>...and so on, and so on. Until at some point the code becomes a mess of string checks, regular expressions, etc. because we've <em>evolved</em> it instead of thinking about it and writing it in a different way from the start.</p>\n\n<p>So how is TDD supposed to work with this type of situation where evolving the code from the simplest possible code to pass the test is not the right approach? (Particularly if it is known in advance that the best implementations cannot be trivially evolved).</p>\n",
    "score": 4,
    "creation_date": 1240217117,
    "view_count": 730,
    "answer_count": 9,
    "tags": "tdd;machine-learning;nlp;classification"
  },
  {
    "question_id": 73365305,
    "title": "AttributeError: module &#39;dill._dill&#39; has no attribute &#39;stack&#39;",
    "body": "<p>Use this colab\n<a href=\"https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing</a></p>\n<p>my CUDA Version: 11.2</p>\n<p>when do this</p>\n<pre><code>train_dataset = train_dataset.map(\n    process_data_to_model_inputs,\n    batched=True,\n    batch_size=batch_size,\n    remove_columns=[&quot;article&quot;, &quot;abstract&quot;, &quot;section_names&quot;],\n)\n</code></pre>\n<blockquote>\n<p><strong>AttributeError: module 'dill._dill' has no attribute 'stack'</strong></p>\n</blockquote>\n<p>I have try this</p>\n<pre><code>pip install dill==0.3.4\n</code></pre>\n<p>but not work</p>\n<p>How to solve this problems, Thinks!</p>\n",
    "score": 4,
    "creation_date": 1660589743,
    "view_count": 2356,
    "answer_count": 1,
    "tags": "nlp;dill"
  },
  {
    "question_id": 73315383,
    "title": "In spacy: Add a span (doc[a:b]) as entity in a spacy doc (python)",
    "body": "<p>I am using regex over a whole document to catch the spans in which such regex occurs:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nimport re\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\ndoc = nlp(&quot;The United States of America (USA) are commonly known as the United States (U.S. or US) or America.&quot;)\n\nexpression = r&quot;[Uu](nited|\\.?) ?[Ss](tates|\\.?)&quot;\nfor match in re.finditer(expression, doc.text):\n    start, end = match.span()\n    span = doc.char_span(start, end)\n    # This is a Span object or None \n    # if match doesn't map to valid token sequence\n    if span is not None:\n        print(&quot;Found match:&quot;, span.text)\n</code></pre>\n<p>There is a way to get the span (list of tokens) corresponding to the regex match on the doc even if the boundaries of the regex match do not correspond to token boundaries.\nSee:\nHow can I expand the match to a valid token sequence? In <a href=\"https://spacy.io/usage/rule-based-matching\" rel=\"nofollow noreferrer\">https://spacy.io/usage/rule-based-matching</a></p>\n<p>So far so good.</p>\n<p>Now that I have a collectuon of spans how do I convert them into entities?\nI am aware of the entity ruler:\nThe EntityRuler is a pipeline component (see also the link above) but that entityruler takes patterns as inputs to search in the doc and not spans.</p>\n<p>If I want to use regex over the whole document to get the collection os spans I want to convert into ents what is the next step here? Entityruler? How? Or something else?</p>\n<p>Put simpler:</p>\n<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)\ndoc = nlp(&quot;The aplicable law is article 102 section b sentence 6 that deals with robery&quot;)\n</code></pre>\n<p>I would like to generate an spacy ent (entity) out of doc[5,10] with label &quot;law&quot; in order to be able to:\nA) loop over all the law entities in the texts\nB) use the visualizer to display the different entities contained in the doc</p>\n",
    "score": 4,
    "creation_date": 1660193147,
    "view_count": 1953,
    "answer_count": 1,
    "tags": "python;nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 73132769,
    "title": "What is the right way to get unit vector to index Elasticsearch ann dot_product?",
    "body": "<p>I am trying to index word embedding vectors to Elasticsearch V8 ann <code>dense_vector</code> <code>dot_product</code>.</p>\n<p>I can successfully index <code>vec</code> to <code>cosine</code>, so I converted it to unit vector with numpy for <code>dot_product</code>.</p>\n<pre><code>    unit_vector = vec / np.linalg.norm(vec)\n</code></pre>\n<p>but I get an 400 error saying like this.</p>\n<pre><code>The [dot_product] similarity can only be used with unit-length vectors. Preview of invalid vector: [-0.0038341882, -0.1564709, 0.08771773, -0.14555556, -0.07952896, ...]\n</code></pre>\n<p>Am I missing something?</p>\n",
    "score": 4,
    "creation_date": 1658900641,
    "view_count": 1076,
    "answer_count": 1,
    "tags": "python;elasticsearch;nlp"
  },
  {
    "question_id": 72673637,
    "title": "The decoder part in a transformer model",
    "body": "<p>I'm fairly new to NLP and I was reading a blog explaining the transformer model. I was quite confused about the input/output for the decoder block (attached below). I get that y_true is fed into the decoder during the training step to combine with the output of the encoder block. What I don't get is, if we already know y_true, why run this step to get the output probability? I just don't quite get the relationship between the bottom right &quot;Output Embedding&quot; and the top right &quot;Output Probabilities&quot;. When we use the model, we wouldn't really have y_true, do we just use y_pred and feed them into the decoder instead? This might be a noob question. Thanks in advance.</p>\n<p><a href=\"https://i.sstatic.net/nQ2f5.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/nQ2f5.png\" alt=\"The Decoder Block of the Transformer Architecture\nTaken from “Attention Is All You Need“\" /></a></p>\n",
    "score": 4,
    "creation_date": 1655599754,
    "view_count": 6754,
    "answer_count": 2,
    "tags": "nlp;transformer-model;decoder"
  },
  {
    "question_id": 65575990,
    "title": "Why FastText is not handling finding multi-word phrases?",
    "body": "<p>FastText pre-trained model works great for finding similar words:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from pyfasttext import FastText\nmodel = FastText('cc.en.300.bin')\nmodel.nearest_neighbors('dog', k=2000)\n\n[('dogs', 0.8463464975357056),\n ('puppy', 0.7873005270957947),\n ('pup', 0.7692237496376038),\n ('canine', 0.7435278296470642),\n ...\n</code></pre>\n<p>However, it seems to fail for multi-word phrases, e.g.:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model.nearest_neighbors('Gone with the Wind', k=2000)\n\n[('DEky4M0BSpUOTPnSpkuL5I0GTSnRI4jMepcaFAoxIoFnX5kmJQk1aYvr2odGBAAIfkECQoABAAsCQAAABAAEgAACGcAARAYSLCgQQEABBokkFAhAQEQHQ4EMKCiQogRCVKsOOAiRocbLQ7EmJEhR4cfEWoUOTFhRIUNE44kGZOjSIQfG9rsyDCnzp0AaMYMyfNjS6JFZWpEKlDiUqALJ0KNatKmU4NDBwYEACH5BAUKAAQALAkAAAAQABIAAAhpAAEQGEiQIICDBAUgLEgAwICHAgkImBhxoMOHAyJOpGgQY8aBGxV2hJgwZMWLFTcCUIjwoEuLBym69PgxJMuDNAUqVDkz50qZLi',\n  0.71047443151474),\n</code></pre>\n<p>or</p>\n<pre class=\"lang-py prettyprint-override\"><code>model.nearest_neighbors('Star Wars', k=2000)\n[('clockHauser', 0.5432934761047363),\n ('CrônicasEsdrasNeemiasEsterJóSalmosProvérbiosEclesiastesCânticosIsaíasJeremiasLamentaçõesEzequielDanielOséiasJoelAmósObadiasJonasMiquéiasNaumHabacuqueSofoniasAgeuZacariasMalaquiasNovo',\n  0.5197194218635559),\n</code></pre>\n<p>Is it a limitation of FastText pre-trained models?</p>\n",
    "score": 4,
    "creation_date": 1609837996,
    "view_count": 1082,
    "answer_count": 2,
    "tags": "nlp;fasttext"
  },
  {
    "question_id": 64495524,
    "title": "How to properly extract entities like facilities and establishment from text using NLP and Entity recognition?",
    "body": "<p>I need to identify all the <code>establishments</code> and <code>facilities</code> from a given text using natural language processing and NER.</p>\n<p><strong>Example text:</strong></p>\n<blockquote>\n<p>The government panned to build new parks, swimming pool and commercial complex for out town and improve existing housing complex, schools and townhouse.</p>\n</blockquote>\n<p><strong>Expected entities to be identified:</strong></p>\n<blockquote>\n<p>parks, swimming pool, commercial complex, housing complex, school and townhouse</p>\n</blockquote>\n<p>I did explore some python libraries like  Spacy and NLTK but results were not great only 2 entities were identified. I reckon the data needs to be pre-processed properly.</p>\n<p>What should I do to improve the results ? Is there any other libraries/framework that is better for this use case ? Is there any way to train our model using the existing db ?</p>\n",
    "score": 4,
    "creation_date": 1603437199,
    "view_count": 2658,
    "answer_count": 3,
    "tags": "python;nlp;stanford-nlp;spacy;named-entity-recognition"
  },
  {
    "question_id": 64312421,
    "title": "OpenAI API and GPT-3, not clear how can I access or set up a learning/dev?",
    "body": "<p>I am reading tons of GPT-3 samples, and came cross many code samples.\nNone of them mentions that how and where I can run and play with the code myself... and especially not mentioning I can not.</p>\n<p>So I did my research, and concluded, I can not, but I may be wrong:</p>\n<ul>\n<li>There is no way to run the &quot;thing&quot; on-premises on a dev machine, it is a hosted service by definition (?)</li>\n<li>As of now (Oct. 11th 2020) the OpenAI API is in invite only beta (?)</li>\n</ul>\n<p>Did I miss something?</p>\n",
    "score": 4,
    "creation_date": 1602482370,
    "view_count": 2501,
    "answer_count": 2,
    "tags": "nlp;openai-api;gpt-3"
  },
  {
    "question_id": 64112225,
    "title": "Tokenize Sentences or Tweets with Emoji Skin Tone Modifiers",
    "body": "<p>I want to tokenize a tweet containing multiple emojis and they are not space-separated. I tried both <code>NLTK TweetTokenizer</code> and <code>Spacy</code> but they fail to tokenize Emoji Skin Tone Modifiers. This needs to be applied to a huge dataset so performance might be an issue. Any suggestions?</p>\n<p><strong>You may need to use Firefox or Safari to see the exact color tone emoji because Chrome sometimes fails to render it!</strong></p>\n<pre><code># NLTK\nfrom nltk.tokenize.casual import TweetTokenizer\nsentence = &quot;I'm the most famous emoji 😂😂😂 but what about 👍 and 🚗👍🏼😂👍🏿&quot;\nt = TweetTokenizer()\nprint(t.tokenize(sentence))\n\n# Output\n[&quot;I'm&quot;, 'the', 'most', 'famous', 'emoji', '😂', '😂', '😂', 'but', 'what', 'about', '👍', 'and', '🚗', '👍', '🏼', '😂', '👍', '🏿']\n</code></pre>\n<p>And</p>\n<pre><code># Spacy\nimport spacy\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\nsentence = nlp(&quot;I'm the most famous emoji 😂😂😂 but what about 👍 and 🚗👍🏼😂👍🏿&quot;)\nprint([token.text for token in sentence])\n\nOutput\n['I', &quot;'m&quot;, 'the', 'most', 'famous', 'emoji', '😂', '😂', '😂', 'but', 'what', 'about', '👍', 'and', '🚗', '👍', '🏼', '😂', '👍', '🏿']\n</code></pre>\n<p>Expected Output</p>\n<pre><code>[&quot;I'm&quot;, 'the', 'most', 'famous', 'emoji', '😂', '😂', '😂', 'but', 'what', 'about', '👍', 'and', '🚗', '👍🏼', '😂', '👍🏿']\n</code></pre>\n",
    "score": 4,
    "creation_date": 1601348289,
    "view_count": 705,
    "answer_count": 2,
    "tags": "python-3.x;nlp;nltk;spacy;emoji-tones"
  },
  {
    "question_id": 62776477,
    "title": "How to extract sentences with key phrases in spaCy",
    "body": "<p>I have worked with Spacy and so far, found very intuitative and robust in NLP.\nI am trying to make out of text sentences search which is both ways <code>word base</code> as well as <code>content type base</code> search but so far, I would not find any solution with spacy.</p>\n<p>I have the text like:</p>\n<blockquote>\n<p>In computer science, artificial intelligence (AI), sometimes called\nmachine intelligence, is intelligence demonstrated by machines, unlike\nthe natural intelligence displayed by humans and animals. Leading AI\ntextbooks define the field as the study of &quot;intelligent agents&quot;: any\ndevice that perceives its environment and takes actions that maximize\nits chance of successfully achieving its goals.[1] Colloquially, the\nterm &quot;artificial intelligence&quot; is often used to describe machines (or\ncomputers) that mimic &quot;cognitive&quot; functions that humans associate with\nthe human mind, such as &quot;learning&quot; and &quot;problem solving&quot;.[2]</p>\n<p>As machines become increasingly capable, tasks considered to require\n&quot;intelligence&quot; are often removed from the definition of AI, a\nphenomenon known as the AI effect.[3] A quip in Tesler's Theorem says\n&quot;AI is whatever hasn't been done yet.&quot;[4] For instance, optical\ncharacter recognition is frequently excluded from things considered to\nbe AI,[5] having become a routine technology.[6] Modern machine\ncapabilities generally classified as AI include successfully\nunderstanding human speech,[7] competing at the highest level in\nstrategic game systems (such as chess and Go),[8] autonomously\noperating cars, intelligent routing in content delivery networks, and\nmilitary simulations[9].</p>\n<p>Artificial intelligence was founded as an academic discipline in 1955,\nand in the years since has experienced several waves of\noptimism,[10][11] followed by disappointment and the loss of funding\n(known as an &quot;AI winter&quot;),[12][13] followed by new approaches, success\nand renewed funding.[11][14] For most of its history, AI research has\nbeen divided into sub-fields that often fail to communicate with each\nother.[15] These sub-fields are based on technical considerations,\nsuch as particular goals (e.g. &quot;robotics&quot; or &quot;machine learning&quot;),[16]\nthe use of particular tools (&quot;logic&quot; or artificial neural networks),\nor deep philosophical differences.[17][18][19] Sub-fields have also\nbeen based on social factors (particular institutions or the work of\nparticular researchers).[15]</p>\n</blockquote>\n<p>Now, I want to extract the sentences complete in multiple with multiple words or string matching. E.g., i want to search <code>intelligent</code> and <code>machine learning</code>. and it prints all complete sentences which contain this single or both given strings.</p>\n<p>Is there any way that importing model of spacy with spacy can sense the phrase match ..  like it finds all the intelligent and machine learning containing words and print that ? and also with other option, can it also finds as with search machine learning, also suggests deep learning, artificial intelligence, pattern recognition etc?</p>\n<pre><code>import spacy\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\nfrom spacy.matcher import PhraseMatcher\nphrase_matcher = PhraseMatcher(nlp.vocab)\n\nphrases = ['machine learning', ''intelligent, 'human']\n\npatterns = [nlp(text) for text in phrases]\n\nphrase_matcher.add('AI', None, *patterns)\n\nsentence = nlp (processed_article)\n\nmatched_phrases = phrase_matcher(sentence)\n\nfor match_id, start, end in matched_phrases:\n    string_id = nlp.vocab.strings[match_id]  \n    span = sentence[start:end]                   \n    print(match_id, string_id, start, end, span.text)\n</code></pre>\n<p>I tried this which is not providing the complete sentence but only the word with matching ID number.</p>\n<p>in short,</p>\n<ol>\n<li>I am trying to search with multiple words input and find complete sentences which contain either out of input single string or all</li>\n<li>I am trying to use the trained model to also find suggested sentence out of input.</li>\n</ol>\n",
    "score": 4,
    "creation_date": 1594128688,
    "view_count": 10293,
    "answer_count": 2,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 61885245,
    "title": "How to visualize the SpaCy word embedding as scatter plot?",
    "body": "<p>Each word in SpaCy is represented by a vector of length 300. How can I plot these words on a scatter plot to get a visual perspective on how close any 2 words are?</p>\n",
    "score": 4,
    "creation_date": 1589872525,
    "view_count": 1331,
    "answer_count": 2,
    "tags": "matplotlib;machine-learning;nlp;spacy;word-embedding"
  },
  {
    "question_id": 61633485,
    "title": "Extract Noun Phrases with Stanza and CoreNLPClient",
    "body": "<p>I am trying to extract noun phrases from sentences using Stanza(with Stanford CoreNLP). This can only be done with the CoreNLPClient module in Stanza. </p>\n\n<pre><code># Import client module\nfrom stanza.server import CoreNLPClient\n# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\nclient = CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse'], memory='4G', endpoint='http://localhost:9001')\n</code></pre>\n\n<p>Here is an example of a sentence, and I am using the <code>tregrex</code> function in client to get all the noun phrases. <code>Tregex</code> function returns a <code>dict of dicts</code> in python. Thus I needed to process the output of the <code>tregrex</code> before passing it to the <code>Tree.fromstring</code> function in NLTK to correctly extract the Noun phrases as strings. </p>\n\n<pre><code>pattern = 'NP'\ntext = \"Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.\"\nmatches = client.tregrex(text, pattern) ``\n</code></pre>\n\n<p>Hence, I came up with the method <code>stanza_phrases</code> which has to loop through the <code>dict of dicts</code> which is the output of <code>tregrex</code> and correctly format for <code>Tree.fromstring</code> in NLTK.</p>\n\n<pre><code>def stanza_phrases(matches):\n  Nps = []\n  for match in matches:\n    for items in matches['sentences']:\n      for keys,values in items.items():\n        s = '(ROOT\\n'+ values['match']+')'\n        Nps.extend(extract_phrase(s, pattern))\n  return set(Nps)\n</code></pre>\n\n<p>generates a tree to be used by NLTK </p>\n\n<pre><code>from nltk.tree import Tree\ndef extract_phrase(tree_str, label):\n    phrases = []\n    trees = Tree.fromstring(tree_str)\n    for tree in trees:\n        for subtree in tree.subtrees():\n            if subtree.label() == label:\n                t = subtree\n                t = ' '.join(t.leaves())\n                phrases.append(t)\n\n    return phrases\n</code></pre>\n\n<p>Here is my output:</p>\n\n<pre><code>{'Albert Einstein', 'He', 'a German-born theoretical physicist', 'relativity',  'the theory', 'the theory of relativity'}\n</code></pre>\n\n<p>Is there a way I can make this more code efficient with less number of lines (especially, <code>stanza_phrases</code> and <code>extract_phrase</code> methods)</p>\n",
    "score": 4,
    "creation_date": 1588762829,
    "view_count": 2888,
    "answer_count": 2,
    "tags": "python;nlp;stanford-nlp;stanford-stanza"
  },
  {
    "question_id": 61368630,
    "title": "Unsupervised finetuning of BERT for embeddings only?",
    "body": "<p>I would like to fine-tuning BERT for a specific domain on unlabeled data and get the output layer to check the similarity between them. How can I do it? Do I need to fine-tuning first a classifier task (or question answer, etc..) and get the embeddings? Or can I just use a pre-trained Bert model without task and fine-tuning with my own data?</p>\n",
    "score": 4,
    "creation_date": 1587568511,
    "view_count": 4922,
    "answer_count": 1,
    "tags": "nlp;similarity;bert-language-model"
  },
  {
    "question_id": 61331415,
    "title": "How to split long strings in pandas columns by punctuation",
    "body": "<p>I have a df that looks like this:</p>\n\n<pre><code>words                                              col_a   col_b  \nI guess, because I have thought over that. Um,       1       0 \nThat? yeah.                                          1       1\nI don't always think you're up to something.         0       1                                                       \n</code></pre>\n\n<p>I want to split df.words wherever a punctuation character is present <code>(.,?!:;)</code> into a separate row. However I want to preserve the col_b and col_b values from the original row for each new row. For example, the above df should look like this:</p>\n\n<pre><code>words                                              col_a   col_b  \nI guess,                                             1       0\nbecause I have thought over that.                    1       0\nUm,                                                  1       0 \nThat?                                                1       1\nyeah.                                                1       1\nI don't always think you're up to something.         0       1\n</code></pre>\n",
    "score": 4,
    "creation_date": 1587414280,
    "view_count": 316,
    "answer_count": 1,
    "tags": "python;pandas;nlp"
  },
  {
    "question_id": 55502572,
    "title": "R: Memory-Efficient Method of Getting Every Word Permutation within a String",
    "body": "<p>I've a string with a list of words, and I want to get all the possible word combinations from it. </p>\n\n<pre><code>fruits &lt;- \"Apple Banana Cherry\"\n</code></pre>\n\n<p>To get this output:</p>\n\n<pre><code>\"Apple, Banana, Cherry, Apple Banana, Apple Cherry, Banana Cherry, Apple Banana Cherry\"\n</code></pre>\n\n<p>Using the function defined <a href=\"https://stackoverflow.com/a/33122758/5195054\">here</a>, slightly modified:</p>\n\n<pre><code>f1 &lt;- function(str1){\n  v1 &lt;- strsplit(str1, ' ')[[1]]\n  paste(unlist(sapply(seq(length(v1)), function(i)\n    apply(combn(v1, i), 2, paste, collapse=\" \"))), collapse= ', ')\n}\n\nf1(fruits)\n</code></pre>\n\n<p>This works fine when there's relatively few rows, but the real-life example has a total of 93,300 characters across 3,350 rows with an median string length of 25 characters, causing an error similar to <a href=\"https://stackoverflow.com/questions/53120436/error-in-pastev-collapse-n-result-would-exceed-231-1-bytes\">this:</a> </p>\n\n<blockquote>\n  <p>Error in paste(unlist(sapply(seq(length(v1)), function(i) apply(combn(v1,  : \n    result would exceed 2^31-1 bytes</p>\n</blockquote>\n\n<p>I tried changing <code>utils::combn</code> to <code>RcppAlgos::comboGeneral</code> within the function , because it's apparently <a href=\"https://stackoverflow.com/a/47983855/5195054\">quicker</a>, but still encountered the same issue. Any suggestions for ways around this?</p>\n",
    "score": 4,
    "creation_date": 1554318427,
    "view_count": 203,
    "answer_count": 3,
    "tags": "r;nlp;out-of-memory;permutation"
  },
  {
    "question_id": 55348709,
    "title": "How does SpaCy keeps track of character and token offset during tokenization?",
    "body": "<p><strong>How does SpaCy keeps track of character and token offset during tokenization?</strong></p>\n\n<p>In SpaCy, there's a Span object that keeps the start and end offset of the token/span <a href=\"https://spacy.io/api/span#init\" rel=\"nofollow noreferrer\">https://spacy.io/api/span#init</a></p>\n\n<p>There's a <a href=\"https://github.com/explosion/spaCy/blob/72889a16d558848191e51f4bfb200e70d3bc413a/spacy/tokens/span.pyx#L312\" rel=\"nofollow noreferrer\"><code>_recalculate_indices</code></a> method seems to be retrieving the <code>token_by_start</code> and <code>token_by_end</code> but that looks like all the recalcuation is doing. </p>\n\n<p>When looking at extraneous spaces, it's doing some <a href=\"https://github.com/explosion/spaCy/blob/72889a16d558848191e51f4bfb200e70d3bc413a/spacy/tokenizer.pyx#L95\" rel=\"nofollow noreferrer\">smart alignment of the spans</a>. </p>\n\n<p>Does it recalculate after every regex execution, does it keep track of the character's movement? Does it do a post regexes execution span search?</p>\n",
    "score": 4,
    "creation_date": 1553564624,
    "view_count": 1791,
    "answer_count": 1,
    "tags": "python;algorithm;nlp;cython;spacy"
  },
  {
    "question_id": 53445949,
    "title": "How to do text classification with DeepPavlov",
    "body": "<p>I am interested in doing text classification with <a href=\"http://deeppavlov.ai\" rel=\"nofollow noreferrer\">DeepPavlov</a> chatbot framework. </p>\n\n<p>The problem is I don't have enough training data. Ideally, I would like to do text classification with just few samples for each class.</p>\n",
    "score": 4,
    "creation_date": 1542972879,
    "view_count": 654,
    "answer_count": 1,
    "tags": "nlp;artificial-intelligence;chatbot"
  },
  {
    "question_id": 53316174,
    "title": "Using pre-trained word embeddings - how to create vector for unknown / OOV Token?",
    "body": "<p>I wan't to add <a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">pre-trained embeddings</a> to a model. But as it seems there is no <em>out-of-vocabulary (OOV)</em> token resp. no vector for unseen words existent. </p>\n\n<p>So what can I do to handle <em>OOV-tokens</em> I come across? I have some ideas, but none of them seem to be very good:</p>\n\n<ul>\n<li><p>I could just create a random vector for this token, but ideally I'd like the vector to within the <em>logic</em> of the existing model. If I just create it randomly I'm afraid the vector accidentally could be very similar to a very frequent word like <em>'the', 'for', 'that'</em> etc. which is not my intention.</p></li>\n<li><p>Or should I just initialize the vector with plain zeros instead?</p></li>\n<li><p>Another idea would be averaging the token over other existing vectors. But averaging on what vectors then? On all? This doesn't seem to be very conclusive either.</p></li>\n<li><p>I also thought about trying to train this vector. However this doesn't come very handy if I want to freeze the rest of the embedding during training. </p></li>\n</ul>\n\n<p><em>(A general solution is appreciated, but I wanted to add that I'm using PyTorch - just in case PyTorch already comes with a handy solution to this problem.)</em></p>\n\n<p><strong>So what would be a good and <em>easy</em> strategy to create such a vector?</strong></p>\n",
    "score": 4,
    "creation_date": 1542273989,
    "view_count": 3846,
    "answer_count": 1,
    "tags": "neural-network;deep-learning;nlp;pytorch;word-embedding"
  },
  {
    "question_id": 53183341,
    "title": "Converting Fasttext vector to word",
    "body": "<p>I am having trouble converting a fast FastText vector back to a word.\nHere is my python code: </p>\n\n<pre><code>from gensim.models import KeyedVectors\nen_model = KeyedVectors.load_word2vec_format('wiki.en/wiki.en.vec')\nvect = en_model.get_vector(\"turtles\")\n</code></pre>\n\n<p>How can I take the vector (especially an arbitrary vector with the proper dimensions) and have it spit out a word?</p>\n",
    "score": 4,
    "creation_date": 1541562697,
    "view_count": 2686,
    "answer_count": 1,
    "tags": "python;nlp;data-science;gensim;fasttext"
  },
  {
    "question_id": 52080542,
    "title": "How to load a spark-nlp pre-trained model from disk",
    "body": "<p>From the <code>spark-nlp</code> Github <a href=\"https://github.com/JohnSnowLabs/spark-nlp#downloading-models-for-offline-use\" rel=\"nofollow noreferrer\">page</a> I downloaded a <code>.zip</code> file containing a pre-trained NerCRFModel. The zip contains three folders: embeddings, fields, and metadata.</p>\n<p>How do I load that into a Scala <code>NerCrfModel</code> so that I can use it? Do I have to drop it into HDFS or the host where I launch my Spark Shell? How do I reference it?</p>\n",
    "score": 4,
    "creation_date": 1535554560,
    "view_count": 3321,
    "answer_count": 1,
    "tags": "scala;apache-spark;nlp;apache-spark-mllib;johnsnowlabs-spark-nlp"
  },
  {
    "question_id": 50039310,
    "title": "Is nltk wordnet lemmatizer language independent?",
    "body": "<p>Is it true that <a href=\"http://www.nltk.org/_modules/nltk/stem/wordnet.html\" rel=\"nofollow noreferrer\">nltk's wordnet lemmatizer</a> does not depend on the language of the input text ? Would I use the same sequence of commands:</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer\n&gt;&gt;&gt; wnl = WordNetLemmatizer()\n&gt;&gt;&gt; print(wnl.lemmatize('dogs'))\ndog\n&gt;&gt;&gt; print(wnl.lemmatize('churches'))\nchurch\n&gt;&gt;&gt; print(wnl.lemmatize('aardwolves'))\naardwolf\n&gt;&gt;&gt; print(wnl.lemmatize('abaci'))\nabacus\n&gt;&gt;&gt; print(wnl.lemmatize('hardrock'))\nhardrock\n</code></pre>\n\n<p>for both english and french for instance ?</p>\n",
    "score": 4,
    "creation_date": 1524734464,
    "view_count": 4728,
    "answer_count": 1,
    "tags": "nlp;nltk;lemmatization"
  },
  {
    "question_id": 49952762,
    "title": "Building a POS tagger for a new language",
    "body": "<p>I'm kind of new to NLP and I'm trying to build a POS tagger for Sinhala language. Are there any specific steps to follow to build the system?</p>\n",
    "score": 4,
    "creation_date": 1524289536,
    "view_count": 4294,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 49917033,
    "title": "Built-in function to get the frequency of one word with spaCy?",
    "body": "<p>I'm looking for faster alternatives to NLTK to analyze big corpora and do basic things like calculating frequencies, PoS tagging etc... SpaCy seems great and easy to use in many ways, but I can't find any built-in function to count the frequency of a specific word for example. I've looked at the spaCy documentation, but I can't find a straightforward way to do it. Am I missing something?</p>\n\n<p>What I would like would be the NLTK equivalent of:</p>\n\n<pre><code>tokens.count(\"word\") #where tokens is the tokenized text in which the word is to be counted\n</code></pre>\n\n<p>In NLTK, the above code would tell me that in my text, the word \"word\" appears X number of times.</p>\n\n<p>Note that I've come by the count_by function, but it doesn't seem to do what I'm looking for.</p>\n",
    "score": 4,
    "creation_date": 1524128852,
    "view_count": 7314,
    "answer_count": 3,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 49861842,
    "title": "AttributeError: &#39;Tokenizer&#39; object has no attribute &#39;oov_token&#39; in Keras",
    "body": "<p>I am trying to encode my text using my loaded tokenizer but am getting the following error </p>\n\n<blockquote>\n  <p><code>AttributeError: 'Tokenizer' object has no attribute 'oov_token'</code></p>\n</blockquote>\n\n<p>I included the code below:</p>\n\n<pre><code>from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Model, Input, Sequential, load_model\nimport pickle\nimport h5py\n\nmaxlen = 100\ntok = open('tokenizer.pickle', 'rb')\ntokenizer = pickle.load(tok)\ntok.close()\nmodel = load_model('weights.h5')\n\ndef predict():\n    new_text = sequence.pad_sequences((tokenizer.texts_to_sequences(['heyyyy'])), maxlen=maxlen)\n    prediction = model.predict(new_text,batch_size=1,verbose=2)\n</code></pre>\n\n<p>The problem occurs on the line <code>tokenizer.texts_to_sequences(['heyyyy'])</code> and I'm not sure why. Is the problem with pickle? the <code>tokenizer.texts_to_sequences</code> works with <code>'hey'</code>, <code>'heyy'</code>, and <code>'heyyy'</code>. </p>\n\n<p>Any guidance is appreciated!</p>\n",
    "score": 4,
    "creation_date": 1523895239,
    "view_count": 9287,
    "answer_count": 1,
    "tags": "python;nlp;keras;pickle;tokenize"
  },
  {
    "question_id": 48865150,
    "title": "Pipeline for text cleaning / processing in python",
    "body": "<p>I am pretty new to the python environment (jupyter notebook), and I am trying to work on a relatively huge text data. I want to process it by applying the following steps and in the same order:</p>\n\n<p><strong>strip whitespaces,\nlower case,\nstemming,\nremove punctuation but preserve intra-word dashes or hyphens,\nremove stopwords,\nremove symbols,\nStrip whitespaces,</strong></p>\n\n<p>I was hoping I could get a single function that could perform the task, instead of doing them individually, is there any single library and/or function out there that could help? if not, what could be the simplest way of defining a function to perform them just with one run?</p>\n",
    "score": 4,
    "creation_date": 1519039523,
    "view_count": 6323,
    "answer_count": 2,
    "tags": "python-3.x;nlp;nltk;jupyter-notebook;text-processing"
  },
  {
    "question_id": 48721152,
    "title": "String matching from NLP input",
    "body": "<p>I am fooling around with Alexa a little. My task is to match the user input with a list of possible answers dynamically loaded from the web. In this case, it's a list of movies.</p>\n\n<p>Of course I'm not able to assume there will always be a perfect match, either the user or the Echo device won't get it quite right. My current approach to overcome this is the SequenceMatcher function. So I measure the similarity of the user input and all items in the list and the winner probably is the list item the user really was talking about:</p>\n\n<pre><code>from difflib import SequenceMatcher\n\nmaxi = 0\nhaystack = [\"Die Verurteilten\", \"Der Pate\", \"Der Pate 2\", \"The Dark Knight\", \"Die zwölf Geschworenen\", \"Schindlers Liste\", \"Pulp Fiction\", \"Der Herr der Ringe - Die Rückkehr des Königs\", \"Zwei glorreiche Halunken\", \"Fight Club\", \"Der Herr der Ringe - Die Gefährten\", \"Forrest Gump\", \"Das Imperium schlägt zurück\", \"Inception\", \"Der Herr der Ringe - Die zwei Türme\", \"einer flog über das Kuckucksnest\", \"GoodFellas - Drei Jahrzehnte in der Mafia\", \"Matrix\", \"Die sieben Samurai\", \"Krieg der Sterne\", \"City of God\", \"Sieben\", \"Das Schweigen der Lämmer\", \"Ist das Leben nicht schön?\", \"Das Leben ist schön\"]\nneedle = \"Die Gefährten\"\n\nfor hay in haystack:\n    ratio = SequenceMatcher(None, needle, hay).ratio()\n    print('%.5f' % ratio + \" \" + hay)\n    if ratio &gt; maxi:\n        maxi = ratio\n        result = hay\n\nprint(result)\n</code></pre>\n\n<p>Most of the time I'm happy with the result. However, sometimes (and a little too often) I'm not. In case the user might ask for \"Die Gefährten\" like in the example above, this will happen:</p>\n\n<pre><code># 0.62069 Die Verurteilten\n# 0.55319 Der Herr der Ringe - Die Gefährten\n# Die Verurteilten\n</code></pre>\n\n<p>For this particular case it may be an easy solution to split the list items by the separator <code>-</code>, do calculations for all resulting parts and give back the maximum score. But as the list may be anything else (recipes, books, songs, games, ...) I'm wondering if there is a more universal approach. Any ideas?</p>\n\n<p>Thanks.</p>\n",
    "score": 4,
    "creation_date": 1518268245,
    "view_count": 1554,
    "answer_count": 2,
    "tags": "python;nlp;string-matching;alexa-skills-kit"
  },
  {
    "question_id": 47372801,
    "title": "NLTK word_tokenize on French text is not woking properly",
    "body": "<p>I'm trying to use NLTK <code>word_tokenize</code> on a text in <strong>French</strong> by using :</p>\n\n<pre><code>txt = [\"Le télétravail n'aura pas d'effet sur ma vie\"]\nprint(word_tokenize(txt,language='french'))\n</code></pre>\n\n<p>it should print:</p>\n\n<pre><code>['Le', 'télétravail', 'n'','aura', 'pas', 'd'','effet', 'sur', 'ma', 'vie','.']\n</code></pre>\n\n<p>But I get: </p>\n\n<pre><code>['Le', 'télétravail', \"n'aura\", 'pas', \"d'effet\", 'sur', 'ma', 'vie','.']\n</code></pre>\n\n<p>Does anyone know why it's not spliting tokens properly in French and how to overcome this (and other potential issues) when doing NLP in French?</p>\n",
    "score": 4,
    "creation_date": 1511054108,
    "view_count": 4504,
    "answer_count": 3,
    "tags": "python;nlp;nltk;french"
  },
  {
    "question_id": 46694770,
    "title": "IPA (International Phonetic Alphabet) Transcription with Tensorflow",
    "body": "<p>I'm looking into designing a software platform that will aid linguists and anthropologists in their study of previously unstudied languages. Statistics show that around 1,000 languages exist that have never been studied by a person outside of their respective speaker groups.</p>\n\n<p>My goal is to utilize TensorFlow to make a platform that will allow linguists to study and document these languages more efficiently, and to help them create written systems for the ones that don't have a written system already. One of their current methods of accomplishing such a task is three-fold: 1) Record a native speaker conversing in the language, 2) Listening to that recording and trying to transcribe it into the IPA, 3) From the phonetics, analyzing the phonemics and phonotactics of the language to eventually create a written system for the speaker.</p>\n\n<p>My proposed platform would cut that research time down from a minimum of a year to a maximum of six months. Before I start, I have some questions...</p>\n\n<p><strong>What would be required to train TensorFlow to transcribe live audio into the IPA? Has this already been done? and if so, how would I utilize a previous solution for this project? Is a project like this even possible with TensorFlow? if not, what would you recommend using instead?</strong></p>\n\n<p>My apologies for the magnitude of this question. I don't have much experience in the realm of machine learning, as I am just beginning the research process for this project. Any help is appreciated!</p>\n",
    "score": 4,
    "creation_date": 1507744958,
    "view_count": 2367,
    "answer_count": 2,
    "tags": "tensorflow;nlp;linguistics"
  },
  {
    "question_id": 46687065,
    "title": "Can PostgreSQL&#39;s to_tsvector function return tokens/words and not lexemes?",
    "body": "<p>PostgreSQL's <code>to_tsvector</code> function is extremely useful but in regards to my data set it does a little more than I want it to.</p>\n\n<p>For instance:</p>\n\n<pre><code>select * \nfrom to_tsvector('english', 'This is my favourite game. I enjoy everything about it.');\n</code></pre>\n\n<p>produces: <code>'enjoy':7 'everyth':8 'favourit':4 'game':5</code></p>\n\n<p>I am not fussed about stop-words getting filtered out, that is fine. But some words get completely ruined, like <code>everything</code> and <code>favourite</code>.</p>\n\n<p>Is there a way to modify this behaviour or is there a different function that does this?</p>\n\n<p>PS: Yes, I can write my own query that does this (and I have) but I want a faster method.</p>\n",
    "score": 4,
    "creation_date": 1507721201,
    "view_count": 2005,
    "answer_count": 1,
    "tags": "postgresql;nlp;lemmatization"
  },
  {
    "question_id": 46326968,
    "title": "python: word segmentation based on dictionary",
    "body": "<p>I am having a <strong>dictionary</strong> :</p>\n\n<pre><code>dict = [\"as\", \"ass\", \"share\", \"rest\"]\n</code></pre>\n\n<p>and a string <strong>input</strong> :</p>\n\n<pre><code>string = \"xassharest\"\n</code></pre>\n\n<p>And i want to show all possible words can made based on dictionary like this :</p>\n\n<pre><code>[('x', 'as', 's', 'h', 'a', 'rest'), ('x', 'as', 'share', 's', 't'), ('x', 'ass', 'h', 'a', 'rest')]\n</code></pre>\n\n<p>Actually i've tried it using all combinations of string (using library itertools) but it takes so long. Here is my code :</p>\n\n<pre><code>def getallpossiblewords(string):\n    allwords = preprocessingcorpus(\"corpus.txt\")\n    temp = []\n    for i in range(0, len(string)):\n        for j in range(1, len(string) + 1):\n            if string[i:j] in allwords:\n                temp += [string[i:j]]\n\n    allposwords = sorted(temp, key=len, reverse=True)\n    #print(allposwords)\n    return allposwords\n\ndef wordseg(string):\n    a = string\n    b = getallpossiblewords(string)\n    cuts = []\n    allpos = []\n    for i in range(0,len(a)):\n        cuts.extend(combinations(range(1,len(a)),i))\n    for i in cuts:\n        last = 0\n        output = []\n        for j in i:\n            output.append(a[last:j])\n            last = j\n        output.append(a[last:])\n        for x in range(len(output)):\n            if output[x] in b:\n                allpos += [output]\n                #print(output)\n    #print(allpos)\n\n    fixallpos = list()\n    for sublist in allpos:\n        if sublist not in fixallpos:\n            fixallpos.append(sublist)\n</code></pre>\n\n<p>I need the fastest algorithm to solve this problem, because the input of string may be even longer.</p>\n\n<p>Can anyone solve my problem?</p>\n",
    "score": 4,
    "creation_date": 1505923810,
    "view_count": 2185,
    "answer_count": 1,
    "tags": "python;nlp"
  },
  {
    "question_id": 46259946,
    "title": "Speeding up model training using MITIE with Rasa",
    "body": "<p>I'm training a model for recognizing short, one to three sentence strings of text using the MITIE back-end in Rasa. The model trains and works using spaCy, but it isn't quite as accurate as I'd like. Training on spaCy takes no more than five minutes, but training for MITIE ran for several days non-stop on my computer with 16GB of RAM. So I started training it on an Amazon EC2 r4.8xlarge instance with 255GB RAM and 32 threads, but it doesn't seem to be using all the resources available to it.</p>\n\n<p>In the Rasa config file, I have <code>num_threads: 32</code> and set <code>max_training_processes: 1</code>, which I thought would help use all the memory and computing power available. But now that it has been running for a few hours, CPU usage is sitting at 3% (100% usage but only on one thread), and memory usage stays around 25GB, one tenth of what it could be.</p>\n\n<p>Do any of you have any experience with trying to accelerate MITIE training? My model has 175 intents and a total of 6000 intent examples. Is there something to tweak in the Rasa config files? </p>\n",
    "score": 4,
    "creation_date": 1505612742,
    "view_count": 1726,
    "answer_count": 1,
    "tags": "amazon-web-services;amazon-ec2;nlp;rasa-nlu"
  },
  {
    "question_id": 45869881,
    "title": "Finding Similarity between 2 sentences using word2vec of sentence with python",
    "body": "<p>I want to calculate the similarity between two sentences using word2vectors, I am trying to get the vectors of a sentence so that i can calculate the average of a sentence vectors to find the cosine similarity. i have tried this code but its not working. the output it gives the sentence-vectors with ones. i want the actual vectors of sentences in sentence_1_avg_vector &amp; sentence_2_avg_vector.</p>\n\n<p>Code:</p>\n\n<pre><code>    #DataSet#\n    sent1=[['What', 'step', 'step', 'guide', 'invest', 'share', 'market', 'india'],['What', 'story', 'Kohinoor', 'KohiNoor', 'Diamond']]\n    sent2=[['What', 'step', 'step', 'guide', 'invest', 'share', 'market'],['What', 'would', 'happen', 'Indian', 'government', 'stole', 'Kohinoor', 'KohiNoor', 'diamond', 'back']]\n    sentences=sent1+sent2\n\n    #''''Applying Word2vec''''#\n    word2vec_model=gensim.models.Word2Vec(sentences, size=100, min_count=5)\n    bin_file=\"vecmodel.csv\"\n    word2vec_model.wv.save_word2vec_format(bin_file,binary=False)\n\n    #''''Making Sentence Vectors''''#\n    def avg_feature_vector(words, model, num_features, index2word_set):\n        #function to average all words vectors in a given paragraph\n        featureVec = np.ones((num_features,), dtype=\"float32\")\n        #print(featureVec)\n        nwords = 0\n        #list containing names of words in the vocabulary\n        index2word_set = set(model.wv.index2word)# this is moved as input param for performance reasons\n        for word in words:\n            if word in index2word_set:\n                nwords = nwords+1\n                featureVec = np.add(featureVec, model[word])\n                print(featureVec)\n        if(nwords&gt;0):\n            featureVec = np.divide(featureVec, nwords)\n        return featureVec\n\n    i=0\n    while i&lt;len(sent1):\n        sentence_1_avg_vector = avg_feature_vector(mylist1, model=word2vec_model, num_features=300, index2word_set=set(word2vec_model.wv.index2word))\n        print(sentence_1_avg_vector)\n\n        sentence_2_avg_vector = avg_feature_vector(mylist2, model=word2vec_model, num_features=300, index2word_set=set(word2vec_model.wv.index2word))\n        print(sentence_2_avg_vector)\n\n        sen1_sen2_similarity =  1 - spatial.distance.cosine(sentence_1_avg_vector,sentence_2_avg_vector)\n        print(sen1_sen2_similarity)\n\n        i+=1\n</code></pre>\n\n<p>the output this code gives:</p>\n\n<pre><code>[ 1.  1.  ....  1.  1.]\n[ 1.  1.  ....  1.  1.]\n0.999999898245\n[ 1.  1.  ....  1.  1.]\n[ 1.  1.  ....  1.  1.]\n0.999999898245\n</code></pre>\n",
    "score": 4,
    "creation_date": 1503604426,
    "view_count": 4951,
    "answer_count": 2,
    "tags": "python;nlp"
  },
  {
    "question_id": 45690619,
    "title": "Vectorizer the combination of words in Python",
    "body": "<p>I have a dataset with medical text data and I apply tf-idf vectorizer on them and calculate tf idf score for the words just like this:</p>\n\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer as tf\n\nvect = tf(min_df=60,stop_words='english')\n\ndtm = vect.fit_transform(df) \nl=vect.get_feature_names() \n\nx=pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())\n</code></pre>\n\n<p>So basically my question is following-while I'm applying TfidfVectorizer it splits the text in distinct words for example: \"pain\", \"headache\", \"nausea\" and so on. How can I get the words combination in the output of TfidfVectorizer for example: \"severe pain\", \"cluster headache\", \"nausea vomiting\". Thanks</p>\n",
    "score": 4,
    "creation_date": 1502791192,
    "view_count": 668,
    "answer_count": 1,
    "tags": "python;scikit-learn;nlp;tf-idf;countvectorizer"
  },
  {
    "question_id": 45502464,
    "title": "What is the difference between wmd (word mover distance) and wmd based similarity?",
    "body": "<p>I am using WMD to calculate the similarity scale between sentences. For example:</p>\n\n<pre><code>distance = model.wmdistance(sentence_obama, sentence_president)\n</code></pre>\n\n<p>Reference: <a href=\"https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\" rel=\"nofollow noreferrer\">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>\n\n<p>However, there is also WMD based similarity method <code>(WmdSimilarity).</code></p>\n\n<p>Reference: \n<a href=\"https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\" rel=\"nofollow noreferrer\">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>\n\n<p>What is the difference between the two except the obvious that one is distance and another similarity? </p>\n\n<p><strong>Update:</strong> Both are exactly the same except with their different representation. </p>\n\n<pre><code>n_queries = len(query)\nresult = []\nfor qidx in range(n_queries):\n    # Compute similarity for each query.\n    qresult = [self.w2v_model.wmdistance(document, query[qidx]) for document in self.corpus]\n    qresult = numpy.array(qresult)\n    qresult = 1./(1.+qresult)  # Similarity is the negative of the distance.\n\n    # Append single query result to list of all results.\n    result.append(qresult)\n</code></pre>\n\n<p><a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py\" rel=\"nofollow noreferrer\">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py</a></p>\n",
    "score": 4,
    "creation_date": 1501837157,
    "view_count": 2261,
    "answer_count": 1,
    "tags": "nlp;nltk;gensim;word2vec;word-embedding"
  },
  {
    "question_id": 45346418,
    "title": "How to obtain the word list from pyspark word2vec model?",
    "body": "<p>I am trying to generate word vectors using PySpark. Using gensim I can see the words and the closest words as below:</p>\n\n\n\n<pre class=\"lang-python prettyprint-override\"><code>sentences = open(os.getcwd() + \"/tweets.txt\").read().splitlines()\nw2v_input=[]\nfor i in sentences:\n    tokenised=i.split()\n    w2v_input.append(tokenised)\nmodel = word2vec.Word2Vec(w2v_input)\nfor key in model.wv.vocab.keys():\n    print key\n    print model.most_similar(positive=[key])\n</code></pre>\n\n<p>Using PySpark</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>inp = sc.textFile(\"tweet.txt\").map(lambda row: row.split(\" \"))\nword2vec = Word2Vec()\nmodel = word2vec.fit(inp)\n</code></pre>\n\n<p>How can I generate the words from the vector space in model? That is the pyspark equivalent of the gensim <code>model.wv.vocab.keys()</code>?</p>\n\n<p>Background: I need to store the words and the synonyms from the model in a map so I can use them later for finding the sentiment of a tweet. I cannot reuse the word-vector model in the map functions in pyspark as the model belongs to the spark context (error pasted below). I want the pyspark word2vec version instead of gensim because it provides better synonyms for certain test words.</p>\n\n<pre class=\"lang-python prettyprint-override\"><code> Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation.SparkContext can only be used on the driver, not in code that it run on workers.\n</code></pre>\n\n<p>Any alternative solution is also welcome.</p>\n",
    "score": 4,
    "creation_date": 1501147079,
    "view_count": 7185,
    "answer_count": 2,
    "tags": "apache-spark;nlp;pyspark;apache-spark-mllib;word2vec"
  },
  {
    "question_id": 45264957,
    "title": "Storing ngram model python",
    "body": "<p>I am implementing language model as a personal challenge, as a part of simple web application. Still I've avoided using NLTK, however was faced with MemoryError with enough big corpus (vocabulary about 50000 and amount of trigrams was about 440000 - I've used standard python dictionary and after tried numpy array to store all word-ngram probabilities as matrix). So it seems the solution is to use more efficient data structure, something that was mentioned here <a href=\"https://stackoverflow.com/questions/38264636/train-a-language-model-using-google-ngrams\">train a language model using Google Ngrams</a> Or store model on disk. In General, could you advise what approach could be better  for storing ngram model (in memory or disk space) and after using it as a part of web-app?</p>\n",
    "score": 4,
    "creation_date": 1500812060,
    "view_count": 2315,
    "answer_count": 2,
    "tags": "python;data-structures;web-applications;nlp;n-gram"
  },
  {
    "question_id": 44725930,
    "title": "Duplicate elimination of similar company names",
    "body": "<p>I have a table with company names. There are many duplicates because of human input errors. There are different perceptions if the subdivision should be included, typos, etc. I want all these duplicates to be marked as one company \"1c\":</p>\n\n<pre><code>+------------------+\n|     company      |\n+------------------+\n| 1c               |\n| 1c company       |\n| 1c game studios  |\n| 1c wireless      |\n| 1c-avalon        |\n| 1c-softclub      |\n| 1c: maddox games |\n| 1c:inoco         |\n| 1cc games        |\n+------------------+\n</code></pre>\n\n<p>I identified <a href=\"https://en.wikipedia.org/wiki/Levenshtein_distance\" rel=\"nofollow noreferrer\">Levenshtein distance</a> as a good way to eliminate typos. However, when the subdivision is added the Levenshtein distance increases dramatically and is no longer a good algorithm for this. Is this correct? </p>\n\n<p>In general I have barely any experience in Computational Linguistics so I am at a loss what methods I should choose. </p>\n\n<p><strong>What algorithms would you recommend for this problem?</strong> I want to implement it in java. Pure SQL would also be okay. Links to sources would be appreciated. Thanks.</p>\n",
    "score": 4,
    "creation_date": 1498234467,
    "view_count": 2395,
    "answer_count": 1,
    "tags": "duplicates;nlp;linguistics"
  },
  {
    "question_id": 43901083,
    "title": "SGDClassifier giving different accuracy each time for text classification",
    "body": "<p>I'm using the SVM Classifier for classifying text as good text and gibberish. I'm using python's scikit-learn and doing it as follows: </p>\n\n<pre><code>'''\nCreated on May 5, 2017\n'''\n\nimport re\nimport random\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import metrics\n\n# Prepare data\n\ndef prepare_data(data):\n    \"\"\"\n    data is expected to be a list of tuples of category and texts.\n    Returns a tuple of a list of lables and a list of texts\n    \"\"\"\n    random.shuffle(data)\n    return zip(*data)\n\n# Format training data\n\ntraining_data = [\n    (\"good\", \"rain a lot the packs maybe damage.\"),\n    (\"good\", \"15107 Lane Pflugerville, TX customer called me and his phone number and my phone numbers were not masked. thank you customer has had a stroke and items were missing from his delivery the cleaning supplies for his wet vacuum steam cleaner.  he needs a call back from customer support \"),\n    (\"gibber\", \"wh. screen\"),\n    (\"gibber\", \"How will I know if I\"),\n    (\"good\", \"I have problems scheduling blocks they are never any available.  Can I do full time?  Can I get scheduled more than one day a month?\"),\n    (\"good\", \"Suggestion: easier way to sign in due alleviate the tediousness of periodically having to sign back in to the app to check for blocks.\"),\n    (\"good\", \"I am so glad to hear from you. \"),\n    (\"good\", \"loading on today's itinerary takes ages!!!!!! time consuming when you have 150+ packages to deliver!!!!!\"),\n    (\"good\", \"due to the new update that makes hours available at 10 pm. if you worked 8 hours that day you can't see next day hours due to 8 hour limit. please fix this\"),\n    (\"good\", \"omg, PLEASE make it so we don't have to sign in every time we need to go into the app. At least make it good for a week. Thanks.\"),\n    (\"good\", \"Constantly being logged out of app, if we could have a continuous login so we could receive notifications if blocks are available that would be ideal.\"),\n    (\"good\", \"I am having problems  with the App. Every time I exit the App and reopen it asks for my login info.\"),\n    (\"good\", \"15 minute service time due to 33rd floor and  20l lbs of cargo\"),\n    (\"good\", \"I have been sceduled 1 block in 3 weeks. I check for new block availability multiple times a day and have not seen 1 available in three weeks. is there any way to get more blocks.\"),\n    (\"good\", \"When will delivery jobs be available? Everytime I open this app, it says nothing is available. Have deliveries in Cincinnati started yet?\"),\n    (\"good\", \"During delivery had to call customer support and after 10 minutes support person couldn't find my pick up location Kirkland /Bellevue and told me to hang up and call different support team.  Support person were unprofessional and rude, which is not acceptable.\"),\n    (\"good\", \"can you please remove the pick up from my phone\"),\n    (\"good\", \"Dear friends: I'm very very happy it's a big oportunitt\"),\n    (\"good\", \"THANK YOU so much for the block you assigned me for next week.   If you have an additional 5 blocks please go ahead and assign them to me for next week.  My availability is updated and current.  You guys are awesome!!!\"),\n    (\"good\", \"after update every time I open app I have too log in! I used to be able to stay logged in unless I logged out, can you return stay logged in option.\"),\n    (\"good\", \"It looks like my app is not installed properly on my android phone, Note 5. I cannot access or do not see the tab to swipe to start delivering and the map or help button that should be visible for me to work today 5/6 at rpm\"),\n    (\"gibber\", \"AF0000\"),\n    (\"good\", \"awesome app, awesome hiring process, awesome delivery warehouse , awesome team and help in the field! lets deliver I would like more more more delivers , looking forward to the future ! I just bought a new delivery vehical !\"),\n    (\"good\", \"I will like to ask why I can't get more delivery's only one in two weeks\"),\n    (\"good\", \"device too slow software crashing all day\"),\n    (\"good\", \"it doesn't work sometimes.\"),\n    (\"good\", \"can you please remove the old sprouts pick up from my phone\"),\n    (\"good\", \"They ability to zoom in on text screens would be very helpful. Am example would be customer notes when viewing in certain lighting conditions can be difficult.\"),\n    (\"good\", \"I missed out on a delivery day when I clicked check in and waited for my turn to get an order only to find out that not only did my check in not register but the gps showed me down the street. I encountered this issue again when one of the warehouse employees placed an order for that location and the app wanted me to drive in a big circle to get back to where I was standing.\"),\n    (\"good\", \"i am a little concerned that i didn't receive any blocks of time for this coming week, even though i had a perfect delivery score from this past pay period. Did the Cincinnati market over hire drivers where there are many people being shut completely out of any delivery blocks for an entire week? i really enjoy this type of work and the app makes it quite convenient.\"),\n    (\"good\", \"I've arrived at the pick up restaurant but the staff did not have the barecode for me to scan, however I pick up the package and deliver but my is still not let me move on\"),\n    (\"good\", \"might want to check my assigned hours for next week.  5am to 1pm??\"),\n    (\"good\", \"hi team--just want to give some positive feedback.  I have had nothing but positive feedback from customers. Great support when calling help line. Thank you for this opportunity and if there is ever a situation where you need drivers immediately I will drop what I'm doing and help. You guys are the best.\"),\n    (\"good\", \"Allow days or blocks throughout the day to be modified after General availability is set up for time off like doctors appointments.\"),\n    (\"gibber\", \"AL001234\"),\n    (\"good\", \"Please, enlight me.\"),\n    (\"good\", \"it only shows my schedule starting in two weeks. when will we be able to start work\"),\n    (\"good\", \"include more packages for one block, if the packages can be fitted into the car, so driver don't have to come back and pickup every two hours. 25% of the time is wasted coming back for pick up.\"),\n    (\"gibber\", \"BBB h\"),\n    (\"gibber\", \"AG0003006033SDgCJ12344\"),\n    (\"gibber\", \"How will I know if I\"),\n    (\"good\", \"please bring back some sort of hours cap! or possibly stagger the hour drops from 1200 to 1203 so that people with slower internet/slower phone arent at a disadvantage!\"),\n    (\"good\", \"when the hours released tonight all of the people who didn't have 40 hours could see them.    however the drivers that are capped at 40 were unable to see them due to a flawed system.  please fix the system so that we are not continually treated unfairly like all of the drivers that whined so much and got us in to this mess.  the cap system is unfair to people that want to work and it caused problems with a lack of drivers  to deliver today at the hub.  obviously this is not a good system and benefits no one.\"),\n    (\"good\", \"You have seriously messed up the whole scheduling process. Why can't I get any blocks at 10 even if I wait exactly until 10? Midnight was much better. So now that scheduling is a huge random pain in the ass, why would people want to keep doing this? I haven't been able to schedule work for three days now, it's quite frustrating when I don't get a chance to sign up, even when I'm diligent with timing.\"),\n    (\"good\", \"Seriously, that's all I'm going to get is one lousy day? Tell me again why you need drivers if all we get is one day. I'm not sure this is gonna work out for me. I waited forever to get my background check back and this is what I get? smh\"),\n    (\"good\", \"doesn't save updated access codes\"),\n    (\"good\", \"the scheduling of my route is nor done very accurately. it keeps me driving back and forth\"),\n    (\"good\", \"can't understand how to pick up a block. my availability is wide open. when you guys send the alerts about blocks available I open it real quick and there is nothing there. I do it in a matter of seconds\"),\n    (\"good\", \"My availability keeps disappearing from my calender.   I set my availability for three weeks in advance. The gray dots are visible  but disappear on Wednesday or Thursday.  This makes it impossible for me to see and choose available blocks for the upcoming week. How can I get it fix.   Mike\"),\n    (\"good\", \"GPS blank screen\"),\n    (\"gibber\", \"sea swq\"),\n    (\"gibber\", \"hiw o\"),\n    (\"gibber\", \"Dr a\"),\n    (\"gibber\", \"quick to quick to u uhu wu just us\"),\n    (\"gibber\", \"Awa what's\"),\n    (\"gibber\", \"wxdfcs\"),\n    (\"gibber\", \"7k9opu\"),\n    (\"gibber\", \"o.m.day day\"),\n    (\"gibber\", \"GGT part his h\"),\n    (\"gibber\", \"aawfhg\"),\n    (\"gibber\", \"seesaw 2s\"),\n    (\"gibber\", \"wawaa\"),\n    (\"gibber\", \"of ll\"),\n    (\"gibber\", \"rewards\"),\n    (\"gibber\", \"mmqqm5my\"),\n    (\"gibber\", \".in w\"),\n    (\"gibber\", \"play r\"),\n    (\"gibber\", \"was wwnw www www n\"),\n    (\"gibber\", \"wqq2fwqq2fz22\"),\n    (\"gibber\", \"not\"),\n    (\"gibber\", \"I by yu I\"),\n    (\"gibber\", \"Hi just wanted to let you know that it's bee\"),\n    (\"gibber\", \"I erroneously v\"),\n    (\"gibber\", \"I find it\"),\n    (\"gibber\", \"bqyyx I a\"),\n    (\"gibber\", \"are are\"),\n    (\"gibber\", \"wawi waarnnnkwn\"),\n    (\"gibber\", \"t Petey ueteu he\"),\n    (\"gibber\", \"ews ri\"),\n    (\"gibber\", \"bd xd\"),\n    (\"gibber\", \"hatpa\"),\n    (\"gibber\", \"se wests tasgt\"),\n    (\"gibber\", \"wa vgcx azc Jo of\"),\n    (\"gibber\", \"2w222\"),\n    (\"gibber\", \"her u t b\"),\n    (\"gibber\", \"ddddedc\"),\n    (\"gibber\", \"just juju in hiking\"),\n    (\"gibber\", \"wew2ww2wwwew2i2wkkk\"),\n    (\"gibber\", \"meleeee\"),\n    (\"gibber\", \"Aaq wqXD\"),\n\n\n]\ntraining_labels, training_texts = prepare_data(training_data)\n\n\n# Format test data\n\ntest_data = [\n\n(\"gibber\", \"an quality\"),\n    (\"good\", \"Can't check in.   Time was 4:06.  I didn't drive out here for no reason.\"),\n    (\"good\", \"can you do view all full address including postal code how it's in old app that helps do correctly delivery and not waist customer time\"),\n    (\"good\", \"i am available again starting at 10am to 10pm. thanks\"),\n    (\"gibber\", \"Hello, I encountered\"),\n    (\"good\", \"I want to know how we are notified if there is a block I have been signed in and haven't been given a block yet\"),\n    (\"gibber\", \"aawaaw\"),\n    (\"gibber\", \"eeeeeeeeene\"),\n    (\"good\", \"I am not getting enough shifts\"),\n    (\"gibber\", \"hey e75k\"),\n    (\"good\", \"my screen had went black or inverted\"),\n    (\"good\", \"maps packed up again in sr20ls\"),\n    (\"good\", \"how to clear my itinerary from old pickup address ?\"),\n    (\"good\", \"keep signing me out.\"),\n    (\"good\", \"For alcohol delivery,  where does customer sign?\"),\n    (\"gibber\", \"t Petey ueteu he\"),\n    (\"good\", \"can't get blocks.  too many drivers ??\"),\n    (\"good\", \"got a new phone how do i download to new phone\")\n\n\n\n]\ntest_labels, test_texts = prepare_data(test_data)\n\n\n# Create feature vectors\n\n\"\"\"\nConvert a collection of text documents to a matrix of token counts.\nSee: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n\"\"\"\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(training_texts)\ny = training_labels\n\n\n# Train the classifier\n\n\nclf = SGDClassifier()\nclf.fit(X, y)\n\n\n# Test performance\n\nX_test = vectorizer.transform(test_texts)\ny_test = test_labels\n\n# Generates a list of labels corresponding to the samples\ntest_predictions = clf.predict(X_test)\n\n# Convert back to the usual format\nannotated_test_data = list(zip(test_predictions, test_texts))\nprint(annotated_test_data)\n\n# evaluate predictions\ny_test = np.array(test_labels)\nprint(metrics.classification_report(y_test, test_predictions))\nprint(\"Accuracy: %0.4f\" % metrics.accuracy_score(y_test, test_predictions))\n</code></pre>\n\n<p>But, I keep getting different accuracy each time I run it. Why is this happening?</p>\n\n<p>UPDATE:\nSo I moved the training_data to a text file and I'm reading it in the above code like this:</p>\n\n<pre><code>lines = [line.rstrip('\\n') for line in open(\"file.txt\")]\ntraining_data=[]\nfor i in lines:\n    result = i.rstrip(',')\n    l = literal_eval(result)\n    training_data.append(l)\n\ntraining_labels, training_texts = prepare_data(training_data)\n</code></pre>\n\n<p>And I also changed this in my above code:</p>\n\n<pre><code>clf = SGDClassifier(random_state=5000)\n</code></pre>\n\n<p>So, now the random_state is not None. But, I'm still getting different accuracies each time!!</p>\n",
    "score": 4,
    "creation_date": 1494444071,
    "view_count": 2878,
    "answer_count": 2,
    "tags": "python;scikit-learn;nlp;classification"
  },
  {
    "question_id": 39029244,
    "title": "How to detect uncertainty of text in NLTK Python?",
    "body": "<p>I am a beginner at NLTK and machine learning with the goal of giving uncertainty ratings to sentences. \nFor example, a sentence like <code>This is likely caused by a..</code> would receive a certainty score of say 6, where as <code>There is definitely something wrong with me</code> would receive a 10 and <code>I think it could possibly happen</code> would score a 3. </p>\n\n<p>Regardless of the score system, a classification of \"certain\" and \"uncertain\" can also suffice my needs.</p>\n\n<p>I did not find any existing works on this. How would I approach this? I do have some untrained text data.</p>\n",
    "score": 4,
    "creation_date": 1471564816,
    "view_count": 1683,
    "answer_count": 1,
    "tags": "python;nlp;artificial-intelligence;nltk"
  },
  {
    "question_id": 36731784,
    "title": "How to concatenate word vectors to form sentence vector",
    "body": "<p>I have learned in some essays (Tomas Mikolov...) that a better way of forming the vector for a sentence is to concatenate the word-vector.</p>\n<p>but due to my clumsy in mathematics, I am still not sure about the details.</p>\n<p>for example,</p>\n<p>supposing that the dimension of word vector is m; and that a sentence has n words.</p>\n<p>what will be the correct result of concatenating operation?</p>\n<p>is it a row vector of 1 x m*n ?     or a matrix of m x n ?</p>\n",
    "score": 4,
    "creation_date": 1461112354,
    "view_count": 5063,
    "answer_count": 1,
    "tags": "machine-learning;deep-learning;nlp;word2vec"
  },
  {
    "question_id": 35475677,
    "title": "Custom POS tagging with NLTK (error)",
    "body": "<p>I'm trying to combine my own simple custom tagger with the nltk default tagger, in this case the perceptron tagger.</p>\n\n<p>My code is as follows (based on <a href=\"https://stackoverflow.com/a/5922373/4772958\">this answer</a>):</p>\n\n<pre><code>import nltk.tag, nltk.data\n\ndefault_tagger = nltk.data.load(nltk.tag._POS_TAGGER)\nmodel = {'example_one': 'VB' 'example_two': 'NN'}\ntagger = nltk.tag.UnigramTagger(model=model, backoff=default_tagger)\n</code></pre>\n\n<p>However this gives the following error:</p>\n\n<pre><code>  File \"nltk_test.py\", line 24, in &lt;module&gt;\n    default_tagger = nltk.data.load(nltk.tag._POS_TAGGER)\n  AttributeError: 'module' object has no attribute '_POS_TAGGER'\n</code></pre>\n\n<p>I tried to fix this by changing the default tagger to:</p>\n\n<pre><code>from nltk.tag.perceptron import PerceptronTagger\ndefault_tagger = PerceptronTagger()\n</code></pre>\n\n<p>But then I get the following error:</p>\n\n<pre><code>  File \"nltk_test.py\", line 26, in &lt;module&gt;\n    tagger = nltk.tag.UnigramTagger(model=model, backoff=default_tagger)\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/nltk/tag/sequential.py\", line 340, in __init__\n    backoff, cutoff, verbose)\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/nltk/tag/sequential.py\", line 284, in __init__\n    ContextTagger.__init__(self, model, backoff)\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/nltk/tag/sequential.py\", line 125, in __init__\n    SequentialBackoffTagger.__init__(self, backoff)\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/nltk/tag/sequential.py\", line 50, in __init__\n    self._taggers = [self] + backoff._taggers\nAttributeError: 'PerceptronTagger' object has no attribute '_taggers'\n</code></pre>\n\n<p>Looking through the <code>nltk.tag</code> <a href=\"http://www.nltk.org/api/nltk.tag.html\" rel=\"nofollow noreferrer\">documentation</a> it seems that <code>_POS_TAGGER</code> no longer exists. However changing it to <code>_pos_tag</code> or <code>pos_tag</code> also didn't work.</p>\n",
    "score": 4,
    "creation_date": 1455781759,
    "view_count": 3626,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 34923628,
    "title": "t-SNE High Dimension Data Visualisation",
    "body": "<p>I have a twitter corpus which I am using to build sentiment analysis application. The corpus has 5k tweets which have been hand labelled as - negative, neutral or positive</p>\n\n<p>To represent the text - I am using gensim word2vec pretrained vectors. Each word is mapped to 300 dimensions. For a tweet, I add all the word vectors to get a single 300 dim vectors. Thus every tweet is mapped to a single vector of 300 dimension. </p>\n\n<p>I am visualizing my data using t-SNE (tsne python package). See attached image <a href=\"https://i.sstatic.net/3E0Qf.jpg\" rel=\"nofollow noreferrer\">1</a> - Red points = negative tweets, Blue points = neutral tweets and Green points = Positive tweets</p>\n\n<p><a href=\"https://i.sstatic.net/3E0Qf.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/3E0Qf.jpg\" alt=\"tweets represented using word2vec\"></a>   </p>\n\n<p><strong>Question:</strong>\nIn the plot there no clear separation (boundary) among the data points. Can I assume this will also be the case with the original points in 300 Dimensions ?</p>\n\n<p>i.e if points overlap in t-SNE graph then they also overlap in original space and vice-versa ?</p>\n",
    "score": 4,
    "creation_date": 1453378695,
    "view_count": 1460,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;scikit-learn;data-analysis"
  },
  {
    "question_id": 34153930,
    "title": "Using RNN tensorflow language model to predict the probabilities of test sentences",
    "body": "<p>I was able to train a language model using the <a href=\"https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html#language-modeling\" rel=\"nofollow noreferrer\">tensorflow tutorials</a> , the models are saved as checkpoint files as per the <a href=\"https://stackoverflow.com/questions/33980496/tensorflow-rnn-model-path/33981028?noredirect=1#comment55719523_33981028\">code given here</a>.</p>\n\n<pre><code>save_path = saver.save(sess, \"/tmp/model.epoch.%03d.ckpt\" % (i + 1))\n</code></pre>\n\n<p>Now I need to restore the checkpoint and use it in the following code:</p>\n\n<pre><code>    def run_epoch(session, m, data, eval_op, verbose=False):\n  \"\"\"Runs the model on the given data.\"\"\"\n  epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n  start_time = time.time()\n  costs = 0.0\n  iters = 0\n  state = m.initial_state.eval()\n  for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size,\n                                                    m.num_steps)):\n    cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n                                 {m.input_data: x,\n                                  m.targets: y,\n                                  m.initial_state: state})\n    costs += cost\n    iters += m.num_steps\n\n    if verbose and step % (epoch_size // 10) == 10:\n      print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n            (step * 1.0 / epoch_size, np.exp(costs / iters),\n             iters * m.batch_size / (time.time() - start_time)))\n\n  return np.exp(costs / iters)\n</code></pre>\n\n<p>I cannot find any way of encoding the test sentences and getting sentence probability output from the trained checkpoint model.</p>\n\n<p>The tutorials mention following code:</p>\n\n<pre><code> probabilities = tf.nn.softmax(logits)\n</code></pre>\n\n<p>but that it is for training and I cannot figure out how do I get the actual probabilities.\nI should Ideally get something like :</p>\n\n<pre><code>&gt;&gt;getprob('this is a temp sentence')\n&gt;&gt;0.322\n</code></pre>\n",
    "score": 4,
    "creation_date": 1449571857,
    "view_count": 2883,
    "answer_count": 3,
    "tags": "python;machine-learning;nlp;tensorflow;linguistics"
  },
  {
    "question_id": 32352116,
    "title": "Are limitations of CPU speed and memory prevent us from creating AI systems?",
    "body": "<p>Many technology optimists say that in 15 years the speed of computers will be comparable with the speed of the human brain. This is why they believe that computers will achieve the same level of intelligence as humans.</p>\n\n<p>If Moore's law holds, then every 18 months we should expect doubling of CPU speed. 15 years is 180 months. So, we will have the doubling 10 times. Which means that in 15 years computer will be 1024 times faster than they are now.</p>\n\n<p>But is the speed the reason of the problem? If it is so, we would be able to build an AI system NOW, it would just 1024 times slower than in 15 years. Which means that to answer a question it will need 1024 second (17 minutes) instead of acceptable 1 second. But do we have now strong (but slow) AI system? I think no. Even if now (2015) we give to a system 1 hour instead of 17 minutes, or 1 day, or 1 month or even 1 year, it still will be unable to answer complex questions formulated in natural language. So, it is not the speed that causes problems.</p>\n\n<p>It means that in 15 years our intelligence will not be 1024 faster than now (because we have no intelligence). Instead our \"stupidity\" will be 1024 times faster than now.</p>\n",
    "score": 4,
    "creation_date": 1441193842,
    "view_count": 573,
    "answer_count": 3,
    "tags": "nlp;artificial-intelligence;cpu-speed"
  },
  {
    "question_id": 29476963,
    "title": "comparing synonyms NLTK",
    "body": "<p>I can't come up with a stranger problem, guess you'll help me.</p>\n\n<pre><code>for p in wn.synsets('change'):&lt;br&gt;\n    print(p)\n</code></pre>\n\n<p>Getting:</p>\n\n<pre><code>Synset('change.n.01')\nSynset('change.n.02')\nSynset('change.n.03')\nSynset('change.n.04')\nSynset('change.n.05')\nSynset('change.n.06')\nSynset('change.n.07')\nSynset('change.n.08')\nSynset('change.n.09')\nSynset('variety.n.06')\nSynset('change.v.01')\nSynset('change.v.02')\nSynset('change.v.03')\nSynset('switch.v.03')\nSynset('change.v.05')\nSynset('change.v.06')\nSynset('exchange.v.01')\nSynset('transfer.v.06')\nSynset('deepen.v.04')\nSynset('change.v.10')\n</code></pre>\n\n<p>For example I have an a string:</p>\n\n<pre><code>a = 'transfer'\n</code></pre>\n\n<p>I'd like to be able to identify all kinds of synonyms of word <strong>'change'</strong> and know f.e. <strong>'transfer'</strong> is the one of them. How can I ask my program:\n<em>\"Is 'transfer' is one of the synonyms of 'change'?\"</em></p>\n",
    "score": 4,
    "creation_date": 1428343983,
    "view_count": 2258,
    "answer_count": 4,
    "tags": "python;nlp;nltk;wordnet;synonym"
  },
  {
    "question_id": 28246146,
    "title": "StanfordCoreNLP: Why multiple roots for SemanticGraph (e.g. dependency parsing)",
    "body": "<p>In the the definition of the SemanticGraph class which is being used for Dependency Parsing. </p>\n\n<p>Here is the definition of the variable \"roots\" as a collection of vertices: </p>\n\n<pre><code>private final Collection&lt;IndexedWord&gt; roots;\n</code></pre>\n\n<p>My question is why <em>collection</em>? In what cases we would need more than one vertex as the root? </p>\n\n<p><a href=\"https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/semgraph/SemanticGraph.java\" rel=\"nofollow\">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/semgraph/SemanticGraph.java</a></p>\n",
    "score": 4,
    "creation_date": 1422659444,
    "view_count": 355,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 27301800,
    "title": "Is it possible to append words to an existing OpenNLP POS corpus/model?",
    "body": "<p>Is there a way to train the existing Apache OpenNLP POS Tagger model? I need to add a few more proper nouns to the model that are specific to my application. When I try to use the below command:</p>\n\n<pre><code>opennlp POSTaggerTrainer -type maxent -model en-pos-maxent.bin \\\n        -lang en -data en-pos.train -encoding UTF-8\n</code></pre>\n\n<p>the entire model is retrained. I'd only like to append a few new sentences to <code>en-pos-maxent.bin</code> </p>\n\n<p>This is how my training file looks:</p>\n\n<pre><code>Where_WRB is_VBZ the_DT Seven_DNNP Dwarfs_DNNP Mine_DNNP Train_DNNP ?_?\nWhere_WRB is_VBZ the_DT Astro_DNNP Orbiter_DNNP ?_?\nWhere_WRB is_VBZ the_DT Barnstormer_DNNP  ?_?\nWhere_WRB is_VBZ the_DT Big_DNNP Thunder_DNNP Mountain_DNNP Railroad_DNNP  ?_?\nWhere_WRB is_VBZ the_DT Buzz_DNNP Lightyears_DNNP Space_DNNP Ranger_DNNP Spin_DNNP  ?_?\nWhere_WRB is_VBZ the_DT Casey_DNNP Jr_DNNP Splash_DNNP N_DNNP Soak_DNNP Station_DNNP  ?_?\nWhere_WRB is_VBZ the_DT Cinderella_DNNP Castle_DNNP  ?_?\nWhere_WRB is_VBZ the_DT Country_DNNP Bear_DNNP Jamboree_DNNP  ?_?\nWhere_WRB is_VBZ the_DT Dumbo_DNNP the_DNNP Flying_DNNP Elephant_DNNP  ?_?\nWhere_WRB is_VBZ the_DT Enchanted_DNNP Tales_DNNP with_DNNP Belle_DNNP  ?_?\nWhere_WRB is_VBZ the_DT Frontierland_DNNP Shootin_DNNP Arcade_DNNP  ?_?\n</code></pre>\n\n<p>After training the model, all words except those in the training file are tagged as <code>DNNP</code>.\nFor example, if I ask for the word 'Where' (present in the training file) to be tagged, the answer is <code>WRB</code>, but if I ask the word 'hello' (not present in the training file) to be tagged, it is tagged as <code>DNNP</code>.  So I want to add a few words. How can I do that?</p>\n",
    "score": 4,
    "creation_date": 1417719410,
    "view_count": 1095,
    "answer_count": 2,
    "tags": "nlp;text-mining;opennlp;pos-tagger"
  },
  {
    "question_id": 26879761,
    "title": "What is pos of `r` or `s` in Wordnet via NLTK",
    "body": "<p>I am accessing Wordnet via nltk. If I look up the word big, I find some unknown parts of speech like s or r. The <a href=\"http://www.nltk.org/howto/wordnet.html\" rel=\"nofollow\">docs</a> say \"a synset is identified with a 3-part name of the form: word.pos.nn.\" The WordNet does not seem to mention a part of speech that could be shortened to s or r. So what are these synsets? </p>\n\n<pre><code>$wordnet.synsets('big')\n[Synset('large.a.01'), Synset('big.s.02'), Synset('bad.s.02'), Synset('big.s.04'), Synset('big.s.05'), Synset('big.s.06'), Synset('boastful.s.01'), Synset('big.s.08'), Synset('adult.s.01'), Synset('big.s.10'), Synset('big.s.11'), Synset('big.s.12'), Synset('big.s.13'), Synset('big.r.01'), Synset('boastfully.r.01'), Synset('big.r.03'), Synset('big.r.04')]\n</code></pre>\n",
    "score": 4,
    "creation_date": 1415769373,
    "view_count": 1516,
    "answer_count": 1,
    "tags": "nlp;nltk;wordnet"
  },
  {
    "question_id": 21357881,
    "title": "Custom NER and POS tagging",
    "body": "<p>I was checking out Stanford CoreNLP in order to understand NER and POS tagging. But what if I want to create custom tags for entities like<code>&lt;title&gt;Nights&lt;/title&gt;, &lt;genre&gt;Jazz&lt;/genre&gt;, &lt;year&gt;1992&lt;/year&gt;</code> How can I do it? is CoreNLP useful in this case?</p>\n",
    "score": 4,
    "creation_date": 1390694582,
    "view_count": 2496,
    "answer_count": 2,
    "tags": "nlp;stanford-nlp;named-entity-recognition;pos-tagger"
  },
  {
    "question_id": 21091224,
    "title": "NLP to classify/label the content of a sentence (Ruby binding necesarry)",
    "body": "<p>I am analysing a few million emails. My aim is to be able to classify then into groups. Groups could be e.g.:</p>\n\n<ul>\n<li><strong>Delivery problems</strong> (slow delivery, slow handling before dispatch, incorrect availability information, etc.)</li>\n<li><strong>Customer service problems</strong> (slow email response time, impolite response, etc.)</li>\n<li><strong>Return issues</strong> (slow handling of return request, lack of helpfulness from the customer service, etc.)</li>\n<li><strong>Pricing complaint</strong> (hidden fee's discovered, etc.)</li>\n</ul>\n\n<p>In order to perform this classification, I need a NLP that can recognize the combination of word groups like:</p>\n\n<ul>\n<li><em>\"[they|the company|the firm|the website|the merchant]\"</em></li>\n<li><em>\"[did not|didn't|no]\"</em></li>\n<li><em>\"[response|respond|answer|reply]\"</em></li>\n<li><em>\"[before the next day|fast enough|at all]\"</em></li>\n<li>etc.</li>\n</ul>\n\n<p>A few of these exemplified groups in combination should then match sentences like:</p>\n\n<ul>\n<li>\"They didn't respond\"</li>\n<li>\"They didn't respond at all\"</li>\n<li>\"There was no response at all\"</li>\n<li>\"I received no response from the website\"</li>\n</ul>\n\n<p>And then classify the sentence as <strong>Customer service problems</strong>.</p>\n\n<p>Which NLP would be able to handle such a task? From what I read these are the most relevant:</p>\n\n<ul>\n<li>Stanford CoreNLP</li>\n<li>OpenNLP</li>\n</ul>\n\n<p>Check also <a href=\"https://stackoverflow.com/questions/999410/natural-language-processing-in-ruby/10056667#10056667\">these suggested NLP's</a>.</p>\n",
    "score": 4,
    "creation_date": 1389615771,
    "view_count": 1616,
    "answer_count": 2,
    "tags": "ruby;nlp;stanford-nlp;opennlp;text-analysis"
  },
  {
    "question_id": 16740154,
    "title": "Labelled LDA usage",
    "body": "<p>I am working on a project which requires applying the topic model LDA. Because each document in my case is short, I have to use Labelled LDA. I do not have much knowledge in this area, and all I need to do is to apply the LLDA to my data. </p>\n\n<p>After searching on web I found an LLDA implementation on <a href=\"http://nlp.stanford.edu/software/tmt/tmt-0.4/\" rel=\"nofollow\">Stanford TMT</a>. What I understand from section <em>Training a Labeled LDA model</em> is: I should label each input document before training. Am I misunderstanding something?</p>\n\n<p>If my understanding is correct, this will involves too much work on labeling documents. Instead, can I provide a separate list of topics, and train the documents without labels?</p>\n",
    "score": 4,
    "creation_date": 1369415015,
    "view_count": 3899,
    "answer_count": 1,
    "tags": "machine-learning;nlp;lda;topic-modeling"
  },
  {
    "question_id": 15184655,
    "title": "Which gensim corpora class should I use to load an LDA transformed corpus? - Python",
    "body": "<p><strong>How do I load an LDA transformed corpus from python's <code>gensim</code> ?</strong> What i've tried:</p>\n\n<pre><code>from gensim import corpora, models\nimport numpy.random\nnumpy.random.seed(10)\n\ndoc0 = [(0, 1), (1, 1)]\ndoc1 = [(0,1)]\ndoc2 = [(0, 1), (1, 1)]\ndoc3 = [(0, 3), (1, 1)]\n\ncorpus = [doc0,doc1,doc2,doc3]\ndictionary = corpora.Dictionary(corpus)\n\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\ncorpus_tfidf.save('x.corpus_tfidf')\n\n# To access the tfidf fitted corpus i've saved i used corpora.MmCorpus.load()\ncorpus_tfidf = corpora.MmCorpus.load('x.corpus_tfidf')\n\nlda = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)\ncorpus_lda = lda[corpus]\ncorpus_lda.save('x.corpus_lda')\n\nfor i,j in enumerate(corpus_lda):\n  print j, corpus[i]\n</code></pre>\n\n<p>The above code will output:</p>\n\n<pre><code>[(0, 0.54259038344543631), (1, 0.45740961655456358)] [(0, 1), (1, 1)]\n[(0, 0.56718063124157458), (1, 0.43281936875842542)] [(0, 1)]\n[(0, 0.54255407573666647), (1, 0.45744592426333358)] [(0, 1), (1, 1)]\n[(0, 0.75229707773868093), (1, 0.2477029222613191)] [(0, 3), (1, 1)]\n\n# [(&lt;topic_number_from x.corpus_lda model&gt;, \n#   &lt;probability of this topic for this document&gt;), \n#  (&lt;topic# from lda model&gt;, &lt;prob of this top for this doc&gt;)] [&lt;document[i] from corpus&gt;]\n</code></pre>\n\n<p><strong>If i want to load the saved LDA transformed corpus, which class from <code>gensim</code> should i be using to load?</strong></p>\n\n<p>I have tried using <code>corpora.MmCorpus.load()</code>, it doesn't give me the same output of the transformed corpus as shown above:</p>\n\n<pre><code>&gt;&gt;&gt; lda_corpus = corpora.MmCorpus.load('x.corpus_lda')\n&gt;&gt;&gt; for i,j in enumerate(lda_corpus):\n...   print j, corpus[i]\n... \n[(0, 0.55087839240547309), (1, 0.44912160759452685)] [(0, 1), (1, 1)]\n[(0, 0.56715974584850259), (1, 0.43284025415149735)] [(0, 1)]\n[(0, 0.54275680271070581), (1, 0.45724319728929413)] [(0, 1), (1, 1)]\n[(0, 0.75233330695720912), (1, 0.24766669304279079)] [(0, 3), (1, 1)]\n</code></pre>\n",
    "score": 4,
    "creation_date": 1362306055,
    "view_count": 3303,
    "answer_count": 2,
    "tags": "python;nlp;corpus;lda;gensim"
  },
  {
    "question_id": 15183685,
    "title": "Python - Regex &quot;Machine Learning&quot;",
    "body": "<p>I have thousands of lines of text where I need to find money-representations e.g.:</p>\n\n<pre><code>Lorem ipsum dolor sit amet, 100.000,00 USD sadipscing elitr, sed diam nonumy eirmod \nGBP 400 ut labore et dolore magna aliquyam erat, sed diam voluptua. At USD 20 eos et \naccusam et justo duo dolores et 100,000.00 USD  ea rebum. Stet 3,-- USD gubergren, no \n</code></pre>\n\n<p>The Python script should return the amount converted to USD. (e.g. 100000USF, 400 GBP -> USD, etc) </p>\n\n<p>What I did so far was manually creating Regular expressions for number - currency combinations to retreive the value, then compare the currency against a database and calculate the exchange.</p>\n\n<p>However, this is neither efficient nor future proof (e.g. if another currency is added)\nSo I'm wondering wether there is an efficient machine learning algorithm that I could \"train\" with some examples and it then tries to find sich \"value - currency\" combinations?</p>\n",
    "score": 4,
    "creation_date": 1362297862,
    "view_count": 1865,
    "answer_count": 3,
    "tags": "python;regex;machine-learning;nlp"
  },
  {
    "question_id": 12917928,
    "title": "Calculate correlation coefficient between words?",
    "body": "<p>For a text analysis program, I would like to analyze the co-occurrence of certain words in a text. For example, I would like to see that e.g. the words \"Barack\" and \"Obama\" appear more often together (i.e. have a positive correlation) than others. </p>\n\n<p>This does not seem to be that difficult. However, to be honest, I only know how to calculate the correlation between two numbers, but not between two words in a text. </p>\n\n<ol>\n<li>How can I best approach this problem?</li>\n<li>How can I calculate the correlation between words?</li>\n</ol>\n\n<p><em>I thought of using conditional probabilities, since e.g. Barack Obama is much more probable than Obama Barack; however, the problem I try to solve is much more fundamental and does not depend on the ordering of the words</em></p>\n",
    "score": 4,
    "creation_date": 1350400535,
    "view_count": 8891,
    "answer_count": 4,
    "tags": "math;statistics;correlation;text-analysis"
  },
  {
    "question_id": 12525980,
    "title": "Finding similar/related texts algorithms",
    "body": "<p>I searched a lot in stackoverflow and Google but I didn't find the best answer for this.\nActually, I'm going to develop a news reader system that crawl and collect news from web (with a crawler) and then, I want to find similar or related news in websites (In order to prevent showing duplicated news in website)</p>\n\n<p>I think the best live example for that is Google News, it collect news from web and then categorize and find related news and articles. This is what I want to do.</p>\n\n<p>What's the best algorithm for doing this?</p>\n",
    "score": 4,
    "creation_date": 1348212368,
    "view_count": 1468,
    "answer_count": 2,
    "tags": "nlp;artificial-intelligence;similarity"
  },
  {
    "question_id": 12053241,
    "title": "Sentence comparison with NLP",
    "body": "<p>I used lingpipe for sentence detection but I don't have any idea if there is a better tool. As far as I have understood, there is no way to compare two sentences and see if they mean the same thing. </p>\n\n<p>Is there anyother good source where I can have a pre-built method for comparing two sentences and see if they are similar?</p>\n\n<p>My requirement is as below:</p>\n\n<pre><code>String sent1 = \"Mary and Meera are my classmates.\";\n\nString sent2 = \"Meera and Mary are my classmates.\";\n\nString sent3 = \"I am in Meera and Mary's class.\";\n\n// several sentences will be formed and basically what I need to do is\n// this\n\nboolean bothAreEqual = compareOf(sent1, sent2);\n\nsop(bothAreEqual); // should print true\n\nboolean bothAreEqual = compareOf(sent2, sent3);\n\nsop(bothAreEqual);// should print true\n</code></pre>\n",
    "score": 4,
    "creation_date": 1345545983,
    "view_count": 2614,
    "answer_count": 2,
    "tags": "java;nlp"
  },
  {
    "question_id": 11515339,
    "title": "NLTK stem words produces odd results",
    "body": "<p>After running <code>nltk.stem.porter.PorterStemmer().stem_word(word)</code> I get many words with 'ing' cut off or 'y' swapped with 'i' . e.g. 'Quality' becomes 'Qualiti' and (even stranger) 'value' becomes 'valu'?</p>\n\n<p>As the resulting words are not actual english words, I am not sure how am I meant to use them? My best guess is that I am meant to put the stem words into another function which will give me all the derived/child words from this stem (e.g. 'valu' would return <code>['valuing','valued', 'values', ...]</code>. Is there such a function?</p>\n",
    "score": 4,
    "creation_date": 1342493031,
    "view_count": 2140,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 10272417,
    "title": "Checking if word segmentation is possible",
    "body": "<p>This is a follow up question to <a href=\"https://stackoverflow.com/questions/3466972/how-to-split-a-string-into-words-ex-stringintowords-string-into-words/3469228#3469228\">this response</a> and the pseudo-code algorithm that the user posted. I didn't comment on that question because of its age. I am only interested in validating whether or not a string can be split into words. The algorithm doesn't need to actually split the string. This is the response from the linked question:</p>\n\n<blockquote>\n  <p>Let S[1..length(w)] be a table with Boolean entries. S[i] is true if\n  the word w[1..i] can be split. Then set S[1] = isWord(w[1]) and for\n  i=2 to length(w) calculate</p>\n  \n  <p>S[i] = (isWord[w[1..i] or for any j in {2..i}: S[j-1] and\n  isWord[j..i]).</p>\n</blockquote>\n\n<p>I'm translating this algorithm into simple python code, but I'm not sure if I'm understanding it properly. Code:</p>\n\n<pre><code>def is_all_words(a_string, dictionary)):\n    str_len = len(a_string)\n    S = [False] * str_len\n    S[0] = is_word(a_string[0], dictionary)\n    for i in range(1, str_len):\n        check = is_word(a_string[0:i], dictionary)\n        if (check):\n            S[i] = check\n        else:\n            for j in range(1, str_len):\n                check = (S[j - 1] and is_word(a_string[j:i]), dictionary)\n                if (check):\n                    S[i] == True\n                    break\n    return S\n</code></pre>\n\n<p>I have two related questions. 1) Is this code a proper translation of the linked algorithm into Python, and if it is, 2) Now that I have S, how do I use it to tell if the string <em>is</em> only comprised of words? In this case, <code>is_word</code> is a function that simply looks a given word up in a list. I haven't implemented it as a trie yet. </p>\n\n<p>UPDATE: After updating the code to include the suggested change, it doesn't work. This is the updated code:</p>\n\n<pre><code>def is_all_words(a_string, dictionary)):\n    str_len = len(a_string)\n    S = [False] * str_len\n    S[0] = is_word(a_string[0], dictionary)\n    for i in range(1, str_len):\n        check = is_word(a_string[0:i], dictionary)\n        if (check):\n            S[i] = check\n        else:\n            for j in range(1, i): #THIS LINE WAS UPDATED\n                check = (S[j - 1] and is_word(a_string[j:i]), dictionary)\n                if (check):\n                    S[i] == True\n                    break\n    return S\n\na_string = \"carrotforever\"\nS = is_all_words(a_string, dictionary)\nprint(S[len(S) - 1]) #prints FALSE\n\na_string = \"hello\"\nS = is_all_words(a_string, dictionary)\nprint(S[len(S) - 1]) #prints TRUE\n</code></pre>\n\n<p>It should return <code>True</code> for both of these. </p>\n",
    "score": 4,
    "creation_date": 1335131386,
    "view_count": 2266,
    "answer_count": 3,
    "tags": "python;algorithm;nlp;dynamic-programming;text-segmentation"
  },
  {
    "question_id": 10041290,
    "title": "Detect whether the word you is a subject or object pronoun based on sentence context.",
    "body": "<p>Ideally using regex, in python. I'm making a simple chatbot, and it's currently having problems responding to phrases like \"I love you\" correctly (it'll throw back \"You love I\" out of the grammar handler, when it should be giving back \"You love me\").</p>\n\n<p>In addition, I'd like it if you could think of good phrases to throw into this grammar handler, that'd be great. I'd love some testing data. </p>\n\n<p>If there's a good list of transitive verbs out there (something like a \"top 100 used\") it may be acceptable to use that and special case the \"transitive verb + you\" pattern. </p>\n",
    "score": 4,
    "creation_date": 1333702809,
    "view_count": 2085,
    "answer_count": 2,
    "tags": "python;regex;nlp;linguistics"
  },
  {
    "question_id": 9072001,
    "title": "Natural language interface to semantic web",
    "body": "<p>we are working on an educational project one of it's components is a  smart search engine that is placed on top sparql end points like DBpedia , imdb  ...etc </p>\n\n<p>we explored some related work in how to make NLI to semantic web </p>\n\n<ul>\n<li><p>Ginseng: <a href=\"http://www.ifi.uzh.ch/pax/uploads/pdf/publication/106/Bernstein_JenaConf_2006.pdf\" rel=\"nofollow\">A Guided Input Natural\nLanguage Search Engine for Querying\nOntologies</a></p></li>\n<li><p><a href=\"http://www.ifi.uzh.ch/pax/uploads/pdf/publication/843/Kaufmann_ISWC2007.pdf\" rel=\"nofollow\">How Useful Are Natural Language\nInterfaces to the Semantic Web for\nCasual End-Users?</a>-</p></li>\n<li><p>The author of the second paper wrote\nher entire PhD thesis about that\ntopic: Talking to the Semantic Web –\n<a href=\"http://www.ifi.uzh.ch/pax/uploads/pdf/publication/1384/Kaufmann_2007.pdf--\" rel=\"nofollow\">Natural Language Query Interfaces\nfor Casual End-users</a>-</p></li>\n</ul>\n\n<p>still we face some problems in implementations because the papers are too broad and no source code is available . so i wonder if there are some open sources  or Documented projects to learn how to build similar projects . \nor if  i can use similar techniques of  NLI to SQL databases , if so  from where to start.     </p>\n",
    "score": 4,
    "creation_date": 1327963721,
    "view_count": 1296,
    "answer_count": 2,
    "tags": "nlp;sparql;semantic-web"
  },
  {
    "question_id": 8100044,
    "title": "How to use Freebase to label a very large unlabeled NLP dataset?",
    "body": "<p>Vocabulary that I am using:</p>\n\n<p>nounphrase -- A short phrase that refers to a specific person, place, or idea. Examples of different nounphrases include \"Barack Obama\", \"Obama\", \"Water Bottle\", \"Yellowstone National Park\", \"Google Chrome web browser\", etc.</p>\n\n<p>category -- The semantic concept defining which nounphrases belong to it and which ones do not. Examples of categories include, \"Politician\", \"Household items\", \"Food\", \"People\", \"Sports teams\", etc. So, we would have that \"Barack Obama\" belongs to \"Politician\" and \"People\" but does not belong to \"Food\" or \"Sports teams\".</p>\n\n<p>I have a very lage unlabeled NLP dataset consisting of millions of nounphrases. I would like to use Freebase to label these nounphrases. I have a mapping of Freebase types to my own categories. What I need to do is download every single examples for every single Freebase type that I have. </p>\n\n<p>The problem that I face is that need to figure out how to structure this type of query. At a high level, the query should ask Freebase \"what are all of the examples of topic XX?\" and Freebase should respond with \"here's a list of all examples of topic XX.\" I would be very grateful if someone could give me the syntax of this query. If it can be done in Python, that would be awesome :)</p>\n",
    "score": 4,
    "creation_date": 1321045977,
    "view_count": 1773,
    "answer_count": 2,
    "tags": "python;nlp;freebase"
  },
  {
    "question_id": 7783916,
    "title": "Finding word collocations in Java",
    "body": "<p>I am trying to find <a href=\"http://nlp.stanford.edu/fsnlp/promo/colloc.pdf\" rel=\"nofollow\">collocations (PDF)</a> in Java.</p>\n\n<p>I know <a href=\"http://nltk.googlecode.com/svn/trunk/doc/api/nltk.collocations-module.html\" rel=\"nofollow\">NLTK</a> has a collocations module, but do not want to use Jython.</p>\n\n<p>I looked at OpenNLP and GATE, but they did not seem to have a collocation finder.</p>\n\n<p>Does anybody know a free open source collocation finder implemented \nin Java? </p>\n",
    "score": 4,
    "creation_date": 1318763931,
    "view_count": 2011,
    "answer_count": 3,
    "tags": "java;nlp"
  },
  {
    "question_id": 6065067,
    "title": "Looking for a way to check if a word is pronounceable",
    "body": "<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"https://stackoverflow.com/questions/1186213/measure-the-pronounceability-of-a-word\">Measure the pronounceability of a word?</a>  </p>\n</blockquote>\n\n\n\n<p>There are a lot of pronounceable random password generators.\nI am looking for the reverse.\nI like to know if a given word is pronounceable.</p>\n\n<p>Purpose:\nI am looking for a new domain name, you probably have gone though this as well.</p>\n",
    "score": 4,
    "creation_date": 1305840377,
    "view_count": 2025,
    "answer_count": 4,
    "tags": "php;string;nlp"
  },
  {
    "question_id": 5379531,
    "title": "memory usage of a probabilistic parser",
    "body": "<p>I am writing a CKY parser for a Range Concatenation Grammar. I want to use a treebank as grammar, so the grammar will be large. I've written a prototype <a href=\"http://www.nltk.org/download\" rel=\"nofollow\">1</a> in Python and it seems to work well when I simulate a treebank of a couple tens of sentences, but the memory usage is unacceptable. I tried writing it in C++ but so far that has been very frustrating as I have never used C++ before. Here's some data (n is number of sentences the grammar is based on):</p>\n\n<pre><code>n    mem\n9    173M\n18   486M\n36   836M\n</code></pre>\n\n<p>This growth pattern is what is to be expected given the best-first algorithm, but the amount of overhead is what concerns me. The memory usage according to heapy is a factor ten smaller than these numbers, valgrind reported something similar. What causes this discrepancy and is there anything I can do about it in Python (or Cython)? Perhaps it's due to fragmentation? Or maybe it is the overhead of python dictionaries?</p>\n\n<p>Some background: the two important datastructures are the agenda mapping edges to probabilities, and the chart, which is a dictionary mapping nonterminals and positions to edges. The agenda is implemented with a heapdict (which internally uses a dict and a heapq list), the chart with a dictionary mapping nonterminals and positions to edges. The agenda is frequently inserted and removed from, the chart only gets insertions and lookups. I represent edges with tuples like this:</p>\n\n<pre><code>((\"S\", 111), (\"NP\", 010), (\"VP\", 100, 001))\n</code></pre>\n\n<p>The strings are the nonterminal labels from the grammar, the positions are encoded as a bitmask. There can be multiple positions when a constituent is discontinuous. So this edge could be represent an analysis of \"is Mary happy\", where \"is\" and happy\" both belong to the VP. The chart dictionary is indexed by the first element of this edge, (\"S\", 111) in this case. In a new version I tried transposing this representation in the hope that it would save memory due to reuse:</p>\n\n<pre><code>((\"S\", \"NP\", \"VP), (111, 100, 011))\n</code></pre>\n\n<p>I figured that Python would store the first part only once if it would occur in combination  with different positions, although I'm not actually sure this is true. In either case, it didn't seem to make any difference.</p>\n\n<p>So basically what I am wondering is if it is worth pursuing my Python implementation any further, including doing things with Cython and different datastructures, or that writing it from the ground up in C++ is the only viable option.</p>\n\n<p><strong>UPDATE</strong>: After some improvements I no longer have issues with memory usage. I'm working on an optimized Cython version. I'll award the bounty to the most useful suggestion for increasing efficiency of the code. There is an annotated version at <a href=\"http://student.science.uva.nl/~acranenb/plcfrs_cython.html\" rel=\"nofollow\">http://student.science.uva.nl/~acranenb/plcfrs_cython.html</a></p>\n\n<p><a href=\"http://www.nltk.org/download\" rel=\"nofollow\">1</a> <a href=\"https://github.com/andreasvc/disco-dop/\" rel=\"nofollow\">https://github.com/andreasvc/disco-dop/</a>\n-- run test.py to parse some sentences. Requires python 2.6, <a href=\"http://www.nltk.org/download\" rel=\"nofollow\">nltk</a> and <a href=\"http://pypi.python.org/pypi/HeapDict\" rel=\"nofollow\">heapdict</a></p>\n",
    "score": 4,
    "creation_date": 1300720005,
    "view_count": 550,
    "answer_count": 3,
    "tags": "python;memory;nlp;cython"
  },
  {
    "question_id": 4374296,
    "title": "about lda inference",
    "body": "<p>Right now, I'm using LDA topic modelling tool from the MALLET package to do some topic detection on my documents. Everything's fine initially, I got 20 topics from it. However, when I try to infer new document using the model, the result is kinda baffling.</p>\n\n<p>For instance I deliberately run my model over a document that I manually created which contains nothing but keywords from one of the topics \"FLU\", but the topic distributions I got was &lt;0.1 for every topic. I then try the same thing on one of the already sampled document which has a high score of 0.7 for one of the topics. Again the same thing happened.</p>\n\n<p>Can someone give some clue on the reason?</p>\n\n<p>Tried asking on MALLET mailing list but apparently no one has replied.</p>\n",
    "score": 4,
    "creation_date": 1291707253,
    "view_count": 2775,
    "answer_count": 4,
    "tags": "nlp;topic-modeling;mallet"
  },
  {
    "question_id": 3836369,
    "title": "opennlp vs stanford nlptools vs berkeley",
    "body": "<p>Hi the aim is to parse a sizeable corpus like wikipedia to generate the most probable parse tree,and named entity recognition. Which is the best library to achieve this in terms of performance and accuracy?  Has anyone used more than one of the above libraries?</p>\n",
    "score": 4,
    "creation_date": 1285903799,
    "view_count": 4907,
    "answer_count": 3,
    "tags": "parsing;nlp;stanford-nlp;opennlp"
  },
  {
    "question_id": 3259035,
    "title": "Perl and NLP, parse Names out of Biographies",
    "body": "<p>I'm pretty new to NLP in general, but getting really good at Perl, and I was wondering what kind of powerful NLP modules are out there. Basically, I have a file with a bunch of paragraphs, and some of them are people's biographies. So, first I need to look for a person's name, and that helps with the rest of the process later.</p>\n\n<p>So I was roughly starting with something like this:</p>\n\n<pre><code>foreach $PPid (0 .. $PPscalar) {\n$paragraph = @PP[$PPid];\nif ($paragraph =~ /^(\\w+ \\w\\. \\w+|\\w+ \\w+)( also|)( has served| served| worked| joined| currently serves| has| was| is|, )/){\n    $possibleName = $1;\n    $badName = 0;\n    foreach $piece (@pieces){\n    if ($possibleName =~ /$piece/){\n        $badName = 1;\n    }\n    }\n    if ($badName == 0){\n    push @namePile, $possibleName;\n    }\n}\n\n}\n</code></pre>\n\n<p>Because most of the names start at the beginning of the paragraphs. And then I'm looking for keywords that denote action or possession, but right now, that picks up extra junk that is not a name. There has to be a module to do this, right?</p>\n",
    "score": 4,
    "creation_date": 1279220837,
    "view_count": 483,
    "answer_count": 3,
    "tags": "perl;module;nlp"
  },
  {
    "question_id": 3017455,
    "title": "Ngram IDF smoothing",
    "body": "<p>I am trying to use IDF scores to find interesting phrases in my pretty huge corpus of documents.<br>\nI basically need something like Amazon's Statistically Improbable Phrases, i.e. phrases that distinguish a document from all the others<br>\nThe problem that I am running into is that some (3,4)-grams in my data which have super-high idf actually consist of component unigrams and bigrams which have really low idf..<br>\nFor example, \"you've never tried\" has a very high idf, while each of the component unigrams have very low idf..<br>\nI need to come up with a function that can take in document frequencies of an n-gram and all its component (n-k)-grams and return a more meaningful measure of how much this phrase will distinguish the parent document from the rest.<br>\nIf I were dealing with probabilities, I would try interpolation or backoff models.. I am not sure what assumptions/intuitions those models leverage to perform well, and so how well they would do for IDF scores.<br>\nAnybody has any better ideas?</p>\n",
    "score": 4,
    "creation_date": 1276195678,
    "view_count": 1997,
    "answer_count": 1,
    "tags": "machine-learning;nlp;information-retrieval;tf-idf"
  },
  {
    "question_id": 2168793,
    "title": "How can I create my own corpus in the Python Natural Language Toolkit?",
    "body": "<p>I have recently expanded the names corpus in nltk and would like to know how I can turn the two files I have (male.txt, female.txt) in to a corpus so I can access them using the existing nltk.corpus methods. Does anyone have any suggestions?</p>\n\n<p>Many thanks,\nJames.</p>\n",
    "score": 4,
    "creation_date": 1264873725,
    "view_count": 7342,
    "answer_count": 3,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 1696914,
    "title": "Extracting pure content / text from HTML Pages by excluding navigation and chrome content",
    "body": "<p>I am crawling news websites and want to extract News Title, News Abstract (First Paragraph), etc</p>\n\n<p>I plugged into the webkit parser code to easily navigate webpage as a tree. To eliminate navigation and other non news content I take the text version of the article (minus the html tags, webkit provides api for the same). Then I run the diff algorithm comparing various article's text from same website this results in similar text being eliminated. This gives me content minus the common navigation content etc.</p>\n\n<p>Despite the above approach I am still getting quite some junk in my final text. This results in incorrect News Abstract being extracted. The error rate is 5 in 10 article i.e. 50%. Error as in   </p>\n\n<p>Can you </p>\n\n<ol>\n<li><p>Suggest an alternative strategy for extraction of pure content,</p></li>\n<li><p>Would/Can learning Natural Language rocessing help in extracting correct abstract from these articles ? </p></li>\n<li><p>How would you approach the above problem ?.</p></li>\n<li><p>Are these any research papers on the same ?.</p></li>\n</ol>\n\n<p>Regards</p>\n\n<p>Ankur Gupta</p>\n",
    "score": 4,
    "creation_date": 1257694924,
    "view_count": 2699,
    "answer_count": 3,
    "tags": "html;artificial-intelligence;nlp;html-content-extraction;text-extraction"
  },
  {
    "question_id": 742711,
    "title": "Shorten a text and only keep important sentences",
    "body": "<p>The German website nandoo.net offers the possibility to shorten a news article. If you change the percentage value with a slider, the text changes and some sentences are left out.</p>\n\n<p>You can see that in action here:</p>\n\n<blockquote>\n  <p><a href=\"http://www.nandoo.net/read/article/299925/\" rel=\"nofollow noreferrer\">http://www.nandoo.net/read/article/299925/</a></p>\n</blockquote>\n\n<p>The news article is on the left side and tags are marked. The slider is on the top of the second column. The more you move the slider to the left, the shorter the text becomes.</p>\n\n<p>How can you offer something like that? Are there any algorithms which you can use to achieve that?</p>\n\n<p>My idea was that their algorithm counts the number of tags and nouns in a sentence. Then the sentences with fewest number of tags/nouns are left out.</p>\n\n<p>Could that be true? Or do you have another idea?</p>\n\n<p>I hope you can help me. Thanks in advance!</p>\n",
    "score": 4,
    "creation_date": 1239582787,
    "view_count": 3280,
    "answer_count": 2,
    "tags": "algorithm;nlp;semantics"
  },
  {
    "question_id": 79330283,
    "title": "Can&#39;t compile Marian NMT",
    "body": "<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=\"https://marian-nmt.github.io/docs/#installation\" rel=\"nofollow noreferrer\">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>\n<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>\n<p>These are the steps that I followed:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n</code></pre>\n<p>This is the result I had:</p>\n<pre><code>➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2\n</code></pre>\n<p>Please help.</p>\n",
    "score": 4,
    "creation_date": 1736057099,
    "view_count": 65,
    "answer_count": 1,
    "tags": "gcc;cmake;nlp;g++"
  },
  {
    "question_id": 77613507,
    "title": "Training llm for Query Generation in a Graph Database",
    "body": "<p>If I have developed a graph database which has its own query language. I have to find a way to feed llm the graph and then llm should be able to generate the queries of our database.</p>\n<p>I have found something similar in langchain that we can feed it the rdf file and then it will generate the sparql queries.</p>\n<p>So I have many doubts regarding this as I am very new to this:</p>\n<p>Is it possible to train a llm on an entirely new technology like here it is our database. If it is possible then how.</p>\n<p>I know that we have to provide the training data to the llm. So in this case, will it be the dataset with our database queries. If yes , then how many queries we have to provide in a dataset.</p>\n<p>Sorry if the question is not detailed , its only my second time asking here.</p>\n",
    "score": 4,
    "creation_date": 1701869646,
    "view_count": 1365,
    "answer_count": 1,
    "tags": "machine-learning;nlp;sparql;langchain;large-language-model"
  },
  {
    "question_id": 77078671,
    "title": "What&#39;s the difference between PeftModel.from_pretrained &amp; get_peft_model in initiating a peft model?",
    "body": "<p>In the examples from PEFT source code, I found two ways to load the model:</p>\n<pre><code>model = PeftModel.from_pretrained(model, peft_model_id, device_map=&quot;auto&quot;, max_memory=max_memory)\n</code></pre>\n<pre><code>model = get_peft_model(model, peft_config)\n</code></pre>\n<p>Is there any difference between them?</p>\n<p>Im expecting someone gonna help me to understand this</p>\n",
    "score": 4,
    "creation_date": 1694400638,
    "view_count": 2716,
    "answer_count": 1,
    "tags": "nlp;large-language-model;fine-tuning;peft"
  },
  {
    "question_id": 75951190,
    "title": "sentence transformer use of evaluator",
    "body": "<p>I came across <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/sts/training_stsbenchmark_continue_training.py\" rel=\"nofollow noreferrer\">this script</a> which is second link on <a href=\"https://www.sbert.net/examples/training/sts/README.html\" rel=\"nofollow noreferrer\">this page</a>  and <a href=\"https://www.sbert.net/docs/package_reference/SentenceTransformer.html\" rel=\"nofollow noreferrer\">this explanation</a>\nI am using <code>all-mpnet-base-v2</code> (<a href=\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2\" rel=\"nofollow noreferrer\">link</a>) and I am using my custom data</p>\n<p>I am having hard time understanding use of</p>\n<pre><code>evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n    dev_samples, name='sts-dev')\n</code></pre>\n<p>The documentation says:</p>\n<blockquote>\n<p>evaluator – An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.</p>\n</blockquote>\n<p>But in this case, as we are fine tuning on our own examples, <code>train_dataloader</code>has <code>train_samples</code> which has our model sentences and scores.</p>\n<p><strong>Q1. How is <code>train_samples</code> different than <code>dev_samples</code>?</strong></p>\n<p><strong>Q2a: If the model is going to print performance against <code>dev_samples</code> then how is it going to help &quot;<em>to determine the best model that is saved to disc</em>&quot;?</strong></p>\n<p><strong>Q2b: Are we required to run <code>dev_samples</code> against the model saved on the disc and then compare scores?</strong></p>\n<p><strong>Q3. If my goal is to take a single model and then fine tune it, is it okay to skip parameters <code>evaluator</code> and <code>evaluation_steps</code>?</strong></p>\n<p><strong>Q4. How to determine total steps in the model? Do I need to set <code>evaluation_steps</code>?</strong></p>\n<hr />\n<h3>Updated</h3>\n<p>I followed the answer provided by Kyle and have below follow up questions</p>\n<p>In the <code>fit</code> method I used the <code>evaluator</code> and below data was written to a file\n<a href=\"https://i.sstatic.net/9QhYz.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/9QhYz.jpg\" alt=\"enter image description here\" /></a></p>\n<p><strong>Q5. which metric is used to select the best epoch? is it <code>cosine_pearson</code>?</strong></p>\n<p><strong>Q6: why steps are <code>-1</code> in the above output?</strong></p>\n<p><strong>Q7a: how to find steps based upon size of my data, batch size etc.</strong></p>\n<p>Currently i have kept them to 1000. But not sure if that it is too much. I am running for 10 epochs, i have 2509 examples in the training data and batch size is 64.</p>\n<p><strong>Q7b: are my steps going to be 2509/64?</strong> if yes then 1000 seems to be too high number</p>\n",
    "score": 4,
    "creation_date": 1680794562,
    "view_count": 2729,
    "answer_count": 1,
    "tags": "python;nlp;sentence-transformers"
  },
  {
    "question_id": 74386814,
    "title": "Language names of Languages supported by Fasttext",
    "body": "<p>I am trying to find out the names of languages supported by Fasttext's LID tool, given these language codes listed <a href=\"https://fasttext.cc/docs/en/language-identification.html\" rel=\"nofollow noreferrer\">here</a>:</p>\n<pre><code>af als am an ar arz as ast av az azb ba bar bcl be bg bh bn bo bpy br bs bxr ca cbk ce ceb ckb co cs cv cy da de diq dsb dty dv el eml en eo es et eu fa fi fr frr fy ga gd gl gn gom gu gv he hi hif hr hsb ht hu hy ia id ie ilo io is it ja jbo jv ka kk km kn ko krc ku kv kw ky la lb lez li lmo lo lrc lt lv mai mg mhr min mk ml mn mr mrj ms mt mwl my myv mzn nah nap nds ne new nl nn no oc or os pa pam pfl pl pms pnb ps pt qu rm ro ru rue sa sah sc scn sco sd sh si sk sl so sq sr su sv sw ta te tg th tk tl tr tt tyv ug uk ur uz vec vep vi vls vo wa war wuu xal xmf yi yo yue zh\n</code></pre>\n<p>I tried to map the ISO codes to each language, but it seems non-standard, either using ISO-639-1 or ISO-639-3. Does anyone have a list of language names for these codes, or know how to find them?<br />\n<a href=\"https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\" rel=\"nofollow noreferrer\">Wikipedia's list</a> does not cover all of them either, so a manual mapping too did not help.</p>\n<p>UPDATE: Opened an issue on <a href=\"https://github.com/facebookresearch/fastText/issues/1305\" rel=\"nofollow noreferrer\">GitHub</a>.</p>\n",
    "score": 4,
    "creation_date": 1668072666,
    "view_count": 803,
    "answer_count": 1,
    "tags": "python;nlp;fasttext"
  },
  {
    "question_id": 73177807,
    "title": "Unable to build vocab for a torchtext text classification",
    "body": "<p>I'm trying to prepare a custom dataset loaded from a csv file in order to use in a torchtext text binary classification problem. It's a basic dataset with news headlines and a market sentiment label assigned &quot;positive&quot; or &quot;negative&quot;. I've been following some online tutorials on PyTorch to get this far but they've made some significant changes in the latest torchtext package so most of the stuff is out of date.</p>\n<p>Below I've successfully parsed my csv file into a pandas dataframe with two columns - text headline and a label which is either 0 or 1 for positive/negative, split into a training and test dataset then wrapped them as a PyTorch dataset class:</p>\n<pre><code>train, test = train_test_split(eurusd_df, test_size=0.2)\nclass CustomTextDataset(Dataset):\ndef __init__(self, text, labels):\n    self.text = text\n    self.labels = labels\n    \ndef __getitem__(self, idx):\n    label = self.labels.iloc[idx]\n    text = self.text.iloc[idx]\n    sample = {&quot;Label&quot;: label, &quot;Text&quot;: text}\n    return sample\n\ndef __len__(self):\n    return len(self.labels)\ntrain_dataset = CustomTextDataset(train['Text'], train['Labels'])\ntest_dataset = CustomTextDataset(test['Text'], test['Labels'])\n</code></pre>\n<p>I'm now trying to build a vocabulary of tokens following this tutorial <a href=\"https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-simple-guide-to-text-classification\" rel=\"nofollow noreferrer\">https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-simple-guide-to-text-classification</a> and the official pytorch tutorial <a href=\"https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\" rel=\"nofollow noreferrer\">https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html</a> .</p>\n<p>However using the below code</p>\n<pre><code>from torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\ntrain_iter = train_dataset\n\ndef yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n        \nvocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[&quot;&lt;unk&gt;&quot;])\nvocab.set_default_index(vocab[&quot;&lt;unk&gt;&quot;])\n</code></pre>\n<p>yields a very small length of vocabulary, and applying the example <code>vocab(['here', 'is', 'an', 'example'])</code> on a text field taken from the original dataframe yields a list of 0s, implying the vocab is being built from the label field, containing only 0s and 1s, not the text field. Could anyone review and show me how to build the vocab targeting the text field?</p>\n",
    "score": 4,
    "creation_date": 1659202498,
    "view_count": 5847,
    "answer_count": 2,
    "tags": "python;nlp;pytorch;torchtext"
  },
  {
    "question_id": 71711847,
    "title": "Apple&#39;s Natural Language API returns unexpected results",
    "body": "<p><strong>I'm trying to figure out why Apple's Natural Language API returns unexpected results.</strong></p>\n<p>What am I doing wrong? Is it a grammar issue?</p>\n<p>I have the following four strings, and I want to extract each word's &quot;<strong>stem form</strong>.&quot;</p>\n<pre><code>    // text 1 has two &quot;accredited&quot; in a different order\n    let text1: String = &quot;accredit accredited accrediting accredited accreditation accredits&quot;\n    \n    // text 2 has three &quot;accredited&quot; in different order\n    let text2: String = &quot;accredit accredits accredited accrediting accredited accredited accreditation&quot;\n    \n    // text 3 has &quot;accreditation&quot;\n    let text3: String = &quot;accreditation&quot;\n    \n    // text 4 has &quot;accredited&quot;\n    let text4: String = &quot;accredited&quot;\n</code></pre>\n<p>The issue is with the words <strong>accreditation</strong> and <strong>accredited</strong>.</p>\n<p>The word <strong>accreditation</strong> never returned the stem. And <strong>accredited</strong> returns different results based on the words' order in the string, as shown in Text 1 and Text 2 in the attached image.</p>\n<p><a href=\"https://i.sstatic.net/8b4Lg.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/8b4Lg.jpg\" alt=\"enter image description here\" /></a></p>\n<p>I've used the code from <a href=\"https://developer.apple.com/documentation/naturallanguage/identifying_parts_of_speech\" rel=\"nofollow noreferrer\">Apple's documentation</a></p>\n<p>And here is the full code in SwiftUI:</p>\n<pre><code>import SwiftUI\nimport NaturalLanguage\n\nstruct ContentView: View {\n    \n    // text 1 has two &quot;accredited&quot; in a different order\n    let text1: String = &quot;accredit accredited accrediting accredited accreditation accredits&quot;\n    \n    // text 2 has three &quot;accredited&quot; in a different order\n    let text2: String = &quot;accredit accredits accredited accrediting accredited accredited accreditation&quot;\n    \n    // text 3 has &quot;accreditation&quot;\n    let text3: String = &quot;accreditation&quot;\n    \n    // text 4 has &quot;accredited&quot;\n    let text4: String = &quot;accredited&quot;\n    \n    var body: some View {\n        ScrollView {\n            VStack {\n                \n                Text(&quot;Text 1&quot;).bold()\n                tagText(text: text1, scheme: .lemma).padding(.bottom)\n                \n                Text(&quot;Text 2&quot;).bold()\n                tagText(text: text2, scheme: .lemma).padding(.bottom)\n                \n                Text(&quot;Text 3&quot;).bold()\n                tagText(text: text3, scheme: .lemma).padding(.bottom)\n                \n                Text(&quot;Text 4&quot;).bold()\n                tagText(text: text4, scheme: .lemma).padding(.bottom)\n                \n            }\n        }\n    }\n    \n    // MARK: - tagText\n    func tagText(text: String, scheme: NLTagScheme) -&gt; some View {\n        VStack {\n            ForEach(partsOfSpeechTagger(for: text, scheme: scheme)) { word in\n                Text(word.description)\n            }\n        }\n    }\n    \n    // MARK: - partsOfSpeechTagger\n    func partsOfSpeechTagger(for text: String, scheme: NLTagScheme) -&gt; [NLPTagResult] {\n        \n        var listOfTaggedWords: [NLPTagResult] = []\n        let tagger = NLTagger(tagSchemes: [scheme])\n        tagger.string = text\n        \n        let range = text.startIndex..&lt;text.endIndex\n        let options: NLTagger.Options = [.omitPunctuation, .omitWhitespace]\n        \n        tagger.enumerateTags(in: range, unit: .word, scheme: scheme, options: options) { tag, tokenRange in\n            \n            if let tag = tag {\n                let word: String = String(text[tokenRange])\n                let result = NLPTagResult(word: word, tag: tag)\n                \n                //if !word.localizedCaseInsensitiveContains(tag.rawValue) {\n                listOfTaggedWords.append(result)\n                //}\n            }\n            return true\n        }\n        \n        return listOfTaggedWords\n    }\n    \n    // MARK: - NLPTagResult\n    struct NLPTagResult: Identifiable, Equatable, Hashable, Comparable {\n        var id = UUID()\n        var word: String\n        var tag: NLTag?\n        \n        var description: String {\n            var newString: String = &quot;\\(word)&quot;\n            \n            if let tag = tag {\n                newString += &quot; : \\(tag.rawValue)&quot;\n            }\n            \n            return newString\n        }\n        \n        // MARK: - Equatable &amp; Hashable requirements\n        static func == (lhs: Self, rhs: Self) -&gt; Bool {\n            lhs.id == rhs.id\n        }\n        \n        func hash(into hasher: inout Hasher) {\n            hasher.combine(id)\n        }\n        \n        // MARK: - Comparable requirements\n        static func &lt;(lhs: NLPTagResult, rhs: NLPTagResult) -&gt; Bool {\n            lhs.id.uuidString &lt; rhs.id.uuidString\n        }\n    }\n    \n}\n\n// MARK: - Previews\nstruct ContentView_Previews: PreviewProvider {\n    static var previews: some View {\n        ContentView()\n    }\n}\n</code></pre>\n<p>Thanks for your help!</p>\n",
    "score": 4,
    "creation_date": 1648840863,
    "view_count": 394,
    "answer_count": 1,
    "tags": "swift;swiftui;nlp"
  },
  {
    "question_id": 69836422,
    "title": "BERT outputs explained",
    "body": "<p>The keys of the BERT encoder's output are <code>default</code>, <code>encoder_outputs</code>, <code>pooled_output</code> and <code>sequence_output</code></p>\n<p>As far as I can know, <code>encoder_outputs</code> are the output of each encoder, <code>pooled_output</code> is the output of the global context and <code>sequence_output</code> is the output context of each token (correct me if I'm wrong please). But what is <code>default</code>? Can you give me a more detailed explanation of each one?</p>\n<p><a href=\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\" rel=\"nofollow noreferrer\">This is the link to the encoder</a></p>\n",
    "score": 4,
    "creation_date": 1636015277,
    "view_count": 7668,
    "answer_count": 1,
    "tags": "python;tensorflow;deep-learning;nlp;bert-language-model"
  },
  {
    "question_id": 69467309,
    "title": "Restrict Vocab for BERT Encoder-Decoder Text Generation",
    "body": "<p>Is there any way to restrict the vocabulary of the decoder in a Huggingface BERT encoder-decoder model? I'd like to force the decoder to choose from a small vocabulary when generating text rather than BERT's entire ~30k vocabulary.</p>\n",
    "score": 4,
    "creation_date": 1633529248,
    "view_count": 847,
    "answer_count": 1,
    "tags": "nlp;huggingface-transformers;bert-language-model;seq2seq;sentence-transformers"
  },
  {
    "question_id": 69189754,
    "title": "How to find whether a sentence contain a noun using spacy?",
    "body": "<p>Currently doing a project in NLP. I need to find out whether a sentence have a noun in it. How can I achieve this using spacy?</p>\n",
    "score": 4,
    "creation_date": 1631695126,
    "view_count": 921,
    "answer_count": 1,
    "tags": "python;nlp;spacy;dependency-parsing"
  },
  {
    "question_id": 68195257,
    "title": "Find similarties using nlp/spacy",
    "body": "<p>I have a large dataframe to compare with another dataframe and correct the id. I'm gonna illustrate my problem into this simple exp.</p>\n<pre><code>import spacy\nimport pandas as pd\nnlp = spacy.load('en_core_web_sm', disable=['ner'])\nruler = nlp.add_pipe(&quot;entity_ruler&quot;, config={&quot;phrase_matcher_attr&quot;: &quot;LOWER&quot;})\n\ndf = pd.DataFrame({'id':['nan','nan','nan'],\n                    'description':['JOHN HAS 25 YEAR OLD LIVES IN At/12','STEVE has  50 OLD LIVES IN At.14','ALICIE HAS 10 YEAR OLD LIVES IN AT13']})\nprint(df)\n\ndf1 = pd.DataFrame({'id':[1203,1205,1045],\n                    'description':['JOHN HAS 25year OLD LIVES IN At 2','STEVE has  50year OLD LIVES IN At 14','ALICIE HAS 10year OLD LIVES IN At 13']})\nprint(df1)\nage = [&quot;50year&quot;, &quot;25year&quot;, &quot;10year&quot;]\nfor a in age:\n    ruler.add_patterns([{&quot;label&quot;: &quot;age&quot;, &quot;pattern&quot;: a}])\n\nnames = [&quot;JOHN&quot;, &quot;STEVE&quot;, &quot;ALICIA&quot;]\nfor n in names:\n    ruler.add_patterns([{&quot;label&quot;: &quot;name&quot;, &quot;pattern&quot;: n}])\n\nref = [&quot;AT 2&quot;, &quot;At 13&quot;, &quot;At 14&quot;]\nfor r in ref:\n    ruler.add_patterns([{&quot;label&quot;: &quot;ref&quot;, &quot;pattern&quot;: r}])\n#exp to check text difference\ndoc = nlp(&quot;JOHN has 25 YEAR OLD LIVES IN At.12 &quot;)\nfor ent in doc.ents:\n    print(ent, ent.label_)\n</code></pre>\n<p>Actually there is a difference in the text of the two dataframe df and <strong>df1 which is the reference</strong>, as shown in the picture bellow</p>\n<p><a href=\"https://i.sstatic.net/u6605.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/u6605.png\" alt=\"enter image description here\" /></a>\nI dont know how to get similarties 100% in this case.\nI tried to use spacy but i dont how to fix difference and correct the id in df.</p>\n<p><strong>This is my dataframe1:</strong></p>\n<pre><code>   id                           description\n0  nan      STEVE has 50 OLD LIVES IN At.14\n1  nan   JOHN HAS 25 YEAR OLD LIVES IN At/12\n2  nan  ALICIE HAS 10 YEAR OLD LIVES IN AT15\n</code></pre>\n<p><strong>This my reference dataframe:</strong></p>\n<pre><code>     id                           description\n0  1203   STEVEN HAS 25year OLD lives in At 6\n1  1205     JOHN HAS 25year OLD LIVES IN At 2\n2  1045  ALICIE HAS 50year OLD LIVES IN At 13\n3  3045   STEVE HAS 50year OLD LIVES IN At 14\n4  3465  ALICIE HAS 10year OLD LIVES IN At 13\n</code></pre>\n<p><strong>My expected output:</strong></p>\n<pre><code>     id                           description\n0   3045     STEVE has 50 OLD LIVES IN At.14\n1   1205     JOHN HAS 25 YEAR OLD LIVES IN At/12\n2   3465     ALICIE HAS 10year OLD LIVES IN AT15\n</code></pre>\n<p><strong>NB:The sentences are not in the same order / The dataframes don't have equal length</strong></p>\n",
    "score": 4,
    "creation_date": 1625057922,
    "view_count": 1005,
    "answer_count": 4,
    "tags": "python;dataframe;nlp"
  },
  {
    "question_id": 68158379,
    "title": "How to track progress with nlp.pipe function of spacy?",
    "body": "<p>I'm coding with Python and Spacy.\nI want to track the progress of the execution of <code>nlp.pipe(sentences)</code> because it lasts a lot.\nHow to do that?</p>\n<pre><code>nlp = spacy.load('en_core_web_sm')\nsentences = [...]\ndocs = nlp.pipe(sentences, n_process=8)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1624860894,
    "view_count": 3288,
    "answer_count": 2,
    "tags": "python;nlp;pipe;spacy;tracking"
  },
  {
    "question_id": 66906652,
    "title": "How to download hugging face sentiment-analysis pipeline to use it offline?",
    "body": "<p><strong>How to download hugging face sentiment-analysis pipeline to use it offline?</strong> I'm unable to use hugging face sentiment analysis pipeline without internet. How to download that pipeline?</p>\n<p>The basic code for sentiment analysis using hugging face is</p>\n<pre><code>from transformers import pipeline\nclassifier = pipeline('sentiment-analysis') #This code will download the pipeline\nclassifier('We are very happy to show you the 🤗 Transformers library.')\n</code></pre>\n<p>And the output is</p>\n<pre><code>[{'label': 'POSITIVE', 'score': 0.9997795224189758}]\n</code></pre>\n",
    "score": 4,
    "creation_date": 1617288221,
    "view_count": 3728,
    "answer_count": 2,
    "tags": "deep-learning;nlp;huggingface-transformers;huggingface-tokenizers"
  },
  {
    "question_id": 64562582,
    "title": "how can I read ann file provided by brat annotation toll and convert them to dataframe in python?",
    "body": "<p>I am working on the sequence tagging classification based IOB scheme,</p>\n<p>firstly, I  want to kind of read my corpus and their labels, but the corpus has been saved in kind of format  called .ann file  that I have never worked as you here. it annotated using\n<a href=\"https://brat.nlplab.org/\" rel=\"nofollow noreferrer\">https://brat.nlplab.org/</a>\nwhen I oped it i see this</p>\n<pre><code>T1  Claim 78 140    competition can effectively promote the development of economy\nA1  Stance T1 Against\nT2  MajorClaim 503 550  we should attach more importance to cooperation\nT3  Premise 142 283 In order to survive in the competition, companies continue to improve their products and service, and as a result, the whole society prospers\nT4  Claim 591 714   through cooperation, children can learn about interpersonal skills which are significant in the future life of all students\nA2  Stance T4 For\nT5  Premise 716 851 What we acquired from team work is not only how to achieve the same goal with others but more importantly, how to get along with others\nT6  Premise 853 1086    During the process of cooperation, children can learn about how to listen to opinions of others, how to communicate with others, how to think comprehensively, and even how to compromise with other team members when conflicts occurred\nT7  Premise 1088 1191   All of these skills help them to get on well with other people and will benefit them for the whole life\nT8  Claim 1332 1376 competition makes the society more effective\nA3  Stance T8 Against\nT9  Premise 1212 1301   the significance of competition is that how to become more excellence to gain the victory\nT10 Premise 1387 1492   when we consider about the question that how to win the game, we always find that we need the cooperation\nT11 Premise 1549 1846   Take Olympic games which is a form of competition for instance, it is hard to imagine how an athlete could win the game without the training of his or her coach, and the help of other professional staffs such as the people who take care of his diet, and those who are in charge of the medical care\nT12 Premise 1848 1915   The winner is the athlete but the success belongs to the whole team\nT13 Claim 1927 1992 without the cooperation, there would be no victory of competition\nA4  Stance T13 For\nT14 Claim 2154 2231 a more cooperative attitudes towards life is more profitable in one's success\nA5  Stance T14 For\nR1  supports Arg1:T3 Arg2:T1    \nR2  attacks Arg1:T1 Arg2:T2 \nR3  supports Arg1:T5 Arg2:T4    \nR4  supports Arg1:T6 Arg2:T4    \nR5  supports Arg1:T7 Arg2:T4    \nR6  supports Arg1:T9 Arg2:T8    \nR7  supports Arg1:T11 Arg2:T12  \nR8  supports Arg1:T12 Arg2:T13  \nR9  supports Arg1:T10 Arg2:T13  \nR10 supports Arg1:T4 Arg2:T2    \nR11 attacks Arg1:T8 Arg2:T2 \nR12 supports Arg1:T13 Arg2:T2   \nR13 supports Arg1:T14 Arg2:T2   \n\n</code></pre>\n<p>I want to easily decode that, and saved my data as dataframe  in this format:</p>\n<p>sentence with their labels  ( claim or Premise or MAJORCLAIM  , as you see in the text)</p>\n<p>something similar this format</p>\n<p><a href=\"https://i.sstatic.net/EhhNh.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/EhhNh.jpg\" alt=\"enter image description here\" /></a></p>\n<p>sentences with their labels</p>\n<p>I have tried to read  .txt file using this function</p>\n<pre><code>myList = []                #read the whole text from \nfor root, dirs, files in os.walk(path):\n    for file in files:\n        if file.endswith('.txt'):\n            with open(os.path.join(root, file), 'r', encoding=&quot;utf-8&quot;) as f:\n                text = f.read()\n                myList.append(text)\n</code></pre>\n<pre><code>df = pd.DataFrame(np.array(myList),index=list(range(1,len(myList)+1)),columns=[&quot;Paragraph&quot;])\n\n</code></pre>\n<p>but for this ann file provided by brat, I have no idea</p>\n",
    "score": 4,
    "creation_date": 1603831247,
    "view_count": 3945,
    "answer_count": 2,
    "tags": "python;nlp;brat"
  },
  {
    "question_id": 63863161,
    "title": "Detect Tense in German Sentence (with SpaCy)",
    "body": "<p>I would like to (programmatically) detect the tense (and mood) of German sentences, preferably with SpaCy. I am able to find the root in the sentence and to determine whether it is a finite verb or not. However, Searching SpaCy's documentation I didn't find a solution to determine the tense. Is this possible with SpaCy, or do I need to create my own solution for this?</p>\n<p>If it is possible with SpaCy, how?</p>\n<p>If not, what would be a good approach to do this? My first approach would be to discriminate between Perfekt and Plusquamperfekt tense based on the existence of a participle verb form, and to identify Futur by checking if the root is a form of werden and the existence of a dependent infinite verb form, with some extra logic to check for Futur II, analogue to checking for Plusquamperfekt. For discrimination of Präteritum against Präsens I would think of doing a look-up in a verb table. Is that a good idea, or is there a better approach, maybe a prebuilt tool?</p>\n<p>I have found this paper: <a href=\"https://www.cis.uni-muenchen.de/%7Efraser/pubs/ramm_acldemo2017.pdf\" rel=\"nofollow noreferrer\">Annotating tense, mood and voice for English, French and German</a>, but they are not overly explicit how they do it; at least I am unable to reproduce their work.</p>\n",
    "score": 4,
    "creation_date": 1599932462,
    "view_count": 765,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 63837534,
    "title": "How can I use Google Natural Language API to enrich data in a Bigquery table?",
    "body": "<p>I want to use data stored in a BigQuery table as input to <a href=\"https://cloud.google.com/natural-language\" rel=\"nofollow noreferrer\">Google's Natural Language API</a>, perform entity extraction and sentiment analysis, and persist the result back to BigQuery. What tools/services could I use to handle this in GCP? Performance is not a concern, and running this in an overnight batch would be acceptable for this use-case.</p>\n",
    "score": 4,
    "creation_date": 1599771204,
    "view_count": 1190,
    "answer_count": 1,
    "tags": "google-cloud-platform;nlp;google-bigquery;google-cloud-dataflow"
  },
  {
    "question_id": 63320748,
    "title": "Extracting human names from text in NodeJS, NLP",
    "body": "<p>I am new to NLP using node js and I am trying to figure out NLP libraries that are available to figure out if a word is a human name in a given text.</p>\n<p>Any help related to libraries or code samples or any ideas on how to approach this problem in nodeJS is much appreciated.</p>\n<p>Thank you.</p>\n",
    "score": 4,
    "creation_date": 1596925244,
    "view_count": 3525,
    "answer_count": 2,
    "tags": "node.js;nlp"
  },
  {
    "question_id": 61550968,
    "title": "Implementation details of positional encoding in transformer model?",
    "body": "<p>How exactly does this positional encoding being calculated?</p>\n\n<p>Let's assume a machine translation scenario and these are input sentences,</p>\n\n<pre><code>english_text = [this is good, this is bad]\ngerman_text = [das ist gut, das ist schlecht]\n</code></pre>\n\n<p>Now our input vocabulary size is 4 and embedding dimension is 4.</p>\n\n<pre><code>#words     #embeddings\nthis     - [0.5, 0.2, 0.3, 0.1]\nis       - [0.1, 0.2, 0.5, 0.1]\ngood     - [0.9, 0.7, 0.9, 0.1]\nbad      - [0.7, 0.3, 0.4, 0.1]\n</code></pre>\n\n<p>As per transformer paper we add the <strong>each word position encoding</strong> with <strong>each word embedding</strong> and then pass it to encoder like seen in the image below,</p>\n\n<p><a href=\"https://i.sstatic.net/E1aEA.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/E1aEA.jpg\" alt=\"attention is all you need\"></a></p>\n\n<p>As far as the  paper is concerned they given this formula for calculating position encoding of each word,\n<a href=\"https://i.sstatic.net/vN3p5.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/vN3p5.jpg\" alt=\"attention paper\"></a></p>\n\n<p>So, this is how I think I can implement it,</p>\n\n<pre><code>d_model = 4 # Embedding dimension\n\npositional_embeddings = np.zeros((max_sentence_length, d_model))\n\nmax_sentence_length = 3 # as per my examples above\n\nfor position in range(maximum_sentence_length):\n    for i in range(0, d_model, 2):\n       positional_embeddings[position, i] = (\n                                          sin(position / (10000 ** ( (2*i) / d_model) ) )\n                                            )\n       positional_embeddings[position, i + 1] = (\n                                              cos(position / (10000 ** ( (2 * (i + 1) ) / d_model) ) )\n                                                )\n</code></pre>\n\n<p>Then, the new embedding vector will be </p>\n\n<pre><code>[[0.5, 0.2, 0.3, 0.1], \n [0.1, 0.2, 0.5, 0.1], \n [0.9, 0.7, 0.9, 0.1]] + positional_embeddings = NEW EMBEDDINGS\n\n ## shapes\n  3 x 4                + 3 x 4                 = 3 x 4     \n</code></pre>\n\n<p>Is this how the calculation will be carried out in the implementation? Do correct me if there's any mistake in my above pseudo implementation.</p>\n\n<p>If everything is correct then <strong><em>I have three doubts</em></strong> hope someone can clear them,</p>\n\n<p>1) From the above implementation we are using sin formula for even positions and cos formula for odd positions but I couldn't understand the reason behind it? I read that it's taking use of cyclic properties but couldn't understand it.</p>\n\n<p>2) Is there a reason behind choosing <code>10000/(2i/d)</code> or <code>10000/(2i+1/d)</code> as scaling factor in formula.</p>\n\n<p>3) All the sentence will not be equal to max sentence length so we might have to padded the sentence so do we also calculate positional encondings to padding tokens.</p>\n",
    "score": 4,
    "creation_date": 1588367937,
    "view_count": 4535,
    "answer_count": 1,
    "tags": "encoding;deep-learning;nlp;transformer-model;attention-model"
  },
  {
    "question_id": 58519650,
    "title": "SpaCy Rule Based Phrase Matching for Hello World",
    "body": "<p>I am doing ruled based phrase matching in Spacy. I am trying the following example but it is not working. </p>\n\n<p><strong>Example</strong></p>\n\n<pre><code>import spacy\nfrom spacy.matcher import Matcher\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp('Hello world!')\n\npattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n\nmatcher = Matcher(nlp.vocab)\nmatcher.add('HelloWorld', None, pattern)\n\nmatches = matcher(doc)\nprint(matches) \n</code></pre>\n\n<p>then final <code>matches</code> is giving empty string. Would you please correct me?</p>\n",
    "score": 4,
    "creation_date": 1571822806,
    "view_count": 277,
    "answer_count": 2,
    "tags": "python;python-3.x;nlp;spacy"
  },
  {
    "question_id": 57798839,
    "title": "Is it possible to fine tune FastText models",
    "body": "<p>I'm working on a project for text similarity using FastText, the basic example I have found to train a model is:</p>\n\n<pre><code>from gensim.models import FastText\n\nmodel = FastText(tokens, size=100, window=3, min_count=1, iter=10, sorted_vocab=1)\n</code></pre>\n\n<p>As I understand it, since I'm specifying the vector and ngram size,  the model is been trained from scratch here and if the dataset is small I would spect great resutls.</p>\n\n<p>The other option I have found is to load the original Wikipedia model which is a huge file: </p>\n\n<pre><code>from gensim.models.wrappers import FastText\n\nmodel = FastText.load_fasttext_format('wiki.simple')\n</code></pre>\n\n<p>My question is, can I load the Wikipedia or any other model, and fine tune it with my dataset? </p>\n",
    "score": 4,
    "creation_date": 1567660670,
    "view_count": 8033,
    "answer_count": 1,
    "tags": "python;nlp;fasttext"
  },
  {
    "question_id": 57453751,
    "title": "Remove accents and keep under dots in Python",
    "body": "<p>I am working on an NLP task that requires using a corpus of the language called Yoruba. Yoruba is a language that has diacritics (accents) and under dots in its alphabets. For instance, this is a Yoruba string: <code>\"ọmọàbúròẹlẹ́wà\"</code>, and I need to remove the accents and keep the under dots.</p>\n\n<p>I have tried using the <code>unidecode</code> library in Python, but it removes accents and under dots.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import unidecode\nac_stng = \"ọmọàbúròẹlẹ́wà\"\nunac_stng = unidecode.unidecode(ac_stng)\n</code></pre>\n\n<p>I expect the output to be <code>\"ọmọaburoẹlẹwa\"</code>. However, when I used the <code>unidecode</code> library in Python, I got <code>\"omoaburoelewa\"</code>.</p>\n",
    "score": 4,
    "creation_date": 1565558899,
    "view_count": 1420,
    "answer_count": 2,
    "tags": "python;python-3.x;string;nlp;python-unicode"
  },
  {
    "question_id": 55174358,
    "title": "How cosine similarity differs from Okapi BM25?",
    "body": "<p>I'm conducting a research using elasticsearch. I was planning to use cosine similarity but I noted that it is unavailable and instead we have BM25 as default scoring function.</p>\n\n<p>Is there a reason for that? Is cosine similarity improper for querying documents? Why was BM25 chosen as default?\nThanks</p>\n",
    "score": 4,
    "creation_date": 1552613523,
    "view_count": 3065,
    "answer_count": 1,
    "tags": "elasticsearch;nlp;information-retrieval;cosine-similarity"
  },
  {
    "question_id": 53798582,
    "title": "Is Elmo a word embedding or a sentence embedding?",
    "body": "<p>Supposedly, Elmo is a word embedding.\nSo if the input is a sentence or a sequence of words, the output should be a sequence of vectors. Apparently, this is not the case.</p>\n\n<p>The code below uses keras and tensorflow_hub.</p>\n\n<pre><code>a = ['aaa bbbb cccc uuuu vvvv wrwr', 'ddd ee fffff ppppp']\na = np.array(a, dtype=object)[:, np.newaxis]\n#a.shape==(2,1)\n\ninput_text = layers.Input(shape=(1,), dtype=\"string\")\nembedding = ElmoEmbeddingLayer()(input_text)\nmodel = Model(inputs=[input_text], outputs=embedding)\n\nmodel.summary()\n</code></pre>\n\n<p>The class ElmoEmbedding is from <a href=\"https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb\" rel=\"nofollow noreferrer\">https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb</a>.</p>\n\n<pre><code>b = model.predict(a)\n#b.shape == (2, 1024)\n</code></pre>\n\n<p>Apparently, the embedding assigns a 1024-dimensional vector to each sentence. This is confusing.</p>\n\n<p>Thank you.</p>\n",
    "score": 4,
    "creation_date": 1544922263,
    "view_count": 4110,
    "answer_count": 1,
    "tags": "python;tensorflow;keras;nlp"
  },
  {
    "question_id": 53623432,
    "title": "Understanding word embeddings, convolutional layer and max pooling layer in LSTMs and RNNs for NLP Text Classification",
    "body": "<p>Here is my input data:</p>\n\n<pre><code>data['text'].head()\n\n0    process however afforded means ascertaining di...\n1          never occurred fumbling might mere mistake \n2    left hand gold snuff box which capered hill cu...\n3    lovely spring looked windsor terrace sixteen f...\n4    finding nothing else even gold superintendent ...\nName: text, dtype: object\n</code></pre>\n\n<p>And here is the one hot encoded label (multi-class classification where the number of classes = 3)</p>\n\n<pre><code>[[1 0 0]\n [0 1 0]\n [1 0 0]\n ...\n [1 0 0]\n [1 0 0]\n [0 1 0]]\n</code></pre>\n\n<p>Here is what I think happens step by step, please correct me if I'm wrong:</p>\n\n<ol>\n<li><p>Converting my input text <code>data['text']</code> to a bag of indices (sequences)</p>\n\n<pre><code>vocabulary_size = 20000\n\ntokenizer = Tokenizer(num_words = vocabulary_size)\ntokenizer.fit_on_texts(data['text'])\nsequences = tokenizer.texts_to_sequences(data['text'])\n\ndata = pad_sequences(sequences, maxlen=50)\n</code></pre></li>\n</ol>\n\n<p>What is happening is my <code>data['text'].shape</code> which is of shape <code>(19579, )</code> is being converted into an array of indices of shape <code>(19579, 50)</code>, where each word is being replaced by the index found in <code>tokenizer.word_index.items()</code></p>\n\n<ol start=\"2\">\n<li><p>Loading the <code>glove 100d</code> word vector</p>\n\n<pre><code>embeddings_index = dict()\nf = open('/Users/abhishekbabuji/Downloads/glove.6B/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint(embedding_index)\n    {'the': array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n    -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n     0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n    -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n     0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n    -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n     0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n     0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n    -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n    -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n    -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n    -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n    -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n    -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n    -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n     0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n    -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32),\n</code></pre></li>\n</ol>\n\n<p>So what we have now are the word vectors for every word of 100 dimensions. </p>\n\n<ol start=\"3\">\n<li><p>Creating the embedding matrix using the glove word vector </p>\n\n<pre><code>vocabulary_size = 20000\nembedding_matrix = np.zeros((vocabulary_size, 100))\n\nfor word, index in tokenizer.word_index.items():\n    if index &gt; vocabulary_size - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector\n</code></pre></li>\n</ol>\n\n<p>So we now have the a <code>vector</code> of 100 dimensions for EACH of the 20000 words. The </p>\n\n<p>And here is the architecture:</p>\n\n<pre><code>model_glove = Sequential()\nmodel_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\nmodel_glove.add(Dropout(0.5))\nmodel_glove.add(Conv1D(64, 5, activation='relu')) \nmodel_glove.add(MaxPooling1D(pool_size=4))\nmodel_glove.add(LSTM(100))\nmodel_glove.add(Dense(3, activation='softmax'))\nmodel_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_glove.summary())\n</code></pre>\n\n<p>I get </p>\n\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_7 (Embedding)      (None, 50, 100)           2000000   \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 50, 100)           0         \n_________________________________________________________________\nconv1d_7 (Conv1D)            (None, 46, 64)            32064     \n_________________________________________________________________\nmax_pooling1d_7 (MaxPooling1 (None, 11, 64)            0         \n_________________________________________________________________\nlstm_7 (LSTM)                (None, 100)               66000     \n_________________________________________________________________\ndense_7 (Dense)              (None, 3)                 303       \n=================================================================\nTotal params: 2,098,367\nTrainable params: 98,367\nNon-trainable params: 2,000,000\n_________________________________________________________________\n</code></pre>\n\n<p>The input to the above architecture will be the training data</p>\n\n<pre><code>array([[    0,     0,     0, ...,  4867,    22,   340],\n       [    0,     0,     0, ...,    12,   327,  2301],\n       [    0,     0,     0, ...,   255,   388,  2640],\n       ...,\n       [    0,     0,     0, ...,    17, 15609, 15242],\n       [    0,     0,     0, ...,  9517,  9266,   442],\n       [    0,     0,     0, ...,  3399,   379,  5927]], dtype=int32)\n</code></pre>\n\n<p>of shape <code>(19579, 50)</code></p>\n\n<p>and labels as one hot encodings..</p>\n\n<p>My trouble is understanding the following what exactly is happening to my <code>(19579, 50)</code> as it goes through each of the following lines:</p>\n\n<pre><code>model_glove = Sequential()\nmodel_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\nmodel_glove.add(Dropout(0.5))\nmodel_glove.add(Conv1D(64, 5, activation='relu')) \nmodel_glove.add(MaxPooling1D(pool_size=4))\n</code></pre>\n\n<p>I understand why we need <code>model_glove.add(Dropout(0.5))</code>, this is to shut down some hidden units with a probability of 0.5 to avoid the model from being overly complex. But I have no idea why we need the <code>Conv1D(64, 5, activation='relu')</code>, the <code>MaxPooling1D(pool_size=4)</code> and how this goes into my <code>model_glove.add(LSTM(100))</code> unit..</p>\n",
    "score": 4,
    "creation_date": 1543969723,
    "view_count": 2940,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;lstm;word-embedding"
  },
  {
    "question_id": 52724444,
    "title": "Need help in creating an appropriate model to predict semantic similarity between two sentences",
    "body": "<p>I am new to ML field and trying my hands on creating a model which will predict semantic similarity between two sentences.\nI am using following approach:</p>\n\n<p>1.Using word2vec model in gensim package vectorise each word present in the sentences in question</p>\n\n<p>2.Calculate the average vector for all words in every sentence/document </p>\n\n<pre><code>import numpy as np\nfrom scipy import spatial\n\nindex2word_set = set(model.wv.index2word)\n\ndef avg_feature_vector(sentence, model, num_features, index2word_set):\n    words = sentence.split()\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n        if word in index2word_set:\n            n_words += 1\n            feature_vec = np.add(feature_vec, model[word])\n    if (n_words &gt; 0):\n        feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec\n</code></pre>\n\n<p>3.Next calculate cosine similarity between these two average vectors</p>\n\n<pre><code>s1_afv = avg_feature_vector('this is a sentence', model=model, \nnum_features=300, index2word_set=index2word_set)\ns2_afv = avg_feature_vector('this is also sentence', model=model, \nnum_features=300, index2word_set=index2word_set)\nsim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\nprint(sim)\n</code></pre>\n\n<p>Reference stackoverflow question:\n<a href=\"https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt\">How to calculate the sentence similarity using word2vec model of gensim with python</a></p>\n\n<p>Help needed for the following challenge:</p>\n\n<p>As I want to create a model which would predict semantic similarity between two sentences, I am not quite sure about:</p>\n\n<p>1.Which model would be best suited for this problem</p>\n\n<p>2.Next more importantly how to train that model? </p>\n\n<p>Should I create a matrix where each row will contain two sentences: \nsen1 and sen2 and I would vectorise them and calculate cosine similarity(as per the above mentioned approach)</p>\n\n<p>Then for training data:</p>\n\n<p>X_Train: avg vectors for sen1 and sen2 and their cosine similarity value</p>\n\n<p>y_Train(prediction) : a set of binary values(1 or similar if cosine similarity > 0.7 and 0 otherwise)</p>\n\n<p>I am quite confused whether my approach is correct and how to put a proper approach in the form of a working codebase.</p>\n\n<p>Internet and materials available online are my only teachers to learn ML; thus requesting your guidance in help clearing my gap in understanding and help in coming up with a good working model for my problem.</p>\n",
    "score": 4,
    "creation_date": 1539098521,
    "view_count": 1110,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;data-modeling;word2vec"
  },
  {
    "question_id": 51394206,
    "title": "How to iterate TfidfVectorizer() on pandas dataframe",
    "body": "<p>I have a large pandas dataframe with 10 million records of news articles. So, this is how I have applied <code>TfidfVectorizer</code>.</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nfeature_matrix = tfidf.fit_transform(df['articles'])\n</code></pre>\n\n<p>It took alot of time to process all documents. All I wants to iterate each article in dataframe one at a time or is it possible that I can pass documents in chunks and it keep updating existing vocabulary without overwriting old dictionary of vocabulary? </p>\n\n<p>I have gone through this SO <a href=\"https://stackoverflow.com/questions/16453855/tfidfvectorizer-for-corpus-that-cannot-fit-in-memory\">post</a> but not exactly getting how to applied it on pandas. I have also heard about <code>Python generators</code> but not exactly whether its useful here.</p>\n",
    "score": 4,
    "creation_date": 1531891926,
    "view_count": 2152,
    "answer_count": 1,
    "tags": "python-3.x;pandas;scikit-learn;nlp;tfidfvectorizer"
  },
  {
    "question_id": 50171512,
    "title": "How to predict probability of a sentence?",
    "body": "<p>How to determine the probability of a sentence \"what is a cat\" ? with associated PCFG :</p>\n\n<pre><code>Rule , Probability\nS -&gt; NP VB\nNN -&gt; CAT , 1\nDT -&gt; what , 1\nVB-&gt;is , .5\nVB-&gt;be , .5\n</code></pre>\n\n<p>How can this pcfg with sentence be represented as hidden markov model ?</p>\n\n<p>Each node in the model is \"what\" , \"is\" , \"a\" , \"cat\" ? , how to model the probability connections between the nodes from PCFG ?</p>\n",
    "score": 4,
    "creation_date": 1525425777,
    "view_count": 1815,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;probability;hidden-markov-models;markov-models"
  },
  {
    "question_id": 49745192,
    "title": "Correct gradients for word2vec negative sampling skip gram model",
    "body": "<p>I am trying to implement skip-gram word2vec in python using negative sampling.\nFrom my understanding, I should be maximizing the equation (4) from the paper by <a href=\"https://arxiv.org/pdf/1310.4546.pdf\" rel=\"nofollow noreferrer\">Mikolov Et al</a>. </p>\n\n<p>I have taken the gradients of this equation with respect to Vc, U, and U_rand. Where Vc is the center vector corresponding to the center word, U is the context vector corresponding to a word in the context of the center word and U_rand is the context vector of a randomly sampled word.</p>\n\n<p>I am then calculating the cost function for each combination of word and context word adding them up and printing out the total sum of the whole corpus. I am running this a few times and I do not see an improvement on the whole corpus sum of costs. The cost goes up and then down repeatedly.  </p>\n\n<p>I got the following gradients</p>\n\n<blockquote>\n  <p>grad J with respect to Vc = (1-sigma(V•U))*U - Summation over random\n  vectors (1-sigma(-V•U_rand))*U_rand</p>\n  \n  <p>grad J with respect to U = (1-sigma(V•U))*V</p>\n  \n  <p>grad J with respect to U_rand = (1-sigma(-V•U_rand))*-V</p>\n</blockquote>\n\n<p>So with that being said, I have a few questions: </p>\n\n<ol>\n<li>Are these gradients correct?</li>\n<li>Should I be taking a step in the direction of the gradient? (as opposed to the negative of the gradient) To me, I should be as we are maximizing the cost function</li>\n<li>for the randomly sampled word are we using its center word representation or context work representation. From the Stanford lecture I watched on <a href=\"https://www.youtube.com/watch?v=ASn7ExxLZws&amp;t=3582s\" rel=\"nofollow noreferrer\">youtube</a> it seems to be its context vector. But this <a href=\"https://web.stanford.edu/~jurafsky/slp3/16.pdf\" rel=\"nofollow noreferrer\">source</a> seems to differ. </li>\n<li>Is adding all the cost function results for the whole corpus a valid way to see improvement? (I do not see why not)</li>\n</ol>\n",
    "score": 4,
    "creation_date": 1523333538,
    "view_count": 3839,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;word2vec;word-embedding"
  },
  {
    "question_id": 49161277,
    "title": "How do you use nltk.util.breadth_first to search?",
    "body": "<p>I'm trying to use breadth_first to search for (first) a specific leaf word and then a certain label (NP) in a ParentedTree.  I'd really rather not implement it myself if there's already a method for it. This is what I've tried (including how I made the tree, in case that's where I messed up):</p>\n\n<pre><code>import nltk\nfrom nltk.util import breadth_first\n\ngrammar = nltk.data.load(\"/path/to/grammar.cfg\")\nparser = nltk.parse.EarleyChartParser(grammar)\nsent = \"They are happy people\"\nparse1 = list(parser.parse(sent.split()))\ntree1 = nltk.tree.ParentedTree.convert(parse1[0])\nbf = breadth_first(tree1)\n</code></pre>\n\n<p>This gives me a generator object, but I'm not sure how to use it to search for what I want (the pronoun \"They\").  I tried doing a simple \"for node in bf: print(node)\" and it printed every single letter of the string on a line by itself, repeating forever, until I had to close the window.  </p>\n\n<p>I've read the docs and I've done a lot of googling, but I can't find an example of it actually being used for searching.  What am I doing wrong? </p>\n",
    "score": 4,
    "creation_date": 1520457161,
    "view_count": 469,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 48926938,
    "title": "Error while loading spacy model AttributeError: module &#39;msgpack._unpacker&#39; has no attribute &#39;unpack&#39;",
    "body": "<p>I have a problem while loading model for spacy 2.0.8, but the same happens for previous version 2.0.7. Do you have any ideas what's going on?</p>\n\n<p>Thanks in advance</p>\n\n<pre><code>    nlp = spacy.load('en_core_web_lg', disable=['ner'])\n  File \"/usr/lib64/python3.6/site-packages/spacy/__init__.py\", line 19, in load\n    return util.load_model(name, **overrides)\n  File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 113, in load_model\n    return load_model_from_link(name, **overrides)\n  File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 132, in load_model_from_link\n    return cls.load(**overrides)\n  File \"/usr/lib64/python3.6/site-packages/spacy/data/en_core_web_lg/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 177, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 159, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/usr/lib64/python3.6/site-packages/spacy/language.py\", line 638, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 522, in from_disk\n    reader(path / key)\n  File \"/usr/lib64/python3.6/site-packages/spacy/language.py\", line 625, in &lt;lambda&gt;\n    ('vocab', lambda p: self.vocab.from_disk(p)),\n  File \"vocab.pyx\", line 383, in spacy.vocab.Vocab.from_disk\n  File \"vectors.pyx\", line 372, in spacy.vectors.Vectors.from_disk\n  File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 522, in from_disk\n    reader(path / key)\n  File \"vectors.pyx\", line 350, in spacy.vectors.Vectors.from_disk.load_key2row\n  File \"vectors.pyx\", line 351, in spacy.vectors.Vectors.from_disk.load_key2row\n  File \"/usr/lib/python3.6/site-packages/msgpack_numpy.py\", line 179, in unpack\n    return _unpacker.unpack(stream, encoding=encoding, **kwargs)\nAttributeError: module 'msgpack._unpacker' has no attribute 'unpack'\n</code></pre>\n",
    "score": 4,
    "creation_date": 1519300978,
    "view_count": 2447,
    "answer_count": 3,
    "tags": "python-3.x;nlp;python-3.6;spacy;msgpack"
  },
  {
    "question_id": 48705138,
    "title": "Why adding documents to gensim Dictionary gets slow when reaching 2 million words?",
    "body": "<p>I noticed that when adding documents to a gensim Dictionary, execution time jumps from 0.2s to more than 6s when reaching 2 million words.</p>\n\n<p>The code below is a quick example. I loop through int and add the number to the dictionary at each iteraion.</p>\n\n<pre><code>from gensim import corpora\nimport time\n\n\n\ndict_transcript = corpora.Dictionary()\n\n\nfor i in range(1,10000000):\n\n    start_time = time.time()\n\n    doc = [str(i)]\n\n    dict_transcript.add_documents([doc])\n\n    print(\"Iter \"+str(i)+\" done in \" + str(time.time() - start_time) + ' w/ '+str(len(doc)) + ' words and dico size ' +\n          str(len(dict_transcript)))\n</code></pre>\n\n<p>I do get the following output when reaching 2 million words:</p>\n\n<pre><code>Iter 1999999 done in 0.0 w/ 1 words and dico size 1999999\nIter 2000000 done in 0.0 w/ 1 words and dico size 2000000\nIter 2000001 done in 0.0 w/ 1 words and dico size 2000001\nIter 2000002 done in 7.940511226654053 w/ 1 words and dico size 2000001\n</code></pre>\n\n<p>Is there any reason why? And does anyone know how to bypass that problem?\nI'm using this dictionary on a big corpus that I tokenize into bigrams so I'm expecting the dictionary to be a few million rows.</p>\n\n<p>Many thanks</p>\n",
    "score": 4,
    "creation_date": 1518176848,
    "view_count": 425,
    "answer_count": 1,
    "tags": "python;dictionary;nlp;gensim"
  },
  {
    "question_id": 48244053,
    "title": "PyTorch: Relation between Dynamic Computational Graphs - Padding - DataLoader",
    "body": "<p>As far as I understand, the strength of PyTorch is supposed to be that it works with dynamic computational graphs. In the context of NLP, that means that sequences with variable lengths do not necessarily need to be padded to the same length. But, if I want to use PyTorch DataLoader, I need to pad my sequences anyway because the DataLoader only takes tensors - given that me as a total beginner does not want to build some customized collate_fn.</p>\n\n<p>Now this makes me wonder - doesn’t this wash away the whole advantage of dynamic computational graphs in this context?\nAlso, if I pad my sequences to feed it into the DataLoader as a tensor with many zeros as padding tokens at the end (in the case of word ids), will it have any negative effect on my training since PyTorch may not be optimized for computations with padded sequences (since the whole premise is that it can work with variable sequence lengths in the dynamic graphs), or does it simply not make any difference?</p>\n\n<p>I will also post this question in the PyTorch Forum...</p>\n\n<p>Thanks!</p>\n",
    "score": 4,
    "creation_date": 1515875580,
    "view_count": 737,
    "answer_count": 1,
    "tags": "nlp;deep-learning;padding;pytorch"
  },
  {
    "question_id": 48058189,
    "title": "Difference between dependencies(basic and enhanced) from Stanford CoreNLP?",
    "body": "<p>Difference between basic-dependencies,collapsed-dependencies and collapsed-ccprocessed-dependencies in Stanford CoreNLP and how to use them to understand query?</p>\n",
    "score": 4,
    "creation_date": 1514884022,
    "view_count": 1819,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;chatbot"
  },
  {
    "question_id": 47266183,
    "title": "R: How to create clusters based on row strings",
    "body": "<p>I m trying to create clusters from data based on the string value of each row. I m using the R langage. What I m calling a \"cluster\" is a big thematic (= family) that can define each keywords. I imagine something autogenearated based on the keyword, maybe by using lemmatization or ngram.</p>\n\n<p>For example both keywords \"cloud services\" and \"the cloud service\" should be in the \"service\" cluster.</p>\n\n<p>Here is my input vector:</p>\n\n<pre><code>keywords_df &lt;- c(\"cloud storage\", \"cloud computing\", \"google cloud storage\", \"the cloud service\", \n        \"free cloud storage\", \"what is cloud computing\", \"best cloud storage\",\"cloud computing definition\", \n        \"amazon cloud services\", \"cloud service providers\", \"cloud services\", \"google cloud computing\", \"cloud computing services\", \"benefits of cloud computing\")\n</code></pre>\n\n<p>Here is the expected output dataframe:</p>\n\n<pre><code>| Keyword                   |  Thematic |\n|---------------------------|:---------:|\n|cloud storage              |storage  |\n|cloud computing            |computing|\n|google cloud storage       |storage  |\n|the cloud service          |service  |\n|free cloud storage         |storage  |\n|what is cloud computing    |computing|\n|best cloud storage         |storage  |\n|cloud computing definition |computing|\n|amazon cloud service       |service |\n|cloud service providers        |services |\n|cloud service              |service |\n|google cloud computing     |computing|\n|cloud computing services   |service |\n|benefits of cloud computing|computing|\n</code></pre>\n\n<p>The goal is to clean up the data in the \"keyword\" column and auto extract a kind of lemm or ngram.</p>\n\n<p>Here is what I have done for now :</p>\n\n<ol>\n<li><p>Create the \"Thematic\" column based on keyword column:</p>\n\n<pre><code>keywords_df &lt;- mutate(keywords_df,Thematic=Keyword)\nkeywords_df$Thematic &lt;- as.character(keywords_df$Thematic)\n</code></pre></li>\n<li><p>Remove Stopwords:</p>\n\n<pre><code>stopwords_list&lt;-(c(\"cloud\")) #Remove the main word\nstopwords &lt;- stopwords(kind = \"en\")\nstopwords &lt;- append(stopwords,stopwords_list)\nx  = keywords_df$Thematic        \nx  =  removeWords(x,stopwords)\nkeywords_df$Thematic &lt;- x  \n</code></pre></li>\n</ol>\n",
    "score": 4,
    "creation_date": 1510581934,
    "view_count": 815,
    "answer_count": 1,
    "tags": "r;nlp;n-gram;lemmatization"
  },
  {
    "question_id": 45179185,
    "title": "Stemming full strings on Python",
    "body": "<p>I need to perform stemming on portuguese strings.  To do so, i'm tokening the string using nltk.word_tokenize() function a then stemming each word individually. After that, I rebuild the string. It's working, but not performing well. How can i make it faster? The string length is about 2 million words.</p>\n\n<pre><code>    tokenAux=\"\"\n    tokens = nltk.word_tokenize(portugueseString)\n        for token in tokens:\n            tokenAux = token\n            tokenAux = stemmer.stem(token)    \n            textAux = textAux + \" \"+ tokenAux\n    print(textAux)\n</code></pre>\n\n<p>Sorry for bad english and thanks!</p>\n",
    "score": 4,
    "creation_date": 1500424706,
    "view_count": 6727,
    "answer_count": 3,
    "tags": "python;nlp;nltk;stemming"
  },
  {
    "question_id": 45170093,
    "title": "Latent Dirichlet Allocation with prior topic words",
    "body": "<p><strong>Context</strong></p>\n<p>I'm trying to extract topics from a set of texts using <a href=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"nofollow noreferrer\">Latent Dirichlet allocation</a> from <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\" rel=\"nofollow noreferrer\">Scikit-Learn's decomposition module</a>.\nThis works really well, except for the quality of topic words found/selected.</p>\n<p>In a article by <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2931934\" rel=\"nofollow noreferrer\">Li et al (2017)</a>, the authors describe using prior topic words as input for the LDA. They manually choose 4 topics and the main words associated/belonging to these topics. For these words they set the default value to a high number for the associated topic and 0 for the other topics. All other words (not manually selected for a topic) are given equal values for all topics (1). This matrix of values is used as input for the LDA.</p>\n<p><strong>My question</strong></p>\n<p>How can I create a similar analysis with the LatentDirichletAllocation module from Scikit-Learn using a customized default values matrix (prior topics words) as input?</p>\n<p>(I know there's a <code>topic_word_prior</code> parameter, but it only takes one float instead of a matrix with different 'default values'.)</p>\n",
    "score": 4,
    "creation_date": 1500389161,
    "view_count": 2890,
    "answer_count": 2,
    "tags": "python;scikit-learn;nlp;topic-modeling"
  },
  {
    "question_id": 44360774,
    "title": "How to deactivate the default stop words feature for sklearn TfidfVectorizer",
    "body": "<p>I am trying to get the tf-idf values for Japanese words.\nThe problem I am having is that sklearn TfidfVectorizer removes some Japanese characters, which I want to keep, as stop words. </p>\n\n<p>The following is the example:</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(stop_words = None)\n\nwords_list = [\"歯\",\"が\",\"痛い\"]\ntfidf_matrix =  tf.fit_transform(words_list)\nfeature_names = tf.get_feature_names() \nprint (feature_names)\n</code></pre>\n\n<p>The output is:<code>['痛い']</code></p>\n\n<p>However, I want to keep all those three characters in the list.\nI believe TfidfVectorizer removes characters with length of 1 as stop words.\nHow could I deactivate the default stop words feature and keep all characters?</p>\n",
    "score": 4,
    "creation_date": 1496628493,
    "view_count": 1105,
    "answer_count": 1,
    "tags": "python;machine-learning;scikit-learn;nlp;tf-idf"
  },
  {
    "question_id": 42159641,
    "title": "MetaClass couldn&#39;t create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]",
    "body": "<p>I am using the method described on the stanford CoreNLP page <a href=\"http://stanfordnlp.github.io/CoreNLP/cmdline.html#classpath\" rel=\"nofollow noreferrer\">here.</a></p>\n\n<p>In order to run Stanford CoreNLP from the command line the following command is used :</p>\n\n<pre><code>java -cp \"*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt\n</code></pre>\n\n<p>I have run this command from the distribution directory. However. I am getting the following error : </p>\n\n<pre><code>Exception in thread \"main\" edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl\n    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:40)\n    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:57)\n    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:38)\n    at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.&lt;init&gt;(NumberSequenceClassifier.java:86)\n    at edu.stanford.nlp.ie.NERClassifierCombiner.&lt;init&gt;(NERClassifierCombiner.java:136)\n    at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:121)\n    at edu.stanford.nlp.pipeline.AnnotatorFactories$6.create(AnnotatorFactories.java:273)\n    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:154)\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:150)\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:137)\n    at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1323)\nCaused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]\n    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)\n    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)\n    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)\n    ... 12 more\nCaused by: java.lang.reflect.InvocationTargetException\n    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:466)\n    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)\n    ... 14 more\nCaused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException\n    at de.jollyday.util.CalendarUtil.&lt;init&gt;(CalendarUtil.java:42)\n    at de.jollyday.HolidayManager.&lt;init&gt;(HolidayManager.java:66)\n    at de.jollyday.impl.DefaultHolidayManager.&lt;init&gt;(DefaultHolidayManager.java:46)\n    at edu.stanford.nlp.time.JollyDayHolidays$MyXMLManager.&lt;init&gt;(JollyDayHolidays.java:148)\n    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:466)\n    at java.base/java.lang.Class.newInstance(Class.java:556)\n    at de.jollyday.caching.HolidayManagerValueHandler.instantiateManagerImpl(HolidayManagerValueHandler.java:60)\n    at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:41)\n    at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:13)\n    at de.jollyday.util.Cache.get(Cache.java:51)\n    at de.jollyday.HolidayManager.createManager(HolidayManager.java:168)\n    at de.jollyday.HolidayManager.getInstance(HolidayManager.java:148)\n    at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:57)\n    at edu.stanford.nlp.time.Options.&lt;init&gt;(Options.java:90)\n    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)\n    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.&lt;init&gt;(TimeExpressionExtractorImpl.java:39)\n    ... 19 more\nCaused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException\n    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:532)\n    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:186)\n    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:473)\n    ... 38 more\n</code></pre>\n\n<p>My java version is 1.9 and the CoreNLP which I have downloaded is the latest one,i.e.  3.7.0.</p>\n\n<p>Also, I do not have multiple versions of Stanford CoreNLP installed. Is this problem in the version? Where could I be going wrong? </p>\n",
    "score": 4,
    "creation_date": 1486730380,
    "view_count": 1688,
    "answer_count": 3,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 42101027,
    "title": "keep trailing punctuation in python nltk.word_tokenize",
    "body": "<p>There's a ton available about removing punctuation, but I can't seem to find anything keeping it.</p>\n\n<p>If I do:</p>\n\n<pre><code>from nltk import word_tokenize\ntest_str = \"Some Co Inc. Other Co L.P.\"\nword_tokenize(test_str)\nOut[1]: ['Some', 'Co', 'Inc.', 'Other', 'Co', 'L.P', '.']\n</code></pre>\n\n<p>the last \".\" is pushed into its own token. However, if instead there is another word at the end, the last \".\" is preserved:</p>\n\n<pre><code>from nltk import word_tokenize\ntest_str = \"Some Co Inc. Other Co L.P. Another Co\"\nword_tokenize(test_str)\nOut[1]: ['Some', 'Co', 'Inc.', 'Other', 'Co', 'L.P.', 'Another', 'Co']\n</code></pre>\n\n<p>I'd like this to always perform as the second case. For now, I'm hackishly doing:</p>\n\n<pre><code>from nltk import word_tokenize\ntest_str = \"Some Co Inc. Other Co L.P.\"\nword_tokenize(test_str + \" |||\")\n</code></pre>\n\n<p>since I feel pretty confident in throwing away \"|||\" at any given time, but don't know what other punctuation I might want to preserve that could get dropped. Is there a better way to accomplish this ? </p>\n",
    "score": 4,
    "creation_date": 1486505626,
    "view_count": 3640,
    "answer_count": 2,
    "tags": "python;nlp;nltk;tokenize"
  },
  {
    "question_id": 41680019,
    "title": "How to identify address location from text string php?",
    "body": "<p>I'm trying to identify and extract any input address location (Not limited to US - <a href=\"https://smartystreets.com/features#extract\" rel=\"nofollow noreferrer\">SmartyStreet</a>) from a long string of text using php on my xampp.</p>\n<p>I've read several topics/libraries regarding on how to do this, which revolves around using NLP, Google's Geocoding API and regex to perform the above mentioned task. These 3 links are some plausible link that may help <a href=\"https://stackoverflow.com/questions/14087116/extract-address-from-string\">Link 1</a>, <a href=\"https://gist.github.com/Jonathonbyrd/536049\" rel=\"nofollow noreferrer\">Link 2</a>, <a href=\"https://github.com/openvenues/libpostal\" rel=\"nofollow noreferrer\">Link 3/GitHub Library(Seems Promising)</a>.</p>\n<p>However, I do not know whether these links may be of any help with the implementation? Can anyone help me with it?</p>\n",
    "score": 4,
    "creation_date": 1484581492,
    "view_count": 2972,
    "answer_count": 1,
    "tags": "php;regex;nlp;google-geocoding-api;street-address"
  },
  {
    "question_id": 41241216,
    "title": "Fast way to break a joined string of words into individual words",
    "body": "<p>Say I had this string:</p>\n\n<pre><code>hellohowareyou\n</code></pre>\n\n<p>Is there a fast way to separate this into individual words, so the end result is <code>hello how are you</code>? I can think of several ways, but they would be EXTREMELY slow (first I need to identify each letter against a dictionary, see which letters compose a words, and there would be probably multiple combinations, then I need to decide the most likely combination etc.)</p>\n",
    "score": 4,
    "creation_date": 1482233056,
    "view_count": 1772,
    "answer_count": 3,
    "tags": "python;string;nlp"
  },
  {
    "question_id": 40637537,
    "title": "Dynamic number of topics in topic models",
    "body": "<p>I am new to topic modelling.\nMy aim is to find key topics from a document. I am planning to use lda for the purpose. But in lda the number of topics should be predefined.I believe if a document from some other domain which was not in the training corpus comes,it will not give proper results. Is there any alternative solution? Is my thought is correct?    </p>\n",
    "score": 4,
    "creation_date": 1479314463,
    "view_count": 2125,
    "answer_count": 1,
    "tags": "nlp;lda;gensim;topic-modeling"
  },
  {
    "question_id": 40367944,
    "title": "Simplify a Logic Expression using NLTK",
    "body": "<p>I have a doubt at using Natural Language ToolKit (NLTK). I'm trying to make an app in order to translate a Natural Language Question into it's logic representation, and query to a database.</p>\n\n<p>The result I got after using the simplify() method under nltk.sem.logic package and got the following expression:</p>\n\n<pre><code>exists z2.(owner(fido, z2) &amp; (z0 = z2))\n</code></pre>\n\n<p>But what I need is to simplify it as follow:</p>\n\n<pre><code>owner(fido, z0)\n</code></pre>\n\n<p>Is there another method that could reduce the sentence as I want?</p>\n",
    "score": 4,
    "creation_date": 1478031391,
    "view_count": 1364,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 39618418,
    "title": "Human language based searches in elasticsearch",
    "body": "<p>Is it possible to make elasticsearch understand human languages?</p>\n\n<p>user types \"need a laptop for less than $800 with 8 gb ram\" in the searchbox, elasticsearch understands that and filter laptops that have 8gb ram and less than $800?</p>\n\n<p>Are there any packages for this or elasticsearch supports it naturally? Or if it's theoretically possible, any basic idea to achieve this</p>\n",
    "score": 4,
    "creation_date": 1474465580,
    "view_count": 1660,
    "answer_count": 1,
    "tags": "elasticsearch;nlp;named-entity-recognition;information-extraction"
  },
  {
    "question_id": 39117878,
    "title": "Keras SimpleRNN input shape and masking",
    "body": "<p>Newbie to Keras alert!!!</p>\n\n<p>I've got some questions related to Recurrent Layers in Keras (over theano)</p>\n\n<ol>\n<li>How is the input supposed to be formatted regarding timesteps (say for instance I want a layer that will have 3 timesteps 1 in the future 1 in the past and 1 current) I see some <a href=\"https://stackoverflow.com/questions/36136562/python-keras-simplernn-wrong-number-of-dimensions-on-model-fit\" title=\"input for simpleRNN question\">answers</a> and the API proposing padding and using the embedding layer or to shape the input using a time window (3 in this case) and in any case I can't make heads or tails of the API and SimpleRNN examples are scarce and don't seem to agree.</li>\n<li>How would the input time window formatting work with a masking layer?</li>\n<li>Some related answers propose performing masking with an embedding layer. What does masking have to do with embedding layers anyway, aren't embedding layers basically 1-hot word embeddings? (my application would use phonemes or characters as input)</li>\n</ol>\n",
    "score": 4,
    "creation_date": 1472026364,
    "view_count": 5393,
    "answer_count": 1,
    "tags": "nlp;neural-network;keras"
  },
  {
    "question_id": 34704519,
    "title": "applying word2vec on small text files",
    "body": "<p>I'm totally new to word2vec so please bear it with me. I have a set of text files each containing a set of tweets, between 1000-3000. I have chosen a common keyword (<code>\"kw1\"</code>) and I want to find semantically relevant terms for <code>\"kw1\"</code> using word2vec. For example if the keyword is <code>\"apple\"</code> I would expect to see related terms such as <code>\"ipad\" \"os\" \"mac\"</code>... based on the input file. So this set of related terms for <code>\"kw1\"</code> would be different for each input file as word2vec would be trained on individual files (eg., 5 input files, run word2vec 5 times on each file). </p>\n\n<p>My goal is to find sets of related terms for each input file given the common keyword (<code>\"kw1\"</code>), which would be used for some other purposes. </p>\n\n<p>My questions/doubts are:</p>\n\n<ul>\n<li>Does it make sense to use word2vec for a task like this? is it technically right to use considering the small size of an input file?</li>\n</ul>\n\n<p>I have downloaded the code from code.google.com: <a href=\"https://code.google.com/p/word2vec/\" rel=\"nofollow\">https://code.google.com/p/word2vec/</a> and have just given it a dry run as follows:</p>\n\n<pre><code> time ./word2vec -train $file -output vectors.bin -cbow 1 -size 200 -window 10 -negative 25 -hs 1 -sample 1e-3 -threads 12 -binary 1 -iter 50\n\n./distance vectors.bin \n</code></pre>\n\n<ul>\n<li><p>From my results I saw I'm getting many noisy terms (stopwords) when I'm using the <code>'distance'</code> tool to get related terms to <code>\"kw1\"</code>. So I did remove stopwords and other noisy terms such as user mentions. But I haven't seen anywhere that word2vec requires cleaned input data?</p></li>\n<li><p>How do you choose right parameters? I see the results (from running the <code>distance</code> tool) varies greatly when I change parameters such as <code>'-window'</code>, <code>'-iter'</code>. Which technique should I use to find the correct values for the parameters. (manual trial and error is not possible for me as I'll be scaling up the dataset). </p></li>\n</ul>\n",
    "score": 4,
    "creation_date": 1452422862,
    "view_count": 1126,
    "answer_count": 1,
    "tags": "nlp;text-mining;word2vec"
  },
  {
    "question_id": 34557078,
    "title": "Why nltk.align.bleu_score.bleu gives an error?",
    "body": "<p>I found zero-value when I calculate BLEU score for Chinese sentences.</p>\n\n<p>The candidate sentence is <code>c</code> and two references are <code>r1</code> and <code>r2</code></p>\n\n<pre><code>c=[u'\\u9274\\u4e8e', u'\\u7f8e\\u56fd', u'\\u96c6', u'\\u7ecf\\u6d4e', u'\\u4e0e', u'\\u8d38\\u6613', u'\\u6700\\u5927', u'\\u56fd\\u4e8e', u'\\u4e00\\u8eab', u'\\uff0c', u'\\u4e0a\\u8ff0', u'\\u56e0\\u7d20', u'\\u76f4\\u63a5', u'\\u5f71\\u54cd', u'\\u7740', u'\\u4e16\\u754c', u'\\u8d38\\u6613', u'\\u3002']\n\nr1 = [u'\\u8fd9\\u4e9b', u'\\u76f4\\u63a5', u'\\u5f71\\u54cd', u'\\u5168\\u7403', u'\\u8d38\\u6613', u'\\u548c', u'\\u7f8e\\u56fd', u'\\u662f', u'\\u4e16\\u754c', u'\\u4e0a', u'\\u6700\\u5927', u'\\u7684', u'\\u5355\\u4e00', u'\\u7684', u'\\u7ecf\\u6d4e', u'\\u548c', u'\\u8d38\\u6613\\u5546', u'\\u3002']\n\nr2=[u'\\u8fd9\\u4e9b', u'\\u76f4\\u63a5', u'\\u5f71\\u54cd', u'\\u5168\\u7403', u'\\u8d38\\u6613', u'\\uff0c', u'\\u56e0\\u4e3a', u'\\u7f8e\\u56fd', u'\\u662f', u'\\u4e16\\u754c', u'\\u4e0a', u'\\u6700\\u5927', u'\\u7684', u'\\u5355\\u4e00', u'\\u7684', u'\\u7ecf\\u6d4e\\u4f53', u'\\u548c', u'\\u8d38\\u6613\\u5546', u'\\u3002']\n</code></pre>\n\n<p>The code is： </p>\n\n<pre><code>weights = [0.1, 0.8, 0.05, 0.05]\nprint nltk.align.bleu_score.bleu(c, [r1, r2], weights)\n</code></pre>\n\n<p>But I got a result <code>0</code>. When I step into the <code>bleu</code> process, I found that </p>\n\n<pre><code>try:\n    s = math.fsum(w * math.log(p_n) for w, p_n in zip(weights, p_ns))\nexcept ValueError:\n    # some p_ns is 0\n    return 0\n</code></pre>\n\n<p>The above program goes to <code>except ValueError</code>. However, I don't know why this returns an error. If I try other sentences, I can get a non-zero value. </p>\n",
    "score": 4,
    "creation_date": 1451659208,
    "view_count": 3037,
    "answer_count": 1,
    "tags": "python;nlp;nltk;machine-translation;bleu"
  },
  {
    "question_id": 32253125,
    "title": "How to get word count from TF*IDF value in sklearn",
    "body": "<p>I want to get the count of a word in a given sentence using only tf*idf matrix of a set of sentences. I use TfidfVectorizer from sklearn.feature_extraction.text.</p>\n\n<p>Example : </p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\nsentences = (\"The sun is shiny i like the sun\",\"I have been exposed to sun\")\nvect = TfidfVectorizer(stop_words=\"english\",lowercase=False)\ntfidf_matrix = vect.fit_transform(sentences).toarray()\n</code></pre>\n\n<p>I want to be able to calculate the number of times the term \"sun\" occurs in the first sentence (which is 2) using only tfidf_matrix[0] and probably vect.idf_ .\nI know there are infinite ways to get term frequency and words count but I have a special case where I only have a tf<em>idf matrix.\nI already tried to divide the tf</em>idf value of the word \"sun\" in the first sentence by its idf value to get tf. Then I multiplied tf by the total number of words in the sentence to get the words count. Unfortunately, I get wrong values.</p>\n",
    "score": 4,
    "creation_date": 1440688163,
    "view_count": 6282,
    "answer_count": 1,
    "tags": "python;nlp;scikit-learn;tf-idf"
  },
  {
    "question_id": 31789934,
    "title": "What is the difference between comparable corpus and parallel corpus?",
    "body": "<p>What is the difference between them, and what can they be used for respectively?</p>\n",
    "score": 4,
    "creation_date": 1438612093,
    "view_count": 2870,
    "answer_count": 1,
    "tags": "nlp;corpus"
  },
  {
    "question_id": 30629439,
    "title": "How to determine if a piece of text mentions a product",
    "body": "<p>I'm new to natural language process so I apologize if my question is unclear. I have read a book or two on the subject and done general research of various libraries to figure out how i should be doing this, but I'm not confident yet that know what to do.</p>\n\n<p>I'm playing with an idea for an application and part of it is trying to find product mentions in unstructured text (e.g. tweets, facebook posts, emails, websites, etc.) in real-time. I wont go into what the products are but it can be assumed that they are known (stored in a file or database). Some examples:</p>\n\n<ul>\n<li>\"starting tomorrow, we have 5 boxes of @hersheys snickers available for $5 each - limit 1 pp\" (snickers is the product from the hershey company [mentioned as \"@hersheys\"])</li>\n<li>\"Big news: 12-oz. bottles of Coke and Pepsi on sale starting Fri.\" (coca-cola is the product [aliased as \"coke\"] from coca-cola company and Pepsi is the product from the PepsiCo company)</li>\n<li>\"#OMG, i just bought my dream car. a mustang!!!!\" (mustang is the product from Ford)</li>\n</ul>\n\n<p>So basically, given a piece of text, query the text to see if it mentions a product and receive some indication (boolean or confidence number) that it does mention the product.</p>\n\n<p>Some concerns I have are:</p>\n\n<ul>\n<li>Missing products because of misspellings. I thought maybe i could use a string similarity check to catch these.</li>\n<li>Product names that are also English words or things would get caught. Like mustang the horse versus mustang the car</li>\n<li>Needing to keep a list of alternative names for products (e.g. \"coke\" for \"coco-cola\", etc.)</li>\n</ul>\n\n<p>I don't really know where to start with this but any help would be appreciated. I've already looked at NLTK and SciKit and didn't really gleam how to do this from there. If you know of examples or papers that explain this, links would be helpful. I'm not specific to any language at this point. Java preferably but Python and Scala are acceptable.</p>\n",
    "score": 4,
    "creation_date": 1433361326,
    "view_count": 2896,
    "answer_count": 2,
    "tags": "nlp"
  },
  {
    "question_id": 29848095,
    "title": "How do I group companies having different names but are essentially the same semantically?",
    "body": "<p>I am doing competitor analysis using Open Government Data from UK public sector. But there are some anomalies in my results. When I am grouping the contracts by the company names, there are a lot of issues like companies are misspelt or they vary in their names.e.g HP, Hewlett-Packard, Hewlett-Packard Limited , ibm, ibm UK, ibm UK limited etc. The thing is I already ran my code and fixed the results manually. Now I have changed some parts of the code and need to run it again. But I can't go back doing the same thing again as it's costly. At the moment I am thinking about writing a general rule that will sort these companies alphabetically, and merge them when they match on the first few words. But it's not a full-proof approach as HP and Hewlett-Packard will be different. Has anyone done any similar kind of work before or can reference me to their work please. I would be grateful. Thanks.</p>\n",
    "score": 4,
    "creation_date": 1429880115,
    "view_count": 919,
    "answer_count": 1,
    "tags": "nlp;semantic-analysis;textmatching;record-linkage"
  },
  {
    "question_id": 24990527,
    "title": "How to get Coarse-grained Part of Speech Tags?",
    "body": "<p>I have a data set which is annotated by Collins parser. Right now, I am keeping the POS of each word in the data set as a feature. The problem is that I don't need fine-grained POS. So, I have combined some of the tags. For example, I assume all VBD,VBP,VBZ,VBG under the category of \"Verb\". And for nouns, I assume NNP and NNS as \"Noun\" category.</p>\n\n<p>So, here is the list of POS tags that I have after doing all combinations:</p>\n\n<blockquote>\n  <p>VB, NN, TO, JJ, IN, EX, RB, WP, PRP, MD, UH, WRB, WDT, RP, CD, POS, DT, PRP$, WP$, CC, RBR</p>\n</blockquote>\n\n<p>Now, my question is where can I find a list of coarse-grained POS tags? Is there any standard coarse-grained POS tag list?</p>\n\n<p>In my system, If I don't combine other POS tags, I can get better results. I am wondering if I am allowed to keep my current list? Or should I combine them as well?</p>\n\n<p>Thanks in advance,</p>\n",
    "score": 4,
    "creation_date": 1406533084,
    "view_count": 1668,
    "answer_count": 1,
    "tags": "parsing;nlp;classification;feature-extraction;part-of-speech"
  },
  {
    "question_id": 24723984,
    "title": "How to get phrase-level sentiment from Stanford Core NLP package",
    "body": "<p>This might not be a very relevant question to this community. But I thought it would let me reach out to the wider computer science community and get help.</p>\n\n<p>I am using the Stanford Core NLP package, more specifically the <a href=\"http://nlp.stanford.edu/sentiment/index.html\" rel=\"nofollow noreferrer\">Sentiment module</a> of it. I am getting sentence level sentiment by using the following command. </p>\n\n<pre><code>java -cp stanford-corenlp-3.4.jar:stanford-corenlp-3.4-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-0.23.jar -mx2g edu.stanford.nlp.sentiment.SentimentPipeline -stdin &lt; input.txt\n</code></pre>\n\n<p>But I need the phrase-level sentiment, like we see in the <a href=\"http://nlp.stanford.edu:8080/sentiment/rntnDemo.html\" rel=\"nofollow noreferrer\">online demo</a>. I am not being able to figure out how.</p>\n\n<p>EDIT: </p>\n\n<p>After looking into the source code, I figured that just by adding another argument to the above-mentioned command, it is possible to get sentiment score for each node of the parse tree representation of a sentence. However, this gives only a numeric sentiment score as opposed to a positive/negative sentiment. But I think it is fairly trivial to translate this score to a binary positive/negative sentiment. The command is:</p>\n\n<pre><code>java -cp stanford-corenlp-3.4.jar:stanford-corenlp-3.4-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-0.23.jar -mx2g edu.stanford.nlp.sentiment.SentimentPipeline -stdin -output PENNTREES &lt; input.txt\n</code></pre>\n",
    "score": 4,
    "creation_date": 1405263532,
    "view_count": 1940,
    "answer_count": 1,
    "tags": "java;nlp;stanford-nlp;sentiment-analysis"
  },
  {
    "question_id": 23509481,
    "title": "Tokenizing first and last name as one token",
    "body": "<p>Is is possible to tokenize a text in tokens such that first and last name are combined in one token?\nFor example if my text is:</p>\n\n<pre><code>text = \"Barack Obama is the President\"\n</code></pre>\n\n<p>Then:</p>\n\n<pre><code>text.split()\n</code></pre>\n\n<p>results in: </p>\n\n<pre><code>['Barack', 'Obama', 'is', 'the, 'President']\n</code></pre>\n\n<p>how can I recognize the first and last name? So I get only <code>['Barack Obama', 'is', 'the', 'President']</code> as tokens.</p>\n\n<p>Is there a way to achieve it in Python?</p>\n",
    "score": 4,
    "creation_date": 1399440722,
    "view_count": 1382,
    "answer_count": 2,
    "tags": "python;nlp;tokenize"
  },
  {
    "question_id": 20500176,
    "title": "How can i determine the head word in a Sentence, using NLP?",
    "body": "<p>For example, if I have been given a  sentence:</p>\n\n<blockquote>\n  <p>A British soldier was killed in the fighting in Afghanistan</p>\n</blockquote>\n\n<p>The head word of that sentence is \"killed\".</p>\n\n<p>How can I find it, given the nltk package in Python? I am not talking about stemming, I refer to the head word.</p>\n",
    "score": 4,
    "creation_date": 1386693765,
    "view_count": 4101,
    "answer_count": 1,
    "tags": "python;nlp"
  },
  {
    "question_id": 18884296,
    "title": "How to match words from a list in a huge corpus using regexp (in Perl or *nix terminal)?",
    "body": "<p>from a given noun list in a .txt file, where nouns are separated by new lines, such as this one: </p>\n\n<pre><code>hooligan\nfootball\nbrother\nbollocks\n</code></pre>\n\n<p>...and a separate .txt file containing a series of regular expressions separated by new lines, like this:</p>\n\n<pre><code>[a-z]+\\tNN(S)?\n[a-z]+\\tJJ(S)?\n</code></pre>\n\n<p>...I would like to run the regular expressions through each sentence of a corpus and, every time the regexp matches a pattern, if that pattern contains one of the nouns in the list of nouns, I would like to print that noun in the output and (separated it by tab) the regular expression that matched it. Here is an example of how the resulting output could be:</p>\n\n<pre><code>football    [a-z]+NN(S)?\\'s POS[a-z]+NN(S)?\nhooligan    [a-z]+NN(S)?,,[a-z]+JJ[a-z]+NN(S)?\nhooligan    [a-z]+NN(S)?,,[a-z]+JJ[a-z]+NN(S)?\nfootball    [a-z]+NN(S)?[a-z]+NN(S)?\nbrother [a-z]+PP$[a-z]+NN(S)?\nbollocks    [a-z]+DT[a-z]+NN(S)?\nfootball    [a-z]+NN(s)?(be)VBZnotRB\n</code></pre>\n\n<p>The corpus I would use is huge (tens of GB) and has the following format (each sentence is contained in the tag <code>&lt;s&gt;</code>):</p>\n\n<pre><code>&lt;s&gt;\nHooligans   hooligan    NNS 1   4   NMOD\n,   ,   ,   2   4   P\nunbridled   unbridled   JJ  3   4   NMOD\npassion passion NN  4   0   ROOT\n-   -   :   5   4   P\nand and CC  6   4   CC\nno  no  DT  7   9   NMOD\nexecutive   executive   JJ  8   9   NMOD\nboxes   box NNS 9   4   COORD\n.   .   SENT    10  0   ROOT\n&lt;/s&gt;\n&lt;s&gt;\nHooligans   hooligan    NNS 1   4   NMOD\n,   ,   ,   2   4   P\nunbridled   unbridled   JJ  3   4   NMOD\npassion passion NN  4   0   ROOT\n-   -   :   5   4   P\nand and CC  6   4   CC\nno  no  DT  7   9   NMOD\nexecutive   executive   JJ  8   9   NMOD\nboxes   box NNS 9   4   COORD\n.   .   SENT    10  0   ROOT\n&lt;/s&gt;\n&lt;s&gt;\nPortsmouth  Portsmouth  NP  1   2   SBJ\nbring   bring   VVP 2   0   ROOT\nsomething   something   NN  3   2   OBJ\nentirely    entirely    RB  4   5   AMOD\ndifferent   different   JJ  5   3   NMOD\nto  to  TO  6   5   AMOD\nthe the DT  7   12  NMOD\nPremiership Premiership NP  8   12  NMOD\n:   :   :   9   12  P\nfootball    football    NN  10  12  NMOD\n's  's  POS 11  10  NMOD\npast    past    NN  12  6   PMOD\n.   .   SENT    13  2   P\n&lt;/s&gt;\n&lt;s&gt;\nThis    this    DT  1   2   SBJ\nis  be  VBZ 2   0   ROOT\none one CD  3   2   PRD\nof  of  IN  4   3   NMOD\nBritain Britain NP  5   10  NMOD\n's  's  POS 6   5   NMOD\nmost    most    RBS 7   8   AMOD\nardent  ardent  JJ  8   10  NMOD\nfootball    football    NN  9   10  NMOD\ncities  city    NNS 10  4   PMOD\n:   :   :   11  2   P\nthink   think   VVP 12  2   COORD\nLiverpool   Liverpool   NP  13  0   ROOT\nor  or  CC  14  13  CC\nNewcastle   Newcastle   NP  15  19  SBJ\nin  in  IN  16  15  ADV\nminiature   miniature   NN  17  16  PMOD\n,   ,   ,   18  15  P\nwound   wind    VVD 19  13  COORD\nback    back    RB  20  19  ADV\nthree   three   CD  21  22  NMOD\ndecades decade  NNS 22  19  OBJ\n.   .   SENT    23  2   P\n&lt;/s&gt;\n</code></pre>\n\n<p>I started to work to a script in PERL to achieve my goal, and in order to not run out of memory with such a huge dataset I used the module <a href=\"http://search.cpan.org/~toddr/Tie-File-0.98/lib/Tie/File.pm\" rel=\"nofollow\">Tie::File</a> so that my script would read one line at a time (instead of trying to open the entire corpus file in memory). This would work perfectly with a corpus where each sentence corresponds to one single line, but not in the current case where sentences are spread on more lines and delimited by a tag.</p>\n\n<p>Is there a way to achieve what I want using a combination unix terminal commands (e.g. cat and grep)? Alternatively, which would be the best solution for this issue? (Some code examples would be great).</p>\n",
    "score": 4,
    "creation_date": 1379548966,
    "view_count": 1608,
    "answer_count": 2,
    "tags": "regex;perl;grep;nlp;corpus"
  },
  {
    "question_id": 16216177,
    "title": "Clustering Using Latent Symantic Analysis",
    "body": "<p>Suppose I have a corpus of documents and I run LSA algorithm on it. How can I use the final matrix obtained after applying SVD to semantically cluster all the words appearing in my corpus of documents? Wikipedia says LSA can be used to find relation between terms. Is there any library available in Python which can help me accomplish my task of semantically clustering words based on LSA?</p>\n",
    "score": 4,
    "creation_date": 1366896795,
    "view_count": 2256,
    "answer_count": 1,
    "tags": "python;nlp;cluster-analysis;lsa"
  },
  {
    "question_id": 15992813,
    "title": "Extract clause form sentence",
    "body": "<p>I want to extract subordinate clause,main clause,relative clause,restrictive relative clause,non-restrictive relative clause from sentences but I don't know how doing this work. for example:</p>\n\n<p>\"I first saw her in Paris, where I lived in the early nineties.\"<br>\n[main clause][relative clause]</p>\n\n<p>\"She held out the hand that was hurt.\"<br>\n[main clause][restrictive relative clause]</p>\n\n<p>please help me to do this work?</p>\n",
    "score": 4,
    "creation_date": 1365886379,
    "view_count": 3215,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;opennlp;sentence"
  },
  {
    "question_id": 15563396,
    "title": "(Python Scipy) How to flatten a csr_matrix and append it to another csr_matrix?",
    "body": "<p>I am representing each XML document as a feature matrix in a csr_matrix format. Now that I have around 3000 XML documents, I got a list of csr_matrices. I want to flatten each of these matrices to become feature vectors, then I want to combine all of these feature vectors to form one csr_matrix representing all the XML documents as one, where each row is a document and each column is a feature. </p>\n\n<p>One way to achieve this is through this code </p>\n\n<pre><code>X= csr_matrix([a.toarray().ravel().tolist() for a in ls])\n</code></pre>\n\n<p>where ls is the list of csr_matrices, however, this is highly inefficient, as with 3000 documents, this simply crashes!</p>\n\n<p>In other words, my question is, how to flatten each csr_matrix in that list 'ls' without having to turn it into an array, and how to append the flattened csr_matrices into another csr_matrix.</p>\n\n<p>Please note that I am using python with Scipy</p>\n\n<p>Thanks in advance!</p>\n",
    "score": 4,
    "creation_date": 1363930471,
    "view_count": 3092,
    "answer_count": 1,
    "tags": "python;xml;nlp;scipy;classification"
  },
  {
    "question_id": 15431139,
    "title": "java program to get parse score of a sentence using stanford parser",
    "body": "<p>I am able to get the output of Tags and words for the sentence like \"My name is Rahul.\" as </p>\n\n<blockquote>\n  <p>My/PRP$, name/NN, is/VBZ, Rahul/NNP, ./.]</p>\n</blockquote>\n\n<p>with the program:</p>\n\n<pre><code>LexicalizedParser lp = LexicalizedParser.loadModel(\n    \"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\"\n);\nlp.setOptionFlags(new String[]{\"-maxLength\", \"80\", \"-retainTmpSubcategories\"});\n\nString sent = \"My name is Rahul\";\nTree parse = (Tree) lp.apply(sent);\n\nList taggedWords = parse.taggedYield();\nSystem.out.println(taggedWords);\n</code></pre>\n\n<p>But, I also need to get the parse score of the sentence. Is there any kind of modification that I can do to my program to get the parse score? </p>\n\n<p>Thanks.</p>\n",
    "score": 4,
    "creation_date": 1363346591,
    "view_count": 2181,
    "answer_count": 1,
    "tags": "java;parsing;nlp;stanford-nlp"
  },
  {
    "question_id": 13392791,
    "title": "Reading POS tag models in Android",
    "body": "<p>I have tried doing POS tagging using <a href=\"http://opennlp.sourceforge.net/models-1.5/\" rel=\"nofollow noreferrer\">openNLP POS Models</a> on a normal Java application. Now I would like to implement it on Android platform. I am not sure what is the Android requirement or restrictions as I am not able to read the models (binary file) and execute the POS tagging properly.</p>\n<p>I tried getting the .bin file from external storage as well as putting it in an external libraries but still it couldn't work. These are my codes:</p>\n<pre><code>InputStream modelIn = null;\nPOSModel model = null;\n\nString path = Environment.getExternalStorageDirectory().getPath() + &quot;/TextSumIt/en-pos-maxent.bin&quot;;\n\nmodelIn = new BufferedInputStream( new FileInputStream(path));\nmodel = new POSModel(modelIn);\n</code></pre>\n<p>The error I got:</p>\n<pre><code>11-15 06:39:35.072: W/System.err(565): opennlp.tools.util.InvalidFormatException: The profile data stream has an invalid format!\n11-15 06:39:35.177: W/System.err(565):  at opennlp.tools.dictionary.serializer.DictionarySerializer.create(DictionarySerializer.java:224)\n11-15 06:39:35.177: W/System.err(565):  at opennlp.tools.postag.POSDictionary.create(POSDictionary.java:282)\n11-15 06:39:35.182: W/System.err(565):  at opennlp.tools.postag.POSModel$POSDictionarySerializer.create(POSModel.java:48)\n11-15 06:39:35.182: W/System.err(565):  at opennlp.tools.postag.POSModel$POSDictionarySerializer.create(POSModel.java:44)\n11-15 06:39:35.182: W/System.err(565):  at opennlp.tools.util.model.BaseModel.&lt;init&gt;(BaseModel.java:135)\n11-15 06:39:35.197: W/System.err(565):  at opennlp.tools.postag.POSModel.&lt;init&gt;(POSModel.java:93)\n11-15 06:39:35.197: W/System.err(565):  at com.main.textsumit.SummarizationActivity.postagWords(SummarizationActivity.java:676)\n11-15 06:39:35.205: W/System.err(565):  at com.main.textsumit.SummarizationActivity.generateSummary(SummarizationActivity.java:252)\n11-15 06:39:35.205: W/System.err(565):  at com.main.textsumit.SummarizationActivity.onCreate(SummarizationActivity.java:127)\n</code></pre>\n<p>What is it that cause it not reading the model properly? And how should I resolve this? Please help.</p>\n<p>Thank you.</p>\n",
    "score": 4,
    "creation_date": 1352962511,
    "view_count": 1662,
    "answer_count": 3,
    "tags": "android;machine-learning;nlp;opennlp"
  },
  {
    "question_id": 12295251,
    "title": "NLTK extracting terms of chunker parse tree",
    "body": "<blockquote>\n<p><strong>John Edward Grey</strong> started <strong>running</strong> now that he knows he is <strong>fat</strong></p>\n<p><strong>She</strong> was <strong>listening</strong> to <strong>smack that</strong> by that <strong>awful singer</strong></p>\n</blockquote>\n<p>I want to extract interesting terms from a sentence. I currently use POS tagging to identify grammatical types of each entity. Then I update each token to a counter (with different weights for nouns, verbs and adjectives).</p>\n<p>I now wish to use a chunker for this. I think the <strong>leaf nodes of the parse tree holds all interesting words and phrases</strong>. How do I extract the terms from a chunker output?</p>\n",
    "score": 4,
    "creation_date": 1346917505,
    "view_count": 1329,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 12240566,
    "title": "Natural Language Generation - how to test if it sounds natural",
    "body": "<p>I just have a set of sentences, which I have generated based on painting analysis. However I need to test how natural they sound. Is there any api or application which does this?</p>\n\n<p>I am using the <a href=\"http://nlp.stanford.edu:8080/parser/index.jsp\" rel=\"nofollow\">Standford Parser</a> to give me a breakdown, but this doesn't exactly do the job I want!</p>\n\n<p>Also can one test how similar sentences are? As I randomly generating parts of sentences and want to check the variety of the sentences produced. </p>\n",
    "score": 4,
    "creation_date": 1346628363,
    "view_count": 1378,
    "answer_count": 1,
    "tags": "text;nlp"
  },
  {
    "question_id": 10669069,
    "title": "Is there an existing library or api I can use to separate words in character based languages?",
    "body": "<p>I'm working on a little hobby Python project that involves creating dictionaries for various languages using large bodies of text written in that language. For most languages this is relatively straightforward because I can use the space delimiter between words to tokenize a paragraph into words for the dictionary, but for example, Chinese does not use a space character between words. How can I tokenize a paragraph of Chinese text into words?</p>\n\n<p>My searching has found that this is a somewhat complex problem, so I'm wondering if there are off the shelf solutions to solve this in Python or elsewhere via an api or any other language. This must be a common problem because any search engine made for asian languages would need to overcome this issue in order to provide relevant results.</p>\n\n<p>I tried to search around using Google, but I'm not even sure what this type of tokenizing is called, so my results aren't finding anything. Maybe just a nudge in the right direction would help.</p>\n",
    "score": 4,
    "creation_date": 1337463939,
    "view_count": 402,
    "answer_count": 1,
    "tags": "python;api;unicode;utf-8;nlp"
  },
  {
    "question_id": 10639090,
    "title": "Adding a new line character at the end of the sentence",
    "body": "<p>I have a string which is a fragment of a book (its around 1 chapter) \nthis string is all one line. \nI would like to make a new line at the end of each sentence</p>\n\n<p>I solved it by a not-so-sophisticated code of </p>\n\n<pre><code>text = text.replaceAll(\"\\\\.\",\"\\\\.\\n\"); //same for ? same for !\n</code></pre>\n\n<p>and of course this does not yield very nice results.\nI dont need this to be perfect but the nicer i can get it the better.</p>\n\n<p>I would like at least to check for following before making a new line character:</p>\n\n<pre><code>the word before the . is longer then 2 characters\nthere are no dots before the . in the same \"word\"\nthe character before the . is not a number\nthe character after the dot (and possibly a whitespace after that dot) is not a (\n</code></pre>\n\n<p>Any other suggestions would be really appreciated, along with actual code which will make it happen.</p>\n\n<p>Similar question: \n<a href=\"https://stackoverflow.com/questions/4373612/how-to-parse-text-into-sentences-in-java\">Here</a></p>\n\n<p><strong>Update:</strong></p>\n\n<p>Although not high on my list of priorities because my book doesnt contain a lot of direct quotations nor direct speeches but a rule that handles sentences that are inside those would also be in order so that sentences from the same qoute dont end up on new lines  </p>\n",
    "score": 4,
    "creation_date": 1337269867,
    "view_count": 2658,
    "answer_count": 3,
    "tags": "java;nlp"
  },
  {
    "question_id": 10419437,
    "title": "NLP Library (Subject Extraction+Sentiment Analysis) for a Java-based Web Application",
    "body": "<p>I'm a college student looking for a NLP library to perform subject extraction and sentiment analysis in a Java-based web application for a summer-hobby project.</p>\n\n<p>To give you a little context on what I'm trying to do... I want to build a Java-based web application that will extract subjects out of a Reddit submission's headlines, as well as identify the OP's sentiment for the headline (when possible). </p>\n\n<p>Example Inputs:</p>\n\n<ul>\n<li>Reddit, we took the anti-SOPA petition from 943,702 signatures to\n3,460,313. The anti-CISPA petition is at 691,768, a bill expansively\nworse than SOPA. Please bump it, then let us discuss further measures\nor our past efforts are in vain. We did it before, I'm afraid we are\ncalled on to do it again.</li>\n<li>My friend calls him \"Mr Ridiculously Photogenic Guy\"</li>\n<li>Insanity: CISPA Just Got Way Worse, And Then Passed On Rushed Vote</li>\n</ul>\n\n<p>I'm currently trying out AlchemyAPI, but it sounds like better NLP libraries exist out there. Preferablly, I wouldn't be restricted to a limited number of API requests in a given time period (AlchemyAPI has a quota). I've heard the names of GATE, LingPipe, and OpenNLP - however, I'm unsure whether they fit my needs.</p>\n\n<p>I'm looking for framework/library/api recommendations, or even better, comparisons from experienced users. My experience with NLP is extremely limited, which is why I'm asking for help here (ps: if anyone has any resources for learning more, outside of www.nlp-class.org, please let me know!) :)</p>\n",
    "score": 4,
    "creation_date": 1335982761,
    "view_count": 2252,
    "answer_count": 1,
    "tags": "java;nlp;sentiment-analysis"
  },
  {
    "question_id": 10190633,
    "title": "Regular expression to find syllables in Bengali word",
    "body": "<p>Here is the code:</p>\n\n<pre><code>BanglaAlphabet = {\n    'Consonant'                   : '[\\u0995-\\u09B9\\u09CE\\u09DC-\\u09DF]',\n    'IndependantVowel'            : '[\\u0985-\\u0994]',\n    'DependantVowel'              : '[\\u09BE-\\u09CC\\u09D7]',\n    'Nukta'                       : '[\\u09BC]'\n}\nBanglaWordPattern = ur\"\"\"(\n    ({DependantVowel}{Nukta}{Consonant}) |\n    ({DependantVowel}{Consonant}) |\n    {IndependantVowel} |\n    {Consonant} |\n)+\"\"\".format(**BanglaAlphabet)\nBanglaWordPattern = re.compile(BanglaWordPattern, re.VERBOSE)\n</code></pre>\n\n<p>The matching is done with:</p>\n\n<pre><code>re.match(BanglaWordPattern, w[::-1])\n</code></pre>\n\n<p>This is meant to match a valid Bengali word when matched from right to left.</p>\n\n<p>However, it is matching invalid words, such as োগাড় and িদগ.</p>\n\n<p>What could be the problem?</p>\n\n<h3>Edit</h3>\n\n<p>After numerous corrections as suggested by @GarethRees and @ChrisMorgan, I ended up with:</p>\n\n<pre><code>bangla_alphabet = dict(\n    consonant         = u'[\\u0995-\\u09b9\\u09ce\\u09dc-\\u09df]',\n    independent_vowel = u'[\\u0985-\\u0994]',\n    dependent_vowel   = u'[\\u09be-\\u09cc\\u09d7]',\n    dependent_sign    = u'[\\u0981-\\u0983\\u09cd]',\n    virama            = u'[\\u09cd]'\n)\nbangla_word_pattern = re.compile(ur'''(?:\n    {consonant}\n    ({virama}{consonant})?\n    ({virama}{consonant})?\n    {dependent_vowel}?\n    {dependent_sign}?\n    |\n    {independent_vowel}\n    {dependent_sign}?\n)+\n</code></pre>\n\n<p>The matching is now:</p>\n\n<pre><code>bangla_word_pattern.match(w)\n</code></pre>\n\n<p>This code not only corrects errors, but accounts for more characters and valid constructs than before.</p>\n\n<p>I am happy to report that it is working as expected. As such, this code now serves as a <strong>very basic</strong> regular expression for validating the syntax of Bengali words.</p>\n\n<p>There are several special rules / exceptions not implemented. I will be looking into those and adding them to this basic structure incrementally.</p>\n\n<p>Many ''.format(**bangla_alphabet), re.VERBOSE)</p>\n\n<p>The matching is now:</p>\n\n<p><em>xCodexBlockxPlacexHolderx</em></p>\n\n<p>This code not only corrects errors, but accounts for more characters and valid constructs than before.</p>\n\n<p>I am happy to report that it is working as expected. As such, this code now serves as a <strong>very basic</strong> regular expression for validating the syntax of Bengali words.</p>\n\n<p>There are several special rules / exceptions not implemented. I will be looking into those and adding them to this basic structure incrementally.</p>\n",
    "score": 4,
    "creation_date": 1334663312,
    "view_count": 3242,
    "answer_count": 2,
    "tags": "python;regex;unicode;nlp"
  },
  {
    "question_id": 8135148,
    "title": "How to programmatically determine what language the content of a website is written in",
    "body": "<p>I would like to programmatically determine language that content of a website is written in.</p>\n\n<p>The only thing that comes into my mind is to compare content of the website with some set of words that are common to the particular language, and based on match percentage determine the language.</p>\n\n<p>Are there any better and more robust ways to solve the problem?</p>\n",
    "score": 4,
    "creation_date": 1321354156,
    "view_count": 1868,
    "answer_count": 3,
    "tags": "artificial-intelligence;nlp"
  },
  {
    "question_id": 6775975,
    "title": "Algorithm to determine if a word could be English?",
    "body": "<p>I have a list of strings that I need to check against an English dictionary.\nHowever I don't want to start checking every piece of gibberish in the list. First, I want to check if the string could be an English word.</p>\n\n<p>Does anyone know of an algorithm that does this or at least the rules that I need to apply to verify a word?</p>\n\n<p>For example:</p>\n\n<p>No spoken word can start with more than 3 consonants, and if there are are 3 initial consonants in a word, the first one must be \"s\".</p>\n",
    "score": 4,
    "creation_date": 1311251005,
    "view_count": 2038,
    "answer_count": 2,
    "tags": "algorithm;nlp"
  },
  {
    "question_id": 5353193,
    "title": "Looking for information on a Sentiment Analysis algorithm / tool for .NET",
    "body": "<p>I need to use/purchase some sort of tool for doing Sentiment Analysis to determine positive or negative connotation in text content. There are some terrific threads on this topic which I have read and listed below:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/293000/algorithm-to-determine-how-positive-or-negative-a-statement-text-is\">Algorithm to determine how positive or negative a statement/text is</a><br>\n<a href=\"https://stackoverflow.com/questions/122595/nlp-qualitatively-positive-vs-negative-sentence\">NLP: Qualitatively &quot;positive&quot; vs &quot;negative&quot; sentence</a><br>\n<a href=\"https://stackoverflow.com/questions/1326171/algorithm-to-determine-how-positive-or-negative-a-statement-text-is\">Algorithm to determine how positive or negative a statement/text is</a> (same name different thread)</p>\n\n<p>The problem is each one gives a great description of the algorithm, but alludes to the complexity of doing the job from scratch and offers links to the algorithm explanations.</p>\n\n<p>I need a <strong>.NET</strong> (VB.NET or C#) solution either in the form of a toolkit, API, .dll, etc. I have seen links to JAVA solutions but none really in my searches for .NET solutions.</p>\n\n<p>My fallback plan is to create dictionaries of words with weights and go that route, but I would prefer something a bit more robust.</p>\n\n<p>Does anyone have any information on a Sentiment Analysis solution specifically for .NET?</p>\n\n<p>Thanks!</p>\n",
    "score": 4,
    "creation_date": 1300457489,
    "view_count": 3418,
    "answer_count": 2,
    "tags": ".net;algorithm;nlp"
  },
  {
    "question_id": 5091389,
    "title": "Does anyone know how to configure the hunpos wrapper class on nltk?",
    "body": "<p>i've tried the following code and installed \nfrom <a href=\"http://code.google.com/p/hunpos/downloads/list\" rel=\"nofollow\">http://code.google.com/p/hunpos/downloads/list</a></p>\n\n<blockquote>\n  <p>english-wsj-1.0  </p>\n  \n  <p>hunpos-1.0-linux.tgz</p>\n</blockquote>\n\n<p>i've extracted the file onto '~/' directory</p>\n\n<p>and when i tried the following python code:</p>\n\n<pre><code>import nltk\nfrom nltk.tag import hunpos\nfrom nltk.tag.hunpos import HunposTagger\nimport os, sys, re, glob\ncwd = os.getcwd()\n\nfor infile in glob.glob(os.path.join(cwd, '*.txt')):\n    (PATH, FILENAME) = os.path.split(infile)\n    read = open(infile)\n    ht = HunposTagger('english.model')\n    ht.tag(read.readline())\n</code></pre>\n\n<p>i get the following error</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 4, in &lt;module&gt;\n  File \"/usr/local/lib/python2.6/dist-packages/nltk-2.0b9-py2.6.egg/nltk/tag/hunpos.py\", line 46, in __init__\n    verbose=verbose)\n  File \"/usr/local/lib/python2.6/dist-packages/nltk-2.0b9-py2.6.egg/nltk/internals.py\", line 503, in find_binary\n    raise LookupError('\\n\\n%s\\n%s\\n%s' % (div, msg, div))\nLookupError: \n\n===========================================================================\n  NLTK was unable to find the hunpos-tag executable!  Use\n  config_hunpos-tag() or set the HUNPOS environment variable.\n\n    &gt;&gt;&gt; config_hunpos-tag('/path/to/hunpos-tag')\n\n  Searched in:\n    - .\n    - /usr/bin\n    - /usr/local/bin\n    - /opt/local/bin\n    - /Applications/bin\n    - /home/ubi/bin\n    - /home/ubi/Applications/bin\n\n  For more information, on hunpos-tag, see:\n    &lt;http://code.google.com/p/hunpos/&gt;\n===========================================================================\n&gt;&gt;&gt; config_hunpos-tag('~/')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nNameError: name 'config_hunpos' is not defined\n</code></pre>\n\n<p>how do i configure hunpos in python? which python command do i need to enter?</p>\n",
    "score": 4,
    "creation_date": 1298466926,
    "view_count": 1172,
    "answer_count": 1,
    "tags": "python;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 3877762,
    "title": "Get the word under the mouse cursor in Windows",
    "body": "<p>Greetings everyone,</p>\n\n<p>A friend and I are discussing the possibility of a new project: A translation program that will pop up a translation whenever you hover over any word in any control, even static, non-editable ones.  I know there are many browser plugins to do this sort of thing on webpages; we're thinking about how we would do it system-wide (on Windows).</p>\n\n<p>Of course, the key difficulty is figuring out the word the user is hovering over.  I'm aware of MSAA and Automation, but as far as I can tell, those things only allow you to get the entire contents of a control, not the specific word the mouse is over.</p>\n\n<p>I stumbled upon this (proprietary) application that does pretty much exactly what we want to do:  <a href=\"http://www.gettranslateit.com/\" rel=\"nofollow\">http://www.gettranslateit.com/</a></p>\n\n<p>Somehow they are able to get the exact word the user is hovering over in almost any application (It seems to have trouble in a few apps, notably Windows Explorer).  It even grabs text out of obviously custom-drawn controls, somehow.  At first I thought it must be using OCR.  But even when I shrink the font so far down that the text becomes a completely unreadable blob, it can still recognize words perfectly.  (And yet, it doesn't recognize anything if I change the font to Wingdings.  But maybe that's by design?)</p>\n\n<p>Any ideas as to how it's achieving this seemingly impossible task?</p>\n\n<p>EDIT: It doesn't work with Wingdings, but it does work with some other nonsense fonts, so I've confirmed it can't be OCR.</p>\n",
    "score": 4,
    "creation_date": 1286409410,
    "view_count": 1966,
    "answer_count": 2,
    "tags": "windows;automation;accessibility;translation;nlp"
  },
  {
    "question_id": 3641162,
    "title": "How do I get a list of the most common words in various languages?",
    "body": "<p>Stack Overflow implemented its \"Related Questions\" feature by taking the title of the current question being asked and removing from it the 10,000 most common English words according to Google. The remaining words are then submitted as a fulltext search to find related questions.</p>\n\n<p>How do I get such a list of the most common English words? Or most common words in other languages? Is this something I can just get off the Google website?</p>\n",
    "score": 4,
    "creation_date": 1283581824,
    "view_count": 1689,
    "answer_count": 1,
    "tags": "search;nlp"
  },
  {
    "question_id": 732091,
    "title": "Match rows containing a word with permutations",
    "body": "<p>Say you've got a big table that contains a varchar column.</p>\n\n<p>How would you match rows that contain the word 'preferred' in the varchar col BUT the data is somewhat noisy and contains occasional spelling errors, e.g.:</p>\n\n<pre><code>['$2.10 Cumulative Convertible Preffered Stock, $25 par value',\n'5.95% Preferres Stock',\n'Class A Preffered',\n'Series A Peferred Shares',\n'Series A Perferred Shares',\n'Series A Prefered Stock',\n'Series A Preffered Stock',\n'Perfered',\n'Preffered  C']\n</code></pre>\n\n<p>The permutations of the word 'preferred' in the spelling errors above appear to exhibit a <a href=\"http://en.wikipedia.org/wiki/Family_resemblance\" rel=\"nofollow noreferrer\">family resemblance</a> but there's very little that they all have in common. Note that splitting out every word and running <a href=\"http://en.wikipedia.org/wiki/Levenshtein_distance\" rel=\"nofollow noreferrer\">levenshtein</a> on every word in every row is going to be prohibitively expensive.</p>\n\n<p>UPDATE:</p>\n\n<p>There are a couple of other examples like this, e.g. with 'restricted':</p>\n\n<pre><code>['Resticted Stock Plan',\n'resticted securities',\n'Ristricted Common Stock',\n'Common stock (restrticted, subject to vesting)',\n'Common Stock (Retricted)',\n'Restircted Stock Award',\n'Restriced Common Stock',]\n</code></pre>\n",
    "score": 4,
    "creation_date": 1239228737,
    "view_count": 267,
    "answer_count": 4,
    "tags": "nlp;information-retrieval"
  },
  {
    "question_id": 76825022,
    "title": "why nn.Embedding layer is used for positional encoding in bert?",
    "body": "<p>In the huggingface implementation of <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L186\" rel=\"nofollow noreferrer\">bert model</a>, for positional embedding nn.Embedding is used.\nWhy it is used instead of traditional sin/cos positional embedding described in the transformer paper? how this two things are same?</p>\n<p>Also I am confused about the nn.Embedding layer? there are many word embedding like word2vec, glove. among them which is actually nn.Embedding layer? Can you please explain the inner structure of nn.Embedding in detail?\n<a href=\"https://stackoverflow.com/questions/75646273/what-is-the-difference-nn-embedding-and-nn-linear\">This question</a> also comes in my mind.</p>\n",
    "score": 4,
    "creation_date": 1691039491,
    "view_count": 4660,
    "answer_count": 1,
    "tags": "pytorch;nlp;huggingface-transformers;bert-language-model;word-embedding"
  },
  {
    "question_id": 76050901,
    "title": "Haystack: save InMemoryDocumentStore and load it in retriever later to save embedding generation time",
    "body": "<p>I am using InMemory Document Store and an Embedding retriever for the Q/A pipeline.</p>\n<pre><code>from haystack.document_stores import InMemoryDocumentStore\ndocument_store = InMemoryDocumentStore(embedding_dim =768,use_bm25=True) \ndocument_store.write_documents(docs_processed)\n     \nfrom haystack.nodes import EmbeddingRetriever\nretriever_model_path ='downloaded_models\\local\\my_local_multi-qa-mpnet-base-dot-v1'\nretriever = EmbeddingRetriever(document_store=document_store,\n                              embedding_model=retriever_model_path,\n                              use_gpu=True)\n\ndocument_store.update_embeddings(retriever=retriever)\n</code></pre>\n<p>As the embedding takes a while, I want to load the embeddings and later use them again in the retriever. (in rest API side). I don't want to use ElasticSearch or Faiss. How can I achieve this using In Memory Store? I tried to use Pickle, but there is no way to store the embeddings. Again, in the embedding retriever, there is no load function.</p>\n<p>I tried to do the following:</p>\n<pre><code>with open(&quot;document_store_res.pkl&quot;, &quot;wb&quot;) as f:\n    pickle.dump(document_store.get_all_documents(), f)\n</code></pre>\n<p>And in the rest API, I am trying to load the document store :</p>\n<pre><code>def reader_retriever():\n# Load the pickled model        \n        with open(os.path.join(settings.BASE_DIR,'\\downloaded_models\\document_store_res.pkl'), 'rb') as f:\n            document_store_new = pickle.load(f)\n\n            retriever_model_path = os.path.join(settings.BASE_DIR, '\\downloaded_models\\my_local_multi-qa-mpnet-base-dot-v1')\n\n            retriever = EmbeddingRetriever(document_store=document_store_new,\n                               embedding_model=retriever_model_path,\n                               use_gpu=True)\n\n            document_store_new.update_embeddings(retriever=retriever,\n                                batch_size=100)\n            farm_reader_path = os.path.join(settings.BASE_DIR, '\\downloaded_models\\my_local_bert-large-uncased-whole-word-masking-squad2')\n\n            reader = FARMReader(model_name_or_path=farm_reader_path,\n                                    use_gpu=True)\n            \n\n            return reader, retriever\n</code></pre>\n",
    "score": 4,
    "creation_date": 1681880670,
    "view_count": 1984,
    "answer_count": 2,
    "tags": "python;nlp;haystack"
  },
  {
    "question_id": 75590491,
    "title": "Difference between MultiheadAttention and Attention layer in Tensorflow",
    "body": "<p>What is the difference between the following layers in Tensorflow: <code>tf.keras.layers.Attention</code>, <code>tf.keras.layers.MultiHeadAttention</code> and <code>tf.keras.layers.AdditiveAttention</code>?</p>\n<p>Also how to implement <code>tf.keras.layers.MultiHeadAttention</code> using fundamental layers like Dense, Add, LayerNormalization, etc? I want to understand the exact operations happening inside <a href=\"https://www.tensorflow.org/text/tutorials/nmt_with_attention#the_attention_layer\" rel=\"nofollow noreferrer\">this</a> tutorial.</p>\n",
    "score": 4,
    "creation_date": 1677578767,
    "view_count": 5447,
    "answer_count": 1,
    "tags": "tensorflow;keras;nlp;translation;attention-model"
  },
  {
    "question_id": 74769552,
    "title": "Classic king - man + woman = queen example with pretrained word-embedding and word2vec package in R",
    "body": "<p>I am really desperate, I just cannot reproduce the allegedly classic example of <code>king - man + woman = queen</code> with the <code>word2vec</code> package in R and any (!) pre-trained embedding model (as a <code>bin</code> file).</p>\n<p>I would be very grateful if anybody could provide working code to reproduce this example... including a link to the necessary pre-trained model which is also downloadable (many are not!).</p>\n<p>Thank you very much!</p>\n",
    "score": 4,
    "creation_date": 1670840129,
    "view_count": 1851,
    "answer_count": 2,
    "tags": "r;nlp;word2vec;word-embedding"
  },
  {
    "question_id": 74664286,
    "title": "Converting a dataset to CoNLL format. Label remaining tokens with O",
    "body": "<p>I have a manually annotated dataset that contains records in the following format:</p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;id&quot;: 1,\n    &quot;text&quot;: &quot;At the end of each fiscal quarter, for the four consecutive fiscal quarters ending as of such fiscal quarter end, from the date of the Third Amendment and until December 30, 1996, the Company shall maintain a fixed charge coverage ratio of not less than 1.25 to 1.0.&quot;,\n    &quot;label&quot;: [\n        [\n            209,\n            230,\n            &quot;COV_3&quot;\n        ],\n        [\n            379,\n            390,\n            &quot;VAL_3&quot;\n        ]\n    ],\n}\n</code></pre>\n<p>In the example above, <code>&quot;label&quot;</code> represents the custom entities I have in my dataset. In the example shown above, the phrase <code>fixed charge coverage</code> is located at position <code>[309, 336]</code> and is given the label <code>COV_3</code>. Likewise, the phrase <code>1.25 to 1.0</code> is located at <code>[379, 390]</code> and is given the label <code>VAL_3</code>.</p>\n<p>Now, I would like to fine-tune some transformer model like BERT on this dataset, however, I realised that the dataset must be in CoNLL format. Or at least, all the tokens of each datapoint must be labelled. Is there any way I can easily label the remaining tokens with label <code>&quot;O&quot;</code> or I can transform this dataset in the CoNLL format?</p>\n",
    "score": 4,
    "creation_date": 1670046104,
    "view_count": 1025,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp;huggingface-transformers;named-entity-recognition"
  },
  {
    "question_id": 71002866,
    "title": "Difference between Tokenizer and TextVectorization layer in tensorflow",
    "body": "<p>New to TensorFlow</p>\n<p>I saw couple of small NLP projects where people use the 'tf.keras.preprocessing.Tokenizer' to pre-process their text (link: <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer</a> )</p>\n<p>In some cases, they directly add 'tf.keras.layers.TextVectorization' layer while making the model (link : <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization</a>)</p>\n<p>May I know what's the difference between the two in terms of usage and when to choose which option?</p>\n",
    "score": 4,
    "creation_date": 1644101957,
    "view_count": 3749,
    "answer_count": 1,
    "tags": "tensorflow;nlp;tokenize;tf.keras"
  },
  {
    "question_id": 70707551,
    "title": "Saving BERT Sentence Embedding",
    "body": "<p>I'm currently working on an information retrieval task. I'm using SBERT to perform a semantic search. I already follows the documentation <a href=\"https://www.sbert.net/examples/applications/semantic-search/README.html\" rel=\"nofollow noreferrer\">here</a></p>\n<p>The model i use</p>\n<pre><code>model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n\n</code></pre>\n<p>The outline is</p>\n<ol>\n<li>You have a list of corpus like this:</li>\n</ol>\n<pre><code>    data = ['A man is eating food.',\n          'A man is eating a piece of bread.',\n          'The girl is carrying a baby.',\n          'A man is riding a horse.',\n          'A woman is playing violin.',\n          'Two men pushed carts through the woods.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'A cheetah is running behind its prey.'\n          ]\n</code></pre>\n<ol start=\"2\">\n<li>You have a query like this:</li>\n</ol>\n<pre><code>queries = ['A man is eating pasta.']\n</code></pre>\n<ol start=\"3\">\n<li>Perform encoding with both query and corpus</li>\n</ol>\n<pre><code>query_embedding = model.encode(query)\ndoc_embedding = model.encode(data)\n</code></pre>\n<p>the encode function outputs a numpy.ndarray like this\n<a href=\"https://i.sstatic.net/d5Bjs.png\" rel=\"nofollow noreferrer\">outputs of model.encode(data)</a></p>\n<ol start=\"4\">\n<li>And calculates the similarity using cosine similarity like this</li>\n</ol>\n<pre><code>similarity = util.cos_sim(query_embedding, doc_embedding)\n</code></pre>\n<ol start=\"5\">\n<li>And if you print the similarity, you'll get the torch.Tensor containing score of similarity like this</li>\n</ol>\n<pre><code>tensor([[0.4389, 0.4288, 0.6079, 0.5571, 0.4063, 0.4432, 0.5467, 0.3392, 0.4293]])\n</code></pre>\n<p>And it works fine and fast. But ofcourse it is only using a small amount of corpus. When using a large amount of corpus it will take time for the encoding to work.</p>\n<p>note: The encoding of query takes no time because it is only one sentence, but the encoding of the corpus will take some time</p>\n<p>So, the question is can we save the doc_embedding locally, and use it again? especially when using a large corpus</p>\n<p>is there any built-in class/function to do it from the transformers?</p>\n",
    "score": 4,
    "creation_date": 1642146960,
    "view_count": 7425,
    "answer_count": 1,
    "tags": "python;nlp;huggingface-transformers;bert-language-model;information-retrieval"
  },
  {
    "question_id": 69931987,
    "title": "GPT-3 davinci gives different results with the same prompt",
    "body": "<p>I am not sure if you have access to GPT-3, particularly DaVinci (the complete-a-sentence tool). You can find the API and info <a href=\"https://beta.openai.com/docs/api-reference/completions/create-via-get\" rel=\"nofollow noreferrer\">here</a></p>\n<p>I've been trying this tool for the past hour and every time I hit their API using the same prompt (indeed the same input), I received a different response.</p>\n<ol>\n<li>Do you happen to encounter the same situation?</li>\n<li>If this is expected, do you happen to know the reason behind it?</li>\n</ol>\n<p>Here are some examples</p>\n<p><strong>Request header</strong> <em>(I tried to use the same example they provide)</em></p>\n<pre><code>{\n  &quot;prompt&quot;: &quot;Once upon a time&quot;,\n  &quot;max_tokens&quot;: 3,\n  &quot;temperature&quot;: 1,\n  &quot;top_p&quot;: 1,\n  &quot;n&quot;: 1,\n  &quot;stream&quot;: false,\n  &quot;logprobs&quot;: null,\n  &quot;stop&quot;: &quot;\\n&quot;\n}\n</code></pre>\n<p><strong>Output 1</strong></p>\n<pre><code>&quot;choices&quot;: [\n        {\n            &quot;text&quot;: &quot;, this column&quot;,\n            &quot;index&quot;: 0,\n            &quot;logprobs&quot;: null,\n            &quot;finish_reason&quot;: &quot;length&quot;\n        }\n    ]\n</code></pre>\n<p><strong>Output 2</strong></p>\n<pre><code>&quot;choices&quot;: [\n        {\n            &quot;text&quot;: &quot;, winter break&quot;,\n            &quot;index&quot;: 0,\n            &quot;logprobs&quot;: null,\n            &quot;finish_reason&quot;: &quot;length&quot;\n        }\n    ]\n</code></pre>\n<p><strong>Output 3</strong></p>\n<pre><code>&quot;choices&quot;: [\n        {\n            &quot;text&quot;: &quot;, the traditional&quot;,\n            &quot;index&quot;: 0,\n            &quot;logprobs&quot;: null,\n            &quot;finish_reason&quot;: &quot;length&quot;\n        }\n    ]\n</code></pre>\n",
    "score": 4,
    "creation_date": 1636649381,
    "view_count": 4486,
    "answer_count": 3,
    "tags": "text;nlp;autocomplete;gpt-3"
  },
  {
    "question_id": 69384414,
    "title": "How to clean non Arabic letters from a text file in python?",
    "body": "<p>UPDATE-\nVery new to python,\nHow to clean the text from everything but Arabic letters. I used regex function but without success.</p>\n<p>This is my code</p>\n<pre><code># load text\nfilename = '/content/drive/MyDrive/Colab Notebooks/ArabicKidsStories.txt'\nfile = open(filename,'rt')\ntext = file.read()\nfile.close()\nimport re\ntext = re.sub('([@A-Za-z0-9_]+)|[^\\w\\s]|#|http\\S+', '', text) # cleaning up\nprint (text)\n</code></pre>\n<p>This is a sample of the output</p>\n<pre><code> تفقدت نظارتي  حين استيقظت صباحا  فلم أجدها في مكانها  وبحثت عنها في كل مكان  دون أن أعثر لها على أثر  يا إلهي  كيف سأخرج اليوم من البيت  وأواجه النهار  \n وتناهى إلي من الخارج  صوت نقار الخشب  فوق جذع شجرة قريبة فأسرعت إلى الباب  وفتحته  وإذا ضوء النهار يبهر بصري  فأغلقت عيني  وهتفت  أيها النقار  أين أنت  \n وحاولت عبثا أن أفتح عيني  وأنا أقول  عفوا  لا أستطيع أن أفتح عيني  إن الضوء يعميني  \n فقال نقار الخشب  هذا طبيعي  يا عزيزتي  فأنت لم تضعي نظارتك الشمسية  \n وتراجعت قليلا  وقلت  لقد اختفت نظارتي  \n فتساءل نقار الخشب  اختفت  ماذا تقولين  \n وبدل أن أجيبه  قلت  أرجوك  ابحث لي عن نظارتي  إنني لا أستطيع الخروج من دونها  \n ولاذ نقار الخشب لحظة  ثم قال  حسن  ابقي أنت في البيت  وسأبحث لك أنا عنها  \n ومضى نقار الخشب  فأغلقت الباب والنافذة  وقبعت في الظلام  يا للغرابة  إنني أرى في الليل أيضا  أوه  كلا  إنني أحب النهار  وأحبذ أن أطير دوما في النور مع رفاقي  إنني لا أحب الليل  ولا أريد أن يكون الظلام عالمي  ترى أين اختفت هذه النظارة اللعينة  \n    \n ـــــــــــــ \n عاد نقار الخشب متعبا  قبل المساء  وقال لي  آسف  يا عزيزي  سألت عن نظارتك الطيور جميعا  لكن أحدا منهم لم يرها  \n فأطرقت برأسي برهة  ثم قلت  أشكرك  يا عزيزي  سأبحث عنها بنفسي ليلا  \n واتسعت عينا نقار الخشب دهشة  وقال  ليلا  \n وقبل أن أجيبه  مضى على عجل  وهو يقول  عفوا  صغاري ينتظرونني الآن  إلى اللقاء  \n</code></pre>\n<p>Any help will be appreciated.\nThanks in advance.</p>\n",
    "score": 4,
    "creation_date": 1632958563,
    "view_count": 3277,
    "answer_count": 5,
    "tags": "python;python-3.x;nlp"
  },
  {
    "question_id": 64493912,
    "title": "Access wordnet file with nltk without nltk.download()",
    "body": "<p>I'm trying to use wordnet without nltk.download('wordnet') as that function is blocked through the companies IT policy. I have downloaded the wordnet file and unzipped it into my local directory and used <code>nltk.data.path.append(&quot;my/wordnet/directory&quot;)</code>. When I try and execute a command say <code>&quot;jump&quot; in wn.words()</code> I get the error -</p>\n<pre><code>Resource wordnet not found. \nSearched in:\n- 'my/wordnet/directory'\n- 'standard/directories'\n</code></pre>\n<p>To be clear, IT is fine with me uploading the file directly.\nHow do I get nltk to interact with this file?\nFile found at <code>https://www.nltk.org/nltk_data/</code>\nCorpus number 69.</p>\n",
    "score": 4,
    "creation_date": 1603427693,
    "view_count": 2854,
    "answer_count": 1,
    "tags": "python-3.x;nlp;nltk;wordnet"
  },
  {
    "question_id": 64445784,
    "title": "How can I apply pruning on a BERT model?",
    "body": "<p>I have trained a <a href=\"https://en.wikipedia.org/wiki/BERT_(language_model)\" rel=\"nofollow noreferrer\">BERT</a> model using ktrain (TensorFlow wrapper) to recognize emotion on text. It works, but it suffers from really slow inference. That makes my model not suitable for a production environment. I have done some research, and it seems pruning could help.</p>\n<p>TensorFlow provides some options for pruning, e.g., <em>tf.contrib.model_pruning</em>. The problem is that it is not a not a widely used technique. What would be a simple enough example that could help me to understand how to use it?</p>\n<p>I provide my working code below for reference.</p>\n<pre><code>import pandas as pd\nimport numpy as np\nimport preprocessor as p\nimport emoji\nimport re\nimport ktrain\nfrom ktrain import text\nfrom unidecode import unidecode\nimport nltk\n\n# Text preprocessing class\nclass TextPreprocessing:\n    def __init__(self):\n        p.set_options(p.OPT.MENTION, p.OPT.URL)\n\n    def _punctuation(self, val):\n        val = re.sub(r'[^\\w\\s]', ' ', val)\n        val = re.sub('_', ' ', val)\n        return val\n\n    def _whitespace(self, val):\n        return &quot; &quot;.join(val.split())\n\n    def _removenumbers(self, val):\n        val = re.sub('[0-9] + ', '', val)\n        return val\n\n    def _remove_unicode(self, text):\n        text = unidecode(text).encode(&quot;ascii&quot;)\n        text = str(text, &quot;ascii&quot;)\n        return text\n\n    def _split_to_sentences(self, body_text):\n        sentences = re.split(r&quot;(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?)\\s&quot;, body_text)\n        return sentences\n\n    def _clean_text(self, val):\n        val = val.lower()\n        val = self._removenumbers(val)\n        val = p.clean(val)\n        val = ' '.join(self._punctuation(emoji.demojize(val)).split())\n        val = self._remove_unicode(val)\n        val = self._whitespace(val)\n        return val\n\n    def text_preprocessor(self, body_text):\n\n        body_text_df = pd.DataFrame({&quot;body_text&quot;: body_text}, index=[1])\n\n        sentence_split_df = body_text_df.copy()\n\n        sentence_split_df[&quot;body_text&quot;] = sentence_split_df[&quot;body_text&quot;].apply(\n            self._split_to_sentences)\n\n        lst_col = &quot;body_text&quot;\n        sentence_split_df = pd.DataFrame(\n            {\n                col: np.repeat(\n                    sentence_split_df[col].values, sentence_split_df[lst_col].str.len(\n                    )\n                )\n                for col in sentence_split_df.columns.drop(lst_col)\n            }\n        ).assign(**{lst_col: np.concatenate(sentence_split_df[lst_col].values)})[\n            sentence_split_df.columns\n        ]\n\n        body_text_df[&quot;body_text&quot;] = body_text_df[&quot;body_text&quot;].apply(self._clean_text)\n\n        final_df = (\n            pd.concat([sentence_split_df, body_text_df])\n            .reset_index()\n            .drop(columns=[&quot;index&quot;])\n        )\n\n        return final_df[&quot;body_text&quot;]\n\n# Instantiate data preprocessing object\ntext1 = TextPreprocessing()\n\n# Import data\ndata_train = pd.read_csv('data_train_v5.csv', encoding='utf8', engine='python')\ndata_test = pd.read_csv('data_test_v5.csv', encoding='utf8', engine='python')\n\n# Clean the data\ndata_train['Text'] = data_train['Text'].apply(text1._clean_text)\ndata_test['Text'] = data_test['Text'].apply(text1._clean_text)\n\nX_train = data_train.Text.tolist()\nX_test = data_test.Text.tolist()\n\ny_train = data_train.Emotion.tolist()\ny_test = data_test.Emotion.tolist()\n\ndata = data_train.append(data_test, ignore_index=True)\n\nclass_names = ['joy', 'sadness', 'fear', 'anger', 'neutral']\n\nencoding = {\n    'joy': 0,\n    'sadness': 1,\n    'fear': 2,\n    'anger': 3,\n    'neutral': 4\n}\n\n# Integer values for each class\ny_train = [encoding[x] for x in y_train]\ny_test = [encoding[x] for x in y_test]\n\ntrn, val, preproc = text.texts_from_array(x_train=X_train, y_train=y_train,\n                                          x_test=X_test, y_test=y_test,\n                                          class_names=class_names,\n                                          preprocess_mode='distilbert',\n                                          maxlen=350)\n\nmodel = text.text_classifier('distilbert', train_data=trn, preproc=preproc)\n\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)\n\npredictor = ktrain.get_predictor(learner.model, preproc)\n\n# Save the model on a file for later use\npredictor.save(&quot;models/bert_model&quot;)\n\nmessage = &quot;This is a happy message&quot;\n\n# Cleaning - takes 5 ms to run\nclean = text1._clean_text(message)\n\n# Prediction - takes 325 ms to run\npredictor.predict_proba(clean)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1603199066,
    "view_count": 1697,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp;bert-language-model;huggingface-transformers"
  },
  {
    "question_id": 64365478,
    "title": "Trouble to extract compound nouns including hyphens in NLP",
    "body": "<h2>Background and Goal</h2>\n<p>I would like to extract nouns and compound nouns including hyphens from each sentence like below.\nIf it includes hyphens, I need to extract it with hyphens.</p>\n<pre><code>{The T-shirt is old.: ['T-shirt'], \nI bought the computer and the new web-cam.: ['computer', 'web-cam'], \nI bought the computer and the new web camera.: ['computer', 'web camera']}\n</code></pre>\n<h2>problem</h2>\n<p>Current out put is below.\nThere are labels , 'compound', on the first word of compound nouns, but I cannot extract what I expect for now.</p>\n<pre><code>T T PROPN NNP compound X True False\nshirt shirt NOUN NN nsubj xxxx True False\ncomputer computer NOUN NN dobj xxxx True False\nweb web NOUN NN compound xxx True False\ncam cam NOUN NN conj xxx True False\ncomputer computer NOUN NN dobj xxxx True False\nweb web NOUN NN compound xxx True False\ncamera camera NOUN NN conj xxxx True False\n\n{The T-shirt is old.: ['T -', 'T', 'T -', 'shirt'], \nI bought the computer and the new web-cam.: ['web -', 'computer', 'web -', 'web', 'web -', 'cam'], \nI bought the computer and the new web camera.: ['web camera', 'computer', 'web camera', 'web', 'web camera', 'camera']}\n\n</code></pre>\n<h2>Current Code</h2>\n<p>I'm using the NLP library, spaCy to distinguish nouns and compound nouns.\nHope to hear your advice how to fix the current code.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\ntexts =  [&quot;The T-shirt is old.&quot;, &quot;I bought the computer and the new web-cam.&quot;, &quot;I bought the computer and the new web camera.&quot;]\n\nnouns = []*len(texts)\ndic = {k: v for k, v in zip(texts, nouns)}\n\nfor i in range(len(texts)):\n    text = nlp(texts[i])\n    words = []\n    for word in text:\n        if word.pos_ == 'NOUN'or word.pos_ == 'PROPN':\n            print(word.text, word.lemma_, word.pos_, word.tag_, word.dep_,\n                word.shape_, word.is_alpha, word.is_stop)\n\n            #compound words\n            for j in range(len(text)):\n                    token = text[j]\n                    if token.dep_ == 'compound':\n                        if j &lt; len(text)-1:\n                            nexttoken = text[j+1]\n                            words.append(str(token.text + ' ' + nexttoken.text))\n\n\n            else:\n                words.append(word.text)\n    dic[text] = words       \nprint(dic)\n</code></pre>\n<h2>Development Environment</h2>\n<p>Python 3.7.4</p>\n<p>spaCy version    2.3.2</p>\n",
    "score": 4,
    "creation_date": 1602740370,
    "view_count": 1586,
    "answer_count": 1,
    "tags": "python;python-3.x;string;nlp;spacy"
  },
  {
    "question_id": 64202958,
    "title": "How the get the indices of the sents&#39;s beginning and ending in spacy?",
    "body": "<p>I am new to using spacy. I have a scenario where I have to get the index where the sentence starts and ends in the sentence. If I use doc. sents then I get a list of sents. sent.beg and sent.end prints the token index but I want the character index.</p>\n<pre><code>for sent in doc.sents:\n    print(sent.start,sent.end)     #prints token index\n</code></pre>\n<p>Example:</p>\n<pre><code>completeText = &quot;Hi, I am using StackOverflow. The community is great.&quot;\nnlp = spacy.load('en_core_web_sm')\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\ndoc = nlp(completeText)\nfor sent in doc.sents:\n    print(sent.start,sent.end)  #prints 0,7 and 7,12 the token indices\n</code></pre>\n<p>The above print statement prints only token indices, not character indices. My desired output is 0,29 and 30, 54.</p>\n<p>I have tried getting the length of the sentence as below. I have added an if statement in the last because space after a full stop is ignored in the sentences.</p>\n<pre><code>start = [0] * len(list(doc.sents))\nend = [0] * len(list(doc.sents))\nfor index, i in enumerate(doc.sents):\n\n    if index !=0:\n        start[index] = end[index-1] + 1\n\n    length += len(str(i))\n\n    if index == 0:\n         end[index] = length\n    else:\n        end[index] = length \n    if end[index] + 1 &lt; len(sent) and sent[end[index]+1] == &quot; &quot;:        \n        length += 1\n</code></pre>\n<p>This works fine when there are only spaces after a full stop. But in the complete text I have(more than 10,000 lines) I am not getting the correct answer. Does spacy ignore any other characters like mentioned above for including in sents?</p>\n<p><strong>Is there a better way to do this?</strong></p>\n",
    "score": 4,
    "creation_date": 1601876267,
    "view_count": 2663,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 64068144,
    "title": "NLP, difference between using NLTK&#39;s sentiment analysis and using ML approach",
    "body": "<p>I recently started to learn NLP and ML using Python.\nI started with Sentiment Analysis.\nI'm having trouble understanding where machine learning comes in to play when doing sentiment analysis.</p>\n<p>Let's say I'm analyzing tweets or news headlines using NLTK's SentimentIntensityAnalyzer and I'm loading a case relevant lexicons so I'm getting polarity and negativity, positivity, neutral scores.\nNow what I don't understand is, in which case should I use code like in this article:</p>\n<p><a href=\"https://medium.com/dataseries/sentiment-classifier-using-tfidf-3ffce3f1cbd5\" rel=\"nofollow noreferrer\">Sentiment with ML toturial</a></p>\n<p>or just the built-in like in NLTK or even something like Google's BERT?</p>\n<p>Any answer or link to Blog or tutorial would be welcomed!</p>\n",
    "score": 4,
    "creation_date": 1601051935,
    "view_count": 2782,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp;nltk;sentiment-analysis"
  },
  {
    "question_id": 63778133,
    "title": "How can I implement meteor score when evaluating a model when using the meteor_score module from nltk?",
    "body": "<p>I currently have 2 files, reference.txt and model.txt. These two text files contain original captions and generated captions after training.<br />\nCan I simply do the following to obtain the meteor score:</p>\n<pre><code>score = nltk.translate.meteor_score.meteor_score(reference, model)\nprint(np.mean(meteor_score))\n</code></pre>\n<p>I have also looked to <a href=\"https://github.com/tylin/coco-caption\" rel=\"nofollow noreferrer\">https://github.com/tylin/coco-caption</a> but I have no clue how to implement this.</p>\n",
    "score": 4,
    "creation_date": 1599483658,
    "view_count": 8679,
    "answer_count": 1,
    "tags": "python;nlp;nltk;metrics"
  },
  {
    "question_id": 62926022,
    "title": "ModuleNotFoundError: No module named &#39;pegasus&#39;",
    "body": "<p>I would like to try PEGASUS to summarize article. <a href=\"https://github.com/google-research/pegasus\" rel=\"nofollow noreferrer\">https://github.com/google-research/pegasus</a></p>\n<p>The original repo's README suggests to use Google Cloud Compute Engine, but I use Colaboratory notebook. I'm an English teacher in Japan and I hope my students to try this software easily. They can experience both Machine Learning and English passage summarization.</p>\n<p>I followed this instruction. <a href=\"https://github.com/google-research/pegasus/tree/f76b63c2886748f7f5c6c9fb547456d8c6002562#setup\" rel=\"nofollow noreferrer\">https://github.com/google-research/pegasus/tree/f76b63c2886748f7f5c6c9fb547456d8c6002562#setup</a></p>\n<p>This is my colab notebook.\n<a href=\"https://colab.research.google.com/drive/1p95tZcjhfuCLYh23X3S_gZqRoWVhpIlE?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/1p95tZcjhfuCLYh23X3S_gZqRoWVhpIlE?usp=sharing</a></p>\n<p>This is my code in the notebook.</p>\n<pre><code>%tensorflow_version 1.x\n\n!git clone https://github.com/google-research/pegasus\n\n!export PYTHONPATH=/content/pegasus\n\n%pip install -r /content/pegasus/requirements.txt\n\n!mkdir /content/pegasus/ckpt\n\n!gsutil cp -r gs://pegasus_ckpt/ /content/pegasus/ckpt/\n\n!python /content/pegasus/pegasus/bin/train.py --params=aeslc_transformer \\\n--param_overrides=vocab_filename=ckpt/pegasus_ckpt/c4.unigram.newline.10pct.96000.model \\\n--train_init_checkpoint=ckpt/pegasus_ckpt/model.ckpt-1500000 \\\n--model_dir=ckpt/pegasus_ckpt/aeslc\n</code></pre>\n<p>Then, I get this error message.</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;/content/pegasus/pegasus/bin/train.py&quot;, line 17, in &lt;module&gt;\n    from pegasus.data import infeed\nModuleNotFoundError: No module named 'pegasus'\n</code></pre>\n<p>This error message says that python can't import 'pegasus' module, but I made python path with <code>!export PYTHONPATH=/content/pegasus</code> this command.</p>\n<p>Could you give me any advice, please?</p>\n",
    "score": 4,
    "creation_date": 1594862561,
    "view_count": 1152,
    "answer_count": 1,
    "tags": "python;tensorflow;machine-learning;nlp;google-colaboratory"
  },
  {
    "question_id": 62371380,
    "title": "Logistic regression: X has 667 features per sample; expecting 74869",
    "body": "<p>Using a imdb movie reviews dataset i have made a logistic regression to predict the sentiment of the review.</p>\n\n<pre><code>tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None, \n\ntokenizer=fill, use_idf=True, norm='l2', smooth_idf=True)\ny = df.sentiment.values\nX = tfidf.fit_transform(df.review)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, shuffle=False)\nclf = LogisticRegressionCV(cv=5, scoring=\"accuracy\", random_state=1, n_jobs=-1, verbose=3,max_iter=300).fit(X_train, y_train)\n\nyhat = clf.predict(X_test)\n\n\nprint(\"accuracy:\")\nprint(clf.score(X_test, y_test))\n\nmodel_performance(X_train, y_train, X_test, y_test, clf)\n</code></pre>\n\n<p>prior to this text preprocessing have been applied.\nModel performance is just a function to create a confusion matrix.\nthis all works well with a good accuracy.</p>\n\n<p>I now scrape new IMDB reviews:</p>\n\n<pre><code>#The movie \"Joker\" IMBD review page\nurl_link='https://www.imdb.com/title/tt7286456/reviews'\nhtml=urlopen(url_link)\n\ncontent_bs=BeautifulSoup(html)\n\nJokerReviews = []\n#All the reviews ends in a div class called text in html, can be found in the imdb source code\nfor b in content_bs.find_all('div',class_='text'):\n  JokerReviews.append(b)\n\ndf = pd.DataFrame.from_records(JokerReviews)\ndf['sentiment'] = \"0\" \njokerData=df[0]\njokerData = jokerData.apply(preprocessor)\n</code></pre>\n\n<p><strong>Problem: Now i wish to test the same logistic regression to predict the sentiment:</strong></p>\n\n<pre><code>tfidf2 = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None, tokenizer=fill, use_idf=True, norm='l2', smooth_idf=True)\ny = df.sentiment.values\nXjoker = tfidf2.fit_transform(jokerData)\n\nyhat = Clf.predict(Xjoker)\n</code></pre>\n\n<p>But i get the error: \nValueError: X has 667 features per sample; expecting 74869</p>\n\n<p>I dont get why it has to have the same amount of features as X_test</p>\n",
    "score": 4,
    "creation_date": 1592131253,
    "view_count": 8922,
    "answer_count": 4,
    "tags": "python;nlp;logistic-regression"
  },
  {
    "question_id": 61060603,
    "title": "Lowercase Lemmatization with spacy in german",
    "body": "<p>There seems to be a problem with noun singularization with spacy in german.\nSpacy seems to rely on words to be capitalized to recognize them as nouns. An example:</p>\n\n<pre><code>import spacy\nnlp = spacy.load(\"C:\\\\Users\\\\somepath\\\\spacy\\\\de_core_md\\\\de_core_news_md\\\\de_core_news_md-2.2.5\")\n\ndef lemmatize_text(text):\n    \"\"\"returns the text with each word in its basic form\"\"\"\n    doc = nlp(text)\n    return [word.lemma_ for word in doc]\n\nlemmatize_text('Das Wort Tests wird erkannt. Allerdings werden tests nicht erkannt')\n--&gt; ['der', 'Wort', 'Test', 'werden', 'erkennen', '.', 'Allerdings', 'werden', 'tests', 'nicht', 'erkennen']\n\n# should say 'Test' for both sentences\n</code></pre>\n\n<p>That would not be a problem if I was lemmatizing the original text right away. However, my preprocessing looks like this:</p>\n\n<ol>\n<li>turn to lowercase</li>\n<li>remove punctuation</li>\n<li>remove stopwords</li>\n<li>lemmatize</li>\n</ol>\n\n<p>Is there a recommended order in which to execute the above steps?</p>\n\n<p>I am not lemmatizing first because words at the beginning of a sentence are then not recognized correctly:</p>\n\n<pre><code>lemmatize_text('Größer wird es nicht mehr. größer wird es nicht mehr.')\n--&gt; ['Größer', 'werden', 'ich', 'nicht', 'mehr', '.', 'groß', 'werden', 'ich', 'nicht', 'mehr', '.']\n\n# should say 'groß' for both sentences\n</code></pre>\n",
    "score": 4,
    "creation_date": 1586178549,
    "view_count": 1004,
    "answer_count": 1,
    "tags": "python;nlp;spacy"
  },
  {
    "question_id": 60485793,
    "title": "Identify Location Within the Sentence where the Missing Word Belongs",
    "body": "<p>I have the code below:</p>\n\n<pre><code>import nltk\nexampleArray = ['The dog barking']\n\ndef processLanguage():\n    for item in exampleArray:\n        tokenized = nltk.word_tokenize(item)\n        tagged = nltk.pos_tag(tokenized)\n        print(tagged)\n\nprocessLanguage()\n</code></pre>\n\n<p>The output of the code above are the tokenized words with their corresponding parts of speech. Example :</p>\n\n<pre><code>[('The', 'DT'), ('dog', 'NN'), ('barking', 'NN'), ('.', '.')]\n\nDT = determiner\nNN = noun\n</code></pre>\n\n<p>The text is supposed to be </p>\n\n<pre><code>The dog is barking\n</code></pre>\n\n<p>and supposed to have the POS sequence of</p>\n\n<pre><code>DT -&gt; NN -&gt; VBZ -&gt; VBG\n\nVBZ = verb, present tense, 3rd person singular\nVBG = verb, present participle or gerund\n</code></pre>\n\n<p>How will I make the program locate within the sentence the position of the missing word?</p>\n",
    "score": 4,
    "creation_date": 1583141039,
    "view_count": 386,
    "answer_count": 1,
    "tags": "python;nlp;nltk;pos-tagger;part-of-speech"
  },
  {
    "question_id": 60443948,
    "title": "Is there a way to turn Google Colab code into web services or rest APIs",
    "body": "<p>I have a Machine learning module which uses Google Colab's free GPU for NLP tasks, and I want to make a web app out of it. I've been thinking of using React js for frontend and spring boot for the back end and was wondering whether there is a way to connect the code at Google Colab with the backend.\nWant to know other alternative suggestions to building a web app incorporating the ML module in Colab as well. Any sort of help is appriciated. </p>\n",
    "score": 4,
    "creation_date": 1582852230,
    "view_count": 5117,
    "answer_count": 1,
    "tags": "python;machine-learning;nlp;google-colaboratory"
  },
  {
    "question_id": 60142937,
    "title": "HuggingFace Transformers For Text Generation with CTRL with Google Colab&#39;s free GPU",
    "body": "<p>I wanted to test TextGeneration with CTRL using PyTorch-Transformers, before using it for fine-tuning. But it doesn't prompt anything like it does with GPT-2 and other similar language generation models. I'm very new for this and am stuck and can't figure out what's going on.</p>\n<p>This is the procedure I followed in my Colab notebook,</p>\n<pre><code>!pip install transformers\n</code></pre>\n<pre><code>!git clone https://github.com/huggingface/pytorch-transformers.git\n</code></pre>\n<pre><code>!python pytorch-transformers/examples/run_generation.py \\\n    --model_type=ctrl \\\n    --length=100 \\\n    --model_name_or_path=ctrl \\\n    --temperature=0.2 \\\n    --repetition_penalty=1.2 \\\n</code></pre>\n<p>And this is what I get after running the script</p>\n<pre><code>02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-vocab.json from cache at /root/.cache/torch/transformers/a858ad854d3847b02da3aac63555142de6a05f2a26d928bb49e881970514e186.285c96a541cf6719677cfb634929022b56b76a0c9a540186ba3d8bbdf02bca42\n02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-merges.txt from cache at /root/.cache/torch/transformers/aa2c569e6648690484ade28535a8157aa415f15202e84a62e82cc36ea0c20fa9.26153bf569b71aaf15ae54be4c1b9254dbeff58ca6fc3e29468c4eed078ac142\n02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   loading configuration file https://storage.googleapis.com/sf-ctrl/pytorch/ctrl-config.json from cache at /root/.cache/torch/transformers/d6492ca334c2a4e079f43df30956acf935134081b2b3844dc97457be69b623d0.1ebc47eb44e70492e0c20494a084f108332d20fea7fe5ad408ef5e7a8f2baef4\n02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   Model config CTRLConfig {\n  &quot;architectures&quot;: null,\n  &quot;attn_pdrop&quot;: 0.1,\n  &quot;bos_token_id&quot;: 0,\n  &quot;dff&quot;: 8192,\n  &quot;do_sample&quot;: false,\n  &quot;embd_pdrop&quot;: 0.1,\n  &quot;eos_token_ids&quot;: 0,\n  &quot;finetuning_task&quot;: null,\n  &quot;from_tf&quot;: false,\n  &quot;id2label&quot;: {\n    &quot;0&quot;: &quot;LABEL_0&quot;\n  },\n  &quot;initializer_range&quot;: 0.02,\n  &quot;is_decoder&quot;: false,\n  &quot;label2id&quot;: {\n    &quot;LABEL_0&quot;: 0\n  },\n  &quot;layer_norm_epsilon&quot;: 1e-06,\n  &quot;length_penalty&quot;: 1.0,\n  &quot;max_length&quot;: 20,\n  &quot;model_type&quot;: &quot;ctrl&quot;,\n  &quot;n_ctx&quot;: 512,\n  &quot;n_embd&quot;: 1280,\n  &quot;n_head&quot;: 16,\n  &quot;n_layer&quot;: 48,\n  &quot;n_positions&quot;: 50000,\n  &quot;num_beams&quot;: 1,\n  &quot;num_labels&quot;: 1,\n  &quot;num_return_sequences&quot;: 1,\n  &quot;output_attentions&quot;: false,\n  &quot;output_hidden_states&quot;: false,\n  &quot;output_past&quot;: true,\n  &quot;pad_token_id&quot;: 0,\n  &quot;pruned_heads&quot;: {},\n  &quot;repetition_penalty&quot;: 1.0,\n  &quot;resid_pdrop&quot;: 0.1,\n  &quot;summary_activation&quot;: null,\n  &quot;summary_first_dropout&quot;: 0.1,\n  &quot;summary_proj_to_labels&quot;: true,\n  &quot;summary_type&quot;: &quot;cls_index&quot;,\n  &quot;summary_use_proj&quot;: true,\n  &quot;temperature&quot;: 1.0,\n  &quot;top_k&quot;: 50,\n  &quot;top_p&quot;: 1.0,\n  &quot;torchscript&quot;: false,\n  &quot;use_bfloat16&quot;: false,\n  &quot;vocab_size&quot;: 246534\n}\n\n02/10/2020 01:02:31 - INFO - transformers.modeling_utils -   loading weights file https://storage.googleapis.com/sf-ctrl/pytorch/seqlen256_v1.bin from cache at /root/.cache/torch/transformers/c146cc96724f27295a0c3ada1fbb3632074adf87e9aef8269e44c9208787f8c8.b986347cbab65fa276683efbb9c2f7ee22552277bcf6e1f1166557ed0852fdf0\ntcmalloc: large alloc 1262256128 bytes == 0x38b92000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x5096b7 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9\ntcmalloc: large alloc 1262256128 bytes == 0x19fdda000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5a067e 0x50d966 0x508245\n^C\n</code></pre>\n<p>and then terminates. Could this be because of a GPU problem?</p>\n",
    "score": 4,
    "creation_date": 1581297261,
    "view_count": 2669,
    "answer_count": 1,
    "tags": "python;deep-learning;nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 59191144,
    "title": "How do attention network works?",
    "body": "<p>Recently I was going through Attention is all you need paper, ongoing through it I found an issue regarding understanding the attention network if I ignore the maths behind it.\nCan anyone make me understand the attention network with an example?</p>\n",
    "score": 4,
    "creation_date": 1575535124,
    "view_count": 155,
    "answer_count": 1,
    "tags": "text;nlp;transformer-model;attention-model"
  },
  {
    "question_id": 58472437,
    "title": "Structural Topic Modeling in R: Plot statistical significance for Topic Content",
    "body": "<p>my question relates to structural topic modeling in R, specifically to the stm package developed by Roberts et al. (<a href=\"https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf\" rel=\"nofollow noreferrer\">https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf</a>).</p>\n\n<p>I implemented a structural topic model in order to investigate, whether there is a statistically significant difference in the vocabulary with which women and men describe certain topics. Thus my question relates to the word rates used in discussing a topic, the authors of the vignette refer to this as topical content analysis, see page 19. </p>\n\n<p>The implementation of the code was successful and I manage to create a similar graph to the one shown in Figure 8 of the Vignette. </p>\n\n<p>My question now is, how do I know  whether the difference in the vocabulary with which in my case women and men describe topics is statistically significant? </p>\n\n<p>And is there a way to plot this for all of my topics in one graph? </p>\n\n<p>Thank you!</p>\n\n<p>My code:</p>\n\n<p>Estimate the Topic Model</p>\n\n<pre><code>stmContent2 &lt;- stm(out$documents, \n                  out$vocab,\n                  K = 80, \n                  prevalence =~ gender,\n                  content =~ gender,\n                  max.em.its = 75,\n                  data = out$meta, \n                  init.type = \"Spectral\",\n                  seed = 8458302)\n\nplot(stmContent2, type = \"perspectives\", topics = 11)\n</code></pre>\n",
    "score": 4,
    "creation_date": 1571570223,
    "view_count": 382,
    "answer_count": 1,
    "tags": "r;nlp;topic-modeling;topicmodels"
  },
  {
    "question_id": 58029149,
    "title": "SparkNLP Sentiment Analysis in Java",
    "body": "<p>I want to use SparkNLP for doing sentiment analysis on a spark dataset on column <code>column1</code> using the default trained model. This is my code:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>DocumentAssembler docAssembler = (DocumentAssembler) new DocumentAssembler().setInputCol(\"column1\")\n                .setOutputCol(\"document\");\n\nTokenizer tokenizer = (Tokenizer) ((Tokenizer) new Tokenizer().setInputCols(new String[] { \"document\" }))\n                .setOutputCol(\"token\");\nString[] inputCols = new String[] { \"token\", \"document\" };\n\nSentimentDetector sentiment = ((SentimentDetector) ((SentimentDetector) new SentimentDetector().setInputCols(inputCols)).setOutputCol(\"sentiment\"));\nPipeline pipeline = new Pipeline().setStages(new PipelineStage[] { docAssembler, tokenizer, sentiment });\n\n// Fit the pipeline to training documents.\nPipelineModel pipelineFit = pipeline.fit(ds);\nds = pipelineFit.transform(ds);\nds.show();\n</code></pre>\n\n<p>Here <code>ds</code> is <code>Dataset&lt;Row&gt;</code> with columns including column <code>column1</code>.I am gettimg the following error.</p>\n\n<pre><code>java.util.NoSuchElementException: Failed to find a default value for dictionary\nat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:780)\nat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:780)\nat scala.Option.getOrElse(Option.scala:121)\nat org.apache.spark.ml.param.Params$class.getOrDefault(params.scala:779)\nat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:42)\nat org.apache.spark.ml.param.Params$class.$(params.scala:786)\nat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:42)\nat com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetector.train(SentimentDetector.scala:62)\nat com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetector.train(SentimentDetector.scala:12)\nat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:45)\nat org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)\nat org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)\nat scala.collection.Iterator$class.foreach(Iterator.scala:891)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\nat scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)\nat scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)\nat org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)\n</code></pre>\n\n<p>I have gone through examples but i was not able to find any clear example/documentation of doing sentiment analysis in java using default model.</p>\n",
    "score": 4,
    "creation_date": 1568985392,
    "view_count": 712,
    "answer_count": 1,
    "tags": "java;apache-spark;nlp;apache-spark-mllib;johnsnowlabs-spark-nlp"
  },
  {
    "question_id": 57082918,
    "title": "Tensorflow: AttributeError: module &#39;tensorflow.python.ops.nn&#39; has no attribute &#39;softmax_cross_entropy_with_logits_v2&#39;",
    "body": "<p>When I run this, I get AttributeError: AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'softmax_cross_entropy_with_logits_v2'. May I get any help?</p>\n\n<pre><code>import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.reset_default_graph()\n\n\nsentences = [\"Bless the Lord oh my soul\",\n             \"Oh my soul\",\n             \"Worship His Holy name\",\n             \"Sing like never before\",\n             \"Oh my soul\",\n             \"I'll worship Your Holy name\"]\n\nword_sequence = \" \".join(sentences).split()\nword_list = \" \".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\n\n# Word2Vec Parameter\nbatch_size = 20\nembedding_size = 2 # To show 2 dim embedding graph\nvoc_size = len(word_list)\n\ndef random_batch(data, size):\n    random_inputs = []\n    random_labels = []\n    random_index = np.random.choice(range(len(data)), size, replace=False)\n\n    for i in random_index:\n        random_inputs.append(np.eye(voc_size)[data[i][0]])  # target\n        random_labels.append(np.eye(voc_size)[data[i][1]])  # context word\n\n    return random_inputs, random_labels\n\n# Make skip gram of one size window\nskip_grams = []\nfor i in range(1, len(word_sequence) - 1):\n    target = word_dict[word_sequence[i]]\n    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n\n    for w in context:\n        skip_grams.append([target, w])\n\n# Model\ninputs = tf.placeholder(tf.float32, shape=[None, voc_size])\nlabels = tf.placeholder(tf.float32, shape=[None, voc_size])\n\n# W and WT is not Traspose relationship\nW = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\nWT = tf.Variable(tf.random_uniform([embedding_size, voc_size], -1.0, 1.0))\n\nhidden_layer = tf.matmul(inputs, W) # [batch_size, embedding_size]\noutput_layer = tf.matmul(hidden_layer, WT) # [batch_size, voc_size]\n\ncost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=labels))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    for epoch in range(5000):\n        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n        _, loss = sess.run([optimizer, cost], feed_dict={inputs: batch_inputs, labels: batch_labels})\n\n        if (epoch + 1)%1000 == 0:\n            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\n        trained_embeddings = W.eval()\n\nfor i, label in enumerate(word_list):\n    x, y = trained_embeddings[i]\n    plt.scatter(x, y)\n    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\nplt.show()\n</code></pre>\n\n<p>In the line 64, in , cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=labels)) AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'softmax_cross_entropy_with_logits_v2'.</p>\n\n<p>I tried to find some help from Google, but I didn't get any useful information.  Thank you for your help.</p>\n",
    "score": 4,
    "creation_date": 1563391687,
    "view_count": 4130,
    "answer_count": 1,
    "tags": "python;tensorflow;nlp"
  },
  {
    "question_id": 56198440,
    "title": "Low rank approximation using scipy",
    "body": "<p>I'm trying to use <strong>low-rank-approximation</strong> for <strong>latent semantic indexing</strong>. I thought that doing low rank approximation reduces matrix dimensions but it contradicts the results I get.</p>\n\n<p>Assume I have my dictionary with 40 000 words and 2000 documents. Then my term-by-document matrix is 40 000 x 2000.\nAccording to wikipedia, I have to do SVD of a matrix and then apply </p>\n\n<p><a href=\"https://i.sstatic.net/8TcNQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/8TcNQ.png\" alt=\"enter image description here\"></a></p>\n\n<p>This is the code I use for SVD and low rank approximation (the matrix is sparse):</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import scipy\nimport numpy as np\n\nu, s, vt = scipy.sparse.linalg.svds(search_matrix, k=20)\nsearch_matrix = u @ np.diag(s) @ vt\n\nprint('u: ', u.shape) # (40000, 20)\nprint('s: ', s.shape) # (20, )\nprint('vt: ', vt.shape) # (20, 2000)\n</code></pre>\n\n<p>The result matrix is: (40 000 x 20) * (20 x 20) * (20, 2000) = 40 000 x 2000, which is exactly what I started with. </p>\n\n<p>So... how does the low-rank-approximation reduce the dimensions of the matrix exactly?</p>\n\n<p>Also, I will be doing queries on this approximated matrix to find correlation between user vector and each document (naive search engine). The user vector has dimensions 40 000 x 1 to start with (<strong>bag of words</strong>). According to the same wikipedia page, this is what I should do:</p>\n\n<p><a href=\"https://i.sstatic.net/KLOdx.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/KLOdx.png\" alt=\"enter image description here\"></a></p>\n\n<p>The code:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>user_vec = np.diag((1 / s)) @ u.T @ user_vec\n</code></pre>\n\n<p>And it produces a matrix 20 x 1 which is what I expected!\n((20 x 20) * (20 x 40 000) * (40 000 x 1) = (20 x 1)). But now, it has dimensions that do not match the search_matrix I want to multiply it with.</p>\n\n<p>So... What am I doing wrong and why?</p>\n\n<p>Sources:</p>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Latent_semantic_analysis\" rel=\"nofollow noreferrer\">https://en.wikipedia.org/wiki/Latent_semantic_analysis</a></li>\n</ul>\n",
    "score": 4,
    "creation_date": 1558178300,
    "view_count": 7544,
    "answer_count": 1,
    "tags": "python;numpy;scipy;nlp;svd"
  },
  {
    "question_id": 52236776,
    "title": "Add a SpaCy Tokenizer Exception: Do not split &#39;&gt;&gt;&#39;",
    "body": "<p>I am trying to add an exception to recognize '>>' and '>> ' as an indicator to start a new sentence. For example, </p>\n\n<pre><code>import spacy\n\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(u'&gt;&gt; We should. &gt;&gt;No.')\n\nfor sent in doc.sents:\n    print (sent)\n</code></pre>\n\n<p>It prints out:</p>\n\n<pre><code>&gt;&gt; We should.\n&gt;\n&gt;\nNo.\n</code></pre>\n\n<p>But, I'd like it to print out: </p>\n\n<pre><code>&gt;&gt; We should.\n&gt;&gt; No. \n</code></pre>\n\n<p>Thank you for your time in advance!</p>\n",
    "score": 4,
    "creation_date": 1536420978,
    "view_count": 1331,
    "answer_count": 1,
    "tags": "nlp;tokenize;spacy"
  },
  {
    "question_id": 50038347,
    "title": "Do gensim Doc2Vec distinguish between same Sentence with positive and negative context.?",
    "body": "<p>While learning Doc2Vec library, I got stuck on the following question.</p>\n\n<p><strong>Do gensim Doc2Vec distinguish between the same Sentence with positive and negative context?</strong></p>\n\n<p>For Example:</p>\n\n<p>Sentence A: \"I love Machine Learning\"</p>\n\n<p>Sentence B: \"I do not love Machine Learning\"</p>\n\n<p>If I train sentence A and B with doc2vec and find cosine similarity between their vectors:</p>\n\n<ol>\n<li>Will the model be able to distinguish the sentence and give a cosine similarity very less than 1 or negative?</li>\n<li>Or Will the model represent both the sentences very close in vector space and give cosine similarity close to 1, as mostly all the words are same except the negative word (do not).</li>\n</ol>\n\n<p>Also, If I train only on sentence A and try to infer Sentence B, will both vectors be close to each other in vector space.?</p>\n\n<p>I would request the NLP community and Doc2Vec experts for helping me out in understanding this.</p>\n\n<p>Thanks in Advance !!</p>\n",
    "score": 4,
    "creation_date": 1524731506,
    "view_count": 988,
    "answer_count": 2,
    "tags": "python;nlp;gensim;doc2vec"
  },
  {
    "question_id": 49663436,
    "title": "Openrefine: Split multi-valued cells by token/word count?",
    "body": "<p>I have a large corpus of text data that I'm pre-processing for <a href=\"http://mallet.cs.umass.edu/\" rel=\"nofollow noreferrer\">document classification with MALLET</a> using <a href=\"http://openrefine.org\" rel=\"nofollow noreferrer\">openrefine</a>.</p>\n\n<p>Some of the cells are long (>150,000 characters) and I'm trying to split them into &lt;1,000 word/token segments.</p>\n\n<p>I'm able to split long cells into 6,000 character chunks using the \"Split multi-valued cells\" by field length, which roughly translates to 1,000 word/token chunks, but it splits words across rows, so I'm losing some of my data.</p>\n\n<p>Is there a function I could use to split long cells by the first whitespace (\" \") after every 6,000th character, or even better, split every 1,000 words?</p>\n",
    "score": 4,
    "creation_date": 1522897011,
    "view_count": 438,
    "answer_count": 2,
    "tags": "nlp;openrefine"
  },
  {
    "question_id": 47687797,
    "title": "When to remove stop words when using bigram_measures like PMI?",
    "body": "<p>I need to verify an overall approach to dealing with bigram stop words that are returned from bigram_measures such as PMI. Why deal with these stop words? Well, they're noise and don’t add any additional value past a certain point.</p>\n<p>I've seen several specific examples of how to use bigram_measures. However, I'm wondering WHEN it's best to remove stop word in the overall process of cleaning data, expansion, lemmatizing/stemming, etc.</p>\n<p>And yes, I am using a corpus that is sufficiently large. I remember the size of your corpus will also affect the quality of the bigram_measures result.</p>\n<p>Based on the accepted answer in this post (<a href=\"https://stackoverflow.com/questions/19145332/nltk-counting-frequency-of-bigram\">NLTK - Counting Frequency of Bigram</a>) it seems that stop words could be removed after PMI or other bigram_measures are used on the corpus.</p>\n<blockquote>\n<p>&quot;Imagine that if filtering collocations was simply deleting them, then there were many probability measures such as liklihood ratio or the PMI itself (that compute probability of a word relative to other words in a corpus) which would not function properly after deleting words from random positions in the given corpus. By deleting some collocations from the given list of words, many potential functionalities and computations would be disabled...&quot;</p>\n</blockquote>\n<p>Therefore, I believe the best process is:</p>\n<ol>\n<li>Clean the text and remove garbage chars like HTML tags, etc.</li>\n<li>Expand contractions (e.g.: they're -&gt; they are)</li>\n<li>Lemmatize or stem to normalize the words</li>\n<li>Calculate bigrams using bigram_measures like PMI. You can calculate bigrams using other methods, but this is what I'm using.</li>\n<li>Apply a frequency filter like &quot;apply_freq_filter(N)&quot; to get the bigrams that occur above your threshold. Note this will still return some bigrams with stop words mixed in with valuable bigrams.</li>\n<li>Check to see if BOTH words are stop words. If yes, then don't include that bigram in the final results but leave them in the corpus for the reasons quoted above.</li>\n</ol>\n<p>Is this a correct overall approach to dealing with bigram stop words mixed in with valuable bigrams?</p>\n",
    "score": 4,
    "creation_date": 1512622153,
    "view_count": 3445,
    "answer_count": 1,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 46519084,
    "title": "NLTK pos_tag module returns LookupError",
    "body": "<p><a href=\"https://i.sstatic.net/Axcln.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Axcln.jpg\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/Ux45K.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Ux45K.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>The details are on the above.\nI run it on Jupiter notebook, and get the error message.</p>\n",
    "score": 4,
    "creation_date": 1506914691,
    "view_count": 4213,
    "answer_count": 1,
    "tags": "python;nlp;nltk;pos-tagger"
  },
  {
    "question_id": 45339229,
    "title": "nltk word_tokenize: why do sentence tokenization before word tokenization?",
    "body": "<p>As noted in the <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/tokenize/__init__.py#L113\" rel=\"nofollow noreferrer\">source code</a>, <code>word_tokenize</code> runs a sentence tokenizer(Punkt) before running the word tokenizer(Treebank):</p>\n\n<pre class=\"lang-python prettyprint-override\"><code># Standard word tokenizer.\n_treebank_word_tokenizer = TreebankWordTokenizer()\n\ndef word_tokenize(text, language='english', preserve_line=False):\n    \"\"\"\n    Return a tokenized copy of *text*,\n    using NLTK's recommended word tokenizer\n    (currently an improved :class:`.TreebankWordTokenizer`\n    along with :class:`.PunktSentenceTokenizer`\n    for the specified language).\n    :param text: text to split into words\n    :param text: str\n    :param language: the model name in the Punkt corpus\n    :type language: str\n    :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n    :type preserver_line: bool\n    \"\"\"\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n    return [token for sent in sentences\n            for token in _treebank_word_tokenizer.tokenize(sent)]\n</code></pre>\n\n<p>What is the benefit of doing sentence tokenization prior to word tokenization? </p>\n",
    "score": 4,
    "creation_date": 1501113454,
    "view_count": 4691,
    "answer_count": 1,
    "tags": "python;nlp;nltk;tokenize"
  },
  {
    "question_id": 45019446,
    "title": "Identifying multiple categories and associated sentiment within text",
    "body": "<p>If you have a corpus of text, how can you identify all the categories (from a list of pre-defined categories) and the associated sentiment (positive/negative writing) with it?</p>\n\n<p>I will be doing this in Python but at this stage I am not necessarily looking for a language specific solution.</p>\n\n<hr>\n\n<p>Let's look at this question with an example to try and clarify what I am asking.</p>\n\n<p>If I have a whole corpus of reviews for products e.g.:</p>\n\n<blockquote>\n  <p>Microsoft's Xbox One offers impressive graphics and a solid list of exclusive 2015 titles. The Microsoft console currently edges ahead of the PS4 with a better selection of media apps. The console's fall-2015 dashboard update is a noticeable improvement. The console has backward compatibility with around 100 Xbox 360 titles, and that list is poised to grow. The Xbox One's new interface is still more convoluted than the PS4's. In general, the PS4 delivers slightly better installation times, graphics and performance on cross-platform games. The Xbox One also lags behind the PS4 in its selection of indie games. The Kinect's legacy is still a blemish. While the PS4 remains our overall preferred choice in the game console race, the Xbox One's significant course corrections and solid exclusives make it a compelling alternative.</p>\n</blockquote>\n\n<p>And I have a list of pre-defined categories e.g. :</p>\n\n<ul>\n<li>Graphics</li>\n<li>Game Play</li>\n<li>Game Selection</li>\n<li>Apps</li>\n<li>Performance</li>\n<li>Irrelevant/Other</li>\n</ul>\n\n<p>I could take my big corpus of reviews and break them down by sentence.  For each sentence in my training data I can hand tag them with the appropriate categories.  The problem is that there could be various categories in 1 sentence.</p>\n\n<p>If it was 1 category per sentence then any classification algorithm from scikit-learn would do the trick.  When working with multi-classes I could use something like multi-label classification. </p>\n\n<p>Adding in the sentiment is the trickier part.  Identifying sentiment in a sentence is a fairly simple task but if there is a mix of sentiment on different labels that becomes different.  </p>\n\n<p>The example sentence \"The Xbox One has a good selection of games but the performance is worse than the PS4\".  We can identify two of our pre-defined categories (game selection, performance) but we have positive sentiment towards game selection and a negative sentiment towards performance.  </p>\n\n<p>What would be a way to identify all categories in text (from our pre-defined list) with their associated sentiment?</p>\n",
    "score": 4,
    "creation_date": 1499711942,
    "view_count": 1674,
    "answer_count": 2,
    "tags": "python;machine-learning;nlp"
  },
  {
    "question_id": 40667782,
    "title": "Techniques other than RegEx to discover &#39;intent&#39; in sentences",
    "body": "<p>I'm embarking on a project for a non-profit organization to help process and classify 1000's of reports annually from their field workers / contractors the world over. I'm relatively new to NLP and as such wanted to seek the group's guidance on the <em>approach</em> to solve our problem. </p>\n\n<p>I'll highlight the current process, and our challenges and would love your help on the best way to solve our problem. </p>\n\n<p><strong>Current process:</strong> Field officers submit reports from locally run projects in the form of best practices. These reports are then processed by a full-time team of curators who (i) ensure they adhere to a best-practice template and (ii) edit the documents to improve language/style/grammar.</p>\n\n<p><strong>Challenge:</strong> As the number of field workers increased the volume of reports being generated has grown and our editors are now becoming the bottle-neck. </p>\n\n<p><strong>Solution:</strong> We would like to automate the 1st step of our process i.e., checking the document for compliance to the organizational best practice template  </p>\n\n<p>Basically, we need to ensure every report has 3 components namely: \n1. States its purpose: What topic / problem does this best practice address?\n2. Identifies Audience: Who is this for?\n3. Highlights Relevance: What can the reader do after reading it?</p>\n\n<p>Here's an example of a <em>good</em> report submission. </p>\n\n<p>\"This document introduces techniques for successfully applying best practices across developing countries. This study is intended to help low-income farmers identify a set of best practices for pricing agricultural products in places where there is no price transparency. By implementing these processes, farmers will be able to get better prices for their produce and raise their household incomes.\"</p>\n\n<p>As of now, our approach has been to use RegEx and check for keywords. i.e., to check for compliance we use the following logic:\n1 To check \"states purpose\" =  we do a regex to match 'purpose', 'intent'\n2 To check \"identifies audience\" = we do a regex to match with 'identifies', 'is for'\n3 To check \"highlights relevance\" = we do a regex to match with 'able to', 'allows', 'enables'</p>\n\n<p>The current approach of RegEx seems very primitive and limited so I wanted to ask the community if there is a better way to solving this problem using something like NLTK, CoreNLP. </p>\n\n<p>Thanks in advance.</p>\n",
    "score": 4,
    "creation_date": 1479431182,
    "view_count": 1286,
    "answer_count": 4,
    "tags": "nlp;nltk;linguistics;stanford-nlp"
  },
  {
    "question_id": 34539527,
    "title": "Why does &#39;corenlp.run&#39; yield different results when I run CoreNLP locally?",
    "body": "<p>The website <a href=\"http://corenlp.run\" rel=\"nofollow\">corenlp.run</a> which is supposed to be CoreNLP's demo site, shows pretty different results from when I run the CoreNLP pipeline on my local machine. </p>\n\n<p>The website actually shows the correct result, while the local machine version does not. I was wondering if anyone close to the CoreNLP project can explain the differences?</p>\n\n<p>Case in point - this is what happens when I use this as an input \"<em>Give me a restaurant on Soquel Drive that serves good french food</em>\" (this is from the RestQuery dataset)</p>\n\n<p>On CoreNLP (local machine, with Stanford's default model), I get this result:</p>\n\n<pre><code>root(ROOT-0, Give-1)\niobj(Give-1, me-2)\ndet(restaurant-4, a-3)\ndobj(Give-1, restaurant-4)\ncase(Drive-7, on-5)\ncompound(Drive-7, Soquel-6)\nnmod:on(Give-1, Drive-7) &lt;--- WRONG HEAD\nnsubj(serves-9, that-8)\nacl:relcl(Drive-7, serves-9) &lt;--- WRONG HEAD\namod(food-12, good-10)\namod(food-12, french-11)\ndobj(serves-9, food-12)\n</code></pre>\n\n<p>While on corenlp.run, I get this result:</p>\n\n<pre><code>root(ROOT-0, Give-1)\niobj(Give-1, me-2)\ndet(restaurant-4, a-3)\ndobj(Give-1, restaurant-4)\ncase(Drive-7, on-5)\ncompound(Drive-7, Soquel-6)\nnmod:on(restaurant-4, Drive-7) &lt;--- CORRECT HEAD\nnsubj(serves-9, that-8)\nacl:relcl(restaurant-4, serves-9) &lt;--- CORRECT HEAD\namod(food-12, good-10)\namod(food-12, french-11)\ndobj(serves-9, food-12)\n</code></pre>\n\n<p>You will note that there are two wrong heads in the local machine version. I have no idea why - especially if this is a model issue (I'm currently trying to debug the output of each annotator to see what the process returns)</p>\n\n<p>These are the annotators I used: \"tokenize,ssplit,pos,lemma,ner,parse,openie\". The models are straight out of CoreNLP version 3.6.0</p>\n\n<p>So can anyone help me understand why my results differ from the demo site's results?</p>\n",
    "score": 4,
    "creation_date": 1451522999,
    "view_count": 385,
    "answer_count": 1,
    "tags": "nlp;stanford-nlp"
  },
  {
    "question_id": 34230592,
    "title": "nltk quadgram collocation finder",
    "body": "<p>I am seeing mulitple questions and answers saying that NLTK collocation cannot be done beyond bi and tri grams. </p>\n\n<p>example this one - \n<a href=\"https://stackoverflow.com/questions/18672082/how-to-get-n-gram-collocations-and-association-in-python-nltk\">How to get n-gram collocations and association in python nltk?</a></p>\n\n<p>I am seeing that there is a something called </p>\n\n<p>nltk.QuadgramCollocationFinder</p>\n\n<p>Similar to </p>\n\n<p>nltk.BigramCollocationFinder and nltk.TrigramCollocationFinder</p>\n\n<p>But at the same time <strong>cannot</strong> see anything like </p>\n\n<p>nltk.collocations.QuadgramAssocMeasures() </p>\n\n<p>similar to \nnltk.collocations.BigramAssocMeasures() and nltk.collocations.TrigramAssocMeasures()</p>\n\n<p>What is the purpose of nltk.QuadgramCollocationFinder if its not possible (without hacks) to find n-grams beyond bi and tri grams.</p>\n\n<p>Maybe I am missing something.</p>\n\n<p>Thanks,</p>\n\n<p>Adding in the code and updating the question as per input from Alvas, this now works</p>\n\n<pre><code>import nltk\nfrom nltk.collocations import *\nfrom nltk.corpus import PlaintextCorpusReader\nfrom nltk.metrics.association import QuadgramAssocMeasures\n\nbigram_measures = nltk.collocations.BigramAssocMeasures()\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nquadgram_measures = QuadgramAssocMeasures()\n\nthe_filter = lambda *w: 'crazy' not in w\n\nfinder = BigramCollocationFinder.from_words(corpus)\nfinder.apply_freq_filter(3)\nfinder.apply_ngram_filter(the_filter)\nprint (finder.nbest(bigram_measures.likelihood_ratio, 10))\n\n\nfinder = QuadgramCollocationFinder.from_words(corpus)\nfinder.apply_freq_filter(3)\nfinder.apply_ngram_filter(the_filter)\nprint(finder.nbest(quadgram_measures.likelihood_ratio,10))\n</code></pre>\n",
    "score": 4,
    "creation_date": 1449860008,
    "view_count": 2706,
    "answer_count": 1,
    "tags": "python;nlp;nltk;n-gram;collocation"
  },
  {
    "question_id": 28769577,
    "title": "How to handle slang words and short forms in Tweets like luv , kool and brb?",
    "body": "<p>I am doing preprocessing of tweets using Python. However, a lot of words used are short forms of other words like luv, kool etc. And also, abbreviations like brb , ttyl etc.</p>\n\n<p>Right now, I can only think of having a huge Hashmap with words as keys and the actual words or expansions as values. Is there any other better way to approach this using NLP ? </p>\n\n<p>NOTE : I know question seems too vague. But please dont report it. I have asked this so that amateurs can benefit from this knowledge</p>\n\n<p>PS : Is there a nicely formatted text list that I can download and use? The links put down are good , but when i copy and paste it - they are not in an easily parsable format</p>\n",
    "score": 4,
    "creation_date": 1425053881,
    "view_count": 5117,
    "answer_count": 1,
    "tags": "twitter;nlp"
  },
  {
    "question_id": 28439522,
    "title": "Is it possible to get a natural word after it has been stemmed?",
    "body": "<p>I have a word <strong><em>play</em></strong> which after stemming has become <strong><em>plai</em></strong>. Now I want to get <strong><em>play</em></strong> again. Is it possible? I have used Porter's Stemmer.</p>\n",
    "score": 4,
    "creation_date": 1423594369,
    "view_count": 434,
    "answer_count": 2,
    "tags": "nlp;stemming;porter-stemmer"
  },
  {
    "question_id": 27730501,
    "title": "Extracting Entity-Verb relations from open knowledge bases like Freebase and DBPedia",
    "body": "<p>Is there any way that we can extract entity-verb relations from already existing online KBs like Freebase, DBPedia, Wikidata or Wordnet, I checked and only found that these sources concentrate on entities.</p>\n\n<p>My aim is to derive relations like \"A Person can Eat\", \"A Car can move\", \"A Man can play football\".</p>\n",
    "score": 4,
    "creation_date": 1420112867,
    "view_count": 386,
    "answer_count": 2,
    "tags": "nlp;freebase;dbpedia;wordnet;wikidata"
  },
  {
    "question_id": 27540082,
    "title": "Determine element styles at all screen sizes",
    "body": "<p>In Javascript we can use something like <code>window.getComputedStyle(element,null).getPropertyValue(property)</code> to retrieve a given element's style property. That being said, any property can change with responsive web design at any screen size.</p>\n\n<p>I'm wondering if it's possible to analyze the element's related stylesheet to determine the styles that will be applied to it at a all window sizes/breakpoints.</p>\n\n<p>For example, a <code>&lt;p&gt;</code> has <code>font-size: 16px</code> on desktop, and <code>font-size: 18px</code> on mobile (set by <code>@media (max-width: 768px) { ... }</code>. I'd like to know that the font size will be 18px on mobile  without having to resize down to mobile size and sample the font size again.</p>\n\n<p>I suppose with some clever text processing in JS I could sort through a stylesheet for <code>@media</code> and see if it reflects on the element, but for larger or multiple stylesheets, inline styles, and injected styles that method would likely be impossible to get 100% accurate.</p>\n\n<p>Any ideas?</p>\n\n<p><strong>Thought...</strong></p>\n\n<p>Is it possible to wrap an element in a simulated (hidden) <code>window</code> element and then use JS to resize it so it triggers the media queries?</p>\n\n<p><strong>Another approach...</strong></p>\n\n<p>I started playing around with <code>document.styleSheets</code> but even that seems like a pretty impossible task to get perfect. In my example below I have wrapped some selected text in an element and then passed it to the method below (written in coffeescript).</p>\n\n<pre><code>analyzeSelectiontyles: (selectionElement) -&gt;\n    selectionParents = []\n    while selectionElement\n        selectionParents.unshift selectionElement\n        selectionElement = selectionElement.parentNode\n\n    pageStylesheets = document.styleSheets\n\n    for stylesheet in pageStylesheets\n        rules = stylesheet.cssRules\n        for rule of rules\n            if rules[rule][\"type\"] is 1\n                console.log 'standard style'\n            if rules[rule][\"type\"] is 4\n                console.log 'media query'\n            # If it's a standard style rule or a media query containing\n            # style rules we have to check to see if the style rule applies\n            # to any one of our selectionParents defined above, and in\n            # Which order so as to simulate \"cascading\"\n</code></pre>\n",
    "score": 4,
    "creation_date": 1418882237,
    "view_count": 142,
    "answer_count": 2,
    "tags": "javascript;responsive-design;media-queries;stylesheet;text-analysis"
  },
  {
    "question_id": 27487671,
    "title": "Extract associated values from text using NLP",
    "body": "<p>I want to extract Cardinal(CD) values associated with Units of Measurement and store it in a dictionary. For example if the text contains tokens like \"20 kgs\", it should extract it and keep it in a dictionary.</p>\n\n<p>Example:</p>\n\n<ol>\n<li><p>for input text, <em>“10-inch fry pan offers superb heat conductivity and distribution”</em>, the output dictionary should look like, <code>{\"dimension\":\"10-inch\"}</code></p></li>\n<li><p>for input text, <em>\"This bucket holds 5 litres of water.\"</em>, the output should look like, <code>{\"volume\": \"5 litres\"}</code></p>\n\n<pre><code>line = 'This bucket holds 5 litres of water.'\ntokenized = nltk.word_tokenize(line)\ntagged = nltk.pos_tag(tokenized)\n</code></pre></li>\n</ol>\n\n<p>The above line would give the output:</p>\n\n<pre><code>[('This', 'DT'), ('bucket', 'NN'), ('holds', 'VBZ'), ('5', 'CD'), ('litres', 'NNS'), ('of', 'IN'), ('water', 'NN'), ('.', '.')]\n</code></pre>\n\n<p>Is there a way to extract the CD and UOM values from the text?</p>\n",
    "score": 4,
    "creation_date": 1418657974,
    "view_count": 2000,
    "answer_count": 2,
    "tags": "python;nlp;nltk"
  },
  {
    "question_id": 27220927,
    "title": "Passing Term-Document Matrix to Gensim LDA Model",
    "body": "<p>My term-document matrix is in a numpy matrix format, and I have a dictionary to represent the  of the term-document matrix.</p>\n\n<p>Is there any way I can easily pass these two into Gensim's LDA model?</p>\n\n<pre><code>tdMatrix = np.load('tdmatrix.npy')\ndictionary = cPickle.load(open('dictionary.p', 'r')) # stores term represented by each column\n</code></pre>\n\n<p>Can I pass this somewhow to gensim.models.ldamodel.LDA?</p>\n",
    "score": 4,
    "creation_date": 1417401615,
    "view_count": 1890,
    "answer_count": 3,
    "tags": "python;numpy;machine-learning;nlp;gensim"
  },
  {
    "question_id": 26932797,
    "title": "Identifying the context of word in sentence",
    "body": "<p>I created classifier to classy the class of nouns,adjectives, Named entities in given sentence. I have used large Wikipedia dataset for classification.</p>\n\n<p>Like : </p>\n\n<p>Where Abraham Lincoln was born?</p>\n\n<p>So classifier will give this short of result - <code>word - class</code></p>\n\n<ul>\n<li>Where  - question </li>\n<li>Abraham Lincoln - Person, Movie, Book (because    classifier find Abraham Lincoln in all there categories) </li>\n<li>born - time</li>\n</ul>\n\n<p>When Titanic was released?</p>\n\n<ul>\n<li>when - question </li>\n<li>Titanic - Song, movie, Vehicle, Game (Titanic\nclassified in all these categories)</li>\n</ul>\n\n<p><strong>Is there any way to identify exact context for word?</strong></p>\n\n<p>Please see :</p>\n\n<ol>\n<li>Word sense disambiguation would not help here. Because there might not be near by word in sentence which can help</li>\n<li><p>Lesk algorithm with wordnet or sysnet also does not help. Because it for suppose word <code>Bank</code> lesk algo will behave like this</p>\n\n<p>======== TESTING simple_lesk ===========</p>\n\n<h1>TESTING simple_lesk() ...</h1>\n\n<p>Context: I went to the bank to deposit my money</p>\n\n<p>Sense: Synset('depository_financial_institution.n.01')</p>\n\n<p>Definition: a financial institution that accepts deposits and channels the money into lending activities</p>\n\n<h1>TESTING simple_lesk() with POS ...</h1>\n\n<p>Context: The river bank was full of dead fishes</p>\n\n<p>Sense: Synset('bank.n.01')</p>\n\n<p>Definition: sloping land (especially the slope beside a body of water)</p></li>\n</ol>\n\n<p>Here for word <code>bank</code> it suggested as <code>financial institute</code> and <code>slopping land</code>. While in my case I am already getting such prediction like <code>Titanic</code> then it can be <code>movie</code> or <code>game</code>.</p>\n\n<p>I want to know is there any other approach apart from <code>Lesk algo</code>, <code>baseline algo</code>, <code>traditional word sense disambiguation</code> which can help me to identify which class is correct for particular keyword? </p>\n\n<p>Titanic - </p>\n",
    "score": 4,
    "creation_date": 1415977203,
    "view_count": 3584,
    "answer_count": 1,
    "tags": "nlp;data-mining;nltk;semantics"
  },
  {
    "question_id": 26798393,
    "title": "Tools for identifying near duplicate documents",
    "body": "<p>I'm doing a NLP project and identifying near duplicate document is a part of that. Can anyone who has experience with this area suggest the tools (implementations like Weka) available for near duplicate detection?</p>\n\n<p>The project is about generating a statistical report for crimes after analyzing news articles of some local English news papers. The crime articles are firstly classified. Then duplicate articles should be detected and merged. Data collection may contain about 1000 crime related articles for near duplicate detection.</p>\n\n<p>I define near duplicates here as the articles containing the same crime incident. Sometimes different news papers may report the same incidents. Also same news paper may report news articles in different days.</p>\n\n<p>The time taken for duplicate detection is not a problem as this is not online processing. The accuracy is very important here.</p>\n\n<p>Thank you in advance.</p>\n",
    "score": 4,
    "creation_date": 1415353709,
    "view_count": 1737,
    "answer_count": 1,
    "tags": "nlp"
  },
  {
    "question_id": 25134160,
    "title": "what&#39;s the meaning of the categories in the corpus reuters of NLTK",
    "body": "<p>I suffered from problems, when doing text topic classification.</p>\n\n<p>I got the data in NLTK \"reuters\" corpus..</p>\n\n<p>However when I try \"reuters.categories()\"</p>\n\n<p>the result is</p>\n\n<p>['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']</p>\n\n<p>I almost don't know what each one means, can I find some explanations ?</p>\n",
    "score": 4,
    "creation_date": 1407226822,
    "view_count": 9583,
    "answer_count": 1,
    "tags": "python;nlp;nltk;corpus"
  },
  {
    "question_id": 24159098,
    "title": "Feature Construction for Text Classification using Autoencoders",
    "body": "<p>Autoencoders can be used to reduce dimensionallity in feature vectors - as far as I understand. In text classification a feature vector is normally constructed via a dictionary - which tends to be extremely large. I have no experience in using autoencoders, so my questions are:</p>\n\n<ol>\n<li>Could autoencoders be used to reduce dimensionallity in text classification? (Why? / Why not?)</li>\n<li>Has anyone already done this? A source would be nice, if so.</li>\n</ol>\n",
    "score": 4,
    "creation_date": 1402478365,
    "view_count": 1646,
    "answer_count": 1,
    "tags": "nlp;text-classification;autoencoder"
  },
  {
    "question_id": 22937922,
    "title": "Automatic semantic role labeling (ASRL) in Java (using Frame net in Java)",
    "body": "<p>I'm looking for a long time to create ASRL analysis in Java, and unfortunately the web offers very little support, it seems like all of the other SO questions relate to \"which tools to use\", but not to \"how to use them\".</p>\n\n<p>I want to create (preferably in java) something exactly like this : <a href=\"http://demo.ark.cs.cmu.edu/parse\" rel=\"nofollow\">http://demo.ark.cs.cmu.edu/parse</a>\n,an algorithm that has sentences as input, and frames as output.</p>\n\n<p>I downloaded the related Jar files of <code>mate tools</code> <a href=\"https://code.google.com/p/mate-tools/downloads/list\" rel=\"nofollow\">https://code.google.com/p/mate-tools/downloads/list</a> and \nSEMAFOR <a href=\"http://www.ark.cs.cmu.edu/SEMAFOR/\" rel=\"nofollow\">http://www.ark.cs.cmu.edu/SEMAFOR/</a>, but from here I'm stuck, i couldn't find any way of creating java code that does that.</p>\n\n<p>Does any of you guys have examples of Java code (<code>SEMAFOR</code> or <code>Mate tools</code>) that demonstrates how to convert a sentence (or any text input) into frame elements?</p>\n\n<p>ill appreciate any help on that. </p>\n",
    "score": 4,
    "creation_date": 1396962126,
    "view_count": 1695,
    "answer_count": 1,
    "tags": "java;nlp"
  },
  {
    "question_id": 22481094,
    "title": "Passing Python strings to Mallet for topic modelling",
    "body": "<p>I'm building a corpus of texts harvested alongside some metadata from HTML with BeautifulSoup. It would be really helpful if I could call Mallet from within Python, and have it model topics from Python strings, rather than from text files in a directory. That way I could put the n keywords located by Mallet into each file. </p>\n\n<p>I get a message saying that Mallet has been recognised when I run:</p>\n\n<pre><code>from nltk.classify import mallet\nfrom subprocess import call\nmallet.config_mallet(\"malletdir/mallet-2.0.7/bin\")\n</code></pre>\n\n<p>But I haven't had any luck with the next steps, and am not even sure if Mallet accepts anything other than saved files.</p>\n\n<p>I have not been able to turn up any documentation that I can really understand. Has anybody seen digestable documentation for this? (The NLTK book doesn't get into Mallet). I would also be happy to learn of any other means of topic modelling within Python that I could operationalise without a really deep knowledge of Python.</p>\n\n<p>Sorry, this is my first rodeo.</p>\n",
    "score": 4,
    "creation_date": 1395149833,
    "view_count": 1123,
    "answer_count": 2,
    "tags": "python;nlp;nltk;topic-modeling;mallet"
  },
  {
    "question_id": 20765738,
    "title": "Get parse tree of a sentence using OpenNLP. Getting stuck with example.",
    "body": "<p>OpenNLP is an Apache project on Natural Language Processing. One of the aims of an NLP program is to parse a sentence giving a tree of its grammatical structure. For example, the sentence \"The sky is blue.\" might be parsed as</p>\n\n<pre><code>      S\n     / \\\n   NP   VP\n  / \\    | \\\nThe sky is blue.\n</code></pre>\n\n<p>where <code>S</code> is Sentence, <code>NP</code> is Noun-phrase, and <code>VP</code> is Verb-phrase. Equivalently the above tree can be written down as a parenthesized string like this: <code>S(NP(The sky) VP(is blue.))</code></p>\n\n<p>I am trying to be able to get the parenthesized strings from sentences using OpenNLP, but I can't get the example code to work. </p>\n\n<p>In particular, I am following along <a href=\"http://www.programcreek.com/2012/05/opennlp-tutorial/#parser\" rel=\"nofollow\">the last part of this tutorial</a> and my code gets stuck at initializing <code>ParserModel</code>.</p>\n\n<p>I have downloaded the appropriate binaries from <a href=\"http://opennlp.apache.org/cgi-bin/download.cgi\" rel=\"nofollow\">here</a> and added <code>opennlp-tools-1.5.3.jar</code> (which includes classes for all of the following objects) as a library to my IntelliJ project. Also, I moved <code>en-parser-chunking.bin</code> to my \"user.dir.\"</p>\n\n<p>The following is the code which should give me a parse tree, but it runs indefinitely at creating the <code>ParserModel</code> object. </p>\n\n<pre><code>    InputStream is = new FileInputStream(\"en-parser-chunking.bin\");\n    ParserModel model = new ParserModel(is);\n    Parser parser = ParserFactory.create(model);\n    String sentence = \"The sky is blue.\";\n    Parse topParses[] = ParserTool.parseLine(sentence, parser, 1);\n    for (Parse p : topParses)\n        p.show();\n    is.close();\n</code></pre>\n\n<p>It's my first day of working with OpenNLP, but I can't even get this simple example to work. </p>\n",
    "score": 4,
    "creation_date": 1387912244,
    "view_count": 4953,
    "answer_count": 2,
    "tags": "java;parsing;intellij-idea;nlp;opennlp"
  },
  {
    "question_id": 19856721,
    "title": "calculating confidence while doing classification",
    "body": "<p>I am using a Naive Bayes algorithm to predict movie ratings as positive or negative. I have been able to rate movies with 81% accuracy. I am, however, trying to assign a 'confidence level' for each of the ratings as well.</p>\n\n<p>I am trying to identify how I can tell the user something like \"we think that the review is positive <strong>with 80% confidence</strong>\". Can someone help me understand how I can calculate a confidence level to our classification result?</p>\n",
    "score": 4,
    "creation_date": 1383906358,
    "view_count": 2693,
    "answer_count": 1,
    "tags": "statistics;machine-learning;nlp;probability"
  },
  {
    "question_id": 17461869,
    "title": "Algorithm to determine quality of an article",
    "body": "<p>I am working on a project that requires me to parse news articles and determine the best among them. I figured out that to determine the quality of an article, I would need three main parameters: Length of an article, facebook shares/ retweets and the time since the article was posted.</p>\n\n<p>The problem I am facing now is how do I put together all three parameters in a mathematical function and come-up with a score for each of the articles? The score assigned to each one of them would help me rank the articles and show it to the users.</p>\n\n<p>Also let me know if there is any other parameter that I need to consider in determining the quality.</p>\n",
    "score": 4,
    "creation_date": 1372911944,
    "view_count": 517,
    "answer_count": 2,
    "tags": "algorithm;nlp;data-mining;data-modeling;text-parsing"
  },
  {
    "question_id": 15988904,
    "title": "Find basic words and estimate their difficulty",
    "body": "<p>I'm looking for a possibly simple solution of the following problem:</p>\n\n<p><strong>Given input of a sentence</strong> like</p>\n\n<pre><code>\"Absence makes the heart grow fonder.\"\n</code></pre>\n\n<p><strong>Produce a list of <em>basic</em> words followed by their difficulty/complexity</strong></p>\n\n<pre><code>[[\"absence\", 0.5], [\"make\", 0.05], [\"the\", 0.01\"], [\"grow\", 0.1\"], [\"fond\", 0.5]]\n</code></pre>\n\n<p><strong>Let's assume</strong> that:</p>\n\n<ul>\n<li>all the words in the sentence are valid English words</li>\n<li>popularity is an acceptable measure of difficulty/complexity</li>\n<li>base word can be understood in any constructive way (see below)</li>\n<li>difficulty/complexity is on scale from 0 - piece of cake to 1 - mind-boggling </li>\n<li>difficulty bias is ok, better to be mistaken saying easy is though than the other way</li>\n<li>working simple solution is preferred to flawless but complicated stuff</li>\n<li><strong>[edit]</strong> there is no interaction with user</li>\n<li><strong>[edit]</strong> we can handle any proper English input</li>\n<li><strong>[edit]</strong> a word is not more difficult than it's basic form (because as smart beings we can create <em>unhappily</em> if we know <em>happy</em>), unless it creates a new word (<em>unlikely</em> is not same difficulty as <em>like</em>)</li>\n</ul>\n\n<p><strong>General ideas:</strong></p>\n\n<p>I considered using Google searches or sites like <a href=\"http://www.wordcount.org/main.php\" rel=\"nofollow\">Wordcount</a> to estimate words popularity that could indicate its difficulty. However, both solutions give different results depending on the form of entered words. Google gives 316m results for <em>fond</em> but 11m for <em>fonder</em>, whereas Wordcount gives them ranks of 6k and 54k.</p>\n\n<p>Transforming words to their basic forms is not a must but solves ambiguity problem (and makes it easy to create dictionary links), however it's not a simple task and its sense could me found arguable. Obviously <em>fond</em> should be taken instead of <em>fonder</em>, however investigating <em>believe</em> instead of <em>unbelievable</em> seems to be an overkill (<strong>[edit]</strong> it might be not the best example, but there is a moment when modifying basic word we create a new one <em>like</em> -> <em>likely</em>) and words like <em>doorkeeper</em> shouldn't be cut into two.</p>\n\n<p>Some ideas of what should be consider basic word can be found <a href=\"http://en.wikipedia.org/wiki/Basic_English#Rules\" rel=\"nofollow\">here on Wikipedia</a> but maybe a simpler way of determining it would be a use of a dictionary. For instance according to dictionary.reference.com <a href=\"http://dictionary.reference.com/browse/unbelievable\" rel=\"nofollow\">unbelievable is a basic word</a> whereas <a href=\"http://dictionary.reference.com/browse/fonder\" rel=\"nofollow\">fonder comes from fond</a> but then <a href=\"http://dictionary.reference.com/browse/growing\" rel=\"nofollow\">grow is not the same as growing</a></p>\n\n<p><strong>Idea of a solution:</strong></p>\n\n<p>It seems to me that the best way to handle the problem would be using a dictionary to find <em>basic</em> words, apply some of the Wikipedia rules and then use Wordcount (maybe combined with number of Google searches) to estimate difficulty.</p>\n\n<p>Still, there might (probably is a simpler and better) way or ready to use algorithms. I would appreciate any solution that deals with this problem and is easy to put in practice. Maybe I'm just trying to reinvent the wheel (or maybe you know my approach would work just fine and I'm wasting my time deliberating instead of coding what I have). I would, however, prefer to avoid implementing frequency analysis algorithms or preparing a corpus of texts.</p>\n",
    "score": 4,
    "creation_date": 1365863610,
    "view_count": 509,
    "answer_count": 2,
    "tags": "algorithm;language-agnostic;nlp;heuristics"
  },
  {
    "question_id": 14540630,
    "title": "Comparison of binary vs tfidf Ngram features in sentiment analysis / classification tasks?",
    "body": "<p>Simple question again: Is it better to use Ngrams (unigram/ bigrams etc) as simple binary features  or rather use their Tfidf scores in ML models such as Support Vectory Machines for performing NLP tasks such as sentiment analysis or text categorization/classification?</p>\n",
    "score": 4,
    "creation_date": 1359227992,
    "view_count": 1645,
    "answer_count": 1,
    "tags": "machine-learning;nlp;artificial-intelligence;n-gram;tf-idf"
  },
  {
    "question_id": 13186995,
    "title": "Finding collocation patterns in Java",
    "body": "<p>I am working in a project which require the use of collocations. I have created the following code to extract them. The code takes a string and returns a list of the collocation patterns in this string. I have used Stanford POS to do the tagging. </p>\n\n<p>I need your suggestion on the code, it seems very slow as I process huge amount of text.\nAny suggestion to improve the code would be highly appreciated.</p>\n\n<pre><code>/**\n*\n*  A COLLOCATION is an expression consisting of two or more words that\n*  correspond to some conventional way of saying things.\n* \n*  I used the seventh Part-of-speech-tag patterns for collocation filtering that \n*  were suggested by Justeson and Katz(1995).\n*  These patterns are:\n* \n*  -----------------------------------------\n*  |Tag |     Pattern Example              |\n*  -----------------------------------------\n*  |AN  | linear function                  |\n*  |NN  | regression coefficients          |\n*  |AAN | Gaussian random variable         |\n*  |ANN | cumulative distribution function |\n*  |NAN | mean squared error               |\n*  |NNN | class probability function       |\n*  |NPN | degrees of freedom               |                     \n*  -----------------------------------------\n*  Where A=adjective, P=preposition, &amp; N=noun.\n* \n*  Stanford POS have been used for the extraction process. \n*  see: http://nlp.stanford.edu/software/tagger.shtml#Download\n* \n*  more on collocation:    http://nlp.stanford.edu/fsnlp/promo/colloc.pdf\n*  more on POS:            http://acl.ldc.upenn.edu/J/J93/J93-2004.pdf\n*  \n*/\n\npublic class GetCollocations {\n    public static ArrayList&lt;String&gt; GetCollocations(String text) throws IOException,                ClassNotFoundException{\n       MaxentTagger tagger = new MaxentTagger(\"taggers/wsj-0-18-left3words.tagger\");\n       String[] tagged = tagger.tagString(text).split(\"\\\\s+\");\n\n       ArrayList&lt;String&gt; collocations = new ArrayList();\n       for (int i = 0; i &lt; tagged.length; i++) {\n\n           String pot = tagged[i].substring(tagged[i].indexOf(\"_\") + 1);\n           if (pot.equals(\"NN\") || pot.equals(\"NNS\") || pot.equals(\"NNP\") ||    pot.equals(\"NNPS\")) {\n\n               pot = tagged[i + 1].substring(tagged[i + 1].indexOf(\"_\") + 1);\n               if (pot.equals(\"NN\") || pot.equals(\"NNS\") || pot.equals(\"NNP\") || pot.equals(\"NNPS\")) {\n\n                collocations.add(GetWordWithoutTag(tagged[i]) + \" \" + GetWordWithoutTag(tagged[i + 1]));\n\n                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(\"_\") + 1);\n                if (pot.equals(\"NN\") || pot.equals(\"NNS\") || pot.equals(\"NNP\") || pot.equals(\"NNPS\")) {\n                    collocations.add(GetWordWithoutTag(tagged[i]) + \" \" + GetWordWithoutTag(tagged[i + 1]) + \" \" + GetWordWithoutTag(tagged[i + 2]));\n                }\n\n            } else if (pot.equals(\"JJ\") || pot.equals(\"JJR\") || pot.equals(\"JJS\")) {\n                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(\"_\") + 1);\n\n                if (pot.equals(\"NN\") || pot.equals(\"NNS\") || pot.equals(\"NNP\") || pot.equals(\"NNPS\")) {\n                    collocations.add(GetWordWithoutTag(tagged[i]) + \" \" + GetWordWithoutTag(tagged[i + 1]) + \" \" + GetWordWithoutTag(tagged[i + 2]));\n                }\n\n            } else if (pot.equals(\"IN\")) {\n                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(\"_\") + 1);\n\n                if (pot.equals(\"NN\") || pot.equals(\"NNS\") || pot.equals(\"NNP\") || pot.equals(\"NNPS\")) {\n                    collocations.add(GetWordWithoutTag(tagged[i]) + \" \" + GetWordWithoutTag(tagged[i + 1]) + \" \" + GetWordWithoutTag(tagged[i + 2]));\n                }\n\n            }\n\n\n        } else if (pot.equals(\"JJ\") || pot.equals(\"JJR\") || pot.equals(\"JJS\")) {\n            pot = tagged[i + 1].substring(tagged[i + 1].indexOf(\"_\") + 1);\n            if (pot.equals(\"NN\") || pot.equals(\"NNS\") || pot.equals(\"NNP\") || pot.equals(\"NNPS\")) {\n                collocations.add(GetWordWithoutTag(tagged[i]) + \" \" + GetWordWithoutTag(tagged[i + 1]));\n                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(\"_\") + 1);\n                if (pot.equals(\"NN\") || pot.equals(\"NNS\") || pot.equals(\"NNP\") || pot.equals(\"NNPS\")) {\n                    collocations.add(GetWordWithoutTag(tagged[i]) + \" \" + GetWordWithoutTag(tagged[i + 1]) + \" \" + GetWordWithoutTag(tagged[i + 2]));\n                }\n\n            } else if (pot.equals(\"JJ\") || pot.equals(\"JJR\") || pot.equals(\"JJS\")) {\n                pot = tagged[i + 2].substring(tagged[i + 2].indexOf(\"_\") + 1);\n                if (pot.equals(\"NN\") || pot.equals(\"NNS\") || pot.equals(\"NNP\") || pot.equals(\"NNPS\")) {\n                    collocations.add(GetWordWithoutTag(tagged[i]) + \" \" + GetWordWithoutTag(tagged[i + 1]) + \" \" + GetWordWithoutTag(tagged[i + 2]));\n                }\n            }\n\n        }\n\n    }\n    return collocations;\n\n}\npublic static String GetWordWithoutTag(String wordWithTag){\n    String wordWithoutTag = wordWithTag.substring(0,wordWithTag.indexOf(\"_\"));\n    return wordWithoutTag;\n}\n\n}\n</code></pre>\n",
    "score": 4,
    "creation_date": 1351811967,
    "view_count": 1874,
    "answer_count": 1,
    "tags": "java;nlp;stanford-nlp"
  },
  {
    "question_id": 10020451,
    "title": "algorithm to extract simple sentences from complex(mixed) sentences?",
    "body": "<p>Is there an algorithm that can be used to extract simple sentences from paragraphs?</p>\n\n<p>My ultimate goal is to later run another algorithm on the resulted simple sentence to determine the author's sentiment.</p>\n\n<p>I've researched this from sources such as Chae-Deug Park but none discuss preparing simple sentences as training data.</p>\n\n<p>Thanks in advance</p>\n",
    "score": 4,
    "creation_date": 1333580295,
    "view_count": 1856,
    "answer_count": 2,
    "tags": "nlp;extract;text-mining;text-extraction;information-extraction"
  },
  {
    "question_id": 6497152,
    "title": "Simple toolkits for emotion (sentiment) analysis (not using machine learning)",
    "body": "<p>I am looking for a tool that can analyze the emotion of short texts. I searched for a week and I couldn't find a good one that is publicly available. The ideal tool is one that takes a short text as input and guesses the emotion. It is preferably a standalone application or library.</p>\n\n<p>I don't need tools that is trained by texts. And although similar questions are asked before no satisfactory answers are got.</p>\n\n<p>I searched the Internet and read some papers but I can't find a good tool I want. Currently I found SentiStrength, but the accuracy is not good. I am using emotional dictionaries right now. I felt that some syntax parsing may be necessary but it's too complex for me to build one. Furthermore, it's researched by some people and I don't want to reinvent the wheels. Does anyone know such publicly/research available software? I need a tool that doesn't need training before using.\nThanks in advance.</p>\n",
    "score": 4,
    "creation_date": 1309198686,
    "view_count": 1911,
    "answer_count": 3,
    "tags": "nlp;sentiment-analysis"
  },
  {
    "question_id": 6308185,
    "title": "English lemmatizer databases?",
    "body": "<p>Do you know any big enough lemmatizer database that returns correct result for following sample words:</p>\n\n<pre><code>geese: goose\nplantes: //not found\n</code></pre>\n\n<p>Wordnet's morphological analyzer is not sufficient, since it gives the following incorrect results:</p>\n\n<pre><code>geese: //not found\nplantes: plant\n</code></pre>\n",
    "score": 4,
    "creation_date": 1307718050,
    "view_count": 2516,
    "answer_count": 2,
    "tags": "nlp;stemming;lemmatization;morphological-analysis"
  },
  {
    "question_id": 5927688,
    "title": "How can I detect whether a given line in a file is a proper English sentence?",
    "body": "<p>I need to detect if a given \"line\" in a file is an English sentence or not. I am using Python. An approximate answer would do. I understand this is an NLP question but is there a lightweight tool that gives a reasonable approximation? I do not want to use a full-fledged NLP toolkit for this though if that is the only way then it is fine.</p>\n\n<p>If NLP toolkit is the answer then the one that I am reading about is the <a href=\"http://www.nltk.org/\" rel=\"nofollow\">Natural Language Toolkit</a>. If anyone has a simple example on how to detect a sentence handy, please point me to it.</p>\n",
    "score": 4,
    "creation_date": 1304860905,
    "view_count": 3036,
    "answer_count": 3,
    "tags": "python;nlp"
  },
  {
    "question_id": 5550617,
    "title": "how to replace all non alphanumeric characters with space in php?",
    "body": "<pre><code>$html=strip_tags($html);\n$html=ereg_replace(\"[^A-Za-zäÄÜüÖö]\",\" \",$html);\n$words = preg_split(\"/[\\s,]+/\", $html);\n</code></pre>\n\n<p>doesnt this replace all non (A-Z, a-z, a o u with umlauts) characters with space?\nI am losing words like zugänglich etc with umlauts</p>\n\n<p>is there any thing wrong with the regex?</p>\n\n<p>edit:</p>\n\n<p>I replaced ereg_replace with preg_replace but somehow the special characters like :, ® are not getting replace by space...</p>\n",
    "score": 4,
    "creation_date": 1302000533,
    "view_count": 2852,
    "answer_count": 3,
    "tags": "php;regex;nlp"
  },
  {
    "question_id": 1453552,
    "title": "What is a fast and unsupervised way of checking quality of pdf-extracted text?",
    "body": "<p>I am working on a somewhat large corpus with articles numbering the tens of thousands. I am currently using PDFBox to extract with various success, and I am looking for a way to programatically check each file to see if the extraction was moderately successful or not. I'm currently thinking of running a spellchecker on each of them, but the language can differ, I am not yet sure which languages I'm dealing with. Natural language detection with scores may also be an idea.</p>\n\n<p>Oh, and any method also has to play nice with Java, be fast and relatively quick to integrate.</p>\n",
    "score": 4,
    "creation_date": 1253524281,
    "view_count": 441,
    "answer_count": 3,
    "tags": "java;pdf;text;nlp"
  },
  {
    "question_id": 75972061,
    "title": "Creating a custom dataset based on CoNLL2003",
    "body": "<p>I'm working on a named entity recognition (NER) project and would like to create my own dataset based on the CoNLL2003 dataset (link: <a href=\"https://huggingface.co/datasets/conll2003\" rel=\"nofollow noreferrer\">https://huggingface.co/datasets/conll2003</a>). I've been looking at the CoNLL2003 data and I'm having trouble understanding how the chunk column is labeled. I'm not sure if it's based on the part-of-speech (POS) tags or on something else.\nIdeally, I'd like to automate the process of creating the chunk labels for my custom dataset, rather than doing it manually. Can someone explain how the chunk column is labeled in CoNLL2003 and provide some guidance on how I can programmatically generate the same labels for my own dataset?</p>\n<p>To expalin more let’s take the first row of the dataset and try to working on it and i should have the same results.</p>\n<p>The first sentence of the dataset which is : <code>EU rejects German call to boycott British lamb.</code></p>\n<ul>\n<li>To do this on python this is the code :</li>\n</ul>\n<pre><code># import libraries and modules needed for the project\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag, ne_chunk\nimport re\n# We take the first sentence from the dataset conll2003\nSentence = &quot;EU rejects German call to boycott British lamb.&quot;\n</code></pre>\n<p>The tokens of the same sentence is : <code>['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']</code></p>\n<ul>\n<li>To do this on python this is the code :</li>\n</ul>\n<pre><code>import pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag, ne_chunk\nimport re\n# Tokenize the sentence\ntokens = word_tokenize(Sentence)\n# Print the tokens\nprint(tokens)\n</code></pre>\n<p>The part of speech of the same sentence is : <code>['NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB','JJ', 'NN', '.']</code> which is <code>[22, 42, 16, 21, 35, 37, 16, 21, 7]</code></p>\n<ul>\n<li>To do this on python this is the code :</li>\n</ul>\n<pre><code>pos_tags = {'&quot;': 0, &quot;''&quot;: 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12, 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23, 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33, 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43, 'WP': 44, 'WP$': 45, 'WRB': 46}\n# POS Tagging of the tokens\npos_tagged = pos_tag(tokens)\n# Print the POS Tagged tokens\nprint(pos_tagged)\n# Keep only the POS tags in a list\npos_tags_only = [pos_tags[tag] for word, tag in pos_tagged]\n# Print the POS\nprint(pos_tags_only)\n</code></pre>\n<p>The chunk tags of the same sentence should be : <code>['B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'I-VP','B-NP', 'I-NP', 'O']</code> which is <code>[11, 21, 11, 12, 21, 22, 11, 12, 0]</code> but how they do it i don’t have any idea i alredy test a code but i don’t get exactly the same result as this <code>['B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'I-VP','B-NP', 'I-NP', 'O']</code>\nplease if any one have an idea of how they do it plese guide me</p>\n<pre><code># To start with, this is the chunk tag set used in conll2003 dataset\nchunk_tags = {'O': 0, 'B-ADJP': 1, 'I-ADJP': 2, 'B-ADVP': 3, 'I-ADVP': 4, 'B-CONJP': 5, 'I-CONJP': 6, 'B-INTJ': 7, 'I-INTJ': 8, 'B-LST': 9, 'I-LST': 10, 'B-NP': 11, 'I-NP': 12, 'B-PP': 13, 'I-PP': 14, 'B-PRT': 15, 'I-PRT': 16, 'B-SBAR': 17, 'I-SBAR': 18, 'B-UCP': 19, 'I-UCP': 20, 'B-VP': 21, 'I-VP': 22}  \n</code></pre>\n",
    "score": 4,
    "creation_date": 1681063236,
    "view_count": 938,
    "answer_count": 1,
    "tags": "python;nlp;dataset;conll"
  },
  {
    "question_id": 74708571,
    "title": "Model not calculating loss during training returning ValueError (Huggingface/BERT)",
    "body": "<p>I'm unable to properly pass my encoded data (with hidden states) through Trainer via Huggingface. Below is the call to Trainer with arguments and the full traceback. I'm not really sure where to begin with this error as I believe I've satisfied all requirements to pass the encoded data forward unless the inputs passed should include the labels.</p>\n<pre><code>from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    pred = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, pred, average=&quot;weighted&quot;)\n    acc = accuracy_score(labels, preds)\n    return {&quot;accuracy&quot;: acc, &quot;f1&quot;: f1}\n</code></pre>\n<pre><code>from transformers import Trainer, TrainingArguments\n\nbatch_size = 10\nlogging_steps = len(transcripts_encoded[&quot;train&quot;]) // batch_size\nmodel_name = f&quot;{model_checkpoint}-finetuned-transcripts&quot;\ntraining_args = TrainingArguments(output_dir=model_name,\n                                 num_train_epochs=2,\n                                 learning_rate=2e-5,\n                                 per_device_train_batch_size=batch_size,\n                                 per_device_eval_batch_size=batch_size,\n                                 weight_decay=0.01,\n                                 evaluation_strategy=&quot;epoch&quot;,\n                                 disable_tqdm=False,\n                                 logging_steps=logging_steps,\n                                 push_to_hub=False,\n                                 log_level=&quot;error&quot;)\n\nfrom transformers import Trainer\n\ntrainer = Trainer(model=model, args=training_args,\n                 compute_metrics=compute_metrics,\n                 train_dataset=transcripts_encoded[&quot;train&quot;],\n                 eval_dataset=transcripts_encoded[&quot;valid&quot;],\n                 tokenizer=tokenizer)\n\ntrainer.train();\n</code></pre>\n<h2><code>Here is the full traceback:</code></h2>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-124-76d295da3120&gt; in &lt;module&gt;\n     24                  tokenizer=tokenizer)\n     25 \n---&gt; 26 trainer.train();\n\n/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n   1503             resume_from_checkpoint=resume_from_checkpoint,\n   1504             trial=trial,\n-&gt; 1505             ignore_keys_for_eval=ignore_keys_for_eval,\n   1506         )\n   1507 \n\n/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n   1747                         tr_loss_step = self.training_step(model, inputs)\n   1748                 else:\n-&gt; 1749                     tr_loss_step = self.training_step(model, inputs)\n   1750 \n   1751                 if (\n\n/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in training_step(self, model, inputs)\n   2506 \n   2507         with self.compute_loss_context_manager():\n-&gt; 2508             loss = self.compute_loss(model, inputs)\n   2509 \n   2510         if self.args.n_gpu &gt; 1:\n\n/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in compute_loss(self, model, inputs, return_outputs)\n   2552             if isinstance(outputs, dict) and &quot;loss&quot; not in outputs:\n   2553                 raise ValueError(\n-&gt; 2554                     &quot;The model did not return a loss from the inputs, only the following keys: &quot;\n   2555                     f&quot;{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.&quot;\n   2556                 )\n\nValueError: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.\n</code></pre>\n<p>I was expecting to for it to the training details (f1, loss, accuracy etc). My assumption is that my encoded data with the hidden states is not properly structured for the model to train per the arguments set.</p>\n<p><strong>UPDATED MODEL CODE:\nhere's where I'm loading and splitting</strong></p>\n<pre><code>category_data = load_dataset(&quot;csv&quot;, data_files=&quot;testdatafinal.csv&quot;)\ncategory_data = category_data.remove_columns([&quot;someid&quot;, &quot;someid&quot;, &quot;somedimension&quot;])\ncategory_data = category_data['train']\ntrain_testvalid = category_data.train_test_split(test_size=0.3)\ntest_valid = train_testvalid['test'].train_test_split(test_size=0.5)\nfrom datasets.dataset_dict import DatasetDict\ncd = DatasetDict({\n    'train': train_testvalid['train'],\n    'test': test_valid['test'],\n    'valid': test_valid['train']})\nprint(cd)\n\nDatasetDict({\n    train: Dataset({\n        features: ['Transcript', 'Primary Label'],\n        num_rows: 646\n    })\n    test: Dataset({\n        features: ['Transcript', 'Primary Label'],\n        num_rows: 139\n    })\n    valid: Dataset({\n        features: ['Transcript', 'Primary Label'],\n        num_rows: 139\n    })\n})\n</code></pre>\n<p><strong>Here's where I'm grabbing the model checkpoint</strong></p>\n<pre><code>model_checkpoint = 'distilbert-base-uncased'\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = AutoModel.from_pretrained(model_checkpoint).to(device)\n</code></pre>\n<p><strong>Here's where I'm mapping the encoded text</strong></p>\n<pre><code>transcripts_encoded_one = transcripts_encoded.set_format(&quot;torch&quot;,\n                              columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;Primary Label&quot;])\n</code></pre>\n<p><strong>Here's where i'm extracting hidden states and then mapping as well</strong></p>\n<pre><code>def extract_hidden_states(batch):\n    #Place model inputs on the GPU/CPU\n    inputs = {k:v.to(device) for k, v in batch.items()\n              if k in tokenizer.model_input_names}\n    #Extract last hidden states\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n    # Return vecot for [CLS] Token\n    return {&quot;hidden_state&quot;: last_hidden_state[:,0].cpu().numpy()}\n\ntranscripts_hidden = transcripts_encoded.map(extract_hidden_states, batched=True)\n</code></pre>\n<p><strong>Calling AutoModel</strong></p>\n<pre><code>from transformers import AutoModelForSequenceClassification\n\nnum_labels = 10\nmodel =(AutoModelForSequenceClassification\n       .from_pretrained(model_checkpoint, num_labels=num_labels)\n       .to(device))\n</code></pre>\n<p><strong>Accuracy Metrics</strong></p>\n<pre><code>from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    pred = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, pred, average=&quot;weighted&quot;)\n    acc = accuracy_score(labels, preds)\n    return {&quot;accuracy&quot;: acc, &quot;f1&quot;: f1}\n</code></pre>\n<p><strong>Trainer</strong></p>\n<pre><code>from transformers import Trainer, TrainingArguments\n\nbatch_size = 10\nlogging_steps = len(transcripts_encoded_one[&quot;train&quot;]) // batch_size\nmodel_name = f&quot;{model_checkpoint}-finetuned-transcripts&quot;\ntraining_args = TrainingArguments(output_dir=model_name,\n                                 num_train_epochs=2,\n                                 learning_rate=2e-5,\n                                 per_device_train_batch_size=batch_size,\n                                 per_device_eval_batch_size=batch_size,\n                                 weight_decay=0.01,\n                                 evaluation_strategy=&quot;epoch&quot;,\n                                 disable_tqdm=False,\n                                 logging_steps=logging_steps,\n                                 push_to_hub=False,\n                                 log_level=&quot;error&quot;)\n\nfrom transformers import Trainer\n\ntrainer = Trainer(model=model, args=training_args,\n                 compute_metrics=compute_metrics,\n                 train_dataset=transcripts_encoded_one[&quot;train&quot;],\n                 eval_dataset=transcripts_encoded_one[&quot;valid&quot;],\n                 tokenizer=tokenizer)\n\ntrainer.train();\n</code></pre>\n<p><strong>I've tried passing &quot;transcripts_encoded(without hidden states) and &quot;transcripts_hidden (with hidden states) as the train and validation splits and both produce the same error</strong></p>\n<pre><code>trainer.train_dataset[0]\n\n{'Primary Label': 'cancel',\n 'input_ids': tensor([  101,  2047,  3446,  2003,  2205,  6450,  2005,  1996,  2051,  1045,\n          2064,  5247,  3752,  4790,  1012,  2009,  2001,  2026,  5165,  2000,\n          6509,  2017,  2651,   999,  4067,  2017,  2005,  3967,  2075,  1996,\n          2047,  2259,  2335,   999,  2031,  1037,  6919,  2717,  1997,  1996,\n          2154,   999,  2994,  3647,  1998,  7965,   999,  2065,  2045,  2003,\n          2505,  2842,  2057,  2089,  2022,  2583,  2000,  6509,  2017,  2007,\n          3531,  2514,  2489,  2000,  3967,  2149,  2153,  1012,  1045,  2001,\n          2074,  2667,  2000, 17542,  2026, 15002,  1012,  2038,  2009,  2042,\n         13261,  1029,  7632,  1010,  2045,   999,  1045,  3246,  2017,  1005,\n          2128,  2725,  2092,  2651,  1012,  4067,  2017,  2005,  3967,  2075,\n           102]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1])}\n</code></pre>\n",
    "score": 4,
    "creation_date": 1670357929,
    "view_count": 5004,
    "answer_count": 2,
    "tags": "nlp;huggingface-transformers;huggingface"
  },
  {
    "question_id": 73210957,
    "title": "Improving accuracy of NER on Spacy for a tag that is not following one format",
    "body": "<p>I am using Spacy model for NER on my datasets. It is showing poor tagging on B-Address and I-ADDRESS. The reason is because I have different type of address in my document. Some start with number, some start with name of building, some start with po box. Any idea how I can increase my accuracy on address tag?</p>\n",
    "score": 4,
    "creation_date": 1659458465,
    "view_count": 2826,
    "answer_count": 1,
    "tags": "nlp;tags;spacy;named-entity-recognition"
  },
  {
    "question_id": 71864019,
    "title": "Python programming finding similar names from a list of names",
    "body": "<p>I am using a dataset of company names with that may contains not identical duplicates.</p>\n<p>The list may contains : company A but also c.o.m.p.a.n.y A or comp A</p>\n<p>Is there any python script using NLP for example that can find similar names from a dataset.</p>\n<p>Thanks in advance</p>\n",
    "score": 4,
    "creation_date": 1649885360,
    "view_count": 891,
    "answer_count": 1,
    "tags": "python;database;nlp;data-science"
  },
  {
    "question_id": 70990722,
    "title": "Which model/technique to use for specific sentence extraction?",
    "body": "<p>I have a dataset of tens of thousands of dialogues / conversations between a customer and customer support. These dialogues, which could be forum posts, or long-winded email conversations, have been hand-annotated to highlight the sentence containing the customers problem. For example:</p>\n<blockquote>\n<p>Dear agent, I am writing to you because I have a very annoying problem with my washing machine. I bought it three weeks ago and was very happy with it. However, this morning the door does not lock properly. Please help</p>\n</blockquote>\n<blockquote>\n<p>Dear customer.... etc</p>\n</blockquote>\n<p>The highlighted sentence would be:</p>\n<blockquote>\n<p>However, this morning the door does not lock properly.</p>\n</blockquote>\n<ol>\n<li>What approaches can I take to model this, so that in future I can automatically extract the customers problem? The domain of the datasets are broad, but within the hardware space, so it could be appliances, gadgets, machinery etc.</li>\n<li>What is this type of problem called?\nI thought this might be called &quot;intent recognition&quot;, but most guides seem to refer to multiclass classification. The sentence either is or isn't the customers problem. I considered analysing each sentence and performing binary classification, but I'd like to explore options that take into account the context of the rest of the conversation if possible.</li>\n<li>What resources are available to research how to implement this in Python (using tensorflow or pytorch)</li>\n</ol>\n<p>I found a <a href=\"https://huggingface.co/TODBERT\" rel=\"nofollow noreferrer\">model on HuggingFace</a> which has been pre-trained with customer dialogues, and have read the research paper, so I was considering fine-tuning this as a starting point, but I only have experience with text (multiclass/multilabel) classification when it comes to transformers.</p>\n",
    "score": 4,
    "creation_date": 1643996929,
    "view_count": 965,
    "answer_count": 2,
    "tags": "python;tensorflow;nlp;pytorch;huggingface-transformers"
  },
  {
    "question_id": 68718901,
    "title": "What does nltk.download(&quot;wordnet&quot;) accomplish",
    "body": "<p>I wanted to know what <code>nltk.download()</code> do. Also, if I add &quot;wordnet&quot; as an argument, then what happens. Is <code>wordnet</code> like some dataset or something, I would like more clarification on that.</p>\n",
    "score": 4,
    "creation_date": 1628545524,
    "view_count": 3453,
    "answer_count": 1,
    "tags": "deep-learning;nlp;nltk"
  }
]