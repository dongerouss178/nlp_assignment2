{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoang\\anaconda3\\envs\\GPU_TF\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import Lib\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import swifter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\n",
    "\n",
    "\n",
    "color_pal = sns.color_palette()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection:\n",
    "- We use StackExchange Query function to get data\n",
    "```sql\n",
    "SELECT TOP 25000 \n",
    "  q.Id AS QuestionId, \n",
    "  q.Title,\n",
    "  q.Body,\n",
    "  q.CreationDate,\n",
    "  q.Score,\n",
    "  q.ViewCount,\n",
    "  q.AnswerCount,\n",
    "  a.Id AS AcceptedAnswerId,\n",
    "  a.Body AS AcceptedAnswerBody,\n",
    "  a.Score AS AcceptedAnswerScore\n",
    "FROM Posts q\n",
    "LEFT JOIN Posts a ON q.AcceptedAnswerId = a.Id\n",
    "JOIN PostTags pt ON q.Id = pt.PostId\n",
    "JOIN Tags t ON pt.TagId = t.Id\n",
    "WHERE t.TagName = 'nlp'\n",
    "ORDER BY q.CreationDate DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "AcceptedAnswerScore",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c4f5094c-76cb-415b-b659-0bafb23ff95e",
       "rows": [
        [
         "0",
         "79557354",
         "Sentencepiece not generating models after preprocessing",
         "<p>So this is the log that I see on the terminal:</p>\n<pre><code>sentencepiece_trainer.cc(78) LOG(INFO) Starts training with :  \ntrainer_spec {  \n  input: C:\\Users\\xxxx\\OneDrive\\Documents\\Projects\\py\\xxxxx\\data\\tokenizer\\final_text_corpus.txt  \n  input_format:  \n  model_prefix: C:\\Users\\xxxx\\OneDrive\\Documents\\Projects\\py\\xxxxxxxx\\tokenizer\\multilingual_unigram  \n  model_type: UNIGRAM  \n  vocab_size: 50000  \n  self_test_sample_size: 0  \n  character_coverage: 1  \n  input_sentence_size: 10000000  \n  shuffle_input_sentence: 1  \n  seed_sentencepiece_size: 1000000  \n  shrinking_factor: 0.75  \n  max_sentence_length: 16384  \n  num_threads: 16  \n  num_sub_iterations: 2  \n  max_sentencepiece_length: 16  \n  split_by_unicode_script: 1  \n  split_by_number: 1  \n  split_by_whitespace: 1  \n  split_digits: 0  \n  pretokenization_delimiter:  \n  treat_whitespace_as_suffix: 0  \n  allow_whitespace_only_pieces: 0  \n  user_defined_symbols: &lt;newline&gt;  \n  required_chars:  \n  byte_fallback: 0  \n  vocabulary_output_piece_score: 1  \n  train_extremely_large_corpus: 1  \n  seed_sentencepieces_file:  \n  hard_vocab_limit: 1  \n  use_all_vocab: 0  \n  unk_id: 1  \n  bos_id: 2  \n  eos_id: 3  \n  pad_id: 0  \n  unk_piece: &lt;unk&gt;  \n  bos_piece: &lt;s&gt;  \n  eos_piece: &lt;/s&gt;  \n  pad_piece: &lt;pad&gt;  \n  unk_surface:  Γüç  \n  enable_differential_privacy: 0  \n  differential_privacy_noise_level: 0  \n  differential_privacy_clipping_threshold: 0  \n}  \nnormalizer_spec {  \n  name: nmt_nfkc  \n  add_dummy_prefix: 1  \n  remove_extra_whitespaces: 1  \n  escape_whitespaces: 1  \n  normalization_rule_tsv:  \n}  \ndenormalizer_spec {}  \ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.  \ntrainer_interface.cc(185) LOG(INFO) Loading corpus: C:\\Users\\xxxxxxx\\OneDrive\\Documents\\Projects\\py\\xxxxxxx\\data\\tokenizer\\final_text_corpus.txt  \ntrainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines  \ntrainer_interface.cc(147) LOG(INFO) Loaded 2000000 lines  \ntrainer_interface.cc(147) LOG(INFO) Loaded 3000000 lines  \ntrainer_interface.cc(147) LOG(INFO) Loaded 4000000 lines  \ntrainer_interface.cc(147) LOG(INFO) Loaded 5000000 lines  \ntrainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (5816781), which may slow down training.  \ntrainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=&lt;size&gt; and --shuffle_input_sentence=true.  \ntrainer_interface.cc(129) LOG(WARNING) They allow to randomly sample &lt;size&gt; sentences from the entire corpus.  \ntrainer_interface.cc(409) LOG(INFO) Loaded all 5816781 sentences  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;pad&gt;  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;unk&gt;  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;s&gt;  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;/s&gt;  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;newline&gt;  \ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...  \ntrainer_interface.cc(539) LOG(INFO) all chars count=731130164  \ntrainer_interface.cc(550) LOG(INFO) Done: 100% characters are covered.  \ntrainer_interface.cc(560) LOG(INFO) Alphabet size=1280  \ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1  \ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 5816741 sentences.\n</code></pre>\n<p>After this the terminal closes.</p>\n<p>This is the part of the code that trains:</p>\n<pre><code>import sys\nimport os\nfrom pathlib import Path\nimport sentencepiece as spm\n\n# === Paths ===\nroot_dir = Path(__file__).resolve().parent.parent  \ninput_path = root_dir / &quot;data&quot; / &quot;tokenizer&quot; / &quot;final_text_corpus.txt&quot;\noutput_dir = root_dir / &quot;tokenizer&quot;\noutput_dir.mkdir(parents=True, exist_ok=True)\nmodel_prefix = &quot;spm_tokenizer&quot;\nlog_path = output_dir / &quot;training.log&quot;\n\n# === Logging setup ===\nwith open(log_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as log_file:\n    sys.stdout = log_file\n    sys.stderr = log_file\n\n    print(&quot;Starting tokenizer training...&quot;)\n    print(f&quot;Input corpus: {input_path}&quot;)\n    print(f&quot;Output prefix: {model_prefix}&quot;)\n    print(f&quot;Vocab size: {50000}&quot;)\n\n    # === Train Tokenizer ===\n    spm.SentencePieceTrainer.train(\n        input=str(input_path),\n        model_prefix=model_prefix,\n        vocab_size=50000,\n        model_type=&quot;unigram&quot;,\n        character_coverage=1.0, \n        pad_id=0,\n        unk_id=1,\n        bos_id=2,\n        eos_id=3,\n        user_defined_symbols=[&quot;&lt;newline&gt;&quot;],\n        train_extremely_large_corpus=True,\n        input_sentence_size=10_000_000,\n        shuffle_input_sentence=True,\n        max_sentence_length=16384\n    )\n\n    print(f&quot;Tokenizer trained! Model saved to: {model_prefix}.model / .vocab&quot;)\n\n</code></pre>\n<p>I have added the <code>train_extremely_large_corpus</code> flag to True and the <code>input_sentence_size</code> and <code>max_sentence_length</code> flags in case it's because of memory bottlenecks, but still no help.I tried finding this issue all over the internet, still no luck.\nCan anyone explain what's causing the problem?</p>\n",
         "2025-04-05 18:21:09",
         "0",
         "20",
         "0",
         null,
         null,
         null
        ],
        [
         "1",
         "79557315",
         "How should I approach the word synonyms for review aspects matching",
         "<p>I have created an aspect based list for analysing various aspects for a review and the list is below:</p>\n<p>Now the issue here is that it only catches the words which are there in the list but now I want to further expand this...how should I go ahead ? I'm thinking of using spacy model and bert or a nltk approach but which one will be more reliable.</p>\n<p>I have lematized the words using def process function:</p>\n<pre><code>aspect_categories = {\n   &quot;Product Quality&quot;: [&quot;quality&quot;,&quot;shade&quot;, &quot;durability&quot;, &quot;performance&quot;,&quot;smooth&quot;,&quot;silky&quot;,&quot;moisturizer&quot;,&quot;glides&quot;,&quot;matte&quot;,&quot;color&quot;,&quot;benefits&quot;,&quot;cream&quot;,&quot;heart&quot;,&quot;jelly&quot;,&quot;cleans&quot;,&quot;dry&quot;,&quot;moist&quot;,&quot;crack&quot;,&quot;effective&quot;, &quot;results&quot;, &quot;improvement&quot;, &quot;noticeable&quot;, &quot;long-lasting&quot;, &quot;short-lived&quot;, &quot;works&quot;, &quot;doesn't work&quot;,&quot;wrinkles&quot;,&quot;hydration&quot;,&quot;shine&quot;,&quot;coverage&quot;,&quot;texture&quot;, &quot;consistency&quot;, &quot;thick&quot;,&quot;reduce&quot;,&quot;dark spots&quot;,&quot;thin&quot;, &quot;watery&quot;, &quot;oily&quot;, &quot;greasy&quot;, &quot;sticky&quot;, &quot;absorbent&quot;, &quot;spreadability&quot;, &quot;application&quot;, &quot;blendable&quot;, &quot;cakey&quot;,&quot;patchy&quot;, &quot;flaky&quot;, &quot;lightweight&quot;, &quot;heavy&quot;,&quot;pleasent&quot;,&quot;unpleasent&quot;,&quot;allergic&quot;,&quot;glossy&quot;,&quot;smell&quot;,&quot;fragrance&quot;,],\n   &quot;Cost&quot;: [&quot;price&quot;, &quot;cost&quot;, &quot;expensive&quot;, &quot;cheap&quot;, &quot;value&quot;, &quot;affordable&quot;,&quot;worth it&quot;, &quot;reasonable&quot;],\n &quot;Shipping &amp; Delivery&quot;: [&quot;delivery&quot;, &quot;shipping&quot;, &quot;late&quot;, &quot;fast&quot;, &quot;delay&quot;,&quot;on time&quot;,&quot;timely&quot;,&quot;before&quot;,&quot;super fast&quot;],\n&quot;Packaging&quot;: [&quot;packaging&quot;, &quot;box&quot;, &quot;damaged&quot;, &quot;seal&quot;,&quot;build&quot;,&quot;small&quot;,&quot;large&quot;,&quot;big&quot;,&quot;design&quot;,&quot;bottle&quot;,&quot;tube&quot;,&quot;attractive&quot;,&quot;ugly&quot;,&quot;cute&quot;,&quot;difficult to open&quot;,],\n}\nflattened_keywords = {kw: cat for cat, kws in aspect_categories.items() for kw in kws}```\n\ndef preprocess(text):\n    doc = nlp(text.lower())\n    return [token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in stop_words]\n</code></pre>\n<p>Then after this I'm using tf-idf for identifying top 5 aspects mentioned in each and every review whose code is below:</p>\n<pre><code>def extract_tfidf_aspects(df, max_features=100):\n    vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=&quot;english&quot;, max_features=max_features)\n    tfidf_matrix = vectorizer.fit_transform(df[&quot;review_text&quot;])\n    feature_names = vectorizer.get_feature_names_out()\n\n    def get_top_aspects(row):\n        review_tokens=set(preprocess(row[&quot;review_text&quot;]))\n        row_vector = tfidf_matrix[row.name]\n        sorted_idx = row_vector.toarray().flatten().argsort()[::-1]\n        top_aspects = [feature_names[i] for i in sorted_idx[:NUM_TOP_TFIDF_ASPECTS]]\n        return top_aspects\n\n    df[&quot;tfidf_aspects&quot;] = df.apply(get_top_aspects, axis=1)\n    return df\n</code></pre>\n",
         "2025-04-05 17:32:46",
         "-1",
         "92",
         "1",
         null,
         null,
         null
        ],
        [
         "2",
         "79557313",
         "No attention output in jinaai/jina-embeddings-v3 embedding model",
         "<p>When I use this model like so -</p>\n<pre><code>from transformers import AutoModel, AutoTokenizer\n\nmodel_id = &quot;jinaai/jina-embeddings-v3&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n\ninputs = tokenizer([\n    &quot;The weather is lovely today.&quot;,\n    &quot;It's so sunny outside!&quot;,\n    &quot;He drove to the stadium.&quot;\n], return_tensors=&quot;pt&quot;, padding=True, truncation=True)\n\noutputs = model(**inputs, output_attentions=True)\n\nattentions = outputs.attentions\n</code></pre>\n<p>I get this warning which seems contradictory -</p>\n<pre><code>flash_attn is not installed. Using PyTorch native attention implementation.\nFlash attention implementation does not support kwargs: output_attentions\n</code></pre>\n<p>attentions is None</p>\n<p>I tried it with other models and it works as expected.</p>\n",
         "2025-04-05 17:29:15",
         "0",
         "15",
         "0",
         null,
         null,
         null
        ],
        [
         "3",
         "79549787",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?",
         "<p>I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER):</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;pl_core_news_lg&quot;)\ntext = &quot;Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.&quot;\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n</code></pre>\n<p>Output:</p>\n<pre><code>[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n</code></pre>\n<p>However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities.</p>\n<pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=&quot;path_to_my_file/nlp_config.yaml&quot;) \nnlp_engine = provider.create_engine()\n\nprint(f&quot;Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}&quot;)\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[&quot;pl&quot;])\nregistry.load_predefined_recognizers([&quot;pl&quot;])\n\nprint(f&quot;Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}&quot;)\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, &quot;pl&quot;)\n\nfor entity in results:\n    print(f&quot;Found entity: {entity.entity_type} with score {entity.score}&quot;)\n</code></pre>\n<p>Output:</p>\n<pre><code>Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n</code></pre>\n<p>Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text.</p>\n<p>My config file:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n<p>Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?</p>\n",
         "2025-04-02 05:56:11",
         "0",
         "68",
         "1",
         "79552218.0",
         "<p>The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine :</p>\n<pre><code>  labels_to_ignore:\n    - O\n</code></pre>\n<p>On your configuration it would look like this:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n",
         "1.0"
        ],
        [
         "4",
         "79548202",
         "GPT-2 and other models from huggingface -100 label index for training, instead of pad token",
         "<p>I understand the -100 label id is used so that the predictions for these are not included when calculating the loss.</p>\n<p>However on <a href=\"https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16#bert2gpt2-summarization-with-%F0%9F%A4%97-encoderdecoder-framework\" rel=\"nofollow noreferrer\">huggingface</a>, they state\n&quot;complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not&quot;, when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument &quot;ignore_index&quot;.</p>\n<p>Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same?</p>\n<p>The way it is written makes me think there is some benefit, but the description of &quot;ignore_index&quot; appears to achieve what is wanted.</p>\n",
         "2025-04-01 09:21:17",
         "0",
         "46",
         "1",
         "79551169.0",
         "<p>The author of the tutorial you mentioned sets it to <code>-100</code> <strong>and</strong> uses <code>ignore_index</code> to save a few lines of code. You don't see the line where the author pass something to <code>ignore_index</code> because it has a default value. The default value of <code>ignore_index</code> for <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\" rel=\"nofollow noreferrer\">nn.CrossEntropyLoss</a> is <code>-100</code>. Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.</p>\n",
         "1.0"
        ],
        [
         "5",
         "79546188",
         "simpler gmail Filter syntax for \"word family\" [verif +(y/ied/ification] + similar loanwords [term +(s/es/a)]?",
         "<p>Is there simpler filter that I can use for below cases?\nGoogle has a very smart AI gemini, I hope there is a shortcut for this as I am receiving bilingual emails and loan words in Malay/Indonesia are quite similar to English.</p>\n<p>a. variations of the same word?</p>\n<pre><code>subject:{+updates +updated +updating +update}\nsubject:{+verify +verification +verified}\n</code></pre>\n<p>b. similar transliteration of words</p>\n<pre><code>subject:{+terms +termes +terma}\nsubject:{+invois +invoice}\nsubject:{+privacy +privasi +privacidad}\n</code></pre>\n<p>c. words with different character(s)/prefix/suffix</p>\n<pre><code>subject:{+e-statement +estatement +statement}\nsubject:{+&quot;log in&quot; +login +&quot;log-ins&quot; &quot;logged in&quot;}\n</code></pre>\n",
         "2025-03-31 12:10:55",
         "0",
         "25",
         "0",
         null,
         null,
         null
        ],
        [
         "6",
         "79543496",
         "Evaluating Spell Correction Models",
         "<p>How are spell correction models evaluated? For evaluation, I have 3 strings.</p>\n<ul>\n<li>Original</li>\n<li>Predicted</li>\n<li>Expected</li>\n</ul>\n<p>Right now I'm counting the TP, TN, FP, FN values on a character level separately for detection and correction as mentioned below.</p>\n<ul>\n<li><p>Detection</p>\n<ul>\n<li><strong>TP</strong> - The original character is erroneous. The model detected the error.</li>\n<li><strong>TN</strong> - The original character is correct. The model did not detect an error.</li>\n<li><strong>FP</strong> - The original character is correct. The model detected an error.</li>\n<li><strong>FN</strong> - The original character is erroneous. The model did not detect the error.</li>\n</ul>\n</li>\n<li><p>Correction</p>\n<ul>\n<li><strong>TP</strong> - The original character is erroneous. The model changed the character correctly.</li>\n<li><strong>TN</strong> - The original character is correct. The model did not change the character.</li>\n<li><strong>FP</strong> - The original character is correct. The model changed the character. <strong>Or</strong> The original character is erroneous. The model changed the character incorrectly.</li>\n<li><strong>FN</strong> - The original character is erroneous. The model did not change the character.</li>\n</ul>\n</li>\n</ul>\n<p>And I'm using these values to calculate <strong>precision, recall and F measures</strong>. Is this method correct? Because I couldn't find any document that explicitly mentioned how spell correction models are evaluated.</p>\n",
         "2025-03-29 15:50:20",
         "-1",
         "23",
         "0",
         null,
         null,
         null
        ],
        [
         "7",
         "79533402",
         "Creating regular expression(s) which finds capitalization errors",
         "<blockquote>\n<p>This is a Sentence which contains<br/>\nSome capitalization errors.</p>\n</blockquote>\n<p>So far I have this: <code>(?&lt;![.!?]\\s)(?&lt;!^)(?&lt;!\\sI\\s)(?!I['’][a-z])(?!\\b(?:Dr|Mr|Mrs)\\.[\\s\\r\\n])\\b(?!I\\b)[A-Z]\\w*</code></p>\n<p>It will find &quot;Sentence&quot; in the above. It avoids hitting on I and I' contractions, and Dr. / Mr. / Mrs.</p>\n<p>What I can't get it to do is find &quot;Some&quot; in the above.</p>\n<p>I feel maybe a second expression might be better for document scanning, as the first expression is quite long and probably not optimized.</p>\n<p>I need the expression to be PCRE compliant that avoids non fixed width errors and such.</p>\n<p>Just can't solve this on my own unfortunately. As expected AI is no help here... the best models struggle with regular expressions unless they are more simple.</p>\n<p>Tried many different RegEx's to match the word &quot;Some&quot; in the above. It should NOT match a preceding line that ends with a period, question mark, or exclamation point. It should also NOT match on I and I' contractions, or on Dr. / Mr. / Mrs.</p>\n",
         "2025-03-25 10:47:13",
         "0",
         "56",
         "0",
         null,
         null,
         null
        ],
        [
         "8",
         "79527785",
         "SFTTrainer Error : prepare_model_for_kbit_training() got an unexpected keyword argument 'gradient_checkpointing_kwargs'",
         "<p>I'm trying to fine-tune a model using SFTTrainer from trl.</p>\n<p>This is how my SFTConfig arguments look like,</p>\n<pre><code>from trl import SFTConfig\ntraining_arguments = SFTConfig(\n       output_dir=output_dir,\n       num_train_epochs=num_train_epochs,\n       per_device_train_batch_size=per_device_train_batch_size,\n       gradient_accumulation_steps=gradient_accumulation_steps,\n       optim=optim,\n       save_steps=save_steps,\n       logging_steps=logging_steps,\n       learning_rate=learning_rate,\n       weight_decay=weight_decay,\n       fp16=fp16,\n       bf16=bf16,\n       max_grad_norm=max_grad_norm,\n       max_steps=max_steps,\n       warmup_ratio=warmup_ratio,\n       group_by_length=group_by_length,\n       lr_scheduler_type=lr_scheduler_type,\n       report_to=&quot;tensorboard&quot;,\n       dataset_text_field=&quot;instruction&quot;,\n       max_seq_length=None,\n       packing=False,\n       gradient_checkpointing=False,\n   )\n\n</code></pre>\n<p>and this is my SFTTrainer block.</p>\n<pre><code>trainer = SFTTrainer(\n   model=model,\n   train_dataset=dataset,\n   peft_config=peft_config,\n   tokenizer=tokenizer,\n   args=training_arguments,\n)\n</code></pre>\n<p>The error comes from internal function <code>SFTTrainer._prepare_model_for_kbit_training</code>.</p>\n<pre><code> &quot;&quot;&quot;Prepares a quantized model for kbit training.&quot;&quot;&quot;\n    330 prepare_model_kwargs = {\n    331     &quot;use_gradient_checkpointing&quot;: args.gradient_checkpointing,\n    332     &quot;gradient_checkpointing_kwargs&quot;: args.gradient_checkpointing_kwargs or {},\n    333 }\n\n</code></pre>\n<p>I tried passing <code>gradient_checkpointing</code> as False and <code>gradient_checkpointing_kwargs</code> as an empty dictionary, but no luck.</p>\n<p>How can I avoid this error?</p>\n",
         "2025-03-22 16:58:47",
         "0",
         "41",
         "1",
         null,
         null,
         null
        ],
        [
         "9",
         "79527581",
         "AllenNLP all models about ccg_supertagger are unavailable. How to fix or download it?",
         "<p>I am trying to use AllenNLP models to parse a file to create a CCG dataset, because as a student I can't afford the CCGBank dataset, However I have to, cuz I need a dataset to help me to train a model to resolve syntactic ambiguities, parsing the sentence to ccg format is an inevitable step. I really need the model like\npredictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/ccg_supertagger-2020.02.10.tar.gz&quot;)\nor if you have better option , I am willing to have a try!\nIt's my code below</p>\n<pre><code>import pandas as pd\nfrom allennlp.predictors.predictor import Predictor\nimport allennlp_models.tagging\n\n# 读取原始 CSV 文件\ninput_path = &quot;validation.csv&quot;  # 替换为你的本地路径\ndf = pd.read_csv(input_path)\nsentences = df[&quot;sentence&quot;].tolist()\n\n# 加载 AllenNLP 的预训练 CCG Supertagger 模型\npredictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/ccg_supertagger-2020.02.10.tar.gz&quot;)\n\n# 定义预测函数：输入句子，输出 “词/范畴” 序列\ndef get_ccg_tags(sentence):\n    output = predictor.predict(sentence=sentence)\n    tokens = output[&quot;words&quot;]\n    tags = output[&quot;ccg_tags&quot;]\n    tagged = [f&quot;{w}/{t}&quot; for w, t in zip(tokens, tags)]\n    return &quot; &quot;.join(tagged)\n\n# 批量处理每个句子，添加 ccg_tags 列\ndf[&quot;ccg_tags&quot;] = df[&quot;sentence&quot;].apply(get_ccg_tags)\n\n# 保存结果到新文件\noutput_path = &quot;validation_with_allennlp_ccg.csv&quot;\ndf.to_csv(output_path, index=False)\n\nprint(f&quot; AllenNLP CCG ：{output_path}&quot;)\n</code></pre>\n",
         "2025-03-22 14:20:18",
         "0",
         "21",
         "0",
         null,
         null,
         null
        ],
        [
         "10",
         "79526774",
         "Unable to get the tokenizer of Gemma-3",
         "<p>I am trying to get the tokenizer using huggingface AutoTokenizer library, but I am unable to fetch, is there any other way to get it?\nWhere I am doing wrong?</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoTokenizer\n# Load Gemma 3‑27B‑IT’s tokenizer\nMODEL_GEMMA = &quot;google/gemma-3-27b-it&quot;\ngemma_tokenizer = AutoTokenizer.from_pretrained(MODEL_GEMMA, trust_remote_code=True)\n</code></pre>\n<p>I am getting the following error on running it</p>\n<pre><code>ValueError: Tokenizer class GemmaTokenizer does not exist or is not \ncurrently imported.\n</code></pre>\n",
         "2025-03-22 00:20:17",
         "1",
         "149",
         "2",
         null,
         null,
         null
        ],
        [
         "11",
         "79523696",
         "how to modify a step or a prompt of an existing langchain chain (customize SelfQueryRetriever)?",
         "<p>I need to customize a SelfQueryRetriever(the reason is: the generated target queries in OpenSearch are being generated incorrrectly so we need to tune prompts + we need to add some custom behavior such as multi-tenancy) but we don't want to re-write the whole chain, just the parts what we need to customize. How can we customize specific steps of a chain, is there  a way to modify it by position, let's say something like this (pseudo-code):</p>\n<pre><code>retriever = SelfQueryRetriever(**config)\nretriever[2] = create_custom_module1()\nretriever[4] = create_custom_module2()\n</code></pre>\n<p>In this example we preserve the majority of the chain but  customize only the third and fifth elements.</p>\n<p>Is it possible to do?</p>\n",
         "2025-03-20 17:26:35",
         "0",
         "26",
         "0",
         null,
         null,
         null
        ],
        [
         "12",
         "79523269",
         "Trouble getting importing gensim to work in colab",
         "<p>I am trying to import gensim into colab.</p>\n<pre><code>!pip install gensim\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--&gt; 367             msg = (&quot;The current Numpy installation ({!r}) fails to &quot;\n    368                    &quot;pass simple sanity checks. This can be caused for example &quot;\n    369                    &quot;by incorrect BLAS library being linked in, or by mixing &quot;\n\nModuleNotFoundError: No module named 'numpy.char'\n</code></pre>\n<p>my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks</p>\n",
         "2025-03-20 14:36:02",
         "0",
         "125",
         "1",
         "79523777.0",
         "<p>You have to restart the session for the underlying runtime to notice the package changes. See: <a href=\"https://stackoverflow.com/a/79518359/130288\">https://stackoverflow.com/a/79518359/130288</a></p>\n<p>I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded <code>numpy</code>/etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade.</p>\n<p>But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two.</p>\n<p>Explicitly restarting after the Gensim-install &amp; <code>numpy</code>/<code>scipy</code> downgrades resolves the errors.</p>\n",
         "1.0"
        ],
        [
         "13",
         "79523261",
         "How to Identify Similar Code Parts Using CodeBERT Embeddings?",
         "<p>I'm using CodeBERT to compare how similar two pieces of code are. For example:</p>\n<pre><code># Code 1\ndef calculate_area(radius):\nreturn 3.14 * radius * radius\n</code></pre>\n<pre><code># Code 2\ndef compute_circle_area(r):\nreturn 3.14159 * r * r\n</code></pre>\n<p>CodeBERT creates &quot;embeddings,&quot; which are like detailed descriptions of the code as numbers. I then compare these numerical descriptions to see how similar the codes are. This works well for telling me how much the codes are alike.</p>\n<p>However, I can't tell which parts of the code CodeBERT thinks are similar. Because the &quot;embeddings&quot; are complex, I can't easily see what CodeBERT is focusing on. Comparing the code word-by-word doesn't work here.</p>\n<p>My question is: How can I figure out which specific parts of two code snippets CodeBERT considers similar, beyond just getting a general similarity score?</p>\n<p>I tried simple diff methods but that defeats the purpose of purely using CodeBERT.\nI want to know if it's possible using CodeBERT alone.</p>\n",
         "2025-03-20 14:30:35",
         "2",
         "67",
         "1",
         null,
         null,
         null
        ],
        [
         "14",
         "79520986",
         "Converting data into spacy format \"convert_to_spacy_format\" in Name entity recognition Model",
         "<p><a href=\"https://i.sstatic.net/TMDd85MJ.png\" rel=\"nofollow noreferrer\">Dataset structure</a>Can somebody help me with the NER model in converting the data into spacy format.\nThe dataset format is shown in the screenshot here (<a href=\"https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus\" rel=\"nofollow noreferrer\">https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus</a>)</p>\n<p>Though i build but the model is not giving any output during test.</p>\n<pre><code>#Convert data to spaCy format\ndef convert_to_spacy_format(data):\n    nlp = spacy.blank(&quot;en&quot;)  # Creating blank English language model\n    db = DocBin()  # document bin object\n    \n    for _, row in tqdm(data.iterrows(), total=len(data)):\n        sentence = row[&quot;CleanSentence&quot;]\n        pos_tags = row[&quot;POS&quot;]\n        ner_tags = row[&quot;Tag&quot;]\n        \n        # Create a doc object\n        doc = nlp.make_doc(sentence)\n        \n        # Split the sentence into words (tokens)\n        words = sentence.split()\n        \n        # Check if lengths match\n        if len(words) != len(ner_tags) or len(words) != len(pos_tags):\n            print(f&quot;Warning: Length mismatch: Words: {len(words)}, NER tags: {len(ner_tags)}, POS tags: {len(pos_tags)}&quot;)\n            continue\n            \n        ents = []\n        current_ent = None\n        current_ent_start = None\n        \n        # Process each token\n        for idx, (token, tag) in enumerate(zip(doc, ner_tags)):\n            # If it's the beginning of an entity\n            if tag.startswith(&quot;B-&quot;):\n                # If we were tracking an entity, add it to our list\n                if current_ent is not None:\n                    ents.append((current_ent_start, token.idx + len(token), current_ent))\n                \n                # Start tracking a new entity\n                current_ent = tag[2:]  # Remove &quot;B-&quot; prefix\n                current_ent_start = token.idx\n            \n            # If it's inside an entity\n            elif tag.startswith(&quot;I-&quot;):\n                # Continue tracking the current entity\n                pass\n            \n            # If it's outside any entity\n            elif tag == &quot;O&quot;:\n                # If we were tracking an entity, add it to the list\n                if current_ent is not None:\n                    ents.append((current_ent_start, token.idx, current_ent))\n                    current_ent = None\n                    current_ent_start = None\n        \n        # Add the last entity if we were tracking one\n        if current_ent is not None:\n            ents.append((current_ent_start, len(sentence), current_ent))\n        \n        # Create spans for each entity\n        spans = []\n        for start, end, label in ents:\n            span = doc.char_span(start, end, label=label)\n            if span is not None:\n                spans.append(span)\n        \n        # Filter overlapping spans\n        filtered_spans = filter_spans(spans)\n        \n        # Add entities to the doc\n        doc.ents = filtered_spans\n        \n        # Add the doc to the DocBin\n        db.add(doc)\n    \n    return db\n\n</code></pre>\n<p>I tried to build an NER model but didn't got the expected output. I need help in the function</p>\n<pre><code>convert_to_spacy_format(data)\n</code></pre>\n",
         "2025-03-19 17:43:53",
         "0",
         "31",
         "0",
         null,
         null,
         null
        ],
        [
         "15",
         "79515458",
         "Gensim on Google Colab : ModuleNotFoundError: No module named 'numpy.strings'",
         "<p>I've had some problems using GENSIM.</p>\n<p>After running:</p>\n<pre><code>pip install --upgrade gensim\n</code></pre>\n<p>i execute:</p>\n<pre><code>import gensim.downloader as api\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-7-bfa495c1e338&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 import gensim.downloader as api\n\n9 frames\n/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    374     _sanity_check()\n    375     del _sanity_check\n--&gt; 376 \n    377     def _mac_os_check():\n    378         &quot;&quot;&quot;\n\nModuleNotFoundError: No module named 'numpy.strings'\n\n---------------------------------------------------------------------------\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n&quot;Open Examples&quot; button below.\n---------------------------------------------------------------------------\n</code></pre>\n<p>I understand that the problem is in the versions of some packages that gensim uses.</p>\n<pre><code>Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\nRequirement already satisfied: scipy&lt;1.14.0,&gt;=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\nRequirement already satisfied: smart-open&gt;=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim) (1.17.2)\n</code></pre>\n<p>How to resolve this issue?</p>\n",
         "2025-03-17 18:34:26",
         "2",
         "971",
         "1",
         null,
         null,
         null
        ],
        [
         "16",
         "79513881",
         "How to Fine-Tune Projection Layer in CLIP Model Using LoRA?",
         "<p>I'm trying to fine-tune the projection layers in the CLIP model using LoRA.</p>\n<p>I need help identifying the exact projection layers to modify for my fine-tuning and how I can apply LoRA to them.</p>\n<p>Model loading:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import clip\n\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\nmodel, preprocess = clip.load(&quot;ViT-B/32&quot;, device=device)\n</code></pre>\n<p>Model structure when printed</p>\n<pre><code>CLIP(\n  (visual): VisionTransformer()\n  (transformer): Transformer()\n  (token_embedding): Embedding(49408, 512)\n  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n)\n</code></pre>\n<p>I need help identifying the exact projection layers to modify for my fine-tuning and how I can apply LoRA to them.</p>\n",
         "2025-03-17 07:37:51",
         "1",
         "86",
         "1",
         null,
         null,
         null
        ],
        [
         "17",
         "79510017",
         "How can I deploy and run a Flask web application using heavy NLP libraries (pandas, numpy, sklearn) on a SiteGround shared hosting plan?",
         "<p>I have a Flask-based web application that performs NLP tasks using libraries like pandas, numpy, sklearn, and nltk. I've tried deploying it to my current hosting (SiteGround shared hosting plan), but encountered multiple issues, such as:</p>\n<p>Installation issues (pyahocorasick and other dependency errors).\nResource limitations (KeyboardInterrupt when importing heavy libraries).\nDifficulty running continuously in the background.\nMy current setup:\nHosting provider: SiteGround Shared Hosting\nPython version: 3.13.2\nFlask app with dependencies: pandas, numpy, sklearn, nltk, contractions, etc.\nSSH access available, but no root access.\nTried using virtual environment (venv), encountering build issues.\nMy questions are:</p>\n<ol>\n<li>Is it possible to run resource-intensive NLP applications like this on SiteGround’s shared hosting plan at all?</li>\n<li>If yes, how? What steps or configurations are required to overcome these errors?</li>\n<li>If no, what are the simplest and most cost-effective alternatives to deploy such a Flask NLP application smoothly (PythonAnywhere, Render.com, Heroku, AWS, DigitalOcean, or others)?</li>\n</ol>\n<p>Thanks in advance for any guidance or advice!</p>\n",
         "2025-03-14 19:03:16",
         "0",
         "37",
         "0",
         null,
         null,
         null
        ],
        [
         "18",
         "79507530",
         "How to normalize ingredient names in a recipe dataset and handle NOUN + NOUN cases using spaCy in python?",
         "<p>I'm working on normalizing ingredient names from a recipe dataset using Python and spaCy. My goal is to extract only the relevant ingredients and ignore measurement units, fractions, and other unnecessary details. For instance, if I have a string like: &quot;5 tablespoons butter, divided&quot;. I want to extract &quot;butter&quot; as the normalized ingredient. However, i struggle to parse the string &quot;8 cups broccoli florets&quot; - it loses &quot;broccoli&quot; and outputs only &quot;floret&quot;. Other strings, like &quot;3 cups chicken broth&quot;, work fine and output &quot;chicken broth&quot;.</p>\n<p><strong>My final version of the function is:</strong></p>\n<pre><code>`def normalize_ingredient(ingredient_text):\n    doc = nlp(ingredient_text)\n\n    # Set of measurement units to exclude\n    measurement_units = {\n        &quot;cup&quot;, &quot;teaspoon&quot;, &quot;tablespoon&quot;, &quot;tablespoons&quot;, &quot;gram&quot;, &quot;ounce&quot;, &quot;pound&quot;, &quot;can&quot;,\n        &quot;clove&quot;, &quot;pinch&quot;, &quot;dash&quot;, &quot;quart&quot;, &quot;liter&quot;, &quot;milliliter&quot;, &quot;gallon&quot;,\n        &quot;stick&quot;, &quot;rib&quot;, &quot;head&quot;, &quot;package&quot;, &quot;inch&quot;, &quot;piece&quot;, &quot;fluid&quot;, &quot;container&quot;,\n        &quot;jar&quot;, &quot;loaf&quot;, &quot;bottle&quot;, &quot;pack&quot;, &quot;pint&quot;, &quot;cube&quot;, &quot;stalk&quot;, &quot;slice&quot;, &quot;bulb&quot;,\n        &quot;strip&quot;, &quot;packet&quot;, &quot;envelope&quot;, &quot;box&quot;, &quot;bag&quot;, &quot;carton&quot;, &quot;sprig&quot;, &quot;leaf&quot;,\n        &quot;half&quot;, &quot;purpose&quot;, &quot;pound&quot;, &quot;ounce&quot;, &quot;gram&quot;, &quot;milliliter&quot;, &quot;liter&quot;, &quot;gallon&quot;,\n        &quot;quart&quot;, &quot;pint&quot;, &quot;dash&quot;, &quot;pinch&quot;, &quot;clove&quot;, &quot;can&quot;, &quot;package&quot;, &quot;container&quot;,\n        &quot;jar&quot;, &quot;loaf&quot;, &quot;bottle&quot;, &quot;pack&quot;, &quot;cube&quot;, &quot;stalk&quot;, &quot;bulb&quot;, &quot;strip&quot;, &quot;packet&quot;,\n        &quot;envelope&quot;, &quot;box&quot;, &quot;bag&quot;, &quot;carton&quot;, &quot;sprig&quot;, &quot;leaf&quot;, &quot;fluid&quot;, &quot;inch&quot;, &quot;piece&quot;, &quot;cup&quot;,\n        &quot;bite&quot;, &quot;size&quot;, &quot;bunch&quot;, &quot;cups&quot;,&quot;all&quot;, &quot;sized&quot;, &quot;chunks&quot;\n    }\n\n    # Set of fractions to exclude\n    fractions = {'½', '¼', '¾', '⅓', '⅔', '⅛', '⅜', '⅝', '⅞', '⅙', '⅚', '®'}\n\n    # List to store relevant terms\n    relevant_terms = []\n\n    for token in doc:\n        # Skip numbers, fractions, and measurement units\n        if (token.like_num or\n                token.text in fractions or\n                token.lemma_.lower() in measurement_units):\n            continue\n\n            # Focus on nouns, proper nouns, and adjectives that modify nouns\n        if token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;}:\n            if token.pos_ == &quot;ADJ&quot; and token.head.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                relevant_terms.append(token.lemma_.lower())\n            elif token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                relevant_terms.append(token.lemma_.lower())\n\n    return &quot; &quot;.join(relevant_terms)`\n</code></pre>\n<p><strong>This skips &quot;broccoli&quot; for unknown for me reasons (this is the one and only incorrectly parsed string for now).\nI also tried this approach with compounds:</strong></p>\n<pre><code>`if token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;}:\n            \n            if token.pos_ == &quot;ADJ&quot; and token.head.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                relevant_terms.append(f&quot;{token.lemma_.lower()} {token.head.lemma_.lower()}&quot;)\n            elif token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                compound = [t for t in token.children if t.dep_ == &quot;compound&quot;]\n                if compound:\n                    combined = &quot; &quot;.join([t.lemma_.lower() for t in compound] + [token.lemma_.lower()])\n                    relevant_terms.append(combined)\n                else:\n                    relevant_terms.append(token.lemma_.lower())`\n</code></pre>\n<p><strong>This messed up all the logic and didn't exclude some measurement units.</strong></p>\n<p><strong>Finally, i would like to avoid adding special case with broccoli like this:</strong></p>\n<pre><code>        if token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;} or token.lemma_.lower() == &quot;broccoli&quot;:\n            if token.pos_ == &quot;ADJ&quot; and token.head.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                relevant_terms.append(token.lemma_.lower())\n            elif token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;} or token.lemma_.lower() == &quot;broccoli&quot;:\n                relevant_terms.append(token.lemma_.lower())\n</code></pre>\n",
         "2025-03-13 20:13:03",
         "1",
         "35",
         "0",
         null,
         null,
         null
        ],
        [
         "19",
         "79502509",
         "Difference between TfIdf vectorizer and kwx",
         "<p>I am looking at querying a text database based on keywords and getting the relevant chunks. I want to know what is the difference between python's <a href=\"https://kwx.readthedocs.io/en/latest/index.html\" rel=\"nofollow noreferrer\">kwx package</a> and sklearn's <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"nofollow noreferrer\">tfidfvectoriser</a> in implementing this. Even though kwx extracts keywords and topics, looks like I can't perform a keyword based search using it, For which I anyway have to use sklearn's tfidf vectorizer. Can you please help me understand the difference and implement an elegant solution? TIA :)</p>\n",
         "2025-03-12 02:19:59",
         "0",
         "16",
         "0",
         null,
         null,
         null
        ],
        [
         "20",
         "79501263",
         "Calculate the gradient with respect to attention but also the FFN layers for a pre-trained LLMs",
         "<p>I would like to return the gradient with respect to specific layers and the FFN layer in the Transformer architecture of pre-trained LLMs from the hugging-face model. Is that even possible?</p>\n<p>I am working with the code of this <a href=\"https://github.com/kristosh/xAI/blob/main/attn_vizualizations.py\" rel=\"nofollow noreferrer\">repo</a> which is the following:</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/Phi-3-medium-4k-instruct&quot;)\nmodel = AutoModelForCausalLM.from_pretrained(\n    &quot;microsoft/Phi-3-medium-4k-instruct&quot;,  # note: check spelling if you get error\n    device_map=&quot;auto&quot;,\n    torch_dtype=torch.float16,            # or torch.float32 if preferred\n    trust_remote_code=True\n)\n\n# Create a pipeline\ngenerator = pipeline(\n    &quot;text-generation&quot;,\n    model = model,\n    tokenizer = tokenizer,\n    return_full_text= False,\n    max_new_tokens = 100,\n    do_sample = False\n)\n\n# Prepare a prompt\nprompt = &quot;Whats is the co-capital of Greece according to the country's public opinion?&quot;\ninputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)\ninputs = inputs.to(&quot;cuda:0&quot;)  # send inputs to cuda\n\n# Run the model with attention outputs enabled\n# Make sure to pass output_attentions=True\noutputs = model(input_ids=inputs.input_ids, output_attentions=True)\n\n# outputs.attentions is a tuple with one element per layer\n# Each element is a tensor of shape (batch_size, num_heads, seq_len, seq_len)\nattentions = outputs.attentions\n\n# Generate output\noutput = generator(prompt)\nprint(output[0][&quot;generated_text&quot;])\n</code></pre>\n<p>How to return the gradient concerning the input or a specific attention layer (in a similar fashion with <code>grad-CAM</code> in <code>CNN</code>). Is it possible to do that in <code>transformers</code>?</p>\n",
         "2025-03-11 15:21:57",
         "0",
         "40",
         "0",
         null,
         null,
         null
        ],
        [
         "21",
         "79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">site</a> in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?</p>\n",
         "2025-03-11 14:50:31",
         "0",
         "36",
         "1",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n",
         "1.0"
        ],
        [
         "22",
         "79499885",
         "Split a string into separate words without spaces, preserving special characters",
         "<p><strong>My task:</strong></p>\n<p>Is to split a string that does not have spaces into separate words. For example:</p>\n<pre><code>&quot;ProgramAgreement&quot; -&gt; &quot;Program Agreement&quot;\n\n&quot;drivername&quot; -&gt; &quot;driver name&quot;\n</code></pre>\n<p>For these purposes, I use the <em>wordninja</em> library, and in most cases I get an acceptable result.</p>\n<p><strong>Problem:</strong></p>\n<p>If the string contains special characters, <em>wordninja</em> ignores them, which is unacceptable for me. Here are some examples:</p>\n<pre><code>'addressbelow:' -&gt; 'address below'\n\n&quot;PAGES#NUMBER&quot; -&gt; &quot;PAGES NUMBER&quot;\n</code></pre>\n<p><strong>Desire result:</strong></p>\n<p>However, as you probably already guessed, I would like to preserve special characters. That is, I would like to see the following result:</p>\n<pre><code>'addressbelow:' -&gt; 'address below:'\n\n&quot;PAGES#NUMBER&quot; -&gt; &quot;PAGES# NUMBER&quot;\n</code></pre>\n<p><strong>Code snippet:</strong></p>\n<p>my code in this moment really simple</p>\n<pre><code>    import wordninja\n    separate_words = &quot; &quot;.join(wordninja.split(&quot;PAGES#NUMBER&quot;))\n</code></pre>\n",
         "2025-03-11 06:57:04",
         "-5",
         "102",
         "0",
         null,
         null,
         null
        ],
        [
         "23",
         "79498915",
         "Comparing the similarity of spoken and written form text",
         "<p>I'm converting spoken form text to its written form. For example, &quot;he owes me two-thousand dollars&quot; should be converted to &quot;he owes me $2,000&quot; . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of &quot;two-thousand dollars&quot; to &quot;$2,000&quot; to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this?</p>\n",
         "2025-03-10 18:55:59",
         "1",
         "23",
         "1",
         null,
         null,
         null
        ],
        [
         "24",
         "79488426",
         "Upserting in Pinecone takes too long",
         "<p>I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using <code>jina-embedding-v3</code>. For 204 reviews this takes around <strong>2.5 hours!</strong> in Colab. Tried using GPU but the embeddings arent using GPU.\nAm i doing something wrong? Is there a way that i can speed up the process? The code is below:</p>\n<p>Initialising DB:</p>\n<pre><code>if index_name not in pc.list_indexes().names():\n  pc.create_index(\n    name=index_name,\n    dimension=1024,\n    metric=&quot;cosine&quot;,\n    spec=ServerlessSpec(\n        cloud=&quot;aws&quot;,\n        region=&quot;us-east-1&quot;\n    )\n)\n</code></pre>\n<p>Embedding &amp; Upserting:</p>\n<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the Jina embedding model and tokenizer from Hugging Face\nmodel_name = &quot;jinaai/jina-embeddings-v3&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\nfrom langchain.text_splitter import SpacyTextSplitter\ntext_splitter = SpacyTextSplitter(chunk_size=500)\n\n# Function to generate embeddings\ndef generate_embeddings(text, task='retrieval.passage'):\n    return model.encode(text, convert_to_tensor=True, task=task).numpy()\n\nfor review_id, review in enumerate(all_reviews[:2]):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n\n# Generate and store embeddings\nfor review_id, review in enumerate(all_reviews):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n</code></pre>\n",
         "2025-03-06 06:22:35",
         "1",
         "59",
         "1",
         null,
         null,
         null
        ],
        [
         "25",
         "79485287",
         "how to pass additional query filters to a SelfQueryRetriever?",
         "<p>We are implementing a SelfQueryRetriever using OpenSearch as vectorstore, in general it works fine generating the metadata filters from the user query but we need some way to append other filters to the query, and I cannot find how to do that in the documentation, the use case is to make some things such as:</p>\n<ul>\n<li>The UI will have some filters that product owners want exposed as tradditional filters instead of filters to be extracted from user query.</li>\n<li>There are roles and the space of possible search results for a query is restricted by the data available only for users of that role.</li>\n</ul>\n<p>A possible solution is to add those to the metadata fields of the SelfQueryRetriever and append a &quot;system&quot; component to the user query and let the self query retriever create the filters, but to me it does't sound so clean and intuitive.</p>\n<p>How can additional filters be added to the query?</p>\n",
         "2025-03-05 02:03:39",
         "0",
         "41",
         "0",
         null,
         null,
         null
        ],
        [
         "26",
         "79485259",
         "Spacy rules matching entities before text",
         "<p>I'm trying to write a spacy parser to extract the names and terms of a contract.\nTo do that, I've written a rule to extract the sellers and buyers, except it's extracting multiple times over a simple sentence.</p>\n<p>Here's my rule</p>\n<pre><code>[{&quot;label&quot;: &quot;seller&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;:  &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;seller&quot;}]},\n{&quot;label&quot;: &quot;buyer&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;buyer&quot;}]},]\n</code></pre>\n<p>Which results in spans like this:</p>\n<pre><code>span seller Text: john e. smith and wife judy c. smith, seller\nspan seller Text: e. smith and wife judy c. smith, seller\nspan seller Text: smith and wife judy c. smith, seller\nspan seller Text: judy c. smith, seller\nspan seller Text: c. smith, seller\nspan seller Text: smith, seller\n</code></pre>\n<p>It seems that spacy is chunking the person entities. How can I produce a rule that matches multiple sellers (or buyers), but doesn't cut them up like this example?</p>\n<p>My code is below.</p>\n<pre><code>#!/usr/bin/env python3\n\nimport spacy\nfrom spacy.tokens import SpanGroup, DocBin, Span\nfrom spacy import displacy\nimport bodytext\nimport sys\nrules  = [{&quot;label&quot;: &quot;seller&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;seller&quot;}]},\n        {&quot;label&quot;: &quot;buyer&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;buyer&quot;}]},]\nnlp = spacy.load(&quot;en_core_web_lg&quot;)\nruler = nlp.add_pipe(&quot;span_ruler&quot;)\nruler.add_patterns(rules)\n\ntext = &quot;THIS AGREEMENT made on this 12 day of December, 2008, between John E. Smith and wife Judy C. Smith, Seller (whether one or more), whose address is: 1234 CRD 5000, midland, Texas, 79221-2016, and real estate investors, LLC, Buyer, whose address is: 4321 Harvard Ave, Midland, Texas 79701.  &quot;\ndoc = nlp(text.lower())\n\ndoc.spans[&quot;test&quot;] = SpanGroup(doc)\ndb = DocBin()\n\nfor sentence in doc.sents:\n    for span in doc.spans[&quot;ruler&quot;]:\n        print(&quot;span &quot;+ span.label_+&quot; Text: &quot;+span.text)\n        if span.start &gt;= sentence.start and span.end &lt;= sentence.end:\n            doc.spans[&quot;test&quot;] += [\n                Span(doc, start=sentence.start, end=sentence.end, label=span.label_)\n            ]\n            doc.set_ents(entities=[span], default=&quot;unmodified&quot;)\n</code></pre>\n",
         "2025-03-05 01:34:23",
         "0",
         "29",
         "0",
         null,
         null,
         null
        ],
        [
         "27",
         "79484448",
         "How does ELMo generate words for training ? Is it autoregressive?",
         "<p>I'm confused about using Bidirectional LM for words prediction and loss computing  while training.</p>\n<p>At first we have a sequence of tokens X1, ..., Xn.</p>\n<p>After gathering their context independent embeddings via CNN... we pass them to 2-layer Stacked BiLSTM.</p>\n<p>BiLSTM works as a tagger here, and for every input token X1, ...,Xn we get probabilities of the next token (for X1 it <strong>should</strong> be X2, for X2 - X3 and etc.). For Xn it will be a probability of the next word in the sentence - Y_n.</p>\n<p>Now, we can compute loss for every token.</p>\n<p>So, I don't understand what we do next. Does ELMo works like an autoregressive LM?</p>\n<p>If it does and we put the newly predicted token Y_n right after X1, ...,Xn and feed it to the BiLSTM to predict the second new word - Y_{n+1}, won't predictions for all previous tokens change ?</p>\n<p>I guess they should because of the Bidirectional nature of the model. There will be new context from the right, and the model can change everything.</p>\n<p>Can we simply compute loss for every token again, not just for a new one ?</p>\n<p>But, if we want to use this model like LM in inference, and predict next <strong>several</strong> words, newly predicted words will affect previous. What predictions should we use ?</p>\n<p>We pass X1, ..., Xn to the ElMo. We get new word Y1. We pass X1, ..., Xn, Y1 to the ELMo. We get another new word Y2 and the previous word Y1 changes to Z1. Should we take Z1, Y2 as an answer, or we freeze Y1, ignore Z1 and use Y1, Y2 as an answer?</p>\n<p>In Transformer this problem is solved by Masked Self-Attention in decoder, and newly predicted words won't affect previous.</p>\n<p>I tried looking for the answer in the original paper, but didn't find anything about the training process.</p>\n",
         "2025-03-04 17:32:14",
         "-1",
         "40",
         "1",
         null,
         null,
         null
        ],
        [
         "28",
         "79482290",
         "How to handle German language specific characters like (ä, ö, ü, ß) while tokenizing using GPT2Tokenizer?",
         "<p>I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer.</p>\n<p>To tokenize the text, I wrote the implementation as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import GPT2Tokenizer\n\ntext = &quot;zügiger Transport des ABCD stabilen Kindes in die Notaufnahme UKA&quot;\ntext = text.encode(&quot;utf-8&quot;).decode(&quot;utf-8&quot;)  # Re-encode to fix encoding issues\n\n# Load GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\n\nprint(tokens)  # Should properly tokenize &quot;zügiger&quot; instead of splitting &quot;ü&quot;\n</code></pre>\n<p>Now, when I execute this code snippet I get output as follows:</p>\n<pre><code>['z', 'Ã¼', 'g', 'iger', 'ĠTransport', 'Ġdes', 'ĠABC', 'D', 'Ġstabil', 'en', 'ĠKind', 'es', 'Ġin', 'Ġdie', 'ĠNot', 'au', 'fn', 'ah', 'me', 'ĠUK', 'A']\n</code></pre>\n<p>After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below.</p>\n<pre class=\"lang-markdown prettyprint-override\"><code>| Character | UTF-8 Bytes | Misdecoded as Latin-1 | Resulting String |\n|-----------|-------------|-----------------------|------------------|\n| ä         | C3 A4       | Ã + ¤                 | Ã¤               |\n| ö         | C3 B6       | Ã + ¶                 | Ã¶               |\n| ü         | C3 BC       | Ã + ¼                 | Ã¼               |\n| ß         | C3 9F       | Ã + Ÿ                 | ÃŸ               |\n</code></pre>\n<p>Now, how I can keep German language specific characters like (ä, ö, ü, ß) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. &quot;zügiger&quot; becomes something like ['z', 'ü', 'g', 'iger'].</p>\n",
         "2025-03-03 22:32:36",
         "1",
         "66",
         "1",
         null,
         null,
         null
        ],
        [
         "29",
         "79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03 22:27:07",
         "4",
         "230",
         "2",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n",
         "-2.0"
        ],
        [
         "30",
         "79475268",
         "Count of Combination of bigrams",
         "<p>I have create a dataset as follows using bigrams</p>\n<pre><code>index                   |   product_action\n-------------------------------------------------------|\n('customer', 'called')  |   action  \n('customer', 'service') |   action\n('blue', 'dress')       |   product\n('the', 'service')      |   product\n('to', 'complain')      |   action\n('complain', 'about')   |   action\n('service', 'received') |   action\n('the', 'dress')        |   product\n</code></pre>\n<p>I want to know if in each sentence how many times the combination has occured for the entire dataset</p>\n<p>I have tried to use a count on the bigram</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_bigrams(text):\n    tokens = nltk.word_tokenize(text.lower()) \n    return list(ngrams(tokens, 2))  \n\ndef count_bigrams(text, bigram):\n    bigrams = get_bigrams(text)\n    return bigrams.count(bigram)\n</code></pre>\n<p>The dataset I have in mind is as follows:</p>\n<pre><code>product           |      action               |  count\n---------------------------------------------------------|\n('blue', 'dress') |  ('customer', 'called')   |   10\n</code></pre>\n",
         "2025-02-28 10:47:22",
         "0",
         "41",
         "0",
         null,
         null,
         null
        ],
        [
         "31",
         "79465047",
         "Where is the HuggingFace model saved in when loading a model on colab?",
         "<p>I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.).</p>\n<pre><code>model_id = &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;\n\n\npipeline = transformers.pipeline(\n            &quot;text-generation&quot;,\n            model=model_id,\n            #model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16, &quot;cache_dir&quot;: cache_dir},\n            device_map=&quot;auto&quot;)\n</code></pre>\n",
         "2025-02-24 23:25:42",
         "1",
         "53",
         "1",
         null,
         null,
         null
        ],
        [
         "32",
         "79464254",
         "Understanding the Difference Between Entropy and Cross-Entropy in Language Models: Practical Example with Character-Level Unigram Model",
         "<p>I'm trying to understand the difference between entropy and cross-entropy, as I often hear about the entropy of a language and the cross-entropy of a language model, and I want to understand the link between the two.</p>\n<p>To simplify things, let's consider a language (with a vocabulary) and a language model trained on that language.</p>\n<p>We'll work at the character level (which gives us 26 characters), and a limited number of words (let's take the 20 names below).</p>\n<pre><code>prenoms = [\n    &quot;Alice&quot;, &quot;Alfred&quot;, &quot;Alina&quot;, &quot;Aline&quot;, &quot;Alexandre&quot;, \n    &quot;Alicia&quot;, &quot;Alison&quot;, &quot;Alma&quot;, &quot;Alva&quot;, &quot;Elise&quot;, \n    &quot;Elisa&quot;, &quot;Eliane&quot;, &quot;Alain&quot;, &quot;Amélie&quot;, &quot;Arline&quot;, \n    &quot;Olivier&quot;, &quot;Oline&quot;, &quot;Alva&quot;, &quot;Eliott&quot;, &quot;Julien&quot;\n]\n</code></pre>\n<p>How do we calculate the entropy over these 20 names (i.e., the entropy of our language) and the cross-entropy for our language model (let's take a unigram model or any language model you prefer to help me to understand)?</p>\n<p>If you have a more relevant example, I’m open to it.</p>\n<p>PS: My confusion comes from the fact that, in general definitions, we talk about a (language) distribution P when calculating entropy (without quite knowing how to calculate it), and about two distributions P and Q when calculating cross-entropy (where P is a one-hot encoding vector in this case, when calculating cross-entropy-loss).</p>\n<p>PS2:  A python code could help me to well understand, here is my understanding. I based it on y understanding of Jurasky Book (and <a href=\"https://huggingface.co/docs/transformers/perplexity\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/perplexity</a>)</p>\n<pre class=\"lang-py prettyprint-override\"><code>def distribution_ngrams(text, n=4):\n    &quot;&quot;&quot;\n    &quot;&quot;&quot;\n    import math\n    from collections import Counter \n\n    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n    counts = Counter(ngrams)\n    total = len(ngrams)\n    \n    # Calculate the distribution of n-grams\n    distribution = {ngram: count/total for ngram, count in counts.items()}\n    return distribution\n\ndef language_entropy_ngrams(text, n_approx=4):\n    &quot;&quot;&quot;\n    Calculate an estimate of the entropy of a text using n-grams (normally, we take a very large n and consider an infinite sequence L)\n    &quot;&quot;&quot;\n    import math\n    distribution = distribution_ngrams(text, n_approx)\n    # Calculate entropy\n    entropy = -sum((p * math.log2(p)) for ngram,p in distribution.items())\n    entropy_rate = entropy / n_approx  # normalize by the size of the n-gram\n    return entropy_rate  \n\ndef model_cross_entropy(text,n_approx=4):\n    &quot;&quot;&quot;\n    Calculate the cross-entropy between the true text and the model's predictions\n    &quot;&quot;&quot;\n    import math\n    unigram_model_distribution =  distribution_ngrams(text, 1)\n    language_model_distribution_approximation = distribution_ngrams(text, n_approx)\n\n    q = {}\n    cross_entropy = 0\n    for ngram,p in language_model_distribution_approximation.items():\n        q[ngram] = 1\n        for c in ngram:\n            q[ngram] = q[ngram]*unigram_model_distribution[c]\n        cross_entropy -= p*math.log2(q[ngram])\n        \n    return cross_entropy/n_approx\n\nif __name__ == &quot;__main__&quot;:\n    prenoms = [&quot;Alice&quot;, &quot;Alfred&quot;, &quot;Alina&quot;, &quot;Aline&quot;, &quot;Alexandre&quot;, &quot;Alicia&quot;, \n            &quot;Alison&quot;, &quot;Alma&quot;, &quot;Alva&quot;, &quot;Elise&quot;, &quot;Elisa&quot;, &quot;Eliane&quot;, &quot;Alain&quot;, \n            &quot;Amélie&quot;, &quot;Arline&quot;, &quot;Olivier&quot;, &quot;Oline&quot;, &quot;Alva&quot;, &quot;Eliott&quot;, &quot;Julien&quot;] #each prenonm can be seen as a sequence of characters\n\n\n    L = ''.join(prenoms).lower() #the corpus/language L can be seen as the concatenation of the sequences\n    print(language_entropy_ngrams(L))\n    print(model_cross_entropy(L))\n\n\n\n</code></pre>\n",
         "2025-02-24 16:35:11",
         "1",
         "51",
         "0",
         null,
         null,
         null
        ],
        [
         "33",
         "79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22 16:06:11",
         "-1",
         "40",
         "1",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n",
         "1.0"
        ],
        [
         "34",
         "79455954",
         "Where is GENIA corpus with annotated uncertainity?",
         "<p>I am looking for the well known GENIA corpus, which annotates events and uncertainity of the events (doubtful, etc.) The old webpage seems not to be available any more. I cannot find a Genia corpus version on hugging face, which would contain the uncertainity annotation. Does anybody know? I am usually quite good in finding data I want :-( MAybe I am blind this time :-(</p>\n",
         "2025-02-20 22:14:15",
         "0",
         "18",
         "0",
         null,
         null,
         null
        ],
        [
         "35",
         "79455927",
         "how to get the target generated query on a self-query retriever(langchain)",
         "<p>I'm implementing a <a href=\"https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html\" rel=\"nofollow noreferrer\">self-query retriever</a>  using langchain with OpenSearch as the target vectore store, so far everything is good but we need to capture the generated query in DSL, for debugging and auditing purposes, after some testing I cannot find how to do it, I found how to return thye <a href=\"https://api.python.langchain.com/en/latest/chains/langchain.chains.query_constructor.ir.StructuredQuery.html\" rel=\"nofollow noreferrer\">StructuredQuery</a>, and how to use the StructuredQuery and <a href=\"https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.self_query.opensearch.OpenSearchTranslator.html\" rel=\"nofollow noreferrer\">OpenSearchTranslator</a> to  get a step closer to the final query, however it is not the final query sent to OpenSearch. Question is, how to get the query? This is my current code(that returns something close to it but not the final version):</p>\n<pre><code>opensearch_translator = OpenSearchTranslator()\ndef show_translated_query(query):\n    chain_structured_query = retriever.llm_chain.invoke(query)\n    print(&quot;langchain structured query:&quot;)\n    print(chain_structured_query)\n    os_structured_query = opensearch_translator.visit_structured_query(chain_structured_query)\n    print(&quot;OS query(semantic, filter):&quot;)\n    print(os_structured_query)\n\nshow_translated_query(&quot;a fire ocurring before 2023&quot;)\n&gt;&gt;langchain structured query:\n&gt;&gt;query='fire' filter=Comparison(comparator=&lt;Comparator.LT: 'lt'&gt;, attribute='year', value=2023) limit=None\n&gt;&gt;OS query(semantic, filter):\n&gt;&gt;('fire', {'filter': {'range': {'metadata.year': {'lt': 2023}}}})\n</code></pre>\n",
         "2025-02-20 21:57:26",
         "0",
         "88",
         "0",
         null,
         null,
         null
        ],
        [
         "36",
         "79451974",
         "word/ sentence similarities",
         "<p>I am trying to find if a given word/ set of words are similar to a definition.</p>\n<p>Example - Definition - &quot;vegetarian User&quot;</p>\n<p>Now, if I want to check a set of sentences like below</p>\n<pre><code>sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n</code></pre>\n<p>I tried using some sentence transformer like below</p>\n<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n</code></pre>\n<p>But this is not giving me expected results.</p>\n<p>What is the best approach to achieve results like below?</p>\n<pre><code>[False,True,True,False]\n</code></pre>\n<p>Is it doable with nlp/ some other technique?</p>\n",
         "2025-02-19 15:47:45",
         "1",
         "50",
         "1",
         "79461281.0",
         "<p>Yes, it’s definitely doable using NLP! The key here is that you don’t need a full similarity matrix; you want to check if each sentence is semantically similar to the given definition.</p>\n<p>✅ Better Approach:\nEncode both the definition and sentences using a sentence transformer.\nCompute cosine similarity between the definition embedding and each sentence embedding.\nSet a threshold (e.g., 0.6 or 0.7) to determine if they are &quot;similar enough.&quot;</p>\n<pre><code>from sentence_transformers import SentenceTransformer, util\n# Load the pre-trained model\nmodel = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\n\n# Definition and sentences\ndefinition = &quot;vegetarian User&quot;\nsentences = [\n  'vegetarian User',\n  'user sometimes eats chicken',\n  'user is vegetarian',\n  'user only eats fruits',\n  'user likes fish'\n]\n\n# Encode the definition and sentences\ndefinition_embedding = model.encode(definition, convert_to_tensor=True)\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine similarities\nsimilarities = util.cos_sim(definition_embedding, sentence_embeddings)[0]\n\n# Set a threshold for similarity (tune this value as needed)\nthreshold = 0.6\nresults = [sim &gt;= threshold for sim in similarities]\n\n# Print results\nprint(results)  # Example output: [True, False, True, False, False]\n</code></pre>\n<p>💡 Explanation:\nutil.cos_sim computes the cosine similarity between the definition and each sentence.\nThreshold tuning:\nIf the similarity is above the threshold, consider it True.\nAdjust the threshold based on how strict you want the matching.</p>\n<p>🔍 Why the original approach didn’t work:\nmodel.similarity doesn’t exist in the SentenceTransformers API.\nYou were computing a sentence-to-sentence matrix, not definition-to-sentence comparisons.</p>\n",
         "1.0"
        ],
        [
         "37",
         "79449476",
         "How do I remove escape characters from output of nltk.word_tokenize?",
         "<p>How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output.</p>\n<p>Here's my code:</p>\n<pre><code>from __future__ import division\nimport nltk, re, pprint\nfrom urllib.request import urlopen\n\nurl = &quot;https://www.gutenberg.org/cache/epub/75394/pg75394.txt&quot;\nraw = urlopen(url).read()\nraw = raw.decode('utf-8')\ntokens = nltk.word_tokenize(raw)\nprint(type(tokens))\nprint(len(tokens))\nprint(tokens[:10])\n</code></pre>\n<p>And the output, with the escapes visible in the first list item:\n<a href=\"https://i.sstatic.net/L1QJ1Mdr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/L1QJ1Mdr.png\" alt=\"enter image description here\" /></a></p>\n<p>I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong?</p>\n<p>I'm using Python 3.12.1 on Windows 11.</p>\n<p>Thanks in advance - please do let me know if I can provide any further helpful information.</p>\n",
         "2025-02-18 20:10:13",
         "1",
         "65",
         "1",
         null,
         null,
         null
        ],
        [
         "38",
         "79449168",
         "MiniBatchKMeans BERTopic not returning topics for half of data",
         "<p>I am trying to topic a dataset of tweets. I have around 50 million tweets. Unfortunately, such a large dataset will not fit in ram (even 128GB) due to the embeddings. Therefore, I have been working on making an incremental BERTopic as per the <a href=\"https://maartengr.github.io/BERTopic/getting_started/online/online.html\" rel=\"nofollow noreferrer\">docs</a></p>\n<p>As such:</p>\n<pre class=\"lang-none prettyprint-override\"><code>from bertopic.vectorizers import OnlineCountVectorizer\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom sklearn.cluster import MiniBatchKMeans\nimport numpy as np\n\n\nclass SafeIncrementalPCA(IncrementalPCA):\n    def partial_fit(self, X, y=None):\n        # Ensure the input is contiguous and in float64\n        X = np.ascontiguousarray(X, dtype=np.float64)\n        return super().partial_fit(X, y)\n    \n    def transform(self, X):\n        result = super().transform(X)\n        # Force the output to be float64 and contiguous\n        return np.ascontiguousarray(result, dtype=np.float64)\n\n\nvectorizer_model = OnlineCountVectorizer(stop_words=&quot;english&quot;)\nctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\numap_model = SafeIncrementalPCA(n_components=100)\ncluster_model = MiniBatchKMeans(n_clusters=1000, random_state=0)\n\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic(umap_model=umap_model,\n                       hdbscan_model=cluster_model,\n\nfor docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):\n\n    docs_pdf = docs_delayed.compute()\n    emb_pdf = emb_delayed.compute()\n\n    docs = docs_pdf[&quot;text&quot;].tolist()\n    embeddings = np.vstack(emb_pdf['embeddings'].tolist())\n    \n    # Partial fit your model (make sure your model supports partial_fit, like many scikit-learn estimators do)\n    topic_model.partial_fit(docs, embeddings)\n\n</code></pre>\n<p>and then transforming the dataset into a SQL database:</p>\n<pre class=\"lang-none prettyprint-override\"><code>\nfor docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):\n\n    docs_pdf = docs_delayed.compute()\n    emb_pdf = emb_delayed.compute()\n    docs = docs_pdf[&quot;text&quot;].tolist()\n    embeddings = np.vstack(emb_pdf['embeddings'].tolist())\n\n    # 3) Apply BERTopic on this shard\n    topics, probs = topic_model.transform(docs, embeddings)\n\n    # Save topics to DataFrame\n    df_topics = pd.DataFrame({\n        &quot;tweet_id&quot;: docs_pdf[&quot;id&quot;].tolist(),\n        &quot;topic&quot;: topics,\n        &quot;probability&quot;: probs\n    })\n\n    ## Merge &amp; store in DB\n    docs_pdf[&quot;topic&quot;] = df_topics[&quot;topic&quot;]\n    docs_pdf[&quot;probability&quot;] = df_topics[&quot;probability&quot;]\n    docs_pdf.to_sql(&quot;tweets&quot;, engine, if_exists=&quot;append&quot;, index=False)\n</code></pre>\n<p>I've been trying to do this for a quite a while and this is the closest working example I have gotten. The only issue is half of the dataset has null topics in the database at the end. From what I understand of the theory, MiniBatchKMeans should not have any outliers and therefore all tweets should be assigned to at least one topic, right? I've checked out the unclassified tweets in question and there is nothing in their doc that should suggest it would be hard to classify (relative to others that are classified).</p>\n<p>I would be very happy to hear any sort of suggestion on what could be going wrong and how I could fix this!</p>\n<p>Thanks!</p>\n",
         "2025-02-18 17:42:59",
         "0",
         "35",
         "0",
         null,
         null,
         null
        ],
        [
         "39",
         "79448878",
         "Python Farm-haystack Dependencies",
         "<p>i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these</p>\n<p>from this implementation:</p>\n<pre><code>import os\nfrom typing import List\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes import PreProcessor, BM25Retriever, FARMReader\n\n# Initialize an in-memory document store (replaceable with Elasticsearch)\ndocument_store = InMemoryDocumentStore()\n\n# Folder where uploaded documents are stored\nUPLOAD_FOLDER = &quot;uploaded_docs&quot;\n\n# Ensure the upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\n\ndef list_documents() -&gt; List[str]:\n    &quot;&quot;&quot;List all uploaded documents.&quot;&quot;&quot;\n    try:\n        return os.listdir(UPLOAD_FOLDER)\n    except FileNotFoundError:\n        raise RuntimeError(f&quot;Upload folder '{UPLOAD_FOLDER}' not found. Please create it.&quot;)\n\n\ndef read_document(file_path: str) -&gt; str:\n    &quot;&quot;&quot;Read the content of a document.&quot;&quot;&quot;\n    try:\n        with open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:\n            return f.read()\n    except Exception as e:\n        raise RuntimeError(f&quot;Error reading file '{file_path}': {str(e)}&quot;)\n\n\ndef preprocess_document(content: str) -&gt; List[dict]:\n    &quot;&quot;&quot;Preprocess the document content into smaller chunks for indexing.&quot;&quot;&quot;\n    preprocessor = PreProcessor(\n        split_by=&quot;word&quot;,  # Split the content into chunks by word count\n        split_length=200,  # Chunk size\n        split_overlap=20,  # Overlap between chunks\n        split_respect_sentence_boundary=True,\n    )\n    return preprocessor.process({&quot;content&quot;: content})\n\n\ndef index_document(file_name: str):\n    &quot;&quot;&quot;Read, preprocess, and index a document.&quot;&quot;&quot;\n    file_path = os.path.join(UPLOAD_FOLDER, file_name)\n    if not os.path.isfile(file_path):\n        raise RuntimeError(f&quot;File '{file_name}' not found in '{UPLOAD_FOLDER}'.&quot;)\n\n    content = read_document(file_path)\n    chunks = preprocess_document(content)\n\n    # Prepare chunks in Haystack-compatible format\n    formatted_chunks = [{&quot;content&quot;: chunk[&quot;content&quot;]} for chunk in chunks]\n    document_store.write_documents(formatted_chunks)\n\n    return {\n        &quot;message&quot;: f&quot;Document '{file_name}' indexed successfully.&quot;,\n        &quot;chunks_count&quot;: len(formatted_chunks),\n    }\n\n\ndef search_documents(query: str):\n    &quot;&quot;&quot;Search indexed documents using a query.&quot;&quot;&quot;\n    retriever = BM25Retriever(document_store=document_store)\n    reader = FARMReader(model_name_or_path=&quot;deepset/roberta-base-squad2&quot;, use_gpu=False)\n    \n    # Retrieve documents\n    retrieved_docs = retriever.retrieve(query)\n    if not retrieved_docs:\n        return {&quot;message&quot;: &quot;No relevant documents found.&quot;}\n\n    # Reader to predict answers from retrieved documents\n    answers = reader.predict(query=query, documents=retrieved_docs, top_k=3)\n\n    # Serialize the results to avoid unsupported types\n    results = [\n        {\n            &quot;answer&quot;: ans.answer,\n            &quot;score&quot;: ans.score,\n            &quot;context&quot;: ans.context,\n            &quot;document_id&quot;: ans.document_id,\n        }\n        for ans in answers[&quot;answers&quot;]\n    ]\n\n    return {&quot;results&quot;: results}\n</code></pre>\n<p>But i keep getting this error:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 122, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 131, in _main\n    prepare(preparation_data)\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 244, in prepare\n    _fixup_main_from_name(data['init_main_from_name'])\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 268, in _fixup_main_from_name\n    main_content = runpy.run_module(mod_name,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;&lt;frozen runpy&gt;&quot;, line 226, in run_module\n  File &quot;&lt;frozen runpy&gt;&quot;, line 98, in _run_module_code\n  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/main.py&quot;, line 7, in &lt;module&gt;\n    from app.views.routes import router\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/views/routes.py&quot;, line 2, in &lt;module&gt;\n    from app.services.document_service import list_documents, index_document, search_documents\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/services/document_service.py&quot;, line 3, in &lt;module&gt;\n    from haystack.document_stores import InMemoryDocumentStore\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/__init__.py&quot;, line 8, in &lt;module&gt;\n    from haystack.schema import Document, Answer, Label, MultiLabel, Span, EvaluationResult, TableCell\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/schema.py&quot;, line 42, in &lt;module&gt;\n    @dataclass\n     ^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 250, in dataclass\n    return create_dataclass(_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 241, in create_dataclass\n    pydantic_complete = _pydantic_dataclasses.complete_dataclass(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py&quot;, line 159, in complete_dataclass\n    schema = gen_schema.generate_schema(cls, from_dunder_get_core_schema=False)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 832, in match_type\n    return self._dataclass_schema(obj, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1561, in _dataclass_schema\n    args = sorted(\n           ^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1562, in &lt;genexpr&gt;\n    (self._generate_dc_field_schema(k, v, decorators) for k, v in fields.items()),\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 933, in _generate_dc_field_schema\n    common_field = self._common_field_schema(name, field_info, decorators)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1081, in _common_field_schema\n    schema = self._apply_annotations(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1825, in _apply_annotations\n    schema = get_inner_schema(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py&quot;, line 82, in __call__\n    schema = self._handler(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1806, in inner_handler\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 840, in match_type\n    return self._match_generic_type(obj, origin)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 864, in _match_generic_type\n    return self._union_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1152, in _union_schema\n    choices.append(self.generate_schema(arg))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 844, in match_type\n    return self._unknown_type_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 405, in _unknown_type_schema\n    raise PydanticSchemaGenerationError(\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for &lt;class 'pandas.core.frame.DataFrame'&gt;. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(&lt;some type&gt;) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(&lt;some type&gt;)` since we do not call `__get_pydantic_core_schema__` on `&lt;some type&gt;` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type\n</code></pre>\n",
         "2025-02-18 16:05:55",
         "-1",
         "41",
         "1",
         null,
         null,
         null
        ],
        [
         "40",
         "79437334",
         "How many obs per class are necessary? - transfer learning w. BERT fine-tuning",
         "<p>I seek advice on a classification problem in industry.</p>\n<p>The rows in a dataset must be classified/labeled--it lacks a target/column (labels have dot-separated levels like 'x.x.x.x.x.x.x')--during every business cycle. For each cycle, a dataset is given as input to this exercise. The dataset includes some variables, most importantly 1. an ID variable and 2. a short text description. When the dataset is correctly classified, the ID will correspond perfectly to a class. At every iteration, most of the ID---class links as identities will carry over, but some won't: There will be new labels and some labels get redefined. Basically, <em>there will be unlabeled rows at every iteration</em>.</p>\n<p>The classes are assigned using variable 2, the text description (besides a few others) compared with the content of a, let's say, scoring manual accompanying the cycle's new dataset. (Well, rather, the dataset and the key come from two independent business processes, but we can ignore that here.) As a de facto scoring manual, the classes are unique and are described only once using a handful of descriptions, which are short text fields about what identifies, =, and what contrasts, !=, a class. So, <em>in the manual (available as a second dataset), each class is described only once</em>.</p>\n<p>The desire is to classify the datasets ad infinitum as automatically as possible. The dataset has already been classified at time T0 but will need more new classes at T1. It is possible to supervise a learning algorithm on the first batch. <strong>The question</strong> is whether subsequent batches require labeling by experts for training the model or, ideally, whether the 'scoring manual' with one row/obs/example per label may suffice for fine-tuning? (more loosely formulated:  <a href=\"https://stats.stackexchange.com/q/310947/207649\">How much data is needed for transfer learning?</a>; and in the case of training from scratch: <a href=\"https://stats.stackexchange.com/questions/446667/how-many-data-points-per-class-is-neccesary-to-train-a-multi-class-deep-learning\">how many data points per class is neccesary to train a multi-class deep learning</a>)</p>\n",
         "2025-02-13 17:54:14",
         "0",
         "45",
         "0",
         null,
         null,
         null
        ],
        [
         "41",
         "79425052",
         "Generating an n-gram dataset based on an LLM",
         "<p>I want a dataset of common n-grams and their log likelihoods. Normally I would download the <a href=\"https://storage.googleapis.com/books/ngrams/books/datasetsv3.html\" rel=\"nofollow noreferrer\">Google Books Ngram Exports</a>, but I wonder if I can generate a better dataset using a large language model. For example, this script uses <a href=\"https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_completion\" rel=\"nofollow noreferrer\">llama_cpp.Llama.create_completion</a> to find likely 3-grams starting with &quot;welcome to&quot;:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from llama_cpp import Llama # pip install llama-cpp-python\n\nllm = Llama.from_pretrained(\n    repo_id=&quot;unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF&quot;,\n    filename=&quot;DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf&quot;,\n    logits_all=True,\n)\nprint(\n    llm.create_completion(\n        &quot;welcome to&quot;,\n        max_tokens=1,\n        logprobs=10,\n    )[&quot;choices&quot;][0][&quot;logprobs&quot;][&quot;top_logprobs&quot;][0],\n)\n</code></pre>\n<p>Output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>{' the': np.float32(-0.18572943), ' this': np.float32(-3.444591), ' our': np.float32(-4.0559974), ' python': np.float32(-4.3010955), ' a': np.float32(-4.571982), ' bc': np.float32(-5.036485), ' module': np.float32(-5.4879394), ' week': np.float32(-5.7402453), ' all': np.float32(-6.2308974), ' thread': np.float32(-6.272795)}\n</code></pre>\n<p>One issue is that the prompt gets prefixed with a <a href=\"https://www.linkedin.com/pulse/what-special-tokens-tokenization-farhan-naqvi-uxxsf/\" rel=\"nofollow noreferrer\">BOS token</a>, so I only get n-grams that appear at the beginning of a sentence. I can fix this by passing a list of tokens instead of a string. But this leads to a problem when generating 1-grams:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(\n    llm.create_completion(\n        [],\n        max_tokens=1,\n        logprobs=10,\n    )[&quot;choices&quot;][0][&quot;logprobs&quot;][&quot;top_logprobs&quot;][0],\n)\n</code></pre>\n<p><code>AssertionError</code> at <a href=\"https://github.com/abetlen/llama-cpp-python/blob/v0.3.7/llama_cpp/llama.py#L788\" rel=\"nofollow noreferrer\">llama.py line 788</a>: <code>assert self.n_tokens &gt; 0</code></p>\n<p>Apparently <a href=\"https://github.com/ggerganov/llama.cpp\" rel=\"nofollow noreferrer\">llama.cpp</a> is unable to generate text when no context is provided. I confirmed this by using llama.cpp directly without the Python wrapper. My question is, <strong>is this an arbitrary limitation of the library, or a fundamental limitation of the language model?</strong></p>\n<p>I found a possible clue in the model's <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/blob/6393b7559e403fd1d80bfead361586fd6f630a4d/config.json#L10\" rel=\"nofollow noreferrer\">config.json</a> file:</p>\n<pre class=\"lang-json prettyprint-override\"><code>  &quot;initializer_range&quot;: 0.02,\n</code></pre>\n<p>The <a href=\"https://huggingface.co/docs/transformers/en/model_doc/qwen2#transformers.Qwen2Config.initializer_range\" rel=\"nofollow noreferrer\">documentation</a> says that <code>initializer_range</code> is</p>\n<blockquote>\n<p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>\n</blockquote>\n<p>I imagine that the model has a hidden state vector which is initialized with random values sampled from a normal distribution, and the values get updated as context is added. I wonder if it's possible to sample from the model in its initial random state, and get a list of the most common words by sampling multiple times with different random seeds.</p>\n",
         "2025-02-09 14:21:10",
         "0",
         "45",
         "0",
         null,
         null,
         null
        ],
        [
         "42",
         "79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07 02:48:25",
         "-1",
         "45",
         "1",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n",
         "0.0"
        ],
        [
         "43",
         "79417001",
         "How to pass AzureOpenAIEmbeddings in CrewAI, I don't have api keys I use azure_ad_token?",
         "<p>How to pass AzureOpenAIEmbeddings, I don't have api keys I use azure_ad_token?</p>\n<pre><code>from langchain_openai.embeddings import AzureOpenAIEmbeddings\n\nazure_embeddings = AzureOpenAIEmbeddings(\nopenai_api_version=conf.pf_api_version,\nazure_endpoint=conf.pf_oa_endpoint_embed,\nazure_ad_token=tokens,\nmodel=conf.pf_embedding_engine,\n)\n\ncrew = Crew(\n  agents=[support_agent, support_quality_assurance_agent],\n  tasks=[inquiry_resolution, quality_assurance_review],\n  verbose=2,\n  memory=True,\n  embedder=embedder_config,\n)\n</code></pre>\n<pre><code>raise SchemaError([message] + x.autos, [e.format(data) if e else None] + x.errors)\nschema.SchemaError: Key 'embedder' error:\nKey 'config' error:\nWrong keys 'azure_ad_token', 'azure_endpoint', 'openai_api_version' in {'azure_endpoint\n</code></pre>\n",
         "2025-02-06 06:51:23",
         "0",
         "34",
         "0",
         null,
         null,
         null
        ],
        [
         "44",
         "79412932",
         "Calculating Topic Correlations or Coocurrences for keyATM",
         "<p>I have been playing around with the keyATM package extensively, however unfortunately there is no approach how to calculate topic correlations and cooccurences, once the model is calculated. I already adjusted the the alpha prior to receive more topic correlation, however I am stuck at this point.</p>\n<pre><code># Compute total topics\ntotal_k &lt;- 25 + length(keywords)  \n# Construct Gamma matrix for priors\ngamma_matrix &lt;- matrix(as.numeric(1), nrow = total_k, ncol = 2)\n\n\nfor(al in 1:5){\n\nprint(paste0(&quot;Alpha set to &quot;,al))  \n  \npriors          &lt;-  list(alpha = rep(al, total_k),\n                         beta = 0.01,\n                         beta_s = 0.1,\n                         gamma = gamma_matrix)\n\nout_temp &lt;- keyATM(\n  docs              = keyATM_docs,    # text input\n  no_keyword_topics = 25,              # number of topics without keywords\n  keywords          = keywords,       # keywords\n  model             = &quot;base&quot;,         # select the model\n  options           = list(seed = 250,\n                           iterations = 1500,\n                           verbose = TRUE),\n  priors = priors\n)\n\nassign(paste0(&quot;out_alpha&quot;,al), out_temp)\n\n}\n</code></pre>\n<p>I tried several approaches from other topic model packages, but nothing worked.\nIf anyone has any recommendations how to proceed from here, I would be very thankful.</p>\n",
         "2025-02-04 20:17:43",
         "0",
         "28",
         "0",
         null,
         null,
         null
        ],
        [
         "45",
         "79411913",
         "How to resolve ValueError while training Seq2Seq using DataCollatorForSeq2Seq",
         "<p>I want to fine tune a VisionEncoderDecoderModel.from_pretrained(model_name)\nI use a CustomOCRDataset from <a href=\"https://learnopencv.com/fine-tuning-trocr-training-trocr-to-recognize-curved-text/\" rel=\"nofollow noreferrer\">Learn Open CV</a>.\nBut the default_data_collator fails to stack the inputs because the samples have a different shape , so I decided to use DataCollatorForSeq2Seq and Resize in augmentation.</p>\n<p>I get an error\nValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['pixel_values']</p>\n<p>So I changed <strong>getitem</strong>  to get input_ids, but the error is still the same.</p>\n<pre><code>def __getitem__(self, idx):\n        file_name = self.df['file_name'][idx]\n        text = self.df['text'][idx]\n\n        assert text.strip() != &quot;&quot;, f&quot;ERROR Empty text in {idx}&quot;\n\n        # Read the image, apply augmentations, and get the transformed pixels.\n        image = Image.open(self.root_dir + file_name).convert('RGB')\n        image = train_transforms(image)\n        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n        # Pass the text through the tokenizer and get the labels,\n        # i.e. tokenized labels.\n        labels = self.processor.tokenizer(\n            text,\n            padding='max_length',\n            max_length=self.max_target_length,\n            return_tensors='pt'\n        ).input_ids.squeeze(0)\n\n        # We are using -100 as the padding token.\n        labels = torch.where(labels == self.processor.tokenizer.pad_token_id, torch.tensor(-100), labels)\n        encoding = {&quot;pixel_values&quot;: pixel_values.squeeze(0),\n                    &quot;input_ids&quot;: labels}\n        return encoding\n</code></pre>\n<pre><code>@dataclass(frozen=True)\nclass TrainingConfig:\n    BATCH_SIZE:    int = 16\n    EPOCHS:        int = 5\n    LEARNING_RATE: float = 0.00005\n\n@dataclass(frozen=True)\nclass DatasetConfig:\n    DATA_ROOT:     str = image_dir\n\n@dataclass(frozen=True)\nclass ModelConfig:\n    MODEL_NAME: str = 'microsoft/trocr-base-handwritten'\n</code></pre>\n<pre><code># Augmentations.\ntrain_transforms = transforms.Compose([\n    transforms.Resize((1024, 880))\n])\n</code></pre>\n<pre><code>processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\ntrain_dataset = CustomOCRDataset(\n    root_dir=os.path.join(DatasetConfig.DATA_ROOT, train_destination),\n    df=train_df,\n    processor=processor\n)\nvalid_dataset = CustomOCRDataset(\n    root_dir=os.path.join(DatasetConfig.DATA_ROOT, test_destination),\n    df=test_df,\n    processor=processor\n)\n</code></pre>\n<pre><code>training_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy='epoch',\n    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n    fp16=True,\n    output_dir='seq2seq_model_printed/',\n    logging_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=5,\n    report_to='tensorboard',\n    num_train_epochs=TrainingConfig.EPOCHS\n)\n</code></pre>\n<pre><code>data_collator = DataCollatorForSeq2Seq(tokenizer=processor.tokenizer, model=model, padding=True)\n# Initialize trainer.\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.feature_extractor,\n    args=training_args,\n    compute_metrics=compute_cer,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    data_collator=data_collator\n)\ntrainer.train()\n</code></pre>\n<p>The full ERROR :</p>\n<pre><code>File \\transformers\\data\\data_collator.py:599, in DataCollatorForSeq2Seq.__call__(self, features, return_tensors)\n    596 non_labels_features = [{k: v for k, v in feature.items() if k != label_name} for feature in features]\n    598 # run through tokenizer without labels to ensure no side effects\n--&gt; 599 batch = pad_without_fast_tokenizer_warning(\n    600     self.tokenizer,\n    601     non_labels_features,\n    602     padding=self.padding,\n    603     max_length=self.max_length,\n    604     pad_to_multiple_of=self.pad_to_multiple_of,\n    605     return_tensors=return_tensors,\n    606 )\n    608 # we have to pad the labels manually as we cannot rely on `tokenizer.pad` and we need them to be of the same length to return tensors\n    609 no_padding = self.padding is False or self.padding == PaddingStrategy.DO_NOT_PAD\n\nFile \\transformers\\data\\data_collator.py:66, in pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs)\n     63 tokenizer.deprecation_warnings[&quot;Asking-to-pad-a-fast-tokenizer&quot;] = True\n     65 try:\n---&gt; 66     padded = tokenizer.pad(*pad_args, **pad_kwargs)\n     67 finally:\n     68     # Restore the state of the warning.\n     69     tokenizer.deprecation_warnings[&quot;Asking-to-pad-a-fast-tokenizer&quot;] = warning_state\n\nFile \\transformers\\tokenization_utils_base.py:3305, in PreTrainedTokenizerBase.pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\n   3303 # The model's main input name, usually `input_ids`, has been passed for padding\n   3304 if self.model_input_names[0] not in encoded_inputs:\n-&gt; 3305     raise ValueError(\n   3306         &quot;You should supply an encoding or a list of encodings to this method &quot;\n   3307         f&quot;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&quot;\n   3308     )\n   3310 required_input = encoded_inputs[self.model_input_names[0]]\n   3312 if required_input is None or (isinstance(required_input, Sized) and len(required_input) == 0):\n\nValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['pixel_values']\n</code></pre>\n<p>The solutions that I've already tried were:\n-&gt; to do transforms.Resize((1024, 880)),\n-&gt; to use a custom data collator\nsuch as:\n`def collate_fn(batch):\nbatch = list(filter(lambda x: x is not None, batch))</p>\n<pre><code># Pack pixel vals und labels separately in lists\npixel_values = [item['pixel_values'] for item in batch]\nlabels = [item['labels'] for item in batch]\n\n# Convert lists to tensors + padding\npixel_values = torch.stack(pixel_values)\nlabels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n\nreturn {\n    &quot;pixel_values&quot;: pixel_values,\n    &quot;labels&quot;: labels\n}`\n</code></pre>\n<p>I got a TypeError: ViTModel.forward() got an unexpected keyword argument 'num_items_in_batch'. That's why I decided to use DataCollatorForSeq2Seq.</p>\n<p>But I'm getting another error over and over: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['pixel_values'].\nThanks in advance!</p>\n",
         "2025-02-04 13:44:44",
         "2",
         "38",
         "0",
         null,
         null,
         null
        ],
        [
         "46",
         "79408298",
         "PunktTokenizer does not work with Russian `я.`",
         "<p>When tokenizing paragraphs to sentences in the Russian language, I am observing the special case when the sequence is not treated as the end of the sentence. The case is with the <code>я.</code> at the end of the sentence. See the working example:</p>\n<pre><code>import nltk\n\ntok = nltk.tokenize.PunktTokenizer('russian')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять я. Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять я. - Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n</code></pre>\n<p><a href=\"https://i.sstatic.net/yrj2ukn0.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/yrj2ukn0.png\" alt=\"the output\" /></a></p>\n<p>The second case works as expected. It differs only in adding the dash (kind of introduction of the speaker's note [sorry for my lack of terms]).</p>\n<p>The <code>я</code> is not present in the Russian abbreviations (<code>c:\\nltk_data\\tokenizers\\punkt_tab\\russian\\abbrev_types.txt</code>). However, even when added to the file with abbreviations, it does not make a difference.</p>\n<p>How the situation should be fixed?</p>\n<p>The nltk is of the version 3.9.1, the nltk_data are shared -- stored in c:\\nltk_data; fresh download (30. 1. 2025). Python 3.12 on Windows 10 was used.</p>\n<p><strong>Update:</strong></p>\n<p>(... observing donwnvotes). Please, I am very new to nltk, and I tried my best to get the answer by myself. When down-voting, write the comment why. I am searching for the answer in various sources without success.</p>\n<p>I am using UTF-8 encoding for both the test script here, and also in the file that is actually being processed (the image take from notepad++).\n<a href=\"https://i.sstatic.net/bmEd4NOU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/bmEd4NOU.png\" alt=\"UTF-8 encoding used\" /></a></p>\n<p><strong>2nd Update:</strong>\nAs Joop Eggen suggested, I have tried with other single letters:</p>\n<pre><code>import nltk\n\ntok = nltk.tokenize.PunktTokenizer('russian')\n\nprint('\\n================= я in the original question (я not in the abbrev_types.txt)\\n')\nline = 'Родилась заново, стал размышлять я. Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять я. - Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('\\n================= г is in the abbrev_types.txt\\n')\nline = 'Родилась заново, стал размышлять г. Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять г. - Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('\\n================= а IS NOT the abbrev_types.txt\\n')\nline = 'Родилась заново, стал размышлять а. Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять а. - Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n</code></pre>\n<p><a href=\"https://i.sstatic.net/9dezvzKN.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/9dezvzKN.png\" alt=\"other single letters\" /></a></p>\n<p>The <code>а</code> is also <strong>not</strong> in the abbreviations; however, it is treated as if it was (differently from <code>я</code>). The <code>г</code> is in the abbreviations, so here the behavior is expected.</p>\n",
         "2025-02-03 09:12:19",
         "0",
         "35",
         "0",
         null,
         null,
         null
        ],
        [
         "47",
         "79406743",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text",
         "<p>I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns &quot;UNK&quot;. Here is my code:</p>\n<pre><code>from quickumls import QuickUMLS\n\nquickumls_fp = &quot;med7_en/lib/python3.10/site-packages/quickumls&quot;\nmatcher = QuickUMLS(quickumls_fp)\n\ndef extract_umls_cuis(text):\n    &quot;&quot;&quot;Extract UMLS CUIs using QuickUMLS.&quot;&quot;&quot;\n    if isinstance(text, str):\n        matches = matcher.match(text)\n        if matches:\n            return [match['cui'] for match in matches[0]]\n        else:\n            return &quot;UNK&quot;\n\nsample_text = &quot;diclofenac.&quot;\nprint(extract_umls_cuis(sample_text))\n</code></pre>\n<p>What I Have Checked:</p>\n<ul>\n<li>QuickUMLS Installation: I have installed QuickUMLS correctly.</li>\n<li>UMLS Data Availability: I have set the correct path to QuickUMLS.</li>\n<li>Different Input Words: I tried various medical terms, but all return &quot;UNK&quot;.</li>\n</ul>\n",
         "2025-02-02 14:12:55",
         "0",
         "27",
         "1",
         null,
         null,
         null
        ],
        [
         "48",
         "79402492",
         "Transformers PaliGemma evaluate and compute_loss fail with tensors/device errors",
         "<p>I'm loading a PaliGemma2 model <code>google/paligemma2-3b-pt-224</code> and trying to fine-tune using Trainer/Seq2SeqTrainer. If I add evaluation, this fails. After doing some digging, I found that this only happens if the model is in evaluate mode.</p>\n<pre><code>batch = [valid_dataset[i] for i in range(8)]\ninputs = collate_fn(batch)\n#generate_ids = model.generate(**inputs, max_length=286+30)\ntrainer.model.train()\ntrainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\nprint(&quot;works&quot;)\ntrainer.model.train(False)\ntrainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\nprint(&quot;fails.&quot;)\n</code></pre>\n<p>I've worked around it by mokey-patching compute_loss_context_manager as follows:</p>\n<pre><code>orig_context_manager = trainer.compute_loss_context_manager\nclass TempTrainContext(object):\n    def __init__(self, trainer):\n        self.trainer = trainer\n        self.orig_context_manager = trainer.compute_loss_context_manager\n    def __enter__(self):\n        self.orig_context_inst = self.orig_context_manager()\n        self.orig_context_inst.__enter__()\n        self.training_enter = self.trainer.model.training\n        self.trainer.model.train()\n    def __exit__(self, type, value, traceback):\n        self.trainer.model.train(self.training_enter)\n        self.orig_context_inst.__exit__(type, value, traceback)\n    def __call__(self):\n        return self\n\ntrainer.compute_loss_context_manager = TempTrainContext(trainer)\n</code></pre>\n<p>(Bonus question: Is this safe to do, or will I train on the test set?)</p>\n<p>My versions are:</p>\n<pre><code>Python Version: 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 16:05:46) [GCC 13.3.0]\nTorch Version: 2.5.1+cu124\nCUDA Available: True\nCUDA Device Count: 2\nGPU Name: NVIDIA GeForce RTX 3090\nTransformers Version: 4.48.1\nTokenizers Version: 0.21.0\nAccelerate Version: 1.3.0\n</code></pre>\n<p>Error:</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[13], line 8\n      6 print(&quot;works&quot;)\n      7 trainer.model.train(False)\n----&gt; 8 trainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\n      9 print(&quot;fails.&quot;)\n     12 orig_context_manager = trainer.compute_loss_context_manager\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/trainer.py:3731, in Trainer.compute_loss(self, model, inputs, return_outputs, num_items_in_batch)\n   3729         loss_kwargs[&quot;num_items_in_batch&quot;] = num_items_in_batch\n   3730     inputs = {**inputs, **loss_kwargs}\n-&gt; 3731 outputs = model(**inputs)\n   3732 # Save past state if it exists\n   3733 # TODO: this needs to be fixed and made cleaner later.\n   3734 if self.args.past_index &gt;= 0:\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/accelerate/hooks.py:170, in add_hook_to_module.&lt;locals&gt;.new_forward(module, *args, **kwargs)\n    168         output = module._old_forward(*args, **kwargs)\n    169 else:\n--&gt; 170     output = module._old_forward(*args, **kwargs)\n    171 return module._hf_hook.post_forward(module, output)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/paligemma/modeling_paligemma.py:530, in PaliGemmaForConditionalGeneration.forward(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep)\n    525     labels = torch.where(input_ids == self.pad_token_id, self.config.ignore_index, labels)\n    527 causal_mask = self._update_causal_mask(\n    528     attention_mask, token_type_ids, past_key_values, cache_position, input_ids, inputs_embeds, is_training\n    529 )\n--&gt; 530 outputs = self.language_model(\n    531     attention_mask=causal_mask,\n    532     position_ids=position_ids,\n    533     past_key_values=past_key_values,\n    534     inputs_embeds=inputs_embeds,\n    535     use_cache=use_cache,\n    536     output_attentions=output_attentions,\n    537     output_hidden_states=output_hidden_states,\n    538     return_dict=return_dict,\n    539     cache_position=cache_position,\n    540     num_logits_to_keep=num_logits_to_keep,\n    541 )\n    543 logits = outputs.logits\n    544 loss = None\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:842, in Gemma2ForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\n    840 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n    841 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n--&gt; 842 outputs = self.model(\n    843     input_ids=input_ids,\n    844     attention_mask=attention_mask,\n    845     position_ids=position_ids,\n    846     past_key_values=past_key_values,\n    847     inputs_embeds=inputs_embeds,\n    848     use_cache=use_cache,\n    849     output_attentions=output_attentions,\n    850     output_hidden_states=output_hidden_states,\n    851     return_dict=return_dict,\n    852     cache_position=cache_position,\n    853 )\n    855 hidden_states = outputs[0]\n    856 # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:629, in Gemma2Model.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\n    617     layer_outputs = self._gradient_checkpointing_func(\n    618         decoder_layer.__call__,\n    619         hidden_states,\n   (...)\n    626         cache_position,\n    627     )\n    628 else:\n--&gt; 629     layer_outputs = decoder_layer(\n    630         hidden_states,\n    631         position_embeddings=position_embeddings,\n    632         attention_mask=causal_mask,\n    633         position_ids=position_ids,\n    634         past_key_value=past_key_values,\n    635         output_attentions=output_attentions,\n    636         use_cache=use_cache,\n    637         cache_position=cache_position,\n    638         **flash_attn_kwargs,\n    639     )\n    641 hidden_states = layer_outputs[0]\n    643 if output_attentions:\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/accelerate/hooks.py:170, in add_hook_to_module.&lt;locals&gt;.new_forward(module, *args, **kwargs)\n    168         output = module._old_forward(*args, **kwargs)\n    169 else:\n--&gt; 170     output = module._old_forward(*args, **kwargs)\n    171 return module._hf_hook.post_forward(module, output)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:299, in Gemma2DecoderLayer.forward(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\n    296 hidden_states = self.input_layernorm(hidden_states)\n    298 # Self Attention\n--&gt; 299 hidden_states, self_attn_weights = self.self_attn(\n    300     hidden_states=hidden_states,\n    301     position_embeddings=position_embeddings,\n    302     attention_mask=attention_mask,\n    303     position_ids=position_ids,\n    304     past_key_value=past_key_value,\n    305     output_attentions=output_attentions,\n    306     use_cache=use_cache,\n    307     cache_position=cache_position,\n    308 )\n    309 hidden_states = self.post_attention_layernorm(hidden_states)\n    310 hidden_states = residual + hidden_states\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/accelerate/hooks.py:170, in add_hook_to_module.&lt;locals&gt;.new_forward(module, *args, **kwargs)\n    168         output = module._old_forward(*args, **kwargs)\n    169 else:\n--&gt; 170     output = module._old_forward(*args, **kwargs)\n    171 return module._hf_hook.post_forward(module, output)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:224, in Gemma2Attention.forward(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\n    221 if past_key_value is not None:\n    222     # sin and cos are specific to RoPE models; cache_position needed for the static cache\n    223     cache_kwargs = {&quot;sin&quot;: sin, &quot;cos&quot;: cos, &quot;cache_position&quot;: cache_position}\n--&gt; 224     key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n    226 attention_interface: Callable = eager_attention_forward\n    227 if self.config._attn_implementation != &quot;eager&quot;:\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/cache_utils.py:1717, in HybridCache.update(self, key_states, value_states, layer_idx, cache_kwargs)\n   1714 else:\n   1715     update_fn = self._static_update\n-&gt; 1717 return update_fn(\n   1718     cache_position,\n   1719     layer_idx,\n   1720     key_states,\n   1721     value_states,\n   1722     k_out,\n   1723     v_out,\n   1724     k_out.shape[2],\n   1725 )\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/cache_utils.py:1694, in HybridCache._static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len)\n   1693 def _static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n-&gt; 1694     k_out[:, :, cache_position] = key_states\n   1695     v_out[:, :, cache_position] = value_states\n   1697     self.key_cache[layer_idx] = k_out\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!&quot;\n</code></pre>\n<p>Error of Evaluator (bottom half of file): <a href=\"https://gist.github.com/BlGene/607c7bee450e03835aa2bf0d2fd2959a\" rel=\"nofollow noreferrer\">https://gist.github.com/BlGene/607c7bee450e03835aa2bf0d2fd2959a</a></p>\n",
         "2025-01-31 10:44:20",
         "0",
         "44",
         "0",
         null,
         null,
         null
        ],
        [
         "49",
         "79402035",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query",
         "<p>How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation.\nI have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n",
         "2025-01-31 07:26:35",
         "1",
         "50",
         "1",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 20442
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AcceptedAnswerBody</th>\n",
       "      <th>AcceptedAnswerScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79557354</td>\n",
       "      <td>Sentencepiece not generating models after prep...</td>\n",
       "      <td>&lt;p&gt;So this is the log that I see on the termin...</td>\n",
       "      <td>2025-04-05 18:21:09</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79557315</td>\n",
       "      <td>How should I approach the word synonyms for re...</td>\n",
       "      <td>&lt;p&gt;I have created an aspect based list for ana...</td>\n",
       "      <td>2025-04-05 17:32:46</td>\n",
       "      <td>-1</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79557313</td>\n",
       "      <td>No attention output in jinaai/jina-embeddings-...</td>\n",
       "      <td>&lt;p&gt;When I use this model like so -&lt;/p&gt;\\n&lt;pre&gt;&lt;...</td>\n",
       "      <td>2025-04-05 17:29:15</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>2025-04-02 05:56:11</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>79552218.0</td>\n",
       "      <td>&lt;p&gt;The configuration file is missing the 'labe...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79548202</td>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>&lt;p&gt;I understand the -100 label id is used so t...</td>\n",
       "      <td>2025-04-01 09:21:17</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>79551169.0</td>\n",
       "      <td>&lt;p&gt;The author of the tutorial you mentioned se...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20437</th>\n",
       "      <td>42489</td>\n",
       "      <td>How to implement a \"related\" degree measure al...</td>\n",
       "      <td>&lt;p&gt;I was going to Ask a Question earlier today...</td>\n",
       "      <td>2008-09-03 20:21:04</td>\n",
       "      <td>8</td>\n",
       "      <td>456</td>\n",
       "      <td>2</td>\n",
       "      <td>42532.0</td>\n",
       "      <td>&lt;p&gt;One such way to implement such an algorithm...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20438</th>\n",
       "      <td>41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03 10:36:13</td>\n",
       "      <td>118</td>\n",
       "      <td>33200</td>\n",
       "      <td>11</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>&lt;p&gt;Actually what Google does is very much non-...</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20439</th>\n",
       "      <td>36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31 01:08:48</td>\n",
       "      <td>3</td>\n",
       "      <td>5661</td>\n",
       "      <td>6</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>&lt;p&gt;Citation from Vista &lt;a href=\"http://blogs.m...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20440</th>\n",
       "      <td>25332</td>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>&lt;p&gt;I'm looking for an existing library to summ...</td>\n",
       "      <td>2008-08-24 20:57:33</td>\n",
       "      <td>14</td>\n",
       "      <td>6491</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20441</th>\n",
       "      <td>23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22 22:45:10</td>\n",
       "      <td>27</td>\n",
       "      <td>6484</td>\n",
       "      <td>9</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>&lt;p&gt;We developed exactly what you are looking f...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20442 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       QuestionId                                              Title  \\\n",
       "0        79557354  Sentencepiece not generating models after prep...   \n",
       "1        79557315  How should I approach the word synonyms for re...   \n",
       "2        79557313  No attention output in jinaai/jina-embeddings-...   \n",
       "3        79549787  Why does Presidio with spacy nlp engine not re...   \n",
       "4        79548202  GPT-2 and other models from huggingface -100 l...   \n",
       "...           ...                                                ...   \n",
       "20437       42489  How to implement a \"related\" degree measure al...   \n",
       "20438       41424             How do you implement a \"Did you mean\"?   \n",
       "20439       36533     Vista speech recognition in multiple languages   \n",
       "20440       25332  What's a good natural language library to use ...   \n",
       "20441       23689        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                    Body         CreationDate  \\\n",
       "0      <p>So this is the log that I see on the termin...  2025-04-05 18:21:09   \n",
       "1      <p>I have created an aspect based list for ana...  2025-04-05 17:32:46   \n",
       "2      <p>When I use this model like so -</p>\\n<pre><...  2025-04-05 17:29:15   \n",
       "3      <p>I'm using spaCy with the pl_core_news_lg mo...  2025-04-02 05:56:11   \n",
       "4      <p>I understand the -100 label id is used so t...  2025-04-01 09:21:17   \n",
       "...                                                  ...                  ...   \n",
       "20437  <p>I was going to Ask a Question earlier today...  2008-09-03 20:21:04   \n",
       "20438  <blockquote>\\n  <p><strong>Possible Duplicate:...  2008-09-03 10:36:13   \n",
       "20439  <p>my primary language is spanish, but I use a...  2008-08-31 01:08:48   \n",
       "20440  <p>I'm looking for an existing library to summ...  2008-08-24 20:57:33   \n",
       "20441  <p>Does anyone know of a .NET date/time parser...  2008-08-22 22:45:10   \n",
       "\n",
       "       Score  ViewCount  AnswerCount  AcceptedAnswerId  \\\n",
       "0          0         20            0               NaN   \n",
       "1         -1         92            1               NaN   \n",
       "2          0         15            0               NaN   \n",
       "3          0         68            1        79552218.0   \n",
       "4          0         46            1        79551169.0   \n",
       "...      ...        ...          ...               ...   \n",
       "20437      8        456            2           42532.0   \n",
       "20438    118      33200           11           41448.0   \n",
       "20439      3       5661            6           36684.0   \n",
       "20440     14       6491            4               NaN   \n",
       "20441     27       6484            9          631134.0   \n",
       "\n",
       "                                      AcceptedAnswerBody  AcceptedAnswerScore  \n",
       "0                                                    NaN                  NaN  \n",
       "1                                                    NaN                  NaN  \n",
       "2                                                    NaN                  NaN  \n",
       "3      <p>The configuration file is missing the 'labe...                  1.0  \n",
       "4      <p>The author of the tutorial you mentioned se...                  1.0  \n",
       "...                                                  ...                  ...  \n",
       "20437  <p>One such way to implement such an algorithm...                  5.0  \n",
       "20438  <p>Actually what Google does is very much non-...                 87.0  \n",
       "20439  <p>Citation from Vista <a href=\"http://blogs.m...                  8.0  \n",
       "20440                                                NaN                  NaN  \n",
       "20441  <p>We developed exactly what you are looking f...                 12.0  \n",
       "\n",
       "[20442 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read file\n",
    "df_post = pd.read_csv(\"QueryResults_Post.csv\")\n",
    "df_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graphs using popular python libraries to visualise the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preporcess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to clean text, can remove html tag, some punctuation, non-ASCII, and intensifier\n",
    "\n",
    "intensifiers = {\n",
    "    \"very\", \"really\", \"extremely\", \"absolutely\", \"totally\", \"highly\", \"deeply\", \n",
    "    \"strongly\", \"incredibly\", \"exceptionally\", \"remarkably\", \"unbelievably\", \n",
    "    \"insanely\", \"awfully\", \"horribly\", \"hugely\", \"immensely\", \"overly\", \n",
    "    \"particularly\", \"significantly\", \"seriously\", \"tremendously\", \"wildly\",\n",
    "    \"super\", \"ultra\", \"crazy\", \"majorly\"\n",
    "}\n",
    "def clean_text(text,remove_code=True):\n",
    "\n",
    "    # Check if text is None, empty, or NaN\n",
    "    if text is None or text == \"\" or (isinstance(text, float) and np.isnan(text)):\n",
    "        return \"\"\n",
    "    # Convert to string if it's not already (handles numbers, etc.)\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Remove code blocks if requested\n",
    "    if remove_code:\n",
    "        for code in soup.find_all(['code', 'pre']):\n",
    "            code.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    # Clean up excessive whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Remove urls\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r'[\\\"\\'!?\\.;,:\\-\\(\\)]', '', text)\n",
    "\n",
    "    # Remove '@' character using regex\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    text = ''.join([char for char in text if ord(char) < 128])\n",
    "    text = re.sub(r'\\[|\\]', '', text)\n",
    "    \n",
    "    text = text.split()\n",
    "    text = [word for word in text if word.lower() not in intensifiers]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Return the cleaned text as a string\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function to help us extract the code part of the text out and then store it in another column\n",
    "def get_code(html_content):\n",
    "    # Handle None or empty content\n",
    "    if html_content is None or html_content == \"\" or (isinstance(html_content, float) and np.isnan(html_content)):\n",
    "        return \"\"  # Return empty string for consistency\n",
    "\n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract code blocks\n",
    "    code_blocks = []\n",
    "    for code in soup.find_all(['code', 'pre']):\n",
    "        code_text = code.get_text(strip=True)\n",
    "        if code_text:  # Only add non-empty code blocks\n",
    "            code_blocks.append(code_text)\n",
    "        # Remove code blocks from the soup to avoid duplication\n",
    "        code.decompose()\n",
    "    code_content = \"\\n---\\n\".join(code_blocks) if code_blocks else \"\"\n",
    "    return code_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    # Handle non-string inputs\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define function to lematize can work parallel'''\n",
    "# Lemmatization function\n",
    "def lemma_texts_parallel(texts): #code is help by AI - Reference (4)\n",
    "    texts = [str(text) for text in texts]\n",
    "    cleaned_texts_lemma = []\n",
    "    # Lemmatization with spaCy using parallel processing\n",
    "    for doc in nlp.pipe(texts, batch_size=50, n_process=4):  # Process in parallel\n",
    "        cleaned_tokens = [\n",
    "            token.lemma_ for token in doc\n",
    "        ]\n",
    "        cleaned_texts_lemma.append(\" \".join(cleaned_tokens))  # Join cleaned words back\n",
    "    \n",
    "    return pd.Series(cleaned_texts_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''parameter:\n",
    "- data will be a single dataframe columns where we want to combine all the text and provide wordcloud\n",
    "- Title will be the output name of wordcloud make sure it's meaningful'''\n",
    "def wc_generating(data,title):\n",
    "    #Define the model\n",
    "    stopwords = STOPWORDS\n",
    "    wc = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        height=600,\n",
    "        width=400\n",
    "    )\n",
    "    #Combine all text of each rows to big text\n",
    "    all_text = ' '.join(data.fillna(''))\n",
    "    wc.generate(all_text)\n",
    "    wc.to_file(f'{title}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define a function to removing some noise word\n",
    "This will return a new column in dataframe the text that remove defined noise word'''\n",
    "\n",
    "def remove_noise_word(text,noise_words):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    # Remove noise words\n",
    "    filtered = [word for word in words if word.lower() not in noise_words]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to keep post that has accepted answers\n",
    "df_post_answer = df_post.loc[\n",
    "    df_post['AcceptedAnswerBody'].notnull()\n",
    "].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['Question_Code'] = df_post_answer['Body'].apply(get_code)\n",
    "df_post_answer['Answer_Code'] = df_post_answer['AcceptedAnswerBody'].apply(get_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['Title_Clean'] = df_post_answer['Title'].apply(clean_text) \n",
    "df_post_answer['Body_Clean'] = df_post_answer['Body'].apply(clean_text)\n",
    "df_post_answer['AcceptedAnswerBody_Clean'] = df_post_answer['AcceptedAnswerBody'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['combination_text_all'] = df_post_answer['Title_Clean'] + \" \" + df_post_answer['Body_Clean'] + \" \" +  df_post_answer['AcceptedAnswerBody_Clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with remove stop word for all combination text\n",
    "df_post_answer['combination_text_all_no_stopw'] = df_post_answer['combination_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemma Text\n",
    "text = df_post_answer['combination_text_all_no_stopw'].tolist()\n",
    "lemma_text = lemma_texts_parallel(text)\n",
    "df_post_answer['combination_text_all_lemma'] = lemma_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerScore",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Question_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Answer_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerBody_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_no_stopw",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_clean",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f35bc482-c21f-483a-9b35-30ef1de0c88f",
       "rows": [
        [
         "0",
         "3",
         "79549787",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?",
         "<p>I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER):</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;pl_core_news_lg&quot;)\ntext = &quot;Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.&quot;\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n</code></pre>\n<p>Output:</p>\n<pre><code>[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n</code></pre>\n<p>However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities.</p>\n<pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=&quot;path_to_my_file/nlp_config.yaml&quot;) \nnlp_engine = provider.create_engine()\n\nprint(f&quot;Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}&quot;)\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[&quot;pl&quot;])\nregistry.load_predefined_recognizers([&quot;pl&quot;])\n\nprint(f&quot;Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}&quot;)\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, &quot;pl&quot;)\n\nfor entity in results:\n    print(f&quot;Found entity: {entity.entity_type} with score {entity.score}&quot;)\n</code></pre>\n<p>Output:</p>\n<pre><code>Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n</code></pre>\n<p>Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text.</p>\n<p>My config file:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n<p>Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?</p>\n",
         "2025-04-02 05:56:11",
         "0",
         "68",
         "1",
         "79552218.0",
         "<p>The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine :</p>\n<pre><code>  labels_to_ignore:\n    - O\n</code></pre>\n<p>On your configuration it would look like this:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n",
         "1.0",
         "import spacy\n\nnlp = spacy.load(\"pl_core_news_lg\")\ntext = \"Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.\"\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n---\n[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n---\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=\"path_to_my_file/nlp_config.yaml\") \nnlp_engine = provider.create_engine()\n\nprint(f\"Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}\")\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[\"pl\"])\nregistry.load_predefined_recognizers([\"pl\"])\n\nprint(f\"Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}\")\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, \"pl\")\n\nfor entity in results:\n    print(f\"Found entity: {entity.entity_type} with score {entity.score}\")\n---\nSupported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "labels_to_ignore:\n    - O\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does",
         "Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them",
         "The configuration file is missing the labels_to_ignore field stating that no entities should be ignored in the nlp engine On your configuration it would look like this",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them The configuration file is missing the labels_to_ignore field stating that no entities should be ignored in the nlp engine On your configuration it would look like this",
         "presidio spacy nlp engine recognize organizations pesel spacy im using spacy pl_core_news_lg model extract named entities polish text correctly detects organizations org peoples names per output however use presidio pl_core_news_lg model configuration file recognizers correctly detect organizations org pesel numbers even though appear list supported entities output even though organization pl_pesel listed org listed nlp engine supported recognizers presidio detect correctly text config file presidio fail detect organizations org pesel numbers pl_pesel spacy correctly detects configuration file missing labels_to_ignore field stating entities ignored nlp engine configuration would look like",
         "presidio spacy nlp engine recognize organization pesel spacy I m use spacy pl_core_news_lg model extract name entity polish text correctly detect organization org people name per output however use presidio pl_core_news_lg model configuration file recognizer correctly detect organization org pesel number even though appear list support entity output even though organization pl_pesel list org list nlp engine support recognizer presidio detect correctly text config file presidio fail detect organization org pesel number pl_pesel spacy correctly detect configuration file miss labels_to_ignore field state entity ignore nlp engine configuration would look like"
        ],
        [
         "1",
         "4",
         "79548202",
         "GPT-2 and other models from huggingface -100 label index for training, instead of pad token",
         "<p>I understand the -100 label id is used so that the predictions for these are not included when calculating the loss.</p>\n<p>However on <a href=\"https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16#bert2gpt2-summarization-with-%F0%9F%A4%97-encoderdecoder-framework\" rel=\"nofollow noreferrer\">huggingface</a>, they state\n&quot;complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not&quot;, when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument &quot;ignore_index&quot;.</p>\n<p>Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same?</p>\n<p>The way it is written makes me think there is some benefit, but the description of &quot;ignore_index&quot; appears to achieve what is wanted.</p>\n",
         "2025-04-01 09:21:17",
         "0",
         "46",
         "1",
         "79551169.0",
         "<p>The author of the tutorial you mentioned sets it to <code>-100</code> <strong>and</strong> uses <code>ignore_index</code> to save a few lines of code. You don't see the line where the author pass something to <code>ignore_index</code> because it has a default value. The default value of <code>ignore_index</code> for <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\" rel=\"nofollow noreferrer\">nn.CrossEntropyLoss</a> is <code>-100</code>. Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.</p>\n",
         "1.0",
         "",
         "-100\n---\nignore_index\n---\nignore_index\n---\nignore_index\n---\n-100",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token",
         "I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted",
         "The author of the tutorial you mentioned sets it to and uses to save a few lines of code You dont see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you dont have to pass the pad token id from tokenizer down to the loss function",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted The author of the tutorial you mentioned sets it to and uses to save a few lines of code You dont see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you dont have to pass the pad token id from tokenizer down to the loss function",
         "gpt2 models huggingface 100 label index training instead pad token understand 100 label id used predictions included calculating loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label excluded replacing pad tokens implementation use nncrossentropyloss argument ignore_index benefit changing id 100 opposed adding argument ignore_index loss setting pad token id results way written makes think benefit description ignore_index appears achieve wanted author tutorial mentioned sets uses save lines code dont see line author pass something default value default value nncrossentropyloss using value instead respective pad token id allows write model indepent training code dont pass pad token id tokenizer loss function",
         "gpt2 model huggingface 100 label index training instead pad token understand 100 label i d use prediction include calculate loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label exclude replace pad token implementation use nncrossentropyloss argument ignore_index benefit change i d 100 oppose add argument ignore_index loss set pad token i d result way write make think benefit description ignore_index appear achieve wanted author tutorial mention set use save line code do not see line author pass something default value default value nncrossentropyloss use value instead respective pad token i d allow write model indepent training code do not pass pad token i d tokenizer loss function"
        ],
        [
         "2",
         "12",
         "79523269",
         "Trouble getting importing gensim to work in colab",
         "<p>I am trying to import gensim into colab.</p>\n<pre><code>!pip install gensim\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--&gt; 367             msg = (&quot;The current Numpy installation ({!r}) fails to &quot;\n    368                    &quot;pass simple sanity checks. This can be caused for example &quot;\n    369                    &quot;by incorrect BLAS library being linked in, or by mixing &quot;\n\nModuleNotFoundError: No module named 'numpy.char'\n</code></pre>\n<p>my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks</p>\n",
         "2025-03-20 14:36:02",
         "0",
         "125",
         "1",
         "79523777.0",
         "<p>You have to restart the session for the underlying runtime to notice the package changes. See: <a href=\"https://stackoverflow.com/a/79518359/130288\">https://stackoverflow.com/a/79518359/130288</a></p>\n<p>I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded <code>numpy</code>/etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade.</p>\n<p>But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two.</p>\n<p>Explicitly restarting after the Gensim-install &amp; <code>numpy</code>/<code>scipy</code> downgrades resolves the errors.</p>\n",
         "1.0",
         "!pip install gensim\n---\n/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--> 367             msg = (\"The current Numpy installation ({!r}) fails to \"\n    368                    \"pass simple sanity checks. This can be caused for example \"\n    369                    \"by incorrect BLAS library being linked in, or by mixing \"\n\nModuleNotFoundError: No module named 'numpy.char'",
         "numpy\n---\nnumpy\n---\nscipy",
         "Trouble getting importing gensim to work in colab",
         "I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks",
         "You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadnt yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadnt yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "trouble getting importing gensim work colab trying import gensim colab get following error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string related issue thanks restart session underlying runtime notice package changes see recall past colab offering warning possibly also past colab hadnt yet loaded /etc fresh environment ok downgrade behind scenes without problem 1st import downgrade something changed colab recently maybe faststart optimization bunch reports problems like last day two explicitly restarting gensiminstall & / downgrades resolves errors",
         "trouble getting import gensim work colab try import gensim colab get follow error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string relate issue thank restart session underlie runtime notice package change see recall past colab offering warn possibly also past colab have not yet load /etc fresh environment ok downgrade behind scene without problem 1st import downgrade something change colab recently maybe faststart optimization bunch report problem like last day two explicitly restart gensiminstall & / downgrade resolve error"
        ],
        [
         "3",
         "21",
         "79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">site</a> in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?</p>\n",
         "2025-03-11 14:50:31",
         "0",
         "36",
         "1",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n",
         "1.0",
         "server\n---\nSSH\n---\nskip_tokens = [1]  # skip the special token for the start of the text <s>\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = \"playing guitar, hiking, and spending time with his family.\"\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)",
         "matplotlib\n---\nshow=True\n---\nfig, ax\n---\nimport matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(\"output.png\")\n#plt.show()  # eventually show it after saving\n---\nfig\n---\nfig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(\"output.png\")",
         "Store images instead of showing in a server",
         "I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them",
         "I cant test it but I checked source code and it uses for this If you remove then it shouldnt show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them I cant test it but I checked source code and it uses for this If you remove then it shouldnt show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "store images instead showing server running code found site server would like store images instead showing since connected remotely ssh connection via connection code instance one store files locally instead showing cant test checked source code uses remove shouldnt show get think could use matplotlibpyplotsavefigfilename save file probably also use",
         "store image instead show server run code find site server would like store image instead show since connect remotely ssh connection via connection code instance one store file locally instead show can not test check source code use remove should not show get think could use matplotlibpyplotsavefigfilename save file probably also use"
        ],
        [
         "4",
         "29",
         "79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03 22:27:07",
         "4",
         "230",
         "2",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n",
         "-2.0",
         "from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"pl\", \"model_name\": \"pl_core_news_lg\"}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                               languages_config=config)\n\ntext = \"Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\"\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(\"Anonymized text:\", anonymized_result)\nprint(\"Deanonymized text:\", deanonymized_result)\nprint(\"Map:\", anonymizer_tool.deanonymizer_mapping)\nprint(\"Anonymized text:\", anon_result)\n---\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\npython -m spacy download pl_core_news_lg\n---\nanonymizer_tool = PresidioReversibleAnonymizer()\n---\nAnonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n---\nnlp = spacy.load(\"pl_core_news_lg\")\ndoc = nlp(text)\n---\nJan Kowalski persName\nWarszawie placeName",
         "config = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": 'pl', \"model_name\": \"pl_core_news_lg\"}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=\"pl\",\n    supported_entities=[\"persName\"]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"CREDIT_CARD\"], languages_config=config)\n---\nAnonymized text: <persName> mieszka w Warszawie i ma e-mail glenn58@example.org.\n---\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\nMap: {'persName': {'<persName>': 'Jan Kowalski', '<persName_2>': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}\n---\nSpacyRecognizer\n---\nsupported_entities",
         "Presidio with Langchain Experimental does not detect Polish names",
         "I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected",
         "After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCys requirements I believe theres something missing or unclear in the documentation which is causing the misunderstanding",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCys requirements I believe theres something missing or unclear in the documentation which is causing the misunderstanding",
         "presidio langchain experimental detect polish names using presidio/langchain_experimental anonymize text polish detect names eg jan kowalski code output expected name jan kowalski email address anonymized output remains unchanged installed pl_core_news_lg model using missing something configuration presidio support polish entity recognition properly suggestions make detect names polish interesting thing use output look like mentioned use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio works expected test able find solution output look like need directly add formatted according spacys requirements believe theres something missing unclear documentation causing misunderstanding",
         "presidio langchain experimental detect polish name use presidio / langchain_experimental anonymize text polish detect name eg jan kowalski code output expect name jan kowalski email address anonymize output remain unchanged instal pl_core_news_lg model use miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing use output look like mention use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio work expect test able find solution output look like need directly add format accord spacys requirement believe there s something miss unclear documentation cause misunderstanding"
        ],
        [
         "5",
         "33",
         "79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22 16:06:11",
         "-1",
         "40",
         "1",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n",
         "1.0",
         "",
         "",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall",
         "Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isnt The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "OpenNLP POSTaggerME and ChunkerME synergy Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isnt The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "opennlp postaggerme chunkerme synergy im trying use opennlp chunking api chunk portuguese sentence first tokenized sentence using tokenizerme tagged postaggerme used readymade models provided project sentence ivo viu uva postaggerme returns tags propn verb det noun model seems using ud pos tags readymade model chunkerme portuguese followed instructions training first using chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything worked well sentence chunker produced correct tags bnp bvp bnp inp complex sentences hasnt produced good results trying identify could improve chunker training one things noticed difference types tags portuguese corpus bosque 80 seems using portuguese tags example instead propn corpus uses prop instead det uses art seems could lead problems especially since one parameters chunker receives array ud tags trained another type tag writing code creating routine convert portuguese notation ud penn wanted ask indeed impact tool already translation suggestions improving chunker precision/recall q1 yes chosen tag set ud penn custom impact conversion possible bidirectional manner penn > ud work well ud > penn good idea lossy conversion ud tag set less detailed compared classic penn tag set using custom language specific tagset work matter mapping from/to ud correctly might work tag sets languages others might complicated / lossy q2 isnt opennlp project takes code donations upcoming releases want provide mapping/translation pt lang q3 needs details/discussion apache opennlp user and/or dev mailing lists alternatively feel free open jira issue drill topic clear idea proposed code addition",
         "opennlp postaggerme chunkerme synergy I m try use opennlp chunk api chunk portuguese sentence first tokenized sentence use tokenizerme tag postaggerme use readymade model provide project sentence ivo viu uva postaggerme return tag propn verb det noun model seem use ud pos tag readymade model chunkerme portuguese follow instruction training first use chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything work well sentence chunker produce correct tag bnp bvp bnp inp complex sentence have not produce good result try identify could improve chunker training one thing notice difference type tag portuguese corpus bosque 80 seem use portuguese tag example instead propn corpus use prop instead det use art seem could lead problem especially since one parameter chunker receive array ud tag train another type tag write code create routine convert portuguese notation ud penn want ask indeed impact tool already translation suggestion improve chunker precision / recall q1 yes choose tag set ud penn custom impact conversion possible bidirectional manner penn > ud work well ud > penn good idea lossy conversion ud tag set less detailed compare classic penn tag set use custom language specific tagset work matter mapping from / to ud correctly might work tag set language other might complicated / lossy q2 be not opennlp project take code donation upcoming release want provide mapping / translation pt lang q3 need detail / discussion apache opennlp user and/or dev mailing list alternatively feel free open jira issue drill topic clear idea propose code addition"
        ],
        [
         "6",
         "36",
         "79451974",
         "word/ sentence similarities",
         "<p>I am trying to find if a given word/ set of words are similar to a definition.</p>\n<p>Example - Definition - &quot;vegetarian User&quot;</p>\n<p>Now, if I want to check a set of sentences like below</p>\n<pre><code>sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n</code></pre>\n<p>I tried using some sentence transformer like below</p>\n<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n</code></pre>\n<p>But this is not giving me expected results.</p>\n<p>What is the best approach to achieve results like below?</p>\n<pre><code>[False,True,True,False]\n</code></pre>\n<p>Is it doable with nlp/ some other technique?</p>\n",
         "2025-02-19 15:47:45",
         "1",
         "50",
         "1",
         "79461281.0",
         "<p>Yes, it’s definitely doable using NLP! The key here is that you don’t need a full similarity matrix; you want to check if each sentence is semantically similar to the given definition.</p>\n<p>✅ Better Approach:\nEncode both the definition and sentences using a sentence transformer.\nCompute cosine similarity between the definition embedding and each sentence embedding.\nSet a threshold (e.g., 0.6 or 0.7) to determine if they are &quot;similar enough.&quot;</p>\n<pre><code>from sentence_transformers import SentenceTransformer, util\n# Load the pre-trained model\nmodel = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\n\n# Definition and sentences\ndefinition = &quot;vegetarian User&quot;\nsentences = [\n  'vegetarian User',\n  'user sometimes eats chicken',\n  'user is vegetarian',\n  'user only eats fruits',\n  'user likes fish'\n]\n\n# Encode the definition and sentences\ndefinition_embedding = model.encode(definition, convert_to_tensor=True)\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine similarities\nsimilarities = util.cos_sim(definition_embedding, sentence_embeddings)[0]\n\n# Set a threshold for similarity (tune this value as needed)\nthreshold = 0.6\nresults = [sim &gt;= threshold for sim in similarities]\n\n# Print results\nprint(results)  # Example output: [True, False, True, False, False]\n</code></pre>\n<p>💡 Explanation:\nutil.cos_sim computes the cosine similarity between the definition and each sentence.\nThreshold tuning:\nIf the similarity is above the threshold, consider it True.\nAdjust the threshold based on how strict you want the matching.</p>\n<p>🔍 Why the original approach didn’t work:\nmodel.similarity doesn’t exist in the SentenceTransformers API.\nYou were computing a sentence-to-sentence matrix, not definition-to-sentence comparisons.</p>\n",
         "1.0",
         "sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n---\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n---\n[False,True,True,False]",
         "from sentence_transformers import SentenceTransformer, util\n# Load the pre-trained model\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\n\n# Definition and sentences\ndefinition = \"vegetarian User\"\nsentences = [\n  'vegetarian User',\n  'user sometimes eats chicken',\n  'user is vegetarian',\n  'user only eats fruits',\n  'user likes fish'\n]\n\n# Encode the definition and sentences\ndefinition_embedding = model.encode(definition, convert_to_tensor=True)\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine similarities\nsimilarities = util.cos_sim(definition_embedding, sentence_embeddings)[0]\n\n# Set a threshold for similarity (tune this value as needed)\nthreshold = 0.6\nresults = [sim >= threshold for sim in similarities]\n\n# Print results\nprint(results)  # Example output: [True, False, True, False, False]",
         "word/ sentence similarities",
         "I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique",
         "Yes its definitely doable using NLP The key here is that you dont need a full similarity matrix you want to check if each sentence is semantically similar to the given definition Better Approach Encode both the definition and sentences using a sentence transformer Compute cosine similarity between the definition embedding and each sentence embedding Set a threshold eg 06 or 07 to determine if they are similar enough Explanation utilcos_sim computes the cosine similarity between the definition and each sentence Threshold tuning If the similarity is above the threshold consider it True Adjust the threshold based on how strict you want the matching Why the original approach didnt work modelsimilarity doesnt exist in the SentenceTransformers API You were computing a sentencetosentence matrix not definitiontosentence comparisons",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique Yes its definitely doable using NLP The key here is that you dont need a full similarity matrix you want to check if each sentence is semantically similar to the given definition Better Approach Encode both the definition and sentences using a sentence transformer Compute cosine similarity between the definition embedding and each sentence embedding Set a threshold eg 06 or 07 to determine if they are similar enough Explanation utilcos_sim computes the cosine similarity between the definition and each sentence Threshold tuning If the similarity is above the threshold consider it True Adjust the threshold based on how strict you want the matching Why the original approach didnt work modelsimilarity doesnt exist in the SentenceTransformers API You were computing a sentencetosentence matrix not definitiontosentence comparisons",
         "word/ sentence similarities trying find given word/ set words similar definition example definition vegetarian user want check set sentences like tried using sentence transformer like giving expected results best approach achieve results like doable nlp/ technique yes definitely doable using nlp key dont need full similarity matrix want check sentence semantically similar given definition better approach encode definition sentences using sentence transformer compute cosine similarity definition embedding sentence embedding set threshold eg 06 07 determine similar enough explanation utilcos_sim computes cosine similarity definition sentence threshold tuning similarity threshold consider true adjust threshold based strict want matching original approach didnt work modelsimilarity doesnt exist sentencetransformers api computing sentencetosentence matrix definitiontosentence comparisons",
         "word/ sentence similarity try find give word/ set word similar definition example definition vegetarian user want check set sentence like try use sentence transformer like give expect result good approach achieve result like doable nlp/ technique yes definitely doable use nlp key do not need full similarity matrix want check sentence semantically similar give definition well approach encode definition sentence use sentence transformer compute cosine similarity definition embed sentence embed set threshold eg 06 07 determine similar enough explanation utilcos_sim compute cosine similarity definition sentence threshold tuning similarity threshold consider true adjust threshold base strict want match original approach do not work modelsimilarity do not exist sentencetransformer api compute sentencetosentence matrix definitiontosentence comparison"
        ],
        [
         "7",
         "42",
         "79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07 02:48:25",
         "-1",
         "45",
         "1",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n",
         "0.0",
         "lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()",
         "",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged",
         "I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue",
         "Based on extra information in the comments Im going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort Im fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldnt find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they dont handle multiple meanings of the word So queen might be a female king as in embeddings party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers dont have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue Based on extra information in the comments Im going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort Im fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldnt find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they dont handle multiple meanings of the word So queen might be a female king as in embeddings party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers dont have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "underfitting pretrained glove + lstm model accurcacy unchanged sentiment classification using pretrained glove lstm model use google play review scrap resulting 50k++ texts implement random sampling minority classes however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embedding size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set padded 140997 200 total x validation set padded 17625 200 total x testing set padded 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue based extra information comments im going say reason lstm model hits wall unspecified lower accuracy 85 % trying reach best type model problem case tweaking parameters likely wasted effort im fairly sure encoder transformers eg bert surpassed sentiment analysis benchmarks number years back sorry quick search couldnt find killer reference insert transformers got bigger better since extra thought building top glove embeddings presents problem dont handle multiple meanings word queen might female king embeddings party trick king male + female = queen might pop group might gay man might chess piece going put limit accuracy models built whereas transformers dont limitation look whole string see words context possible argue course bringing context lstm comes transformers still scaling 20+ layers whereas lstms tend choke two layers",
         "underfitte pretraine glove + lstm model accurcacy unchanged sentiment classification use pretraine glove lstm model use google play review scrap result 50k++ text implement random sampling minority class however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue base extra information comment I m go say reason lstm model hit wall unspecified low accuracy 85 % try reach good type model problem case tweak parameter likely waste effort I m fairly sure encoder transformer eg bert surpass sentiment analysis benchmark number year back sorry quick search could not find killer reference insert transformer get big well since extra thought build top glove embedding present problem do not handle multiple meaning word queen might female king embedding party trick king male + female = queen might pop group might gay man might chess piece going put limit accuracy model build whereas transformer do not limitation look whole string see word context possible argue course bring context lstm come transformer still scale 20 + layer whereas lstms tend choke two layer"
        ],
        [
         "8",
         "70",
         "79330283",
         "Can't compile Marian NMT",
         "<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=\"https://marian-nmt.github.io/docs/#installation\" rel=\"nofollow noreferrer\">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>\n<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>\n<p>These are the steps that I followed:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n</code></pre>\n<p>This is the result I had:</p>\n<pre><code>➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2\n</code></pre>\n<p>Please help.</p>\n",
         "2025-01-05 06:04:59",
         "4",
         "68",
         "1",
         "79332711.0",
         "<p>The diagnostic that your build is tripping, <code>Wtemplate-id-cdtor</code>, was introduced\nwith GCC 14.1. It is a warning, not an error, but your build promotes all warnings to\nerrors, so it breaks your build.</p>\n<p>Although your build specifies <code>-std=c++11</code> in <code>src/3rd_party/spdlog/CMakeLists.txt</code>, which\ngenerates the failure, g++-14 emits <code>Wtemplate-id-cdtor</code> to warn you that the code <em>would be</em>\nillegal under the more recent standard c++20 (and later). Then the warning is made an error.</p>\n<p>The warning is made an error by the compile option <code>-Werror</code>. This option is included in the list\nof compile options <code>ALL_WARNINGS</code>, which is created in the top-level <code>marian/CMakeLists.txt</code>\nat line 227 <em>et seq</em>:</p>\n<pre><code># These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n</code></pre>\n<p>and then applied as compile options for the <code>marian</code> library target in <code>src/CMakeLists.txt</code>\nat line 133:</p>\n<pre><code>target_compile_options(marian PRIVATE ${ALL_WARNINGS})\n</code></pre>\n<p>whence the options are operative for the failing compilation of <code>src/CMakeFiles/marian.dir/common/logging.cpp</code>.</p>\n<p>This failure is a bug in the <code>marian</code> repo which you should <a href=\"https://github.com/marian-nmt/marian/issues\" rel=\"nofollow noreferrer\">report to the maintainers</a>, as\nit does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14.</p>\n<p>Pending a fix, you seem to have three interim options to get your build done. Either:</p>\n<ul>\n<li><p>Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:</p>\n<pre><code>/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\n</code></pre>\n</li>\n</ul>\n<p>e.g. make it <code>registry_t(const registry_t&lt;Mutex&gt;&amp;) = delete;</code> in this occurrence.</p>\n<p>Or:</p>\n<ul>\n<li><p>Locally disable <code>-Wtemplate-id-cdtor</code> at each occurrence, e.g:</p>\n<pre><code>#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored &quot;-Wtemplate-id-cdtor&quot;\nregistry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n#pragma GCC diagnostic pop\n</code></pre>\n</li>\n</ul>\n<p>Or:</p>\n<ul>\n<li>Remove <code>-Werror</code> from the <code>ALL_WARNINGS</code> list in <code>marian/CMakeLists.txt</code> so that <code>Wtemplate-id-cdtor</code> remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status).</li>\n</ul>\n<p>I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.</p>\n",
         "4.0",
         "CMakeLists.txt\n---\nset (CMAKE_CXX_STANDARD 11)\n---\ngit clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n---\n➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2",
         "Wtemplate-id-cdtor\n---\n-std=c++11\n---\nsrc/3rd_party/spdlog/CMakeLists.txt\n---\nWtemplate-id-cdtor\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\n# These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n---\nmarian\n---\nsrc/CMakeLists.txt\n---\ntarget_compile_options(marian PRIVATE ${ALL_WARNINGS})\n---\nsrc/CMakeFiles/marian.dir/common/logging.cpp\n---\nmarian\n---\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\n---\nregistry_t(const registry_t<Mutex>&) = delete;\n---\n-Wtemplate-id-cdtor\n---\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wtemplate-id-cdtor\"\nregistry_t<Mutex>(const registry_t<Mutex>&) = delete;\n#pragma GCC diagnostic pop\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\nWtemplate-id-cdtor",
         "Cant compile Marian NMT",
         "Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help",
         "The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I havent tested any of these options as Id need to go to the trouble of installing CUDA",
         "Cant compile Marian NMT Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I havent tested any of these options as Id need to go to the trouble of installing CUDA",
         "cant compile marian nmt im using endeavouros im trying compile marian instructions fails error message seemingly indicates conflict code c++20 files repo line steps followed result please help diagnostic build tripping introduced gcc 141 warning error build promotes warnings errors breaks build although build specifies generates failure g++14 emits warn code would illegal recent standard c++20 later warning made error warning made error compile option option included list compile options created toplevel line 227 et seq applied compile options library target line 133 whence options operative failing compilation failure bug repo report maintainers seem reported already head revision v1120 year older gcc 14 pending fix seem three interim options get build done either make code legal c++11 c++20 diagnostic advice says occurrence eg make occurrence locally disable occurrence eg remove list remains warning may result diagnostics demoted errors warnings default status havent tested options id need go trouble installing cuda",
         "can not compile marian nmt I m use endeavouros I m try compile marian instruction fail error message seemingly indicate conflict code c++20 file repo line step follow result please help diagnostic build tripping introduce gcc 141 warning error build promote warning error break build although build specifie generate failure g++14 emit warn code would illegal recent standard c++20 later warn make error warning make error compile option option include list compile option create toplevel line 227 et seq apply compile option library target line 133 whence option operative failing compilation failure bug repo report maintainer seem report already head revision v1120 year old gcc 14 pende fix seem three interim option get build do either make code legal c++11 c++20 diagnostic advice say occurrence eg make occurrence locally disable occurrence eg remove list remains warn may result diagnostic demote error warning default status have not test option i d need go trouble instal cuda"
        ],
        [
         "9",
         "71",
         "79328514",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )\n</code></pre>\n<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>\n",
         "2025-01-04 08:57:44",
         "2",
         "34",
         "1",
         "79328698.0",
         "<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>\n<p>To modify the <strong>data collator</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>class CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])\n        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])\n        return batch\n</code></pre>\n<p>then pass it to <code>Trainer</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n</code></pre>\n<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>\n<p>Hope, it will work!</p>\n",
         "1.0",
         "input_ids\n---\nlabels\n---\ninteract_ids\n---\ncandidate_ids\n---\nclass LLMWithCustomLayer(LlamaForCausalLM)\n---\ndef forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )",
         "interact_ids\n---\ncandidate_ids\n---\nclass CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[\"interact_ids\"] = torch.tensor([f[\"interact_ids\"] for f in features])\n        batch[\"candidate_ids\"] = torch.tensor([f[\"candidate_ids\"] for f in features])\n        return batch\n---\nTrainer\n---\ntrainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(\"your-llama-model\"),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n---\nforward()\n---\ninteract_ids\n---\ncandidate_ids",
         "how to get custom column in the models forward function when training with Huggingface Trainer",
         "I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much",
         "You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "how to get custom column in the models forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "get custom column models forward function training huggingface trainer using huggingface trainer train cumstom model subclassing llama llm tokenized tokenizer dataset fields additionally add 2 custom colunms cant get custom fields forward function model new llm fine tuning anyone help would grateful much need modify data collator pass model trainer ignores extra columns default modify data collator pass method receive hope work",
         "get custom column model forward function training huggingface trainer use huggingface trainer train cumstom model subclasse llama llm tokenized tokenizer dataset field additionally add 2 custom colunms can not get custom field forward function model new llm fine tuning anyone help would grateful much need modify datum collator pass model trainer ignore extra column default modify datum collator pass method receive hope work"
        ],
        [
         "10",
         "75",
         "79312133",
         "Getting all leaf words (reverse stemming) into one Python List",
         "<p>On the same lines as the solution provided <a href=\"https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword\">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>\n<p>Imagine I have a shorter sample word list as follows:</p>\n<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n</code></pre>\n<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>\n<pre><code>get_word_forms(&quot;command&quot;)\n</code></pre>\n<p>and get the following output:</p>\n<pre><code>{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n</code></pre>\n<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>\n<p>If I try to reverse-stem the entire list in one go:</p>\n<pre><code>[get_word_forms(word) for word in sample]\n</code></pre>\n<p>I fail at getting any output:</p>\n<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n</code></pre>\n<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>\n<p>something like:</p>\n<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. \n</code></pre>\n",
         "2024-12-27 15:04:05",
         "1",
         "47",
         "1",
         "79312987.0",
         "<p>One solution using nested list comprehensions after stripping forgotten spaces:</p>\n<pre><code>all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']\n</code></pre>\n",
         "1.0",
         "get_word_forms\n---\nmy_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n---\nget_word_forms(\"command\")\n---\n{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n---\n[get_word_forms(word) for word in sample]\n---\n[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n---\n['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.",
         "all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']",
         "Getting all leaf words reverse stemming into one Python List",
         "On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like",
         "One solution using nested list comprehensions after stripping forgotten spaces",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like One solution using nested list comprehensions after stripping forgotten spaces",
         "getting leaf words reverse stemming one python list lines solution provided link trying get leaf words one stem word using communitycontributed srivastava package imagine shorter sample word list follows work manually following go wordbyword timeconsuming list 200 words get following output n noun adjective v verb r adverb try reversestem entire list one go fail getting output think failing saving output dictionary eventually would like output list without breaking noun adjective adverb verb something like one solution using nested list comprehensions stripping forgotten spaces",
         "get leaf word reverse stem one python list line solution provide link try get leaf word one stem word use communitycontributed srivastava package imagine short sample word list follow work manually follow go wordbyword timeconsuming list 200 word get follow output n noun adjective v verb r adverb try reversestem entire list one go fail get output think fail save output dictionary eventually would like output list without break noun adjective adverb verb something like one solution use nested list comprehension strip forget space"
        ],
        [
         "11",
         "80",
         "79298368",
         "Inspect all probabilities of BERTopic model",
         "<p>Say I build a BERTopic model using</p>\n<pre><code>from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre>\n<p>Inspecting <code>probs</code> gives me just a single value for each item in <code>docs</code>.</p>\n<pre><code>probs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n</code></pre>\n<p>I would like the entire probability vector across all topics (so in this case, where <code>nr_topics=20</code>, I want a vector of 20 probabilities for each item in <code>docs</code>). In other words, if I have N items in <code>docs</code> and K topics, I would like an NxK output.</p>\n",
         "2024-12-20 20:49:34",
         "1",
         "52",
         "1",
         "79299703.0",
         "<p>For individual topic probability across each document you need to add one more argument.</p>\n<pre><code>topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n</code></pre>\n<p>Note: This calculate_probabilities = True will only work if you are using <strong><code>HDBSCAN</code></strong> clustering embedding model. And Bertopic by default uses <code>all-MiniLM-L6-v2</code>.</p>\n<p><strong>Official documentation:</strong> <a href=\"https://maartengr.github.io/BERTopic/api/bertopic.html\" rel=\"nofollow noreferrer\">https://maartengr.github.io/BERTopic/api/bertopic.html</a></p>\n<p>They have mentioned the same in document as well.</p>\n",
         "1.0",
         "from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n---\nprobs\n---\ndocs\n---\nprobs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n---\nnr_topics=20\n---\ndocs\n---\ndocs",
         "topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n---\nHDBSCAN\n---\nall-MiniLM-L6-v2",
         "Inspect all probabilities of BERTopic model",
         "Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output",
         "For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "inspect probabilities bertopic model say build bertopic model using inspecting gives single value item would like entire probability vector across topics case want vector 20 probabilities item words n items k topics would like nxk output individual topic probability across document need add one argument note calculate_probabilities = true work using clustering embedding model bertopic default uses official documentation mentioned document well",
         "inspect probability bertopic model say build bertopic model use inspecting give single value item would like entire probability vector across topic case want vector 20 probability item word n item k topic would like nxk output individual topic probability across document need add one argument note calculate_probabilitie = true work use cluster embed model bertopic default use official documentation mention document well"
        ],
        [
         "12",
         "82",
         "79293919",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>\n<p>For example:\nA sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>\n<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>\n<p>Any and all help is welcome and appreciated.</p>\n<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>\n",
         "2024-12-19 10:24:04",
         "0",
         "63",
         "2",
         "79294074.0",
         "<p>You need a external dataset for this task. You can try dataset such as google n gram dataset.</p>\n<p>Here is the breakdown of the problem statement:</p>\n<ol>\n<li>Input: &quot;I enjoy a cold glass of water on a hot day&quot;. <code>Output</code>: &quot;water&quot;.</li>\n<li>Split the sentences into words list.</li>\n</ol>\n<blockquote>\n<p>Example: [&quot;I&quot;, &quot;enjoy&quot;, &quot;a&quot;, &quot;cold&quot;, &quot;glass&quot;, &quot;of&quot;, &quot;water&quot;, &quot;on&quot;,\n&quot;a&quot;, &quot;hot&quot;, &quot;day&quot;]</p>\n</blockquote>\n<ol start=\"3\">\n<li>First loop in through all the word of the sentences. so let say you are at first word &quot;I&quot;.</li>\n<li>Now you will look the same word &quot;I&quot; in external dataset and will look for the frequency of that word.\nLet say the word &quot;I&quot; in external dataset is repeated <code>5000000</code> times</li>\n<li>Repeat this task for all the word.</li>\n<li>Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.\nFrequency in the below example is random value not exact value.</li>\n</ol>\n<blockquote>\n<pre><code>{\n    &quot;I&quot;: 5000000,\n    &quot;enjoy&quot;: 50000,\n    &quot;a&quot;: 10000000,\n    &quot;cold&quot;: 30000,\n    &quot;glass&quot;: 100000,\n    &quot;of&quot;: 8000000,\n    &quot;water&quot;: 1200000,\n    &quot;on&quot;: 6000000,\n    &quot;hot&quot;: 700000,\n    &quot;day&quot;: 400000\n}\n</code></pre>\n</blockquote>\n<ol start=\"7\">\n<li>Pick the word with highest frequency.</li>\n</ol>\n<p>Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself</p>\n",
         "2.0",
         "",
         "Output\n---\n5000000\n---\n{\n    \"I\": 5000000,\n    \"enjoy\": 50000,\n    \"a\": 10000000,\n    \"cold\": 30000,\n    \"glass\": 100000,\n    \"of\": 8000000,\n    \"water\": 1200000,\n    \"on\": 6000000,\n    \"hot\": 700000,\n    \"day\": 400000\n}",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess",
         "You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "determining popular words english dictionary within dictionary words forgive wording awful im trying figure determine used words english language set words dictionary ive made ive done research nltk cant seem find function within library matter help need example sentence enjoy cold glass water hot day would return water used word day day conversation sentence essentially need returned value frequently used word conversations figure ill likely involve ai time ive tried use ai wind copy pasting code dont understand im trying avoid going route help welcome appreciated context decided start project would essentially guess predetermined word based characters user says doesnt computers guess need external dataset task try dataset google n gram dataset breakdown problem statement input enjoy cold glass water hot day water split sentences words list example enjoy cold glass water hot day first loop word sentences let say first word look word external dataset look frequency word let say word external dataset repeated times repeat task word dictionary word sentence key value frequency word get external data frequency example random value exact value pick word highest frequency note try big corpus external data using big corpus english word used conversation even frequency mentioned create",
         "determine popular word english dictionary within dictionary word forgive word awful I m try figure determine use word english language set word dictionary I ve make I ve do research nltk can not seem find function within library matter help need example sentence enjoy cold glass water hot day would return water use word day day conversation sentence essentially need return value frequently use word conversation figure ill likely involve ai time I ve try use ai wind copy paste code do not understand I m try avoid go route help welcome appreciated context decide start project would essentially guess predetermine word base character user say do not computer guess need external dataset task try dataset google n gram dataset breakdown problem statement input enjoy cold glass water hot day water split sentence word list example enjoy cold glass water hot day first loop word sentence let say first word look word external dataset look frequency word let say word external dataset repeat time repeat task word dictionary word sentence key value frequency word get external data frequency example random value exact value pick word high frequency note try big corpus external datum use big corpus english word use conversation even frequency mention create"
        ],
        [
         "13",
         "83",
         "79293889",
         "catelog sentences into 5 words that represent them",
         "<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>\n<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>\n<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>\n<p>I will glad for recomendations how to do that</p>\n<p><strong>edit</strong></p>\n<p>represnt = the semantic distance between the word to the text.</p>\n<p>for example -\nlets say in row 1 the text is &quot;i want to eat&quot;\nand I have 2 words : food and house.</p>\n<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>\n",
         "2024-12-19 10:16:47",
         "0",
         "54",
         "1",
         "79294099.0",
         "<p>You could use a pre-trained sentence transformer model from <a href=\"https://pypi.org/project/sentence-transformers/\" rel=\"nofollow noreferrer\"><code>sentence_transformers</code></a>:</p>\n<pre><code>import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -&gt; None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -&gt; list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -&gt; pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838\n</code></pre>\n",
         "0.0",
         "df['text']\n---\ndf[\"word1\"]\n---\ndf[\"word2\"]\n---\ndf[\"food \"]\n---\ndf[\"house\"]",
         "sentence_transformers\n---\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -> None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -> list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -> pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n---\ntext  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838",
         "catelog sentences into 5 words that represent them",
         "I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in",
         "You could use a pretrained sentence transformer model from Output",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in You could use a pretrained sentence transformer model from Output",
         "catelog sentences 5 words represent dataframe 1000 text rows also 5 words want know one much represnt text 0 1 every score etc glad recomendations edit represnt = semantic distance word text example lets say row 1 text want eat 2 words food house would higher score could use pretrained sentence transformer model output",
         "catelog sentence 5 word represent dataframe 1000 text row also 5 word want know one much represnt text 0 1 every score etc glad recomendation edit represnt = semantic distance word text example let say row 1 text want eat 2 word food house would high score could use pretraine sentence transformer model output"
        ],
        [
         "14",
         "92",
         "79253283",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>\n<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>\n<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n</code></pre>\n<p>A small Example -</p>\n<pre><code>text_dict = {\n    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]\n}\n\ndf = pd.DataFrame(text_dict)\n</code></pre>\n<p>My expected outcome is -</p>\n<pre><code>                 ITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n</code></pre>\n<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>\n<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>\n",
         "2024-12-05 03:05:06",
         "0",
         "81",
         "1",
         "79263000.0",
         "<p>You need to process each row of text by identifying occurrences of <code>key_words</code> and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique <code>search_word</code> found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique <code>search_words</code> near <code>key_words</code>.</p>\n<pre><code>import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        &quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process &quot;\n        &quot;of adopting Symantec. We believe these technologies will improve our access control. &quot;\n        &quot;Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = &quot; &quot;.join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n</code></pre>\n<p>Which returns</p>\n<pre><code>                                               ITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2\n</code></pre>\n",
         "0.0",
         "search words\n---\nkey words\n---\nsearch words\n---\nkey words\n---\nkey words\n---\nsearch words\n---\nsearch word\n---\nkey word\n---\nsearch_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n---\ntext_dict = {\n    'ITEM7':[\"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"]\n}\n\ndf = pd.DataFrame(text_dict)\n---\nITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n---\ndf\n---\nAVG\n---\nAlibaba Security\n---\nsearch_words\n---\nkey_words\n---\nSymantec\n---\nAccess Control\n---\ndata security\n---\ndiagnostic program\n---\nsearch_words\n---\nadopting\n---\nimprove\n---\nintegrated\n---\nkey_words\n---\nFrequency\n---\ndf\n---\nkey_words",
         "key_words\n---\nsearch_word\n---\nsearch_words\n---\nkey_words\n---\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        \"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \"\n        \"of adopting Symantec. We believe these technologies will improve our access control. \"\n        \"Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = \" \".join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n---\nITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also",
         "You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "counting frequency words within key words text two sets word lists first one called second one called goal calculate frequency within 10 words example assume word acquire list look words list within 10 words acquire within 10 words mean 10 words forward key words 10 words backward key words meaning forward backward movement lists small example expected outcome first row see word list around word acquired base form acquire list similarly list words within 10 words list total search words 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note words basically base form variation like adopted adopting counted key words also need process row text identifying occurrences capturing 10word window around within window need check multiword search_words ensuring matched phrases unique found within windows needs counted avoiding doublecounting across row stored results frequency count row accurately reflecting number unique near returns",
         "count frequency word within key word text two set word list first one call second one call goal calculate frequency within 10 word example assume word acquire list look word list within 10 word acquire within 10 word mean 10 word forward key word 10 word backward key word mean forward backward movement list small example expect outcome first row see word list around word acquire base form acquire list similarly list word within 10 word list total search word 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note word basically base form variation like adopt adopt count key word also need process row text identify occurrence capture 10word window around within window need check multiword search_words ensure match phrase unique find within window need count avoid doublecounte across row store result frequency count row accurately reflect number unique near return"
        ],
        [
         "15",
         "94",
         "79247672",
         "Error in getting Captum text explanations for text classification",
         "<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>\n<pre><code>import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f&quot;Test Accuracy: {accuracy:.2f}&quot;)\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n<p>But I am getting the following error in running the above.</p>\n<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n</code></pre>\n",
         "2024-12-03 12:47:45",
         "2",
         "84",
         "1",
         "79248379.0",
         "<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>\n<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=\"https://captum.ai/tutorials/Bert_SQUAD_Interpret\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>\n<p>Below please find snippet that works:</p>\n<pre><code>from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n",
         "1.0",
         "import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)\n---\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-f047b509c98d> in <cell line: 90>()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
         "from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    # print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)",
         "Error in getting Captum text explanations for text classification",
         "I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above",
         "You need to slightly change the gradients calculation class Also you didnt include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above You need to slightly change the gradients calculation class Also you didnt include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "error getting captum text explanations text classification following code using identify influential words used correctly predict text test dataset getting following error running need slightly change gradients calculation class also didnt include forward_func gradients class constructor attribute method able launch stuff properly think using layerintegratedgradients better debugging bert line tutorial please find snippet works",
         "error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset get follow error run need slightly change gradient calculation class also do not include forward_func gradient class constructor attribute method able launch stuff properly think use layerintegratedgradient well debug bert line tutorial please find snippet work"
        ],
        [
         "16",
         "95",
         "79247594",
         "euclidian distance from word to sentence after doing Vectorizer",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did TfidfVectorizer.</p>\n<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>\n<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n",
         "2024-12-03 12:25:05",
         "1",
         "44",
         "1",
         "79248087.0",
         "<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([&quot;king&quot;]).toarray()\nexcept KeyError:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n</code></pre>\n<p>which would give you</p>\n<pre><code>                                                text  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0  \n</code></pre>\n<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif &quot;king&quot; in feature_names:\n    king_index = np.where(feature_names == &quot;king&quot;)[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n\n</code></pre>\n<p>which gives</p>\n<pre><code>                                                text  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214\n</code></pre>\n",
         "1.0",
         "",
         "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([\"king\"]).toarray()\nexcept KeyError:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n---\ntext  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0\n---\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif \"king\" in feature_names:\n    king_index = np.where(feature_names == \"king\")[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n---\ntext  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214",
         "euclidian distance from word to sentence after doing Vectorizer",
         "I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method",
         "I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance word sentence vectorizer dataframe 1000 text rows tfidfvectorizer want create new field give distance sentence word want lets say word king dfking thought taking sentence 5 closet words word king make average glad know hear another method convinced euclidean distance would optimal measure would actually look similarity scores would give said want focus euclidean distance method gives",
         "euclidian distance word sentence vectorizer dataframe 1000 text row tfidfvectorizer want create new field give distance sentence word want let say word king dfke think take sentence 5 closet word word king make average glad know hear another method convince euclidean distance would optimal measure would actually look similarity score would give say want focus euclidean distance method give"
        ],
        [
         "17",
         "96",
         "79234004",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "<p>I want to use <code>Llama-3.2-1B-Instruct</code> model, and although I have set <code>&quot;temperature&quot;: 0.0, &quot;top_p&quot;:0.0 and &quot;top_k&quot;:0</code>, it still generates inconsistent output. This is how my pipeline looks like:</p>\n<pre><code>pipe = pipeline(\n    &quot;text-generation&quot;,\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=&quot;mps&quot;,\n        model_kwargs={&quot;temperature&quot;: 0.0,\n                  &quot;do_sample&quot;:True,\n                              &quot;top_p&quot;:0.0,\n                              &quot;top_k&quot;:0,},\n)\n</code></pre>\n<p>Any idea how to solve this issue?</p>\n",
         "2024-11-28 13:02:37",
         "1",
         "641",
         "2",
         "79246602.0",
         "<p>The model inconsistent output can be due to two main factors:</p>\n<p><strong>1. Temperature:</strong></p>\n<p>setting temperature to zero give more inconsistent result. You can refer <a href=\"https://community.openai.com/t/why-the-api-output-is-inconsistent-even-after-the-temperature-is-set-to-0/329541/2\" rel=\"nofollow noreferrer\">Opeani discussion page</a> for detail.</p>\n<p>So the best option is to set temperature to very low values such as 0.00001 instead of zero.</p>\n<p><strong>2. do_sample</strong></p>\n<p>You already set it false, and it should remain that way only.</p>\n",
         "1.0",
         "Llama-3.2-1B-Instruct\n---\n\"temperature\": 0.0, \"top_p\":0.0 and \"top_k\":0\n---\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"mps\",\n        model_kwargs={\"temperature\": 0.0,\n                  \"do_sample\":True,\n                              \"top_p\":0.0,\n                              \"top_k\":0,},\n)",
         "",
         "Llama321BInstruct generate inconsistent output",
         "I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue",
         "The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "llama321binstruct generate inconsistent output want use model although set still generates inconsistent output pipeline looks like idea solve issue model inconsistent output due two main factors 1 temperature setting temperature zero give inconsistent result refer opeani discussion page detail best option set temperature low values 000001 instead zero 2 do_sample already set false remain way",
         "llama321binstruct generate inconsistent output want use model although set still generate inconsistent output pipeline look like idea solve issue model inconsistent output due two main factor 1 temperature set temperature zero give inconsistent result refer opeani discussion page detail good option set temperature low value 000001 instead zero 2 do_sample already set false remain way"
        ],
        [
         "18",
         "109",
         "79192130",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>\n<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>\n",
         "2024-11-15 11:13:36",
         "1",
         "56",
         "2",
         "79192427.0",
         "<p>In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:</p>\n<ol>\n<li>Package your KeyBERT script in a lambda and easily deploy it with a container.</li>\n<li>Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.</li>\n</ol>\n",
         "1.0",
         "",
         "",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT",
         "I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks",
         "In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model Im using is resourceintensive and requires GPU acceleration Id go with SageMaker otherwise Lambda is a good solution So for your case heres what Id do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model Im using is resourceintensive and requires GPU acceleration Id go with SageMaker otherwise Lambda is a good solution So for your case heres what Id do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "using aws service execute python script extract keywords text using keybert simple python script given two blocks text extracts keywords using keybert compares lists keywords sort two lists depending lists share keywords aws service would best fit needs want able esentially spin needed give blocks text execute return results dont want integrate projects dont use python ive attempted use lambda im concerned potential cost running thanks cases would normally think two resources aligned best practices aws software engineering sagemaker lambda model im using resourceintensive requires gpu acceleration id go sagemaker otherwise lambda good solution case heres id package keybert script lambda easily deploy container invoke whenever need process text blocks aws lambda charges execution time costefficient occasional tasks",
         "use aws service execute python script extract keyword text use keybert simple python script give two block text extract keyword use keybert compare list keyword sort two list depend list share keyword aws service would well fit need want able esentially spin need give block text execute return result do not want integrate project do not use python I ve attempt use lambda I m concerned potential cost run thank case would normally think two resource align good practice aws software engineering sagemaker lambda model I m use resourceintensive require gpu acceleration i d go sagemaker otherwise lambda good solution case here i d package keybert script lambda easily deploy container invoke whenever need process text block aws lambda charge execution time costefficient occasional task"
        ],
        [
         "19",
         "113",
         "79178041",
         "Normalization of token embeddings in BERT encoder blocks",
         "<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>\n",
         "2024-11-11 14:30:31",
         "2",
         "162",
         "2",
         "79238393.0",
         "<p>I tracked down full details of layer normalization (LN) in BERT <a href=\"https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para\">here</a>.</p>\n<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>\n",
         "0.0",
         "",
         "",
         "Normalization of token embeddings in BERT encoder blocks",
         "Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings",
         "I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token its per embedding dimension",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token its per embedding dimension",
         "normalization token embeddings bert encoder blocks following multiheaded attention layer bert encoder block layer normalization done separately embedding token ie one mean variance per token embedding concatenated vector token embeddings mean variance embeddings tracked full details layer normalization ln bert mean variance computed per token weight bias parameters learned ln per token per embedding dimension",
         "normalization token embedding bert encoder block follow multiheade attention layer bert encoder block layer normalization done separately embed token ie one mean variance per token embed concatenate vector token embedding mean variance embedding track full detail layer normalization ln bert mean variance compute per token weight bias parameter learn ln per token per embed dimension"
        ],
        [
         "20",
         "117",
         "79173053",
         "How to convert character indices to BERT token indices",
         "<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>\n<pre><code>from datasets import load_dataset\nds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\n</code></pre>\n<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>\n<pre><code>d0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n</code></pre>\n<p>After tokenization, the answer indices are 56  and 16:</p>\n<pre><code>from transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'\n</code></pre>\n<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>\n<p>This is from a <a href=\"https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false\" rel=\"nofollow noreferrer\">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=\"https://i.sstatic.net/GsZ6mfcQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/GsZ6mfcQ.png\" alt=\"QA dataset with token answer indices\" /></a></p>\n",
         "2024-11-09 15:15:33",
         "2",
         "33",
         "1",
         "79175157.0",
         "<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>\n<p>The following function does the above for you:</p>\n<pre><code>def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start &lt;= char_start &lt; end: \n            start_token_idx = i\n        if start &lt; char_end &lt;= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n</code></pre>\n<p>Here's how you can use and test this function:</p>\n<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])\nprint(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(&quot;Tokenized answer:&quot;, answer_tokens)\n</code></pre>\n<p>Output:</p>\n<pre><code>Tokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream\n</code></pre>\n",
         "2.0",
         "UCLNLP/adversarial_qa\n---\nfrom datasets import load_dataset\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\n---\nd0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n---\nfrom transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'",
         "def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start <= char_start < end: \n            start_token_idx = i\n        if start < char_end <= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n---\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(\"Tokenized start index:\", d0_tokenized['answer_start_token_idx'])\nprint(\"Tokenized end index:\", d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(\"Tokenized answer:\", answer_tokens)\n---\nTokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream",
         "How to convert character indices to BERT token indices",
         "I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result",
         "You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Heres how you can use and test this function Output",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Heres how you can use and test this function Output",
         "convert character indices bert token indices working questionanswer dataset map characterbased answer indices tokenbased indices tokenizing context question together using tokenizer like bert heres example row dataset tokenization answer indices 56 16 want create new dataset answers token indices eg 56 ad 60 linkedin learning class instructor conversion created csv file share code expected result encode question context locate token span answer within tokenized context update dataset tokenlevel indices following function heres use test function output",
         "convert character index bert token indices work questionanswer dataset map characterbase answer index tokenbase index tokenize context question together use tokenizer like bert heres example row dataset tokenization answer indice 56 16 want create new dataset answer token indices eg 56 ad 60 linkedin learn class instructor conversion create csv file share code expect result encode question context locate token span answer within tokenized context update dataset tokenlevel index follow function here use test function output"
        ],
        [
         "21",
         "121",
         "79159805",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage?",
         "<p>I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage.</p>\n<p>I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing.</p>\n<ul>\n<li><p>Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process?</p>\n</li>\n<li><p>Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes?</p>\n</li>\n<li><p>If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects?</p>\n</li>\n</ul>\n<p>Any suggestions or examples would be greatly appreciated! Thank you!</p>\n",
         "2024-11-05 15:49:33",
         "3",
         "93",
         "2",
         "79162232.0",
         "<p>I would strongly advise you not to treat NLP models like any other Python object. I would always prefer to load an NLP model using a microservice approach, which is more aligned with ML/software engineering best practices by separating the model logic from the main application.</p>\n<p>Instead of loading the model in each process (which can be memory-intensive), the model is loaded just once in a dedicated service. This setup allows the model to be used by multiple parts of the application without duplicating memory usage, making it efficient, modular, and scalable. Not only is your concern about memory efficiency addressed, but scalability and modularity are also improved.</p>\n<p>An example of implementing such a microservice using FastAPI + Docker could look like this:</p>\n<pre><code># main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(&quot;en_core_web_lg&quot;)  # Load model once\n\n@app.post(&quot;/process/&quot;)\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {&quot;tokens&quot;: [(token.text, token.pos_) for token in doc]}\n</code></pre>\n<p>To containerize above FastAPI service:</p>\n<pre><code># Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt &amp;&amp; python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [&quot;gunicorn&quot;, &quot;-w&quot;, &quot;4&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;main:app&quot;]\n</code></pre>\n",
         "3.0",
         "",
         "# main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(\"en_core_web_lg\")  # Load model once\n\n@app.post(\"/process/\")\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {\"tokens\": [(token.text, token.pos_) for token in doc]}\n---\n# Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt && python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [\"gunicorn\", \"-w\", \"4\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"main:app\"]",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage",
         "Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you",
         "I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "share complex spacy nlp model across multiple python processes minimize memory usage im working multiprocessing python application multiple processes need access large preloaded spacy nlp model eg en_core_web_lg since model memoryintensive want avoid loading separately process since quickly run main memory object readonly instead id like load shared location processes read without duplicating memory usage looked multiprocessingmanager multiprocessingshared_memory approaches seem better suited numpy arrays raw data buffers simple objects complex objects internal references like nlp model also looked mpis mpiwinallocate_shared ran issues using redis server make rank 0 processing works mpi since processing done single rank defeats propose using multiprocessing efficient way share spacy model instance across multiple processes python avoid reloading process libraries techniques specifically suited sharing complex readonly objects like nlp models memory across processes multiprocessingmanager shared_memory viable ways improve performance reduce memory overhead working complex objects suggestions examples would greatly appreciated thank would advise treat nlp models like python object would always prefer load nlp model using microservice approach aligned ml/software engineering best practices separating model logic main application instead loading model process memoryintensive model loaded dedicated service setup allows model used multiple parts application without duplicating memory usage making efficient modular scalable concern memory efficiency addressed scalability modularity also improved example implementing microservice using fastapi + docker could look like containerize fastapi service",
         "share complex spacy nlp model across multiple python process minimize memory usage I m work multiprocesse python application multiple process need access large preloade spacy nlp model eg en_core_web_lg since model memoryintensive want avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage look multiprocessingmanager multiprocessingshared_memory approach seem well suited numpy array raw datum buffer simple object complex object internal reference like nlp model also look mpis mpiwinallocate_shared run issue use redis server make rank 0 processing work mpi since process do single rank defeat propose use multiprocesse efficient way share spacy model instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp model memory across process multiprocessingmanager shared_memory viable way improve performance reduce memory overhead work complex object suggestion example would greatly appreciate thank would advise treat nlp model like python object would always prefer load nlp model use microservice approach align ml / software engineering good practice separate model logic main application instead load model process memoryintensive model load dedicated service setup allow model use multiple part application without duplicate memory usage make efficient modular scalable concern memory efficiency address scalability modularity also improve example implement microservice use fastapi + docker could look like containerize fastapi service"
        ],
        [
         "22",
         "122",
         "79155290",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing",
         "<p>When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data.</p>\n<p><a href=\"https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment\" rel=\"nofollow noreferrer\">https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment</a></p>\n<p>There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative.</p>\n<p><strong>Is there a way to get neutral labels for such examples in RobBERTje?</strong></p>\n<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = &quot;DTAI-KULeuven/robbert-v2-dutch-sentiment&quot;\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n</code></pre>\n<p>Output:</p>\n<pre><code>[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]\n</code></pre>\n",
         "2024-11-04 11:36:35",
         "2",
         "54",
         "1",
         "79155380.0",
         "<p>This model was trained only on <code>negative</code> and <code>positive</code> labels. Therefore, it will try to categorize every input as positive or negative, even if it is nonsensical or neutral.</p>\n<p>what you can do is to:\n1- Find other models that was trained to include <code>neutral</code> label.\n2- Fine-tune this model on a dataset that includes <code>neutral</code> label.\n3- Empirically define a threshold based on the confidence outputs and interpret it as <code>neutral</code>.</p>\n<p>The first 2 choices are extensive in effort. I would suggest you go with the third option for a quick workaround. Try feeding the model with a few neutral input and observe the range of confidence score in the output. then use that threshold to classify as <code>neutral</code>.</p>\n<p>Here's a sample:</p>\n<pre><code>def classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] &lt; threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result\n</code></pre>\n",
         "3.0",
         "from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n---\n[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]",
         "negative\n---\npositive\n---\nneutral\n---\nneutral\n---\nneutral\n---\nneutral\n---\ndef classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] < threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing",
         "When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output",
         "This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Heres a sample",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Heres a sample",
         "dutch sentiment analysis robbertje outputs positive/negative labels netural label missing run dutch sentiment analysis robbertje outputs positive/negative labels netural label missing data obvious neutral sentences/words eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral labels examples robbertje output model trained labels therefore try categorize every input positive negative even nonsensical neutral 1 find models trained include label 2 finetune model dataset includes label 3 empirically define threshold based confidence outputs interpret first 2 choices extensive effort would suggest go third option quick workaround try feeding model neutral input observe range confidence score output use threshold classify heres sample",
         "dutch sentiment analysis robbertje outputs positive / negative label netural label miss run dutch sentiment analysis robbertje outputs positive / negative label netural label miss datum obvious neutral sentence / word eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral label example robbertje output model train label therefore try categorize every input positive negative even nonsensical neutral 1 find model train include label 2 finetune model dataset include label 3 empirically define threshold base confidence output interpret first 2 choice extensive effort would suggest go third option quick workaround try feed model neutral input observe range confidence score output use threshold classify here sample"
        ],
        [
         "23",
         "123",
         "79148979",
         "Finding Root Form of Verbs using Curiosity-AI/Catalyst",
         "<p>I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match <code>PartOfSpeech.VERB</code> but I don't know how to continue from there.</p>\n<p>This is what I have so far:</p>\n<pre><code>const string text = &quot;The disastrous cat runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}\n</code></pre>\n<p>Any help is greatly appreciated.</p>\n",
         "2024-11-01 17:58:01",
         "2",
         "135",
         "1",
         "79160163.0",
         "<p>The following code (targeting .NET 8.0) illustrates one method to obtain the root form of a verb from an inflected form.</p>\n<p>(I have annonoted, as code comments, the three NuGet packages (with versions) required. Most of the code is identical to your original sample above.)</p>\n<pre><code>//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = &quot;The disastrous cat quickly runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($&quot;Root of the verb '{token.Value}' is '{token.Lemma}'.&quot;);\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(&quot;Complete; press any key.&quot;);\nConsole.ReadKey();\n</code></pre>\n<p><strong>Note:</strong> For this specific sentence, I have added an adverb (&quot;quickly&quot;) before the verb (&quot;runs&quot;). Without this, the library incorrectly interprets &quot;runs&quot; as a noun. Depending on your source text, this might be an issue for you, but I believe it is separate from the question being asked.</p>\n",
         "1.0",
         "PartOfSpeech.VERB\n---\nconst string text = \"The disastrous cat runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}",
         "//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = \"The disastrous cat quickly runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($\"Root of the verb '{token.Value}' is '{token.Lemma}'.\");\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(\"Complete; press any key.\");\nConsole.ReadKey();",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst",
         "Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated",
         "The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "finding root form verbs using curiosityai/catalyst im trying find root form verb run text pipeline identify tokens match dont know continue far help greatly appreciated following code targeting net 80 illustrates one method obtain root form verb inflected form annonoted code comments three nuget packages versions required code identical original sample note specific sentence added adverb quickly verb runs without library incorrectly interprets runs noun depending source text might issue believe separate question asked",
         "find root form verb use curiosityai / catalyst I m try find root form verb run text pipeline identify tokens match do not know continue far help greatly appreciate follow code target net 80 illustrate one method obtain root form verb inflect form annonote code comment three nuget package version require code identical original sample note specific sentence add adverb quickly verb run without library incorrectly interpret run noun depend source text might issue believe separate question ask"
        ],
        [
         "24",
         "125",
         "79145419",
         "Is it possible to get embeddings from NV-Embed using Candle?",
         "<p>What I want to do is a CLI program that outputs embeddings of an arbitrary input.\nTo do that, I want to do an inference with an embeddings model, and I chose <code>NV-Embed-v2</code>. My framework of choice is <a href=\"https://github.com/huggingface/candle\" rel=\"nofollow noreferrer\">Candle</a>, but I also looked at <a href=\"https://github.com/EricLBuehler/mistral.rs\" rel=\"nofollow noreferrer\">Mistral-RS</a>.</p>\n<p>Basically, what I'm trying to do is this code fragment:\n<a href=\"https://huggingface.co/nvidia/NV-Embed-v2\" rel=\"nofollow noreferrer\">https://huggingface.co/nvidia/NV-Embed-v2</a>\nbut with Rust and Candle.</p>\n<p>What I tried is to start off with <a href=\"https://github.com/huggingface/candle/blob/main/candle-examples/examples/mistral/main.rs\" rel=\"nofollow noreferrer\">Mistral Candle's example</a> because the NV-Embed's HF page says: <code>Model Details / Base Decoder-only LLM: Mistral-7B-v0.1</code>.</p>\n<p>I replaced the model id in the original code with <code>nvidia/NV-Embed-v2</code>, and was able to download the weights from Hugging Face, but upon loading the config, I got this:</p>\n<pre><code>Error: missing field `vocab_size` at line 101 column 1\n</code></pre>\n<p>Then I hardcoded the values from the JSON config loaded from HF to a newly created <code>candle_transformers::models::mistral::Config</code> instance. And after that, <code>Mistral::new(&amp;config, vb)</code> fails with:</p>\n<pre><code>Error: cannot find tensor model.embed_tokens.weight\n</code></pre>\n<p>Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?</p>\n",
         "2024-10-31 15:55:49",
         "0",
         "329",
         "1",
         "79156470.0",
         "<p>candle looking for <code>model.embed_tokens.weight</code> whereas the original tensor name is <code>embedding_model.embed_tokens.weight</code>. You just have to change this line of <code>mistral.rs</code> in candle_transformers.</p>\n<pre class=\"lang-rust prettyprint-override\"><code>// from\nlet vb_m = vb.pp(&quot;model&quot;);\n//to\nlet vb_m = vb.pp(&quot;embedding_model&quot;);\n</code></pre>\n",
         "2.0",
         "NV-Embed-v2\n---\nModel Details / Base Decoder-only LLM: Mistral-7B-v0.1\n---\nnvidia/NV-Embed-v2\n---\nError: missing field `vocab_size` at line 101 column 1\n---\ncandle_transformers::models::mistral::Config\n---\nMistral::new(&config, vb)\n---\nError: cannot find tensor model.embed_tokens.weight",
         "model.embed_tokens.weight\n---\nembedding_model.embed_tokens.weight\n---\nmistral.rs\n---\n// from\nlet vb_m = vb.pp(\"model\");\n//to\nlet vb_m = vb.pp(\"embedding_model\");",
         "Is it possible to get embeddings from NVEmbed using Candle",
         "What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed",
         "candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "possible get embeddings nvembed using candle want cli program outputs embeddings arbitrary input want inference embeddings model chose framework choice candle also looked mistralrs basically im trying code fragment rust candle tried start mistral candles example nvembeds hf page says replaced model id original code able download weights hugging face upon loading config got hardcoded values json config loaded hf newly created instance fails way around maybe candlebased open source works could use inspiration maybe thats common mistake could easily diagnosed candle looking whereas original tensor name change line candle_transformers",
         "possible get embedding nvembe use candle want cli program output embedding arbitrary input want inference embedding model choose framework choice candle also look mistralrs basically I m try code fragment rust candle try start mistral candle example nvembed hf page say replace model i d original code able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail way around maybe candlebase open source work could use inspiration maybe that s common mistake could easily diagnose candle look whereas original tensor name change line candle_transformer"
        ],
        [
         "25",
         "140",
         "79111733",
         "How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)",
         "<p>How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)</p>\n<p>I have short product descriptions that I’d like to transform into structured attributes.</p>\n<p>Example:</p>\n<p>Input:</p>\n<pre><code>“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n</code></pre>\n<p>Output:</p>\n<pre><code>Year = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml\n</code></pre>\n<p>If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:</p>\n<ol>\n<li><p>There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.</p>\n</li>\n<li><p>There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.</p>\n</li>\n<li><p>Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.</p>\n</li>\n<li><p>I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.</p>\n</li>\n</ol>\n<p>I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.</p>\n<p>I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.</p>\n<p>Appreciate any insight into approaches or suggested learning resources.</p>\n<p></p>\n<p>I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.</p>\n",
         "2024-10-21 20:54:56",
         "0",
         "166",
         "1",
         "79113907.0",
         "<p>LLM would work nicely for this.  I'v done similar tasks before and it worked nicely with minimal training.  Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate,  but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions.</p>\n<p>For you task I would use a framework like Langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  When run with a model it will create an XML output which would be trivial to parse.  You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me.</p>\n<pre><code>You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like &quot;Red&quot; or &quot;White&quot; (e.g., &quot;Red Head Vineyard&quot;) which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., &quot;1500 ml&quot;) rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n&lt;Wine&gt;\n&lt;Year&gt;{{Year}}&lt;/Year&gt;\n&lt;Color&gt;{{Color}}&lt;/Color&gt;\n&lt;Weight&gt;{{Weight}}&lt;/Weight&gt;\n&lt;WeightUnit&gt;{{WeightUnit}}&lt;/WeightUnit&gt;\n&lt;Brand&gt;{{Brand}}&lt;/Brand&gt;\n&lt;GrapeVariety&gt;{{GrapeVariety}}&lt;/GrapeVariety&gt;\n&lt;/Wine&gt;\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   &lt;Wine&gt;\n     &lt;Year&gt;2017&lt;/Year&gt;\n     &lt;Color&gt;Red&lt;/Color&gt;\n     &lt;Weight&gt;750&lt;/Weight&gt;\n     &lt;WeightUnit&gt;ml&lt;/WeightUnit&gt;\n     &lt;Brand&gt;La Lecciaia&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Cabernet Sauvignon&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   &lt;Wine&gt;\n     &lt;Year&gt;2020&lt;/Year&gt;\n     &lt;Color&gt;&lt;/Color&gt;\n     &lt;Weight&gt;1.5&lt;/Weight&gt;\n     &lt;WeightUnit&gt;L&lt;/WeightUnit&gt;\n     &lt;Brand&gt;Red Head Vineyard&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Chardonnay&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}\n</code></pre>\n<p>Keep in mind that LLMs are not cheap to run.  But for this tasks given ambiguousness of the domain it is most likely the best choice.  For this particular task it would be 1/1000 of a penny per label using OpenAI service.  You might find a cheaper model / provider.  However when working with LLM it is very important to ensure accuracy first,  then optimize for costs.</p>\n<p>The whole thing will probably take 1-2 hours to build for the intermediate LLM developer.  If you are learning it may vary.  But this is a perfect project to learn about LLMs</p>\n",
         "1.0",
         "“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n---\nYear = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml",
         "You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like \"Red\" or \"White\" (e.g., \"Red Head Vineyard\") which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., \"1500 ml\") rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n<Wine>\n<Year>{{Year}}</Year>\n<Color>{{Color}}</Color>\n<Weight>{{Weight}}</Weight>\n<WeightUnit>{{WeightUnit}}</WeightUnit>\n<Brand>{{Brand}}</Brand>\n<GrapeVariety>{{GrapeVariety}}</GrapeVariety>\n</Wine>\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   <Wine>\n     <Year>2017</Year>\n     <Color>Red</Color>\n     <Weight>750</Weight>\n     <WeightUnit>ml</WeightUnit>\n     <Brand>La Lecciaia</Brand>\n     <GrapeVariety>Cabernet Sauvignon</GrapeVariety>\n   </Wine>\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   <Wine>\n     <Year>2020</Year>\n     <Color></Color>\n     <Weight>1.5</Weight>\n     <WeightUnit>L</WeightUnit>\n     <Brand>Red Head Vineyard</Brand>\n     <GrapeVariety>Chardonnay</GrapeVariety>\n   </Wine>\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}",
         "How to derive attributes/labels from short plain text descriptions NER LLM",
         "How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense",
         "LLM would work nicely for this Iv done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense LLM would work nicely for this Iv done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "derive attributes/labels short plain text descriptions ner llm derive attributes/labels short plain text descriptions ner llm short product descriptions id like transform structured attributes example input output everything format would trivial write regular expression done many different formats nuances increasingly cumbersome hardcode logic format trying create generic solution immediately run issues basic approach several different data providers format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even given provider may multiple formats may change time formats always strictly followed many ways expressing particular components example weight might expressed one 15l 1 1/2 liters 1500ml etc parts description may confused target components may white wine brand called red head vineyard weight 2000 ml may confused year etc im using wine examples sake simplicity general audience product domain conceptual issues id consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work harder get right theres straightforward method would good know id like develop generalpurpose function accept description format little experience nlp/artificial intelligence suspect useful tools/algos leverage 1000+ example records could potentially use train model something run locally would preferred necessary im looking specific implementation guidance anyone whos worked similar problem open hybrid approaches additional logic manual oversight could account initial inaccuracies appreciate insight approaches suggested learning resources ive looked online information many approaches involve significant amount front work unclear theyll work practical sense llm would work nicely iv done similar tasks worked nicely minimal training keep mind statistical methods nlp / llm / ner never 100 % accurate practical purposes find llms accurate custom soup regular expressions task would use framework like langchain following prompt note might need work prompt bit example run model create xml output would trivial parse modify prompt create different type outputs personally find xml working well keep mind llms cheap run tasks given ambiguousness domain likely best choice particular task would 1/1000 penny per label using openai service might find cheaper model / provider however working llm important ensure accuracy first optimize costs whole thing probably take 12 hours build intermediate llm developer learning may vary perfect project learn llms",
         "derive attribute / label short plain text description ner llm derive attribute / label short plain text description ner llm short product description i d like transform structure attribute example input output everything format would trivial write regular expression do many different format nuance increasingly cumbersome hardcode logic format try create generic solution immediately run issue basic approach several different datum provider format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even give provider may multiple format may change time format always strictly follow many way express particular component example weight might express one 15l 1 1/2 liter 1500ml etc part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year etc I m use wine example sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work hard get right there s straightforward method would good know I d like develop generalpurpose function accept description format little experience nlp / artificial intelligence suspect useful tool / algo leverage 1000 + example record could potentially use train model something run locally would prefer necessary I m look specific implementation guidance anyone who s work similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource I ve look online information many approach involve significant amount front work unclear they ll work practical sense llm would work nicely iv do similar task work nicely minimal training keep mind statistical method nlp / llm / ner never 100 % accurate practical purpose find llm accurate custom soup regular expression task would use framework like langchain follow prompt note might need work prompt bit example run model create xml output would trivial parse modify prompt create different type output personally find xml work well keep mind llm cheap run task give ambiguousness domain likely good choice particular task would 1/1000 penny per label use openai service might find cheap model / provider however work llm important ensure accuracy first optimize cost whole thing probably take 12 hour build intermediate llm developer learning may vary perfect project learn llm"
        ],
        [
         "26",
         "141",
         "79102797",
         "Varying embedding dim due to changing padding in batch size",
         "<p>I want to train a simple neural network, which has <strong>embedding_dim</strong> as a parameter:</p>\n<pre><code>class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n</code></pre>\n<p>To load the data I used torchs DataLoader with a custom collate_fn.</p>\n<pre><code>train_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n</code></pre>\n<p>The collate_fn_padd function looks the following:</p>\n<pre><code>def collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels\n\n</code></pre>\n<p><strong>The problem:</strong> For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch).</p>\n<p>That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch.</p>\n<p>Due to that, I receive errors like that: <strong>mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64)</strong></p>\n<p>Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?</p>\n",
         "2024-10-18 15:54:51",
         "0",
         "42",
         "1",
         "79105117.0",
         "<p>You can add a maximum length argument set to <code>embedding_dim</code> to pad and truncate all the data to a fixed length:</p>\n<pre><code>padded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]\n</code></pre>\n",
         "1.0",
         "class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n---\ntrain_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n---\ndef collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels",
         "embedding_dim\n---\npadded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]",
         "Varying embedding dim due to changing padding in batch size",
         "I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size",
         "You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "varying embedding dim due changing padding batch size want train simple neural network embedding_dim parameter load data used torchs dataloader custom collate_fn collate_fn_padd function looks following problem every batch want train model embedded text gets padded differently long takes longest sequence current batch means embedding dim/input size linear layer neural network changes batch batch althoug want size every batch due receive errors like mat1 mat2 shapes multiplied 16x182 301x64 possible adjust collate_fn_pad function padds sequence size independet batch size add maximum length argument set pad truncate data fixed length",
         "vary embed dim due change padding batch size want train simple neural network embedding_dim parameter load datum use torchs dataloader custom collate_fn collate_fn_padd function look follow problem every batch want train model embed text get pad differently long take long sequence current batch mean embed dim / input size linear layer neural network change batch batch althoug want size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collate_fn_pad function padd sequence size independet batch size add maximum length argument set pad truncate datum fix length"
        ],
        [
         "27",
         "144",
         "79100835",
         "How can I adjust the performance of tokenizer?",
         "<p>Working with the tokenizer from the <code>transformers</code> library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not.</p>\n<p>I'm wondering if I can <strong>&quot;adjust&quot;</strong> (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to.</p>\n<p>To be more specific, the type of tokenizer is <code>transformers.XLMRobertaTokenizerFast</code>, which is a unigram tokenizer, and the model is <code>paraphrase-multilingual-mpnet-base-v2</code>.</p>\n",
         "2024-10-18 06:45:15",
         "0",
         "45",
         "1",
         "79107575.0",
         "<p>You can change the tokenizer's vocabulary:</p>\n<pre><code>tokenizer.add_tokens([&quot;asadaf&quot;, &quot;sdfsaf&quot;])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = &quot;This is asadaf and sdfsaf&quot;\nprint(tokenizer(input_text))\n</code></pre>\n<p>As a result, <em>asadaf</em> and <em>sdfsaf</em> would be tokenized as unique words.</p>\n",
         "1.0",
         "transformers\n---\ntransformers.XLMRobertaTokenizerFast\n---\nparaphrase-multilingual-mpnet-base-v2",
         "tokenizer.add_tokens([\"asadaf\", \"sdfsaf\"])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = \"This is asadaf and sdfsaf\"\nprint(tokenizer(input_text))",
         "How can I adjust the performance of tokenizer",
         "Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is",
         "You can change the tokenizers vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is You can change the tokenizers vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "adjust performance tokenizer working tokenizer library hugging face tokenizer works fine cases cases im wondering adjust train new tokenizer scratch performance tokenizer handle bad cases still maintaining good performance cases used specific type tokenizer unigram tokenizer model change tokenizers vocabulary result asadaf sdfsaf would tokenized unique words",
         "adjust performance tokenizer working tokenizer library hug face tokenizer work fine case case I m wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case use specific type tokenizer unigram tokenizer model change tokenizer vocabulary result asadaf sdfsaf would tokenize unique word"
        ],
        [
         "28",
         "149",
         "79081924",
         "With spaCy, how can I get all lemmas from a string?",
         "<p>I have a pandas data frame with a column of text values (documents).  I want to apply lemmatization on these values with the spaCy library using the pandas <code>apply</code> function.  I've defined my <code>to_lemma</code> function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow.  Is there a way to extract the lemmatized form of a document in spaCy?</p>\n<pre><code>def to_lemma(text):\n    tp = nlp(text)\n    line = &quot;&quot;\n    for word in tp:\n        line = line + word.lemma_ + &quot; &quot;\n    return line\n</code></pre>\n",
         "2024-10-12 21:03:21",
         "-1",
         "97",
         "2",
         "79086290.0",
         "<p>There are many ways to speed up SpaCy processing. The question which of them make sense for you depends mostly on the size of your input.</p>\n<ol>\n<li>The most obvious one is not individually apply the model to every single row, but rather use batch processing. Use <code>nlp.pipe()</code> with an Iterable of strings. This means it is easier to not use apply.</li>\n<li>Disable components that you do not use. For token level processing where you need the lemmas this would be <code>'parser'</code> (the dependency parser) and <code>'ner'</code> (the Named Entity Recognition component).</li>\n<li>Increase the <code>batch_size</code> (objects to buffer) in pipe(). The default is 1000. Obviously this only makes sense to touch if you have the memory to increase it a lot.</li>\n<li>Increase the number of processors used using <code>n_process</code>. This will increase the time it takes to initially load the model but decrease the processing time. In my experience this starts making sense at about 500k+ texts. Note that this also requires the code to be run in an <code>if __name__ == '__main__':</code> wrapper.</li>\n</ol>\n<p>Basic example with 1. and 2.:</p>\n<pre><code>texts = df[&quot;column_name&quot;]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\ndf[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n<p>Advanced example for all four:</p>\n<pre><code>if __name__ == '__main__':\n    texts = df[&quot;column_name&quot;]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\n    df[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n",
         "2.0",
         "apply\n---\nto_lemma\n---\ndef to_lemma(text):\n    tp = nlp(text)\n    line = \"\"\n    for word in tp:\n        line = line + word.lemma_ + \" \"\n    return line",
         "nlp.pipe()\n---\n'parser'\n---\n'ner'\n---\nbatch_size\n---\nn_process\n---\nif __name__ == '__main__':\n---\ntexts = df[\"column_name\"]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\ndf[\"column_name_lemmas\"] = lemmas\n---\nif __name__ == '__main__':\n    texts = df[\"column_name\"]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\n    df[\"column_name_lemmas\"] = lemmas",
         "With spaCy how can I get all lemmas from a string",
         "I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy",
         "There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "spacy get lemmas string pandas data frame column text values documents want apply lemmatization values spacy library using pandas function ive defined function iterate words document concatenate corresponding lemmas output string however slow way extract lemmatized form document spacy many ways speed spacy processing question make sense depends mostly size input obvious one individually apply model every single row rather use batch processing use iterable strings means easier use apply disable components use token level processing need lemmas would dependency parser named entity recognition component increase objects buffer pipe default 1000 obviously makes sense touch memory increase lot increase number processors used using increase time takes initially load model decrease processing time experience starts making sense 500k+ texts note also requires code run wrapper basic example 1 2 advanced example four",
         "spacy get lemmas string panda datum frame column text value document want apply lemmatization value spacy library use panda function I ve define function iterate word document concatenate correspond lemmas output string however slow way extract lemmatize form document spacy many way speed spacy processing question make sense depends mostly size input obvious one individually apply model every single row rather use batch processing use iterable string mean easy use apply disable component use token level processing need lemmas would dependency parser name entity recognition component increase object buffer pipe default 1000 obviously make sense touch memory increase lot increase number processor use use increase time take initially load model decrease processing time experience start make sense 500k+ text note also require code run wrapper basic example 1 2 advanced example four"
        ],
        [
         "29",
         "157",
         "79057082",
         "Avoiding overlap in frequency and document frequency count in Quanteda",
         "<p>Below is a dummy corpus of 4 documents.</p>\n<p>The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in.</p>\n<p>The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive.</p>\n<p>Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each,\nbut are intended to be counted once, according to the dictionary.</p>\n<p>The expected overall frequency count is (extracted from the 'pattern&quot; column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key.</p>\n<p>Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key.</p>\n<p>I have three questions:</p>\n<ol>\n<li>are there any problems/issues that could affect output accuracy using this approach?</li>\n<li>is there a better/more parsimonius approach to achieve what I am trying to do?</li>\n<li>what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table?</li>\n</ol>\n<pre><code>        library (quanteda)\n        library(quanteda.textstats)\n\n        txt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\n\n\n         x &lt;- x %&gt;%\n         tokens_compound(phrase(&quot;dairy farmers&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farms&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farm&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farming&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy industry&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;indigenous australians&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;australia post&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farmer&quot;), concatenator = &quot; &quot;)\n              x\n\n         dict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign- \n         owned&quot;, &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, &quot;multinational \n         oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n         dairy = c(&quot;dairy farmers&quot;, &quot;dairy farms&quot;,&quot;dairy farm&quot;,&quot;dairy farming&quot;,&quot;dairy \n         industry&quot;, &quot;dairy farmer&quot;,&quot;dairy&quot;, &quot;milk&quot;),\n         auspost = &quot;australia post&quot;,\n         oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n         farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n         foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n         business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n         indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres \n         strait&quot;),\n         peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, &quot;people of \n         this nation&quot;, &quot;people of this country&quot;),\n         industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\n        kwicdict &lt;- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, &quot;D:/Output/TEST.csv&quot;)\n\n       DF &lt;- read.csv(&quot;D://Output/TEST.csv&quot;,header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       &gt; x2 &lt;- DF[,8]\n       &gt; \n       &gt; table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2 \n</code></pre>\n",
         "2024-10-05 12:43:52",
         "1",
         "57",
         "1",
         "79058791.0",
         "<p>I don't think that <code>kwic()</code> is what you want here. <code>tokens_lookup()</code> lets you specify that the nested scope should be mutually exclusive across keys, not just within keys. Observe the difference below. (And note the use of wildcarding for dairy key.)</p>\n<pre class=\"lang-r prettyprint-override\"><code>library(quanteda)\n#&gt; Package version: 4.1.0\n#&gt; Unicode version: 14.0\n#&gt; ICU version: 71.1\n#&gt; Parallel computing: 10 of 10 threads used.\n#&gt; See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\ndict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign-owned&quot;, \n                                     &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, \n                                     &quot;multinational oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n                        dairy = c(&quot;dairy farm*&quot;, &quot;dairy industry&quot;, &quot;dairy&quot;, &quot;milk&quot;),\n                        auspost = &quot;australia post&quot;,\n                        oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n                        farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n                        foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n                        business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n                        indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres strait&quot;),\n                        peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, \n                                 &quot;people of this nation&quot;, &quot;people of this country&quot;),\n                        industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\nx &lt;- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       5       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        1\n#&gt;   doc3        1     1       0  0       1       0        1     1    1        0\n#&gt;   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = &quot;dictionary&quot;) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       3       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        0\n#&gt;   doc3        1     1       0  0       1       0        1     1    0        0\n#&gt;   doc4        0     0       1  1       1       1        0     0    0        0\n</code></pre>\n<p><sup>Created on 2024-10-06 with <a href=\"https://reprex.tidyverse.org\" rel=\"nofollow noreferrer\">reprex v2.1.1</a></sup></p>\n",
         "0.0",
         "library (quanteda)\n        library(quanteda.textstats)\n\n        txt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\n\n\n         x <- x %>%\n         tokens_compound(phrase(\"dairy farmers\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farms\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farm\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farming\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy industry\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"indigenous australians\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"australia post\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farmer\"), concatenator = \" \")\n              x\n\n         dict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign- \n         owned\", \"foreign owned\", \"foreign companies\", \"multinational\", \"multinational \n         oil companies\", \"multinationals\", \"transnational\"),\n         dairy = c(\"dairy farmers\", \"dairy farms\",\"dairy farm\",\"dairy farming\",\"dairy \n         industry\", \"dairy farmer\",\"dairy\", \"milk\"),\n         auspost = \"australia post\",\n         oz = c(\"australia\", \"this country\", \"our country\"),\n         farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n         foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n         business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n         indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres \n         strait\"),\n         peep = c(\"australians\", \"people of australia\", \"australian people\", \"people of \n         this nation\", \"people of this country\"),\n         industry = c(\"industry\", \"industries\")))\n\n        kwicdict <- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, \"D:/Output/TEST.csv\")\n\n       DF <- read.csv(\"D://Output/TEST.csv\",header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       > x2 <- DF[,8]\n       > \n       > table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2",
         "kwic()\n---\ntokens_lookup()\n---\nlibrary(quanteda)\n#> Package version: 4.1.0\n#> Unicode version: 14.0\n#> ICU version: 71.1\n#> Parallel computing: 10 of 10 threads used.\n#> See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\ndict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign-owned\", \n                                     \"foreign owned\", \"foreign companies\", \"multinational\", \n                                     \"multinational oil companies\", \"multinationals\", \"transnational\"),\n                        dairy = c(\"dairy farm*\", \"dairy industry\", \"dairy\", \"milk\"),\n                        auspost = \"australia post\",\n                        oz = c(\"australia\", \"this country\", \"our country\"),\n                        farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n                        foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n                        business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n                        indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres strait\"),\n                        peep = c(\"australians\", \"people of australia\", \"australian people\", \n                                 \"people of this nation\", \"people of this country\"),\n                        industry = c(\"industry\", \"industries\")))\n\nx <- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       5       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        1\n#>   doc3        1     1       0  0       1       0        1     1    1        0\n#>   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = \"dictionary\") |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       3       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        0\n#>   doc3        1     1       0  0       1       0        1     1    0        0\n#>   doc4        0     0       1  1       1       1        0     0    0        0",
         "Avoiding overlap in frequency and document frequency count in Quanteda",
         "Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table",
         "I dont think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table I dont think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "avoiding overlap frequency document frequency count quanteda dummy corpus 4 documents dictionary developed identify frequency words phrases corpus well number documents word phrases occurs world australians occurs two dictionary keys peep indig key content intended mutually exclusive similarly australia oz australia post foreign foreign multinat farm/farmers dairy farmers occur two dictionary keys intended counted according dictionary expected overall frequency count extracted pattern column kwic table reported x2 note word industry appears allocated industry define din indig key dairy frequency occuring key occuring three documents calculated unique rows kwic table doc names column key three questions problems/issues could affect output accuracy using approach better/more parsimonius approach achieve trying would best way extract equivalent tetxstat frequency count data kwic table dont think want lets specify nested scope mutually exclusive across keys within keys observe difference note use wildcarding dairy key created 20241006 reprex v211",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency word phrase corpus well number document word phrase occur world australian occur two dictionary key peep indig key content intend mutually exclusive similarly australia oz australia post foreign foreign multinat farm / farmer dairy farmer occur two dictionary key intend count accord dictionary expect overall frequency count extract pattern column kwic table report x2 note word industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table doc name column key three question problem / issue could affect output accuracy use approach well / more parsimonius approach achieve trying would well way extract equivalent tetxstat frequency count datum kwic table do not think want let specify nest scope mutually exclusive across key within key observe difference note use wildcarde dairy key create 20241006 reprex v211"
        ],
        [
         "30",
         "169",
         "79005985",
         "Seq2Seq trainer.train() keeps giving indexing error",
         "<p>I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error:</p>\n<blockquote>\n<p>IndexError: Invalid key: 39463 is out of bounds for size 0.</p>\n</blockquote>\n<ul>\n<li>The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B</li>\n<li>The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also.</li>\n</ul>\n<p>Detailed error message:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;nllbtrain.py&quot;, line 273, in &lt;module&gt;\n    print(trainer.train())\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1645, in train\n    return inner_training_loop(\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 631, in __next__\n    data = self._next_data()\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2810, in __getitem__\n    return self._getitem(key)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 583, in query_table\n    _check_valid_index_key(key, size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 526, in _check_valid_index_key\n    raise IndexError(f&quot;Invalid key: {key} is out of bounds for size {size}&quot;)\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n</code></pre>\n<p>The code of the preprocessing done for the data:</p>\n<pre><code>def preprocess_function(examples):\n        inputs = [example + ' &lt;/s&gt;' + f' &lt;2{s_lang}&gt;' for example in examples[source_lang]]\n        targets = [f'&lt;2{t_lang}&gt; ' + example + ' &lt;/s&gt;' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n</code></pre>\n<p>Data after preprocessing:</p>\n<pre><code>DatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n</code></pre>\n<p>The code of model params and training:</p>\n<pre><code>model_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=&quot;hin_Deva&quot;, tgt_lang=&quot;san_Deva&quot;, max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=&quot;epoch&quot;,\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=&quot;./output_dir&quot;,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())\n\n</code></pre>\n<p>Any idea why this error is persisting?</p>\n",
         "2024-09-20 08:43:32",
         "0",
         "54",
         "1",
         "79007590.0",
         "<p><code>size 0</code> indicates that the dataset your trainer gets when the fine-tuning starts is empty. Looking at this (<a href=\"https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25</a>) and this (<a href=\"https://github.com/huggingface/datasets/issues/6535\" rel=\"nofollow noreferrer\">https://github.com/huggingface/datasets/issues/6535</a>) thread suggests adding <code>remove_unused_columns = False</code> to your <code>training_args</code> might resolve the issue, so you could give that a try.</p>\n",
         "0.0",
         "Traceback (most recent call last):\n  File \"nllbtrain.py\", line 273, in <module>\n    print(trainer.train())\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2810, in __getitem__\n    return self._getitem(key)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\n    _check_valid_index_key(key, size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n---\ndef preprocess_function(examples):\n        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]\n        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n---\nDatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n---\nmodel_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=\"hin_Deva\", tgt_lang=\"san_Deva\", max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=\"epoch\",\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=\"./output_dir\",\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())",
         "size 0\n---\nremove_unused_columns = False\n---\ntraining_args",
         "Seq2Seq trainertrain keeps giving indexing error",
         "I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting",
         "indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "seq2seq trainertrain keeps giving indexing error trying machine translation hindi sanskrit using nllb model keep getting error indexerror invalid key 39463 bounds size 0 error coming training pretrained nllb model ` facebook/nllb20013b input data ~40k hindi sentences error arises tried training sample data also detailed error message code preprocessing done data data preprocessing code model params training idea error persisting indicates dataset trainer gets finetuning starts empty looking thread suggests adding might resolve issue could give try",
         "seq2seq trainertrain keep give indexing error try machine translation hindi sanskrit use nllb model keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb model ` facebook / nllb20013b input datum ~40k hindi sentence error arises try train sample datum also detail error message code preprocessing do data datum preprocesse code model param train idea error persisting indicate dataset trainer get finetuning start empty look thread suggest add might resolve issue could give try"
        ],
        [
         "31",
         "175",
         "78985137",
         "Alternative to device_map = \"auto\" in Huggingface Pretrained",
         "<p>I have a model that I was reading from huggingface using the following code:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n</code></pre>\n<p>Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model.</p>\n<p>Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like <code>device_map=&quot;auto&quot;</code> but after reading the model.</p>\n<p>So simply something like</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\nmodel.device_map = &quot;auto&quot;\n</code></pre>\n",
         "2024-09-14 12:42:03",
         "2",
         "1034",
         "1",
         "79007343.0",
         "<p>I found out that there are actually several methods in <code>accelerate</code> for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map</a></p>\n<p>The second one is used to match your model with the devices:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model</a></p>\n<p>So basically, in your case, you can use the following code:</p>\n<pre><code>from accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)\n</code></pre>\n<p>P.S. This code still needs to be tested on fine-tuning.</p>\n",
         "2.0",
         "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n---\ndevice_map=\"auto\"\n---\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\nmodel.device_map = \"auto\"",
         "accelerate\n---\nfrom accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)",
         "Alternative to device_map = auto in Huggingface Pretrained",
         "I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like",
         "I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "alternative device_map = auto huggingface pretrained model reading huggingface using following code read model modifications internal layers added layers started training/finetuning get everything model investigations found custom layers arent distributed multi gpus original model need something like reading model simply something like found actually several methods first one used analyze model calculate total amount available memory occupied model second one used match model devices basically case use following code ps code still needs tested finetuning",
         "alternative device_map = auto huggingface pretraine model reading huggingface use follow code read model modification internal layer add layer start training / finetune get everything model investigation find custom layer be not distribute multi gpus original model need something like read model simply something like find actually several method first one use analyze model calculate total amount available memory occupy model second one use match model device basically case use follow code ps code still need tested finetune"
        ],
        [
         "32",
         "180",
         "78966943",
         "How are the weights of the Mistral models reinitialized in Huggingface?",
         "<p>From <a href=\"https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic\">How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model?</a> and <a href=\"https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4</a> there's different suggestions to reinitialize the model.</p>\n<p>When I tried this, it seems to work.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;, token=&quot;hf_*****&quot;)\n\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n</code></pre>\n<p>[out]:</p>\n<pre><code>OrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n</code></pre>\n<p>How are the layers re-initialized through the <code>from_config</code> function? Is it using <a href=\"https://cs230.stanford.edu/section/4/\" rel=\"nofollow noreferrer\">Xaiver/He initialization</a> or just random initialization?</p>\n",
         "2024-09-09 19:25:52",
         "3",
         "175",
         "2",
         "78969695.0",
         "<p><a href=\"https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralConfig\" rel=\"nofollow noreferrer\">MistralConfig</a> has a default parameter <code>initializer_range</code> which is set to 0.02 and described as <code>The standard deviation of the truncated_normal_initializer for initializing all weight matrices</code>, so one can assume they use a truncated normal distribution with a standard deviation of 0.02.</p>\n<p>If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 0.02 looks like, it seems like a fit to me:</p>\n<pre><code>import numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/M67S0rKp.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/M67S0rKp.png\" alt=\"model_weights\" /></a></p>\n",
         "2.0",
         "from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", token=\"hf_*****\")\n\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n---\nOrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n---\nfrom_config",
         "initializer_range\n---\nThe standard deviation of the truncated_normal_initializer for initializing all weight matrices\n---\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()",
         "How are the weights of the Mistral models reinitialized in Huggingface",
         "From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization",
         "MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "weights mistral models reinitialized huggingface one reinitialize weights hugging face llama v2 model official way original model theres different suggestions reinitialize model tried seems work layers reinitialized function using xaiver/he initialization random initialization mistralconfig default parameter set 002 described one assume use truncated normal distribution standard deviation 002 plot actual model weights distribution truncated normal distribution standard deviation 002 looks like seems like fit",
         "weight mistral model reinitialize huggingface one reinitialize weight hug face llama v2 model official way original model there s different suggestion reinitialize model try seem work layer reinitialize function use xaiver / he initialization random initialization mistralconfig default parameter set 002 describe one assume use truncate normal distribution standard deviation 002 plot actual model weight distribution truncate normal distribution standard deviation 002 look like seem like fit"
        ],
        [
         "33",
         "184",
         "78957322",
         "Break after first PER sequence found with Spacy",
         "<p>I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all &quot;PER&quot; tags, but I want to reduce the overhead and get only the first contiguous sequence of &quot;PER&quot; entities. Here’s the example output I get:</p>\n<pre><code>Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n</code></pre>\n<p>But I want the result to be:</p>\n<pre><code>Detected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n</code></pre>\n<p>Here is the code I am currently using:</p>\n<pre><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}]\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == &quot;PER&quot;:\n            per_entities.append(ent.text)\n        elif ent.label_ == &quot;MISC&quot; and misc_count &lt; 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n<p>What I'm looking for:</p>\n<p>I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of &quot;PER&quot; entities in the text, ignoring any subsequent &quot;PER&quot; entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?</p>\n",
         "2024-09-06 13:14:32",
         "0",
         "39",
         "1",
         "78957722.0",
         "<p>The issues is that <code>doc[start_idx:].ents</code> is <a href=\"https://spacy.io/api/doc#ents\" rel=\"nofollow noreferrer\">only the named entities</a> in that slice of the doc. Thus, you will never process &quot;habló&quot; for the first entry, you will just go straight from &quot;García&quot; to &quot;López&quot;. To actually iterate over the tokens so that you see when the PER sequence ends, you have to leave out the <code>.ents</code> part. Then you just wait until you see the first token with <code>ent_type_</code> PER and start appending, then break after one of your conditions is met. I ended up refactoring your code a little as I debugged this, but here's an edited version of your program that produces the desired outputs:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;,\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}],\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == &quot;PER&quot;:\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == &quot;MISC&quot; and misc_count &lt; 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n",
         "1.0",
         "Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n---\nDetected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\"\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}]\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == \"PER\":\n            per_entities.append(ent.text)\n        elif ent.label_ == \"MISC\" and misc_count < 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "doc[start_idx:].ents\n---\n.ents\n---\nent_type_\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\",\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}],\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == \"PER\":\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == \"MISC\" and misc_count < 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "Break after first PER sequence found with Spacy",
         "I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this",
         "The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but heres an edited version of your program that produces the desired outputs",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but heres an edited version of your program that produces the desired outputs",
         "break first per sequence found spacy trying extract first speakers name list texts using spacy currently function returns per tags want reduce overhead get first contiguous sequence per entities heres example output get want result code currently using im looking want modify find_per_sequence function returns first contiguous sequence per entities text ignoring subsequent per entities encountering different type entity provided function returns multiple names partial names need way ensure first name sequence included achieve issues named entities slice doc thus never process habl first entry go straight garca lpez actually iterate tokens see per sequence ends leave part wait see first token per start appending break one conditions met ended refactoring code little debugged heres edited version program produces desired outputs",
         "break first per sequence find spacy try extract first speaker name list text use spacy currently function return per tag want reduce overhead get first contiguous sequence per entity here example output get want result code currently use I m look want modify find_per_sequence function return first contiguous sequence per entity text ignore subsequent per entity encounter different type entity provide function return multiple name partial name need way ensure first name sequence include achieve issue name entity slice doc thus never process habl first entry go straight garca lpez actually iterate token see per sequence end leave part wait see first token per start append break one condition meet end refactore code little debugged here edit version program produce desire output"
        ],
        [
         "34",
         "189",
         "78949607",
         "Trainer huggingface - RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned",
         "<p>I recently got the following error:\n<code>RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned</code>\nwhen doing LoRA on a small LLM.</p>\n<p>I saw on a discord someone saying:</p>\n<blockquote>\n<p>The issue likely stems from the fact that you are manually placing\nyour inputs on the GPU (with to(model.device)), but the Trainer\nexpects data to be on the CPU and will handle the transfer to the GPU\ninternally.</p>\n</blockquote>\n<p>I can't find anything of the sort written in the Trainer documentation of huggingface <a href=\"https://huggingface.co/docs/transformers/en/main_classes/trainer\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/en/main_classes/trainer</a>.</p>\n<p>Is it true? If not, how can I get rid of that error?</p>\n<p>MRE:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = &quot;croissantllm/CroissantLLMBase&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=&quot;auto&quot;)\n\ntexts = [\n    &quot;The first sentence for fine-tuning. &lt;/s&gt;&quot;,\n    &quot;The second sentence for fine-tuning. &lt;/s&gt;&quot;\n]\n\ninputs = [tokenizer(text, return_tensors=&quot;pt&quot;).to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {&quot;input_ids&quot;: input_ids, &quot;labels&quot;: labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n</code></pre>\n<p>The issue is fairly easy to reproduce directly on colab (run <code>%pip install --upgrade torch transformers peft</code> in the first cell).</p>\n",
         "2024-09-04 16:09:18",
         "2",
         "1370",
         "1",
         "79112186.0",
         "<p>Since pinning memory is only available on CPU and not GPU, when running on GPU on Colab, you can just disable it by setting <code>dataloader_pin_memory</code> to <code>False</code> for <code>TrainingArguments</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n</code></pre>\n",
         "3.0",
         "RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned\n---\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = \"croissantllm/CroissantLLMBase\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\ntexts = [\n    \"The first sentence for fine-tuning. </s>\",\n    \"The second sentence for fine-tuning. </s>\"\n]\n\ninputs = [tokenizer(text, return_tensors=\"pt\").to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {\"input_ids\": input_ids, \"labels\": labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n---\n%pip install --upgrade torch transformers peft",
         "dataloader_pin_memory\n---\nFalse\n---\nTrainingArguments\n---\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned",
         "I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell",
         "Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensors pinned recently got following error lora small llm saw discord someone saying issue likely stems fact manually placing inputs gpu tomodeldevice trainer expects data cpu handle transfer gpu internally cant find anything sort written trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell since pinning memory available cpu gpu running gpu colab disable setting",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin recently get follow error lora small llm see discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not find anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell since pin memory available cpu gpu run gpu colab disable setting"
        ],
        [
         "35",
         "193",
         "78943401",
         "Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error \"Attempting to Unscale FP16 Gradients\"",
         "<p>I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?</p>\n<p>Here is a minimal example:</p>\n<pre class=\"lang-none prettyprint-override\"><code>import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = &quot;facebook/opt-1.3b&quot;\ncache_dir = './models'\nos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;7&quot;\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=&quot;nf4&quot;,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n</code></pre>\n<p>At the line <code>scaler.step(optimizer)</code>, an error occurs:</p>\n<pre><code>Exception has occurred: ValueError: Attempting to unscale FP16 gradients.\n\n</code></pre>\n",
         "2024-09-03 08:38:23",
         "1",
         "497",
         "1",
         "78945455.0",
         "<p>You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32.</p>\n<p>You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training.</p>\n<p>What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details <a href=\"https://huggingface.co/docs/peft/en/developer_guides/quantization\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "1.0",
         "import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = \"facebook/opt-1.3b\"\ncache_dir = './models'\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n---\nscaler.step(optimizer)\n---\nException has occurred: ValueError: Attempting to unscale FP16 gradients.",
         "",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients",
         "I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs",
         "You cant finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldnt finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs You cant finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldnt finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "finetuning pretrained model quantization amp scaler error attempting unscale fp16 gradients trying finetune pretrained model limited vram achieve using quantization automatic mixed precision amp however encountering issue cant seem resolve could please help identify problem minimal example line error occurs cant finetune fp16/uint8 model amp amp uses fp32 parameters params autocast fp16 forward pass amp expects master set parameters fp32 also shouldnt finetune quantized model first place quantization causes sorts numerical issues instability training supposed keep quantized model static train adapter top quantized model find details",
         "finetune pretraine model quantization amp scaler error attempt unscale fp16 gradient try finetune pretraine model limited vram achieve use quantization automatic mixed precision amp however encounter issue can not seem resolve could please help identify problem minimal example line error occur can not finetune fp16 / uint8 model amp amp use fp32 parameter param autocast fp16 forward pass amp expect master set parameter fp32 also should not finetune quantize model first place quantization cause sort numerical issue instability training suppose keep quantize model static train adapter top quantize model find detail"
        ],
        [
         "36",
         "197",
         "78933232",
         "Keep training pytorch model on new data",
         "<p>I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps:</p>\n<ol>\n<li>Load and process the text.</li>\n<li>Use a TF-IDF Vectorizer.</li>\n<li>Build the neural network and save the TF-IDF Vectorizer and model to predict new data.</li>\n</ol>\n<p>However, every day I need to classify new comments and correct any wrong classifications.</p>\n<p>Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct).</p>\n<p>Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen.</p>\n<p>So the mains questions are:</p>\n<ol>\n<li>How could i check if the propossed way to solve this problem work as i expect?</li>\n<li>What can i do with the vectorizer when it face new tokens, can i just do a <code>.fit_transform()</code> or i would loose the original vectorizer?</li>\n</ol>\n<p>Here its the full training process:</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        &quot;set1.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n&quot;&quot;&quot;\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n&quot;&quot;&quot;\n\nlabel_mapping = {\n    &quot;anger&quot;: 0,\n    &quot;fear&quot;: 1,\n    &quot;joy&quot;: 2,\n    &quot;sadness&quot;: 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[&quot;label&quot;])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[&quot;label&quot;]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = &quot;modelbest.pth&quot;\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\ntorch.save(model.state_dict(), model_path)\n\n</code></pre>\n<p>Proposed code:</p>\n<pre><code>import torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\nsentiments = [&quot;fear&quot;,&quot;joy&quot;,&quot;sadness&quot;,&quot;anger&quot;]\n\nnew_data = (\n    pl\n    .read_csv(\n        &quot;set2.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n    .filter(pl.col(&quot;label&quot;).is_in(sentiments))\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = &quot;modelbest.pth&quot;\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = &quot;new_moedl.pth&quot;\ntorch.save(model.state_dict(), new_best_model_path)\n</code></pre>\n<p>The dataset can be found <a href=\"https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "2024-08-30 17:47:59",
         "2",
         "271",
         "2",
         "78934212.0",
         "<p>use  pre-trained word embeddings like BertForSequenceClassification.  These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words.</p>\n<p><strong>Model Training with BERT</strong></p>\n<pre><code>import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(&quot;set1.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\ntrain_df = train_df.with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[&quot;label&quot;])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[&quot;label&quot;])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy: {results['eval_accuracy']:.4f}&quot;)\n\n# Save the model and tokenizer\nmodel.save_pretrained(&quot;saved_model&quot;)\ntokenizer.save_pretrained(&quot;saved_tokenizer&quot;)\n</code></pre>\n<p><strong>Incremental training with least effort</strong></p>\n<pre><code># Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(&quot;saved_model&quot;)\ntokenizer = BertTokenizer.from_pretrained(&quot;saved_tokenizer&quot;)\n\n# Load new data\nnew_data = (\n    pl.read_csv(&quot;set2.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n    .filter(pl.col(&quot;label&quot;).is_in([&quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;anger&quot;]))\n    .with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}&quot;)\n\n# Save the updated model\nmodel.save_pretrained(&quot;saved_model_incremental&quot;)\n</code></pre>\n",
         "3.0",
         ".fit_transform()\n---\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        \"set1.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n\"\"\"\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n\"\"\"\n\nlabel_mapping = {\n    \"anger\": 0,\n    \"fear\": 1,\n    \"joy\": 2,\n    \"sadness\": 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[\"label\"])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[\"label\"]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = \"modelbest.pth\"\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = \"text_classification_model.pth\"\ntorch.save(model.state_dict(), model_path)\n---\nimport torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = \"text_classification_model.pth\"\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\nsentiments = [\"fear\",\"joy\",\"sadness\",\"anger\"]\n\nnew_data = (\n    pl\n    .read_csv(\n        \"set2.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n    .filter(pl.col(\"label\").is_in(sentiments))\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = \"modelbest.pth\"\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = \"new_moedl.pth\"\ntorch.save(model.state_dict(), new_best_model_path)",
         "import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(\"set1.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\ntrain_df = train_df.with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[\"label\"])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[\"label\"])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")\n\n# Save the model and tokenizer\nmodel.save_pretrained(\"saved_model\")\ntokenizer.save_pretrained(\"saved_tokenizer\")\n---\n# Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"saved_model\")\ntokenizer = BertTokenizer.from_pretrained(\"saved_tokenizer\")\n\n# Load new data\nnew_data = (\n    pl.read_csv(\"set2.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n    .filter(pl.col(\"label\").is_in([\"fear\", \"joy\", \"sadness\", \"anger\"]))\n    .with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}\")\n\n# Save the updated model\nmodel.save_pretrained(\"saved_model_incremental\")",
         "Keep training pytorch model on new data",
         "Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here",
         "use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "Keep training pytorch model on new data Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "keep training pytorch model new data im working text classification task decided use pytorch model purpose process mainly involves following steps load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new data however every day need classify new comments correct wrong classifications currently approach add new comments correct classification dataset retrain entire model process timeconsuming new comments lost validation would like create new dataset newly classified texts continue training new data new comments classified manually label correct using gpt online code write desired process however im sure working expected im making silly mistakes happen mains questions could check propossed way solve problem work expect vectorizer face new tokens would loose original vectorizer full training process proposed code dataset found use pretrained word embeddings like bertforsequenceclassification embeddings handle unseen tokens gracefully since map words continuous vectors based semantic meaning reducing impact unseen words model training bert incremental training least effort",
         "keep train pytorch model new datum I m work text classification task decide use pytorch model purpose process mainly involve follow step load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new datum however every day need classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire model process timeconsume new comment lose validation would like create new dataset newly classify text continue train new datum new comment classify manually label correct use gpt online code write desire process however I m sure working expect I m make silly mistake happen main question could check proposse way solve problem work expect vectorizer face new token would loose original vectorizer full training process propose code dataset find use pretraine word embedding like bertforsequenceclassification embedding handle unseen token gracefully since map word continuous vector base semantic meaning reduce impact unseen word model train bert incremental training least effort"
        ],
        [
         "37",
         "198",
         "78932356",
         "Capitalized words in sentiment analysis",
         "<p>I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive .\nA common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .</p>\n<p>this is my current code :</p>\n<pre><code>from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)\n</code></pre>\n",
         "2024-08-30 13:49:56",
         "-1",
         "128",
         "1",
         "78933236.0",
         "<p>If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data.</p>\n<p>If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis.</p>\n<p>If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out <a href=\"https://github.com/EliasK93/transformer-model-comparison-for-review-sentiment-regression\" rel=\"nofollow noreferrer\">this repository</a> as an example.</p>\n",
         "0.0",
         "from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)",
         "",
         "Capitalized words in sentiment analysis",
         "Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code",
         "If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "Capitalized words in sentiment analysis Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "capitalized words sentiment analysis im currently working data customers reviews products sephora task classify sentiments negative neutral positive common technique text preprocessing lower case words situation upper case words like amazing hide significant emotion behind turning word lower case cause information loss would happy opinion subject still lower case words personally think creating classes distinction sentiments good good positive include importance upper case words current code using bertbased model llm actual classification would recommend use preprocessing least comes capitalization models pretrained nonpreprocessed data want kind analysis resulting labeled sentences could lowercase everything group ngrams simplify analysis thinking multiple classes better distinction prediction think would make sense switch sentiment regression instead classification predict value continuous range comes somewhat natural finetuning language models normal classification would take continuous output model map categorical classes using something like softmax needs skip last step directly use model output many python ml frameworks finetuning using language models classes regression tasks check repository example",
         "capitalize word sentiment analysis I m currently work datum customer review product sephora task classify sentiment negative neutral positive common technique text preprocesse low case word situation upper case word like amazing hide significant emotion behind turn word low case cause information loss would happy opinion subject still low case word personally think create class distinction sentiment good good positive include importance upper case word current code use bertbased model llm actual classification would recommend use preprocessing least come capitalization model pretraine nonpreprocesse datum want kind analysis result label sentence could lowercase everything group ngram simplify analysis think multiple class well distinction prediction think would make sense switch sentiment regression instead classification predict value continuous range come somewhat natural finetune language model normal classification would take continuous output model map categorical class use something like softmax need skip last step directly use model output many python ml framework finetune use language model class regression task check repository example"
        ],
        [
         "38",
         "202",
         "78920095",
         "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub'",
         "<p>I've been using LLAMA 2 for research for a few months now and I import as follows:</p>\n<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(&quot;cuda&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;,torch_dtype=&quot;auto&quot;)\nmodel = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;, torch_dtype=&quot;auto&quot;, load_in_4bit=True)\n</code></pre>\n<p>It has always worked. However, today it is showing the following error:\n<strong>RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/<strong>init</strong>.py)</strong></p>\n<p>Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.</p>\n",
         "2024-08-27 17:20:42",
         "1",
         "5871",
         "1",
         "78920098.0",
         "<p>The error you're encountering is due to the <code>split_torch_state_dict_into_shards</code> function not being available in <code>huggingface-hub version &lt; 0.23.0</code>.</p>\n<p>This function is included starting from version <code>0.23.0</code>.</p>\n<p><strong>To resolve this issue, update the <code>huggingface-hub</code> library to version 0.23.0 or later</strong></p>\n<p>Also please install accelerate:</p>\n<pre><code>pip install accelerate==0.31.0\n</code></pre>\n<p>here is a git link: <strong><a href=\"https://github.com/run-llama/llama_index/discussions/14605\" rel=\"nofollow noreferrer\">https://github.com/run-llama/llama_index/discussions/14605</a></strong></p>\n",
         "4.0",
         "from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\",torch_dtype=\"auto\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\", torch_dtype=\"auto\", load_in_4bit=True)",
         "split_torch_state_dict_into_shards\n---\nhuggingface-hub version < 0.23.0\n---\n0.23.0\n---\nhuggingface-hub\n---\npip install accelerate==0.31.0",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub",
         "Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook",
         "The error youre encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook The error youre encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "import name split_torch_state_dict_into_shards huggingface_hub ive using llama 2 research months import follows always worked however today showing following error runtimeerror failed import transformersmodelsllamamodeling_llama following error look see traceback failed import transformersgenerationutils following error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py recreated hugging face token didnt work using google colab kaggle notebook error youre encountering due function available function included starting version resolve issue update library version 0230 later also please install accelerate git link",
         "import name split_torch_state_dict_into_shard huggingface_hub I ve use llama 2 research month import follows always work however today show follow error runtimeerror fail import transformersmodelsllamamodeling_llama follow error look see traceback fail import transformersgenerationutil follow error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt / conda / lib / python310 / sitepackage / huggingface_hub/ init py recreate hug face token do not work use google colab kaggle notebook error you re encounter due function available function include start version resolve issue update library version 0230 later also please install accelerate git link"
        ],
        [
         "39",
         "205",
         "78917743",
         "How to Process Data on GPU Instead of RAM for This Python Code?",
         "<p>I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance.\nmy code :</p>\n<pre><code>def prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n    batch[&quot;input_features&quot;] = feature_extractor(\n        audio[&quot;array&quot;], \n        sampling_rate=audio[&quot;sampling_rate&quot;]\n    ).input_features[0]\n    batch[&quot;labels&quot;] = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[&quot;train&quot;], \n    num_proc=1\n)\n</code></pre>\n<p>How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!</p>\n",
         "2024-08-27 08:03:28",
         "1",
         "61",
         "1",
         "78918851.0",
         "<p>you can using the following code to process audio data on GPU</p>\n<pre><code>import torch\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n\n    input_features = feature_extractor(audio[&quot;array&quot;], sampling_rate=audio[&quot;sampling_rate&quot;]).input_features[0]\n    batch[&quot;input_features&quot;] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    batch[&quot;labels&quot;] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[&quot;train&quot;])\n</code></pre>\n",
         "2.0",
         "def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n    batch[\"input_features\"] = feature_extractor(\n        audio[\"array\"], \n        sampling_rate=audio[\"sampling_rate\"]\n    ).input_features[0]\n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[\"train\"], \n    num_proc=1\n)",
         "import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    input_features = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n    batch[\"input_features\"] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[\"sentence\"]).input_ids\n    batch[\"labels\"] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"])",
         "How to Process Data on GPU Instead of RAM for This Python Code",
         "Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated",
         "you can using the following code to process audio data on GPU",
         "How to Process Data on GPU Instead of RAM for This Python Code Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated you can using the following code to process audio data on GPU",
         "process data gpu instead ram python code im currently using following code process audio data runs ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific changes much appreciated using following code process audio data gpu",
         "process datum gpu instead ram python code I m currently use follow code process audio data run ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific change much appreciate use follow code process audio data gpu"
        ],
        [
         "40",
         "208",
         "78912171",
         "How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation",
         "<p>I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this.</p>\n<p><strong>What I’ve Tried:</strong></p>\n<ul>\n<li><p><strong>Initial Attempt:</strong> I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me.</p>\n</li>\n<li><p><strong>Using Forward Hooks:</strong> To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation:</p>\n</li>\n</ul>\n<pre class=\"lang-py prettyprint-override\"><code># VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = &quot;Helsinki-NLP/opus-mt-en-de&quot;\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = &quot;Please translate this to German.&quot;\ninputs = tokenizer(input_text, return_tensors=&quot;pt&quot;)\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(&quot;layers, heads, output tokens, input tokens&quot;)\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(&quot;Input Tokens&quot;)\nplt.ylabel(&quot;Output Tokens&quot;)\nplt.title(&quot;Cross-Attention Matrix&quot;)\nplt.show()\n</code></pre>\n<p>This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation.</p>\n<p><strong>My Question</strong></p>\n<p>Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8?</p>\n<p>I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.</p>\n",
         "2024-08-25 20:13:54",
         "1",
         "380",
         "1",
         "78915504.0",
         "<p>Huggingface has built in methods to return attention weights</p>\n<pre class=\"lang-py prettyprint-override\"><code>translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n&gt; odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n</code></pre>\n<p>With <code>return_dict_in_generate=True</code>, <code>model.generate</code> returns a dict-like object. With <code>output_attentions=True</code>, the output dict will contain all attention weights.</p>\n<p>For this model, it will include encoder attentions, decoder attentions and cross attentions.</p>\n",
         "3.0",
         "# VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = \"Helsinki-NLP/opus-mt-en-de\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = \"Please translate this to German.\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(\"layers, heads, output tokens, input tokens\")\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(\"Input Tokens\")\nplt.ylabel(\"Output Tokens\")\nplt.title(\"Cross-Attention Matrix\")\nplt.show()",
         "translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n> odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n---\nreturn_dict_in_generate=True\n---\nmodel.generate\n---\noutput_attentions=True",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation",
         "I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this",
         "Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "visualize crossattention matrices marianmtmodel output generation working machine translation task using marianmtmodel hugging face transformers library specifically want visualize crossattention matrices models translation process however encountered difficulties achieving ive tried initial attempt noticed crossattention matrices directly returned model generates translation example found involved feeding source text translation model however goal access crossattention matrices model generates output translation given using forward hooks achieve implemented forward hooks key query projections attention mechanism disabling keyvalue caching use_cache=false capture full matrices last step heres implementation approach seemed work capturing crossattention matrices however observed matrices 4 attention heads instead expected 8 makes question correctness implementation question given issues ive encountered reliable method extract visualize crossattention matrices translation process additionally current approach fundamentally okay resolve issue capturing 4 attention heads instead 8 suspect issue might related im currently reshaping key k query q tensors head dimension multiplication wanted ask advice case theres easier effective way huggingface built methods return attention weights returns dictlike object output dict contain attention weights model include encoder attentions decoder attentions cross attentions",
         "visualize crossattention matrix marianmtmodel output generation work machine translation task use marianmtmodel hug face transformer library specifically want visualize crossattention matrix model translation process however encounter difficulty achieve I ve try initial attempt notice crossattention matrix directly return model generate translation example find involved feeding source text translation model however goal access crossattention matrix model generate output translation give use forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache use_cache = false capture full matrix last step here implementation approach seem work capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question give issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate I m currently reshape key k query q tensor head dimension multiplication want ask advice case there s easy effective way huggingface build method return attention weight return dictlike object output dict contain attention weight model include encoder attention decoder attention cross attention"
        ],
        [
         "41",
         "212",
         "78905614",
         "Why doesn't permuting positional encodings in BERT affect the output as expected?",
         "<p>I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another <a href=\"https://stackoverflow.com/questions/78902301/why-doesnt-permuting-positional-encodings-in-gpt-2-affect-the-output-as-expecte/78903454#78903454\">question</a> I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results.</p>\n<p><strong>What I expected to happen:</strong></p>\n<ul>\n<li>No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary.</li>\n<li>Permuting only the input IDs should return distribution B.</li>\n<li>Permuting only the positional embeddings should return distribution B.</li>\n<li>Permuting both the input IDs and positional embeddings should return distribution A.</li>\n</ul>\n<p><strong>What actually happens:</strong>\nSometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.</p>\n<p><strong>My question is:</strong> Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding?</p>\n<p>For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in <code>masked_prediction</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(&quot;transformers.modeling_utils&quot;).setLevel(logging.ERROR)\nwarnings.simplefilter(&quot;ignore&quot;, FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) &gt; 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(&quot;You can't use an empty sequence for prediction&quot;)\n            return\n        \n        model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(&quot;You need to include a [MASK] token for prediction&quot;)\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=&quot;Run Prediction&quot;)\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)\n</code></pre>\n",
         "2024-08-23 11:12:08",
         "1",
         "74",
         "1",
         "78906902.0",
         "<p>The model inputs have token ids and position ids. There are four scenarios to consider:</p>\n<ol>\n<li>Baseline. Correct order for tokens and positions</li>\n<li>Permute position ids only</li>\n<li>Permute token ids only</li>\n<li>Permute position ids and token ids</li>\n</ol>\n<p>You are correct that scenario 1 and 4 should produce the same results. However you are incorrect in assuming that permuting tokens or positions separately should give the same result. Consider:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n</code></pre>\n<p>In <code>Ex1</code>, the model is told that token <code>2</code> occurs at position <code>0</code>. In <code>Ex2</code>, the model is told that token <code>2</code> occurs at position <code>1</code>. Even though we used the same permutation, the mapping of tokens to positions is different. This results in different model outputs.</p>\n<p>The reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. This is luck - the average case produces different results.</p>\n<p>It is simple to test this. Huggingface models take a <code>position_ids</code> input parameter. We can use this to test permutations of the input ids without messing with the weight matrices.</p>\n<p>To test this, we'll create input data, permute as needed, compute logits and compare logits.</p>\n<p>When comparing logits, we will permute or depermute as needed to compare on a token to token basis. For example if token <code>i</code> in scenario 1 is permuted to token <code>j</code> in scenario 3, we want to compare logits <code>i</code> from scenario 1 to logits <code>j</code> in scenario 3.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f&quot;Comparison {sa}, {sb}: {val.item():.6f}&quot;)\n</code></pre>\n<p>The code should produce an output like:</p>\n<pre><code>Comparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n</code></pre>\n<p>Run the code a bunch of times. You will find that the <code>S1, S4</code> comparison <em>always</em> has a small deviation. This is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues.</p>\n<p>You will find the <code>S2, S3</code> comparison generally has a large deviation, but <em>sometimes</em> has a small deviation. As discussed, this is due to getting a lucky permutation where positions and ids mostly line up.</p>\n",
         "2.0",
         "masked_prediction\n---\nimport torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) > 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(\"You can't use an empty sequence for prediction\")\n            return\n        \n        model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(\"You need to include a [MASK] token for prediction\")\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=\"Run Prediction\")\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)",
         "# Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n---\nEx1\n---\n2\n---\n0\n---\nEx2\n---\n2\n---\n1\n---\nposition_ids\n---\ni\n---\nj\n---\ni\n---\nj\n---\nimport torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f\"Comparison {sa}, {sb}: {val.item():.6f}\")\n---\nComparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n---\nS1, S4\n---\nS2, S3",
         "Why doesnt permuting positional encodings in BERT affect the output as expected",
         "I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in",
         "The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this well create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "Why doesnt permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this well create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "doesnt permuting positional encodings bert affect output expected working jupyter notebook transformers section positional encodings want demonstrate transformer relies entirely positional encoding understand order sequence previously learned another question posted concept applies models dont use masked attention like gpt2 however attempted approach bert model uses crossattention predict mask token encountered unexpected results expected happen permutation cause model predict different token ie distribution consistent vocabulary permuting input ids return distribution b permuting positional embeddings return distribution b permuting input ids positional embeddings return distribution actually happens sometimes results align expectations times permuting one aspect either input ids positional embeddings leads different outcomes even though occasionally produce result question something else hugging faces bert model might influenced position beyond positional encoding completeness included full code part notebook tried directly important part happens model inputs token ids position ids four scenarios consider baseline correct order tokens positions permute position ids permute token ids permute position ids token ids correct scenario 1 4 produce results however incorrect assuming permuting tokens positions separately give result consider model told token occurs position model told token occurs position even though used permutation mapping tokens positions different results different model outputs reason sometimes see results line random chance sample permutation results token/position embeddings lining way mostly way permuting one luck average case produces different results simple test huggingface models take input parameter use test permutations input ids without messing weight matrices test well create input data permute needed compute logits compare logits comparing logits permute depermute needed compare token token basis example token scenario 1 permuted token scenario 3 want compare logits scenario 1 logits scenario 3 code produce output like run code bunch times find comparison always small deviation permuting tokens positions together always produces result ignoring small deviations caused numeric issues find comparison generally large deviation sometimes small deviation discussed due getting lucky permutation positions ids mostly line",
         "do not permute positional encoding bert affect output expect work jupyter notebook transformer section positional encoding want demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply model do not use mask attention like gpt2 however attempt approach bert model use crossattention predict mask token encounter unexpected result expect happen permutation cause model predict different token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes result align expectation time permute one aspect either input ids positional embedding lead different outcome even though occasionally produce result question something else hug face bert model might influence position beyond positional encoding completeness include full code part notebook try directly important part happen model input token id position id four scenario consider baseline correct order token position permute position ids permute token ids permute position ids token ids correct scenario 1 4 produce result however incorrect assuming permute tokens position separately give result consider model tell token occur position model tell token occur position even though use permutation mapping token position different result different model output reason sometimes see result line random chance sample permutation result token / position embedding line way mostly way permute one luck average case produce different result simple test huggingface model take input parameter use test permutation input id without mess weight matrix test well create input datum permute need compute logit compare logit compare logit permute depermute need compare token token basis example token scenario 1 permute token scenario 3 want compare logits scenario 1 logits scenario 3 code produce output like run code bunch time find comparison always small deviation permute token position together always produce result ignore small deviation cause numeric issue find comparison generally large deviation sometimes small deviation discuss due get lucky permutation position id mostly line"
        ],
        [
         "42",
         "214",
         "78901998",
         "How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus?",
         "<p>I'm working with the <code>OpenAIEmbeddings()</code> class from <code>OpenAI</code>, which uses the <code>text-embedding-3-small</code> model. According to the <a href=\"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\" rel=\"nofollow noreferrer\">documentation</a>, it generates a 1536-dimensional vector for any input text.</p>\n<p>However, I'm a bit confused about how this works:</p>\n<ul>\n<li>Is the 1536-dimensional vector generated for the entire input text?</li>\n<li>If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</li>\n</ul>\n<p><strong>I was expecting this:</strong></p>\n<p>If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536.</p>\n<p>But the output is a single vector of size 1536 for the whole input text.</p>\n<p>Why I expected this?</p>\n<p>Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings?</p>\n<p>I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input.</p>\n<p>Any insights or examples would be greatly appreciated!</p>\n",
         "2024-08-22 14:09:25",
         "2",
         "580",
         "1",
         "78902136.0",
         "<p>Everything you described is 100% expected.</p>\n<h3>Q: Is the 1536-dimensional vector generated for the entire input text?</h3>\n<p>A: Yes.</p>\n<h3>Q: If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</h3>\n<p>A: First, the OpenAI Embeddings model doesn't handle a single word any different than a long text. For the model, it's an input. The input can be even a single character (e.g., &quot;a&quot;), but it doesn't make sense to calculate an embedding vector out of it since &quot;a&quot; doesn't semantically mean anything to us humans.</p>\n<p>Second, what you probably meant with this question is what happens when you do a similarity search with these embeddings. In other words, what happens when you <em>use</em> them? What happens if you use embeddings of words, sentences, paragraphs, or the whole text? Does it matter? Yes!</p>\n<p>This is called chunking. The decision about how to chunk your text depends on the use case. The best thing is probably to simply try and see. If you get meaningful results after doing a similarity search, then this means that chunking is appropriate (even if this means chunking the whole text). If you don't get meaningful results after doing a similarity search, then this means that chunking isn't appropriate (e.g., instead of chunking by paragraph, try chunking by sentences).</p>\n<p>There's an excellent Stack Overflow <a href=\"https://stackoverflow.blog/2024/06/06/breaking-up-is-hard-to-do-chunking-in-rag-applications/\">blog post</a> about this topic you should read (pay attention to the bolded text because this is the best explanation):</p>\n<blockquote>\n<p>With RAG, you create text embeddings of the pieces of data that you\nwant to draw from and retrieve. That allows you to place a piece of\nthe source text within the semantic space that LLMs use to create\nresponses.</p>\n<p>/.../</p>\n<p>When it comes to RAG systems, you’ll need to pay special attention to\nhow big the individual pieces of data are. How you divide your data up\nis called chunking, and it’s more complex than embedding whole\ndocuments.</p>\n<p>/.../</p>\n<p>The size of the chunked data is going to make a huge difference in\nwhat information comes up in a search. When you embed a piece of data,\nthe whole thing is converted into a vector. <strong>Include too much in a\nchunk and the vector loses the ability to be specific to anything it\ndiscusses. Include too little and you lose the context of the data.</strong></p>\n</blockquote>\n",
         "3.0",
         "OpenAIEmbeddings()\n---\nOpenAI\n---\ntext-embedding-3-small",
         "",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus",
         "Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated",
         "Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesnt handle a single word any different than a long text For the model its an input The input can be even a single character eg a but it doesnt make sense to calculate an embedding vector out of it since a doesnt semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you dont get meaningful results after doing a similarity search then this means that chunking isnt appropriate eg instead of chunking by paragraph try chunking by sentences Theres an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesnt handle a single word any different than a long text For the model its an input The input can be even a single character eg a but it doesnt make sense to calculate an embedding vector out of it since a doesnt semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you dont get meaningful results after doing a similarity search then this means that chunking isnt appropriate eg instead of chunking by paragraph try chunking by sentences Theres an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "openaiembeddings work creating single vector size 1536 whole text corpus im working class uses model according documentation generates 1536dimensional vector input text however im bit confused works 1536dimensional vector generated entire input text 1536dimensional vector represents entire input text model handle individual words versus longer texts like sentences paragraphs expecting 100 words input text expected openaiembeddings would output 100 vectors size 1536 output single vector size 1536 whole input text expected learning ive understood embeddings like word2vec glove provide vectors word corpus differ approach taken openaiembeddings im trying understand whether theres way extract embeddings individual words using model output always single vector representing whole input insights examples would greatly appreciated everything described 100 % expected q 1536dimensional vector generated entire input text yes q 1536dimensional vector represents entire input text model handle individual words versus longer texts like sentences paragraphs first openai embeddings model doesnt handle single word different long text model input input even single character eg doesnt make sense calculate embedding vector since doesnt semantically mean anything us humans second probably meant question happens similarity search embeddings words happens use happens use embeddings words sentences paragraphs whole text matter yes called chunking decision chunk text depends use case best thing probably simply try see get meaningful results similarity search means chunking appropriate even means chunking whole text dont get meaningful results similarity search means chunking isnt appropriate eg instead chunking paragraph try chunking sentences theres excellent stack overflow blog post topic read pay attention bolded text best explanation rag create text embeddings pieces data want draw retrieve allows place piece source text within semantic space llms use create responses // comes rag systems youll need pay special attention big individual pieces data divide data called chunking complex embedding whole documents // size chunked data going make huge difference information comes search embed piece data whole thing converted vector include much chunk vector loses ability specific anything discusses include little lose context data",
         "openaiembedding work create single vector size 1536 whole text corpus I m work class use model accord documentation generate 1536dimensional vector input text however I m bit confused work 1536dimensional vector generate entire input text 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph expect 100 word input text expect openaiembedding would output 100 vector size 1536 output single vector size 1536 whole input text expect learning I ve understand embedding like word2vec glove provide vector word corpus differ approach take openaiembedding I m try understand whether there s way extract embedding individual word use model output always single vector represent whole input insight example would greatly appreciate everything describe 100 % expect q 1536dimensional vector generate entire input text yes q 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph first openai embedding model do not handle single word different long text model input input even single character eg do not make sense calculate embed vector since do not semantically mean anything we human second probably mean question happen similarity search embedding word happen use happen use embedding word sentence paragraph whole text matter yes call chunk decision chunk text depends use case good thing probably simply try see get meaningful result similarity search mean chunk appropriate even mean chunk whole text do not get meaningful result similarity search mean chunk be not appropriate eg instead chunk paragraph try chunk sentence there s excellent stack overflow blog post topic read pay attention bolde text good explanation rag create text embedding piece datum want draw retrieve allow place piece source text within semantic space llm use create response // come rag system you ll need pay special attention big individual piece datum divide datum call chunk complex embed whole document // size chunk datum go make huge difference information come search embed piece datum whole thing convert vector include much chunk vector lose ability specific anything discuss include little lose context datum"
        ],
        [
         "43",
         "217",
         "78895710",
         "NER versus LLM to extract name, gender, role and company from text",
         "<p>I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons.</p>\n<p>I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them.</p>\n<p>Is there another, smaller LLM that might be good at this while using fewer processing resources?</p>\n<p>Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.)</p>\n<p>Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass?</p>\n<p>Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction.</p>\n<p>Thanks in advance!</p>\n",
         "2024-08-21 07:39:13",
         "1",
         "1524",
         "2",
         "78896098.0",
         "<p>Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. <code>NER</code> is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use <code>NLTK</code> or <code>SpaCy</code> for this matter. My personal choice is <code>SpaCy</code>, however a <code>gender</code> as you defined is not a known named entity. you can see a list of named entities in <a href=\"https://github.com/explosion/spaCy/discussions/9147\" rel=\"nofollow noreferrer\">this doc</a>.</p>\n<p>I guess what you mean by <code>gender</code> is the possible <code>gender</code> associated with the names of a <code>PERSON</code> mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use <a href=\"https://pypi.org/project/gender-guesser/\" rel=\"nofollow noreferrer\"><code>gender-guesser</code> package</a>.</p>\n<p>A possible solution would be like this:</p>\n<pre><code>import spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == &quot;PERSON&quot;:\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n</code></pre>\n<p>Note that <code>en_core_web_sm</code> is the small model available via spaCy, you can use the large model by specifying <code>en_core_web_lg</code>, just make sure that the model is downloaded before running your code. here's how you can download the model:</p>\n<pre><code>python -m spacy download en_core_web_sm\n</code></pre>\n",
         "1.0",
         "",
         "NER\n---\nNLTK\n---\nSpaCy\n---\nSpaCy\n---\ngender\n---\ngender\n---\ngender\n---\nPERSON\n---\ngender-guesser\n---\nimport spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == \"PERSON\":\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n---\nen_core_web_sm\n---\nen_core_web_lg\n---\npython -m spacy download en_core_web_sm",
         "NER versus LLM to extract name gender role and company from text",
         "I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance",
         "Apart from your limitations I wouldnt recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code heres how you can download the model",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance Apart from your limitations I wouldnt recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code heres how you can download the model",
         "ner versus llm extract name gender role company text need extract name gender job title employer/company name newspaper articles running process local hardware cloud allowed due copyright reasons ive playing around llama 31 im finding dont get useable results models smaller 70b parameters size models run much slowly best hardware throw another smaller llm might good using fewer processing resources ner use extract data ners ive looked extract name gender dont know extract data gender showstopper alternatively approach take first pass ner pass names llm together original newspaper article extract data get better results faster single llm pass answer training model good model use starting point im much beginning machine learning journey would love pointed right direction thanks advance apart limitations wouldnt recommend using llms like llamma 31 task one classic tasks nlp smaller language models tools incorporate achieve goal use matter personal choice however defined known named entity see list named entities doc guess mean possible associated names mentioned articles python packages use lookup genders however note ambiguous substantial tolerance error use package possible solution would like note small model available via spacy use large model specifying make sure model downloaded running code heres download model",
         "ner versus llm extract name gender role company text need extract name gender job title employer / company name newspaper article running process local hardware cloud allow due copyright reason I ve play around llama 31 I m find do not get useable result model small 70b parameter size model run much slowly good hardware throw another small llm might good use few processing resource ner use extract data ner I ve look extract name gender do not know extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get well result fast single llm pass answer training model good model use start point I m much begin machine learn journey would love point right direction thank advance apart limitation would not recommend use llm like llamma 31 task one classic task nlp small language model tool incorporate achieve goal use matter personal choice however define known name entity see list name entity doc guess mean possible associate name mention article python package use lookup gender however note ambiguous substantial tolerance error use package possible solution would like note small model available via spacy use large model specify make sure model download run code here download model"
        ],
        [
         "44",
         "219",
         "78887743",
         "Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask?",
         "<p>In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%).</p>\n<p>I am curious about the following:</p>\n<p>When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively?\nDoes the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact?</p>\n<p>Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%?</p>\n<p>Thank you for your insights!</p>\n",
         "2024-08-19 11:49:06",
         "1",
         "525",
         "1",
         "78890409.0",
         "<p>Attention is computed on a tensor of shape <code>(batch_size, sequence_length, embedding_dimension)</code>. The compute and memory requirements scale with the size of those dimensions.</p>\n<p>For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size.</p>\n<p>With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length <code>8</code> and 10 sequences of length <code>128</code>. Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of <code>128</code>. If you sort by length before batching, you get one batch with sequence length of <code>8</code> and another with length <code>128</code>. The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128).</p>\n<p>That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to &quot;skip over&quot; padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.</p>\n",
         "2.0",
         "",
         "(batch_size, sequence_length, embedding_dimension)\n---\n8\n---\n128\n---\n128\n---\n8\n---\n128",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask",
         "In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights",
         "Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding youre not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you arent going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesnt work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding youre not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you arent going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesnt work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "padding batch sequences affect performance effective attention mask transformer models sequences variable lengths typically padded maximum length batch however sequence lengths vary batch may contain substantial amount padding potentially 50 % curious following pytorch computes transformer padding tokens impact calculation speed negatively presence attention mask allow model effectively skip padding tokens resulting minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero values computation effectively reduce approximately 10 % thank insights attention computed tensor shape compute memory requirements scale size dimensions input fixed size percent padding impact performance minor overhead applying padding mask ie padding mask saves one mask fill operation x % padding % padding youre going see difference overall compute requirements set tensor size respect batching sequences added inefficiencies batching together sequences different length say 10 sequences length 10 sequences length pad batch sequences two batches mix lengths evenly get two batches sequence length sort length batching get one batch sequence length another length first case two batches sequence length 128 requires overall compute compared second case one batch 8 one 128 said fixed input size arent going see performance change percent padding way attention operation skip padding tokens conditional control flow required sort approach doesnt work well way gpus execute operations parallel effect padding mask assigns 0 attention weight padding tokens",
         "padding batch sequence affect performance effective attention mask transformer model sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 % curious follow pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow model effectively skip padding token result minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero value computation effectively reduce approximately 10 % thank insight attention compute tensor shape compute memory requirement scale size dimension input fix size percent padding impact performance minor overhead apply padding mask ie padding mask save one mask fill operation x % padding % padding you re go see difference overall compute requirement set tensor size respect batching sequence add inefficiency batch together sequence different length say 10 sequence length 10 sequence length pad batch sequence two batch mix length evenly get two batch sequence length sort length batching get one batch sequence length another length first case two batch sequence length 128 require overall compute compare second case one batch 8 one 128 say fix input size be not go see performance change percent padding way attention operation skip padding token conditional control flow require sort approach do not work well way gpu execute operation parallel effect padding mask assign 0 attention weight padding token"
        ],
        [
         "45",
         "228",
         "78865486",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text",
         "<p>Using the following Matcher rule:</p>\n<pre><code>{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }\n</code></pre>\n<p>on the text: 'MyLabel: Some Value'</p>\n<p>I get <strong>two</strong> matches: 'MyLabel' and 'MyLabel:'</p>\n<p>For me, that was quite surprising - I was expecting a single match on 'MyLabel:'.\nAdding the new greedy flag didn't make any difference.</p>\n<ul>\n<li>Is this the intended behavior or is it a bug?</li>\n<li>How should I determine that the second match really is just a subset of the first match?</li>\n<li>Will the shorter match always be reported before the longer match?</li>\n</ul>\n<p>SpaCy version 3.7.5</p>\n",
         "2024-08-13 09:37:23",
         "1",
         "34",
         "1",
         "78870921.0",
         "<p>i will say that the behavior you're observing with the SpaCy <code>Matcher</code> is expected, and it is not a bug. When you use the <code>{'TEXT': ':', 'OP': '?'}</code> pattern, the <code>OP: '?'</code> operator means that the colon is optional, so the matcher will generate both the shorter and the longer match, as you've seen.</p>\n<h3>Explanation:</h3>\n<ul>\n<li><strong>Pattern</strong>: <code>{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}</code>.</li>\n<li><strong>Text</strong>: <code>'MyLabel: Some Value'</code>.</li>\n</ul>\n<p>So for this pattern, SpaCy  will try to match:</p>\n<ol>\n<li><code>'MyLabel'</code> alone (because the colon is optional).</li>\n<li><code>'MyLabel:'</code> (because the colon can be included).</li>\n</ol>\n<p>Therefore, you will get two matches: <code>'MyLabel'</code> and <code>'MyLabel:'</code>.</p>\n<h3>Now to  Answer Your Questions:</h3>\n<ol>\n<li><p><strong>Is this the intended behavior or is it a bug?</strong></p>\n<ul>\n<li>This is intended behavior. The <code>OP: '?'</code> operator allows the colon to be optionally matched, leading to multiple matches.</li>\n</ul>\n</li>\n<li><p><strong>How should I determine that the second match really is just a subset of the first match?</strong></p>\n<ul>\n<li>To determine if one match is a subset of another, you can compare the start and end indices of the matches. The longer match will have the same start index but a different end index. Now i wrote a code below even using spacy version 3.7.5, see details below</li>\n</ul>\n</li>\n</ol>\n<pre><code>pip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n</code></pre>\n<p>Now Example in code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\ndoc = nlp(&quot;MyLabel: Some Value&quot;)\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(&quot;R-1&quot;, [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f&quot;Match: {span.text}, Start: {start}, End: {end}&quot;)\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start &gt;= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f&quot;Filtered Match: {span.text}&quot;)\n</code></pre>\n<p>Now, This code will filter out the shorter match and your output will be</p>\n<pre><code>Match: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n\n</code></pre>\n<ol start=\"3\">\n<li><strong>Now Will the shorter match always be reported before the longer match?</strong>\n<ul>\n<li>I don't think the matches are not guaranteed to be reported in a specific order. so to handle this, you can sort the matches by their start and end indices as shown in the code example above.Now, After sorting, you can now filter out matches that are subsets of longer matches.</li>\n</ul>\n</li>\n</ol>\n<h3>Another Alternative Solution:</h3>\n<p>If you want to ensure that only the longest match is returned, you can change the way you define the pattern:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n</code></pre>\n<p>note that the <code>greedy</code> flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings.</p>\n<h3>Now back to the Summary of what i explained:</h3>\n<ul>\n<li>The behavior you're seeing is by design, due to the optional <code>OP: '?'</code> operator.</li>\n<li>in addition, you can filter out the shorter match by comparing start and end indices of the matches.</li>\n<li>furthermore, Sorting the matches by start and end indices allows you to keep only the longest, non-overlapping matches.</li>\n</ul>\n",
         "1.0",
         "{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }",
         "Matcher\n---\n{'TEXT': ':', 'OP': '?'}\n---\nOP: '?'\n---\n{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}\n---\n'MyLabel: Some Value'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\nOP: '?'\n---\npip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"MyLabel: Some Value\")\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(\"R-1\", [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f\"Match: {span.text}, Start: {start}, End: {end}\")\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start >= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f\"Filtered Match: {span.text}\")\n---\nMatch: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n---\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n---\ngreedy\n---\nOP: '?'",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text",
         "Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375",
         "i will say that the behavior youre observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as youve seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I dont think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesnt change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior youre seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375 i will say that the behavior youre observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as youve seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I dont think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesnt change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior youre seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "spacy matcher optional suffix pattern reports multiple matches text using following matcher rule text mylabel value get two matches mylabel mylabel quite surprising expecting single match mylabel adding new greedy flag didnt make difference intended behavior bug determine second match subset first match shorter match always reported longer match spacy version 375 say behavior youre observing spacy expected bug use pattern operator means colon optional matcher generate shorter longer match youve seen explanation pattern text pattern spacy try match alone colon optional colon included therefore get two matches answer questions intended behavior bug intended behavior operator allows colon optionally matched leading multiple matches determine second match subset first match determine one match subset another compare start end indices matches longer match start index different end index wrote code even using spacy version 375 see details example code code filter shorter match output shorter match always reported longer match dont think matches guaranteed reported specific order handle sort matches start end indices shown code example abovenow sorting filter matches subsets longer matches another alternative solution want ensure longest match returned change way define pattern note flag doesnt change behavior matching rather influence overlaps handled certain custom settings back summary explained behavior youre seeing design due optional operator addition filter shorter match comparing start end indices matches furthermore sorting matches start end indices allows keep longest nonoverlapping matches",
         "spacy matcher optional suffix pattern report multiple match text use follow matcher rule text mylabel value get two match mylabel mylabel quite surprising expect single match mylabel add new greedy flag do not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375 say behavior you re observe spacy expect bug use pattern operator mean colon optional matcher generate shorter long match you ve see explanation pattern text pattern spacy try match alone colon optional colon include therefore get two match answer question intend behavior bug intend behavior operator allow colon optionally match lead multiple match determine second match subset first match determine one match subset another compare start end index match long match start index different end index write code even use spacy version 375 see detail example code code filter short match output short match always report long match do not think match guarantee report specific order handle sort match start end index show code example abovenow sort filter match subset long match another alternative solution want ensure long match return change way define pattern note flag do not change behavior match rather influence overlaps handle certain custom setting back summary explain behavior you re see design due optional operator addition filter short match compare start end index match furthermore sort match start end index allow keep long nonoverlapping match"
        ],
        [
         "46",
         "230",
         "78862691",
         "`mlflow.transformers.log_model()` does not finish",
         "<h3>Problem</h3>\n<p>I want to use <code>mlflow.transformers.log_model()</code> to log a finetuned huggingface model.</p>\n<p><strong>However, when the <code>mlflow.transformers.log_model</code> method is running, it simply does not finish - runs forever - throws no errors.</strong></p>\n<p>I suspect my configuration is not right, the model is too big?\nThe output says <code>Skipping saving pretrained model weights to disk</code> so that should not be the problem.</p>\n<p>Any ideas how to do this properly?</p>\n<h3>Example</h3>\n<p>This is more or less how my setup looks like, you cannot run this, it includes some pseudocode...</p>\n<p>I am on python 3.11.9 with <code>transformers = &quot;^4.41.2&quot;</code> &amp; <code>mlflow = &quot;^2.15.1&quot;</code>.</p>\n<pre><code>import mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = &quot;LeoLM/leo-hessianai-7b-chat-bilingual&quot;\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=&quot;auto&quot;,\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n<p>The last output I get in the console is:</p>\n<pre><code>INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(&quot;Setuptools is replacing distutils.&quot;)\n</code></pre>\n",
         "2024-08-12 16:27:32",
         "0",
         "337",
         "1",
         "78877979.0",
         "<p>Before defining the trainer, the model has be turned into a Peft model object via <code>get_peft_model</code>, then the <code>mlflow.transformers.log_model</code> works:</p>\n<pre><code>from peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n",
         "0.0",
         "mlflow.transformers.log_model()\n---\nmlflow.transformers.log_model\n---\nSkipping saving pretrained model weights to disk\n---\ntransformers = \"^4.41.2\"\n---\nmlflow = \"^2.15.1\"\n---\nimport mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = \"LeoLM/leo-hessianai-7b-chat-bilingual\"\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )\n---\nINFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")",
         "get_peft_model\n---\nmlflow.transformers.log_model\n---\nfrom peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )",
         "`mlflowtransformerslog_model` does not finish",
         "Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is",
         "Before defining the trainer the model has be turned into a Peft model object via then the works",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is Before defining the trainer the model has be turned into a Peft model object via then the works",
         "` mlflowtransformerslog_model ` finish problem want use log finetuned huggingface model however method running simply finish runs forever throws errors suspect configuration right model big output says problem ideas properly example less setup looks like run includes pseudocode python 3119 & last output get console defining trainer model turned peft model object via works",
         "` mlflowtransformerslog_model ` finish problem want use log finetune huggingface model however method run simply finish run forever throw error suspect configuration right model big output say problem idea properly example less setup look like run include pseudocode python 3119 & last output get console define trainer model turn peft model object via work"
        ],
        [
         "47",
         "234",
         "78853409",
         "NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation)",
         "<p>I'm attempting to fine-tune the NLLB model <code>&quot;facebook/nllb-200-distilled-600M&quot;</code> for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb.</p>\n<p>Documentation: <a href=\"https://github.com/facebookresearch/fairseq/tree/nllb?tab=readme-ov-file\" rel=\"nofollow noreferrer\">link</a></p>\n<p>This is the code block which is giving error:</p>\n<pre><code>DATA_CONFIG = &quot;/content/sample_data/data_config.json&quot;\nOUTPUT_DIR = &quot;/content/outputs&quot;\nMODEL_FOLDER = &quot;/content/drive/MyDrive/Thesis/nllb-checkpoints&quot;\nDROP = 0.1\nSRC = &quot;eng_Latn&quot;\nTGT = &quot;deu_Latn&quot;\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=&quot;$SRC-$TGT&quot; \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n</code></pre>\n<p>This is the error:</p>\n<pre><code>/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=&quot;conf&quot;, config_name=&quot;base_config&quot;)\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 289, in main\n    train_module = TrainModule(config)\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n</code></pre>\n<p>So far, I understand there is a <code>Missing data_prefix configuration</code>. I created a demo custom data_config.json. Which looks like this:</p>\n<pre><code>{\n    &quot;data_prefix&quot;: &quot;/content/sample_data&quot;,\n    &quot;train_data&quot;: &quot;train_demo.json&quot;,\n    &quot;test_data&quot;: &quot;test_demo.json&quot;,\n    &quot;lang_pairs&quot;: &quot;eng_Latn-deu_Latn&quot;\n}\n</code></pre>\n<p>While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?</p>\n",
         "2024-08-09 14:46:54",
         "1",
         "148",
         "1",
         "78854613.0",
         "<p>While I can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided JSON files), my personal recommendation would be to fine-tune NLLB in the <code>transformers</code> library, specifically using the <code>Seq2SeqTrainer</code>.</p>\n<p>I did this before for multiple models, including NLLB, check out this repository: <a href=\"https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/\" rel=\"nofollow noreferrer\">https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/</a></p>\n<p>This way the fine-tuning and inference process for the NLLB model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so:</p>\n<pre><code>tokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=&quot;eng_Latn&quot;, tgt_lang=&quot;deu_Latn&quot;)\n</code></pre>\n<p>and generate translations like this:</p>\n<pre><code>model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(&quot;deu_Latn&quot;)[1], max_length=512)\n</code></pre>\n",
         "0.0",
         "\"facebook/nllb-200-distilled-600M\"\n---\nDATA_CONFIG = \"/content/sample_data/data_config.json\"\nOUTPUT_DIR = \"/content/outputs\"\nMODEL_FOLDER = \"/content/drive/MyDrive/Thesis/nllb-checkpoints\"\nDROP = 0.1\nSRC = \"eng_Latn\"\nTGT = \"deu_Latn\"\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=\"$SRC-$TGT\" \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n---\n/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=\"conf\", config_name=\"base_config\")\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 289, in main\n    train_module = TrainModule(config)\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n---\nMissing data_prefix configuration\n---\n{\n    \"data_prefix\": \"/content/sample_data\",\n    \"train_data\": \"train_demo.json\",\n    \"test_data\": \"test_demo.json\",\n    \"lang_pairs\": \"eng_Latn-deu_Latn\"\n}",
         "transformers\n---\nSeq2SeqTrainer\n---\ntokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=\"eng_Latn\", tgt_lang=\"deu_Latn\")\n---\nmodel.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(\"deu_Latn\")[1], max_length=512)",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation",
         "Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB",
         "While I cant help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB While I cant help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "nllb finetuning error missing data_prefix configuration englishgerman translation im attempting finetune nllb model scientific translation task english eng_latn german deu_latn followed official guidelines finetuning authors nllb documentation link code block giving error error far understand created demo custom data_configjson looks like official documentation provides information im encountering difficulties applying specific use case someone share detailed guide point helpful resources finetuning nllb cant help concrete error message getting guess would issues structure provided json files personal recommendation would finetune nllb library specifically using multiple models including nllb check repository way finetuning inference process nllb model bilingual model find guides easiely exception load tokenizer like generate translations like",
         "nllb finetune error miss data_prefix configuration englishgerman translation I m attempt finetune nllb model scientific translation task english eng_latn german deu_latn follow official guideline finetune author nllb documentation link code block give error error far understand create demo custom data_configjson look like official documentation provide information I m encounter difficulty apply specific use case someone share detailed guide point helpful resource finetune nllb can not help concrete error message get guess would issue structure provide json file personal recommendation would finetune nllb library specifically use multiple model include nllb check repository way finetune inference process nllb model bilingual model find guide easiely exception load tokenizer like generate translation like"
        ],
        [
         "48",
         "235",
         "78846004",
         "How can I use structured_output with Azure OpenAI with the openai Python library?",
         "<p>I want to use structured output with Azure OpenAI.</p>\n<p>I tried the following code, based on the code given in <a href=\"https://openai.com/index/introducing-structured-outputs-in-the-api/\" rel=\"nofollow noreferrer\">https://openai.com/index/introducing-structured-outputs-in-the-api/</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=&quot;gpt-4omini-2024-07-18-name&quot;,\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful math tutor.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;solve 8x + 31 = 2&quot;},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n</code></pre>\n<p>I get the error:</p>\n<pre><code>openai.BadRequestError: Error code: 400:\n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Invalid parameter: response_format must be one of json_object, text.&quot;,\n        &quot;type&quot;: &quot;invalid_request_error&quot;,\n        &quot;param&quot;: &quot;response_format&quot;,\n        &quot;code&quot;: &quot;None&quot;\n    }\n}\n</code></pre>\n<p>How to fix it?</p>\n<p>I ran <code>pip install -U openai</code>: I use <code>openai==1.40.1</code> and Python 3.11.</p>\n<hr />\n<p>I also tried <a href=\"https://cookbook.openai.com/examples/structured_outputs_intro\" rel=\"nofollow noreferrer\">https://cookbook.openai.com/examples/structured_outputs_intro</a> using  using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message:</p>\n<pre><code>from openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = &quot;https://&lt;your-resource-name&gt;.openai.azure.com&quot;\napi_key = &quot;&lt;your-api-key&gt;&quot;\ndeployment_name = &quot;&lt;your-deployment-name&gt;&quot; # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f&quot;{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01&quot;\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            &quot;role&quot;: &quot;system&quot;,\n            &quot;content&quot;: math_tutor_prompt\n        },\n        {\n            &quot;role&quot;: &quot;user&quot;,\n            &quot;content&quot;: question\n        }\n    ],\n    response_format={\n        &quot;type&quot;: &quot;json_schema&quot;,\n        &quot;json_schema&quot;: {\n            &quot;name&quot;: &quot;math_reasoning&quot;,\n            &quot;schema&quot;: {\n                &quot;type&quot;: &quot;object&quot;,\n                &quot;properties&quot;: {\n                    &quot;steps&quot;: {\n                        &quot;type&quot;: &quot;array&quot;,\n                        &quot;items&quot;: {\n                            &quot;type&quot;: &quot;object&quot;,\n                            &quot;properties&quot;: {\n                                &quot;explanation&quot;: {&quot;type&quot;: &quot;string&quot;},\n                                &quot;output&quot;: {&quot;type&quot;: &quot;string&quot;}\n                            },\n                            &quot;required&quot;: [&quot;explanation&quot;, &quot;output&quot;],\n                            &quot;additionalProperties&quot;: False\n                        }\n                    },\n                    &quot;final_answer&quot;: {&quot;type&quot;: &quot;string&quot;}\n                },\n                &quot;required&quot;: [&quot;steps&quot;, &quot;final_answer&quot;],\n                &quot;additionalProperties&quot;: False\n            },\n            &quot;strict&quot;: True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = &quot;how can I solve 8x + 7 = -23&quot;\n\nresult = get_math_solution(question)\n\nprint(result.content)\n</code></pre>\n",
         "2024-08-07 23:14:19",
         "0",
         "1201",
         "2",
         "78946352.0",
         "<p>Using <code>gpt-4o-2024-08-06</code>, which finally got deployed today (2024-09-03) on Azure, made it work. Code example from <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python-secure\" rel=\"nofollow noreferrer\">learn.microsoft.com</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = &quot;https://your-azure-openai-endpoint.com&quot;\napi_key = &quot;your-azure-openai-key&quot;\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Extract the event information.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Alice and Bob are going to a science fair on Friday.&quot;},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n</code></pre>\n<p>output:</p>\n<pre><code>name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  &quot;id&quot;: &quot;chatcmpl-A3XDRVolXpjeAAQIGddswI990weid&quot;,\n  &quot;choices&quot;: [\n    {\n      &quot;finish_reason&quot;: &quot;stop&quot;,\n      &quot;index&quot;: 0,\n      &quot;logprobs&quot;: null,\n      &quot;message&quot;: {\n        &quot;content&quot;: &quot;{\\&quot;name\\&quot;:\\&quot;Science Fair\\&quot;,\\&quot;date\\&quot;:\\&quot;Friday\\&quot;,\\&quot;participants\\&quot;:[\\&quot;Alice\\&quot;,\\&quot;Bob\\&quot;]}&quot;,\n        &quot;refusal&quot;: null,\n        &quot;role&quot;: &quot;assistant&quot;,\n        &quot;function_call&quot;: null,\n        &quot;tool_calls&quot;: [],\n        &quot;parsed&quot;: {\n          &quot;name&quot;: &quot;Science Fair&quot;,\n          &quot;date&quot;: &quot;Friday&quot;,\n          &quot;participants&quot;: [\n            &quot;Alice&quot;,\n            &quot;Bob&quot;\n          ]\n        }\n      },\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ],\n  &quot;created&quot;: 1725406029,\n  &quot;model&quot;: &quot;gpt-4o-2024-08-06&quot;,\n  &quot;object&quot;: &quot;chat.completion&quot;,\n  &quot;service_tier&quot;: null,\n  &quot;system_fingerprint&quot;: &quot;fp_b2ffeb31ff&quot;,\n  &quot;usage&quot;: {\n    &quot;completion_tokens&quot;: 17,\n    &quot;prompt_tokens&quot;: 32,\n    &quot;total_tokens&quot;: 49\n  },\n  &quot;prompt_filter_results&quot;: [\n    {\n      &quot;prompt_index&quot;: 0,\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ]\n}\n</code></pre>\n<p>Tested with Python 3.11.7 and openai==1.43.0.</p>\n",
         "0.0",
         "from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4omini-2024-07-18-name\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n        {\"role\": \"user\", \"content\": \"solve 8x + 31 = 2\"},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n---\nopenai.BadRequestError: Error code: 400:\n{\n    \"error\": {\n        \"message\": \"Invalid parameter: response_format must be one of json_object, text.\",\n        \"type\": \"invalid_request_error\",\n        \"param\": \"response_format\",\n        \"code\": \"None\"\n    }\n}\n---\npip install -U openai\n---\nopenai==1.40.1\n---\nfrom openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = \"https://<your-resource-name>.openai.azure.com\"\napi_key = \"<your-api-key>\"\ndeployment_name = \"<your-deployment-name>\" # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01\"\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": math_tutor_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": question\n        }\n    ],\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"math_reasoning\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"steps\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"explanation\": {\"type\": \"string\"},\n                                \"output\": {\"type\": \"string\"}\n                            },\n                            \"required\": [\"explanation\", \"output\"],\n                            \"additionalProperties\": False\n                        }\n                    },\n                    \"final_answer\": {\"type\": \"string\"}\n                },\n                \"required\": [\"steps\", \"final_answer\"],\n                \"additionalProperties\": False\n            },\n            \"strict\": True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = \"how can I solve 8x + 7 = -23\"\n\nresult = get_math_solution(question)\n\nprint(result.content)",
         "gpt-4o-2024-08-06\n---\nfrom pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = \"https://your-azure-openai-endpoint.com\"\napi_key = \"your-azure-openai-key\"\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n---\nname='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  \"id\": \"chatcmpl-A3XDRVolXpjeAAQIGddswI990weid\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"{\\\"name\\\":\\\"Science Fair\\\",\\\"date\\\":\\\"Friday\\\",\\\"participants\\\":[\\\"Alice\\\",\\\"Bob\\\"]}\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": [],\n        \"parsed\": {\n          \"name\": \"Science Fair\",\n          \"date\": \"Friday\",\n          \"participants\": [\n            \"Alice\",\n            \"Bob\"\n          ]\n        }\n      },\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"created\": 1725406029,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": \"fp_b2ffeb31ff\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 32,\n    \"total_tokens\": 49\n  },\n  \"prompt_filter_results\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ]\n}",
         "How can I use structured_output with Azure OpenAI with the openai Python library",
         "I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message",
         "Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "use structured_output azure openai openai python library want use structured output azure openai tried following code based code given get error fix ran use python 311 also tried using using azure+ gpt4o mini 20240718 didnt work either error message using finally got deployed today 20240903 azure made work code example learnmicrosoftcom output tested python 3117 openai==1430",
         "use structured_output azure openai openai python library want use structured output azure openai try follow code base code given get error fix run use python 311 also try use use azure+ gpt4o mini 20240718 do not work either error message using finally get deploy today 20240903 azure make work code example learnmicrosoftcom output test python 3117 openai==1430"
        ],
        [
         "49",
         "239",
         "78836208",
         "Removing bi-grams after tokenization for TfidfVectorizer",
         "<p>I'm attempting to remove bi-grams that are created by <code>TfidfVectorizer</code>.  I'm using <code>text.TfidfVectorizer</code> so that I can use my own preprocessor function.</p>\n<p>Test strings and preprocessor function:</p>\n<pre><code>doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, &quot;&quot;)\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n</code></pre>\n<p>My <code>TfidfVectorizer</code> instantiation and <code>fit_transform</code>:</p>\n<pre><code>from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n</code></pre>\n<p>Here is my error:</p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in &lt;cell line: 5&gt;()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----&gt; 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-&gt; 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 &quot;Upper case characters found in&quot;\n   1332                 &quot; vocabulary while 'lowercase'&quot;\n   1333                 &quot; is True. These entries will not&quot;\n   1334                 &quot; be matched with any documents&quot;\n   1335             )\n   1336             break\n-&gt; 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-&gt; 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--&gt; 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object\n</code></pre>\n<p>How to resolve it?</p>\n",
         "2024-08-05 19:46:40",
         "1",
         "38",
         "1",
         "78837616.0",
         "<p>The preprocessor should handle documents, not the whole corpus. (The clues are the &quot;expected string&quot; in the error, and the fact that <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"nofollow noreferrer\">the <code>TfidfVectorizer</code> docs</a> refer to &quot;the preprocessing (string transformation) stage&quot;. The docs could definitely be clearer.)</p>\n<p>This should fix it:</p>\n<pre><code>def remove_bigrams(doc: str) -&gt; str:\n    &quot;&quot;&quot;Remove certain bi-grams from a document.&quot;&quot;&quot;\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, &quot;&quot;)\n    return doc\n</code></pre>\n",
         "2.0",
         "TfidfVectorizer\n---\ntext.TfidfVectorizer\n---\ndoc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, \"\")\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n---\nTfidfVectorizer\n---\nfit_transform\n---\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n---\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in <cell line: 5>()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----> 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-> 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 \"Upper case characters found in\"\n   1332                 \" vocabulary while 'lowercase'\"\n   1333                 \" is True. These entries will not\"\n   1334                 \" be matched with any documents\"\n   1335             )\n   1336             break\n-> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-> 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--> 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object",
         "TfidfVectorizer\n---\ndef remove_bigrams(doc: str) -> str:\n    \"\"\"Remove certain bi-grams from a document.\"\"\"\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, \"\")\n    return doc",
         "Removing bigrams after tokenization for TfidfVectorizer",
         "Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it",
         "The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "Removing bigrams after tokenization for TfidfVectorizer Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "removing bigrams tokenization tfidfvectorizer im attempting remove bigrams created im using use preprocessor function test strings preprocessor function instantiation error resolve preprocessor handle documents whole corpus clues expected string error fact docs refer preprocessing string transformation stage docs could definitely clearer fix",
         "remove bigrams tokenization tfidfvectorizer I m attempt remove bigram create I m use use preprocessor function test string preprocessor function instantiation error resolve preprocessor handle document whole corpus clue expect string error fact docs refer preprocesse string transformation stage doc could definitely clear fix"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 8510
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AcceptedAnswerBody</th>\n",
       "      <th>AcceptedAnswerScore</th>\n",
       "      <th>Question_Code</th>\n",
       "      <th>Answer_Code</th>\n",
       "      <th>Title_Clean</th>\n",
       "      <th>Body_Clean</th>\n",
       "      <th>AcceptedAnswerBody_Clean</th>\n",
       "      <th>combination_text</th>\n",
       "      <th>combination_text_no_stopw</th>\n",
       "      <th>combination_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>2025-04-02 05:56:11</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>79552218.0</td>\n",
       "      <td>&lt;p&gt;The configuration file is missing the 'labe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>import spacy\\n\\nnlp = spacy.load(\"pl_core_news...</td>\n",
       "      <td>labels_to_ignore:\\n    - O\\n---\\nnlp_engine_na...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>Im using spaCy with the pl_core_news_lg model ...</td>\n",
       "      <td>The configuration file is missing the labels_t...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>79548202</td>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>&lt;p&gt;I understand the -100 label id is used so t...</td>\n",
       "      <td>2025-04-01 09:21:17</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>79551169.0</td>\n",
       "      <td>&lt;p&gt;The author of the tutorial you mentioned se...</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>-100\\n---\\nignore_index\\n---\\nignore_index\\n--...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>I understand the 100 label id is used so that ...</td>\n",
       "      <td>The author of the tutorial you mentioned sets ...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>gpt2 models huggingface 100 label index traini...</td>\n",
       "      <td>gpt2 model huggingface 100 label index trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>79523269</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>&lt;p&gt;I am trying to import gensim into colab.&lt;/p...</td>\n",
       "      <td>2025-03-20 14:36:02</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>79523777.0</td>\n",
       "      <td>&lt;p&gt;You have to restart the session for the und...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>!pip install gensim\\n---\\n/usr/local/lib/pytho...</td>\n",
       "      <td>numpy\\n---\\nnumpy\\n---\\nscipy</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>I am trying to import gensim into colab I get ...</td>\n",
       "      <td>You have to restart the session for the underl...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "      <td>trouble getting importing gensim work colab tr...</td>\n",
       "      <td>trouble getting import gensim work colab try i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this &lt;a href...</td>\n",
       "      <td>2025-03-11 14:50:31</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>79501337.0</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>server\\n---\\nSSH\\n---\\nskip_tokens = [1]  # sk...</td>\n",
       "      <td>matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>I am running the code found on this site in my...</td>\n",
       "      <td>I cant test it but I checked source code and i...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>store images instead showing server running co...</td>\n",
       "      <td>store image instead show server run code find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>79482283</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>&lt;p&gt;I am using presidio/langchain_experimental ...</td>\n",
       "      <td>2025-03-03 22:27:07</td>\n",
       "      <td>4</td>\n",
       "      <td>230</td>\n",
       "      <td>2</td>\n",
       "      <td>79495969.0</td>\n",
       "      <td>&lt;p&gt;After some test I was able to find the solu...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>from presidio_anonymizer import PresidioAnonym...</td>\n",
       "      <td>config = {\\n    \"nlp_engine_name\": \"spacy\",\\n ...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>I am using presidio/langchain_experimental to ...</td>\n",
       "      <td>After some test I was able to find the solutio...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>20436</td>\n",
       "      <td>62328</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>&lt;p&gt;input: phrase 1, phrase 2&lt;/p&gt;\\n\\n&lt;p&gt;output:...</td>\n",
       "      <td>2008-09-15 12:26:42</td>\n",
       "      <td>65</td>\n",
       "      <td>49889</td>\n",
       "      <td>11</td>\n",
       "      <td>63076.0</td>\n",
       "      <td>&lt;hr&gt;\\n\\n&lt;p&gt;You might want to check out this pa...</td>\n",
       "      <td>44.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>input phrase 1 phrase 2 output semantic simila...</td>\n",
       "      <td>You might want to check out this paper Sentenc...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>algorithm tells semantic similarity two phrase...</td>\n",
       "      <td>algorithm tell semantic similarity two phrase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8506</th>\n",
       "      <td>20437</td>\n",
       "      <td>42489</td>\n",
       "      <td>How to implement a \"related\" degree measure al...</td>\n",
       "      <td>&lt;p&gt;I was going to Ask a Question earlier today...</td>\n",
       "      <td>2008-09-03 20:21:04</td>\n",
       "      <td>8</td>\n",
       "      <td>456</td>\n",
       "      <td>2</td>\n",
       "      <td>42532.0</td>\n",
       "      <td>&lt;p&gt;One such way to implement such an algorithm...</td>\n",
       "      <td>5.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>I was going to Ask a Question earlier today wh...</td>\n",
       "      <td>One such way to implement such an algorithm wo...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>implement related degree measure algorithm goi...</td>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>20438</td>\n",
       "      <td>41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03 10:36:13</td>\n",
       "      <td>118</td>\n",
       "      <td>33200</td>\n",
       "      <td>11</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>&lt;p&gt;Actually what Google does is very much non-...</td>\n",
       "      <td>87.0</td>\n",
       "      <td>&lt;spell_checked_word&gt;</td>\n",
       "      <td></td>\n",
       "      <td>How do you implement a Did you mean</td>\n",
       "      <td>Possible Duplicate How does the Google Did you...</td>\n",
       "      <td>Actually what Google does is much nontrivial a...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8508</th>\n",
       "      <td>20439</td>\n",
       "      <td>36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31 01:08:48</td>\n",
       "      <td>3</td>\n",
       "      <td>5661</td>\n",
       "      <td>6</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>&lt;p&gt;Citation from Vista &lt;a href=\"http://blogs.m...</td>\n",
       "      <td>8.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>my primary language is spanish but I use all m...</td>\n",
       "      <td>Citation from Vista speech recognition blog In...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>vista speech recognition multiple languages pr...</td>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>20441</td>\n",
       "      <td>23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22 22:45:10</td>\n",
       "      <td>27</td>\n",
       "      <td>6484</td>\n",
       "      <td>9</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>&lt;p&gt;We developed exactly what you are looking f...</td>\n",
       "      <td>12.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Natural language date/time parser for NET</td>\n",
       "      <td>Does anyone know of a NET date/time parser sim...</td>\n",
       "      <td>We developed exactly what you are looking for ...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "      <td>natural language date/time parser net anyone k...</td>\n",
       "      <td>natural language date / time parser net anyone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8510 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  QuestionId                                              Title  \\\n",
       "0         3    79549787  Why does Presidio with spacy nlp engine not re...   \n",
       "1         4    79548202  GPT-2 and other models from huggingface -100 l...   \n",
       "2        12    79523269  Trouble getting importing gensim to work in colab   \n",
       "3        21    79501178        Store images instead of showing in a server   \n",
       "4        29    79482283  Presidio with Langchain Experimental does not ...   \n",
       "...     ...         ...                                                ...   \n",
       "8505  20436       62328  Is there an algorithm that tells the semantic ...   \n",
       "8506  20437       42489  How to implement a \"related\" degree measure al...   \n",
       "8507  20438       41424             How do you implement a \"Did you mean\"?   \n",
       "8508  20439       36533     Vista speech recognition in multiple languages   \n",
       "8509  20441       23689        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                   Body         CreationDate  \\\n",
       "0     <p>I'm using spaCy with the pl_core_news_lg mo...  2025-04-02 05:56:11   \n",
       "1     <p>I understand the -100 label id is used so t...  2025-04-01 09:21:17   \n",
       "2     <p>I am trying to import gensim into colab.</p...  2025-03-20 14:36:02   \n",
       "3     <p>I am running the code found on this <a href...  2025-03-11 14:50:31   \n",
       "4     <p>I am using presidio/langchain_experimental ...  2025-03-03 22:27:07   \n",
       "...                                                 ...                  ...   \n",
       "8505  <p>input: phrase 1, phrase 2</p>\\n\\n<p>output:...  2008-09-15 12:26:42   \n",
       "8506  <p>I was going to Ask a Question earlier today...  2008-09-03 20:21:04   \n",
       "8507  <blockquote>\\n  <p><strong>Possible Duplicate:...  2008-09-03 10:36:13   \n",
       "8508  <p>my primary language is spanish, but I use a...  2008-08-31 01:08:48   \n",
       "8509  <p>Does anyone know of a .NET date/time parser...  2008-08-22 22:45:10   \n",
       "\n",
       "      Score  ViewCount  AnswerCount  AcceptedAnswerId  \\\n",
       "0         0         68            1        79552218.0   \n",
       "1         0         46            1        79551169.0   \n",
       "2         0        125            1        79523777.0   \n",
       "3         0         36            1        79501337.0   \n",
       "4         4        230            2        79495969.0   \n",
       "...     ...        ...          ...               ...   \n",
       "8505     65      49889           11           63076.0   \n",
       "8506      8        456            2           42532.0   \n",
       "8507    118      33200           11           41448.0   \n",
       "8508      3       5661            6           36684.0   \n",
       "8509     27       6484            9          631134.0   \n",
       "\n",
       "                                     AcceptedAnswerBody  AcceptedAnswerScore  \\\n",
       "0     <p>The configuration file is missing the 'labe...                  1.0   \n",
       "1     <p>The author of the tutorial you mentioned se...                  1.0   \n",
       "2     <p>You have to restart the session for the und...                  1.0   \n",
       "3     <p>I can't test it but ...</p>\\n<p>I checked <...                  1.0   \n",
       "4     <p>After some test I was able to find the solu...                 -2.0   \n",
       "...                                                 ...                  ...   \n",
       "8505  <hr>\\n\\n<p>You might want to check out this pa...                 44.0   \n",
       "8506  <p>One such way to implement such an algorithm...                  5.0   \n",
       "8507  <p>Actually what Google does is very much non-...                 87.0   \n",
       "8508  <p>Citation from Vista <a href=\"http://blogs.m...                  8.0   \n",
       "8509  <p>We developed exactly what you are looking f...                 12.0   \n",
       "\n",
       "                                          Question_Code  \\\n",
       "0     import spacy\\n\\nnlp = spacy.load(\"pl_core_news...   \n",
       "1                                                         \n",
       "2     !pip install gensim\\n---\\n/usr/local/lib/pytho...   \n",
       "3     server\\n---\\nSSH\\n---\\nskip_tokens = [1]  # sk...   \n",
       "4     from presidio_anonymizer import PresidioAnonym...   \n",
       "...                                                 ...   \n",
       "8505                                                      \n",
       "8506                                                      \n",
       "8507                               <spell_checked_word>   \n",
       "8508                                                      \n",
       "8509                                                      \n",
       "\n",
       "                                            Answer_Code  \\\n",
       "0     labels_to_ignore:\\n    - O\\n---\\nnlp_engine_na...   \n",
       "1     -100\\n---\\nignore_index\\n---\\nignore_index\\n--...   \n",
       "2                         numpy\\n---\\nnumpy\\n---\\nscipy   \n",
       "3     matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...   \n",
       "4     config = {\\n    \"nlp_engine_name\": \"spacy\",\\n ...   \n",
       "...                                                 ...   \n",
       "8505                                                      \n",
       "8506                                                      \n",
       "8507                                                      \n",
       "8508                                                      \n",
       "8509                                                      \n",
       "\n",
       "                                            Title_Clean  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in colab   \n",
       "3           Store images instead of showing in a server   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507                How do you implement a Did you mean   \n",
       "8508     Vista speech recognition in multiple languages   \n",
       "8509          Natural language date/time parser for NET   \n",
       "\n",
       "                                             Body_Clean  \\\n",
       "0     Im using spaCy with the pl_core_news_lg model ...   \n",
       "1     I understand the 100 label id is used so that ...   \n",
       "2     I am trying to import gensim into colab I get ...   \n",
       "3     I am running the code found on this site in my...   \n",
       "4     I am using presidio/langchain_experimental to ...   \n",
       "...                                                 ...   \n",
       "8505  input phrase 1 phrase 2 output semantic simila...   \n",
       "8506  I was going to Ask a Question earlier today wh...   \n",
       "8507  Possible Duplicate How does the Google Did you...   \n",
       "8508  my primary language is spanish but I use all m...   \n",
       "8509  Does anyone know of a NET date/time parser sim...   \n",
       "\n",
       "                               AcceptedAnswerBody_Clean  \\\n",
       "0     The configuration file is missing the labels_t...   \n",
       "1     The author of the tutorial you mentioned sets ...   \n",
       "2     You have to restart the session for the underl...   \n",
       "3     I cant test it but I checked source code and i...   \n",
       "4     After some test I was able to find the solutio...   \n",
       "...                                                 ...   \n",
       "8505  You might want to check out this paper Sentenc...   \n",
       "8506  One such way to implement such an algorithm wo...   \n",
       "8507  Actually what Google does is much nontrivial a...   \n",
       "8508  Citation from Vista speech recognition blog In...   \n",
       "8509  We developed exactly what you are looking for ...   \n",
       "\n",
       "                                       combination_text  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in co...   \n",
       "3     Store images instead of showing in a server I ...   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507  How do you implement a Did you mean Possible D...   \n",
       "8508  Vista speech recognition in multiple languages...   \n",
       "8509  Natural language date/time parser for NET Does...   \n",
       "\n",
       "                              combination_text_no_stopw  \\\n",
       "0     presidio spacy nlp engine recognize organizati...   \n",
       "1     gpt2 models huggingface 100 label index traini...   \n",
       "2     trouble getting importing gensim work colab tr...   \n",
       "3     store images instead showing server running co...   \n",
       "4     presidio langchain experimental detect polish ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tells semantic similarity two phrase...   \n",
       "8506  implement related degree measure algorithm goi...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple languages pr...   \n",
       "8509  natural language date/time parser net anyone k...   \n",
       "\n",
       "                                 combination_text_clean  \n",
       "0     presidio spacy nlp engine recognize organizati...  \n",
       "1     gpt2 model huggingface 100 label index trainin...  \n",
       "2     trouble getting import gensim work colab try i...  \n",
       "3     store image instead show server run code find ...  \n",
       "4     presidio langchain experimental detect polish ...  \n",
       "...                                                 ...  \n",
       "8505  algorithm tell semantic similarity two phrase ...  \n",
       "8506  implement relate degree measure algorithm go a...  \n",
       "8507  implement mean possible duplicate google mean ...  \n",
       "8508  vista speech recognition multiple language pri...  \n",
       "8509  natural language date / time parser net anyone...  \n",
       "\n",
       "[8510 rows x 19 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_generating(df_post_answer['combination_text_all_lemma'],\"First Attempt Combination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define a function to removing some noise word\n",
    "This will return a new column in dataframe the text that remove defined noise word'''\n",
    "noise_words1 = {'word','example','well','output','code','text','string','sentence','model','work','result','see'}\n",
    "noise_words2 = {'word','example','well','output','code','text','string','sentence','model','work','result','see','one','give','need','list','find','know','look','follow','file','m','etc','two','try','way','use','want','different','two','seem'}\n",
    "def remove_noise_word(text,noise_words):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    # Remove noise words\n",
    "    filtered = [word for word in words if word.lower() not in noise_words]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_words1 = {'word','example','well','output','code','text','string','sentence','model','work','result','see'}\n",
    "df_post_answer['combination_text_all_lemma_no_noise1'] = df_post_answer['combination_text_all_lemma'].apply(lambda x: remove_noise_word(x, noise_words1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_generating(df_post_answer['combination_text_all_lemma_no_noise1'],\"No Noise Word 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'combination_text_all_lemma_no_noise1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\GPU_TF\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'combination_text_all_lemma_no_noise1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m noise_words2 \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwell\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msee\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgive\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfind\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknow\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlook\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfollow\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m,}\n\u001b[1;32m----> 2\u001b[0m df_post_answer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombination_text_all_lemma_no_noise2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_post_answer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcombination_text_all_lemma_no_noise1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: remove_noise_word(x, noise_words2))\n",
      "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\GPU_TF\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\GPU_TF\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'combination_text_all_lemma_no_noise1'"
     ]
    }
   ],
   "source": [
    "noise_words2 = {'word','example','well','output','code','text','string','sentence','model','work','result','see','one','give','need','list','find','know','look','follow','file',}\n",
    "df_post_answer['combination_text_all_lemma_no_noise2'] = df_post_answer['combination_text_all_lemma_no_noise1'].apply(lambda x: remove_noise_word(x, noise_words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_generating(df_post_answer['combination_text_all_lemma_no_noise2'],\"No Noise Word 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can do more text cleaning later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy:\n",
    "\n",
    "- Remove some intensifier words (from Assignment1), noise word (could use wordcloud to detect them, define some lists)\n",
    "- Clustering them by embedding first then DBSCAN\n",
    "- Then could try regex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoang\\anaconda3\\envs\\GPU_TF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:246: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n",
      "c:\\Users\\hoang\\anaconda3\\envs\\GPU_TF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model_embedding = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "def cluster_embedding(dframe,column,model_embedding,cluster_type=\"kmean\",):\n",
    "    embeddings_title = model_embedding.encode(dframe[column].tolist())\n",
    "    dbscan = DBSCAN(eps=0.5,min_samples=5,metric='euclidean')\n",
    "    k = 10  # or whatever value you want to test\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    if cluster_type == \"kmean\":\n",
    "        cluster_kmean = kmeans.fit_predict(embeddings_title)\n",
    "        dframe['cluster_kmean'] = cluster_kmean\n",
    "    elif cluster_type == \"dbscan\":\n",
    "        clusters_dbscan = dbscan.fit_predict(embeddings_title)\n",
    "        dframe['cluster_kmean'] = cluster_kmean\n",
    "    else:\n",
    "        cluster_kmean = kmeans.fit_predict(embeddings_title)\n",
    "        dframe['cluster_kmean'] = cluster_kmean\n",
    "        clusters_dbscan = dbscan.fit_predict(embeddings_title)\n",
    "        dframe['cluster_kmean'] = cluster_kmean\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_embedding(df_post_answer,'combination_text_all_lemma',model_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "combination_text_clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cluster_kmean",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "57c2968b-ea19-467b-a89c-27e4fdc6fc2b",
       "rows": [
        [
         "0",
         "presidio spacy nlp engine recognize organization pesel spacy I m use spacy pl_core_news_lg model extract name entity polish text correctly detect organization org people name per output however use presidio pl_core_news_lg model configuration file recognizer correctly detect organization org pesel number even though appear list support entity output even though organization pl_pesel list org list nlp engine support recognizer presidio detect correctly text config file presidio fail detect organization org pesel number pl_pesel spacy correctly detect configuration file miss labels_to_ignore field state entity ignore nlp engine configuration would look like",
         "5"
        ],
        [
         "1",
         "gpt2 model huggingface 100 label index training instead pad token understand 100 label i d use prediction include calculate loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label exclude replace pad token implementation use nncrossentropyloss argument ignore_index benefit change i d 100 oppose add argument ignore_index loss set pad token i d result way write make think benefit description ignore_index appear achieve wanted author tutorial mention set use save line code do not see line author pass something default value default value nncrossentropyloss use value instead respective pad token i d allow write model indepent training code do not pass pad token i d tokenizer loss function",
         "6"
        ],
        [
         "2",
         "trouble getting import gensim work colab try import gensim colab get follow error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string relate issue thank restart session underlie runtime notice package change see recall past colab offering warn possibly also past colab have not yet load /etc fresh environment ok downgrade behind scene without problem 1st import downgrade something change colab recently maybe faststart optimization bunch report problem like last day two explicitly restart gensiminstall & / downgrade resolve error",
         "5"
        ],
        [
         "3",
         "store image instead show server run code find site server would like store image instead show since connect remotely ssh connection via connection code instance one store file locally instead show can not test check source code use remove should not show get think could use matplotlibpyplotsavefigfilename save file probably also use",
         "5"
        ],
        [
         "4",
         "presidio langchain experimental detect polish name use presidio / langchain_experimental anonymize text polish detect name eg jan kowalski code output expect name jan kowalski email address anonymize output remain unchanged instal pl_core_news_lg model use miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing use output look like mention use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio work expect test able find solution output look like need directly add format accord spacys requirement believe there s something miss unclear documentation cause misunderstanding",
         "9"
        ],
        [
         "5",
         "opennlp postaggerme chunkerme synergy I m try use opennlp chunk api chunk portuguese sentence first tokenized sentence use tokenizerme tag postaggerme use readymade model provide project sentence ivo viu uva postaggerme return tag propn verb det noun model seem use ud pos tag readymade model chunkerme portuguese follow instruction training first use chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything work well sentence chunker produce correct tag bnp bvp bnp inp complex sentence have not produce good result try identify could improve chunker training one thing notice difference type tag portuguese corpus bosque 80 seem use portuguese tag example instead propn corpus use prop instead det use art seem could lead problem especially since one parameter chunker receive array ud tag train another type tag write code create routine convert portuguese notation ud penn want ask indeed impact tool already translation suggestion improve chunker precision / recall q1 yes choose tag set ud penn custom impact conversion possible bidirectional manner penn > ud work well ud > penn good idea lossy conversion ud tag set less detailed compare classic penn tag set use custom language specific tagset work matter mapping from / to ud correctly might work tag set language other might complicated / lossy q2 be not opennlp project take code donation upcoming release want provide mapping / translation pt lang q3 need detail / discussion apache opennlp user and/or dev mailing list alternatively feel free open jira issue drill topic clear idea propose code addition",
         "9"
        ],
        [
         "6",
         "word/ sentence similarity try find give word/ set word similar definition example definition vegetarian user want check set sentence like try use sentence transformer like give expect result good approach achieve result like doable nlp/ technique yes definitely doable use nlp key do not need full similarity matrix want check sentence semantically similar give definition well approach encode definition sentence use sentence transformer compute cosine similarity definition embed sentence embed set threshold eg 06 07 determine similar enough explanation utilcos_sim compute cosine similarity definition sentence threshold tuning similarity threshold consider true adjust threshold base strict want match original approach do not work modelsimilarity do not exist sentencetransformer api compute sentencetosentence matrix definitiontosentence comparison",
         "3"
        ],
        [
         "7",
         "underfitte pretraine glove + lstm model accurcacy unchanged sentiment classification use pretraine glove lstm model use google play review scrap result 50k++ text implement random sampling minority class however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue base extra information comment I m go say reason lstm model hit wall unspecified low accuracy 85 % try reach good type model problem case tweak parameter likely waste effort I m fairly sure encoder transformer eg bert surpass sentiment analysis benchmark number year back sorry quick search could not find killer reference insert transformer get big well since extra thought build top glove embedding present problem do not handle multiple meaning word queen might female king embedding party trick king male + female = queen might pop group might gay man might chess piece going put limit accuracy model build whereas transformer do not limitation look whole string see word context possible argue course bring context lstm come transformer still scale 20 + layer whereas lstms tend choke two layer",
         "7"
        ],
        [
         "8",
         "can not compile marian nmt I m use endeavouros I m try compile marian instruction fail error message seemingly indicate conflict code c++20 file repo line step follow result please help diagnostic build tripping introduce gcc 141 warning error build promote warning error break build although build specifie generate failure g++14 emit warn code would illegal recent standard c++20 later warn make error warning make error compile option option include list compile option create toplevel line 227 et seq apply compile option library target line 133 whence option operative failing compilation failure bug repo report maintainer seem report already head revision v1120 year old gcc 14 pende fix seem three interim option get build do either make code legal c++11 c++20 diagnostic advice say occurrence eg make occurrence locally disable occurrence eg remove list remains warn may result diagnostic demote error warning default status have not test option i d need go trouble instal cuda",
         "5"
        ],
        [
         "9",
         "get custom column model forward function training huggingface trainer use huggingface trainer train cumstom model subclasse llama llm tokenized tokenizer dataset field additionally add 2 custom colunms can not get custom field forward function model new llm fine tuning anyone help would grateful much need modify datum collator pass model trainer ignore extra column default modify datum collator pass method receive hope work",
         "6"
        ],
        [
         "10",
         "get leaf word reverse stem one python list line solution provide link try get leaf word one stem word use communitycontributed srivastava package imagine short sample word list follow work manually follow go wordbyword timeconsuming list 200 word get follow output n noun adjective v verb r adverb try reversestem entire list one go fail get output think fail save output dictionary eventually would like output list without break noun adjective adverb verb something like one solution use nested list comprehension strip forget space",
         "1"
        ],
        [
         "11",
         "inspect probability bertopic model say build bertopic model use inspecting give single value item would like entire probability vector across topic case want vector 20 probability item word n item k topic would like nxk output individual topic probability across document need add one argument note calculate_probabilitie = true work use cluster embed model bertopic default use official documentation mention document well",
         "7"
        ],
        [
         "12",
         "determine popular word english dictionary within dictionary word forgive word awful I m try figure determine use word english language set word dictionary I ve make I ve do research nltk can not seem find function within library matter help need example sentence enjoy cold glass water hot day would return water use word day day conversation sentence essentially need return value frequently use word conversation figure ill likely involve ai time I ve try use ai wind copy paste code do not understand I m try avoid go route help welcome appreciated context decide start project would essentially guess predetermine word base character user say do not computer guess need external dataset task try dataset google n gram dataset breakdown problem statement input enjoy cold glass water hot day water split sentence word list example enjoy cold glass water hot day first loop word sentence let say first word look word external dataset look frequency word let say word external dataset repeat time repeat task word dictionary word sentence key value frequency word get external data frequency example random value exact value pick word high frequency note try big corpus external datum use big corpus english word use conversation even frequency mention create",
         "3"
        ],
        [
         "13",
         "catelog sentence 5 word represent dataframe 1000 text row also 5 word want know one much represnt text 0 1 every score etc glad recomendation edit represnt = semantic distance word text example let say row 1 text want eat 2 word food house would high score could use pretraine sentence transformer model output",
         "7"
        ],
        [
         "14",
         "count frequency word within key word text two set word list first one call second one call goal calculate frequency within 10 word example assume word acquire list look word list within 10 word acquire within 10 word mean 10 word forward key word 10 word backward key word mean forward backward movement list small example expect outcome first row see word list around word acquire base form acquire list similarly list word within 10 word list total search word 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note word basically base form variation like adopt adopt count key word also need process row text identify occurrence capture 10word window around within window need check multiword search_words ensure match phrase unique find within window need count avoid doublecounte across row store result frequency count row accurately reflect number unique near return",
         "3"
        ],
        [
         "15",
         "error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset get follow error run need slightly change gradient calculation class also do not include forward_func gradient class constructor attribute method able launch stuff properly think use layerintegratedgradient well debug bert line tutorial please find snippet work",
         "6"
        ],
        [
         "16",
         "euclidian distance word sentence vectorizer dataframe 1000 text row tfidfvectorizer want create new field give distance sentence word want let say word king dfke think take sentence 5 closet word word king make average glad know hear another method convince euclidean distance would optimal measure would actually look similarity score would give say want focus euclidean distance method give",
         "3"
        ],
        [
         "17",
         "llama321binstruct generate inconsistent output want use model although set still generate inconsistent output pipeline look like idea solve issue model inconsistent output due two main factor 1 temperature set temperature zero give inconsistent result refer opeani discussion page detail good option set temperature low value 000001 instead zero 2 do_sample already set false remain way",
         "6"
        ],
        [
         "18",
         "use aws service execute python script extract keyword text use keybert simple python script give two block text extract keyword use keybert compare list keyword sort two list depend list share keyword aws service would well fit need want able esentially spin need give block text execute return result do not want integrate project do not use python I ve attempt use lambda I m concerned potential cost run thank case would normally think two resource align good practice aws software engineering sagemaker lambda model I m use resourceintensive require gpu acceleration i d go sagemaker otherwise lambda good solution case here i d package keybert script lambda easily deploy container invoke whenever need process text block aws lambda charge execution time costefficient occasional task",
         "1"
        ],
        [
         "19",
         "normalization token embedding bert encoder block follow multiheade attention layer bert encoder block layer normalization done separately embed token ie one mean variance per token embed concatenate vector token embedding mean variance embedding track full detail layer normalization ln bert mean variance compute per token weight bias parameter learn ln per token per embed dimension",
         "6"
        ],
        [
         "20",
         "convert character index bert token indices work questionanswer dataset map characterbase answer index tokenbase index tokenize context question together use tokenizer like bert heres example row dataset tokenization answer indice 56 16 want create new dataset answer token indices eg 56 ad 60 linkedin learn class instructor conversion create csv file share code expect result encode question context locate token span answer within tokenized context update dataset tokenlevel index follow function here use test function output",
         "0"
        ],
        [
         "21",
         "share complex spacy nlp model across multiple python process minimize memory usage I m work multiprocesse python application multiple process need access large preloade spacy nlp model eg en_core_web_lg since model memoryintensive want avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage look multiprocessingmanager multiprocessingshared_memory approach seem well suited numpy array raw datum buffer simple object complex object internal reference like nlp model also look mpis mpiwinallocate_shared run issue use redis server make rank 0 processing work mpi since process do single rank defeat propose use multiprocesse efficient way share spacy model instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp model memory across process multiprocessingmanager shared_memory viable way improve performance reduce memory overhead work complex object suggestion example would greatly appreciate thank would advise treat nlp model like python object would always prefer load nlp model use microservice approach align ml / software engineering good practice separate model logic main application instead load model process memoryintensive model load dedicated service setup allow model use multiple part application without duplicate memory usage make efficient modular scalable concern memory efficiency address scalability modularity also improve example implement microservice use fastapi + docker could look like containerize fastapi service",
         "7"
        ],
        [
         "22",
         "dutch sentiment analysis robbertje outputs positive / negative label netural label miss run dutch sentiment analysis robbertje outputs positive / negative label netural label miss datum obvious neutral sentence / word eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral label example robbertje output model train label therefore try categorize every input positive negative even nonsensical neutral 1 find model train include label 2 finetune model dataset include label 3 empirically define threshold base confidence output interpret first 2 choice extensive effort would suggest go third option quick workaround try feed model neutral input observe range confidence score output use threshold classify here sample",
         "2"
        ],
        [
         "23",
         "find root form verb use curiosityai / catalyst I m try find root form verb run text pipeline identify tokens match do not know continue far help greatly appreciate follow code target net 80 illustrate one method obtain root form verb inflect form annonote code comment three nuget package version require code identical original sample note specific sentence add adverb quickly verb run without library incorrectly interpret run noun depend source text might issue believe separate question ask",
         "8"
        ],
        [
         "24",
         "possible get embedding nvembe use candle want cli program output embedding arbitrary input want inference embedding model choose framework choice candle also look mistralrs basically I m try code fragment rust candle try start mistral candle example nvembed hf page say replace model i d original code able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail way around maybe candlebase open source work could use inspiration maybe that s common mistake could easily diagnose candle look whereas original tensor name change line candle_transformer",
         "6"
        ],
        [
         "25",
         "derive attribute / label short plain text description ner llm derive attribute / label short plain text description ner llm short product description i d like transform structure attribute example input output everything format would trivial write regular expression do many different format nuance increasingly cumbersome hardcode logic format try create generic solution immediately run issue basic approach several different datum provider format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even give provider may multiple format may change time format always strictly follow many way express particular component example weight might express one 15l 1 1/2 liter 1500ml etc part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year etc I m use wine example sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work hard get right there s straightforward method would good know I d like develop generalpurpose function accept description format little experience nlp / artificial intelligence suspect useful tool / algo leverage 1000 + example record could potentially use train model something run locally would prefer necessary I m look specific implementation guidance anyone who s work similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource I ve look online information many approach involve significant amount front work unclear they ll work practical sense llm would work nicely iv do similar task work nicely minimal training keep mind statistical method nlp / llm / ner never 100 % accurate practical purpose find llm accurate custom soup regular expression task would use framework like langchain follow prompt note might need work prompt bit example run model create xml output would trivial parse modify prompt create different type output personally find xml work well keep mind llm cheap run task give ambiguousness domain likely good choice particular task would 1/1000 penny per label use openai service might find cheap model / provider however work llm important ensure accuracy first optimize cost whole thing probably take 12 hour build intermediate llm developer learning may vary perfect project learn llm",
         "2"
        ],
        [
         "26",
         "vary embed dim due change padding batch size want train simple neural network embedding_dim parameter load datum use torchs dataloader custom collate_fn collate_fn_padd function look follow problem every batch want train model embed text get pad differently long take long sequence current batch mean embed dim / input size linear layer neural network change batch batch althoug want size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collate_fn_pad function padd sequence size independet batch size add maximum length argument set pad truncate datum fix length",
         "6"
        ],
        [
         "27",
         "adjust performance tokenizer working tokenizer library hug face tokenizer work fine case case I m wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case use specific type tokenizer unigram tokenizer model change tokenizer vocabulary result asadaf sdfsaf would tokenize unique word",
         "0"
        ],
        [
         "28",
         "spacy get lemmas string panda datum frame column text value document want apply lemmatization value spacy library use panda function I ve define function iterate word document concatenate correspond lemmas output string however slow way extract lemmatize form document spacy many way speed spacy processing question make sense depends mostly size input obvious one individually apply model every single row rather use batch processing use iterable string mean easy use apply disable component use token level processing need lemmas would dependency parser name entity recognition component increase object buffer pipe default 1000 obviously make sense touch memory increase lot increase number processor use use increase time take initially load model decrease processing time experience start make sense 500k+ text note also require code run wrapper basic example 1 2 advanced example four",
         "4"
        ],
        [
         "29",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency word phrase corpus well number document word phrase occur world australian occur two dictionary key peep indig key content intend mutually exclusive similarly australia oz australia post foreign foreign multinat farm / farmer dairy farmer occur two dictionary key intend count accord dictionary expect overall frequency count extract pattern column kwic table report x2 note word industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table doc name column key three question problem / issue could affect output accuracy use approach well / more parsimonius approach achieve trying would well way extract equivalent tetxstat frequency count datum kwic table do not think want let specify nest scope mutually exclusive across key within key observe difference note use wildcarde dairy key create 20241006 reprex v211",
         "3"
        ],
        [
         "30",
         "seq2seq trainertrain keep give indexing error try machine translation hindi sanskrit use nllb model keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb model ` facebook / nllb20013b input datum ~40k hindi sentence error arises try train sample datum also detail error message code preprocessing do data datum preprocesse code model param train idea error persisting indicate dataset trainer get finetuning start empty look thread suggest add might resolve issue could give try",
         "5"
        ],
        [
         "31",
         "alternative device_map = auto huggingface pretraine model reading huggingface use follow code read model modification internal layer add layer start training / finetune get everything model investigation find custom layer be not distribute multi gpus original model need something like read model simply something like find actually several method first one use analyze model calculate total amount available memory occupy model second one use match model device basically case use follow code ps code still need tested finetune",
         "6"
        ],
        [
         "32",
         "weight mistral model reinitialize huggingface one reinitialize weight hug face llama v2 model official way original model there s different suggestion reinitialize model try seem work layer reinitialize function use xaiver / he initialization random initialization mistralconfig default parameter set 002 describe one assume use truncate normal distribution standard deviation 002 plot actual model weight distribution truncate normal distribution standard deviation 002 look like seem like fit",
         "6"
        ],
        [
         "33",
         "break first per sequence find spacy try extract first speaker name list text use spacy currently function return per tag want reduce overhead get first contiguous sequence per entity here example output get want result code currently use I m look want modify find_per_sequence function return first contiguous sequence per entity text ignore subsequent per entity encounter different type entity provide function return multiple name partial name need way ensure first name sequence include achieve issue name entity slice doc thus never process habl first entry go straight garca lpez actually iterate token see per sequence end leave part wait see first token per start append break one condition meet end refactore code little debugged here edit version program produce desire output",
         "0"
        ],
        [
         "34",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin recently get follow error lora small llm see discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not find anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell since pin memory available cpu gpu run gpu colab disable setting",
         "6"
        ],
        [
         "35",
         "finetune pretraine model quantization amp scaler error attempt unscale fp16 gradient try finetune pretraine model limited vram achieve use quantization automatic mixed precision amp however encounter issue can not seem resolve could please help identify problem minimal example line error occur can not finetune fp16 / uint8 model amp amp use fp32 parameter param autocast fp16 forward pass amp expect master set parameter fp32 also should not finetune quantize model first place quantization cause sort numerical issue instability training suppose keep quantize model static train adapter top quantize model find detail",
         "6"
        ],
        [
         "36",
         "keep train pytorch model new datum I m work text classification task decide use pytorch model purpose process mainly involve follow step load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new datum however every day need classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire model process timeconsume new comment lose validation would like create new dataset newly classify text continue train new datum new comment classify manually label correct use gpt online code write desire process however I m sure working expect I m make silly mistake happen main question could check proposse way solve problem work expect vectorizer face new token would loose original vectorizer full training process propose code dataset find use pretraine word embedding like bertforsequenceclassification embedding handle unseen token gracefully since map word continuous vector base semantic meaning reduce impact unseen word model train bert incremental training least effort",
         "2"
        ],
        [
         "37",
         "capitalize word sentiment analysis I m currently work datum customer review product sephora task classify sentiment negative neutral positive common technique text preprocesse low case word situation upper case word like amazing hide significant emotion behind turn word low case cause information loss would happy opinion subject still low case word personally think create class distinction sentiment good good positive include importance upper case word current code use bertbased model llm actual classification would recommend use preprocessing least come capitalization model pretraine nonpreprocesse datum want kind analysis result label sentence could lowercase everything group ngram simplify analysis think multiple class well distinction prediction think would make sense switch sentiment regression instead classification predict value continuous range come somewhat natural finetune language model normal classification would take continuous output model map categorical class use something like softmax need skip last step directly use model output many python ml framework finetune use language model class regression task check repository example",
         "2"
        ],
        [
         "38",
         "import name split_torch_state_dict_into_shard huggingface_hub I ve use llama 2 research month import follows always work however today show follow error runtimeerror fail import transformersmodelsllamamodeling_llama follow error look see traceback fail import transformersgenerationutil follow error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt / conda / lib / python310 / sitepackage / huggingface_hub/ init py recreate hug face token do not work use google colab kaggle notebook error you re encounter due function available function include start version resolve issue update library version 0230 later also please install accelerate git link",
         "5"
        ],
        [
         "39",
         "process datum gpu instead ram python code I m currently use follow code process audio data run ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific change much appreciate use follow code process audio data gpu",
         "6"
        ],
        [
         "40",
         "visualize crossattention matrix marianmtmodel output generation work machine translation task use marianmtmodel hug face transformer library specifically want visualize crossattention matrix model translation process however encounter difficulty achieve I ve try initial attempt notice crossattention matrix directly return model generate translation example find involved feeding source text translation model however goal access crossattention matrix model generate output translation give use forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache use_cache = false capture full matrix last step here implementation approach seem work capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question give issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate I m currently reshape key k query q tensor head dimension multiplication want ask advice case there s easy effective way huggingface build method return attention weight return dictlike object output dict contain attention weight model include encoder attention decoder attention cross attention",
         "6"
        ],
        [
         "41",
         "do not permute positional encoding bert affect output expect work jupyter notebook transformer section positional encoding want demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply model do not use mask attention like gpt2 however attempt approach bert model use crossattention predict mask token encounter unexpected result expect happen permutation cause model predict different token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes result align expectation time permute one aspect either input ids positional embedding lead different outcome even though occasionally produce result question something else hug face bert model might influence position beyond positional encoding completeness include full code part notebook try directly important part happen model input token id position id four scenario consider baseline correct order token position permute position ids permute token ids permute position ids token ids correct scenario 1 4 produce result however incorrect assuming permute tokens position separately give result consider model tell token occur position model tell token occur position even though use permutation mapping token position different result different model output reason sometimes see result line random chance sample permutation result token / position embedding line way mostly way permute one luck average case produce different result simple test huggingface model take input parameter use test permutation input id without mess weight matrix test well create input datum permute need compute logit compare logit compare logit permute depermute need compare token token basis example token scenario 1 permute token scenario 3 want compare logits scenario 1 logits scenario 3 code produce output like run code bunch time find comparison always small deviation permute token position together always produce result ignore small deviation cause numeric issue find comparison generally large deviation sometimes small deviation discuss due get lucky permutation position id mostly line",
         "6"
        ],
        [
         "42",
         "openaiembedding work create single vector size 1536 whole text corpus I m work class use model accord documentation generate 1536dimensional vector input text however I m bit confused work 1536dimensional vector generate entire input text 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph expect 100 word input text expect openaiembedding would output 100 vector size 1536 output single vector size 1536 whole input text expect learning I ve understand embedding like word2vec glove provide vector word corpus differ approach take openaiembedding I m try understand whether there s way extract embedding individual word use model output always single vector represent whole input insight example would greatly appreciate everything describe 100 % expect q 1536dimensional vector generate entire input text yes q 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph first openai embedding model do not handle single word different long text model input input even single character eg do not make sense calculate embed vector since do not semantically mean anything we human second probably mean question happen similarity search embedding word happen use happen use embedding word sentence paragraph whole text matter yes call chunk decision chunk text depends use case good thing probably simply try see get meaningful result similarity search mean chunk appropriate even mean chunk whole text do not get meaningful result similarity search mean chunk be not appropriate eg instead chunk paragraph try chunk sentence there s excellent stack overflow blog post topic read pay attention bolde text good explanation rag create text embedding piece datum want draw retrieve allow place piece source text within semantic space llm use create response // come rag system you ll need pay special attention big individual piece datum divide datum call chunk complex embed whole document // size chunk datum go make huge difference information come search embed piece datum whole thing convert vector include much chunk vector lose ability specific anything discuss include little lose context datum",
         "7"
        ],
        [
         "43",
         "ner versus llm extract name gender role company text need extract name gender job title employer / company name newspaper article running process local hardware cloud allow due copyright reason I ve play around llama 31 I m find do not get useable result model small 70b parameter size model run much slowly good hardware throw another small llm might good use few processing resource ner use extract data ner I ve look extract name gender do not know extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get well result fast single llm pass answer training model good model use start point I m much begin machine learn journey would love point right direction thank advance apart limitation would not recommend use llm like llamma 31 task one classic task nlp small language model tool incorporate achieve goal use matter personal choice however define known name entity see list name entity doc guess mean possible associate name mention article python package use lookup gender however note ambiguous substantial tolerance error use package possible solution would like note small model available via spacy use large model specify make sure model download run code here download model",
         "2"
        ],
        [
         "44",
         "padding batch sequence affect performance effective attention mask transformer model sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 % curious follow pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow model effectively skip padding token result minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero value computation effectively reduce approximately 10 % thank insight attention compute tensor shape compute memory requirement scale size dimension input fix size percent padding impact performance minor overhead apply padding mask ie padding mask save one mask fill operation x % padding % padding you re go see difference overall compute requirement set tensor size respect batching sequence add inefficiency batch together sequence different length say 10 sequence length 10 sequence length pad batch sequence two batch mix length evenly get two batch sequence length sort length batching get one batch sequence length another length first case two batch sequence length 128 require overall compute compare second case one batch 8 one 128 say fix input size be not go see performance change percent padding way attention operation skip padding token conditional control flow require sort approach do not work well way gpu execute operation parallel effect padding mask assign 0 attention weight padding token",
         "6"
        ],
        [
         "45",
         "spacy matcher optional suffix pattern report multiple match text use follow matcher rule text mylabel value get two match mylabel mylabel quite surprising expect single match mylabel add new greedy flag do not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375 say behavior you re observe spacy expect bug use pattern operator mean colon optional matcher generate shorter long match you ve see explanation pattern text pattern spacy try match alone colon optional colon include therefore get two match answer question intend behavior bug intend behavior operator allow colon optionally match lead multiple match determine second match subset first match determine one match subset another compare start end index match long match start index different end index write code even use spacy version 375 see detail example code code filter short match output short match always report long match do not think match guarantee report specific order handle sort match start end index show code example abovenow sort filter match subset long match another alternative solution want ensure long match return change way define pattern note flag do not change behavior match rather influence overlaps handle certain custom setting back summary explain behavior you re see design due optional operator addition filter short match compare start end index match furthermore sort match start end index allow keep long nonoverlapping match",
         "0"
        ],
        [
         "46",
         "` mlflowtransformerslog_model ` finish problem want use log finetune huggingface model however method run simply finish run forever throw error suspect configuration right model big output say problem idea properly example less setup look like run include pseudocode python 3119 & last output get console define trainer model turn peft model object via work",
         "6"
        ],
        [
         "47",
         "nllb finetune error miss data_prefix configuration englishgerman translation I m attempt finetune nllb model scientific translation task english eng_latn german deu_latn follow official guideline finetune author nllb documentation link code block give error error far understand create demo custom data_configjson look like official documentation provide information I m encounter difficulty apply specific use case someone share detailed guide point helpful resource finetune nllb can not help concrete error message get guess would issue structure provide json file personal recommendation would finetune nllb library specifically use multiple model include nllb check repository way finetune inference process nllb model bilingual model find guide easiely exception load tokenizer like generate translation like",
         "5"
        ],
        [
         "48",
         "use structured_output azure openai openai python library want use structured output azure openai try follow code base code given get error fix run use python 311 also try use use azure+ gpt4o mini 20240718 do not work either error message using finally get deploy today 20240903 azure make work code example learnmicrosoftcom output test python 3117 openai==1430",
         "5"
        ],
        [
         "49",
         "remove bigrams tokenization tfidfvectorizer I m attempt remove bigram create I m use use preprocessor function test string preprocessor function instantiation error resolve preprocessor handle document whole corpus clue expect string error fact docs refer preprocesse string transformation stage doc could definitely clear fix",
         "5"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 8510
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combination_text_clean</th>\n",
       "      <th>cluster_kmean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2 model huggingface 100 label index trainin...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trouble getting import gensim work colab try i...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>store image instead show server run code find ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>algorithm tell semantic similarity two phrase ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8506</th>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8508</th>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>natural language date / time parser net anyone...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8510 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 combination_text_clean  cluster_kmean\n",
       "0     presidio spacy nlp engine recognize organizati...              5\n",
       "1     gpt2 model huggingface 100 label index trainin...              6\n",
       "2     trouble getting import gensim work colab try i...              5\n",
       "3     store image instead show server run code find ...              5\n",
       "4     presidio langchain experimental detect polish ...              9\n",
       "...                                                 ...            ...\n",
       "8505  algorithm tell semantic similarity two phrase ...              3\n",
       "8506  implement relate degree measure algorithm go a...              3\n",
       "8507  implement mean possible duplicate google mean ...              9\n",
       "8508  vista speech recognition multiple language pri...              9\n",
       "8509  natural language date / time parser net anyone...              9\n",
       "\n",
       "[8510 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post_answer[['combination_text_all_lemma','cluster_kmean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tags:\n",
    "\n",
    "Plan to get tags?\n",
    "\n",
    "- Keep filtering the Word Cloud and then manually create 10 topics?\n",
    "\n",
    ": first to identify the words that better represent a certain document\n",
    "second to encode them into vectors\n",
    "BERT-based keyword extraction algorithm\n",
    "KeyBERT [20], and a pre-defined dictionary of\n",
    "topic-keywords developed by the team\n",
    "The computation of semantic text similarity is performed after encoding the\n",
    "keywords with Sentence-BERT\n",
    "\n",
    "The first\n",
    "stream of methods include solutions such as a set of logical rules that map words to topics\n",
    "or comparison with a user defined taxonomy or ontology\n",
    "\n",
    "The long-standing Latent Semantic Analysis (LSA) [5] and Latent Dirichlet Allocation (LDA) [3] models are well\n",
    "suited to perform information reduction and exploratory analysis tasks\n",
    "    However, this unsupervised approach has a few drawbacks: topic models might be unstable when not optimized [1] and their outputs might often be difficult to understand [4], as each topic corresponds with a combination of words which need to be interpreted by the user\n",
    "    Another downside is the fact that the researcher needs to make assumptions on the number of topics to be retained from a certain collection\n",
    "\n",
    "    Some scholars tried to mitigate all these issues by developing semi-supervised models that include ”anchor words” [7], or using partial labeling strategies \n",
    "\n",
    "KeyBERT\n",
    "\n",
    "Create a keyword dict, I can start using word cloud for this to do 1 by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword(text):\n",
    "    # Extract keywords using KeyBert; returns list of (keyword, score)\n",
    "    keyword_tuples = kw_model.extract_keywords(\n",
    "        text, \n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        use_maxsum=True, \n",
    "        nr_candidates=10, \n",
    "        top_n=5\n",
    "    )\n",
    "    # Extract only the keyword strings from the tuples\n",
    "    keywords = [kw for kw, score in keyword_tuples]\n",
    "    return \",\".join(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['keywords_fromBert'] = df_post_answer['combination_text_all_lemma_no_noise2'].apply(extract_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer[['keywords_fromBert','cluster_kmean']].to_csv(\"keywords_fromBert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_category = (\n",
    "    df_post_answer[['keywords_fromBert', 'cluster_kmean']]\n",
    "    .replace('', np.nan)    # Replace empty strings with NaN\n",
    "    .dropna(axis=0)        # Now drop rows that have NaN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_category['keywords_list'] = for_category['keywords_fromBert'].str.split(',') \n",
    "cluster_group = for_category['cluster_kmean'].unique()\n",
    "cluster_group.sort()\n",
    "\n",
    "pattern_dfs = []\n",
    "\n",
    "\n",
    "for cluster in cluster_group:\n",
    "    cluster_data = for_category.loc[for_category['cluster_kmean']==cluster,'keywords_list']\n",
    "    pattern =  get_frequency(cluster_data)\n",
    "    pattern = pattern.loc[pattern['itemsets'].apply(lambda x:len(x) > 1)]\n",
    "    # Add a new column for the cluster\n",
    "    pattern['cluster'] = cluster\n",
    "    pattern_dfs.append(pattern)\n",
    "    \n",
    "# Concatenate all clusters into one dataframe\n",
    "frequency_df = pd.concat(pattern_dfs, ignore_index=True)\n",
    "# Rename columns as required: score, pattern, and cluster\n",
    "frequency_df.rename(columns={'support': 'score', 'itemsets': 'pattern'}, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(data,min_support=0.005):\n",
    "    te = TransactionEncoder()\n",
    "    item_te = te.fit(data).transform(data)\n",
    "    df_items_1hot = pd.DataFrame(item_te, columns=te.columns_)\n",
    "    frequent_itemsets_apriori = apriori(df_items_1hot, min_support=min_support, use_colnames=True)\n",
    "    return frequent_itemsets_apriori\n",
    "'''data: dataframe with columns. Passing keywords and cluster columns'''\n",
    "def category_frequency(data,keyword_col,cluster_col):\n",
    "    for_category = (data[[keyword_col,cluster_col]]\n",
    "    .replace('', np.nan)    # Replace empty strings with NaN\n",
    "    .dropna(axis=0)        # Now drop rows that have NaN\n",
    ")\n",
    "    for_category['keywords_list'] = for_category[keyword_col].str.split(',') \n",
    "    cluster_group = for_category[cluster_col].unique()\n",
    "    cluster_group.sort()\n",
    "    \n",
    "    pattern_dfs = []\n",
    "    for cluster in cluster_group:\n",
    "        cluster_data = for_category.loc[for_category['cluster_kmean']==cluster,'keywords_list']\n",
    "        pattern =  get_frequency(cluster_data)\n",
    "        pattern = pattern.loc[pattern['itemsets'].apply(lambda x:len(x) > 1)]\n",
    "        # Add a new column for the cluster\n",
    "        pattern['cluster'] = cluster\n",
    "        pattern_dfs.append(pattern)\n",
    "    \n",
    "    # Concatenate all clusters into one dataframe\n",
    "    frequency_df = pd.concat(pattern_dfs, ignore_index=True)\n",
    "    # Rename columns as required: score, pattern, and cluster\n",
    "    frequency_df.rename(columns={'support': 'score', 'itemsets': 'pattern'}, inplace=True)\n",
    "    return frequency_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d50a74e8-de11-4766-a7b4-c1f601349735",
       "rows": [
        [
         "0",
         "0.013143483023001095",
         "frozenset({'pattern match'})",
         "0"
        ],
        [
         "1",
         "0.0208105147864184",
         "frozenset({'regular expression'})",
         "0"
        ],
        [
         "2",
         "0.013143483023001095",
         "frozenset({'spacy'})",
         "0"
        ],
        [
         "3",
         "0.01095290251916758",
         "frozenset({'split token'})",
         "0"
        ],
        [
         "4",
         "0.012048192771084338",
         "frozenset({'tokenize'})",
         "0"
        ],
        [
         "5",
         "0.012048192771084338",
         "frozenset({'tokenizer'})",
         "0"
        ],
        [
         "6",
         "0.014238773274917854",
         "frozenset({'use regex'})",
         "0"
        ],
        [
         "7",
         "0.03176341730558598",
         "frozenset({'use spacy'})",
         "0"
        ],
        [
         "8",
         "0.029345372460496615",
         "frozenset({'nltk'})",
         "1"
        ],
        [
         "9",
         "0.016930022573363433",
         "frozenset({'nltk python'})",
         "1"
        ],
        [
         "10",
         "0.030474040632054177",
         "frozenset({'python'})",
         "1"
        ],
        [
         "11",
         "0.010158013544018058",
         "frozenset({'python nlp'})",
         "1"
        ],
        [
         "12",
         "0.022573363431151242",
         "frozenset({'python nltk'})",
         "1"
        ],
        [
         "13",
         "0.010158013544018058",
         "frozenset({'python want'})",
         "1"
        ],
        [
         "14",
         "0.010158013544018058",
         "frozenset({'regular expression'})",
         "1"
        ],
        [
         "15",
         "0.011286681715575621",
         "frozenset({'use nltk'})",
         "1"
        ],
        [
         "16",
         "0.01580135440180587",
         "frozenset({'use python'})",
         "1"
        ],
        [
         "17",
         "0.012391573729863693",
         "frozenset({'baye classifier'})",
         "2"
        ],
        [
         "18",
         "0.01858736059479554",
         "frozenset({'entity recognition'})",
         "2"
        ],
        [
         "19",
         "0.013630731102850062",
         "frozenset({'naive baye'})",
         "2"
        ],
        [
         "20",
         "0.011152416356877323",
         "frozenset({'precision recall'})",
         "2"
        ],
        [
         "21",
         "0.013630731102850062",
         "frozenset({'sentiment analysis'})",
         "2"
        ],
        [
         "22",
         "0.01486988847583643",
         "frozenset({'training datum'})",
         "2"
        ],
        [
         "23",
         "0.01107419712070875",
         "frozenset({'calculate similarity'})",
         "3"
        ],
        [
         "24",
         "0.0221483942414175",
         "frozenset({'corpus'})",
         "3"
        ],
        [
         "25",
         "0.03875968992248062",
         "frozenset({'cosine similarity'})",
         "3"
        ],
        [
         "26",
         "0.01107419712070875",
         "frozenset({'document similarity'})",
         "3"
        ],
        [
         "27",
         "0.01107419712070875",
         "frozenset({'python nltk'})",
         "3"
        ],
        [
         "28",
         "0.016611295681063124",
         "frozenset({'semantic similarity'})",
         "3"
        ],
        [
         "29",
         "0.012181616832779624",
         "frozenset({'similar document'})",
         "3"
        ],
        [
         "30",
         "0.014396456256921373",
         "frozenset({'similarity score'})",
         "3"
        ],
        [
         "31",
         "0.018518518518518517",
         "frozenset({'column'})",
         "4"
        ],
        [
         "32",
         "0.011396011396011397",
         "frozenset({'column contain'})",
         "4"
        ],
        [
         "33",
         "0.03561253561253561",
         "frozenset({'column dataframe'})",
         "4"
        ],
        [
         "34",
         "0.014245014245014245",
         "frozenset({'convert dataframe'})",
         "4"
        ],
        [
         "35",
         "0.02849002849002849",
         "frozenset({'dataframe'})",
         "4"
        ],
        [
         "36",
         "0.024216524216524215",
         "frozenset({'dataframe column'})",
         "4"
        ],
        [
         "37",
         "0.011396011396011397",
         "frozenset({'dataframe contain'})",
         "4"
        ],
        [
         "38",
         "0.022792022792022793",
         "frozenset({'dataframe like'})",
         "4"
        ],
        [
         "39",
         "0.01566951566951567",
         "frozenset({'dataframe use'})",
         "4"
        ],
        [
         "40",
         "0.011396011396011397",
         "frozenset({'dataframe want'})",
         "4"
        ],
        [
         "41",
         "0.011396011396011397",
         "frozenset({'new dataframe'})",
         "4"
        ],
        [
         "42",
         "0.045584045584045586",
         "frozenset({'panda'})",
         "4"
        ],
        [
         "43",
         "0.011396011396011397",
         "frozenset({'panda column'})",
         "4"
        ],
        [
         "44",
         "0.04131054131054131",
         "frozenset({'panda dataframe'})",
         "4"
        ],
        [
         "45",
         "0.017094017094017096",
         "frozenset({'use panda'})",
         "4"
        ],
        [
         "46",
         "0.010550996483001172",
         "frozenset({'instal spacy'})",
         "5"
        ],
        [
         "47",
         "0.011723329425556858",
         "frozenset({'jupyt notebook'})",
         "5"
        ],
        [
         "48",
         "0.011723329425556858",
         "frozenset({'nltk'})",
         "5"
        ],
        [
         "49",
         "0.011723329425556858",
         "frozenset({'spacy'})",
         "5"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 99
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013143</td>\n",
       "      <td>(pattern match)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020811</td>\n",
       "      <td>(regular expression)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013143</td>\n",
       "      <td>(spacy)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010953</td>\n",
       "      <td>(split token)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012048</td>\n",
       "      <td>(tokenize)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.033133</td>\n",
       "      <td>(natural language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.022590</td>\n",
       "      <td>(nlp)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.014307</td>\n",
       "      <td>(sentiment analysis)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.015060</td>\n",
       "      <td>(use nltk)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.011295</td>\n",
       "      <td>(wordnet)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score               pattern  cluster\n",
       "0   0.013143       (pattern match)        0\n",
       "1   0.020811  (regular expression)        0\n",
       "2   0.013143               (spacy)        0\n",
       "3   0.010953         (split token)        0\n",
       "4   0.012048            (tokenize)        0\n",
       "..       ...                   ...      ...\n",
       "94  0.033133    (natural language)        9\n",
       "95  0.022590                 (nlp)        9\n",
       "96  0.014307  (sentiment analysis)        9\n",
       "97  0.015060            (use nltk)        9\n",
       "98  0.011295             (wordnet)        9\n",
       "\n",
       "[99 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and compare with BERT topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the BERTopic model\n",
    "docs = df_post_answer['Title_Clean_No_Noise2'].tolist()\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Representation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Representative_Docs",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4e5d126d-59ac-416d-bfeb-d2c4fab9068c",
       "rows": [
        [
         "0",
         "-1",
         "4872",
         "-1_find_nltk_language_file",
         "['find', 'nltk', 'language', 'file', 'sentences', 'error', 'using', 'corpus', 'similar', 'use']",
         "['identify strings two different lists', 'compare context two different paragraph using machine learning', 'spacy create new language data corpus']"
        ],
        [
         "1",
         "0",
         "411",
         "0_pandas_dataframe_column_frame",
         "['pandas', 'dataframe', 'column', 'frame', 'columns', 'rows', 'row', 'df', 'apply', 'dataframes']",
         "['implement function pandas dataframe column', 'pandas data frame make columns pandas', 'way create tfidf column pandas dataframe']"
        ],
        [
         "2",
         "1",
         "344",
         "1_bert_bertmodel_embeddings_fine",
         "['bert', 'bertmodel', 'embeddings', 'fine', 'finetuning', 'bertformaskedlm', 'berts', 'pretrained', 'finetuned', 'finetune']",
         "['language training bert', 'bert classification', 'training bert using bert embeddings']"
        ],
        [
         "3",
         "2",
         "285",
         "2_sentiment_analysis_reviews_aspect",
         "['sentiment', 'analysis', 'reviews', 'aspect', 'negative', 'emotion', 'polarity', 'sentiments', 'twitter', 'positive']",
         "['sentiment analysis object attribute sentiment', 'sentiment analysis using nltk', 'training data sentiment analysis']"
        ],
        [
         "4",
         "3",
         "262",
         "3_word2vec_text2vec_vectors_pretrained",
         "['word2vec', 'text2vec', 'vectors', 'pretrained', 'vector', 'representation', 'wordembedding', 'embeddings', 'embedding', 'sense2vec']",
         "['using word2vec embedding sentences', 'using word2vec', 'using word2vec extract data']"
        ],
        [
         "5",
         "4",
         "226",
         "4_tfidf_tfidfvectorizer_vectorizer_idf",
         "['tfidf', 'tfidfvectorizer', 'vectorizer', 'idf', 'tf', 'sklearn', 'scikit', 'scores', 'scikitlearn', 'calculate']",
         "['document corpus tfidf', 'tfidf value matching output tfidfvectorizer', 'use tfidf classification']"
        ],
        [
         "6",
         "5",
         "164",
         "5_tensorflow_tensor_tensors_tensorflow2",
         "['tensorflow', 'tensor', 'tensors', 'tensorflow2', 'dtype', 'invalidargumenterror', 'gradients', 'tensorflows', 'shape', 'saved']",
         "['slice tensorflow tensor multiple', 'convert tensor written tensor', 'embedding tensorflow language']"
        ],
        [
         "7",
         "6",
         "163",
         "6_tm_package_characters_column",
         "['tm', 'package', 'characters', 'column', 'occur', 'select', 'commas', 'vector', 'rs', 'udpipe']",
         "['find 2 phrase using tm r', 'searching specific corpus r tm package', 'r tm package german']"
        ],
        [
         "8",
         "7",
         "160",
         "7_huggingface_hugging_face_transformers",
         "['huggingface', 'hugging', 'face', 'transformers', 'trainer', 'huggingfaces', 'models', 'hugginface', 'tokenizer', 'pretrained']",
         "['loss function used trainer transformers library hugging face', 'get size hugging face pretrained', 'using huggingface transformers trainer method hugging face datasets']"
        ],
        [
         "9",
         "8",
         "149",
         "8_tokenize_tokenizer_tokenizing_tokens",
         "['tokenize', 'tokenizer', 'tokenizing', 'tokens', 'tokenizers', 'tokenization', 'token', 'tokenized', 'lexer', 'tokenising']",
         "['tokenize using', 'tokenize', 'tokenizing tokenize punctuation like']"
        ],
        [
         "10",
         "9",
         "145",
         "9_opennlp_apache_simplenlg_finder",
         "['opennlp', 'apache', 'simplenlg', 'finder', 'opennlps', 'sentencedetector', 'conduct', 'nodenlp', 'eclipse', 'class']",
         "['using tokenizer opennlp', 'opennlp parser training', 'use opennlp java']"
        ],
        [
         "11",
         "10",
         "140",
         "10_embedding_embeddings_oov_layer",
         "['embedding', 'embeddings', 'oov', 'layer', 'encoder', 'universal', 'pretrained', 'tsne', 'vector', 'vectors']",
         "['difference embedding embedding', 'embedding layer embeddings', 'embedding']"
        ],
        [
         "12",
         "11",
         "136",
         "11_information_unstructured_extraction_extract",
         "['information', 'unstructured', 'extraction', 'extract', 'extracting', 'medical', 'product', 'structured', 'image', 'area']",
         "['information extraction', 'information extraction', 'extracting information unstructured']"
        ],
        [
         "13",
         "12",
         "130",
         "12_topic_lda_modeling_topics",
         "['topic', 'lda', 'modeling', 'topics', 'modelling', 'dirichlet', 'allocation', 'mallet', 'latent', 'bertopic']",
         "['topic modeling using gensim', 'topic modelling using lda', 'lda topic modeling']"
        ],
        [
         "14",
         "13",
         "122",
         "13_corenlp_stanfordnlp_stanfordcorenlp_stanford",
         "['corenlp', 'stanfordnlp', 'stanfordcorenlp', 'stanford', 'server', 'openie', 'stanfords', 'edustanfordnlptimetimeexpressionextractorimpl', 'throwing', 'overhead']",
         "['difference stanford corenlp stanford ner', 'stanford corenlp output', 'using stanford corenlp']"
        ],
        [
         "15",
         "14",
         "115",
         "14_dictionary_dictionaries_keys_values",
         "['dictionary', 'dictionaries', 'keys', 'values', 'dict', 'nested', 'key', 'dictionarys', 'fuzzily', 'create']",
         "['extract dictionary', 'data dictionary', 'dictionary']"
        ],
        [
         "16",
         "15",
         "112",
         "15_transformer_transformers_peft_automodelforsequenceclassification",
         "['transformer', 'transformers', 'peft', 'automodelforsequenceclassification', 'decoder', 'automodelforcausallm', 'automodelforseq2seqlm', 'automodel', 'tfbertforsequenceclassification', 'adapters']",
         "['error loading transformer', 'using pretrained transformer keras', 'dataset transformer']"
        ],
        [
         "17",
         "16",
         "107",
         "16_ngrams_ngram_viewer_google",
         "['ngrams', 'ngram', 'viewer', 'google', 'increases', 'efficiently', 'generator', 'distinct', 'scala', 'next']",
         "['train language using google ngrams', 'ngrams', 'ngrams']"
        ],
        [
         "18",
         "17",
         "102",
         "17_chatbot_bot_witai_chat",
         "['chatbot', 'bot', 'witai', 'chat', 'rasa', 'messenger', 'chatscript', 'conversation', 'conversational', 'dialogflow']",
         "['handle context chatbot', 'mining chatbot', 'training chatbot']"
        ],
        [
         "19",
         "18",
         "102",
         "18_regex_regular_expression_expressions",
         "['regex', 'regular', 'expression', 'expressions', 'match', 'pattern', 'followed', 'left', 'compiled', 'preceded']",
         "['find using regular expression', 'regular expression', 'extract using regex']"
        ],
        [
         "20",
         "19",
         "101",
         "19_characters_spaces_letters_remove",
         "['characters', 'spaces', 'letters', 'remove', 'special', 'space', 'removing', 'character', 'white', 'alphabet']",
         "['remove special characters except n', 'remove special characters analysis', 'function remove special character space']"
        ],
        [
         "21",
         "20",
         "101",
         "20_keywords_keyword_extraction_search",
         "['keywords', 'keyword', 'extraction', 'search', 'keybert', 'key', 'extract', 'rake', 'recommendation', 'relevant']",
         "['java library keywords extraction input', 'extract keywords tags', 'data extraction keywords']"
        ],
        [
         "22",
         "21",
         "98",
         "21_tokenizer_token_spacy_tokens",
         "['tokenizer', 'token', 'spacy', 'tokens', 'tokenization', 'spacytokenstokentoken', 'incorrect', 'str', 'untokenize', 'spacytokensdocdoc']",
         "['make spacy tokenizer split', 'training tokenizer spacy', 'spacy tokenizer add tokenizer exception']"
        ],
        [
         "23",
         "22",
         "97",
         "22_fasttext_pretrained_getsentencevector_fasttexts",
         "['fasttext', 'pretrained', 'getsentencevector', 'fasttexts', 'gensim', 'load', 'opened', 'textlmdatabunch', 'vec', 'trainsupervised']",
         "['loading pretrained fasttext gensim', 'fasttext using pretrained vector classification', 'convert pretrained fasttext vectors gensim']"
        ],
        [
         "24",
         "23",
         "94",
         "23_count_frequency_frequencies_counting",
         "['count', 'frequency', 'frequencies', 'counting', 'occurrences', 'number', 'times', 'row', 'timer', 'cells']",
         "['counting frequency', 'count frequency within another', 'frequency time count frequency date']"
        ],
        [
         "25",
         "24",
         "93",
         "24_date_dates_time_datetime",
         "['date', 'dates', 'time', 'datetime', 'temporal', 'relative', 'sutime', 'natural', 'year', 'timezone']",
         "['date extraction', 'natural language date time parser java', 'date parsing']"
        ],
        [
         "26",
         "25",
         "92",
         "25_recognition_named_entity_lingpipe",
         "['recognition', 'named', 'entity', 'lingpipe', 'amc', 'namedentity', 'recognizer', 'name', 'entities', 'tools']",
         "['named entity recognition', 'named entity recognition', 'named entity recognition']"
        ],
        [
         "27",
         "26",
         "86",
         "26_matcher_rulebased_rule_matching",
         "['matcher', 'rulebased', 'rule', 'matching', 'pattern', 'spacy', 'phrasematcher', 'matches', 'match', 'patterns']",
         "['spacy matcher find tokens matching custom attribute', 'spacy matcher pattern regex tag', 'spacy custom rule matcher']"
        ],
        [
         "28",
         "27",
         "85",
         "27_pos_tagging_tag_tagger",
         "['pos', 'tagging', 'tag', 'tagger', 'tags', 'frequent', 'creating', 'incorrect', 'partofspeechtagging', 'baumwelch']",
         "['pos tagging', 'pos tagging', 'pos tagging']"
        ],
        [
         "29",
         "28",
         "84",
         "28_doc2vec_vectors_pvdbow_document",
         "['doc2vec', 'vectors', 'pvdbow', 'document', 'doc', 'verses', 'taggeddocument', 'paragraph', 'buildvocab', 'predictions']",
         "['doc2vec tensorflow', 'dataset doc2vec', 'document similarity doc2vec']"
        ],
        [
         "30",
         "29",
         "83",
         "29_pytorch_rnn_theano_loss",
         "['pytorch', 'rnn', 'theano', 'loss', 'bidirectional', 'dataloader', 'batchsize', 'pytorchs', 'gru', 'final']",
         "['getting hidden size error pytorch rnn', 'bidirectional rnn implementation pytorch', 'pytorch rnn html generation']"
        ],
        [
         "31",
         "30",
         "81",
         "30_install_version_installing_spacy",
         "['install', 'version', 'installing', 'spacy', 'anaconda', 'import', 'en', 'conda', 'loading', 'download']",
         "['unable install spacy anaconda', 'install issue spacy package anaconda environment', 'install specific version spacy']"
        ],
        [
         "32",
         "31",
         "81",
         "31_noun_spacy_verb_phrases",
         "['noun', 'spacy', 'verb', 'phrases', 'spacys', 'textcatmultilabel', 'spacypython', 'nounchunks', 'phrase', 'chunks']",
         "['spacy noun phrases locate noun phrase span start end token every nounchunk doc spacy', 'spacy extract specific noun phrase', 'get noun phrases spacy']"
        ],
        [
         "33",
         "32",
         "79",
         "32_tweets_twitter_tweet_tweepy",
         "['tweets', 'twitter', 'tweet', 'tweepy', 'trending', 'def', 'twitters', 'retweets', 'location', 'categorize']",
         "['trying find name specific location tweets', 'processing tool tweets', 'extracting ngrams tweets']"
        ],
        [
         "34",
         "33",
         "78",
         "33_grammar_parsing_parser_free",
         "['grammar', 'parsing', 'parser', 'free', 'earley', 'parse', 'cfg', 'grammars', 'productions', 'formal']",
         "['parsing sentences', 'english grammar parsing php link grammar', 'file parsing grammar']"
        ],
        [
         "35",
         "34",
         "76",
         "34_countvectorizer_sklearns_countvectorizers_sklearn",
         "['countvectorizer', 'sklearns', 'countvectorizers', 'sklearn', 'vocabulary', 'notfittederror', 'wasnt', 'fitted', 'dictvectorizer', 'vectorizer']",
         "['set custom stop sklearn countvectorizer', 'sklearn countvectorizer custom vocabulary', 'classification countvectorizer shape error']"
        ],
        [
         "36",
         "35",
         "76",
         "35_scikitlearn_scikit_svc_sklearn",
         "['scikitlearn', 'scikit', 'svc', 'sklearn', 'learn', 'svm', 'featureunion', 'features', 'expecting', 'pipeline']",
         "['wrong prediction svc classifier scikitlearn', 'use strings training data svm using scikitlearn', 'classification scikitlearn large dataset']"
        ],
        [
         "37",
         "36",
         "76",
         "36_lemmatizer_lemmatization_lemma_lemmatize",
         "['lemmatizer', 'lemmatization', 'lemma', 'lemmatize', 'lemmas', 'wordnet', 'lemmatizing', 'lemmatizers', 'machinelearning', 'lemmatized']",
         "['getting root using wordnet lemmatizer', 'lemmatizer r', 'wordnet lemmatizer r']"
        ],
        [
         "38",
         "37",
         "75",
         "37_spell_correction_spelling_checker",
         "['spell', 'correction', 'spelling', 'checker', 'typos', 'misspelled', 'checking', 'misspellings', 'spellcheck', 'autocorrect']",
         "['spell check andor spell correction java', 'spell checker fused spelling error correction algorithm', 'multiple spelling correction']"
        ],
        [
         "39",
         "38",
         "74",
         "38_names_person_name_company",
         "['names', 'person', 'name', 'company', 'persons', 'human', 'organization', 'identify', 'companies', 'distinguish']",
         "['distinguish persons names organization names structured table column', 'extracting person names named entity recognition using', 'identify names']"
        ],
        [
         "40",
         "39",
         "73",
         "39_processing_natural_language_someone",
         "['processing', 'natural', 'language', 'someone', 'statistical', 'algorithms', 'truecaser', 'dependancy', 'qualitative', 'processingsyntatcticsemanticprogmatic']",
         "['natural language processing', 'natural language processing', 'natural language processing']"
        ],
        [
         "41",
         "40",
         "73",
         "40_html_scrape_scraping_web",
         "['html', 'scrape', 'scraping', 'web', 'page', 'webpage', 'beautifulsoup', 'links', 'http', 'url']",
         "['im trying scrape frequent web page filter stop', 'web scraping amazon website giving http error', 'web scraping getting data']"
        ],
        [
         "42",
         "41",
         "69",
         "41_semantic_similarity_sentences_analogy",
         "['semantic', 'similarity', 'sentences', 'analogy', 'two', 'compare', 'meaning', 'similar', 'phrases', 'appropriate']",
         "['find semantic meaning similarity two', 'find semantic similarity 2 sentences', 'semantic similarity sentences']"
        ],
        [
         "43",
         "42",
         "68",
         "42_question_questions_answering_answer",
         "['question', 'questions', 'answering', 'answer', 'answers', 'yesno', 'choice', 'yes', 'interrogative', 'gpt3']",
         "['generate answer questions using', 'question answering dataset multiple answers', 'questions generation question answering']"
        ],
        [
         "44",
         "43",
         "68",
         "43_typeerror_callable_init_iterable",
         "['typeerror', 'callable', 'init', 'iterable', 'argument', 'str', 'keyerror', 'unpack', 'enough', 'object']",
         "['typeerror init got unexpected keyword argument numsamples', 'typeerror init got multiple values keyword argument encoding', 'typeerror object callable']"
        ],
        [
         "45",
         "44",
         "67",
         "44_gensim_word2vec_epoch_vocabulary",
         "['gensim', 'word2vec', 'epoch', 'vocabulary', 'saveword2vecformat', 'iteratively', 'amount', 'word2vecs', 'mostsimilar', 'gensims']",
         "['different models gensim word2vec', 'gensim word2vec find number vocabulary', 'gensim word2vec training data']"
        ],
        [
         "46",
         "45",
         "66",
         "45_pdf_pdfminer_pdfs_page",
         "['pdf', 'pdfminer', 'pdfs', 'page', 'reading', 'images', 'pdfplumber', 'scanned', 'contents', 'extracting']",
         "['extract specific value pdf using', 'extract two column pdf', 'get data pdf']"
        ],
        [
         "47",
         "46",
         "65",
         "46_classification_learning_classify_machine",
         "['classification', 'learning', 'classify', 'machine', 'reinforcement', 'binary', 'examples', 'perceptron', 'classifier', 'approach']",
         "['classification', 'classification', 'machine learning classification use']"
        ],
        [
         "48",
         "47",
         "65",
         "47_tagger_pos_tagging_nltk",
         "['tagger', 'pos', 'tagging', 'nltk', 'tags', 'tagset', 'nltagger', 'stts', 'otherword', 'postag']",
         "['training tagger custom tags nltk', 'pos tagger without nltk', 'nltk language pos tagger']"
        ],
        [
         "49",
         "48",
         "63",
         "48_cleaning_preprocessing_cloud_clean",
         "['cleaning', 'preprocessing', 'cloud', 'clean', 'amounts', 'data', 'dawg', 'dataset', 'store', 'processed']",
         "['cleaning', 'cleaning', 'cleaning dataset']"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 275
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>4872</td>\n",
       "      <td>-1_find_nltk_language_file</td>\n",
       "      <td>[find, nltk, language, file, sentences, error,...</td>\n",
       "      <td>[identify strings two different lists, compare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>0_pandas_dataframe_column_frame</td>\n",
       "      <td>[pandas, dataframe, column, frame, columns, ro...</td>\n",
       "      <td>[implement function pandas dataframe column, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>344</td>\n",
       "      <td>1_bert_bertmodel_embeddings_fine</td>\n",
       "      <td>[bert, bertmodel, embeddings, fine, finetuning...</td>\n",
       "      <td>[language training bert, bert classification, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>285</td>\n",
       "      <td>2_sentiment_analysis_reviews_aspect</td>\n",
       "      <td>[sentiment, analysis, reviews, aspect, negativ...</td>\n",
       "      <td>[sentiment analysis object attribute sentiment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>262</td>\n",
       "      <td>3_word2vec_text2vec_vectors_pretrained</td>\n",
       "      <td>[word2vec, text2vec, vectors, pretrained, vect...</td>\n",
       "      <td>[using word2vec embedding sentences, using wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>269</td>\n",
       "      <td>10</td>\n",
       "      <td>269_ner_rulers_identity_types</td>\n",
       "      <td>[ner, rulers, identity, types, entities, overa...</td>\n",
       "      <td>[large difference overall f score custom spacy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>270_layoutlm_javamodel_falcon7b40b_simpletrans...</td>\n",
       "      <td>[layoutlm, javamodel, falcon7b40b, simpletrans...</td>\n",
       "      <td>[prepare custom training data layoutlm, input ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>271</td>\n",
       "      <td>10</td>\n",
       "      <td>271_java_classifiers_svmhmm_program</td>\n",
       "      <td>[java, classifiers, svmhmm, program, virtual, ...</td>\n",
       "      <td>[classification java, using multiple classifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>272</td>\n",
       "      <td>10</td>\n",
       "      <td>272_cuda_colab_usecuda_torchoutofmemoryerror</td>\n",
       "      <td>[cuda, colab, usecuda, torchoutofmemoryerror, ...</td>\n",
       "      <td>[running process error saying cuda memory, err...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>273</td>\n",
       "      <td>10</td>\n",
       "      <td>273_glove_gloveword2vec_keyedvectorssave_glove...</td>\n",
       "      <td>[glove, gloveword2vec, keyedvectorssave, glove...</td>\n",
       "      <td>[adding additional word2vec glove maybe using ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1   4872                         -1_find_nltk_language_file   \n",
       "1        0    411                    0_pandas_dataframe_column_frame   \n",
       "2        1    344                   1_bert_bertmodel_embeddings_fine   \n",
       "3        2    285                2_sentiment_analysis_reviews_aspect   \n",
       "4        3    262             3_word2vec_text2vec_vectors_pretrained   \n",
       "..     ...    ...                                                ...   \n",
       "270    269     10                      269_ner_rulers_identity_types   \n",
       "271    270     10  270_layoutlm_javamodel_falcon7b40b_simpletrans...   \n",
       "272    271     10                271_java_classifiers_svmhmm_program   \n",
       "273    272     10       272_cuda_colab_usecuda_torchoutofmemoryerror   \n",
       "274    273     10  273_glove_gloveword2vec_keyedvectorssave_glove...   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [find, nltk, language, file, sentences, error,...   \n",
       "1    [pandas, dataframe, column, frame, columns, ro...   \n",
       "2    [bert, bertmodel, embeddings, fine, finetuning...   \n",
       "3    [sentiment, analysis, reviews, aspect, negativ...   \n",
       "4    [word2vec, text2vec, vectors, pretrained, vect...   \n",
       "..                                                 ...   \n",
       "270  [ner, rulers, identity, types, entities, overa...   \n",
       "271  [layoutlm, javamodel, falcon7b40b, simpletrans...   \n",
       "272  [java, classifiers, svmhmm, program, virtual, ...   \n",
       "273  [cuda, colab, usecuda, torchoutofmemoryerror, ...   \n",
       "274  [glove, gloveword2vec, keyedvectorssave, glove...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [identify strings two different lists, compare...  \n",
       "1    [implement function pandas dataframe column, p...  \n",
       "2    [language training bert, bert classification, ...  \n",
       "3    [sentiment analysis object attribute sentiment...  \n",
       "4    [using word2vec embedding sentences, using wor...  \n",
       "..                                                 ...  \n",
       "270  [large difference overall f score custom spacy...  \n",
       "271  [prepare custom training data layoutlm, input ...  \n",
       "272  [classification java, using multiple classifie...  \n",
       "273  [running process error saying cuda memory, err...  \n",
       "274  [adding additional word2vec glove maybe using ...  \n",
       "\n",
       "[275 rows x 5 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a summary of topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "-https://medium.com/@davidlfliang/intro-getting-started-with-text-embeddings-using-bert-9f8c3b98dee6\n",
    "- https://www.sciencedirect.com/science/article/pii/S1877050922008766"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between the two algorithms:\n",
    "\n",
    "DBSCAN is a density-based clustering algorithm, whereas K-Means is a centroid-based clustering algorithm.\n",
    "DBSCAN can discover clusters of arbitrary shapes, whereas K-Means assumes that the clusters are spherical.\n",
    "DBSCAN does not require the number of clusters to be specified in advance, whereas K-Means requires the number of clusters to be specified.\n",
    "DBSCAN is less sensitive to initialization than K-Means.\n",
    "When to use DBSCAN vs. K-Means?\n",
    "\n",
    "Use DBSCAN when the data has irregular shapes or when there is no prior knowledge about the number of clusters.\n",
    "Use K-Means when the data has spherical shapes and when the number of clusters is known beforehand.\n",
    "If you are unsure which algorithm to use, it is always a good idea to try both algorithms and compare their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only using Title and Body of Title to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column only have Title and Body Title\n",
    "df_post_answer['combination_text_only_question'] = df_post_answer['Title_Clean'] + \" \" + df_post_answer['Body_Clean']\n",
    "\n",
    "#Remove noise\n",
    "df_post_answer['combination_text_only_question_no_stopw'] = df_post_answer['combination_text_only_question'].apply(remove_stopwords)\n",
    "\n",
    "#Lemma Text\n",
    "text_for_lemma = df_post_answer['combination_text_only_question_no_stopw'].tolist()\n",
    "lemma_text = lemma_texts_parallel(text_for_lemma)\n",
    "df_post_answer['combination_text_only_question_lemma'] = lemma_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "combination_text_only_question_lemma",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0098696e-f35a-4a63-b74c-b687bef2b56d",
       "rows": [
        [
         "0",
         "presidio spacy nlp engine recognize organization pesel spacy I m use spacy pl_core_news_lg model extract name entity polish text correctly detect organization org people name per output however use presidio pl_core_news_lg model configuration file recognizer correctly detect organization org pesel number even though appear list support entity output even though organization pl_pesel list org list nlp engine support recognizer presidio detect correctly text config file presidio fail detect organization org pesel number pl_pesel spacy correctly detect",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them"
        ],
        [
         "1",
         "gpt2 model huggingface 100 label index training instead pad token understand 100 label i d use prediction include calculate loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label exclude replace pad token implementation use nncrossentropyloss argument ignore_index benefit change i d 100 oppose add argument ignore_index loss set pad token i d result way write make think benefit description ignore_index appear achieve wanted",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted"
        ],
        [
         "2",
         "trouble getting import gensim work colab try import gensim colab get follow error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string relate issue thank",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks"
        ],
        [
         "3",
         "store image instead show server run code find site server would like store image instead show since connect remotely ssh connection via connection code instance one store file locally instead show",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them"
        ],
        [
         "4",
         "presidio langchain experimental detect polish name use presidio / langchain_experimental anonymize text polish detect name eg jan kowalski code output expect name jan kowalski email address anonymize output remain unchanged instal pl_core_news_lg model use miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing use output look like mention use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio work expect",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected"
        ],
        [
         "5",
         "opennlp postaggerme chunkerme synergy I m try use opennlp chunk api chunk portuguese sentence first tokenized sentence use tokenizerme tag postaggerme use readymade model provide project sentence ivo viu uva postaggerme return tag propn verb det noun model seem use ud pos tag readymade model chunkerme portuguese follow instruction training first use chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything work well sentence chunker produce correct tag bnp bvp bnp inp complex sentence have not produce good result try identify could improve chunker training one thing notice difference type tag portuguese corpus bosque 80 seem use portuguese tag example instead propn corpus use prop instead det use art seem could lead problem especially since one parameter chunker receive array ud tag train another type tag write code create routine convert portuguese notation ud penn want ask indeed impact tool already translation suggestion improve chunker precision / recall",
         "OpenNLP POSTaggerME and ChunkerME synergy Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall"
        ],
        [
         "6",
         "word/ sentence similarity try find give word/ set word similar definition example definition vegetarian user want check set sentence like try use sentence transformer like give expect result good approach achieve result like doable nlp/ technique",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique"
        ],
        [
         "7",
         "underfitte pretraine glove + lstm model accurcacy unchanged sentiment classification use pretraine glove lstm model use google play review scrap result 50k++ text implement random sampling minority class however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue"
        ],
        [
         "8",
         "can not compile marian nmt I m use endeavouros I m try compile marian instruction fail error message seemingly indicate conflict code c++20 file repo line step follow result please help",
         "Cant compile Marian NMT Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help"
        ],
        [
         "9",
         "get custom column model forward function training huggingface trainer use huggingface trainer train cumstom model subclasse llama llm tokenized tokenizer dataset field additionally add 2 custom colunms can not get custom field forward function model new llm fine tuning anyone help would grateful much",
         "how to get custom column in the models forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much"
        ],
        [
         "10",
         "get leaf word reverse stem one python list line solution provide link try get leaf word one stem word use communitycontributed srivastava package imagine short sample word list follow work manually follow go wordbyword timeconsuming list 200 word get follow output n noun adjective v verb r adverb try reversestem entire list one go fail get output think fail save output dictionary eventually would like output list without break noun adjective adverb verb something like",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like"
        ],
        [
         "11",
         "inspect probability bertopic model say build bertopic model use inspecting give single value item would like entire probability vector across topic case want vector 20 probability item word n item k topic would like nxk output",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output"
        ],
        [
         "12",
         "determine popular word english dictionary within dictionary word forgive word awful I m try figure determine use word english language set word dictionary I ve make I ve do research nltk can not seem find function within library matter help need example sentence enjoy cold glass water hot day would return water use word day day conversation sentence essentially need return value frequently use word conversation figure ill likely involve ai time I ve try use ai wind copy paste code do not understand I m try avoid go route help welcome appreciated context decide start project would essentially guess predetermine word base character user say do not computer guess",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess"
        ],
        [
         "13",
         "catelog sentence 5 word represent dataframe 1000 text row also 5 word want know one much represnt text 0 1 every score etc glad recomendation edit represnt = semantic distance word text example let say row 1 text want eat 2 word food house would high score",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in"
        ],
        [
         "14",
         "count frequency word within key word text two set word list first one call second one call goal calculate frequency within 10 word example assume word acquire list look word list within 10 word acquire within 10 word mean 10 word forward key word 10 word backward key word mean forward backward movement list small example expect outcome first row see word list around word acquire base form acquire list similarly list word within 10 word list total search word 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note word basically base form variation like adopt adopt count key word also",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also"
        ],
        [
         "15",
         "error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset get follow error run",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above"
        ],
        [
         "16",
         "euclidian distance word sentence vectorizer dataframe 1000 text row tfidfvectorizer want create new field give distance sentence word want let say word king dfke think take sentence 5 closet word word king make average glad know hear another method",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method"
        ],
        [
         "17",
         "llama321binstruct generate inconsistent output want use model although set still generate inconsistent output pipeline look like idea solve issue",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue"
        ],
        [
         "18",
         "use aws service execute python script extract keyword text use keybert simple python script give two block text extract keyword use keybert compare list keyword sort two list depend list share keyword aws service would well fit need want able esentially spin need give block text execute return result do not want integrate project do not use python I ve attempt use lambda I m concerned potential cost run thank",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks"
        ],
        [
         "19",
         "normalization token embedding bert encoder block follow multiheade attention layer bert encoder block layer normalization done separately embed token ie one mean variance per token embed concatenate vector token embedding mean variance embedding",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings"
        ],
        [
         "20",
         "convert character index bert token indices work questionanswer dataset map characterbase answer index tokenbase index tokenize context question together use tokenizer like bert heres example row dataset tokenization answer indice 56 16 want create new dataset answer token indices eg 56 ad 60 linkedin learn class instructor conversion create csv file share code expect result",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result"
        ],
        [
         "21",
         "share complex spacy nlp model across multiple python process minimize memory usage I m work multiprocesse python application multiple process need access large preloade spacy nlp model eg en_core_web_lg since model memoryintensive want avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage look multiprocessingmanager multiprocessingshared_memory approach seem well suited numpy array raw datum buffer simple object complex object internal reference like nlp model also look mpis mpiwinallocate_shared run issue use redis server make rank 0 processing work mpi since process do single rank defeat propose use multiprocesse efficient way share spacy model instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp model memory across process multiprocessingmanager shared_memory viable way improve performance reduce memory overhead work complex object suggestion example would greatly appreciate thank",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you"
        ],
        [
         "22",
         "dutch sentiment analysis robbertje outputs positive / negative label netural label miss run dutch sentiment analysis robbertje outputs positive / negative label netural label miss datum obvious neutral sentence / word eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral label example robbertje output",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output"
        ],
        [
         "23",
         "find root form verb use curiosityai / catalyst I m try find root form verb run text pipeline identify tokens match do not know continue far help greatly appreciate",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated"
        ],
        [
         "24",
         "possible get embedding nvembe use candle want cli program output embedding arbitrary input want inference embedding model choose framework choice candle also look mistralrs basically I m try code fragment rust candle try start mistral candle example nvembed hf page say replace model i d original code able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail way around maybe candlebase open source work could use inspiration maybe that s common mistake could easily diagnose",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed"
        ],
        [
         "25",
         "derive attribute / label short plain text description ner llm derive attribute / label short plain text description ner llm short product description i d like transform structure attribute example input output everything format would trivial write regular expression do many different format nuance increasingly cumbersome hardcode logic format try create generic solution immediately run issue basic approach several different datum provider format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even give provider may multiple format may change time format always strictly follow many way express particular component example weight might express one 15l 1 1/2 liter 1500ml etc part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year etc I m use wine example sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work hard get right there s straightforward method would good know I d like develop generalpurpose function accept description format little experience nlp / artificial intelligence suspect useful tool / algo leverage 1000 + example record could potentially use train model something run locally would prefer necessary I m look specific implementation guidance anyone who s work similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource I ve look online information many approach involve significant amount front work unclear they ll work practical sense",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense"
        ],
        [
         "26",
         "vary embed dim due change padding batch size want train simple neural network embedding_dim parameter load datum use torchs dataloader custom collate_fn collate_fn_padd function look follow problem every batch want train model embed text get pad differently long take long sequence current batch mean embed dim / input size linear layer neural network change batch batch althoug want size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collate_fn_pad function padd sequence size independet batch size",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size"
        ],
        [
         "27",
         "adjust performance tokenizer working tokenizer library hug face tokenizer work fine case case I m wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case use specific type tokenizer unigram tokenizer model",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is"
        ],
        [
         "28",
         "spacy get lemmas string panda datum frame column text value document want apply lemmatization value spacy library use panda function I ve define function iterate word document concatenate correspond lemmas output string however slow way extract lemmatize form document spacy",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy"
        ],
        [
         "29",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency word phrase corpus well number document word phrase occur world australian occur two dictionary key peep indig key content intend mutually exclusive similarly australia oz australia post foreign foreign multinat farm / farmer dairy farmer occur two dictionary key intend count accord dictionary expect overall frequency count extract pattern column kwic table report x2 note word industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table doc name column key three question problem / issue could affect output accuracy use approach well / more parsimonius approach achieve trying would well way extract equivalent tetxstat frequency count datum kwic table",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table"
        ],
        [
         "30",
         "seq2seq trainertrain keep give indexing error try machine translation hindi sanskrit use nllb model keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb model ` facebook / nllb20013b input datum ~40k hindi sentence error arises try train sample datum also detail error message code preprocessing do data datum preprocesse code model param train idea error persist",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting"
        ],
        [
         "31",
         "alternative device_map = auto huggingface pretraine model reading huggingface use follow code read model modification internal layer add layer start training / finetune get everything model investigation find custom layer be not distribute multi gpus original model need something like read model simply something like",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like"
        ],
        [
         "32",
         "weight mistral model reinitialize huggingface one reinitialize weight hug face llama v2 model official way original model there s different suggestion reinitialize model try seem work layer reinitialize function use xaiver / he initialization random initialization",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization"
        ],
        [
         "33",
         "break first per sequence find spacy try extract first speaker name list text use spacy currently function return per tag want reduce overhead get first contiguous sequence per entity here example output get want result code currently use I m look want modify find_per_sequence function return first contiguous sequence per entity text ignore subsequent per entity encounter different type entity provide function return multiple name partial name need way ensure first name sequence include achieve",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this"
        ],
        [
         "34",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin recently get follow error lora small llm see discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not find anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell"
        ],
        [
         "35",
         "finetune pretraine model quantization amp scaler error attempt unscale fp16 gradient try finetune pretraine model limited vram achieve use quantization automatic mixed precision amp however encounter issue can not seem resolve could please help identify problem minimal example line error occur",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs"
        ],
        [
         "36",
         "keep train pytorch model new datum I m work text classification task decide use pytorch model purpose process mainly involve follow step load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new datum however every day need classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire model process timeconsume new comment lose validation would like create new dataset newly classify text continue train new datum new comment classify manually label correct use gpt online code write desire process however I m sure working expect I m make silly mistake happen main question could check proposse way solve problem work expect vectorizer face new token would loose original vectorizer full training process propose code dataset find",
         "Keep training pytorch model on new data Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here"
        ],
        [
         "37",
         "capitalize word sentiment analysis I m currently work datum customer review product sephora task classify sentiment negative neutral positive common technique text preprocesse low case word situation upper case word like amazing hide significant emotion behind turn word low case cause information loss would happy opinion subject still low case word personally think create class distinction sentiment good good positive include importance upper case word current code",
         "Capitalized words in sentiment analysis Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code"
        ],
        [
         "38",
         "import name split_torch_state_dict_into_shard huggingface_hub I ve use llama 2 research month import follows always work however today show follow error runtimeerror fail import transformersmodelsllamamodeling_llama follow error look see traceback fail import transformersgenerationutil follow error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt / conda / lib / python310 / sitepackage / huggingface_hub/ init py recreate hug face token do not work use google colab kaggle notebook",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook"
        ],
        [
         "39",
         "process datum gpu instead ram python code I m currently use follow code process audio data run ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific change much appreciate",
         "How to Process Data on GPU Instead of RAM for This Python Code Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated"
        ],
        [
         "40",
         "visualize crossattention matrix marianmtmodel output generation work machine translation task use marianmtmodel hug face transformer library specifically want visualize crossattention matrix model translation process however encounter difficulty achieve I ve try initial attempt notice crossattention matrix directly return model generate translation example find involved feeding source text translation model however goal access crossattention matrix model generate output translation give use forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache use_cache = false capture full matrix last step here implementation approach seem work capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question give issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate I m currently reshape key k query q tensor head dimension multiplication want ask advice case there s easy effective way",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this"
        ],
        [
         "41",
         "do not permute positional encoding bert affect output expect work jupyter notebook transformer section positional encoding want demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply model do not use mask attention like gpt2 however attempt approach bert model use crossattention predict mask token encounter unexpected result expect happen permutation cause model predict different token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes result align expectation time permute one aspect either input ids positional embedding lead different outcome even though occasionally produce result question something else hug face bert model might influence position beyond positional encoding completeness include full code part notebook try directly important part happen",
         "Why doesnt permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in"
        ],
        [
         "42",
         "openaiembedding work create single vector size 1536 whole text corpus I m work class use model accord documentation generate 1536dimensional vector input text however I m bit confused work 1536dimensional vector generate entire input text 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph expect 100 word input text expect openaiembedding would output 100 vector size 1536 output single vector size 1536 whole input text expect learning I ve understand embedding like word2vec glove provide vector word corpus differ approach take openaiembedding I m try understand whether there s way extract embedding individual word use model output always single vector represent whole input insight example would greatly appreciate",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated"
        ],
        [
         "43",
         "ner versus llm extract name gender role company text need extract name gender job title employer / company name newspaper article running process local hardware cloud allow due copyright reason I ve play around llama 31 I m find do not get useable result model small 70b parameter size model run much slowly good hardware throw another small llm might good use few processing resource ner use extract data ner I ve look extract name gender do not know extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get well result fast single llm pass answer training model good model use start point I m much begin machine learn journey would love point right direction thank advance",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance"
        ],
        [
         "44",
         "padding batch sequence affect performance effective attention mask transformer model sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 % curious follow pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow model effectively skip padding token result minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero value computation effectively reduce approximately 10 % thank insight",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights"
        ],
        [
         "45",
         "spacy matcher optional suffix pattern report multiple match text use follow matcher rule text mylabel value get two match mylabel mylabel quite surprising expect single match mylabel add new greedy flag do not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375"
        ],
        [
         "46",
         "` mlflowtransformerslog_model ` finish problem want use log finetune huggingface model however method run simply finish run forever throw error suspect configuration right model big output say problem idea properly example less setup look like run include pseudocode python 3119 & last output get console",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is"
        ],
        [
         "47",
         "nllb finetune error miss data_prefix configuration englishgerman translation I m attempt finetune nllb model scientific translation task english eng_latn german deu_latn follow official guideline finetune author nllb documentation link code block give error error far understand create demo custom data_configjson look like official documentation provide information I m encounter difficulty apply specific use case someone share detailed guide point helpful resource finetune nllb",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB"
        ],
        [
         "48",
         "use structured_output azure openai openai python library want use structured output azure openai try follow code base code given get error fix run use python 311 also try use use azure+ gpt4o mini 20240718 do not work either error message",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message"
        ],
        [
         "49",
         "remove bigrams tokenization tfidfvectorizer I m attempt remove bigram create I m use use preprocessor function test string preprocessor function instantiation error resolve",
         "Removing bigrams after tokenization for TfidfVectorizer Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 8510
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combination_text_only_question_lemma</th>\n",
       "      <th>combination_text_only_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2 model huggingface 100 label index trainin...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trouble getting import gensim work colab try i...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>store image instead show server run code find ...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>algorithm tell semantic similarity two phrase ...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8506</th>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8508</th>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>natural language date / time parser net anyone...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8510 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   combination_text_only_question_lemma  \\\n",
       "0     presidio spacy nlp engine recognize organizati...   \n",
       "1     gpt2 model huggingface 100 label index trainin...   \n",
       "2     trouble getting import gensim work colab try i...   \n",
       "3     store image instead show server run code find ...   \n",
       "4     presidio langchain experimental detect polish ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tell semantic similarity two phrase ...   \n",
       "8506  implement relate degree measure algorithm go a...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple language pri...   \n",
       "8509  natural language date / time parser net anyone...   \n",
       "\n",
       "                         combination_text_only_question  \n",
       "0     Why does Presidio with spacy nlp engine not re...  \n",
       "1     GPT2 and other models from huggingface 100 lab...  \n",
       "2     Trouble getting importing gensim to work in co...  \n",
       "3     Store images instead of showing in a server I ...  \n",
       "4     Presidio with Langchain Experimental does not ...  \n",
       "...                                                 ...  \n",
       "8505  Is there an algorithm that tells the semantic ...  \n",
       "8506  How to implement a related degree measure algo...  \n",
       "8507  How do you implement a Did you mean Possible D...  \n",
       "8508  Vista speech recognition in multiple languages...  \n",
       "8509  Natural language date/time parser for NET Does...  \n",
       "\n",
       "[8510 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post_answer[['combination_text_only_question_lemma','combination_text_only_question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate WC for this only question and body\n",
    "wc_generating(df_post_answer['combination_text_only_question_lemma'],\"First Attempt Combination Question Only\")\n",
    "\n",
    "df_post_answer['combination_text_only_question_lemma_no_noise'] = df_post_answer['combination_text_only_question_lemma'].apply(lambda x: remove_noise_word(x, noise_words2))\n",
    "wc_generating(df_post_answer['combination_text_only_question_lemma_no_noise'],\"Tile and Body Title No Noise Word\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8510 entries, 0 to 8509\n",
      "Data columns (total 23 columns):\n",
      " #   Column                                         Non-Null Count  Dtype  \n",
      "---  ------                                         --------------  -----  \n",
      " 0   index                                          8510 non-null   int64  \n",
      " 1   QuestionId                                     8510 non-null   int64  \n",
      " 2   Title                                          8510 non-null   object \n",
      " 3   Body                                           8510 non-null   object \n",
      " 4   CreationDate                                   8510 non-null   object \n",
      " 5   Score                                          8510 non-null   int64  \n",
      " 6   ViewCount                                      8510 non-null   int64  \n",
      " 7   AnswerCount                                    8510 non-null   int64  \n",
      " 8   AcceptedAnswerId                               8510 non-null   float64\n",
      " 9   AcceptedAnswerBody                             8510 non-null   object \n",
      " 10  AcceptedAnswerScore                            8510 non-null   float64\n",
      " 11  Question_Code                                  8510 non-null   object \n",
      " 12  Answer_Code                                    8510 non-null   object \n",
      " 13  Title_Clean                                    8510 non-null   object \n",
      " 14  Body_Clean                                     8510 non-null   object \n",
      " 15  AcceptedAnswerBody_Clean                       8510 non-null   object \n",
      " 16  combination_text                               8510 non-null   object \n",
      " 17  combination_text_no_stopw                      8510 non-null   object \n",
      " 18  combination_text_clean                         8510 non-null   object \n",
      " 19  combination_text_only_question                 8510 non-null   object \n",
      " 20  combination_text_only_question_no_stopw        8510 non-null   object \n",
      " 21  combination_text_only_question_lemma           8510 non-null   object \n",
      " 22  combination_text_only_question_lemma_no_noise  8510 non-null   object \n",
      "dtypes: float64(2), int64(5), object(16)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_post_answer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_questions = df_post_answer.drop(['index','combination_text_no_stopw','combination_text_clean'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerScore",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Question_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Answer_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerBody_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question_no_stopw",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question_lemma",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question_lemma_no_noise",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c9d34fbb-dc3c-4c4e-9b9f-10153b1748e0",
       "rows": [
        [
         "0",
         "79549787",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?",
         "<p>I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER):</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;pl_core_news_lg&quot;)\ntext = &quot;Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.&quot;\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n</code></pre>\n<p>Output:</p>\n<pre><code>[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n</code></pre>\n<p>However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities.</p>\n<pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=&quot;path_to_my_file/nlp_config.yaml&quot;) \nnlp_engine = provider.create_engine()\n\nprint(f&quot;Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}&quot;)\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[&quot;pl&quot;])\nregistry.load_predefined_recognizers([&quot;pl&quot;])\n\nprint(f&quot;Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}&quot;)\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, &quot;pl&quot;)\n\nfor entity in results:\n    print(f&quot;Found entity: {entity.entity_type} with score {entity.score}&quot;)\n</code></pre>\n<p>Output:</p>\n<pre><code>Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n</code></pre>\n<p>Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text.</p>\n<p>My config file:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n<p>Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?</p>\n",
         "2025-04-02 05:56:11",
         "0",
         "68",
         "1",
         "79552218.0",
         "<p>The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine :</p>\n<pre><code>  labels_to_ignore:\n    - O\n</code></pre>\n<p>On your configuration it would look like this:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n",
         "1.0",
         "import spacy\n\nnlp = spacy.load(\"pl_core_news_lg\")\ntext = \"Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.\"\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n---\n[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n---\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=\"path_to_my_file/nlp_config.yaml\") \nnlp_engine = provider.create_engine()\n\nprint(f\"Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}\")\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[\"pl\"])\nregistry.load_predefined_recognizers([\"pl\"])\n\nprint(f\"Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}\")\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, \"pl\")\n\nfor entity in results:\n    print(f\"Found entity: {entity.entity_type} with score {entity.score}\")\n---\nSupported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "labels_to_ignore:\n    - O\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does",
         "Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them",
         "The configuration file is missing the labels_to_ignore field stating that no entities should be ignored in the nlp engine On your configuration it would look like this",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them The configuration file is missing the labels_to_ignore field stating that no entities should be ignored in the nlp engine On your configuration it would look like this",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them",
         "presidio spacy nlp engine recognize organizations pesel spacy im using spacy pl_core_news_lg model extract named entities polish text correctly detects organizations org peoples names per output however use presidio pl_core_news_lg model configuration file recognizers correctly detect organizations org pesel numbers even though appear list supported entities output even though organization pl_pesel listed org listed nlp engine supported recognizers presidio detect correctly text config file presidio fail detect organizations org pesel numbers pl_pesel spacy correctly detects",
         "presidio spacy nlp engine recognize organization pesel spacy I m use spacy pl_core_news_lg model extract name entity polish text correctly detect organization org people name per output however use presidio pl_core_news_lg model configuration file recognizer correctly detect organization org pesel number even though appear list support entity output even though organization pl_pesel list org list nlp engine support recognizer presidio detect correctly text config file presidio fail detect organization org pesel number pl_pesel spacy correctly detect",
         "presidio spacy nlp engine recognize organization pesel spacy I spacy plcorenewslg extract name entity polish correctly detect organization org people name per however presidio plcorenewslg configuration recognizer correctly detect organization org pesel number even though appear support entity even though organization plpesel org nlp engine support recognizer presidio detect correctly config presidio fail detect organization org pesel number plpesel spacy correctly detect"
        ],
        [
         "1",
         "79548202",
         "GPT-2 and other models from huggingface -100 label index for training, instead of pad token",
         "<p>I understand the -100 label id is used so that the predictions for these are not included when calculating the loss.</p>\n<p>However on <a href=\"https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16#bert2gpt2-summarization-with-%F0%9F%A4%97-encoderdecoder-framework\" rel=\"nofollow noreferrer\">huggingface</a>, they state\n&quot;complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not&quot;, when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument &quot;ignore_index&quot;.</p>\n<p>Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same?</p>\n<p>The way it is written makes me think there is some benefit, but the description of &quot;ignore_index&quot; appears to achieve what is wanted.</p>\n",
         "2025-04-01 09:21:17",
         "0",
         "46",
         "1",
         "79551169.0",
         "<p>The author of the tutorial you mentioned sets it to <code>-100</code> <strong>and</strong> uses <code>ignore_index</code> to save a few lines of code. You don't see the line where the author pass something to <code>ignore_index</code> because it has a default value. The default value of <code>ignore_index</code> for <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\" rel=\"nofollow noreferrer\">nn.CrossEntropyLoss</a> is <code>-100</code>. Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.</p>\n",
         "1.0",
         "",
         "-100\n---\nignore_index\n---\nignore_index\n---\nignore_index\n---\n-100",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token",
         "I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted",
         "The author of the tutorial you mentioned sets it to and uses to save a few lines of code You dont see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you dont have to pass the pad token id from tokenizer down to the loss function",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted The author of the tutorial you mentioned sets it to and uses to save a few lines of code You dont see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you dont have to pass the pad token id from tokenizer down to the loss function",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted",
         "gpt2 models huggingface 100 label index training instead pad token understand 100 label id used predictions included calculating loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label excluded replacing pad tokens implementation use nncrossentropyloss argument ignore_index benefit changing id 100 opposed adding argument ignore_index loss setting pad token id results way written makes think benefit description ignore_index appears achieve wanted",
         "gpt2 model huggingface 100 label index training instead pad token understand 100 label i d use prediction include calculate loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label exclude replace pad token implementation use nncrossentropyloss argument ignore_index benefit change i d 100 oppose add argument ignore_index loss set pad token i d result way write make think benefit description ignore_index appear achieve wanted",
         "gpt2 huggingface 100 label index training instead pad token understand 100 label i d prediction include calculate loss however huggingface state complicated comprehension padtokenid alone good enough whether label exclude replace pad token implementation nncrossentropyloss argument ignoreindex benefit change i d 100 oppose add argument ignoreindex loss set pad token i d write make think benefit description ignoreindex appear achieve wanted"
        ],
        [
         "2",
         "79523269",
         "Trouble getting importing gensim to work in colab",
         "<p>I am trying to import gensim into colab.</p>\n<pre><code>!pip install gensim\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--&gt; 367             msg = (&quot;The current Numpy installation ({!r}) fails to &quot;\n    368                    &quot;pass simple sanity checks. This can be caused for example &quot;\n    369                    &quot;by incorrect BLAS library being linked in, or by mixing &quot;\n\nModuleNotFoundError: No module named 'numpy.char'\n</code></pre>\n<p>my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks</p>\n",
         "2025-03-20 14:36:02",
         "0",
         "125",
         "1",
         "79523777.0",
         "<p>You have to restart the session for the underlying runtime to notice the package changes. See: <a href=\"https://stackoverflow.com/a/79518359/130288\">https://stackoverflow.com/a/79518359/130288</a></p>\n<p>I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded <code>numpy</code>/etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade.</p>\n<p>But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two.</p>\n<p>Explicitly restarting after the Gensim-install &amp; <code>numpy</code>/<code>scipy</code> downgrades resolves the errors.</p>\n",
         "1.0",
         "!pip install gensim\n---\n/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--> 367             msg = (\"The current Numpy installation ({!r}) fails to \"\n    368                    \"pass simple sanity checks. This can be caused for example \"\n    369                    \"by incorrect BLAS library being linked in, or by mixing \"\n\nModuleNotFoundError: No module named 'numpy.char'",
         "numpy\n---\nnumpy\n---\nscipy",
         "Trouble getting importing gensim to work in colab",
         "I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks",
         "You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadnt yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadnt yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks",
         "trouble getting importing gensim work colab trying import gensim colab get following error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string related issue thanks",
         "trouble getting import gensim work colab try import gensim colab get follow error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string relate issue thank",
         "trouble getting import gensim colab import gensim colab get error numpy version 202 downgrade numpy another version like say 1264 get error always numpy relate issue thank"
        ],
        [
         "3",
         "79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">site</a> in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?</p>\n",
         "2025-03-11 14:50:31",
         "0",
         "36",
         "1",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n",
         "1.0",
         "server\n---\nSSH\n---\nskip_tokens = [1]  # skip the special token for the start of the text <s>\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = \"playing guitar, hiking, and spending time with his family.\"\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)",
         "matplotlib\n---\nshow=True\n---\nfig, ax\n---\nimport matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(\"output.png\")\n#plt.show()  # eventually show it after saving\n---\nfig\n---\nfig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(\"output.png\")",
         "Store images instead of showing in a server",
         "I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them",
         "I cant test it but I checked source code and it uses for this If you remove then it shouldnt show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them I cant test it but I checked source code and it uses for this If you remove then it shouldnt show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them",
         "store images instead showing server running code found site server would like store images instead showing since connected remotely ssh connection via connection code instance one store files locally instead showing",
         "store image instead show server run code find site server would like store image instead show since connect remotely ssh connection via connection code instance one store file locally instead show",
         "store image instead show server run site server would like store image instead show since connect remotely ssh connection via connection instance store locally instead show"
        ],
        [
         "4",
         "79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03 22:27:07",
         "4",
         "230",
         "2",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n",
         "-2.0",
         "from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"pl\", \"model_name\": \"pl_core_news_lg\"}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                               languages_config=config)\n\ntext = \"Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\"\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(\"Anonymized text:\", anonymized_result)\nprint(\"Deanonymized text:\", deanonymized_result)\nprint(\"Map:\", anonymizer_tool.deanonymizer_mapping)\nprint(\"Anonymized text:\", anon_result)\n---\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\npython -m spacy download pl_core_news_lg\n---\nanonymizer_tool = PresidioReversibleAnonymizer()\n---\nAnonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n---\nnlp = spacy.load(\"pl_core_news_lg\")\ndoc = nlp(text)\n---\nJan Kowalski persName\nWarszawie placeName",
         "config = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": 'pl', \"model_name\": \"pl_core_news_lg\"}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=\"pl\",\n    supported_entities=[\"persName\"]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"CREDIT_CARD\"], languages_config=config)\n---\nAnonymized text: <persName> mieszka w Warszawie i ma e-mail glenn58@example.org.\n---\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\nMap: {'persName': {'<persName>': 'Jan Kowalski', '<persName_2>': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}\n---\nSpacyRecognizer\n---\nsupported_entities",
         "Presidio with Langchain Experimental does not detect Polish names",
         "I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected",
         "After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCys requirements I believe theres something missing or unclear in the documentation which is causing the misunderstanding",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCys requirements I believe theres something missing or unclear in the documentation which is causing the misunderstanding",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected",
         "presidio langchain experimental detect polish names using presidio/langchain_experimental anonymize text polish detect names eg jan kowalski code output expected name jan kowalski email address anonymized output remains unchanged installed pl_core_news_lg model using missing something configuration presidio support polish entity recognition properly suggestions make detect names polish interesting thing use output look like mentioned use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio works expected",
         "presidio langchain experimental detect polish name use presidio / langchain_experimental anonymize text polish detect name eg jan kowalski code output expect name jan kowalski email address anonymize output remain unchanged instal pl_core_news_lg model use miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing use output look like mention use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio work expect",
         "presidio langchain experimental detect polish name presidio langchainexperimental anonymize polish detect name eg jan kowalski expect name jan kowalski email address anonymize remain unchanged instal plcorenewslg miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing like mention spacy correct guess problem presidio spacy would like create custom analyzer spacy presidio expect"
        ],
        [
         "5",
         "79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22 16:06:11",
         "-1",
         "40",
         "1",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n",
         "1.0",
         "",
         "",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall",
         "Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isnt The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "OpenNLP POSTaggerME and ChunkerME synergy Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isnt The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "OpenNLP POSTaggerME and ChunkerME synergy Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall",
         "opennlp postaggerme chunkerme synergy im trying use opennlp chunking api chunk portuguese sentence first tokenized sentence using tokenizerme tagged postaggerme used readymade models provided project sentence ivo viu uva postaggerme returns tags propn verb det noun model seems using ud pos tags readymade model chunkerme portuguese followed instructions training first using chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything worked well sentence chunker produced correct tags bnp bvp bnp inp complex sentences hasnt produced good results trying identify could improve chunker training one things noticed difference types tags portuguese corpus bosque 80 seems using portuguese tags example instead propn corpus uses prop instead det uses art seems could lead problems especially since one parameters chunker receives array ud tags trained another type tag writing code creating routine convert portuguese notation ud penn wanted ask indeed impact tool already translation suggestions improving chunker precision/recall",
         "opennlp postaggerme chunkerme synergy I m try use opennlp chunk api chunk portuguese sentence first tokenized sentence use tokenizerme tag postaggerme use readymade model provide project sentence ivo viu uva postaggerme return tag propn verb det noun model seem use ud pos tag readymade model chunkerme portuguese follow instruction training first use chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything work well sentence chunker produce correct tag bnp bvp bnp inp complex sentence have not produce good result try identify could improve chunker training one thing notice difference type tag portuguese corpus bosque 80 seem use portuguese tag example instead propn corpus use prop instead det use art seem could lead problem especially since one parameter chunker receive array ud tag train another type tag write code create routine convert portuguese notation ud penn want ask indeed impact tool already translation suggestion improve chunker precision / recall",
         "opennlp postaggerme chunkerme synergy I opennlp chunk api chunk portuguese first tokenized tokenizerme tag postaggerme readymade provide project ivo viu uva postaggerme return tag propn verb det noun ud pos tag readymade chunkerme portuguese instruction training first chunkerconverter tool convert arvore deitada conll2000 generating chunkertrainerme tool everything chunker produce correct tag bnp bvp bnp inp complex have not produce good identify could improve chunker training thing notice difference type tag portuguese corpus bosque 80 portuguese tag instead propn corpus prop instead det art could lead problem especially since parameter chunker receive array ud tag train another type tag write create routine convert portuguese notation ud penn ask indeed impact tool already translation suggestion improve chunker precision recall"
        ],
        [
         "6",
         "79451974",
         "word/ sentence similarities",
         "<p>I am trying to find if a given word/ set of words are similar to a definition.</p>\n<p>Example - Definition - &quot;vegetarian User&quot;</p>\n<p>Now, if I want to check a set of sentences like below</p>\n<pre><code>sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n</code></pre>\n<p>I tried using some sentence transformer like below</p>\n<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n</code></pre>\n<p>But this is not giving me expected results.</p>\n<p>What is the best approach to achieve results like below?</p>\n<pre><code>[False,True,True,False]\n</code></pre>\n<p>Is it doable with nlp/ some other technique?</p>\n",
         "2025-02-19 15:47:45",
         "1",
         "50",
         "1",
         "79461281.0",
         "<p>Yes, it’s definitely doable using NLP! The key here is that you don’t need a full similarity matrix; you want to check if each sentence is semantically similar to the given definition.</p>\n<p>✅ Better Approach:\nEncode both the definition and sentences using a sentence transformer.\nCompute cosine similarity between the definition embedding and each sentence embedding.\nSet a threshold (e.g., 0.6 or 0.7) to determine if they are &quot;similar enough.&quot;</p>\n<pre><code>from sentence_transformers import SentenceTransformer, util\n# Load the pre-trained model\nmodel = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\n\n# Definition and sentences\ndefinition = &quot;vegetarian User&quot;\nsentences = [\n  'vegetarian User',\n  'user sometimes eats chicken',\n  'user is vegetarian',\n  'user only eats fruits',\n  'user likes fish'\n]\n\n# Encode the definition and sentences\ndefinition_embedding = model.encode(definition, convert_to_tensor=True)\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine similarities\nsimilarities = util.cos_sim(definition_embedding, sentence_embeddings)[0]\n\n# Set a threshold for similarity (tune this value as needed)\nthreshold = 0.6\nresults = [sim &gt;= threshold for sim in similarities]\n\n# Print results\nprint(results)  # Example output: [True, False, True, False, False]\n</code></pre>\n<p>💡 Explanation:\nutil.cos_sim computes the cosine similarity between the definition and each sentence.\nThreshold tuning:\nIf the similarity is above the threshold, consider it True.\nAdjust the threshold based on how strict you want the matching.</p>\n<p>🔍 Why the original approach didn’t work:\nmodel.similarity doesn’t exist in the SentenceTransformers API.\nYou were computing a sentence-to-sentence matrix, not definition-to-sentence comparisons.</p>\n",
         "1.0",
         "sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n---\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n---\n[False,True,True,False]",
         "from sentence_transformers import SentenceTransformer, util\n# Load the pre-trained model\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\n\n# Definition and sentences\ndefinition = \"vegetarian User\"\nsentences = [\n  'vegetarian User',\n  'user sometimes eats chicken',\n  'user is vegetarian',\n  'user only eats fruits',\n  'user likes fish'\n]\n\n# Encode the definition and sentences\ndefinition_embedding = model.encode(definition, convert_to_tensor=True)\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine similarities\nsimilarities = util.cos_sim(definition_embedding, sentence_embeddings)[0]\n\n# Set a threshold for similarity (tune this value as needed)\nthreshold = 0.6\nresults = [sim >= threshold for sim in similarities]\n\n# Print results\nprint(results)  # Example output: [True, False, True, False, False]",
         "word/ sentence similarities",
         "I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique",
         "Yes its definitely doable using NLP The key here is that you dont need a full similarity matrix you want to check if each sentence is semantically similar to the given definition Better Approach Encode both the definition and sentences using a sentence transformer Compute cosine similarity between the definition embedding and each sentence embedding Set a threshold eg 06 or 07 to determine if they are similar enough Explanation utilcos_sim computes the cosine similarity between the definition and each sentence Threshold tuning If the similarity is above the threshold consider it True Adjust the threshold based on how strict you want the matching Why the original approach didnt work modelsimilarity doesnt exist in the SentenceTransformers API You were computing a sentencetosentence matrix not definitiontosentence comparisons",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique Yes its definitely doable using NLP The key here is that you dont need a full similarity matrix you want to check if each sentence is semantically similar to the given definition Better Approach Encode both the definition and sentences using a sentence transformer Compute cosine similarity between the definition embedding and each sentence embedding Set a threshold eg 06 or 07 to determine if they are similar enough Explanation utilcos_sim computes the cosine similarity between the definition and each sentence Threshold tuning If the similarity is above the threshold consider it True Adjust the threshold based on how strict you want the matching Why the original approach didnt work modelsimilarity doesnt exist in the SentenceTransformers API You were computing a sentencetosentence matrix not definitiontosentence comparisons",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique",
         "word/ sentence similarities trying find given word/ set words similar definition example definition vegetarian user want check set sentences like tried using sentence transformer like giving expected results best approach achieve results like doable nlp/ technique",
         "word/ sentence similarity try find give word/ set word similar definition example definition vegetarian user want check set sentence like try use sentence transformer like give expect result good approach achieve result like doable nlp/ technique",
         "similarity set similar definition definition vegetarian user check set like transformer like expect good approach achieve like doable nlp technique"
        ],
        [
         "7",
         "79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07 02:48:25",
         "-1",
         "45",
         "1",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n",
         "0.0",
         "lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()",
         "",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged",
         "I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue",
         "Based on extra information in the comments Im going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort Im fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldnt find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they dont handle multiple meanings of the word So queen might be a female king as in embeddings party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers dont have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue Based on extra information in the comments Im going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort Im fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldnt find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they dont handle multiple meanings of the word So queen might be a female king as in embeddings party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers dont have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue",
         "underfitting pretrained glove + lstm model accurcacy unchanged sentiment classification using pretrained glove lstm model use google play review scrap resulting 50k++ texts implement random sampling minority classes however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embedding size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set padded 140997 200 total x validation set padded 17625 200 total x testing set padded 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue",
         "underfitte pretraine glove + lstm model accurcacy unchanged sentiment classification use pretraine glove lstm model use google play review scrap result 50k++ text implement random sampling minority class however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue",
         "underfitte pretraine glove lstm accurcacy unchanged sentiment classification pretraine glove lstm google play review scrap 50k implement random sampling minority class however train lstm training accuracy remain unchanged several epoch insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling positive 58749 negative 26643 neutral 9106 label distribution random sampling positive 58749 negative 26643 neutral 9106 total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set hot 140997 3 total validation set hot 17625 3 total testing set hot 17625 2003 full enter link description highlight issue"
        ],
        [
         "8",
         "79330283",
         "Can't compile Marian NMT",
         "<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=\"https://marian-nmt.github.io/docs/#installation\" rel=\"nofollow noreferrer\">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>\n<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>\n<p>These are the steps that I followed:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n</code></pre>\n<p>This is the result I had:</p>\n<pre><code>➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2\n</code></pre>\n<p>Please help.</p>\n",
         "2025-01-05 06:04:59",
         "4",
         "68",
         "1",
         "79332711.0",
         "<p>The diagnostic that your build is tripping, <code>Wtemplate-id-cdtor</code>, was introduced\nwith GCC 14.1. It is a warning, not an error, but your build promotes all warnings to\nerrors, so it breaks your build.</p>\n<p>Although your build specifies <code>-std=c++11</code> in <code>src/3rd_party/spdlog/CMakeLists.txt</code>, which\ngenerates the failure, g++-14 emits <code>Wtemplate-id-cdtor</code> to warn you that the code <em>would be</em>\nillegal under the more recent standard c++20 (and later). Then the warning is made an error.</p>\n<p>The warning is made an error by the compile option <code>-Werror</code>. This option is included in the list\nof compile options <code>ALL_WARNINGS</code>, which is created in the top-level <code>marian/CMakeLists.txt</code>\nat line 227 <em>et seq</em>:</p>\n<pre><code># These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n</code></pre>\n<p>and then applied as compile options for the <code>marian</code> library target in <code>src/CMakeLists.txt</code>\nat line 133:</p>\n<pre><code>target_compile_options(marian PRIVATE ${ALL_WARNINGS})\n</code></pre>\n<p>whence the options are operative for the failing compilation of <code>src/CMakeFiles/marian.dir/common/logging.cpp</code>.</p>\n<p>This failure is a bug in the <code>marian</code> repo which you should <a href=\"https://github.com/marian-nmt/marian/issues\" rel=\"nofollow noreferrer\">report to the maintainers</a>, as\nit does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14.</p>\n<p>Pending a fix, you seem to have three interim options to get your build done. Either:</p>\n<ul>\n<li><p>Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:</p>\n<pre><code>/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\n</code></pre>\n</li>\n</ul>\n<p>e.g. make it <code>registry_t(const registry_t&lt;Mutex&gt;&amp;) = delete;</code> in this occurrence.</p>\n<p>Or:</p>\n<ul>\n<li><p>Locally disable <code>-Wtemplate-id-cdtor</code> at each occurrence, e.g:</p>\n<pre><code>#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored &quot;-Wtemplate-id-cdtor&quot;\nregistry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n#pragma GCC diagnostic pop\n</code></pre>\n</li>\n</ul>\n<p>Or:</p>\n<ul>\n<li>Remove <code>-Werror</code> from the <code>ALL_WARNINGS</code> list in <code>marian/CMakeLists.txt</code> so that <code>Wtemplate-id-cdtor</code> remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status).</li>\n</ul>\n<p>I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.</p>\n",
         "4.0",
         "CMakeLists.txt\n---\nset (CMAKE_CXX_STANDARD 11)\n---\ngit clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n---\n➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2",
         "Wtemplate-id-cdtor\n---\n-std=c++11\n---\nsrc/3rd_party/spdlog/CMakeLists.txt\n---\nWtemplate-id-cdtor\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\n# These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n---\nmarian\n---\nsrc/CMakeLists.txt\n---\ntarget_compile_options(marian PRIVATE ${ALL_WARNINGS})\n---\nsrc/CMakeFiles/marian.dir/common/logging.cpp\n---\nmarian\n---\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\n---\nregistry_t(const registry_t<Mutex>&) = delete;\n---\n-Wtemplate-id-cdtor\n---\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wtemplate-id-cdtor\"\nregistry_t<Mutex>(const registry_t<Mutex>&) = delete;\n#pragma GCC diagnostic pop\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\nWtemplate-id-cdtor",
         "Cant compile Marian NMT",
         "Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help",
         "The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I havent tested any of these options as Id need to go to the trouble of installing CUDA",
         "Cant compile Marian NMT Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I havent tested any of these options as Id need to go to the trouble of installing CUDA",
         "Cant compile Marian NMT Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help",
         "cant compile marian nmt im using endeavouros im trying compile marian instructions fails error message seemingly indicates conflict code c++20 files repo line steps followed result please help",
         "can not compile marian nmt I m use endeavouros I m try compile marian instruction fail error message seemingly indicate conflict code c++20 file repo line step follow result please help",
         "can not compile marian nmt I endeavouros I compile marian instruction fail error message seemingly indicate conflict c20 repo line step please help"
        ],
        [
         "9",
         "79328514",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )\n</code></pre>\n<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>\n",
         "2025-01-04 08:57:44",
         "2",
         "34",
         "1",
         "79328698.0",
         "<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>\n<p>To modify the <strong>data collator</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>class CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])\n        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])\n        return batch\n</code></pre>\n<p>then pass it to <code>Trainer</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n</code></pre>\n<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>\n<p>Hope, it will work!</p>\n",
         "1.0",
         "input_ids\n---\nlabels\n---\ninteract_ids\n---\ncandidate_ids\n---\nclass LLMWithCustomLayer(LlamaForCausalLM)\n---\ndef forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )",
         "interact_ids\n---\ncandidate_ids\n---\nclass CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[\"interact_ids\"] = torch.tensor([f[\"interact_ids\"] for f in features])\n        batch[\"candidate_ids\"] = torch.tensor([f[\"candidate_ids\"] for f in features])\n        return batch\n---\nTrainer\n---\ntrainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(\"your-llama-model\"),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n---\nforward()\n---\ninteract_ids\n---\ncandidate_ids",
         "how to get custom column in the models forward function when training with Huggingface Trainer",
         "I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much",
         "You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "how to get custom column in the models forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "how to get custom column in the models forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much",
         "get custom column models forward function training huggingface trainer using huggingface trainer train cumstom model subclassing llama llm tokenized tokenizer dataset fields additionally add 2 custom colunms cant get custom fields forward function model new llm fine tuning anyone help would grateful much",
         "get custom column model forward function training huggingface trainer use huggingface trainer train cumstom model subclasse llama llm tokenized tokenizer dataset field additionally add 2 custom colunms can not get custom field forward function model new llm fine tuning anyone help would grateful much",
         "get custom column forward function training huggingface trainer huggingface trainer train cumstom subclasse llama llm tokenized tokenizer dataset field additionally add 2 custom colunms can not get custom field forward function new llm fine tuning anyone help would grateful much"
        ],
        [
         "10",
         "79312133",
         "Getting all leaf words (reverse stemming) into one Python List",
         "<p>On the same lines as the solution provided <a href=\"https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword\">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>\n<p>Imagine I have a shorter sample word list as follows:</p>\n<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n</code></pre>\n<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>\n<pre><code>get_word_forms(&quot;command&quot;)\n</code></pre>\n<p>and get the following output:</p>\n<pre><code>{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n</code></pre>\n<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>\n<p>If I try to reverse-stem the entire list in one go:</p>\n<pre><code>[get_word_forms(word) for word in sample]\n</code></pre>\n<p>I fail at getting any output:</p>\n<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n</code></pre>\n<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>\n<p>something like:</p>\n<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. \n</code></pre>\n",
         "2024-12-27 15:04:05",
         "1",
         "47",
         "1",
         "79312987.0",
         "<p>One solution using nested list comprehensions after stripping forgotten spaces:</p>\n<pre><code>all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']\n</code></pre>\n",
         "1.0",
         "get_word_forms\n---\nmy_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n---\nget_word_forms(\"command\")\n---\n{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n---\n[get_word_forms(word) for word in sample]\n---\n[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n---\n['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.",
         "all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']",
         "Getting all leaf words reverse stemming into one Python List",
         "On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like",
         "One solution using nested list comprehensions after stripping forgotten spaces",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like One solution using nested list comprehensions after stripping forgotten spaces",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like",
         "getting leaf words reverse stemming one python list lines solution provided link trying get leaf words one stem word using communitycontributed srivastava package imagine shorter sample word list follows work manually following go wordbyword timeconsuming list 200 words get following output n noun adjective v verb r adverb try reversestem entire list one go fail getting output think failing saving output dictionary eventually would like output list without breaking noun adjective adverb verb something like",
         "get leaf word reverse stem one python list line solution provide link try get leaf word one stem word use communitycontributed srivastava package imagine short sample word list follow work manually follow go wordbyword timeconsuming list 200 word get follow output n noun adjective v verb r adverb try reversestem entire list one go fail get output think fail save output dictionary eventually would like output list without break noun adjective adverb verb something like",
         "get leaf reverse stem python line solution provide link get leaf stem communitycontributed srivastava package imagine short sample manually go wordbyword timeconsuming 200 get n noun adjective v verb r adverb reversestem entire go fail get think fail save dictionary eventually would like without break noun adjective adverb verb something like"
        ],
        [
         "11",
         "79298368",
         "Inspect all probabilities of BERTopic model",
         "<p>Say I build a BERTopic model using</p>\n<pre><code>from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre>\n<p>Inspecting <code>probs</code> gives me just a single value for each item in <code>docs</code>.</p>\n<pre><code>probs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n</code></pre>\n<p>I would like the entire probability vector across all topics (so in this case, where <code>nr_topics=20</code>, I want a vector of 20 probabilities for each item in <code>docs</code>). In other words, if I have N items in <code>docs</code> and K topics, I would like an NxK output.</p>\n",
         "2024-12-20 20:49:34",
         "1",
         "52",
         "1",
         "79299703.0",
         "<p>For individual topic probability across each document you need to add one more argument.</p>\n<pre><code>topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n</code></pre>\n<p>Note: This calculate_probabilities = True will only work if you are using <strong><code>HDBSCAN</code></strong> clustering embedding model. And Bertopic by default uses <code>all-MiniLM-L6-v2</code>.</p>\n<p><strong>Official documentation:</strong> <a href=\"https://maartengr.github.io/BERTopic/api/bertopic.html\" rel=\"nofollow noreferrer\">https://maartengr.github.io/BERTopic/api/bertopic.html</a></p>\n<p>They have mentioned the same in document as well.</p>\n",
         "1.0",
         "from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n---\nprobs\n---\ndocs\n---\nprobs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n---\nnr_topics=20\n---\ndocs\n---\ndocs",
         "topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n---\nHDBSCAN\n---\nall-MiniLM-L6-v2",
         "Inspect all probabilities of BERTopic model",
         "Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output",
         "For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output",
         "inspect probabilities bertopic model say build bertopic model using inspecting gives single value item would like entire probability vector across topics case want vector 20 probabilities item words n items k topics would like nxk output",
         "inspect probability bertopic model say build bertopic model use inspecting give single value item would like entire probability vector across topic case want vector 20 probability item word n item k topic would like nxk output",
         "inspect probability bertopic say build bertopic inspecting single value item would like entire probability vector across topic case vector 20 probability item n item k topic would like nxk"
        ],
        [
         "12",
         "79293919",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>\n<p>For example:\nA sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>\n<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>\n<p>Any and all help is welcome and appreciated.</p>\n<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>\n",
         "2024-12-19 10:24:04",
         "0",
         "63",
         "2",
         "79294074.0",
         "<p>You need a external dataset for this task. You can try dataset such as google n gram dataset.</p>\n<p>Here is the breakdown of the problem statement:</p>\n<ol>\n<li>Input: &quot;I enjoy a cold glass of water on a hot day&quot;. <code>Output</code>: &quot;water&quot;.</li>\n<li>Split the sentences into words list.</li>\n</ol>\n<blockquote>\n<p>Example: [&quot;I&quot;, &quot;enjoy&quot;, &quot;a&quot;, &quot;cold&quot;, &quot;glass&quot;, &quot;of&quot;, &quot;water&quot;, &quot;on&quot;,\n&quot;a&quot;, &quot;hot&quot;, &quot;day&quot;]</p>\n</blockquote>\n<ol start=\"3\">\n<li>First loop in through all the word of the sentences. so let say you are at first word &quot;I&quot;.</li>\n<li>Now you will look the same word &quot;I&quot; in external dataset and will look for the frequency of that word.\nLet say the word &quot;I&quot; in external dataset is repeated <code>5000000</code> times</li>\n<li>Repeat this task for all the word.</li>\n<li>Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.\nFrequency in the below example is random value not exact value.</li>\n</ol>\n<blockquote>\n<pre><code>{\n    &quot;I&quot;: 5000000,\n    &quot;enjoy&quot;: 50000,\n    &quot;a&quot;: 10000000,\n    &quot;cold&quot;: 30000,\n    &quot;glass&quot;: 100000,\n    &quot;of&quot;: 8000000,\n    &quot;water&quot;: 1200000,\n    &quot;on&quot;: 6000000,\n    &quot;hot&quot;: 700000,\n    &quot;day&quot;: 400000\n}\n</code></pre>\n</blockquote>\n<ol start=\"7\">\n<li>Pick the word with highest frequency.</li>\n</ol>\n<p>Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself</p>\n",
         "2.0",
         "",
         "Output\n---\n5000000\n---\n{\n    \"I\": 5000000,\n    \"enjoy\": 50000,\n    \"a\": 10000000,\n    \"cold\": 30000,\n    \"glass\": 100000,\n    \"of\": 8000000,\n    \"water\": 1200000,\n    \"on\": 6000000,\n    \"hot\": 700000,\n    \"day\": 400000\n}",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess",
         "You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess",
         "determining popular words english dictionary within dictionary words forgive wording awful im trying figure determine used words english language set words dictionary ive made ive done research nltk cant seem find function within library matter help need example sentence enjoy cold glass water hot day would return water used word day day conversation sentence essentially need returned value frequently used word conversations figure ill likely involve ai time ive tried use ai wind copy pasting code dont understand im trying avoid going route help welcome appreciated context decided start project would essentially guess predetermined word based characters user says doesnt computers guess",
         "determine popular word english dictionary within dictionary word forgive word awful I m try figure determine use word english language set word dictionary I ve make I ve do research nltk can not seem find function within library matter help need example sentence enjoy cold glass water hot day would return water use word day day conversation sentence essentially need return value frequently use word conversation figure ill likely involve ai time I ve try use ai wind copy paste code do not understand I m try avoid go route help welcome appreciated context decide start project would essentially guess predetermine word base character user say do not computer guess",
         "determine popular english dictionary within dictionary forgive awful I figure determine english language set dictionary I ve make I ve do research nltk can not function within library matter help enjoy cold glass water hot day would return water day day conversation essentially return value frequently conversation figure ill likely involve ai time I ve ai wind copy paste do not understand I avoid go route help welcome appreciated context decide start project would essentially guess predetermine base character user say do not computer guess"
        ],
        [
         "13",
         "79293889",
         "catelog sentences into 5 words that represent them",
         "<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>\n<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>\n<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>\n<p>I will glad for recomendations how to do that</p>\n<p><strong>edit</strong></p>\n<p>represnt = the semantic distance between the word to the text.</p>\n<p>for example -\nlets say in row 1 the text is &quot;i want to eat&quot;\nand I have 2 words : food and house.</p>\n<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>\n",
         "2024-12-19 10:16:47",
         "0",
         "54",
         "1",
         "79294099.0",
         "<p>You could use a pre-trained sentence transformer model from <a href=\"https://pypi.org/project/sentence-transformers/\" rel=\"nofollow noreferrer\"><code>sentence_transformers</code></a>:</p>\n<pre><code>import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -&gt; None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -&gt; list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -&gt; pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838\n</code></pre>\n",
         "0.0",
         "df['text']\n---\ndf[\"word1\"]\n---\ndf[\"word2\"]\n---\ndf[\"food \"]\n---\ndf[\"house\"]",
         "sentence_transformers\n---\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -> None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -> list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -> pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n---\ntext  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838",
         "catelog sentences into 5 words that represent them",
         "I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in",
         "You could use a pretrained sentence transformer model from Output",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in You could use a pretrained sentence transformer model from Output",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in",
         "catelog sentences 5 words represent dataframe 1000 text rows also 5 words want know one much represnt text 0 1 every score etc glad recomendations edit represnt = semantic distance word text example lets say row 1 text want eat 2 words food house would higher score",
         "catelog sentence 5 word represent dataframe 1000 text row also 5 word want know one much represnt text 0 1 every score etc glad recomendation edit represnt = semantic distance word text example let say row 1 text want eat 2 word food house would high score",
         "catelog 5 represent dataframe 1000 row also 5 much represnt 0 1 every score glad recomendation edit represnt semantic distance let say row 1 eat 2 food house would high score"
        ],
        [
         "14",
         "79253283",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>\n<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>\n<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n</code></pre>\n<p>A small Example -</p>\n<pre><code>text_dict = {\n    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]\n}\n\ndf = pd.DataFrame(text_dict)\n</code></pre>\n<p>My expected outcome is -</p>\n<pre><code>                 ITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n</code></pre>\n<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>\n<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>\n",
         "2024-12-05 03:05:06",
         "0",
         "81",
         "1",
         "79263000.0",
         "<p>You need to process each row of text by identifying occurrences of <code>key_words</code> and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique <code>search_word</code> found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique <code>search_words</code> near <code>key_words</code>.</p>\n<pre><code>import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        &quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process &quot;\n        &quot;of adopting Symantec. We believe these technologies will improve our access control. &quot;\n        &quot;Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = &quot; &quot;.join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n</code></pre>\n<p>Which returns</p>\n<pre><code>                                               ITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2\n</code></pre>\n",
         "0.0",
         "search words\n---\nkey words\n---\nsearch words\n---\nkey words\n---\nkey words\n---\nsearch words\n---\nsearch word\n---\nkey word\n---\nsearch_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n---\ntext_dict = {\n    'ITEM7':[\"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"]\n}\n\ndf = pd.DataFrame(text_dict)\n---\nITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n---\ndf\n---\nAVG\n---\nAlibaba Security\n---\nsearch_words\n---\nkey_words\n---\nSymantec\n---\nAccess Control\n---\ndata security\n---\ndiagnostic program\n---\nsearch_words\n---\nadopting\n---\nimprove\n---\nintegrated\n---\nkey_words\n---\nFrequency\n---\ndf\n---\nkey_words",
         "key_words\n---\nsearch_word\n---\nsearch_words\n---\nkey_words\n---\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        \"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \"\n        \"of adopting Symantec. We believe these technologies will improve our access control. \"\n        \"Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = \" \".join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n---\nITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also",
         "You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also",
         "counting frequency words within key words text two sets word lists first one called second one called goal calculate frequency within 10 words example assume word acquire list look words list within 10 words acquire within 10 words mean 10 words forward key words 10 words backward key words meaning forward backward movement lists small example expected outcome first row see word list around word acquired base form acquire list similarly list words within 10 words list total search words 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note words basically base form variation like adopted adopting counted key words also",
         "count frequency word within key word text two set word list first one call second one call goal calculate frequency within 10 word example assume word acquire list look word list within 10 word acquire within 10 word mean 10 word forward key word 10 word backward key word mean forward backward movement list small example expect outcome first row see word list around word acquire base form acquire list similarly list word within 10 word list total search word 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note word basically base form variation like adopt adopt count key word also",
         "count frequency within key set first call second call goal calculate frequency within 10 assume acquire within 10 acquire within 10 mean 10 forward key 10 backward key mean forward backward movement small expect outcome first row around acquire base form acquire similarly within 10 total search 6 avgalibaba securitysymantecaccess controldata securitydiagnostic program therefore column value 6 please note basically base form variation like adopt adopt count key also"
        ],
        [
         "15",
         "79247672",
         "Error in getting Captum text explanations for text classification",
         "<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>\n<pre><code>import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f&quot;Test Accuracy: {accuracy:.2f}&quot;)\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n<p>But I am getting the following error in running the above.</p>\n<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n</code></pre>\n",
         "2024-12-03 12:47:45",
         "2",
         "84",
         "1",
         "79248379.0",
         "<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>\n<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=\"https://captum.ai/tutorials/Bert_SQUAD_Interpret\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>\n<p>Below please find snippet that works:</p>\n<pre><code>from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n",
         "1.0",
         "import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)\n---\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-f047b509c98d> in <cell line: 90>()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
         "from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    # print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)",
         "Error in getting Captum text explanations for text classification",
         "I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above",
         "You need to slightly change the gradients calculation class Also you didnt include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above You need to slightly change the gradients calculation class Also you didnt include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above",
         "error getting captum text explanations text classification following code using identify influential words used correctly predict text test dataset getting following error running",
         "error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset get follow error run",
         "error get captum explanation classification identify influential correctly predict test dataset get error run"
        ],
        [
         "16",
         "79247594",
         "euclidian distance from word to sentence after doing Vectorizer",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did TfidfVectorizer.</p>\n<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>\n<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n",
         "2024-12-03 12:25:05",
         "1",
         "44",
         "1",
         "79248087.0",
         "<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([&quot;king&quot;]).toarray()\nexcept KeyError:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n</code></pre>\n<p>which would give you</p>\n<pre><code>                                                text  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0  \n</code></pre>\n<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif &quot;king&quot; in feature_names:\n    king_index = np.where(feature_names == &quot;king&quot;)[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n\n</code></pre>\n<p>which gives</p>\n<pre><code>                                                text  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214\n</code></pre>\n",
         "1.0",
         "",
         "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([\"king\"]).toarray()\nexcept KeyError:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n---\ntext  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0\n---\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif \"king\" in feature_names:\n    king_index = np.where(feature_names == \"king\")[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n---\ntext  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214",
         "euclidian distance from word to sentence after doing Vectorizer",
         "I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method",
         "I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method",
         "euclidian distance word sentence vectorizer dataframe 1000 text rows tfidfvectorizer want create new field give distance sentence word want lets say word king dfking thought taking sentence 5 closet words word king make average glad know hear another method",
         "euclidian distance word sentence vectorizer dataframe 1000 text row tfidfvectorizer want create new field give distance sentence word want let say word king dfke think take sentence 5 closet word word king make average glad know hear another method",
         "euclidian distance vectorizer dataframe 1000 row tfidfvectorizer create new field distance let say king dfke think take 5 closet king make average glad hear another method"
        ],
        [
         "17",
         "79234004",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "<p>I want to use <code>Llama-3.2-1B-Instruct</code> model, and although I have set <code>&quot;temperature&quot;: 0.0, &quot;top_p&quot;:0.0 and &quot;top_k&quot;:0</code>, it still generates inconsistent output. This is how my pipeline looks like:</p>\n<pre><code>pipe = pipeline(\n    &quot;text-generation&quot;,\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=&quot;mps&quot;,\n        model_kwargs={&quot;temperature&quot;: 0.0,\n                  &quot;do_sample&quot;:True,\n                              &quot;top_p&quot;:0.0,\n                              &quot;top_k&quot;:0,},\n)\n</code></pre>\n<p>Any idea how to solve this issue?</p>\n",
         "2024-11-28 13:02:37",
         "1",
         "641",
         "2",
         "79246602.0",
         "<p>The model inconsistent output can be due to two main factors:</p>\n<p><strong>1. Temperature:</strong></p>\n<p>setting temperature to zero give more inconsistent result. You can refer <a href=\"https://community.openai.com/t/why-the-api-output-is-inconsistent-even-after-the-temperature-is-set-to-0/329541/2\" rel=\"nofollow noreferrer\">Opeani discussion page</a> for detail.</p>\n<p>So the best option is to set temperature to very low values such as 0.00001 instead of zero.</p>\n<p><strong>2. do_sample</strong></p>\n<p>You already set it false, and it should remain that way only.</p>\n",
         "1.0",
         "Llama-3.2-1B-Instruct\n---\n\"temperature\": 0.0, \"top_p\":0.0 and \"top_k\":0\n---\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"mps\",\n        model_kwargs={\"temperature\": 0.0,\n                  \"do_sample\":True,\n                              \"top_p\":0.0,\n                              \"top_k\":0,},\n)",
         "",
         "Llama321BInstruct generate inconsistent output",
         "I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue",
         "The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue",
         "llama321binstruct generate inconsistent output want use model although set still generates inconsistent output pipeline looks like idea solve issue",
         "llama321binstruct generate inconsistent output want use model although set still generate inconsistent output pipeline look like idea solve issue",
         "llama321binstruct generate inconsistent although set still generate inconsistent pipeline like idea solve issue"
        ],
        [
         "18",
         "79192130",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>\n<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>\n",
         "2024-11-15 11:13:36",
         "1",
         "56",
         "2",
         "79192427.0",
         "<p>In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:</p>\n<ol>\n<li>Package your KeyBERT script in a lambda and easily deploy it with a container.</li>\n<li>Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.</li>\n</ol>\n",
         "1.0",
         "",
         "",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT",
         "I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks",
         "In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model Im using is resourceintensive and requires GPU acceleration Id go with SageMaker otherwise Lambda is a good solution So for your case heres what Id do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model Im using is resourceintensive and requires GPU acceleration Id go with SageMaker otherwise Lambda is a good solution So for your case heres what Id do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks",
         "using aws service execute python script extract keywords text using keybert simple python script given two blocks text extracts keywords using keybert compares lists keywords sort two lists depending lists share keywords aws service would best fit needs want able esentially spin needed give blocks text execute return results dont want integrate projects dont use python ive attempted use lambda im concerned potential cost running thanks",
         "use aws service execute python script extract keyword text use keybert simple python script give two block text extract keyword use keybert compare list keyword sort two list depend list share keyword aws service would well fit need want able esentially spin need give block text execute return result do not want integrate project do not use python I ve attempt use lambda I m concerned potential cost run thank",
         "aws service execute python script extract keyword keybert simple python script block extract keyword keybert compare keyword sort depend share keyword aws service would fit able esentially spin block execute return do not integrate project do not python I ve attempt lambda I concerned potential cost run thank"
        ],
        [
         "19",
         "79178041",
         "Normalization of token embeddings in BERT encoder blocks",
         "<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>\n",
         "2024-11-11 14:30:31",
         "2",
         "162",
         "2",
         "79238393.0",
         "<p>I tracked down full details of layer normalization (LN) in BERT <a href=\"https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para\">here</a>.</p>\n<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>\n",
         "0.0",
         "",
         "",
         "Normalization of token embeddings in BERT encoder blocks",
         "Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings",
         "I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token its per embedding dimension",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token its per embedding dimension",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings",
         "normalization token embeddings bert encoder blocks following multiheaded attention layer bert encoder block layer normalization done separately embedding token ie one mean variance per token embedding concatenated vector token embeddings mean variance embeddings",
         "normalization token embedding bert encoder block follow multiheade attention layer bert encoder block layer normalization done separately embed token ie one mean variance per token embed concatenate vector token embedding mean variance embedding",
         "normalization token embedding bert encoder block multiheade attention layer bert encoder block layer normalization done separately embed token ie mean variance per token embed concatenate vector token embedding mean variance embedding"
        ],
        [
         "20",
         "79173053",
         "How to convert character indices to BERT token indices",
         "<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>\n<pre><code>from datasets import load_dataset\nds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\n</code></pre>\n<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>\n<pre><code>d0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n</code></pre>\n<p>After tokenization, the answer indices are 56  and 16:</p>\n<pre><code>from transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'\n</code></pre>\n<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>\n<p>This is from a <a href=\"https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false\" rel=\"nofollow noreferrer\">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=\"https://i.sstatic.net/GsZ6mfcQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/GsZ6mfcQ.png\" alt=\"QA dataset with token answer indices\" /></a></p>\n",
         "2024-11-09 15:15:33",
         "2",
         "33",
         "1",
         "79175157.0",
         "<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>\n<p>The following function does the above for you:</p>\n<pre><code>def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start &lt;= char_start &lt; end: \n            start_token_idx = i\n        if start &lt; char_end &lt;= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n</code></pre>\n<p>Here's how you can use and test this function:</p>\n<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])\nprint(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(&quot;Tokenized answer:&quot;, answer_tokens)\n</code></pre>\n<p>Output:</p>\n<pre><code>Tokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream\n</code></pre>\n",
         "2.0",
         "UCLNLP/adversarial_qa\n---\nfrom datasets import load_dataset\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\n---\nd0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n---\nfrom transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'",
         "def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start <= char_start < end: \n            start_token_idx = i\n        if start < char_end <= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n---\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(\"Tokenized start index:\", d0_tokenized['answer_start_token_idx'])\nprint(\"Tokenized end index:\", d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(\"Tokenized answer:\", answer_tokens)\n---\nTokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream",
         "How to convert character indices to BERT token indices",
         "I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result",
         "You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Heres how you can use and test this function Output",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Heres how you can use and test this function Output",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result",
         "convert character indices bert token indices working questionanswer dataset map characterbased answer indices tokenbased indices tokenizing context question together using tokenizer like bert heres example row dataset tokenization answer indices 56 16 want create new dataset answers token indices eg 56 ad 60 linkedin learning class instructor conversion created csv file share code expected result",
         "convert character index bert token indices work questionanswer dataset map characterbase answer index tokenbase index tokenize context question together use tokenizer like bert heres example row dataset tokenization answer indice 56 16 want create new dataset answer token indices eg 56 ad 60 linkedin learn class instructor conversion create csv file share code expect result",
         "convert character index bert token indices questionanswer dataset map characterbase answer index tokenbase index tokenize context question together tokenizer like bert heres row dataset tokenization answer indice 56 16 create new dataset answer token indices eg 56 ad 60 linkedin learn class instructor conversion create csv share expect"
        ],
        [
         "21",
         "79159805",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage?",
         "<p>I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage.</p>\n<p>I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing.</p>\n<ul>\n<li><p>Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process?</p>\n</li>\n<li><p>Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes?</p>\n</li>\n<li><p>If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects?</p>\n</li>\n</ul>\n<p>Any suggestions or examples would be greatly appreciated! Thank you!</p>\n",
         "2024-11-05 15:49:33",
         "3",
         "93",
         "2",
         "79162232.0",
         "<p>I would strongly advise you not to treat NLP models like any other Python object. I would always prefer to load an NLP model using a microservice approach, which is more aligned with ML/software engineering best practices by separating the model logic from the main application.</p>\n<p>Instead of loading the model in each process (which can be memory-intensive), the model is loaded just once in a dedicated service. This setup allows the model to be used by multiple parts of the application without duplicating memory usage, making it efficient, modular, and scalable. Not only is your concern about memory efficiency addressed, but scalability and modularity are also improved.</p>\n<p>An example of implementing such a microservice using FastAPI + Docker could look like this:</p>\n<pre><code># main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(&quot;en_core_web_lg&quot;)  # Load model once\n\n@app.post(&quot;/process/&quot;)\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {&quot;tokens&quot;: [(token.text, token.pos_) for token in doc]}\n</code></pre>\n<p>To containerize above FastAPI service:</p>\n<pre><code># Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt &amp;&amp; python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [&quot;gunicorn&quot;, &quot;-w&quot;, &quot;4&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;main:app&quot;]\n</code></pre>\n",
         "3.0",
         "",
         "# main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(\"en_core_web_lg\")  # Load model once\n\n@app.post(\"/process/\")\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {\"tokens\": [(token.text, token.pos_) for token in doc]}\n---\n# Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt && python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [\"gunicorn\", \"-w\", \"4\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"main:app\"]",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage",
         "Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you",
         "I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you",
         "share complex spacy nlp model across multiple python processes minimize memory usage im working multiprocessing python application multiple processes need access large preloaded spacy nlp model eg en_core_web_lg since model memoryintensive want avoid loading separately process since quickly run main memory object readonly instead id like load shared location processes read without duplicating memory usage looked multiprocessingmanager multiprocessingshared_memory approaches seem better suited numpy arrays raw data buffers simple objects complex objects internal references like nlp model also looked mpis mpiwinallocate_shared ran issues using redis server make rank 0 processing works mpi since processing done single rank defeats propose using multiprocessing efficient way share spacy model instance across multiple processes python avoid reloading process libraries techniques specifically suited sharing complex readonly objects like nlp models memory across processes multiprocessingmanager shared_memory viable ways improve performance reduce memory overhead working complex objects suggestions examples would greatly appreciated thank",
         "share complex spacy nlp model across multiple python process minimize memory usage I m work multiprocesse python application multiple process need access large preloade spacy nlp model eg en_core_web_lg since model memoryintensive want avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage look multiprocessingmanager multiprocessingshared_memory approach seem well suited numpy array raw datum buffer simple object complex object internal reference like nlp model also look mpis mpiwinallocate_shared run issue use redis server make rank 0 processing work mpi since process do single rank defeat propose use multiprocesse efficient way share spacy model instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp model memory across process multiprocessingmanager shared_memory viable way improve performance reduce memory overhead work complex object suggestion example would greatly appreciate thank",
         "share complex spacy nlp across multiple python process minimize memory usage I multiprocesse python application multiple process access large preloade spacy nlp eg encoreweblg since memoryintensive avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage multiprocessingmanager multiprocessingsharedmemory approach suited numpy array raw datum buffer simple object complex object internal reference like nlp also mpis mpiwinallocateshared run issue redis server make rank 0 processing mpi since process do single rank defeat propose multiprocesse efficient share spacy instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp memory across process multiprocessingmanager sharedmemory viable improve performance reduce memory overhead complex object suggestion would greatly appreciate thank"
        ],
        [
         "22",
         "79155290",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing",
         "<p>When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data.</p>\n<p><a href=\"https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment\" rel=\"nofollow noreferrer\">https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment</a></p>\n<p>There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative.</p>\n<p><strong>Is there a way to get neutral labels for such examples in RobBERTje?</strong></p>\n<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = &quot;DTAI-KULeuven/robbert-v2-dutch-sentiment&quot;\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n</code></pre>\n<p>Output:</p>\n<pre><code>[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]\n</code></pre>\n",
         "2024-11-04 11:36:35",
         "2",
         "54",
         "1",
         "79155380.0",
         "<p>This model was trained only on <code>negative</code> and <code>positive</code> labels. Therefore, it will try to categorize every input as positive or negative, even if it is nonsensical or neutral.</p>\n<p>what you can do is to:\n1- Find other models that was trained to include <code>neutral</code> label.\n2- Fine-tune this model on a dataset that includes <code>neutral</code> label.\n3- Empirically define a threshold based on the confidence outputs and interpret it as <code>neutral</code>.</p>\n<p>The first 2 choices are extensive in effort. I would suggest you go with the third option for a quick workaround. Try feeding the model with a few neutral input and observe the range of confidence score in the output. then use that threshold to classify as <code>neutral</code>.</p>\n<p>Here's a sample:</p>\n<pre><code>def classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] &lt; threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result\n</code></pre>\n",
         "3.0",
         "from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n---\n[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]",
         "negative\n---\npositive\n---\nneutral\n---\nneutral\n---\nneutral\n---\nneutral\n---\ndef classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] < threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing",
         "When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output",
         "This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Heres a sample",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Heres a sample",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output",
         "dutch sentiment analysis robbertje outputs positive/negative labels netural label missing run dutch sentiment analysis robbertje outputs positive/negative labels netural label missing data obvious neutral sentences/words eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral labels examples robbertje output",
         "dutch sentiment analysis robbertje outputs positive / negative label netural label miss run dutch sentiment analysis robbertje outputs positive / negative label netural label miss datum obvious neutral sentence / word eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral label example robbertje output",
         "dutch sentiment analysis robbertje outputs positive negative label netural label miss run dutch sentiment analysis robbertje outputs positive negative label netural label miss datum obvious neutral eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative get neutral label robbertje"
        ],
        [
         "23",
         "79148979",
         "Finding Root Form of Verbs using Curiosity-AI/Catalyst",
         "<p>I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match <code>PartOfSpeech.VERB</code> but I don't know how to continue from there.</p>\n<p>This is what I have so far:</p>\n<pre><code>const string text = &quot;The disastrous cat runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}\n</code></pre>\n<p>Any help is greatly appreciated.</p>\n",
         "2024-11-01 17:58:01",
         "2",
         "135",
         "1",
         "79160163.0",
         "<p>The following code (targeting .NET 8.0) illustrates one method to obtain the root form of a verb from an inflected form.</p>\n<p>(I have annonoted, as code comments, the three NuGet packages (with versions) required. Most of the code is identical to your original sample above.)</p>\n<pre><code>//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = &quot;The disastrous cat quickly runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($&quot;Root of the verb '{token.Value}' is '{token.Lemma}'.&quot;);\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(&quot;Complete; press any key.&quot;);\nConsole.ReadKey();\n</code></pre>\n<p><strong>Note:</strong> For this specific sentence, I have added an adverb (&quot;quickly&quot;) before the verb (&quot;runs&quot;). Without this, the library incorrectly interprets &quot;runs&quot; as a noun. Depending on your source text, this might be an issue for you, but I believe it is separate from the question being asked.</p>\n",
         "1.0",
         "PartOfSpeech.VERB\n---\nconst string text = \"The disastrous cat runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}",
         "//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = \"The disastrous cat quickly runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($\"Root of the verb '{token.Value}' is '{token.Lemma}'.\");\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(\"Complete; press any key.\");\nConsole.ReadKey();",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst",
         "Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated",
         "The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated",
         "finding root form verbs using curiosityai/catalyst im trying find root form verb run text pipeline identify tokens match dont know continue far help greatly appreciated",
         "find root form verb use curiosityai / catalyst I m try find root form verb run text pipeline identify tokens match do not know continue far help greatly appreciate",
         "root form verb curiosityai catalyst I root form verb run pipeline identify tokens match do not continue far help greatly appreciate"
        ],
        [
         "24",
         "79145419",
         "Is it possible to get embeddings from NV-Embed using Candle?",
         "<p>What I want to do is a CLI program that outputs embeddings of an arbitrary input.\nTo do that, I want to do an inference with an embeddings model, and I chose <code>NV-Embed-v2</code>. My framework of choice is <a href=\"https://github.com/huggingface/candle\" rel=\"nofollow noreferrer\">Candle</a>, but I also looked at <a href=\"https://github.com/EricLBuehler/mistral.rs\" rel=\"nofollow noreferrer\">Mistral-RS</a>.</p>\n<p>Basically, what I'm trying to do is this code fragment:\n<a href=\"https://huggingface.co/nvidia/NV-Embed-v2\" rel=\"nofollow noreferrer\">https://huggingface.co/nvidia/NV-Embed-v2</a>\nbut with Rust and Candle.</p>\n<p>What I tried is to start off with <a href=\"https://github.com/huggingface/candle/blob/main/candle-examples/examples/mistral/main.rs\" rel=\"nofollow noreferrer\">Mistral Candle's example</a> because the NV-Embed's HF page says: <code>Model Details / Base Decoder-only LLM: Mistral-7B-v0.1</code>.</p>\n<p>I replaced the model id in the original code with <code>nvidia/NV-Embed-v2</code>, and was able to download the weights from Hugging Face, but upon loading the config, I got this:</p>\n<pre><code>Error: missing field `vocab_size` at line 101 column 1\n</code></pre>\n<p>Then I hardcoded the values from the JSON config loaded from HF to a newly created <code>candle_transformers::models::mistral::Config</code> instance. And after that, <code>Mistral::new(&amp;config, vb)</code> fails with:</p>\n<pre><code>Error: cannot find tensor model.embed_tokens.weight\n</code></pre>\n<p>Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?</p>\n",
         "2024-10-31 15:55:49",
         "0",
         "329",
         "1",
         "79156470.0",
         "<p>candle looking for <code>model.embed_tokens.weight</code> whereas the original tensor name is <code>embedding_model.embed_tokens.weight</code>. You just have to change this line of <code>mistral.rs</code> in candle_transformers.</p>\n<pre class=\"lang-rust prettyprint-override\"><code>// from\nlet vb_m = vb.pp(&quot;model&quot;);\n//to\nlet vb_m = vb.pp(&quot;embedding_model&quot;);\n</code></pre>\n",
         "2.0",
         "NV-Embed-v2\n---\nModel Details / Base Decoder-only LLM: Mistral-7B-v0.1\n---\nnvidia/NV-Embed-v2\n---\nError: missing field `vocab_size` at line 101 column 1\n---\ncandle_transformers::models::mistral::Config\n---\nMistral::new(&config, vb)\n---\nError: cannot find tensor model.embed_tokens.weight",
         "model.embed_tokens.weight\n---\nembedding_model.embed_tokens.weight\n---\nmistral.rs\n---\n// from\nlet vb_m = vb.pp(\"model\");\n//to\nlet vb_m = vb.pp(\"embedding_model\");",
         "Is it possible to get embeddings from NVEmbed using Candle",
         "What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed",
         "candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed",
         "possible get embeddings nvembed using candle want cli program outputs embeddings arbitrary input want inference embeddings model chose framework choice candle also looked mistralrs basically im trying code fragment rust candle tried start mistral candles example nvembeds hf page says replaced model id original code able download weights hugging face upon loading config got hardcoded values json config loaded hf newly created instance fails way around maybe candlebased open source works could use inspiration maybe thats common mistake could easily diagnosed",
         "possible get embedding nvembe use candle want cli program output embedding arbitrary input want inference embedding model choose framework choice candle also look mistralrs basically I m try code fragment rust candle try start mistral candle example nvembed hf page say replace model i d original code able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail way around maybe candlebase open source work could use inspiration maybe that s common mistake could easily diagnose",
         "possible get embedding nvembe candle cli program embedding arbitrary input inference embedding choose framework choice candle also mistralrs basically I fragment rust candle start mistral candle nvembed hf page say replace i d original able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail around maybe candlebase open source could inspiration maybe that s common mistake could easily diagnose"
        ],
        [
         "25",
         "79111733",
         "How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)",
         "<p>How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)</p>\n<p>I have short product descriptions that I’d like to transform into structured attributes.</p>\n<p>Example:</p>\n<p>Input:</p>\n<pre><code>“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n</code></pre>\n<p>Output:</p>\n<pre><code>Year = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml\n</code></pre>\n<p>If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:</p>\n<ol>\n<li><p>There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.</p>\n</li>\n<li><p>There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.</p>\n</li>\n<li><p>Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.</p>\n</li>\n<li><p>I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.</p>\n</li>\n</ol>\n<p>I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.</p>\n<p>I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.</p>\n<p>Appreciate any insight into approaches or suggested learning resources.</p>\n<p></p>\n<p>I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.</p>\n",
         "2024-10-21 20:54:56",
         "0",
         "166",
         "1",
         "79113907.0",
         "<p>LLM would work nicely for this.  I'v done similar tasks before and it worked nicely with minimal training.  Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate,  but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions.</p>\n<p>For you task I would use a framework like Langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  When run with a model it will create an XML output which would be trivial to parse.  You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me.</p>\n<pre><code>You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like &quot;Red&quot; or &quot;White&quot; (e.g., &quot;Red Head Vineyard&quot;) which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., &quot;1500 ml&quot;) rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n&lt;Wine&gt;\n&lt;Year&gt;{{Year}}&lt;/Year&gt;\n&lt;Color&gt;{{Color}}&lt;/Color&gt;\n&lt;Weight&gt;{{Weight}}&lt;/Weight&gt;\n&lt;WeightUnit&gt;{{WeightUnit}}&lt;/WeightUnit&gt;\n&lt;Brand&gt;{{Brand}}&lt;/Brand&gt;\n&lt;GrapeVariety&gt;{{GrapeVariety}}&lt;/GrapeVariety&gt;\n&lt;/Wine&gt;\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   &lt;Wine&gt;\n     &lt;Year&gt;2017&lt;/Year&gt;\n     &lt;Color&gt;Red&lt;/Color&gt;\n     &lt;Weight&gt;750&lt;/Weight&gt;\n     &lt;WeightUnit&gt;ml&lt;/WeightUnit&gt;\n     &lt;Brand&gt;La Lecciaia&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Cabernet Sauvignon&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   &lt;Wine&gt;\n     &lt;Year&gt;2020&lt;/Year&gt;\n     &lt;Color&gt;&lt;/Color&gt;\n     &lt;Weight&gt;1.5&lt;/Weight&gt;\n     &lt;WeightUnit&gt;L&lt;/WeightUnit&gt;\n     &lt;Brand&gt;Red Head Vineyard&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Chardonnay&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}\n</code></pre>\n<p>Keep in mind that LLMs are not cheap to run.  But for this tasks given ambiguousness of the domain it is most likely the best choice.  For this particular task it would be 1/1000 of a penny per label using OpenAI service.  You might find a cheaper model / provider.  However when working with LLM it is very important to ensure accuracy first,  then optimize for costs.</p>\n<p>The whole thing will probably take 1-2 hours to build for the intermediate LLM developer.  If you are learning it may vary.  But this is a perfect project to learn about LLMs</p>\n",
         "1.0",
         "“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n---\nYear = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml",
         "You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like \"Red\" or \"White\" (e.g., \"Red Head Vineyard\") which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., \"1500 ml\") rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n<Wine>\n<Year>{{Year}}</Year>\n<Color>{{Color}}</Color>\n<Weight>{{Weight}}</Weight>\n<WeightUnit>{{WeightUnit}}</WeightUnit>\n<Brand>{{Brand}}</Brand>\n<GrapeVariety>{{GrapeVariety}}</GrapeVariety>\n</Wine>\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   <Wine>\n     <Year>2017</Year>\n     <Color>Red</Color>\n     <Weight>750</Weight>\n     <WeightUnit>ml</WeightUnit>\n     <Brand>La Lecciaia</Brand>\n     <GrapeVariety>Cabernet Sauvignon</GrapeVariety>\n   </Wine>\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   <Wine>\n     <Year>2020</Year>\n     <Color></Color>\n     <Weight>1.5</Weight>\n     <WeightUnit>L</WeightUnit>\n     <Brand>Red Head Vineyard</Brand>\n     <GrapeVariety>Chardonnay</GrapeVariety>\n   </Wine>\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}",
         "How to derive attributes/labels from short plain text descriptions NER LLM",
         "How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense",
         "LLM would work nicely for this Iv done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense LLM would work nicely for this Iv done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense",
         "derive attributes/labels short plain text descriptions ner llm derive attributes/labels short plain text descriptions ner llm short product descriptions id like transform structured attributes example input output everything format would trivial write regular expression done many different formats nuances increasingly cumbersome hardcode logic format trying create generic solution immediately run issues basic approach several different data providers format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even given provider may multiple formats may change time formats always strictly followed many ways expressing particular components example weight might expressed one 15l 1 1/2 liters 1500ml etc parts description may confused target components may white wine brand called red head vineyard weight 2000 ml may confused year etc im using wine examples sake simplicity general audience product domain conceptual issues id consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work harder get right theres straightforward method would good know id like develop generalpurpose function accept description format little experience nlp/artificial intelligence suspect useful tools/algos leverage 1000+ example records could potentially use train model something run locally would preferred necessary im looking specific implementation guidance anyone whos worked similar problem open hybrid approaches additional logic manual oversight could account initial inaccuracies appreciate insight approaches suggested learning resources ive looked online information many approaches involve significant amount front work unclear theyll work practical sense",
         "derive attribute / label short plain text description ner llm derive attribute / label short plain text description ner llm short product description i d like transform structure attribute example input output everything format would trivial write regular expression do many different format nuance increasingly cumbersome hardcode logic format try create generic solution immediately run issue basic approach several different datum provider format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even give provider may multiple format may change time format always strictly follow many way express particular component example weight might express one 15l 1 1/2 liter 1500ml etc part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year etc I m use wine example sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work hard get right there s straightforward method would good know I d like develop generalpurpose function accept description format little experience nlp / artificial intelligence suspect useful tool / algo leverage 1000 + example record could potentially use train model something run locally would prefer necessary I m look specific implementation guidance anyone who s work similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource I ve look online information many approach involve significant amount front work unclear they ll work practical sense",
         "derive attribute label short plain description ner llm derive attribute label short plain description ner llm short product description i d like transform structure attribute input everything format would trivial write regular expression do many format nuance increasingly cumbersome hardcode logic format create generic solution immediately run issue basic approach several datum provider format another provider might red 2017 la lecciaia cabernet sauvignon 750 ml even provider may multiple format may change time format always strictly many express particular component weight might express 15l 1 12 liter 1500ml part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year I wine sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough la lecciaia brand cabernet sauvignon grape variety assuming would take front hard get right there s straightforward method would good I d like develop generalpurpose function accept description format little experience nlp artificial intelligence suspect useful tool algo leverage 1000 record could potentially train something run locally would prefer necessary I specific implementation guidance anyone who s similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource I ve online information many approach involve significant amount front unclear they ll practical sense"
        ],
        [
         "26",
         "79102797",
         "Varying embedding dim due to changing padding in batch size",
         "<p>I want to train a simple neural network, which has <strong>embedding_dim</strong> as a parameter:</p>\n<pre><code>class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n</code></pre>\n<p>To load the data I used torchs DataLoader with a custom collate_fn.</p>\n<pre><code>train_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n</code></pre>\n<p>The collate_fn_padd function looks the following:</p>\n<pre><code>def collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels\n\n</code></pre>\n<p><strong>The problem:</strong> For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch).</p>\n<p>That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch.</p>\n<p>Due to that, I receive errors like that: <strong>mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64)</strong></p>\n<p>Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?</p>\n",
         "2024-10-18 15:54:51",
         "0",
         "42",
         "1",
         "79105117.0",
         "<p>You can add a maximum length argument set to <code>embedding_dim</code> to pad and truncate all the data to a fixed length:</p>\n<pre><code>padded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]\n</code></pre>\n",
         "1.0",
         "class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n---\ntrain_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n---\ndef collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels",
         "embedding_dim\n---\npadded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]",
         "Varying embedding dim due to changing padding in batch size",
         "I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size",
         "You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size",
         "varying embedding dim due changing padding batch size want train simple neural network embedding_dim parameter load data used torchs dataloader custom collate_fn collate_fn_padd function looks following problem every batch want train model embedded text gets padded differently long takes longest sequence current batch means embedding dim/input size linear layer neural network changes batch batch althoug want size every batch due receive errors like mat1 mat2 shapes multiplied 16x182 301x64 possible adjust collate_fn_pad function padds sequence size independet batch size",
         "vary embed dim due change padding batch size want train simple neural network embedding_dim parameter load datum use torchs dataloader custom collate_fn collate_fn_padd function look follow problem every batch want train model embed text get pad differently long take long sequence current batch mean embed dim / input size linear layer neural network change batch batch althoug want size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collate_fn_pad function padd sequence size independet batch size",
         "vary embed dim due change padding batch size train simple neural network embeddingdim parameter load datum torchs dataloader custom collatefn collatefnpadd function problem every batch train embed get pad differently long take long sequence current batch mean embed dim input size linear layer neural network change batch batch althoug size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collatefnpad function padd sequence size independet batch size"
        ],
        [
         "27",
         "79100835",
         "How can I adjust the performance of tokenizer?",
         "<p>Working with the tokenizer from the <code>transformers</code> library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not.</p>\n<p>I'm wondering if I can <strong>&quot;adjust&quot;</strong> (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to.</p>\n<p>To be more specific, the type of tokenizer is <code>transformers.XLMRobertaTokenizerFast</code>, which is a unigram tokenizer, and the model is <code>paraphrase-multilingual-mpnet-base-v2</code>.</p>\n",
         "2024-10-18 06:45:15",
         "0",
         "45",
         "1",
         "79107575.0",
         "<p>You can change the tokenizer's vocabulary:</p>\n<pre><code>tokenizer.add_tokens([&quot;asadaf&quot;, &quot;sdfsaf&quot;])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = &quot;This is asadaf and sdfsaf&quot;\nprint(tokenizer(input_text))\n</code></pre>\n<p>As a result, <em>asadaf</em> and <em>sdfsaf</em> would be tokenized as unique words.</p>\n",
         "1.0",
         "transformers\n---\ntransformers.XLMRobertaTokenizerFast\n---\nparaphrase-multilingual-mpnet-base-v2",
         "tokenizer.add_tokens([\"asadaf\", \"sdfsaf\"])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = \"This is asadaf and sdfsaf\"\nprint(tokenizer(input_text))",
         "How can I adjust the performance of tokenizer",
         "Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is",
         "You can change the tokenizers vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is You can change the tokenizers vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is",
         "adjust performance tokenizer working tokenizer library hugging face tokenizer works fine cases cases im wondering adjust train new tokenizer scratch performance tokenizer handle bad cases still maintaining good performance cases used specific type tokenizer unigram tokenizer model",
         "adjust performance tokenizer working tokenizer library hug face tokenizer work fine case case I m wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case use specific type tokenizer unigram tokenizer model",
         "adjust performance tokenizer working tokenizer library hug face tokenizer fine case case I wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case specific type tokenizer unigram tokenizer"
        ],
        [
         "28",
         "79081924",
         "With spaCy, how can I get all lemmas from a string?",
         "<p>I have a pandas data frame with a column of text values (documents).  I want to apply lemmatization on these values with the spaCy library using the pandas <code>apply</code> function.  I've defined my <code>to_lemma</code> function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow.  Is there a way to extract the lemmatized form of a document in spaCy?</p>\n<pre><code>def to_lemma(text):\n    tp = nlp(text)\n    line = &quot;&quot;\n    for word in tp:\n        line = line + word.lemma_ + &quot; &quot;\n    return line\n</code></pre>\n",
         "2024-10-12 21:03:21",
         "-1",
         "97",
         "2",
         "79086290.0",
         "<p>There are many ways to speed up SpaCy processing. The question which of them make sense for you depends mostly on the size of your input.</p>\n<ol>\n<li>The most obvious one is not individually apply the model to every single row, but rather use batch processing. Use <code>nlp.pipe()</code> with an Iterable of strings. This means it is easier to not use apply.</li>\n<li>Disable components that you do not use. For token level processing where you need the lemmas this would be <code>'parser'</code> (the dependency parser) and <code>'ner'</code> (the Named Entity Recognition component).</li>\n<li>Increase the <code>batch_size</code> (objects to buffer) in pipe(). The default is 1000. Obviously this only makes sense to touch if you have the memory to increase it a lot.</li>\n<li>Increase the number of processors used using <code>n_process</code>. This will increase the time it takes to initially load the model but decrease the processing time. In my experience this starts making sense at about 500k+ texts. Note that this also requires the code to be run in an <code>if __name__ == '__main__':</code> wrapper.</li>\n</ol>\n<p>Basic example with 1. and 2.:</p>\n<pre><code>texts = df[&quot;column_name&quot;]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\ndf[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n<p>Advanced example for all four:</p>\n<pre><code>if __name__ == '__main__':\n    texts = df[&quot;column_name&quot;]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\n    df[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n",
         "2.0",
         "apply\n---\nto_lemma\n---\ndef to_lemma(text):\n    tp = nlp(text)\n    line = \"\"\n    for word in tp:\n        line = line + word.lemma_ + \" \"\n    return line",
         "nlp.pipe()\n---\n'parser'\n---\n'ner'\n---\nbatch_size\n---\nn_process\n---\nif __name__ == '__main__':\n---\ntexts = df[\"column_name\"]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\ndf[\"column_name_lemmas\"] = lemmas\n---\nif __name__ == '__main__':\n    texts = df[\"column_name\"]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\n    df[\"column_name_lemmas\"] = lemmas",
         "With spaCy how can I get all lemmas from a string",
         "I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy",
         "There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy",
         "spacy get lemmas string pandas data frame column text values documents want apply lemmatization values spacy library using pandas function ive defined function iterate words document concatenate corresponding lemmas output string however slow way extract lemmatized form document spacy",
         "spacy get lemmas string panda datum frame column text value document want apply lemmatization value spacy library use panda function I ve define function iterate word document concatenate correspond lemmas output string however slow way extract lemmatize form document spacy",
         "spacy get lemmas panda datum frame column value document apply lemmatization value spacy library panda function I ve define function iterate document concatenate correspond lemmas however slow extract lemmatize form document spacy"
        ],
        [
         "29",
         "79057082",
         "Avoiding overlap in frequency and document frequency count in Quanteda",
         "<p>Below is a dummy corpus of 4 documents.</p>\n<p>The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in.</p>\n<p>The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive.</p>\n<p>Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each,\nbut are intended to be counted once, according to the dictionary.</p>\n<p>The expected overall frequency count is (extracted from the 'pattern&quot; column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key.</p>\n<p>Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key.</p>\n<p>I have three questions:</p>\n<ol>\n<li>are there any problems/issues that could affect output accuracy using this approach?</li>\n<li>is there a better/more parsimonius approach to achieve what I am trying to do?</li>\n<li>what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table?</li>\n</ol>\n<pre><code>        library (quanteda)\n        library(quanteda.textstats)\n\n        txt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\n\n\n         x &lt;- x %&gt;%\n         tokens_compound(phrase(&quot;dairy farmers&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farms&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farm&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farming&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy industry&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;indigenous australians&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;australia post&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farmer&quot;), concatenator = &quot; &quot;)\n              x\n\n         dict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign- \n         owned&quot;, &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, &quot;multinational \n         oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n         dairy = c(&quot;dairy farmers&quot;, &quot;dairy farms&quot;,&quot;dairy farm&quot;,&quot;dairy farming&quot;,&quot;dairy \n         industry&quot;, &quot;dairy farmer&quot;,&quot;dairy&quot;, &quot;milk&quot;),\n         auspost = &quot;australia post&quot;,\n         oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n         farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n         foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n         business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n         indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres \n         strait&quot;),\n         peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, &quot;people of \n         this nation&quot;, &quot;people of this country&quot;),\n         industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\n        kwicdict &lt;- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, &quot;D:/Output/TEST.csv&quot;)\n\n       DF &lt;- read.csv(&quot;D://Output/TEST.csv&quot;,header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       &gt; x2 &lt;- DF[,8]\n       &gt; \n       &gt; table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2 \n</code></pre>\n",
         "2024-10-05 12:43:52",
         "1",
         "57",
         "1",
         "79058791.0",
         "<p>I don't think that <code>kwic()</code> is what you want here. <code>tokens_lookup()</code> lets you specify that the nested scope should be mutually exclusive across keys, not just within keys. Observe the difference below. (And note the use of wildcarding for dairy key.)</p>\n<pre class=\"lang-r prettyprint-override\"><code>library(quanteda)\n#&gt; Package version: 4.1.0\n#&gt; Unicode version: 14.0\n#&gt; ICU version: 71.1\n#&gt; Parallel computing: 10 of 10 threads used.\n#&gt; See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\ndict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign-owned&quot;, \n                                     &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, \n                                     &quot;multinational oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n                        dairy = c(&quot;dairy farm*&quot;, &quot;dairy industry&quot;, &quot;dairy&quot;, &quot;milk&quot;),\n                        auspost = &quot;australia post&quot;,\n                        oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n                        farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n                        foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n                        business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n                        indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres strait&quot;),\n                        peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, \n                                 &quot;people of this nation&quot;, &quot;people of this country&quot;),\n                        industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\nx &lt;- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       5       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        1\n#&gt;   doc3        1     1       0  0       1       0        1     1    1        0\n#&gt;   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = &quot;dictionary&quot;) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       3       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        0\n#&gt;   doc3        1     1       0  0       1       0        1     1    0        0\n#&gt;   doc4        0     0       1  1       1       1        0     0    0        0\n</code></pre>\n<p><sup>Created on 2024-10-06 with <a href=\"https://reprex.tidyverse.org\" rel=\"nofollow noreferrer\">reprex v2.1.1</a></sup></p>\n",
         "0.0",
         "library (quanteda)\n        library(quanteda.textstats)\n\n        txt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\n\n\n         x <- x %>%\n         tokens_compound(phrase(\"dairy farmers\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farms\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farm\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farming\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy industry\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"indigenous australians\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"australia post\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farmer\"), concatenator = \" \")\n              x\n\n         dict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign- \n         owned\", \"foreign owned\", \"foreign companies\", \"multinational\", \"multinational \n         oil companies\", \"multinationals\", \"transnational\"),\n         dairy = c(\"dairy farmers\", \"dairy farms\",\"dairy farm\",\"dairy farming\",\"dairy \n         industry\", \"dairy farmer\",\"dairy\", \"milk\"),\n         auspost = \"australia post\",\n         oz = c(\"australia\", \"this country\", \"our country\"),\n         farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n         foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n         business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n         indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres \n         strait\"),\n         peep = c(\"australians\", \"people of australia\", \"australian people\", \"people of \n         this nation\", \"people of this country\"),\n         industry = c(\"industry\", \"industries\")))\n\n        kwicdict <- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, \"D:/Output/TEST.csv\")\n\n       DF <- read.csv(\"D://Output/TEST.csv\",header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       > x2 <- DF[,8]\n       > \n       > table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2",
         "kwic()\n---\ntokens_lookup()\n---\nlibrary(quanteda)\n#> Package version: 4.1.0\n#> Unicode version: 14.0\n#> ICU version: 71.1\n#> Parallel computing: 10 of 10 threads used.\n#> See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\ndict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign-owned\", \n                                     \"foreign owned\", \"foreign companies\", \"multinational\", \n                                     \"multinational oil companies\", \"multinationals\", \"transnational\"),\n                        dairy = c(\"dairy farm*\", \"dairy industry\", \"dairy\", \"milk\"),\n                        auspost = \"australia post\",\n                        oz = c(\"australia\", \"this country\", \"our country\"),\n                        farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n                        foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n                        business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n                        indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres strait\"),\n                        peep = c(\"australians\", \"people of australia\", \"australian people\", \n                                 \"people of this nation\", \"people of this country\"),\n                        industry = c(\"industry\", \"industries\")))\n\nx <- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       5       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        1\n#>   doc3        1     1       0  0       1       0        1     1    1        0\n#>   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = \"dictionary\") |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       3       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        0\n#>   doc3        1     1       0  0       1       0        1     1    0        0\n#>   doc4        0     0       1  1       1       1        0     0    0        0",
         "Avoiding overlap in frequency and document frequency count in Quanteda",
         "Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table",
         "I dont think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table I dont think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table",
         "avoiding overlap frequency document frequency count quanteda dummy corpus 4 documents dictionary developed identify frequency words phrases corpus well number documents word phrases occurs world australians occurs two dictionary keys peep indig key content intended mutually exclusive similarly australia oz australia post foreign foreign multinat farm/farmers dairy farmers occur two dictionary keys intended counted according dictionary expected overall frequency count extracted pattern column kwic table reported x2 note word industry appears allocated industry define din indig key dairy frequency occuring key occuring three documents calculated unique rows kwic table doc names column key three questions problems/issues could affect output accuracy using approach better/more parsimonius approach achieve trying would best way extract equivalent tetxstat frequency count data kwic table",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency word phrase corpus well number document word phrase occur world australian occur two dictionary key peep indig key content intend mutually exclusive similarly australia oz australia post foreign foreign multinat farm / farmer dairy farmer occur two dictionary key intend count accord dictionary expect overall frequency count extract pattern column kwic table report x2 note word industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table doc name column key three question problem / issue could affect output accuracy use approach well / more parsimonius approach achieve trying would well way extract equivalent tetxstat frequency count datum kwic table",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency phrase corpus number document phrase occur world australian occur dictionary key peep indig key content intend mutually exclusive similarly australia oz australia post foreign foreign multinat farm farmer dairy farmer occur dictionary key intend count accord dictionary expect overall frequency count extract pattern column kwic table report x2 note industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table doc name column key three question problem issue could affect accuracy approach more parsimonius approach achieve trying would extract equivalent tetxstat frequency count datum kwic table"
        ],
        [
         "30",
         "79005985",
         "Seq2Seq trainer.train() keeps giving indexing error",
         "<p>I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error:</p>\n<blockquote>\n<p>IndexError: Invalid key: 39463 is out of bounds for size 0.</p>\n</blockquote>\n<ul>\n<li>The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B</li>\n<li>The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also.</li>\n</ul>\n<p>Detailed error message:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;nllbtrain.py&quot;, line 273, in &lt;module&gt;\n    print(trainer.train())\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1645, in train\n    return inner_training_loop(\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 631, in __next__\n    data = self._next_data()\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2810, in __getitem__\n    return self._getitem(key)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 583, in query_table\n    _check_valid_index_key(key, size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 526, in _check_valid_index_key\n    raise IndexError(f&quot;Invalid key: {key} is out of bounds for size {size}&quot;)\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n</code></pre>\n<p>The code of the preprocessing done for the data:</p>\n<pre><code>def preprocess_function(examples):\n        inputs = [example + ' &lt;/s&gt;' + f' &lt;2{s_lang}&gt;' for example in examples[source_lang]]\n        targets = [f'&lt;2{t_lang}&gt; ' + example + ' &lt;/s&gt;' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n</code></pre>\n<p>Data after preprocessing:</p>\n<pre><code>DatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n</code></pre>\n<p>The code of model params and training:</p>\n<pre><code>model_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=&quot;hin_Deva&quot;, tgt_lang=&quot;san_Deva&quot;, max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=&quot;epoch&quot;,\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=&quot;./output_dir&quot;,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())\n\n</code></pre>\n<p>Any idea why this error is persisting?</p>\n",
         "2024-09-20 08:43:32",
         "0",
         "54",
         "1",
         "79007590.0",
         "<p><code>size 0</code> indicates that the dataset your trainer gets when the fine-tuning starts is empty. Looking at this (<a href=\"https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25</a>) and this (<a href=\"https://github.com/huggingface/datasets/issues/6535\" rel=\"nofollow noreferrer\">https://github.com/huggingface/datasets/issues/6535</a>) thread suggests adding <code>remove_unused_columns = False</code> to your <code>training_args</code> might resolve the issue, so you could give that a try.</p>\n",
         "0.0",
         "Traceback (most recent call last):\n  File \"nllbtrain.py\", line 273, in <module>\n    print(trainer.train())\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2810, in __getitem__\n    return self._getitem(key)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\n    _check_valid_index_key(key, size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n---\ndef preprocess_function(examples):\n        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]\n        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n---\nDatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n---\nmodel_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=\"hin_Deva\", tgt_lang=\"san_Deva\", max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=\"epoch\",\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=\"./output_dir\",\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())",
         "size 0\n---\nremove_unused_columns = False\n---\ntraining_args",
         "Seq2Seq trainertrain keeps giving indexing error",
         "I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting",
         "indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting",
         "seq2seq trainertrain keeps giving indexing error trying machine translation hindi sanskrit using nllb model keep getting error indexerror invalid key 39463 bounds size 0 error coming training pretrained nllb model ` facebook/nllb20013b input data ~40k hindi sentences error arises tried training sample data also detailed error message code preprocessing done data data preprocessing code model params training idea error persisting",
         "seq2seq trainertrain keep give indexing error try machine translation hindi sanskrit use nllb model keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb model ` facebook / nllb20013b input datum ~40k hindi sentence error arises try train sample datum also detail error message code preprocessing do data datum preprocesse code model param train idea error persist",
         "seq2seq trainertrain keep indexing error machine translation hindi sanskrit nllb keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb facebook nllb20013b input datum 40k hindi error arises train sample datum also detail error message preprocessing do data datum preprocesse param train idea error persist"
        ],
        [
         "31",
         "78985137",
         "Alternative to device_map = \"auto\" in Huggingface Pretrained",
         "<p>I have a model that I was reading from huggingface using the following code:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n</code></pre>\n<p>Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model.</p>\n<p>Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like <code>device_map=&quot;auto&quot;</code> but after reading the model.</p>\n<p>So simply something like</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\nmodel.device_map = &quot;auto&quot;\n</code></pre>\n",
         "2024-09-14 12:42:03",
         "2",
         "1034",
         "1",
         "79007343.0",
         "<p>I found out that there are actually several methods in <code>accelerate</code> for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map</a></p>\n<p>The second one is used to match your model with the devices:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model</a></p>\n<p>So basically, in your case, you can use the following code:</p>\n<pre><code>from accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)\n</code></pre>\n<p>P.S. This code still needs to be tested on fine-tuning.</p>\n",
         "2.0",
         "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n---\ndevice_map=\"auto\"\n---\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\nmodel.device_map = \"auto\"",
         "accelerate\n---\nfrom accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)",
         "Alternative to device_map = auto in Huggingface Pretrained",
         "I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like",
         "I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like",
         "alternative device_map = auto huggingface pretrained model reading huggingface using following code read model modifications internal layers added layers started training/finetuning get everything model investigations found custom layers arent distributed multi gpus original model need something like reading model simply something like",
         "alternative device_map = auto huggingface pretraine model reading huggingface use follow code read model modification internal layer add layer start training / finetune get everything model investigation find custom layer be not distribute multi gpus original model need something like read model simply something like",
         "alternative devicemap auto huggingface pretraine reading huggingface read modification internal layer add layer start training finetune get everything investigation custom layer be not distribute multi gpus original something like read simply something like"
        ],
        [
         "32",
         "78966943",
         "How are the weights of the Mistral models reinitialized in Huggingface?",
         "<p>From <a href=\"https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic\">How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model?</a> and <a href=\"https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4</a> there's different suggestions to reinitialize the model.</p>\n<p>When I tried this, it seems to work.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;, token=&quot;hf_*****&quot;)\n\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n</code></pre>\n<p>[out]:</p>\n<pre><code>OrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n</code></pre>\n<p>How are the layers re-initialized through the <code>from_config</code> function? Is it using <a href=\"https://cs230.stanford.edu/section/4/\" rel=\"nofollow noreferrer\">Xaiver/He initialization</a> or just random initialization?</p>\n",
         "2024-09-09 19:25:52",
         "3",
         "175",
         "2",
         "78969695.0",
         "<p><a href=\"https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralConfig\" rel=\"nofollow noreferrer\">MistralConfig</a> has a default parameter <code>initializer_range</code> which is set to 0.02 and described as <code>The standard deviation of the truncated_normal_initializer for initializing all weight matrices</code>, so one can assume they use a truncated normal distribution with a standard deviation of 0.02.</p>\n<p>If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 0.02 looks like, it seems like a fit to me:</p>\n<pre><code>import numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/M67S0rKp.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/M67S0rKp.png\" alt=\"model_weights\" /></a></p>\n",
         "2.0",
         "from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", token=\"hf_*****\")\n\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n---\nOrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n---\nfrom_config",
         "initializer_range\n---\nThe standard deviation of the truncated_normal_initializer for initializing all weight matrices\n---\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()",
         "How are the weights of the Mistral models reinitialized in Huggingface",
         "From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization",
         "MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization",
         "weights mistral models reinitialized huggingface one reinitialize weights hugging face llama v2 model official way original model theres different suggestions reinitialize model tried seems work layers reinitialized function using xaiver/he initialization random initialization",
         "weight mistral model reinitialize huggingface one reinitialize weight hug face llama v2 model official way original model there s different suggestion reinitialize model try seem work layer reinitialize function use xaiver / he initialization random initialization",
         "weight mistral reinitialize huggingface reinitialize weight hug face llama v2 official original there s suggestion reinitialize layer reinitialize function xaiver he initialization random initialization"
        ],
        [
         "33",
         "78957322",
         "Break after first PER sequence found with Spacy",
         "<p>I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all &quot;PER&quot; tags, but I want to reduce the overhead and get only the first contiguous sequence of &quot;PER&quot; entities. Here’s the example output I get:</p>\n<pre><code>Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n</code></pre>\n<p>But I want the result to be:</p>\n<pre><code>Detected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n</code></pre>\n<p>Here is the code I am currently using:</p>\n<pre><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}]\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == &quot;PER&quot;:\n            per_entities.append(ent.text)\n        elif ent.label_ == &quot;MISC&quot; and misc_count &lt; 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n<p>What I'm looking for:</p>\n<p>I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of &quot;PER&quot; entities in the text, ignoring any subsequent &quot;PER&quot; entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?</p>\n",
         "2024-09-06 13:14:32",
         "0",
         "39",
         "1",
         "78957722.0",
         "<p>The issues is that <code>doc[start_idx:].ents</code> is <a href=\"https://spacy.io/api/doc#ents\" rel=\"nofollow noreferrer\">only the named entities</a> in that slice of the doc. Thus, you will never process &quot;habló&quot; for the first entry, you will just go straight from &quot;García&quot; to &quot;López&quot;. To actually iterate over the tokens so that you see when the PER sequence ends, you have to leave out the <code>.ents</code> part. Then you just wait until you see the first token with <code>ent_type_</code> PER and start appending, then break after one of your conditions is met. I ended up refactoring your code a little as I debugged this, but here's an edited version of your program that produces the desired outputs:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;,\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}],\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == &quot;PER&quot;:\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == &quot;MISC&quot; and misc_count &lt; 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n",
         "1.0",
         "Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n---\nDetected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\"\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}]\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == \"PER\":\n            per_entities.append(ent.text)\n        elif ent.label_ == \"MISC\" and misc_count < 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "doc[start_idx:].ents\n---\n.ents\n---\nent_type_\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\",\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}],\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == \"PER\":\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == \"MISC\" and misc_count < 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "Break after first PER sequence found with Spacy",
         "I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this",
         "The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but heres an edited version of your program that produces the desired outputs",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but heres an edited version of your program that produces the desired outputs",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this",
         "break first per sequence found spacy trying extract first speakers name list texts using spacy currently function returns per tags want reduce overhead get first contiguous sequence per entities heres example output get want result code currently using im looking want modify find_per_sequence function returns first contiguous sequence per entities text ignoring subsequent per entities encountering different type entity provided function returns multiple names partial names need way ensure first name sequence included achieve",
         "break first per sequence find spacy try extract first speaker name list text use spacy currently function return per tag want reduce overhead get first contiguous sequence per entity here example output get want result code currently use I m look want modify find_per_sequence function return first contiguous sequence per entity text ignore subsequent per entity encounter different type entity provide function return multiple name partial name need way ensure first name sequence include achieve",
         "break first per sequence spacy extract first speaker name spacy currently function return per tag reduce overhead get first contiguous sequence per entity here get currently I modify findpersequence function return first contiguous sequence per entity ignore subsequent per entity encounter type entity provide function return multiple name partial name ensure first name sequence include achieve"
        ],
        [
         "34",
         "78949607",
         "Trainer huggingface - RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned",
         "<p>I recently got the following error:\n<code>RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned</code>\nwhen doing LoRA on a small LLM.</p>\n<p>I saw on a discord someone saying:</p>\n<blockquote>\n<p>The issue likely stems from the fact that you are manually placing\nyour inputs on the GPU (with to(model.device)), but the Trainer\nexpects data to be on the CPU and will handle the transfer to the GPU\ninternally.</p>\n</blockquote>\n<p>I can't find anything of the sort written in the Trainer documentation of huggingface <a href=\"https://huggingface.co/docs/transformers/en/main_classes/trainer\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/en/main_classes/trainer</a>.</p>\n<p>Is it true? If not, how can I get rid of that error?</p>\n<p>MRE:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = &quot;croissantllm/CroissantLLMBase&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=&quot;auto&quot;)\n\ntexts = [\n    &quot;The first sentence for fine-tuning. &lt;/s&gt;&quot;,\n    &quot;The second sentence for fine-tuning. &lt;/s&gt;&quot;\n]\n\ninputs = [tokenizer(text, return_tensors=&quot;pt&quot;).to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {&quot;input_ids&quot;: input_ids, &quot;labels&quot;: labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n</code></pre>\n<p>The issue is fairly easy to reproduce directly on colab (run <code>%pip install --upgrade torch transformers peft</code> in the first cell).</p>\n",
         "2024-09-04 16:09:18",
         "2",
         "1370",
         "1",
         "79112186.0",
         "<p>Since pinning memory is only available on CPU and not GPU, when running on GPU on Colab, you can just disable it by setting <code>dataloader_pin_memory</code> to <code>False</code> for <code>TrainingArguments</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n</code></pre>\n",
         "3.0",
         "RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned\n---\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = \"croissantllm/CroissantLLMBase\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\ntexts = [\n    \"The first sentence for fine-tuning. </s>\",\n    \"The second sentence for fine-tuning. </s>\"\n]\n\ninputs = [tokenizer(text, return_tensors=\"pt\").to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {\"input_ids\": input_ids, \"labels\": labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n---\n%pip install --upgrade torch transformers peft",
         "dataloader_pin_memory\n---\nFalse\n---\nTrainingArguments\n---\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned",
         "I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell",
         "Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensors pinned recently got following error lora small llm saw discord someone saying issue likely stems fact manually placing inputs gpu tomodeldevice trainer expects data cpu handle transfer gpu internally cant find anything sort written trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin recently get follow error lora small llm see discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not find anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin recently get error lora small llm discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell"
        ],
        [
         "35",
         "78943401",
         "Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error \"Attempting to Unscale FP16 Gradients\"",
         "<p>I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?</p>\n<p>Here is a minimal example:</p>\n<pre class=\"lang-none prettyprint-override\"><code>import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = &quot;facebook/opt-1.3b&quot;\ncache_dir = './models'\nos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;7&quot;\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=&quot;nf4&quot;,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n</code></pre>\n<p>At the line <code>scaler.step(optimizer)</code>, an error occurs:</p>\n<pre><code>Exception has occurred: ValueError: Attempting to unscale FP16 gradients.\n\n</code></pre>\n",
         "2024-09-03 08:38:23",
         "1",
         "497",
         "1",
         "78945455.0",
         "<p>You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32.</p>\n<p>You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training.</p>\n<p>What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details <a href=\"https://huggingface.co/docs/peft/en/developer_guides/quantization\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "1.0",
         "import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = \"facebook/opt-1.3b\"\ncache_dir = './models'\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n---\nscaler.step(optimizer)\n---\nException has occurred: ValueError: Attempting to unscale FP16 gradients.",
         "",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients",
         "I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs",
         "You cant finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldnt finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs You cant finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldnt finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs",
         "finetuning pretrained model quantization amp scaler error attempting unscale fp16 gradients trying finetune pretrained model limited vram achieve using quantization automatic mixed precision amp however encountering issue cant seem resolve could please help identify problem minimal example line error occurs",
         "finetune pretraine model quantization amp scaler error attempt unscale fp16 gradient try finetune pretraine model limited vram achieve use quantization automatic mixed precision amp however encounter issue can not seem resolve could please help identify problem minimal example line error occur",
         "finetune pretraine quantization amp scaler error attempt unscale fp16 gradient finetune pretraine limited vram achieve quantization automatic mixed precision amp however encounter issue can not resolve could please help identify problem minimal line error occur"
        ],
        [
         "36",
         "78933232",
         "Keep training pytorch model on new data",
         "<p>I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps:</p>\n<ol>\n<li>Load and process the text.</li>\n<li>Use a TF-IDF Vectorizer.</li>\n<li>Build the neural network and save the TF-IDF Vectorizer and model to predict new data.</li>\n</ol>\n<p>However, every day I need to classify new comments and correct any wrong classifications.</p>\n<p>Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct).</p>\n<p>Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen.</p>\n<p>So the mains questions are:</p>\n<ol>\n<li>How could i check if the propossed way to solve this problem work as i expect?</li>\n<li>What can i do with the vectorizer when it face new tokens, can i just do a <code>.fit_transform()</code> or i would loose the original vectorizer?</li>\n</ol>\n<p>Here its the full training process:</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        &quot;set1.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n&quot;&quot;&quot;\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n&quot;&quot;&quot;\n\nlabel_mapping = {\n    &quot;anger&quot;: 0,\n    &quot;fear&quot;: 1,\n    &quot;joy&quot;: 2,\n    &quot;sadness&quot;: 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[&quot;label&quot;])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[&quot;label&quot;]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = &quot;modelbest.pth&quot;\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\ntorch.save(model.state_dict(), model_path)\n\n</code></pre>\n<p>Proposed code:</p>\n<pre><code>import torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\nsentiments = [&quot;fear&quot;,&quot;joy&quot;,&quot;sadness&quot;,&quot;anger&quot;]\n\nnew_data = (\n    pl\n    .read_csv(\n        &quot;set2.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n    .filter(pl.col(&quot;label&quot;).is_in(sentiments))\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = &quot;modelbest.pth&quot;\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = &quot;new_moedl.pth&quot;\ntorch.save(model.state_dict(), new_best_model_path)\n</code></pre>\n<p>The dataset can be found <a href=\"https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "2024-08-30 17:47:59",
         "2",
         "271",
         "2",
         "78934212.0",
         "<p>use  pre-trained word embeddings like BertForSequenceClassification.  These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words.</p>\n<p><strong>Model Training with BERT</strong></p>\n<pre><code>import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(&quot;set1.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\ntrain_df = train_df.with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[&quot;label&quot;])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[&quot;label&quot;])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy: {results['eval_accuracy']:.4f}&quot;)\n\n# Save the model and tokenizer\nmodel.save_pretrained(&quot;saved_model&quot;)\ntokenizer.save_pretrained(&quot;saved_tokenizer&quot;)\n</code></pre>\n<p><strong>Incremental training with least effort</strong></p>\n<pre><code># Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(&quot;saved_model&quot;)\ntokenizer = BertTokenizer.from_pretrained(&quot;saved_tokenizer&quot;)\n\n# Load new data\nnew_data = (\n    pl.read_csv(&quot;set2.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n    .filter(pl.col(&quot;label&quot;).is_in([&quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;anger&quot;]))\n    .with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}&quot;)\n\n# Save the updated model\nmodel.save_pretrained(&quot;saved_model_incremental&quot;)\n</code></pre>\n",
         "3.0",
         ".fit_transform()\n---\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        \"set1.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n\"\"\"\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n\"\"\"\n\nlabel_mapping = {\n    \"anger\": 0,\n    \"fear\": 1,\n    \"joy\": 2,\n    \"sadness\": 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[\"label\"])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[\"label\"]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = \"modelbest.pth\"\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = \"text_classification_model.pth\"\ntorch.save(model.state_dict(), model_path)\n---\nimport torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = \"text_classification_model.pth\"\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\nsentiments = [\"fear\",\"joy\",\"sadness\",\"anger\"]\n\nnew_data = (\n    pl\n    .read_csv(\n        \"set2.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n    .filter(pl.col(\"label\").is_in(sentiments))\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = \"modelbest.pth\"\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = \"new_moedl.pth\"\ntorch.save(model.state_dict(), new_best_model_path)",
         "import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(\"set1.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\ntrain_df = train_df.with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[\"label\"])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[\"label\"])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")\n\n# Save the model and tokenizer\nmodel.save_pretrained(\"saved_model\")\ntokenizer.save_pretrained(\"saved_tokenizer\")\n---\n# Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"saved_model\")\ntokenizer = BertTokenizer.from_pretrained(\"saved_tokenizer\")\n\n# Load new data\nnew_data = (\n    pl.read_csv(\"set2.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n    .filter(pl.col(\"label\").is_in([\"fear\", \"joy\", \"sadness\", \"anger\"]))\n    .with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}\")\n\n# Save the updated model\nmodel.save_pretrained(\"saved_model_incremental\")",
         "Keep training pytorch model on new data",
         "Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here",
         "use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "Keep training pytorch model on new data Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "Keep training pytorch model on new data Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here",
         "keep training pytorch model new data im working text classification task decided use pytorch model purpose process mainly involves following steps load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new data however every day need classify new comments correct wrong classifications currently approach add new comments correct classification dataset retrain entire model process timeconsuming new comments lost validation would like create new dataset newly classified texts continue training new data new comments classified manually label correct using gpt online code write desired process however im sure working expected im making silly mistakes happen mains questions could check propossed way solve problem work expect vectorizer face new tokens would loose original vectorizer full training process proposed code dataset found",
         "keep train pytorch model new datum I m work text classification task decide use pytorch model purpose process mainly involve follow step load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new datum however every day need classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire model process timeconsume new comment lose validation would like create new dataset newly classify text continue train new datum new comment classify manually label correct use gpt online code write desire process however I m sure working expect I m make silly mistake happen main question could check proposse way solve problem work expect vectorizer face new token would loose original vectorizer full training process propose code dataset find",
         "keep train pytorch new datum I classification task decide pytorch purpose process mainly involve step load process tfidf vectorizer build neural network save tfidf vectorizer predict new datum however every day classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire process timeconsume new comment lose validation would like create new dataset newly classify continue train new datum new comment classify manually label correct gpt online write desire process however I sure working expect I make silly mistake happen main question could check proposse solve problem expect vectorizer face new token would loose original vectorizer full training process propose dataset"
        ],
        [
         "37",
         "78932356",
         "Capitalized words in sentiment analysis",
         "<p>I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive .\nA common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .</p>\n<p>this is my current code :</p>\n<pre><code>from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)\n</code></pre>\n",
         "2024-08-30 13:49:56",
         "-1",
         "128",
         "1",
         "78933236.0",
         "<p>If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data.</p>\n<p>If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis.</p>\n<p>If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out <a href=\"https://github.com/EliasK93/transformer-model-comparison-for-review-sentiment-regression\" rel=\"nofollow noreferrer\">this repository</a> as an example.</p>\n",
         "0.0",
         "from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)",
         "",
         "Capitalized words in sentiment analysis",
         "Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code",
         "If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "Capitalized words in sentiment analysis Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "Capitalized words in sentiment analysis Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code",
         "capitalized words sentiment analysis im currently working data customers reviews products sephora task classify sentiments negative neutral positive common technique text preprocessing lower case words situation upper case words like amazing hide significant emotion behind turning word lower case cause information loss would happy opinion subject still lower case words personally think creating classes distinction sentiments good good positive include importance upper case words current code",
         "capitalize word sentiment analysis I m currently work datum customer review product sephora task classify sentiment negative neutral positive common technique text preprocesse low case word situation upper case word like amazing hide significant emotion behind turn word low case cause information loss would happy opinion subject still low case word personally think create class distinction sentiment good good positive include importance upper case word current code",
         "capitalize sentiment analysis I currently datum customer review product sephora task classify sentiment negative neutral positive common technique preprocesse low case situation upper case like amazing hide significant emotion behind turn low case cause information loss would happy opinion subject still low case personally think create class distinction sentiment good good positive include importance upper case current"
        ],
        [
         "38",
         "78920095",
         "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub'",
         "<p>I've been using LLAMA 2 for research for a few months now and I import as follows:</p>\n<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(&quot;cuda&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;,torch_dtype=&quot;auto&quot;)\nmodel = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;, torch_dtype=&quot;auto&quot;, load_in_4bit=True)\n</code></pre>\n<p>It has always worked. However, today it is showing the following error:\n<strong>RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/<strong>init</strong>.py)</strong></p>\n<p>Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.</p>\n",
         "2024-08-27 17:20:42",
         "1",
         "5871",
         "1",
         "78920098.0",
         "<p>The error you're encountering is due to the <code>split_torch_state_dict_into_shards</code> function not being available in <code>huggingface-hub version &lt; 0.23.0</code>.</p>\n<p>This function is included starting from version <code>0.23.0</code>.</p>\n<p><strong>To resolve this issue, update the <code>huggingface-hub</code> library to version 0.23.0 or later</strong></p>\n<p>Also please install accelerate:</p>\n<pre><code>pip install accelerate==0.31.0\n</code></pre>\n<p>here is a git link: <strong><a href=\"https://github.com/run-llama/llama_index/discussions/14605\" rel=\"nofollow noreferrer\">https://github.com/run-llama/llama_index/discussions/14605</a></strong></p>\n",
         "4.0",
         "from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\",torch_dtype=\"auto\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\", torch_dtype=\"auto\", load_in_4bit=True)",
         "split_torch_state_dict_into_shards\n---\nhuggingface-hub version < 0.23.0\n---\n0.23.0\n---\nhuggingface-hub\n---\npip install accelerate==0.31.0",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub",
         "Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook",
         "The error youre encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook The error youre encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook",
         "import name split_torch_state_dict_into_shards huggingface_hub ive using llama 2 research months import follows always worked however today showing following error runtimeerror failed import transformersmodelsllamamodeling_llama following error look see traceback failed import transformersgenerationutils following error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py recreated hugging face token didnt work using google colab kaggle notebook",
         "import name split_torch_state_dict_into_shard huggingface_hub I ve use llama 2 research month import follows always work however today show follow error runtimeerror fail import transformersmodelsllamamodeling_llama follow error look see traceback fail import transformersgenerationutil follow error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt / conda / lib / python310 / sitepackage / huggingface_hub/ init py recreate hug face token do not work use google colab kaggle notebook",
         "import name splittorchstatedictintoshard huggingfacehub I ve llama 2 research month import follows always however today show error runtimeerror fail import transformersmodelsllamamodelingllama error traceback fail import transformersgenerationutil error traceback import name splittorchstatedictintoshards huggingfacehub opt conda lib python310 sitepackage huggingfacehub init py recreate hug face token do not google colab kaggle notebook"
        ],
        [
         "39",
         "78917743",
         "How to Process Data on GPU Instead of RAM for This Python Code?",
         "<p>I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance.\nmy code :</p>\n<pre><code>def prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n    batch[&quot;input_features&quot;] = feature_extractor(\n        audio[&quot;array&quot;], \n        sampling_rate=audio[&quot;sampling_rate&quot;]\n    ).input_features[0]\n    batch[&quot;labels&quot;] = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[&quot;train&quot;], \n    num_proc=1\n)\n</code></pre>\n<p>How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!</p>\n",
         "2024-08-27 08:03:28",
         "1",
         "61",
         "1",
         "78918851.0",
         "<p>you can using the following code to process audio data on GPU</p>\n<pre><code>import torch\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n\n    input_features = feature_extractor(audio[&quot;array&quot;], sampling_rate=audio[&quot;sampling_rate&quot;]).input_features[0]\n    batch[&quot;input_features&quot;] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    batch[&quot;labels&quot;] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[&quot;train&quot;])\n</code></pre>\n",
         "2.0",
         "def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n    batch[\"input_features\"] = feature_extractor(\n        audio[\"array\"], \n        sampling_rate=audio[\"sampling_rate\"]\n    ).input_features[0]\n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[\"train\"], \n    num_proc=1\n)",
         "import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    input_features = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n    batch[\"input_features\"] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[\"sentence\"]).input_ids\n    batch[\"labels\"] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"])",
         "How to Process Data on GPU Instead of RAM for This Python Code",
         "Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated",
         "you can using the following code to process audio data on GPU",
         "How to Process Data on GPU Instead of RAM for This Python Code Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated you can using the following code to process audio data on GPU",
         "How to Process Data on GPU Instead of RAM for This Python Code Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated",
         "process data gpu instead ram python code im currently using following code process audio data runs ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific changes much appreciated",
         "process datum gpu instead ram python code I m currently use follow code process audio data run ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific change much appreciate",
         "process datum gpu instead ram python I currently process audio data run ram offload processing gpu improve performance modify utilize gpu processing instead ram guidance specific change much appreciate"
        ],
        [
         "40",
         "78912171",
         "How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation",
         "<p>I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this.</p>\n<p><strong>What I’ve Tried:</strong></p>\n<ul>\n<li><p><strong>Initial Attempt:</strong> I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me.</p>\n</li>\n<li><p><strong>Using Forward Hooks:</strong> To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation:</p>\n</li>\n</ul>\n<pre class=\"lang-py prettyprint-override\"><code># VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = &quot;Helsinki-NLP/opus-mt-en-de&quot;\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = &quot;Please translate this to German.&quot;\ninputs = tokenizer(input_text, return_tensors=&quot;pt&quot;)\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(&quot;layers, heads, output tokens, input tokens&quot;)\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(&quot;Input Tokens&quot;)\nplt.ylabel(&quot;Output Tokens&quot;)\nplt.title(&quot;Cross-Attention Matrix&quot;)\nplt.show()\n</code></pre>\n<p>This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation.</p>\n<p><strong>My Question</strong></p>\n<p>Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8?</p>\n<p>I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.</p>\n",
         "2024-08-25 20:13:54",
         "1",
         "380",
         "1",
         "78915504.0",
         "<p>Huggingface has built in methods to return attention weights</p>\n<pre class=\"lang-py prettyprint-override\"><code>translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n&gt; odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n</code></pre>\n<p>With <code>return_dict_in_generate=True</code>, <code>model.generate</code> returns a dict-like object. With <code>output_attentions=True</code>, the output dict will contain all attention weights.</p>\n<p>For this model, it will include encoder attentions, decoder attentions and cross attentions.</p>\n",
         "3.0",
         "# VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = \"Helsinki-NLP/opus-mt-en-de\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = \"Please translate this to German.\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(\"layers, heads, output tokens, input tokens\")\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(\"Input Tokens\")\nplt.ylabel(\"Output Tokens\")\nplt.title(\"Cross-Attention Matrix\")\nplt.show()",
         "translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n> odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n---\nreturn_dict_in_generate=True\n---\nmodel.generate\n---\noutput_attentions=True",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation",
         "I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this",
         "Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this",
         "visualize crossattention matrices marianmtmodel output generation working machine translation task using marianmtmodel hugging face transformers library specifically want visualize crossattention matrices models translation process however encountered difficulties achieving ive tried initial attempt noticed crossattention matrices directly returned model generates translation example found involved feeding source text translation model however goal access crossattention matrices model generates output translation given using forward hooks achieve implemented forward hooks key query projections attention mechanism disabling keyvalue caching use_cache=false capture full matrices last step heres implementation approach seemed work capturing crossattention matrices however observed matrices 4 attention heads instead expected 8 makes question correctness implementation question given issues ive encountered reliable method extract visualize crossattention matrices translation process additionally current approach fundamentally okay resolve issue capturing 4 attention heads instead 8 suspect issue might related im currently reshaping key k query q tensors head dimension multiplication wanted ask advice case theres easier effective way",
         "visualize crossattention matrix marianmtmodel output generation work machine translation task use marianmtmodel hug face transformer library specifically want visualize crossattention matrix model translation process however encounter difficulty achieve I ve try initial attempt notice crossattention matrix directly return model generate translation example find involved feeding source text translation model however goal access crossattention matrix model generate output translation give use forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache use_cache = false capture full matrix last step here implementation approach seem work capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question give issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate I m currently reshape key k query q tensor head dimension multiplication want ask advice case there s easy effective way",
         "visualize crossattention matrix marianmtmodel generation machine translation task marianmtmodel hug face transformer library specifically visualize crossattention matrix translation process however encounter difficulty achieve I ve initial attempt notice crossattention matrix directly return generate translation involved feeding source translation however goal access crossattention matrix generate translation forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache usecache false capture full matrix last step here implementation approach capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate I currently reshape key k query q tensor head dimension multiplication ask advice case there s easy effective"
        ],
        [
         "41",
         "78905614",
         "Why doesn't permuting positional encodings in BERT affect the output as expected?",
         "<p>I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another <a href=\"https://stackoverflow.com/questions/78902301/why-doesnt-permuting-positional-encodings-in-gpt-2-affect-the-output-as-expecte/78903454#78903454\">question</a> I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results.</p>\n<p><strong>What I expected to happen:</strong></p>\n<ul>\n<li>No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary.</li>\n<li>Permuting only the input IDs should return distribution B.</li>\n<li>Permuting only the positional embeddings should return distribution B.</li>\n<li>Permuting both the input IDs and positional embeddings should return distribution A.</li>\n</ul>\n<p><strong>What actually happens:</strong>\nSometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.</p>\n<p><strong>My question is:</strong> Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding?</p>\n<p>For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in <code>masked_prediction</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(&quot;transformers.modeling_utils&quot;).setLevel(logging.ERROR)\nwarnings.simplefilter(&quot;ignore&quot;, FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) &gt; 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(&quot;You can't use an empty sequence for prediction&quot;)\n            return\n        \n        model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(&quot;You need to include a [MASK] token for prediction&quot;)\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=&quot;Run Prediction&quot;)\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)\n</code></pre>\n",
         "2024-08-23 11:12:08",
         "1",
         "74",
         "1",
         "78906902.0",
         "<p>The model inputs have token ids and position ids. There are four scenarios to consider:</p>\n<ol>\n<li>Baseline. Correct order for tokens and positions</li>\n<li>Permute position ids only</li>\n<li>Permute token ids only</li>\n<li>Permute position ids and token ids</li>\n</ol>\n<p>You are correct that scenario 1 and 4 should produce the same results. However you are incorrect in assuming that permuting tokens or positions separately should give the same result. Consider:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n</code></pre>\n<p>In <code>Ex1</code>, the model is told that token <code>2</code> occurs at position <code>0</code>. In <code>Ex2</code>, the model is told that token <code>2</code> occurs at position <code>1</code>. Even though we used the same permutation, the mapping of tokens to positions is different. This results in different model outputs.</p>\n<p>The reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. This is luck - the average case produces different results.</p>\n<p>It is simple to test this. Huggingface models take a <code>position_ids</code> input parameter. We can use this to test permutations of the input ids without messing with the weight matrices.</p>\n<p>To test this, we'll create input data, permute as needed, compute logits and compare logits.</p>\n<p>When comparing logits, we will permute or depermute as needed to compare on a token to token basis. For example if token <code>i</code> in scenario 1 is permuted to token <code>j</code> in scenario 3, we want to compare logits <code>i</code> from scenario 1 to logits <code>j</code> in scenario 3.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f&quot;Comparison {sa}, {sb}: {val.item():.6f}&quot;)\n</code></pre>\n<p>The code should produce an output like:</p>\n<pre><code>Comparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n</code></pre>\n<p>Run the code a bunch of times. You will find that the <code>S1, S4</code> comparison <em>always</em> has a small deviation. This is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues.</p>\n<p>You will find the <code>S2, S3</code> comparison generally has a large deviation, but <em>sometimes</em> has a small deviation. As discussed, this is due to getting a lucky permutation where positions and ids mostly line up.</p>\n",
         "2.0",
         "masked_prediction\n---\nimport torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) > 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(\"You can't use an empty sequence for prediction\")\n            return\n        \n        model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(\"You need to include a [MASK] token for prediction\")\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=\"Run Prediction\")\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)",
         "# Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n---\nEx1\n---\n2\n---\n0\n---\nEx2\n---\n2\n---\n1\n---\nposition_ids\n---\ni\n---\nj\n---\ni\n---\nj\n---\nimport torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f\"Comparison {sa}, {sb}: {val.item():.6f}\")\n---\nComparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n---\nS1, S4\n---\nS2, S3",
         "Why doesnt permuting positional encodings in BERT affect the output as expected",
         "I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in",
         "The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this well create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "Why doesnt permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this well create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "Why doesnt permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in",
         "doesnt permuting positional encodings bert affect output expected working jupyter notebook transformers section positional encodings want demonstrate transformer relies entirely positional encoding understand order sequence previously learned another question posted concept applies models dont use masked attention like gpt2 however attempted approach bert model uses crossattention predict mask token encountered unexpected results expected happen permutation cause model predict different token ie distribution consistent vocabulary permuting input ids return distribution b permuting positional embeddings return distribution b permuting input ids positional embeddings return distribution actually happens sometimes results align expectations times permuting one aspect either input ids positional embeddings leads different outcomes even though occasionally produce result question something else hugging faces bert model might influenced position beyond positional encoding completeness included full code part notebook tried directly important part happens",
         "do not permute positional encoding bert affect output expect work jupyter notebook transformer section positional encoding want demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply model do not use mask attention like gpt2 however attempt approach bert model use crossattention predict mask token encounter unexpected result expect happen permutation cause model predict different token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes result align expectation time permute one aspect either input ids positional embedding lead different outcome even though occasionally produce result question something else hug face bert model might influence position beyond positional encoding completeness include full code part notebook try directly important part happen",
         "do not permute positional encoding bert affect expect jupyter notebook transformer section positional encoding demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply do not mask attention like gpt2 however attempt approach bert crossattention predict mask token encounter unexpected expect happen permutation cause predict token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes align expectation time permute aspect either input ids positional embedding lead outcome even though occasionally produce question something else hug face bert might influence position beyond positional encoding completeness include full part notebook directly important part happen"
        ],
        [
         "42",
         "78901998",
         "How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus?",
         "<p>I'm working with the <code>OpenAIEmbeddings()</code> class from <code>OpenAI</code>, which uses the <code>text-embedding-3-small</code> model. According to the <a href=\"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\" rel=\"nofollow noreferrer\">documentation</a>, it generates a 1536-dimensional vector for any input text.</p>\n<p>However, I'm a bit confused about how this works:</p>\n<ul>\n<li>Is the 1536-dimensional vector generated for the entire input text?</li>\n<li>If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</li>\n</ul>\n<p><strong>I was expecting this:</strong></p>\n<p>If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536.</p>\n<p>But the output is a single vector of size 1536 for the whole input text.</p>\n<p>Why I expected this?</p>\n<p>Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings?</p>\n<p>I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input.</p>\n<p>Any insights or examples would be greatly appreciated!</p>\n",
         "2024-08-22 14:09:25",
         "2",
         "580",
         "1",
         "78902136.0",
         "<p>Everything you described is 100% expected.</p>\n<h3>Q: Is the 1536-dimensional vector generated for the entire input text?</h3>\n<p>A: Yes.</p>\n<h3>Q: If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</h3>\n<p>A: First, the OpenAI Embeddings model doesn't handle a single word any different than a long text. For the model, it's an input. The input can be even a single character (e.g., &quot;a&quot;), but it doesn't make sense to calculate an embedding vector out of it since &quot;a&quot; doesn't semantically mean anything to us humans.</p>\n<p>Second, what you probably meant with this question is what happens when you do a similarity search with these embeddings. In other words, what happens when you <em>use</em> them? What happens if you use embeddings of words, sentences, paragraphs, or the whole text? Does it matter? Yes!</p>\n<p>This is called chunking. The decision about how to chunk your text depends on the use case. The best thing is probably to simply try and see. If you get meaningful results after doing a similarity search, then this means that chunking is appropriate (even if this means chunking the whole text). If you don't get meaningful results after doing a similarity search, then this means that chunking isn't appropriate (e.g., instead of chunking by paragraph, try chunking by sentences).</p>\n<p>There's an excellent Stack Overflow <a href=\"https://stackoverflow.blog/2024/06/06/breaking-up-is-hard-to-do-chunking-in-rag-applications/\">blog post</a> about this topic you should read (pay attention to the bolded text because this is the best explanation):</p>\n<blockquote>\n<p>With RAG, you create text embeddings of the pieces of data that you\nwant to draw from and retrieve. That allows you to place a piece of\nthe source text within the semantic space that LLMs use to create\nresponses.</p>\n<p>/.../</p>\n<p>When it comes to RAG systems, you’ll need to pay special attention to\nhow big the individual pieces of data are. How you divide your data up\nis called chunking, and it’s more complex than embedding whole\ndocuments.</p>\n<p>/.../</p>\n<p>The size of the chunked data is going to make a huge difference in\nwhat information comes up in a search. When you embed a piece of data,\nthe whole thing is converted into a vector. <strong>Include too much in a\nchunk and the vector loses the ability to be specific to anything it\ndiscusses. Include too little and you lose the context of the data.</strong></p>\n</blockquote>\n",
         "3.0",
         "OpenAIEmbeddings()\n---\nOpenAI\n---\ntext-embedding-3-small",
         "",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus",
         "Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated",
         "Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesnt handle a single word any different than a long text For the model its an input The input can be even a single character eg a but it doesnt make sense to calculate an embedding vector out of it since a doesnt semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you dont get meaningful results after doing a similarity search then this means that chunking isnt appropriate eg instead of chunking by paragraph try chunking by sentences Theres an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesnt handle a single word any different than a long text For the model its an input The input can be even a single character eg a but it doesnt make sense to calculate an embedding vector out of it since a doesnt semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you dont get meaningful results after doing a similarity search then this means that chunking isnt appropriate eg instead of chunking by paragraph try chunking by sentences Theres an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated",
         "openaiembeddings work creating single vector size 1536 whole text corpus im working class uses model according documentation generates 1536dimensional vector input text however im bit confused works 1536dimensional vector generated entire input text 1536dimensional vector represents entire input text model handle individual words versus longer texts like sentences paragraphs expecting 100 words input text expected openaiembeddings would output 100 vectors size 1536 output single vector size 1536 whole input text expected learning ive understood embeddings like word2vec glove provide vectors word corpus differ approach taken openaiembeddings im trying understand whether theres way extract embeddings individual words using model output always single vector representing whole input insights examples would greatly appreciated",
         "openaiembedding work create single vector size 1536 whole text corpus I m work class use model accord documentation generate 1536dimensional vector input text however I m bit confused work 1536dimensional vector generate entire input text 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph expect 100 word input text expect openaiembedding would output 100 vector size 1536 output single vector size 1536 whole input text expect learning I ve understand embedding like word2vec glove provide vector word corpus differ approach take openaiembedding I m try understand whether there s way extract embedding individual word use model output always single vector represent whole input insight example would greatly appreciate",
         "openaiembedding create single vector size 1536 whole corpus I class accord documentation generate 1536dimensional vector input however I bit confused 1536dimensional vector generate entire input 1536dimensional vector represent entire input handle individual versus long like paragraph expect 100 input expect openaiembedding would 100 vector size 1536 single vector size 1536 whole input expect learning I ve understand embedding like word2vec glove provide vector corpus differ approach take openaiembedding I understand whether there s extract embedding individual always single vector represent whole input insight would greatly appreciate"
        ],
        [
         "43",
         "78895710",
         "NER versus LLM to extract name, gender, role and company from text",
         "<p>I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons.</p>\n<p>I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them.</p>\n<p>Is there another, smaller LLM that might be good at this while using fewer processing resources?</p>\n<p>Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.)</p>\n<p>Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass?</p>\n<p>Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction.</p>\n<p>Thanks in advance!</p>\n",
         "2024-08-21 07:39:13",
         "1",
         "1524",
         "2",
         "78896098.0",
         "<p>Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. <code>NER</code> is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use <code>NLTK</code> or <code>SpaCy</code> for this matter. My personal choice is <code>SpaCy</code>, however a <code>gender</code> as you defined is not a known named entity. you can see a list of named entities in <a href=\"https://github.com/explosion/spaCy/discussions/9147\" rel=\"nofollow noreferrer\">this doc</a>.</p>\n<p>I guess what you mean by <code>gender</code> is the possible <code>gender</code> associated with the names of a <code>PERSON</code> mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use <a href=\"https://pypi.org/project/gender-guesser/\" rel=\"nofollow noreferrer\"><code>gender-guesser</code> package</a>.</p>\n<p>A possible solution would be like this:</p>\n<pre><code>import spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == &quot;PERSON&quot;:\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n</code></pre>\n<p>Note that <code>en_core_web_sm</code> is the small model available via spaCy, you can use the large model by specifying <code>en_core_web_lg</code>, just make sure that the model is downloaded before running your code. here's how you can download the model:</p>\n<pre><code>python -m spacy download en_core_web_sm\n</code></pre>\n",
         "1.0",
         "",
         "NER\n---\nNLTK\n---\nSpaCy\n---\nSpaCy\n---\ngender\n---\ngender\n---\ngender\n---\nPERSON\n---\ngender-guesser\n---\nimport spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == \"PERSON\":\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n---\nen_core_web_sm\n---\nen_core_web_lg\n---\npython -m spacy download en_core_web_sm",
         "NER versus LLM to extract name gender role and company from text",
         "I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance",
         "Apart from your limitations I wouldnt recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code heres how you can download the model",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance Apart from your limitations I wouldnt recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code heres how you can download the model",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance",
         "ner versus llm extract name gender role company text need extract name gender job title employer/company name newspaper articles running process local hardware cloud allowed due copyright reasons ive playing around llama 31 im finding dont get useable results models smaller 70b parameters size models run much slowly best hardware throw another smaller llm might good using fewer processing resources ner use extract data ners ive looked extract name gender dont know extract data gender showstopper alternatively approach take first pass ner pass names llm together original newspaper article extract data get better results faster single llm pass answer training model good model use starting point im much beginning machine learning journey would love pointed right direction thanks advance",
         "ner versus llm extract name gender role company text need extract name gender job title employer / company name newspaper article running process local hardware cloud allow due copyright reason I ve play around llama 31 I m find do not get useable result model small 70b parameter size model run much slowly good hardware throw another small llm might good use few processing resource ner use extract data ner I ve look extract name gender do not know extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get well result fast single llm pass answer training model good model use start point I m much begin machine learn journey would love point right direction thank advance",
         "ner versus llm extract name gender role company extract name gender job title employer company name newspaper article running process local hardware cloud allow due copyright reason I ve play around llama 31 I do not get useable small 70b parameter size run much slowly good hardware throw another small llm might good few processing resource ner extract data ner I ve extract name gender do not extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get fast single llm pass answer training good start point I much begin machine learn journey would love point right direction thank advance"
        ],
        [
         "44",
         "78887743",
         "Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask?",
         "<p>In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%).</p>\n<p>I am curious about the following:</p>\n<p>When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively?\nDoes the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact?</p>\n<p>Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%?</p>\n<p>Thank you for your insights!</p>\n",
         "2024-08-19 11:49:06",
         "1",
         "525",
         "1",
         "78890409.0",
         "<p>Attention is computed on a tensor of shape <code>(batch_size, sequence_length, embedding_dimension)</code>. The compute and memory requirements scale with the size of those dimensions.</p>\n<p>For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size.</p>\n<p>With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length <code>8</code> and 10 sequences of length <code>128</code>. Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of <code>128</code>. If you sort by length before batching, you get one batch with sequence length of <code>8</code> and another with length <code>128</code>. The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128).</p>\n<p>That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to &quot;skip over&quot; padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.</p>\n",
         "2.0",
         "",
         "(batch_size, sequence_length, embedding_dimension)\n---\n8\n---\n128\n---\n128\n---\n8\n---\n128",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask",
         "In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights",
         "Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding youre not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you arent going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesnt work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding youre not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you arent going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesnt work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights",
         "padding batch sequences affect performance effective attention mask transformer models sequences variable lengths typically padded maximum length batch however sequence lengths vary batch may contain substantial amount padding potentially 50 % curious following pytorch computes transformer padding tokens impact calculation speed negatively presence attention mask allow model effectively skip padding tokens resulting minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero values computation effectively reduce approximately 10 % thank insights",
         "padding batch sequence affect performance effective attention mask transformer model sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 % curious follow pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow model effectively skip padding token result minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero value computation effectively reduce approximately 10 % thank insight",
         "padding batch sequence affect performance effective attention mask transformer sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 curious pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow effectively skip padding token minimal performance impact overall effective attention mask sparse attention mask 10 nonzero value computation effectively reduce approximately 10 thank insight"
        ],
        [
         "45",
         "78865486",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text",
         "<p>Using the following Matcher rule:</p>\n<pre><code>{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }\n</code></pre>\n<p>on the text: 'MyLabel: Some Value'</p>\n<p>I get <strong>two</strong> matches: 'MyLabel' and 'MyLabel:'</p>\n<p>For me, that was quite surprising - I was expecting a single match on 'MyLabel:'.\nAdding the new greedy flag didn't make any difference.</p>\n<ul>\n<li>Is this the intended behavior or is it a bug?</li>\n<li>How should I determine that the second match really is just a subset of the first match?</li>\n<li>Will the shorter match always be reported before the longer match?</li>\n</ul>\n<p>SpaCy version 3.7.5</p>\n",
         "2024-08-13 09:37:23",
         "1",
         "34",
         "1",
         "78870921.0",
         "<p>i will say that the behavior you're observing with the SpaCy <code>Matcher</code> is expected, and it is not a bug. When you use the <code>{'TEXT': ':', 'OP': '?'}</code> pattern, the <code>OP: '?'</code> operator means that the colon is optional, so the matcher will generate both the shorter and the longer match, as you've seen.</p>\n<h3>Explanation:</h3>\n<ul>\n<li><strong>Pattern</strong>: <code>{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}</code>.</li>\n<li><strong>Text</strong>: <code>'MyLabel: Some Value'</code>.</li>\n</ul>\n<p>So for this pattern, SpaCy  will try to match:</p>\n<ol>\n<li><code>'MyLabel'</code> alone (because the colon is optional).</li>\n<li><code>'MyLabel:'</code> (because the colon can be included).</li>\n</ol>\n<p>Therefore, you will get two matches: <code>'MyLabel'</code> and <code>'MyLabel:'</code>.</p>\n<h3>Now to  Answer Your Questions:</h3>\n<ol>\n<li><p><strong>Is this the intended behavior or is it a bug?</strong></p>\n<ul>\n<li>This is intended behavior. The <code>OP: '?'</code> operator allows the colon to be optionally matched, leading to multiple matches.</li>\n</ul>\n</li>\n<li><p><strong>How should I determine that the second match really is just a subset of the first match?</strong></p>\n<ul>\n<li>To determine if one match is a subset of another, you can compare the start and end indices of the matches. The longer match will have the same start index but a different end index. Now i wrote a code below even using spacy version 3.7.5, see details below</li>\n</ul>\n</li>\n</ol>\n<pre><code>pip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n</code></pre>\n<p>Now Example in code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\ndoc = nlp(&quot;MyLabel: Some Value&quot;)\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(&quot;R-1&quot;, [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f&quot;Match: {span.text}, Start: {start}, End: {end}&quot;)\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start &gt;= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f&quot;Filtered Match: {span.text}&quot;)\n</code></pre>\n<p>Now, This code will filter out the shorter match and your output will be</p>\n<pre><code>Match: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n\n</code></pre>\n<ol start=\"3\">\n<li><strong>Now Will the shorter match always be reported before the longer match?</strong>\n<ul>\n<li>I don't think the matches are not guaranteed to be reported in a specific order. so to handle this, you can sort the matches by their start and end indices as shown in the code example above.Now, After sorting, you can now filter out matches that are subsets of longer matches.</li>\n</ul>\n</li>\n</ol>\n<h3>Another Alternative Solution:</h3>\n<p>If you want to ensure that only the longest match is returned, you can change the way you define the pattern:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n</code></pre>\n<p>note that the <code>greedy</code> flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings.</p>\n<h3>Now back to the Summary of what i explained:</h3>\n<ul>\n<li>The behavior you're seeing is by design, due to the optional <code>OP: '?'</code> operator.</li>\n<li>in addition, you can filter out the shorter match by comparing start and end indices of the matches.</li>\n<li>furthermore, Sorting the matches by start and end indices allows you to keep only the longest, non-overlapping matches.</li>\n</ul>\n",
         "1.0",
         "{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }",
         "Matcher\n---\n{'TEXT': ':', 'OP': '?'}\n---\nOP: '?'\n---\n{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}\n---\n'MyLabel: Some Value'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\nOP: '?'\n---\npip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"MyLabel: Some Value\")\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(\"R-1\", [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f\"Match: {span.text}, Start: {start}, End: {end}\")\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start >= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f\"Filtered Match: {span.text}\")\n---\nMatch: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n---\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n---\ngreedy\n---\nOP: '?'",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text",
         "Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375",
         "i will say that the behavior youre observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as youve seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I dont think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesnt change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior youre seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375 i will say that the behavior youre observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as youve seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I dont think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesnt change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior youre seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375",
         "spacy matcher optional suffix pattern reports multiple matches text using following matcher rule text mylabel value get two matches mylabel mylabel quite surprising expecting single match mylabel adding new greedy flag didnt make difference intended behavior bug determine second match subset first match shorter match always reported longer match spacy version 375",
         "spacy matcher optional suffix pattern report multiple match text use follow matcher rule text mylabel value get two match mylabel mylabel quite surprising expect single match mylabel add new greedy flag do not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375",
         "spacy matcher optional suffix pattern report multiple match matcher rule mylabel value get match mylabel mylabel quite surprising expect single match mylabel add new greedy flag do not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375"
        ],
        [
         "46",
         "78862691",
         "`mlflow.transformers.log_model()` does not finish",
         "<h3>Problem</h3>\n<p>I want to use <code>mlflow.transformers.log_model()</code> to log a finetuned huggingface model.</p>\n<p><strong>However, when the <code>mlflow.transformers.log_model</code> method is running, it simply does not finish - runs forever - throws no errors.</strong></p>\n<p>I suspect my configuration is not right, the model is too big?\nThe output says <code>Skipping saving pretrained model weights to disk</code> so that should not be the problem.</p>\n<p>Any ideas how to do this properly?</p>\n<h3>Example</h3>\n<p>This is more or less how my setup looks like, you cannot run this, it includes some pseudocode...</p>\n<p>I am on python 3.11.9 with <code>transformers = &quot;^4.41.2&quot;</code> &amp; <code>mlflow = &quot;^2.15.1&quot;</code>.</p>\n<pre><code>import mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = &quot;LeoLM/leo-hessianai-7b-chat-bilingual&quot;\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=&quot;auto&quot;,\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n<p>The last output I get in the console is:</p>\n<pre><code>INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(&quot;Setuptools is replacing distutils.&quot;)\n</code></pre>\n",
         "2024-08-12 16:27:32",
         "0",
         "337",
         "1",
         "78877979.0",
         "<p>Before defining the trainer, the model has be turned into a Peft model object via <code>get_peft_model</code>, then the <code>mlflow.transformers.log_model</code> works:</p>\n<pre><code>from peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n",
         "0.0",
         "mlflow.transformers.log_model()\n---\nmlflow.transformers.log_model\n---\nSkipping saving pretrained model weights to disk\n---\ntransformers = \"^4.41.2\"\n---\nmlflow = \"^2.15.1\"\n---\nimport mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = \"LeoLM/leo-hessianai-7b-chat-bilingual\"\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )\n---\nINFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")",
         "get_peft_model\n---\nmlflow.transformers.log_model\n---\nfrom peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )",
         "`mlflowtransformerslog_model` does not finish",
         "Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is",
         "Before defining the trainer the model has be turned into a Peft model object via then the works",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is Before defining the trainer the model has be turned into a Peft model object via then the works",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is",
         "` mlflowtransformerslog_model ` finish problem want use log finetuned huggingface model however method running simply finish runs forever throws errors suspect configuration right model big output says problem ideas properly example less setup looks like run includes pseudocode python 3119 & last output get console",
         "` mlflowtransformerslog_model ` finish problem want use log finetune huggingface model however method run simply finish run forever throw error suspect configuration right model big output say problem idea properly example less setup look like run include pseudocode python 3119 & last output get console",
         "mlflowtransformerslogmodel finish problem log finetune huggingface however method run simply finish run forever throw error suspect configuration right big say problem idea properly less setup like run include pseudocode python 3119 last get console"
        ],
        [
         "47",
         "78853409",
         "NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation)",
         "<p>I'm attempting to fine-tune the NLLB model <code>&quot;facebook/nllb-200-distilled-600M&quot;</code> for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb.</p>\n<p>Documentation: <a href=\"https://github.com/facebookresearch/fairseq/tree/nllb?tab=readme-ov-file\" rel=\"nofollow noreferrer\">link</a></p>\n<p>This is the code block which is giving error:</p>\n<pre><code>DATA_CONFIG = &quot;/content/sample_data/data_config.json&quot;\nOUTPUT_DIR = &quot;/content/outputs&quot;\nMODEL_FOLDER = &quot;/content/drive/MyDrive/Thesis/nllb-checkpoints&quot;\nDROP = 0.1\nSRC = &quot;eng_Latn&quot;\nTGT = &quot;deu_Latn&quot;\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=&quot;$SRC-$TGT&quot; \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n</code></pre>\n<p>This is the error:</p>\n<pre><code>/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=&quot;conf&quot;, config_name=&quot;base_config&quot;)\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 289, in main\n    train_module = TrainModule(config)\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n</code></pre>\n<p>So far, I understand there is a <code>Missing data_prefix configuration</code>. I created a demo custom data_config.json. Which looks like this:</p>\n<pre><code>{\n    &quot;data_prefix&quot;: &quot;/content/sample_data&quot;,\n    &quot;train_data&quot;: &quot;train_demo.json&quot;,\n    &quot;test_data&quot;: &quot;test_demo.json&quot;,\n    &quot;lang_pairs&quot;: &quot;eng_Latn-deu_Latn&quot;\n}\n</code></pre>\n<p>While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?</p>\n",
         "2024-08-09 14:46:54",
         "1",
         "148",
         "1",
         "78854613.0",
         "<p>While I can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided JSON files), my personal recommendation would be to fine-tune NLLB in the <code>transformers</code> library, specifically using the <code>Seq2SeqTrainer</code>.</p>\n<p>I did this before for multiple models, including NLLB, check out this repository: <a href=\"https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/\" rel=\"nofollow noreferrer\">https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/</a></p>\n<p>This way the fine-tuning and inference process for the NLLB model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so:</p>\n<pre><code>tokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=&quot;eng_Latn&quot;, tgt_lang=&quot;deu_Latn&quot;)\n</code></pre>\n<p>and generate translations like this:</p>\n<pre><code>model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(&quot;deu_Latn&quot;)[1], max_length=512)\n</code></pre>\n",
         "0.0",
         "\"facebook/nllb-200-distilled-600M\"\n---\nDATA_CONFIG = \"/content/sample_data/data_config.json\"\nOUTPUT_DIR = \"/content/outputs\"\nMODEL_FOLDER = \"/content/drive/MyDrive/Thesis/nllb-checkpoints\"\nDROP = 0.1\nSRC = \"eng_Latn\"\nTGT = \"deu_Latn\"\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=\"$SRC-$TGT\" \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n---\n/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=\"conf\", config_name=\"base_config\")\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 289, in main\n    train_module = TrainModule(config)\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n---\nMissing data_prefix configuration\n---\n{\n    \"data_prefix\": \"/content/sample_data\",\n    \"train_data\": \"train_demo.json\",\n    \"test_data\": \"test_demo.json\",\n    \"lang_pairs\": \"eng_Latn-deu_Latn\"\n}",
         "transformers\n---\nSeq2SeqTrainer\n---\ntokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=\"eng_Latn\", tgt_lang=\"deu_Latn\")\n---\nmodel.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(\"deu_Latn\")[1], max_length=512)",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation",
         "Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB",
         "While I cant help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB While I cant help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB",
         "nllb finetuning error missing data_prefix configuration englishgerman translation im attempting finetune nllb model scientific translation task english eng_latn german deu_latn followed official guidelines finetuning authors nllb documentation link code block giving error error far understand created demo custom data_configjson looks like official documentation provides information im encountering difficulties applying specific use case someone share detailed guide point helpful resources finetuning nllb",
         "nllb finetune error miss data_prefix configuration englishgerman translation I m attempt finetune nllb model scientific translation task english eng_latn german deu_latn follow official guideline finetune author nllb documentation link code block give error error far understand create demo custom data_configjson look like official documentation provide information I m encounter difficulty apply specific use case someone share detailed guide point helpful resource finetune nllb",
         "nllb finetune error miss dataprefix configuration englishgerman translation I attempt finetune nllb scientific translation task english englatn german deulatn official guideline finetune author nllb documentation link block error error far understand create demo custom dataconfigjson like official documentation provide information I encounter difficulty apply specific case someone share detailed guide point helpful resource finetune nllb"
        ],
        [
         "48",
         "78846004",
         "How can I use structured_output with Azure OpenAI with the openai Python library?",
         "<p>I want to use structured output with Azure OpenAI.</p>\n<p>I tried the following code, based on the code given in <a href=\"https://openai.com/index/introducing-structured-outputs-in-the-api/\" rel=\"nofollow noreferrer\">https://openai.com/index/introducing-structured-outputs-in-the-api/</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=&quot;gpt-4omini-2024-07-18-name&quot;,\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful math tutor.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;solve 8x + 31 = 2&quot;},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n</code></pre>\n<p>I get the error:</p>\n<pre><code>openai.BadRequestError: Error code: 400:\n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Invalid parameter: response_format must be one of json_object, text.&quot;,\n        &quot;type&quot;: &quot;invalid_request_error&quot;,\n        &quot;param&quot;: &quot;response_format&quot;,\n        &quot;code&quot;: &quot;None&quot;\n    }\n}\n</code></pre>\n<p>How to fix it?</p>\n<p>I ran <code>pip install -U openai</code>: I use <code>openai==1.40.1</code> and Python 3.11.</p>\n<hr />\n<p>I also tried <a href=\"https://cookbook.openai.com/examples/structured_outputs_intro\" rel=\"nofollow noreferrer\">https://cookbook.openai.com/examples/structured_outputs_intro</a> using  using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message:</p>\n<pre><code>from openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = &quot;https://&lt;your-resource-name&gt;.openai.azure.com&quot;\napi_key = &quot;&lt;your-api-key&gt;&quot;\ndeployment_name = &quot;&lt;your-deployment-name&gt;&quot; # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f&quot;{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01&quot;\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            &quot;role&quot;: &quot;system&quot;,\n            &quot;content&quot;: math_tutor_prompt\n        },\n        {\n            &quot;role&quot;: &quot;user&quot;,\n            &quot;content&quot;: question\n        }\n    ],\n    response_format={\n        &quot;type&quot;: &quot;json_schema&quot;,\n        &quot;json_schema&quot;: {\n            &quot;name&quot;: &quot;math_reasoning&quot;,\n            &quot;schema&quot;: {\n                &quot;type&quot;: &quot;object&quot;,\n                &quot;properties&quot;: {\n                    &quot;steps&quot;: {\n                        &quot;type&quot;: &quot;array&quot;,\n                        &quot;items&quot;: {\n                            &quot;type&quot;: &quot;object&quot;,\n                            &quot;properties&quot;: {\n                                &quot;explanation&quot;: {&quot;type&quot;: &quot;string&quot;},\n                                &quot;output&quot;: {&quot;type&quot;: &quot;string&quot;}\n                            },\n                            &quot;required&quot;: [&quot;explanation&quot;, &quot;output&quot;],\n                            &quot;additionalProperties&quot;: False\n                        }\n                    },\n                    &quot;final_answer&quot;: {&quot;type&quot;: &quot;string&quot;}\n                },\n                &quot;required&quot;: [&quot;steps&quot;, &quot;final_answer&quot;],\n                &quot;additionalProperties&quot;: False\n            },\n            &quot;strict&quot;: True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = &quot;how can I solve 8x + 7 = -23&quot;\n\nresult = get_math_solution(question)\n\nprint(result.content)\n</code></pre>\n",
         "2024-08-07 23:14:19",
         "0",
         "1201",
         "2",
         "78946352.0",
         "<p>Using <code>gpt-4o-2024-08-06</code>, which finally got deployed today (2024-09-03) on Azure, made it work. Code example from <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python-secure\" rel=\"nofollow noreferrer\">learn.microsoft.com</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = &quot;https://your-azure-openai-endpoint.com&quot;\napi_key = &quot;your-azure-openai-key&quot;\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Extract the event information.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Alice and Bob are going to a science fair on Friday.&quot;},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n</code></pre>\n<p>output:</p>\n<pre><code>name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  &quot;id&quot;: &quot;chatcmpl-A3XDRVolXpjeAAQIGddswI990weid&quot;,\n  &quot;choices&quot;: [\n    {\n      &quot;finish_reason&quot;: &quot;stop&quot;,\n      &quot;index&quot;: 0,\n      &quot;logprobs&quot;: null,\n      &quot;message&quot;: {\n        &quot;content&quot;: &quot;{\\&quot;name\\&quot;:\\&quot;Science Fair\\&quot;,\\&quot;date\\&quot;:\\&quot;Friday\\&quot;,\\&quot;participants\\&quot;:[\\&quot;Alice\\&quot;,\\&quot;Bob\\&quot;]}&quot;,\n        &quot;refusal&quot;: null,\n        &quot;role&quot;: &quot;assistant&quot;,\n        &quot;function_call&quot;: null,\n        &quot;tool_calls&quot;: [],\n        &quot;parsed&quot;: {\n          &quot;name&quot;: &quot;Science Fair&quot;,\n          &quot;date&quot;: &quot;Friday&quot;,\n          &quot;participants&quot;: [\n            &quot;Alice&quot;,\n            &quot;Bob&quot;\n          ]\n        }\n      },\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ],\n  &quot;created&quot;: 1725406029,\n  &quot;model&quot;: &quot;gpt-4o-2024-08-06&quot;,\n  &quot;object&quot;: &quot;chat.completion&quot;,\n  &quot;service_tier&quot;: null,\n  &quot;system_fingerprint&quot;: &quot;fp_b2ffeb31ff&quot;,\n  &quot;usage&quot;: {\n    &quot;completion_tokens&quot;: 17,\n    &quot;prompt_tokens&quot;: 32,\n    &quot;total_tokens&quot;: 49\n  },\n  &quot;prompt_filter_results&quot;: [\n    {\n      &quot;prompt_index&quot;: 0,\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ]\n}\n</code></pre>\n<p>Tested with Python 3.11.7 and openai==1.43.0.</p>\n",
         "0.0",
         "from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4omini-2024-07-18-name\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n        {\"role\": \"user\", \"content\": \"solve 8x + 31 = 2\"},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n---\nopenai.BadRequestError: Error code: 400:\n{\n    \"error\": {\n        \"message\": \"Invalid parameter: response_format must be one of json_object, text.\",\n        \"type\": \"invalid_request_error\",\n        \"param\": \"response_format\",\n        \"code\": \"None\"\n    }\n}\n---\npip install -U openai\n---\nopenai==1.40.1\n---\nfrom openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = \"https://<your-resource-name>.openai.azure.com\"\napi_key = \"<your-api-key>\"\ndeployment_name = \"<your-deployment-name>\" # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01\"\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": math_tutor_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": question\n        }\n    ],\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"math_reasoning\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"steps\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"explanation\": {\"type\": \"string\"},\n                                \"output\": {\"type\": \"string\"}\n                            },\n                            \"required\": [\"explanation\", \"output\"],\n                            \"additionalProperties\": False\n                        }\n                    },\n                    \"final_answer\": {\"type\": \"string\"}\n                },\n                \"required\": [\"steps\", \"final_answer\"],\n                \"additionalProperties\": False\n            },\n            \"strict\": True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = \"how can I solve 8x + 7 = -23\"\n\nresult = get_math_solution(question)\n\nprint(result.content)",
         "gpt-4o-2024-08-06\n---\nfrom pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = \"https://your-azure-openai-endpoint.com\"\napi_key = \"your-azure-openai-key\"\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n---\nname='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  \"id\": \"chatcmpl-A3XDRVolXpjeAAQIGddswI990weid\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"{\\\"name\\\":\\\"Science Fair\\\",\\\"date\\\":\\\"Friday\\\",\\\"participants\\\":[\\\"Alice\\\",\\\"Bob\\\"]}\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": [],\n        \"parsed\": {\n          \"name\": \"Science Fair\",\n          \"date\": \"Friday\",\n          \"participants\": [\n            \"Alice\",\n            \"Bob\"\n          ]\n        }\n      },\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"created\": 1725406029,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": \"fp_b2ffeb31ff\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 32,\n    \"total_tokens\": 49\n  },\n  \"prompt_filter_results\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ]\n}",
         "How can I use structured_output with Azure OpenAI with the openai Python library",
         "I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message",
         "Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message",
         "use structured_output azure openai openai python library want use structured output azure openai tried following code based code given get error fix ran use python 311 also tried using using azure+ gpt4o mini 20240718 didnt work either error message",
         "use structured_output azure openai openai python library want use structured output azure openai try follow code base code given get error fix run use python 311 also try use use azure+ gpt4o mini 20240718 do not work either error message",
         "structuredoutput azure openai openai python library structured azure openai base given get error fix run python 311 also azure gpt4o mini 20240718 do not either error message"
        ],
        [
         "49",
         "78836208",
         "Removing bi-grams after tokenization for TfidfVectorizer",
         "<p>I'm attempting to remove bi-grams that are created by <code>TfidfVectorizer</code>.  I'm using <code>text.TfidfVectorizer</code> so that I can use my own preprocessor function.</p>\n<p>Test strings and preprocessor function:</p>\n<pre><code>doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, &quot;&quot;)\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n</code></pre>\n<p>My <code>TfidfVectorizer</code> instantiation and <code>fit_transform</code>:</p>\n<pre><code>from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n</code></pre>\n<p>Here is my error:</p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in &lt;cell line: 5&gt;()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----&gt; 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-&gt; 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 &quot;Upper case characters found in&quot;\n   1332                 &quot; vocabulary while 'lowercase'&quot;\n   1333                 &quot; is True. These entries will not&quot;\n   1334                 &quot; be matched with any documents&quot;\n   1335             )\n   1336             break\n-&gt; 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-&gt; 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--&gt; 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object\n</code></pre>\n<p>How to resolve it?</p>\n",
         "2024-08-05 19:46:40",
         "1",
         "38",
         "1",
         "78837616.0",
         "<p>The preprocessor should handle documents, not the whole corpus. (The clues are the &quot;expected string&quot; in the error, and the fact that <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"nofollow noreferrer\">the <code>TfidfVectorizer</code> docs</a> refer to &quot;the preprocessing (string transformation) stage&quot;. The docs could definitely be clearer.)</p>\n<p>This should fix it:</p>\n<pre><code>def remove_bigrams(doc: str) -&gt; str:\n    &quot;&quot;&quot;Remove certain bi-grams from a document.&quot;&quot;&quot;\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, &quot;&quot;)\n    return doc\n</code></pre>\n",
         "2.0",
         "TfidfVectorizer\n---\ntext.TfidfVectorizer\n---\ndoc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, \"\")\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n---\nTfidfVectorizer\n---\nfit_transform\n---\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n---\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in <cell line: 5>()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----> 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-> 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 \"Upper case characters found in\"\n   1332                 \" vocabulary while 'lowercase'\"\n   1333                 \" is True. These entries will not\"\n   1334                 \" be matched with any documents\"\n   1335             )\n   1336             break\n-> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-> 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--> 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object",
         "TfidfVectorizer\n---\ndef remove_bigrams(doc: str) -> str:\n    \"\"\"Remove certain bi-grams from a document.\"\"\"\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, \"\")\n    return doc",
         "Removing bigrams after tokenization for TfidfVectorizer",
         "Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it",
         "The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "Removing bigrams after tokenization for TfidfVectorizer Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "Removing bigrams after tokenization for TfidfVectorizer Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it",
         "removing bigrams tokenization tfidfvectorizer im attempting remove bigrams created im using use preprocessor function test strings preprocessor function instantiation error resolve",
         "remove bigrams tokenization tfidfvectorizer I m attempt remove bigram create I m use use preprocessor function test string preprocessor function instantiation error resolve",
         "remove bigrams tokenization tfidfvectorizer I attempt remove bigram create I preprocessor function test preprocessor function instantiation error resolve"
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 8510
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AcceptedAnswerBody</th>\n",
       "      <th>AcceptedAnswerScore</th>\n",
       "      <th>Question_Code</th>\n",
       "      <th>Answer_Code</th>\n",
       "      <th>Title_Clean</th>\n",
       "      <th>Body_Clean</th>\n",
       "      <th>AcceptedAnswerBody_Clean</th>\n",
       "      <th>combination_text</th>\n",
       "      <th>combination_text_only_question</th>\n",
       "      <th>combination_text_only_question_no_stopw</th>\n",
       "      <th>combination_text_only_question_lemma</th>\n",
       "      <th>combination_text_only_question_lemma_no_noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>2025-04-02 05:56:11</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>79552218.0</td>\n",
       "      <td>&lt;p&gt;The configuration file is missing the 'labe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>import spacy\\n\\nnlp = spacy.load(\"pl_core_news...</td>\n",
       "      <td>labels_to_ignore:\\n    - O\\n---\\nnlp_engine_na...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>Im using spaCy with the pl_core_news_lg model ...</td>\n",
       "      <td>The configuration file is missing the labels_t...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79548202</td>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>&lt;p&gt;I understand the -100 label id is used so t...</td>\n",
       "      <td>2025-04-01 09:21:17</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>79551169.0</td>\n",
       "      <td>&lt;p&gt;The author of the tutorial you mentioned se...</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>-100\\n---\\nignore_index\\n---\\nignore_index\\n--...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>I understand the 100 label id is used so that ...</td>\n",
       "      <td>The author of the tutorial you mentioned sets ...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>gpt2 models huggingface 100 label index traini...</td>\n",
       "      <td>gpt2 model huggingface 100 label index trainin...</td>\n",
       "      <td>gpt2 huggingface 100 label index training inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79523269</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>&lt;p&gt;I am trying to import gensim into colab.&lt;/p...</td>\n",
       "      <td>2025-03-20 14:36:02</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>79523777.0</td>\n",
       "      <td>&lt;p&gt;You have to restart the session for the und...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>!pip install gensim\\n---\\n/usr/local/lib/pytho...</td>\n",
       "      <td>numpy\\n---\\nnumpy\\n---\\nscipy</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>I am trying to import gensim into colab I get ...</td>\n",
       "      <td>You have to restart the session for the underl...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "      <td>trouble getting importing gensim work colab tr...</td>\n",
       "      <td>trouble getting import gensim work colab try i...</td>\n",
       "      <td>trouble getting import gensim colab import gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this &lt;a href...</td>\n",
       "      <td>2025-03-11 14:50:31</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>79501337.0</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>server\\n---\\nSSH\\n---\\nskip_tokens = [1]  # sk...</td>\n",
       "      <td>matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>I am running the code found on this site in my...</td>\n",
       "      <td>I cant test it but I checked source code and i...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>store images instead showing server running co...</td>\n",
       "      <td>store image instead show server run code find ...</td>\n",
       "      <td>store image instead show server run site serve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79482283</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>&lt;p&gt;I am using presidio/langchain_experimental ...</td>\n",
       "      <td>2025-03-03 22:27:07</td>\n",
       "      <td>4</td>\n",
       "      <td>230</td>\n",
       "      <td>2</td>\n",
       "      <td>79495969.0</td>\n",
       "      <td>&lt;p&gt;After some test I was able to find the solu...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>from presidio_anonymizer import PresidioAnonym...</td>\n",
       "      <td>config = {\\n    \"nlp_engine_name\": \"spacy\",\\n ...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>I am using presidio/langchain_experimental to ...</td>\n",
       "      <td>After some test I was able to find the solutio...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>62328</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>&lt;p&gt;input: phrase 1, phrase 2&lt;/p&gt;\\n\\n&lt;p&gt;output:...</td>\n",
       "      <td>2008-09-15 12:26:42</td>\n",
       "      <td>65</td>\n",
       "      <td>49889</td>\n",
       "      <td>11</td>\n",
       "      <td>63076.0</td>\n",
       "      <td>&lt;hr&gt;\\n\\n&lt;p&gt;You might want to check out this pa...</td>\n",
       "      <td>44.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>input phrase 1 phrase 2 output semantic simila...</td>\n",
       "      <td>You might want to check out this paper Sentenc...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>algorithm tells semantic similarity two phrase...</td>\n",
       "      <td>algorithm tell semantic similarity two phrase ...</td>\n",
       "      <td>algorithm tell semantic similarity phrase inpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8506</th>\n",
       "      <td>42489</td>\n",
       "      <td>How to implement a \"related\" degree measure al...</td>\n",
       "      <td>&lt;p&gt;I was going to Ask a Question earlier today...</td>\n",
       "      <td>2008-09-03 20:21:04</td>\n",
       "      <td>8</td>\n",
       "      <td>456</td>\n",
       "      <td>2</td>\n",
       "      <td>42532.0</td>\n",
       "      <td>&lt;p&gt;One such way to implement such an algorithm...</td>\n",
       "      <td>5.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>I was going to Ask a Question earlier today wh...</td>\n",
       "      <td>One such way to implement such an algorithm wo...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>implement related degree measure algorithm goi...</td>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03 10:36:13</td>\n",
       "      <td>118</td>\n",
       "      <td>33200</td>\n",
       "      <td>11</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>&lt;p&gt;Actually what Google does is very much non-...</td>\n",
       "      <td>87.0</td>\n",
       "      <td>&lt;spell_checked_word&gt;</td>\n",
       "      <td></td>\n",
       "      <td>How do you implement a Did you mean</td>\n",
       "      <td>Possible Duplicate How does the Google Did you...</td>\n",
       "      <td>Actually what Google does is much nontrivial a...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8508</th>\n",
       "      <td>36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31 01:08:48</td>\n",
       "      <td>3</td>\n",
       "      <td>5661</td>\n",
       "      <td>6</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>&lt;p&gt;Citation from Vista &lt;a href=\"http://blogs.m...</td>\n",
       "      <td>8.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>my primary language is spanish but I use all m...</td>\n",
       "      <td>Citation from Vista speech recognition blog In...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>vista speech recognition multiple languages pr...</td>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22 22:45:10</td>\n",
       "      <td>27</td>\n",
       "      <td>6484</td>\n",
       "      <td>9</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>&lt;p&gt;We developed exactly what you are looking f...</td>\n",
       "      <td>12.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Natural language date/time parser for NET</td>\n",
       "      <td>Does anyone know of a NET date/time parser sim...</td>\n",
       "      <td>We developed exactly what you are looking for ...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "      <td>natural language date/time parser net anyone k...</td>\n",
       "      <td>natural language date / time parser net anyone...</td>\n",
       "      <td>natural language date time parser net anyone n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8510 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      QuestionId                                              Title  \\\n",
       "0       79549787  Why does Presidio with spacy nlp engine not re...   \n",
       "1       79548202  GPT-2 and other models from huggingface -100 l...   \n",
       "2       79523269  Trouble getting importing gensim to work in colab   \n",
       "3       79501178        Store images instead of showing in a server   \n",
       "4       79482283  Presidio with Langchain Experimental does not ...   \n",
       "...          ...                                                ...   \n",
       "8505       62328  Is there an algorithm that tells the semantic ...   \n",
       "8506       42489  How to implement a \"related\" degree measure al...   \n",
       "8507       41424             How do you implement a \"Did you mean\"?   \n",
       "8508       36533     Vista speech recognition in multiple languages   \n",
       "8509       23689        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                   Body         CreationDate  \\\n",
       "0     <p>I'm using spaCy with the pl_core_news_lg mo...  2025-04-02 05:56:11   \n",
       "1     <p>I understand the -100 label id is used so t...  2025-04-01 09:21:17   \n",
       "2     <p>I am trying to import gensim into colab.</p...  2025-03-20 14:36:02   \n",
       "3     <p>I am running the code found on this <a href...  2025-03-11 14:50:31   \n",
       "4     <p>I am using presidio/langchain_experimental ...  2025-03-03 22:27:07   \n",
       "...                                                 ...                  ...   \n",
       "8505  <p>input: phrase 1, phrase 2</p>\\n\\n<p>output:...  2008-09-15 12:26:42   \n",
       "8506  <p>I was going to Ask a Question earlier today...  2008-09-03 20:21:04   \n",
       "8507  <blockquote>\\n  <p><strong>Possible Duplicate:...  2008-09-03 10:36:13   \n",
       "8508  <p>my primary language is spanish, but I use a...  2008-08-31 01:08:48   \n",
       "8509  <p>Does anyone know of a .NET date/time parser...  2008-08-22 22:45:10   \n",
       "\n",
       "      Score  ViewCount  AnswerCount  AcceptedAnswerId  \\\n",
       "0         0         68            1        79552218.0   \n",
       "1         0         46            1        79551169.0   \n",
       "2         0        125            1        79523777.0   \n",
       "3         0         36            1        79501337.0   \n",
       "4         4        230            2        79495969.0   \n",
       "...     ...        ...          ...               ...   \n",
       "8505     65      49889           11           63076.0   \n",
       "8506      8        456            2           42532.0   \n",
       "8507    118      33200           11           41448.0   \n",
       "8508      3       5661            6           36684.0   \n",
       "8509     27       6484            9          631134.0   \n",
       "\n",
       "                                     AcceptedAnswerBody  AcceptedAnswerScore  \\\n",
       "0     <p>The configuration file is missing the 'labe...                  1.0   \n",
       "1     <p>The author of the tutorial you mentioned se...                  1.0   \n",
       "2     <p>You have to restart the session for the und...                  1.0   \n",
       "3     <p>I can't test it but ...</p>\\n<p>I checked <...                  1.0   \n",
       "4     <p>After some test I was able to find the solu...                 -2.0   \n",
       "...                                                 ...                  ...   \n",
       "8505  <hr>\\n\\n<p>You might want to check out this pa...                 44.0   \n",
       "8506  <p>One such way to implement such an algorithm...                  5.0   \n",
       "8507  <p>Actually what Google does is very much non-...                 87.0   \n",
       "8508  <p>Citation from Vista <a href=\"http://blogs.m...                  8.0   \n",
       "8509  <p>We developed exactly what you are looking f...                 12.0   \n",
       "\n",
       "                                          Question_Code  \\\n",
       "0     import spacy\\n\\nnlp = spacy.load(\"pl_core_news...   \n",
       "1                                                         \n",
       "2     !pip install gensim\\n---\\n/usr/local/lib/pytho...   \n",
       "3     server\\n---\\nSSH\\n---\\nskip_tokens = [1]  # sk...   \n",
       "4     from presidio_anonymizer import PresidioAnonym...   \n",
       "...                                                 ...   \n",
       "8505                                                      \n",
       "8506                                                      \n",
       "8507                               <spell_checked_word>   \n",
       "8508                                                      \n",
       "8509                                                      \n",
       "\n",
       "                                            Answer_Code  \\\n",
       "0     labels_to_ignore:\\n    - O\\n---\\nnlp_engine_na...   \n",
       "1     -100\\n---\\nignore_index\\n---\\nignore_index\\n--...   \n",
       "2                         numpy\\n---\\nnumpy\\n---\\nscipy   \n",
       "3     matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...   \n",
       "4     config = {\\n    \"nlp_engine_name\": \"spacy\",\\n ...   \n",
       "...                                                 ...   \n",
       "8505                                                      \n",
       "8506                                                      \n",
       "8507                                                      \n",
       "8508                                                      \n",
       "8509                                                      \n",
       "\n",
       "                                            Title_Clean  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in colab   \n",
       "3           Store images instead of showing in a server   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507                How do you implement a Did you mean   \n",
       "8508     Vista speech recognition in multiple languages   \n",
       "8509          Natural language date/time parser for NET   \n",
       "\n",
       "                                             Body_Clean  \\\n",
       "0     Im using spaCy with the pl_core_news_lg model ...   \n",
       "1     I understand the 100 label id is used so that ...   \n",
       "2     I am trying to import gensim into colab I get ...   \n",
       "3     I am running the code found on this site in my...   \n",
       "4     I am using presidio/langchain_experimental to ...   \n",
       "...                                                 ...   \n",
       "8505  input phrase 1 phrase 2 output semantic simila...   \n",
       "8506  I was going to Ask a Question earlier today wh...   \n",
       "8507  Possible Duplicate How does the Google Did you...   \n",
       "8508  my primary language is spanish but I use all m...   \n",
       "8509  Does anyone know of a NET date/time parser sim...   \n",
       "\n",
       "                               AcceptedAnswerBody_Clean  \\\n",
       "0     The configuration file is missing the labels_t...   \n",
       "1     The author of the tutorial you mentioned sets ...   \n",
       "2     You have to restart the session for the underl...   \n",
       "3     I cant test it but I checked source code and i...   \n",
       "4     After some test I was able to find the solutio...   \n",
       "...                                                 ...   \n",
       "8505  You might want to check out this paper Sentenc...   \n",
       "8506  One such way to implement such an algorithm wo...   \n",
       "8507  Actually what Google does is much nontrivial a...   \n",
       "8508  Citation from Vista speech recognition blog In...   \n",
       "8509  We developed exactly what you are looking for ...   \n",
       "\n",
       "                                       combination_text  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in co...   \n",
       "3     Store images instead of showing in a server I ...   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507  How do you implement a Did you mean Possible D...   \n",
       "8508  Vista speech recognition in multiple languages...   \n",
       "8509  Natural language date/time parser for NET Does...   \n",
       "\n",
       "                         combination_text_only_question  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in co...   \n",
       "3     Store images instead of showing in a server I ...   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507  How do you implement a Did you mean Possible D...   \n",
       "8508  Vista speech recognition in multiple languages...   \n",
       "8509  Natural language date/time parser for NET Does...   \n",
       "\n",
       "                combination_text_only_question_no_stopw  \\\n",
       "0     presidio spacy nlp engine recognize organizati...   \n",
       "1     gpt2 models huggingface 100 label index traini...   \n",
       "2     trouble getting importing gensim work colab tr...   \n",
       "3     store images instead showing server running co...   \n",
       "4     presidio langchain experimental detect polish ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tells semantic similarity two phrase...   \n",
       "8506  implement related degree measure algorithm goi...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple languages pr...   \n",
       "8509  natural language date/time parser net anyone k...   \n",
       "\n",
       "                   combination_text_only_question_lemma  \\\n",
       "0     presidio spacy nlp engine recognize organizati...   \n",
       "1     gpt2 model huggingface 100 label index trainin...   \n",
       "2     trouble getting import gensim work colab try i...   \n",
       "3     store image instead show server run code find ...   \n",
       "4     presidio langchain experimental detect polish ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tell semantic similarity two phrase ...   \n",
       "8506  implement relate degree measure algorithm go a...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple language pri...   \n",
       "8509  natural language date / time parser net anyone...   \n",
       "\n",
       "          combination_text_only_question_lemma_no_noise  \n",
       "0     presidio spacy nlp engine recognize organizati...  \n",
       "1     gpt2 huggingface 100 label index training inst...  \n",
       "2     trouble getting import gensim colab import gen...  \n",
       "3     store image instead show server run site serve...  \n",
       "4     presidio langchain experimental detect polish ...  \n",
       "...                                                 ...  \n",
       "8505  algorithm tell semantic similarity phrase inpu...  \n",
       "8506  implement relate degree measure algorithm go a...  \n",
       "8507  implement mean possible duplicate google mean ...  \n",
       "8508  vista speech recognition multiple language pri...  \n",
       "8509  natural language date time parser net anyone n...  \n",
       "\n",
       "[8510 rows x 20 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_embedding(df_post_questions,'combination_text_only_question_lemma_no_noise',model_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerScore",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Question_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Answer_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerBody_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question_no_stopw",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question_lemma",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question_lemma_no_noise",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cluster_kmean",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e132d253-9e6e-4e2d-97a3-d12708c99dd4",
       "rows": [
        [
         "0",
         "79549787",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?",
         "<p>I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER):</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;pl_core_news_lg&quot;)\ntext = &quot;Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.&quot;\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n</code></pre>\n<p>Output:</p>\n<pre><code>[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n</code></pre>\n<p>However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities.</p>\n<pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=&quot;path_to_my_file/nlp_config.yaml&quot;) \nnlp_engine = provider.create_engine()\n\nprint(f&quot;Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}&quot;)\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[&quot;pl&quot;])\nregistry.load_predefined_recognizers([&quot;pl&quot;])\n\nprint(f&quot;Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}&quot;)\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, &quot;pl&quot;)\n\nfor entity in results:\n    print(f&quot;Found entity: {entity.entity_type} with score {entity.score}&quot;)\n</code></pre>\n<p>Output:</p>\n<pre><code>Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n</code></pre>\n<p>Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text.</p>\n<p>My config file:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n<p>Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?</p>\n",
         "2025-04-02 05:56:11",
         "0",
         "68",
         "1",
         "79552218.0",
         "<p>The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine :</p>\n<pre><code>  labels_to_ignore:\n    - O\n</code></pre>\n<p>On your configuration it would look like this:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n",
         "1.0",
         "import spacy\n\nnlp = spacy.load(\"pl_core_news_lg\")\ntext = \"Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.\"\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n---\n[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n---\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=\"path_to_my_file/nlp_config.yaml\") \nnlp_engine = provider.create_engine()\n\nprint(f\"Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}\")\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[\"pl\"])\nregistry.load_predefined_recognizers([\"pl\"])\n\nprint(f\"Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}\")\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, \"pl\")\n\nfor entity in results:\n    print(f\"Found entity: {entity.entity_type} with score {entity.score}\")\n---\nSupported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "labels_to_ignore:\n    - O\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does",
         "Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them",
         "The configuration file is missing the labels_to_ignore field stating that no entities should be ignored in the nlp engine On your configuration it would look like this",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them The configuration file is missing the labels_to_ignore field stating that no entities should be ignored in the nlp engine On your configuration it would look like this",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them",
         "presidio spacy nlp engine recognize organizations pesel spacy im using spacy pl_core_news_lg model extract named entities polish text correctly detects organizations org peoples names per output however use presidio pl_core_news_lg model configuration file recognizers correctly detect organizations org pesel numbers even though appear list supported entities output even though organization pl_pesel listed org listed nlp engine supported recognizers presidio detect correctly text config file presidio fail detect organizations org pesel numbers pl_pesel spacy correctly detects",
         "presidio spacy nlp engine recognize organization pesel spacy I m use spacy pl_core_news_lg model extract name entity polish text correctly detect organization org people name per output however use presidio pl_core_news_lg model configuration file recognizer correctly detect organization org pesel number even though appear list support entity output even though organization pl_pesel list org list nlp engine support recognizer presidio detect correctly text config file presidio fail detect organization org pesel number pl_pesel spacy correctly detect",
         "presidio spacy nlp engine recognize organization pesel spacy I spacy plcorenewslg extract name entity polish correctly detect organization org people name per however presidio plcorenewslg configuration recognizer correctly detect organization org pesel number even though appear support entity even though organization plpesel org nlp engine support recognizer presidio detect correctly config presidio fail detect organization org pesel number plpesel spacy correctly detect",
         "9"
        ],
        [
         "1",
         "79548202",
         "GPT-2 and other models from huggingface -100 label index for training, instead of pad token",
         "<p>I understand the -100 label id is used so that the predictions for these are not included when calculating the loss.</p>\n<p>However on <a href=\"https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16#bert2gpt2-summarization-with-%F0%9F%A4%97-encoderdecoder-framework\" rel=\"nofollow noreferrer\">huggingface</a>, they state\n&quot;complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not&quot;, when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument &quot;ignore_index&quot;.</p>\n<p>Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same?</p>\n<p>The way it is written makes me think there is some benefit, but the description of &quot;ignore_index&quot; appears to achieve what is wanted.</p>\n",
         "2025-04-01 09:21:17",
         "0",
         "46",
         "1",
         "79551169.0",
         "<p>The author of the tutorial you mentioned sets it to <code>-100</code> <strong>and</strong> uses <code>ignore_index</code> to save a few lines of code. You don't see the line where the author pass something to <code>ignore_index</code> because it has a default value. The default value of <code>ignore_index</code> for <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\" rel=\"nofollow noreferrer\">nn.CrossEntropyLoss</a> is <code>-100</code>. Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.</p>\n",
         "1.0",
         "",
         "-100\n---\nignore_index\n---\nignore_index\n---\nignore_index\n---\n-100",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token",
         "I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted",
         "The author of the tutorial you mentioned sets it to and uses to save a few lines of code You dont see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you dont have to pass the pad token id from tokenizer down to the loss function",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted The author of the tutorial you mentioned sets it to and uses to save a few lines of code You dont see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you dont have to pass the pad token id from tokenizer down to the loss function",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted",
         "gpt2 models huggingface 100 label index training instead pad token understand 100 label id used predictions included calculating loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label excluded replacing pad tokens implementation use nncrossentropyloss argument ignore_index benefit changing id 100 opposed adding argument ignore_index loss setting pad token id results way written makes think benefit description ignore_index appears achieve wanted",
         "gpt2 model huggingface 100 label index training instead pad token understand 100 label i d use prediction include calculate loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label exclude replace pad token implementation use nncrossentropyloss argument ignore_index benefit change i d 100 oppose add argument ignore_index loss set pad token i d result way write make think benefit description ignore_index appear achieve wanted",
         "gpt2 huggingface 100 label index training instead pad token understand 100 label i d prediction include calculate loss however huggingface state complicated comprehension padtokenid alone good enough whether label exclude replace pad token implementation nncrossentropyloss argument ignoreindex benefit change i d 100 oppose add argument ignoreindex loss set pad token i d write make think benefit description ignoreindex appear achieve wanted",
         "2"
        ],
        [
         "2",
         "79523269",
         "Trouble getting importing gensim to work in colab",
         "<p>I am trying to import gensim into colab.</p>\n<pre><code>!pip install gensim\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--&gt; 367             msg = (&quot;The current Numpy installation ({!r}) fails to &quot;\n    368                    &quot;pass simple sanity checks. This can be caused for example &quot;\n    369                    &quot;by incorrect BLAS library being linked in, or by mixing &quot;\n\nModuleNotFoundError: No module named 'numpy.char'\n</code></pre>\n<p>my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks</p>\n",
         "2025-03-20 14:36:02",
         "0",
         "125",
         "1",
         "79523777.0",
         "<p>You have to restart the session for the underlying runtime to notice the package changes. See: <a href=\"https://stackoverflow.com/a/79518359/130288\">https://stackoverflow.com/a/79518359/130288</a></p>\n<p>I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded <code>numpy</code>/etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade.</p>\n<p>But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two.</p>\n<p>Explicitly restarting after the Gensim-install &amp; <code>numpy</code>/<code>scipy</code> downgrades resolves the errors.</p>\n",
         "1.0",
         "!pip install gensim\n---\n/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--> 367             msg = (\"The current Numpy installation ({!r}) fails to \"\n    368                    \"pass simple sanity checks. This can be caused for example \"\n    369                    \"by incorrect BLAS library being linked in, or by mixing \"\n\nModuleNotFoundError: No module named 'numpy.char'",
         "numpy\n---\nnumpy\n---\nscipy",
         "Trouble getting importing gensim to work in colab",
         "I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks",
         "You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadnt yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadnt yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks",
         "trouble getting importing gensim work colab trying import gensim colab get following error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string related issue thanks",
         "trouble getting import gensim work colab try import gensim colab get follow error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string relate issue thank",
         "trouble getting import gensim colab import gensim colab get error numpy version 202 downgrade numpy another version like say 1264 get error always numpy relate issue thank",
         "4"
        ],
        [
         "3",
         "79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">site</a> in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?</p>\n",
         "2025-03-11 14:50:31",
         "0",
         "36",
         "1",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n",
         "1.0",
         "server\n---\nSSH\n---\nskip_tokens = [1]  # skip the special token for the start of the text <s>\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = \"playing guitar, hiking, and spending time with his family.\"\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)",
         "matplotlib\n---\nshow=True\n---\nfig, ax\n---\nimport matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(\"output.png\")\n#plt.show()  # eventually show it after saving\n---\nfig\n---\nfig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(\"output.png\")",
         "Store images instead of showing in a server",
         "I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them",
         "I cant test it but I checked source code and it uses for this If you remove then it shouldnt show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them I cant test it but I checked source code and it uses for this If you remove then it shouldnt show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them",
         "store images instead showing server running code found site server would like store images instead showing since connected remotely ssh connection via connection code instance one store files locally instead showing",
         "store image instead show server run code find site server would like store image instead show since connect remotely ssh connection via connection code instance one store file locally instead show",
         "store image instead show server run site server would like store image instead show since connect remotely ssh connection via connection instance store locally instead show",
         "4"
        ],
        [
         "4",
         "79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03 22:27:07",
         "4",
         "230",
         "2",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n",
         "-2.0",
         "from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"pl\", \"model_name\": \"pl_core_news_lg\"}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                               languages_config=config)\n\ntext = \"Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\"\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(\"Anonymized text:\", anonymized_result)\nprint(\"Deanonymized text:\", deanonymized_result)\nprint(\"Map:\", anonymizer_tool.deanonymizer_mapping)\nprint(\"Anonymized text:\", anon_result)\n---\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\npython -m spacy download pl_core_news_lg\n---\nanonymizer_tool = PresidioReversibleAnonymizer()\n---\nAnonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n---\nnlp = spacy.load(\"pl_core_news_lg\")\ndoc = nlp(text)\n---\nJan Kowalski persName\nWarszawie placeName",
         "config = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": 'pl', \"model_name\": \"pl_core_news_lg\"}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=\"pl\",\n    supported_entities=[\"persName\"]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"CREDIT_CARD\"], languages_config=config)\n---\nAnonymized text: <persName> mieszka w Warszawie i ma e-mail glenn58@example.org.\n---\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\nMap: {'persName': {'<persName>': 'Jan Kowalski', '<persName_2>': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}\n---\nSpacyRecognizer\n---\nsupported_entities",
         "Presidio with Langchain Experimental does not detect Polish names",
         "I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected",
         "After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCys requirements I believe theres something missing or unclear in the documentation which is causing the misunderstanding",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCys requirements I believe theres something missing or unclear in the documentation which is causing the misunderstanding",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected",
         "presidio langchain experimental detect polish names using presidio/langchain_experimental anonymize text polish detect names eg jan kowalski code output expected name jan kowalski email address anonymized output remains unchanged installed pl_core_news_lg model using missing something configuration presidio support polish entity recognition properly suggestions make detect names polish interesting thing use output look like mentioned use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio works expected",
         "presidio langchain experimental detect polish name use presidio / langchain_experimental anonymize text polish detect name eg jan kowalski code output expect name jan kowalski email address anonymize output remain unchanged instal pl_core_news_lg model use miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing use output look like mention use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio work expect",
         "presidio langchain experimental detect polish name presidio langchainexperimental anonymize polish detect name eg jan kowalski expect name jan kowalski email address anonymize remain unchanged instal plcorenewslg miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing like mention spacy correct guess problem presidio spacy would like create custom analyzer spacy presidio expect",
         "9"
        ],
        [
         "5",
         "79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22 16:06:11",
         "-1",
         "40",
         "1",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n",
         "1.0",
         "",
         "",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall",
         "Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isnt The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "OpenNLP POSTaggerME and ChunkerME synergy Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isnt The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "OpenNLP POSTaggerME and ChunkerME synergy Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall",
         "opennlp postaggerme chunkerme synergy im trying use opennlp chunking api chunk portuguese sentence first tokenized sentence using tokenizerme tagged postaggerme used readymade models provided project sentence ivo viu uva postaggerme returns tags propn verb det noun model seems using ud pos tags readymade model chunkerme portuguese followed instructions training first using chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything worked well sentence chunker produced correct tags bnp bvp bnp inp complex sentences hasnt produced good results trying identify could improve chunker training one things noticed difference types tags portuguese corpus bosque 80 seems using portuguese tags example instead propn corpus uses prop instead det uses art seems could lead problems especially since one parameters chunker receives array ud tags trained another type tag writing code creating routine convert portuguese notation ud penn wanted ask indeed impact tool already translation suggestions improving chunker precision/recall",
         "opennlp postaggerme chunkerme synergy I m try use opennlp chunk api chunk portuguese sentence first tokenized sentence use tokenizerme tag postaggerme use readymade model provide project sentence ivo viu uva postaggerme return tag propn verb det noun model seem use ud pos tag readymade model chunkerme portuguese follow instruction training first use chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything work well sentence chunker produce correct tag bnp bvp bnp inp complex sentence have not produce good result try identify could improve chunker training one thing notice difference type tag portuguese corpus bosque 80 seem use portuguese tag example instead propn corpus use prop instead det use art seem could lead problem especially since one parameter chunker receive array ud tag train another type tag write code create routine convert portuguese notation ud penn want ask indeed impact tool already translation suggestion improve chunker precision / recall",
         "opennlp postaggerme chunkerme synergy I opennlp chunk api chunk portuguese first tokenized tokenizerme tag postaggerme readymade provide project ivo viu uva postaggerme return tag propn verb det noun ud pos tag readymade chunkerme portuguese instruction training first chunkerconverter tool convert arvore deitada conll2000 generating chunkertrainerme tool everything chunker produce correct tag bnp bvp bnp inp complex have not produce good identify could improve chunker training thing notice difference type tag portuguese corpus bosque 80 portuguese tag instead propn corpus prop instead det art could lead problem especially since parameter chunker receive array ud tag train another type tag write create routine convert portuguese notation ud penn ask indeed impact tool already translation suggestion improve chunker precision recall",
         "3"
        ],
        [
         "6",
         "79451974",
         "word/ sentence similarities",
         "<p>I am trying to find if a given word/ set of words are similar to a definition.</p>\n<p>Example - Definition - &quot;vegetarian User&quot;</p>\n<p>Now, if I want to check a set of sentences like below</p>\n<pre><code>sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n</code></pre>\n<p>I tried using some sentence transformer like below</p>\n<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n</code></pre>\n<p>But this is not giving me expected results.</p>\n<p>What is the best approach to achieve results like below?</p>\n<pre><code>[False,True,True,False]\n</code></pre>\n<p>Is it doable with nlp/ some other technique?</p>\n",
         "2025-02-19 15:47:45",
         "1",
         "50",
         "1",
         "79461281.0",
         "<p>Yes, it’s definitely doable using NLP! The key here is that you don’t need a full similarity matrix; you want to check if each sentence is semantically similar to the given definition.</p>\n<p>✅ Better Approach:\nEncode both the definition and sentences using a sentence transformer.\nCompute cosine similarity between the definition embedding and each sentence embedding.\nSet a threshold (e.g., 0.6 or 0.7) to determine if they are &quot;similar enough.&quot;</p>\n<pre><code>from sentence_transformers import SentenceTransformer, util\n# Load the pre-trained model\nmodel = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\n\n# Definition and sentences\ndefinition = &quot;vegetarian User&quot;\nsentences = [\n  'vegetarian User',\n  'user sometimes eats chicken',\n  'user is vegetarian',\n  'user only eats fruits',\n  'user likes fish'\n]\n\n# Encode the definition and sentences\ndefinition_embedding = model.encode(definition, convert_to_tensor=True)\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine similarities\nsimilarities = util.cos_sim(definition_embedding, sentence_embeddings)[0]\n\n# Set a threshold for similarity (tune this value as needed)\nthreshold = 0.6\nresults = [sim &gt;= threshold for sim in similarities]\n\n# Print results\nprint(results)  # Example output: [True, False, True, False, False]\n</code></pre>\n<p>💡 Explanation:\nutil.cos_sim computes the cosine similarity between the definition and each sentence.\nThreshold tuning:\nIf the similarity is above the threshold, consider it True.\nAdjust the threshold based on how strict you want the matching.</p>\n<p>🔍 Why the original approach didn’t work:\nmodel.similarity doesn’t exist in the SentenceTransformers API.\nYou were computing a sentence-to-sentence matrix, not definition-to-sentence comparisons.</p>\n",
         "1.0",
         "sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n---\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n---\n[False,True,True,False]",
         "from sentence_transformers import SentenceTransformer, util\n# Load the pre-trained model\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\n\n# Definition and sentences\ndefinition = \"vegetarian User\"\nsentences = [\n  'vegetarian User',\n  'user sometimes eats chicken',\n  'user is vegetarian',\n  'user only eats fruits',\n  'user likes fish'\n]\n\n# Encode the definition and sentences\ndefinition_embedding = model.encode(definition, convert_to_tensor=True)\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine similarities\nsimilarities = util.cos_sim(definition_embedding, sentence_embeddings)[0]\n\n# Set a threshold for similarity (tune this value as needed)\nthreshold = 0.6\nresults = [sim >= threshold for sim in similarities]\n\n# Print results\nprint(results)  # Example output: [True, False, True, False, False]",
         "word/ sentence similarities",
         "I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique",
         "Yes its definitely doable using NLP The key here is that you dont need a full similarity matrix you want to check if each sentence is semantically similar to the given definition Better Approach Encode both the definition and sentences using a sentence transformer Compute cosine similarity between the definition embedding and each sentence embedding Set a threshold eg 06 or 07 to determine if they are similar enough Explanation utilcos_sim computes the cosine similarity between the definition and each sentence Threshold tuning If the similarity is above the threshold consider it True Adjust the threshold based on how strict you want the matching Why the original approach didnt work modelsimilarity doesnt exist in the SentenceTransformers API You were computing a sentencetosentence matrix not definitiontosentence comparisons",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique Yes its definitely doable using NLP The key here is that you dont need a full similarity matrix you want to check if each sentence is semantically similar to the given definition Better Approach Encode both the definition and sentences using a sentence transformer Compute cosine similarity between the definition embedding and each sentence embedding Set a threshold eg 06 or 07 to determine if they are similar enough Explanation utilcos_sim computes the cosine similarity between the definition and each sentence Threshold tuning If the similarity is above the threshold consider it True Adjust the threshold based on how strict you want the matching Why the original approach didnt work modelsimilarity doesnt exist in the SentenceTransformers API You were computing a sentencetosentence matrix not definitiontosentence comparisons",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique",
         "word/ sentence similarities trying find given word/ set words similar definition example definition vegetarian user want check set sentences like tried using sentence transformer like giving expected results best approach achieve results like doable nlp/ technique",
         "word/ sentence similarity try find give word/ set word similar definition example definition vegetarian user want check set sentence like try use sentence transformer like give expect result good approach achieve result like doable nlp/ technique",
         "similarity set similar definition definition vegetarian user check set like transformer like expect good approach achieve like doable nlp technique",
         "0"
        ],
        [
         "7",
         "79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07 02:48:25",
         "-1",
         "45",
         "1",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n",
         "0.0",
         "lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()",
         "",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged",
         "I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue",
         "Based on extra information in the comments Im going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort Im fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldnt find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they dont handle multiple meanings of the word So queen might be a female king as in embeddings party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers dont have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue Based on extra information in the comments Im going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort Im fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldnt find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they dont handle multiple meanings of the word So queen might be a female king as in embeddings party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers dont have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue",
         "underfitting pretrained glove + lstm model accurcacy unchanged sentiment classification using pretrained glove lstm model use google play review scrap resulting 50k++ texts implement random sampling minority classes however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embedding size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set padded 140997 200 total x validation set padded 17625 200 total x testing set padded 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue",
         "underfitte pretraine glove + lstm model accurcacy unchanged sentiment classification use pretraine glove lstm model use google play review scrap result 50k++ text implement random sampling minority class however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue",
         "underfitte pretraine glove lstm accurcacy unchanged sentiment classification pretraine glove lstm google play review scrap 50k implement random sampling minority class however train lstm training accuracy remain unchanged several epoch insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling positive 58749 negative 26643 neutral 9106 label distribution random sampling positive 58749 negative 26643 neutral 9106 total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set hot 140997 3 total validation set hot 17625 3 total testing set hot 17625 2003 full enter link description highlight issue",
         "7"
        ],
        [
         "8",
         "79330283",
         "Can't compile Marian NMT",
         "<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=\"https://marian-nmt.github.io/docs/#installation\" rel=\"nofollow noreferrer\">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>\n<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>\n<p>These are the steps that I followed:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n</code></pre>\n<p>This is the result I had:</p>\n<pre><code>➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2\n</code></pre>\n<p>Please help.</p>\n",
         "2025-01-05 06:04:59",
         "4",
         "68",
         "1",
         "79332711.0",
         "<p>The diagnostic that your build is tripping, <code>Wtemplate-id-cdtor</code>, was introduced\nwith GCC 14.1. It is a warning, not an error, but your build promotes all warnings to\nerrors, so it breaks your build.</p>\n<p>Although your build specifies <code>-std=c++11</code> in <code>src/3rd_party/spdlog/CMakeLists.txt</code>, which\ngenerates the failure, g++-14 emits <code>Wtemplate-id-cdtor</code> to warn you that the code <em>would be</em>\nillegal under the more recent standard c++20 (and later). Then the warning is made an error.</p>\n<p>The warning is made an error by the compile option <code>-Werror</code>. This option is included in the list\nof compile options <code>ALL_WARNINGS</code>, which is created in the top-level <code>marian/CMakeLists.txt</code>\nat line 227 <em>et seq</em>:</p>\n<pre><code># These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n</code></pre>\n<p>and then applied as compile options for the <code>marian</code> library target in <code>src/CMakeLists.txt</code>\nat line 133:</p>\n<pre><code>target_compile_options(marian PRIVATE ${ALL_WARNINGS})\n</code></pre>\n<p>whence the options are operative for the failing compilation of <code>src/CMakeFiles/marian.dir/common/logging.cpp</code>.</p>\n<p>This failure is a bug in the <code>marian</code> repo which you should <a href=\"https://github.com/marian-nmt/marian/issues\" rel=\"nofollow noreferrer\">report to the maintainers</a>, as\nit does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14.</p>\n<p>Pending a fix, you seem to have three interim options to get your build done. Either:</p>\n<ul>\n<li><p>Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:</p>\n<pre><code>/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\n</code></pre>\n</li>\n</ul>\n<p>e.g. make it <code>registry_t(const registry_t&lt;Mutex&gt;&amp;) = delete;</code> in this occurrence.</p>\n<p>Or:</p>\n<ul>\n<li><p>Locally disable <code>-Wtemplate-id-cdtor</code> at each occurrence, e.g:</p>\n<pre><code>#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored &quot;-Wtemplate-id-cdtor&quot;\nregistry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n#pragma GCC diagnostic pop\n</code></pre>\n</li>\n</ul>\n<p>Or:</p>\n<ul>\n<li>Remove <code>-Werror</code> from the <code>ALL_WARNINGS</code> list in <code>marian/CMakeLists.txt</code> so that <code>Wtemplate-id-cdtor</code> remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status).</li>\n</ul>\n<p>I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.</p>\n",
         "4.0",
         "CMakeLists.txt\n---\nset (CMAKE_CXX_STANDARD 11)\n---\ngit clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n---\n➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2",
         "Wtemplate-id-cdtor\n---\n-std=c++11\n---\nsrc/3rd_party/spdlog/CMakeLists.txt\n---\nWtemplate-id-cdtor\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\n# These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n---\nmarian\n---\nsrc/CMakeLists.txt\n---\ntarget_compile_options(marian PRIVATE ${ALL_WARNINGS})\n---\nsrc/CMakeFiles/marian.dir/common/logging.cpp\n---\nmarian\n---\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\n---\nregistry_t(const registry_t<Mutex>&) = delete;\n---\n-Wtemplate-id-cdtor\n---\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wtemplate-id-cdtor\"\nregistry_t<Mutex>(const registry_t<Mutex>&) = delete;\n#pragma GCC diagnostic pop\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\nWtemplate-id-cdtor",
         "Cant compile Marian NMT",
         "Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help",
         "The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I havent tested any of these options as Id need to go to the trouble of installing CUDA",
         "Cant compile Marian NMT Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I havent tested any of these options as Id need to go to the trouble of installing CUDA",
         "Cant compile Marian NMT Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help",
         "cant compile marian nmt im using endeavouros im trying compile marian instructions fails error message seemingly indicates conflict code c++20 files repo line steps followed result please help",
         "can not compile marian nmt I m use endeavouros I m try compile marian instruction fail error message seemingly indicate conflict code c++20 file repo line step follow result please help",
         "can not compile marian nmt I endeavouros I compile marian instruction fail error message seemingly indicate conflict c20 repo line step please help",
         "4"
        ],
        [
         "9",
         "79328514",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )\n</code></pre>\n<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>\n",
         "2025-01-04 08:57:44",
         "2",
         "34",
         "1",
         "79328698.0",
         "<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>\n<p>To modify the <strong>data collator</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>class CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])\n        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])\n        return batch\n</code></pre>\n<p>then pass it to <code>Trainer</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n</code></pre>\n<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>\n<p>Hope, it will work!</p>\n",
         "1.0",
         "input_ids\n---\nlabels\n---\ninteract_ids\n---\ncandidate_ids\n---\nclass LLMWithCustomLayer(LlamaForCausalLM)\n---\ndef forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )",
         "interact_ids\n---\ncandidate_ids\n---\nclass CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[\"interact_ids\"] = torch.tensor([f[\"interact_ids\"] for f in features])\n        batch[\"candidate_ids\"] = torch.tensor([f[\"candidate_ids\"] for f in features])\n        return batch\n---\nTrainer\n---\ntrainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(\"your-llama-model\"),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n---\nforward()\n---\ninteract_ids\n---\ncandidate_ids",
         "how to get custom column in the models forward function when training with Huggingface Trainer",
         "I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much",
         "You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "how to get custom column in the models forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "how to get custom column in the models forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much",
         "get custom column models forward function training huggingface trainer using huggingface trainer train cumstom model subclassing llama llm tokenized tokenizer dataset fields additionally add 2 custom colunms cant get custom fields forward function model new llm fine tuning anyone help would grateful much",
         "get custom column model forward function training huggingface trainer use huggingface trainer train cumstom model subclasse llama llm tokenized tokenizer dataset field additionally add 2 custom colunms can not get custom field forward function model new llm fine tuning anyone help would grateful much",
         "get custom column forward function training huggingface trainer huggingface trainer train cumstom subclasse llama llm tokenized tokenizer dataset field additionally add 2 custom colunms can not get custom field forward function new llm fine tuning anyone help would grateful much",
         "7"
        ],
        [
         "10",
         "79312133",
         "Getting all leaf words (reverse stemming) into one Python List",
         "<p>On the same lines as the solution provided <a href=\"https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword\">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>\n<p>Imagine I have a shorter sample word list as follows:</p>\n<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n</code></pre>\n<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>\n<pre><code>get_word_forms(&quot;command&quot;)\n</code></pre>\n<p>and get the following output:</p>\n<pre><code>{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n</code></pre>\n<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>\n<p>If I try to reverse-stem the entire list in one go:</p>\n<pre><code>[get_word_forms(word) for word in sample]\n</code></pre>\n<p>I fail at getting any output:</p>\n<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n</code></pre>\n<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>\n<p>something like:</p>\n<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. \n</code></pre>\n",
         "2024-12-27 15:04:05",
         "1",
         "47",
         "1",
         "79312987.0",
         "<p>One solution using nested list comprehensions after stripping forgotten spaces:</p>\n<pre><code>all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']\n</code></pre>\n",
         "1.0",
         "get_word_forms\n---\nmy_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n---\nget_word_forms(\"command\")\n---\n{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n---\n[get_word_forms(word) for word in sample]\n---\n[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n---\n['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.",
         "all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']",
         "Getting all leaf words reverse stemming into one Python List",
         "On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like",
         "One solution using nested list comprehensions after stripping forgotten spaces",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like One solution using nested list comprehensions after stripping forgotten spaces",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like",
         "getting leaf words reverse stemming one python list lines solution provided link trying get leaf words one stem word using communitycontributed srivastava package imagine shorter sample word list follows work manually following go wordbyword timeconsuming list 200 words get following output n noun adjective v verb r adverb try reversestem entire list one go fail getting output think failing saving output dictionary eventually would like output list without breaking noun adjective adverb verb something like",
         "get leaf word reverse stem one python list line solution provide link try get leaf word one stem word use communitycontributed srivastava package imagine short sample word list follow work manually follow go wordbyword timeconsuming list 200 word get follow output n noun adjective v verb r adverb try reversestem entire list one go fail get output think fail save output dictionary eventually would like output list without break noun adjective adverb verb something like",
         "get leaf reverse stem python line solution provide link get leaf stem communitycontributed srivastava package imagine short sample manually go wordbyword timeconsuming 200 get n noun adjective v verb r adverb reversestem entire go fail get think fail save dictionary eventually would like without break noun adjective adverb verb something like",
         "3"
        ],
        [
         "11",
         "79298368",
         "Inspect all probabilities of BERTopic model",
         "<p>Say I build a BERTopic model using</p>\n<pre><code>from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre>\n<p>Inspecting <code>probs</code> gives me just a single value for each item in <code>docs</code>.</p>\n<pre><code>probs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n</code></pre>\n<p>I would like the entire probability vector across all topics (so in this case, where <code>nr_topics=20</code>, I want a vector of 20 probabilities for each item in <code>docs</code>). In other words, if I have N items in <code>docs</code> and K topics, I would like an NxK output.</p>\n",
         "2024-12-20 20:49:34",
         "1",
         "52",
         "1",
         "79299703.0",
         "<p>For individual topic probability across each document you need to add one more argument.</p>\n<pre><code>topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n</code></pre>\n<p>Note: This calculate_probabilities = True will only work if you are using <strong><code>HDBSCAN</code></strong> clustering embedding model. And Bertopic by default uses <code>all-MiniLM-L6-v2</code>.</p>\n<p><strong>Official documentation:</strong> <a href=\"https://maartengr.github.io/BERTopic/api/bertopic.html\" rel=\"nofollow noreferrer\">https://maartengr.github.io/BERTopic/api/bertopic.html</a></p>\n<p>They have mentioned the same in document as well.</p>\n",
         "1.0",
         "from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n---\nprobs\n---\ndocs\n---\nprobs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n---\nnr_topics=20\n---\ndocs\n---\ndocs",
         "topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n---\nHDBSCAN\n---\nall-MiniLM-L6-v2",
         "Inspect all probabilities of BERTopic model",
         "Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output",
         "For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output",
         "inspect probabilities bertopic model say build bertopic model using inspecting gives single value item would like entire probability vector across topics case want vector 20 probabilities item words n items k topics would like nxk output",
         "inspect probability bertopic model say build bertopic model use inspecting give single value item would like entire probability vector across topic case want vector 20 probability item word n item k topic would like nxk output",
         "inspect probability bertopic say build bertopic inspecting single value item would like entire probability vector across topic case vector 20 probability item n item k topic would like nxk",
         "0"
        ],
        [
         "12",
         "79293919",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>\n<p>For example:\nA sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>\n<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>\n<p>Any and all help is welcome and appreciated.</p>\n<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>\n",
         "2024-12-19 10:24:04",
         "0",
         "63",
         "2",
         "79294074.0",
         "<p>You need a external dataset for this task. You can try dataset such as google n gram dataset.</p>\n<p>Here is the breakdown of the problem statement:</p>\n<ol>\n<li>Input: &quot;I enjoy a cold glass of water on a hot day&quot;. <code>Output</code>: &quot;water&quot;.</li>\n<li>Split the sentences into words list.</li>\n</ol>\n<blockquote>\n<p>Example: [&quot;I&quot;, &quot;enjoy&quot;, &quot;a&quot;, &quot;cold&quot;, &quot;glass&quot;, &quot;of&quot;, &quot;water&quot;, &quot;on&quot;,\n&quot;a&quot;, &quot;hot&quot;, &quot;day&quot;]</p>\n</blockquote>\n<ol start=\"3\">\n<li>First loop in through all the word of the sentences. so let say you are at first word &quot;I&quot;.</li>\n<li>Now you will look the same word &quot;I&quot; in external dataset and will look for the frequency of that word.\nLet say the word &quot;I&quot; in external dataset is repeated <code>5000000</code> times</li>\n<li>Repeat this task for all the word.</li>\n<li>Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.\nFrequency in the below example is random value not exact value.</li>\n</ol>\n<blockquote>\n<pre><code>{\n    &quot;I&quot;: 5000000,\n    &quot;enjoy&quot;: 50000,\n    &quot;a&quot;: 10000000,\n    &quot;cold&quot;: 30000,\n    &quot;glass&quot;: 100000,\n    &quot;of&quot;: 8000000,\n    &quot;water&quot;: 1200000,\n    &quot;on&quot;: 6000000,\n    &quot;hot&quot;: 700000,\n    &quot;day&quot;: 400000\n}\n</code></pre>\n</blockquote>\n<ol start=\"7\">\n<li>Pick the word with highest frequency.</li>\n</ol>\n<p>Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself</p>\n",
         "2.0",
         "",
         "Output\n---\n5000000\n---\n{\n    \"I\": 5000000,\n    \"enjoy\": 50000,\n    \"a\": 10000000,\n    \"cold\": 30000,\n    \"glass\": 100000,\n    \"of\": 8000000,\n    \"water\": 1200000,\n    \"on\": 6000000,\n    \"hot\": 700000,\n    \"day\": 400000\n}",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess",
         "You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess",
         "determining popular words english dictionary within dictionary words forgive wording awful im trying figure determine used words english language set words dictionary ive made ive done research nltk cant seem find function within library matter help need example sentence enjoy cold glass water hot day would return water used word day day conversation sentence essentially need returned value frequently used word conversations figure ill likely involve ai time ive tried use ai wind copy pasting code dont understand im trying avoid going route help welcome appreciated context decided start project would essentially guess predetermined word based characters user says doesnt computers guess",
         "determine popular word english dictionary within dictionary word forgive word awful I m try figure determine use word english language set word dictionary I ve make I ve do research nltk can not seem find function within library matter help need example sentence enjoy cold glass water hot day would return water use word day day conversation sentence essentially need return value frequently use word conversation figure ill likely involve ai time I ve try use ai wind copy paste code do not understand I m try avoid go route help welcome appreciated context decide start project would essentially guess predetermine word base character user say do not computer guess",
         "determine popular english dictionary within dictionary forgive awful I figure determine english language set dictionary I ve make I ve do research nltk can not function within library matter help enjoy cold glass water hot day would return water day day conversation essentially return value frequently conversation figure ill likely involve ai time I ve ai wind copy paste do not understand I avoid go route help welcome appreciated context decide start project would essentially guess predetermine base character user say do not computer guess",
         "1"
        ],
        [
         "13",
         "79293889",
         "catelog sentences into 5 words that represent them",
         "<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>\n<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>\n<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>\n<p>I will glad for recomendations how to do that</p>\n<p><strong>edit</strong></p>\n<p>represnt = the semantic distance between the word to the text.</p>\n<p>for example -\nlets say in row 1 the text is &quot;i want to eat&quot;\nand I have 2 words : food and house.</p>\n<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>\n",
         "2024-12-19 10:16:47",
         "0",
         "54",
         "1",
         "79294099.0",
         "<p>You could use a pre-trained sentence transformer model from <a href=\"https://pypi.org/project/sentence-transformers/\" rel=\"nofollow noreferrer\"><code>sentence_transformers</code></a>:</p>\n<pre><code>import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -&gt; None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -&gt; list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -&gt; pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838\n</code></pre>\n",
         "0.0",
         "df['text']\n---\ndf[\"word1\"]\n---\ndf[\"word2\"]\n---\ndf[\"food \"]\n---\ndf[\"house\"]",
         "sentence_transformers\n---\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -> None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -> list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -> pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n---\ntext  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838",
         "catelog sentences into 5 words that represent them",
         "I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in",
         "You could use a pretrained sentence transformer model from Output",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in You could use a pretrained sentence transformer model from Output",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in",
         "catelog sentences 5 words represent dataframe 1000 text rows also 5 words want know one much represnt text 0 1 every score etc glad recomendations edit represnt = semantic distance word text example lets say row 1 text want eat 2 words food house would higher score",
         "catelog sentence 5 word represent dataframe 1000 text row also 5 word want know one much represnt text 0 1 every score etc glad recomendation edit represnt = semantic distance word text example let say row 1 text want eat 2 word food house would high score",
         "catelog 5 represent dataframe 1000 row also 5 much represnt 0 1 every score glad recomendation edit represnt semantic distance let say row 1 eat 2 food house would high score",
         "0"
        ],
        [
         "14",
         "79253283",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>\n<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>\n<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n</code></pre>\n<p>A small Example -</p>\n<pre><code>text_dict = {\n    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]\n}\n\ndf = pd.DataFrame(text_dict)\n</code></pre>\n<p>My expected outcome is -</p>\n<pre><code>                 ITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n</code></pre>\n<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>\n<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>\n",
         "2024-12-05 03:05:06",
         "0",
         "81",
         "1",
         "79263000.0",
         "<p>You need to process each row of text by identifying occurrences of <code>key_words</code> and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique <code>search_word</code> found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique <code>search_words</code> near <code>key_words</code>.</p>\n<pre><code>import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        &quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process &quot;\n        &quot;of adopting Symantec. We believe these technologies will improve our access control. &quot;\n        &quot;Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = &quot; &quot;.join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n</code></pre>\n<p>Which returns</p>\n<pre><code>                                               ITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2\n</code></pre>\n",
         "0.0",
         "search words\n---\nkey words\n---\nsearch words\n---\nkey words\n---\nkey words\n---\nsearch words\n---\nsearch word\n---\nkey word\n---\nsearch_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n---\ntext_dict = {\n    'ITEM7':[\"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"]\n}\n\ndf = pd.DataFrame(text_dict)\n---\nITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n---\ndf\n---\nAVG\n---\nAlibaba Security\n---\nsearch_words\n---\nkey_words\n---\nSymantec\n---\nAccess Control\n---\ndata security\n---\ndiagnostic program\n---\nsearch_words\n---\nadopting\n---\nimprove\n---\nintegrated\n---\nkey_words\n---\nFrequency\n---\ndf\n---\nkey_words",
         "key_words\n---\nsearch_word\n---\nsearch_words\n---\nkey_words\n---\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        \"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \"\n        \"of adopting Symantec. We believe these technologies will improve our access control. \"\n        \"Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = \" \".join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n---\nITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also",
         "You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also",
         "counting frequency words within key words text two sets word lists first one called second one called goal calculate frequency within 10 words example assume word acquire list look words list within 10 words acquire within 10 words mean 10 words forward key words 10 words backward key words meaning forward backward movement lists small example expected outcome first row see word list around word acquired base form acquire list similarly list words within 10 words list total search words 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note words basically base form variation like adopted adopting counted key words also",
         "count frequency word within key word text two set word list first one call second one call goal calculate frequency within 10 word example assume word acquire list look word list within 10 word acquire within 10 word mean 10 word forward key word 10 word backward key word mean forward backward movement list small example expect outcome first row see word list around word acquire base form acquire list similarly list word within 10 word list total search word 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note word basically base form variation like adopt adopt count key word also",
         "count frequency within key set first call second call goal calculate frequency within 10 assume acquire within 10 acquire within 10 mean 10 forward key 10 backward key mean forward backward movement small expect outcome first row around acquire base form acquire similarly within 10 total search 6 avgalibaba securitysymantecaccess controldata securitydiagnostic program therefore column value 6 please note basically base form variation like adopt adopt count key also",
         "8"
        ],
        [
         "15",
         "79247672",
         "Error in getting Captum text explanations for text classification",
         "<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>\n<pre><code>import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f&quot;Test Accuracy: {accuracy:.2f}&quot;)\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n<p>But I am getting the following error in running the above.</p>\n<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n</code></pre>\n",
         "2024-12-03 12:47:45",
         "2",
         "84",
         "1",
         "79248379.0",
         "<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>\n<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=\"https://captum.ai/tutorials/Bert_SQUAD_Interpret\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>\n<p>Below please find snippet that works:</p>\n<pre><code>from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n",
         "1.0",
         "import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)\n---\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-f047b509c98d> in <cell line: 90>()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
         "from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    # print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)",
         "Error in getting Captum text explanations for text classification",
         "I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above",
         "You need to slightly change the gradients calculation class Also you didnt include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above You need to slightly change the gradients calculation class Also you didnt include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above",
         "error getting captum text explanations text classification following code using identify influential words used correctly predict text test dataset getting following error running",
         "error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset get follow error run",
         "error get captum explanation classification identify influential correctly predict test dataset get error run",
         "4"
        ],
        [
         "16",
         "79247594",
         "euclidian distance from word to sentence after doing Vectorizer",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did TfidfVectorizer.</p>\n<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>\n<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n",
         "2024-12-03 12:25:05",
         "1",
         "44",
         "1",
         "79248087.0",
         "<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([&quot;king&quot;]).toarray()\nexcept KeyError:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n</code></pre>\n<p>which would give you</p>\n<pre><code>                                                text  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0  \n</code></pre>\n<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif &quot;king&quot; in feature_names:\n    king_index = np.where(feature_names == &quot;king&quot;)[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n\n</code></pre>\n<p>which gives</p>\n<pre><code>                                                text  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214\n</code></pre>\n",
         "1.0",
         "",
         "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([\"king\"]).toarray()\nexcept KeyError:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n---\ntext  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0\n---\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif \"king\" in feature_names:\n    king_index = np.where(feature_names == \"king\")[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n---\ntext  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214",
         "euclidian distance from word to sentence after doing Vectorizer",
         "I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method",
         "I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method",
         "euclidian distance word sentence vectorizer dataframe 1000 text rows tfidfvectorizer want create new field give distance sentence word want lets say word king dfking thought taking sentence 5 closet words word king make average glad know hear another method",
         "euclidian distance word sentence vectorizer dataframe 1000 text row tfidfvectorizer want create new field give distance sentence word want let say word king dfke think take sentence 5 closet word word king make average glad know hear another method",
         "euclidian distance vectorizer dataframe 1000 row tfidfvectorizer create new field distance let say king dfke think take 5 closet king make average glad hear another method",
         "6"
        ],
        [
         "17",
         "79234004",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "<p>I want to use <code>Llama-3.2-1B-Instruct</code> model, and although I have set <code>&quot;temperature&quot;: 0.0, &quot;top_p&quot;:0.0 and &quot;top_k&quot;:0</code>, it still generates inconsistent output. This is how my pipeline looks like:</p>\n<pre><code>pipe = pipeline(\n    &quot;text-generation&quot;,\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=&quot;mps&quot;,\n        model_kwargs={&quot;temperature&quot;: 0.0,\n                  &quot;do_sample&quot;:True,\n                              &quot;top_p&quot;:0.0,\n                              &quot;top_k&quot;:0,},\n)\n</code></pre>\n<p>Any idea how to solve this issue?</p>\n",
         "2024-11-28 13:02:37",
         "1",
         "641",
         "2",
         "79246602.0",
         "<p>The model inconsistent output can be due to two main factors:</p>\n<p><strong>1. Temperature:</strong></p>\n<p>setting temperature to zero give more inconsistent result. You can refer <a href=\"https://community.openai.com/t/why-the-api-output-is-inconsistent-even-after-the-temperature-is-set-to-0/329541/2\" rel=\"nofollow noreferrer\">Opeani discussion page</a> for detail.</p>\n<p>So the best option is to set temperature to very low values such as 0.00001 instead of zero.</p>\n<p><strong>2. do_sample</strong></p>\n<p>You already set it false, and it should remain that way only.</p>\n",
         "1.0",
         "Llama-3.2-1B-Instruct\n---\n\"temperature\": 0.0, \"top_p\":0.0 and \"top_k\":0\n---\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"mps\",\n        model_kwargs={\"temperature\": 0.0,\n                  \"do_sample\":True,\n                              \"top_p\":0.0,\n                              \"top_k\":0,},\n)",
         "",
         "Llama321BInstruct generate inconsistent output",
         "I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue",
         "The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue",
         "llama321binstruct generate inconsistent output want use model although set still generates inconsistent output pipeline looks like idea solve issue",
         "llama321binstruct generate inconsistent output want use model although set still generate inconsistent output pipeline look like idea solve issue",
         "llama321binstruct generate inconsistent although set still generate inconsistent pipeline like idea solve issue",
         "4"
        ],
        [
         "18",
         "79192130",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>\n<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>\n",
         "2024-11-15 11:13:36",
         "1",
         "56",
         "2",
         "79192427.0",
         "<p>In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:</p>\n<ol>\n<li>Package your KeyBERT script in a lambda and easily deploy it with a container.</li>\n<li>Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.</li>\n</ol>\n",
         "1.0",
         "",
         "",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT",
         "I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks",
         "In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model Im using is resourceintensive and requires GPU acceleration Id go with SageMaker otherwise Lambda is a good solution So for your case heres what Id do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model Im using is resourceintensive and requires GPU acceleration Id go with SageMaker otherwise Lambda is a good solution So for your case heres what Id do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks",
         "using aws service execute python script extract keywords text using keybert simple python script given two blocks text extracts keywords using keybert compares lists keywords sort two lists depending lists share keywords aws service would best fit needs want able esentially spin needed give blocks text execute return results dont want integrate projects dont use python ive attempted use lambda im concerned potential cost running thanks",
         "use aws service execute python script extract keyword text use keybert simple python script give two block text extract keyword use keybert compare list keyword sort two list depend list share keyword aws service would well fit need want able esentially spin need give block text execute return result do not want integrate project do not use python I ve attempt use lambda I m concerned potential cost run thank",
         "aws service execute python script extract keyword keybert simple python script block extract keyword keybert compare keyword sort depend share keyword aws service would fit able esentially spin block execute return do not integrate project do not python I ve attempt lambda I concerned potential cost run thank",
         "8"
        ],
        [
         "19",
         "79178041",
         "Normalization of token embeddings in BERT encoder blocks",
         "<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>\n",
         "2024-11-11 14:30:31",
         "2",
         "162",
         "2",
         "79238393.0",
         "<p>I tracked down full details of layer normalization (LN) in BERT <a href=\"https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para\">here</a>.</p>\n<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>\n",
         "0.0",
         "",
         "",
         "Normalization of token embeddings in BERT encoder blocks",
         "Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings",
         "I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token its per embedding dimension",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token its per embedding dimension",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings",
         "normalization token embeddings bert encoder blocks following multiheaded attention layer bert encoder block layer normalization done separately embedding token ie one mean variance per token embedding concatenated vector token embeddings mean variance embeddings",
         "normalization token embedding bert encoder block follow multiheade attention layer bert encoder block layer normalization done separately embed token ie one mean variance per token embed concatenate vector token embedding mean variance embedding",
         "normalization token embedding bert encoder block multiheade attention layer bert encoder block layer normalization done separately embed token ie mean variance per token embed concatenate vector token embedding mean variance embedding",
         "2"
        ],
        [
         "20",
         "79173053",
         "How to convert character indices to BERT token indices",
         "<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>\n<pre><code>from datasets import load_dataset\nds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\n</code></pre>\n<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>\n<pre><code>d0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n</code></pre>\n<p>After tokenization, the answer indices are 56  and 16:</p>\n<pre><code>from transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'\n</code></pre>\n<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>\n<p>This is from a <a href=\"https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false\" rel=\"nofollow noreferrer\">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=\"https://i.sstatic.net/GsZ6mfcQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/GsZ6mfcQ.png\" alt=\"QA dataset with token answer indices\" /></a></p>\n",
         "2024-11-09 15:15:33",
         "2",
         "33",
         "1",
         "79175157.0",
         "<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>\n<p>The following function does the above for you:</p>\n<pre><code>def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start &lt;= char_start &lt; end: \n            start_token_idx = i\n        if start &lt; char_end &lt;= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n</code></pre>\n<p>Here's how you can use and test this function:</p>\n<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])\nprint(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(&quot;Tokenized answer:&quot;, answer_tokens)\n</code></pre>\n<p>Output:</p>\n<pre><code>Tokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream\n</code></pre>\n",
         "2.0",
         "UCLNLP/adversarial_qa\n---\nfrom datasets import load_dataset\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\n---\nd0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n---\nfrom transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'",
         "def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start <= char_start < end: \n            start_token_idx = i\n        if start < char_end <= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n---\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(\"Tokenized start index:\", d0_tokenized['answer_start_token_idx'])\nprint(\"Tokenized end index:\", d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(\"Tokenized answer:\", answer_tokens)\n---\nTokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream",
         "How to convert character indices to BERT token indices",
         "I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result",
         "You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Heres how you can use and test this function Output",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Heres how you can use and test this function Output",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result",
         "convert character indices bert token indices working questionanswer dataset map characterbased answer indices tokenbased indices tokenizing context question together using tokenizer like bert heres example row dataset tokenization answer indices 56 16 want create new dataset answers token indices eg 56 ad 60 linkedin learning class instructor conversion created csv file share code expected result",
         "convert character index bert token indices work questionanswer dataset map characterbase answer index tokenbase index tokenize context question together use tokenizer like bert heres example row dataset tokenization answer indice 56 16 want create new dataset answer token indices eg 56 ad 60 linkedin learn class instructor conversion create csv file share code expect result",
         "convert character index bert token indices questionanswer dataset map characterbase answer index tokenbase index tokenize context question together tokenizer like bert heres row dataset tokenization answer indice 56 16 create new dataset answer token indices eg 56 ad 60 linkedin learn class instructor conversion create csv share expect",
         "2"
        ],
        [
         "21",
         "79159805",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage?",
         "<p>I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage.</p>\n<p>I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing.</p>\n<ul>\n<li><p>Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process?</p>\n</li>\n<li><p>Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes?</p>\n</li>\n<li><p>If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects?</p>\n</li>\n</ul>\n<p>Any suggestions or examples would be greatly appreciated! Thank you!</p>\n",
         "2024-11-05 15:49:33",
         "3",
         "93",
         "2",
         "79162232.0",
         "<p>I would strongly advise you not to treat NLP models like any other Python object. I would always prefer to load an NLP model using a microservice approach, which is more aligned with ML/software engineering best practices by separating the model logic from the main application.</p>\n<p>Instead of loading the model in each process (which can be memory-intensive), the model is loaded just once in a dedicated service. This setup allows the model to be used by multiple parts of the application without duplicating memory usage, making it efficient, modular, and scalable. Not only is your concern about memory efficiency addressed, but scalability and modularity are also improved.</p>\n<p>An example of implementing such a microservice using FastAPI + Docker could look like this:</p>\n<pre><code># main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(&quot;en_core_web_lg&quot;)  # Load model once\n\n@app.post(&quot;/process/&quot;)\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {&quot;tokens&quot;: [(token.text, token.pos_) for token in doc]}\n</code></pre>\n<p>To containerize above FastAPI service:</p>\n<pre><code># Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt &amp;&amp; python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [&quot;gunicorn&quot;, &quot;-w&quot;, &quot;4&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;main:app&quot;]\n</code></pre>\n",
         "3.0",
         "",
         "# main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(\"en_core_web_lg\")  # Load model once\n\n@app.post(\"/process/\")\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {\"tokens\": [(token.text, token.pos_) for token in doc]}\n---\n# Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt && python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [\"gunicorn\", \"-w\", \"4\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"main:app\"]",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage",
         "Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you",
         "I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you",
         "share complex spacy nlp model across multiple python processes minimize memory usage im working multiprocessing python application multiple processes need access large preloaded spacy nlp model eg en_core_web_lg since model memoryintensive want avoid loading separately process since quickly run main memory object readonly instead id like load shared location processes read without duplicating memory usage looked multiprocessingmanager multiprocessingshared_memory approaches seem better suited numpy arrays raw data buffers simple objects complex objects internal references like nlp model also looked mpis mpiwinallocate_shared ran issues using redis server make rank 0 processing works mpi since processing done single rank defeats propose using multiprocessing efficient way share spacy model instance across multiple processes python avoid reloading process libraries techniques specifically suited sharing complex readonly objects like nlp models memory across processes multiprocessingmanager shared_memory viable ways improve performance reduce memory overhead working complex objects suggestions examples would greatly appreciated thank",
         "share complex spacy nlp model across multiple python process minimize memory usage I m work multiprocesse python application multiple process need access large preloade spacy nlp model eg en_core_web_lg since model memoryintensive want avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage look multiprocessingmanager multiprocessingshared_memory approach seem well suited numpy array raw datum buffer simple object complex object internal reference like nlp model also look mpis mpiwinallocate_shared run issue use redis server make rank 0 processing work mpi since process do single rank defeat propose use multiprocesse efficient way share spacy model instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp model memory across process multiprocessingmanager shared_memory viable way improve performance reduce memory overhead work complex object suggestion example would greatly appreciate thank",
         "share complex spacy nlp across multiple python process minimize memory usage I multiprocesse python application multiple process access large preloade spacy nlp eg encoreweblg since memoryintensive avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage multiprocessingmanager multiprocessingsharedmemory approach suited numpy array raw datum buffer simple object complex object internal reference like nlp also mpis mpiwinallocateshared run issue redis server make rank 0 processing mpi since process do single rank defeat propose multiprocesse efficient share spacy instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp memory across process multiprocessingmanager sharedmemory viable improve performance reduce memory overhead complex object suggestion would greatly appreciate thank",
         "8"
        ],
        [
         "22",
         "79155290",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing",
         "<p>When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data.</p>\n<p><a href=\"https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment\" rel=\"nofollow noreferrer\">https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment</a></p>\n<p>There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative.</p>\n<p><strong>Is there a way to get neutral labels for such examples in RobBERTje?</strong></p>\n<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = &quot;DTAI-KULeuven/robbert-v2-dutch-sentiment&quot;\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n</code></pre>\n<p>Output:</p>\n<pre><code>[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]\n</code></pre>\n",
         "2024-11-04 11:36:35",
         "2",
         "54",
         "1",
         "79155380.0",
         "<p>This model was trained only on <code>negative</code> and <code>positive</code> labels. Therefore, it will try to categorize every input as positive or negative, even if it is nonsensical or neutral.</p>\n<p>what you can do is to:\n1- Find other models that was trained to include <code>neutral</code> label.\n2- Fine-tune this model on a dataset that includes <code>neutral</code> label.\n3- Empirically define a threshold based on the confidence outputs and interpret it as <code>neutral</code>.</p>\n<p>The first 2 choices are extensive in effort. I would suggest you go with the third option for a quick workaround. Try feeding the model with a few neutral input and observe the range of confidence score in the output. then use that threshold to classify as <code>neutral</code>.</p>\n<p>Here's a sample:</p>\n<pre><code>def classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] &lt; threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result\n</code></pre>\n",
         "3.0",
         "from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n---\n[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]",
         "negative\n---\npositive\n---\nneutral\n---\nneutral\n---\nneutral\n---\nneutral\n---\ndef classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] < threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing",
         "When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output",
         "This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Heres a sample",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Heres a sample",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output",
         "dutch sentiment analysis robbertje outputs positive/negative labels netural label missing run dutch sentiment analysis robbertje outputs positive/negative labels netural label missing data obvious neutral sentences/words eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral labels examples robbertje output",
         "dutch sentiment analysis robbertje outputs positive / negative label netural label miss run dutch sentiment analysis robbertje outputs positive / negative label netural label miss datum obvious neutral sentence / word eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral label example robbertje output",
         "dutch sentiment analysis robbertje outputs positive negative label netural label miss run dutch sentiment analysis robbertje outputs positive negative label netural label miss datum obvious neutral eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative get neutral label robbertje",
         "0"
        ],
        [
         "23",
         "79148979",
         "Finding Root Form of Verbs using Curiosity-AI/Catalyst",
         "<p>I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match <code>PartOfSpeech.VERB</code> but I don't know how to continue from there.</p>\n<p>This is what I have so far:</p>\n<pre><code>const string text = &quot;The disastrous cat runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}\n</code></pre>\n<p>Any help is greatly appreciated.</p>\n",
         "2024-11-01 17:58:01",
         "2",
         "135",
         "1",
         "79160163.0",
         "<p>The following code (targeting .NET 8.0) illustrates one method to obtain the root form of a verb from an inflected form.</p>\n<p>(I have annonoted, as code comments, the three NuGet packages (with versions) required. Most of the code is identical to your original sample above.)</p>\n<pre><code>//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = &quot;The disastrous cat quickly runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($&quot;Root of the verb '{token.Value}' is '{token.Lemma}'.&quot;);\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(&quot;Complete; press any key.&quot;);\nConsole.ReadKey();\n</code></pre>\n<p><strong>Note:</strong> For this specific sentence, I have added an adverb (&quot;quickly&quot;) before the verb (&quot;runs&quot;). Without this, the library incorrectly interprets &quot;runs&quot; as a noun. Depending on your source text, this might be an issue for you, but I believe it is separate from the question being asked.</p>\n",
         "1.0",
         "PartOfSpeech.VERB\n---\nconst string text = \"The disastrous cat runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}",
         "//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = \"The disastrous cat quickly runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($\"Root of the verb '{token.Value}' is '{token.Lemma}'.\");\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(\"Complete; press any key.\");\nConsole.ReadKey();",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst",
         "Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated",
         "The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated",
         "finding root form verbs using curiosityai/catalyst im trying find root form verb run text pipeline identify tokens match dont know continue far help greatly appreciated",
         "find root form verb use curiosityai / catalyst I m try find root form verb run text pipeline identify tokens match do not know continue far help greatly appreciate",
         "root form verb curiosityai catalyst I root form verb run pipeline identify tokens match do not continue far help greatly appreciate",
         "5"
        ],
        [
         "24",
         "79145419",
         "Is it possible to get embeddings from NV-Embed using Candle?",
         "<p>What I want to do is a CLI program that outputs embeddings of an arbitrary input.\nTo do that, I want to do an inference with an embeddings model, and I chose <code>NV-Embed-v2</code>. My framework of choice is <a href=\"https://github.com/huggingface/candle\" rel=\"nofollow noreferrer\">Candle</a>, but I also looked at <a href=\"https://github.com/EricLBuehler/mistral.rs\" rel=\"nofollow noreferrer\">Mistral-RS</a>.</p>\n<p>Basically, what I'm trying to do is this code fragment:\n<a href=\"https://huggingface.co/nvidia/NV-Embed-v2\" rel=\"nofollow noreferrer\">https://huggingface.co/nvidia/NV-Embed-v2</a>\nbut with Rust and Candle.</p>\n<p>What I tried is to start off with <a href=\"https://github.com/huggingface/candle/blob/main/candle-examples/examples/mistral/main.rs\" rel=\"nofollow noreferrer\">Mistral Candle's example</a> because the NV-Embed's HF page says: <code>Model Details / Base Decoder-only LLM: Mistral-7B-v0.1</code>.</p>\n<p>I replaced the model id in the original code with <code>nvidia/NV-Embed-v2</code>, and was able to download the weights from Hugging Face, but upon loading the config, I got this:</p>\n<pre><code>Error: missing field `vocab_size` at line 101 column 1\n</code></pre>\n<p>Then I hardcoded the values from the JSON config loaded from HF to a newly created <code>candle_transformers::models::mistral::Config</code> instance. And after that, <code>Mistral::new(&amp;config, vb)</code> fails with:</p>\n<pre><code>Error: cannot find tensor model.embed_tokens.weight\n</code></pre>\n<p>Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?</p>\n",
         "2024-10-31 15:55:49",
         "0",
         "329",
         "1",
         "79156470.0",
         "<p>candle looking for <code>model.embed_tokens.weight</code> whereas the original tensor name is <code>embedding_model.embed_tokens.weight</code>. You just have to change this line of <code>mistral.rs</code> in candle_transformers.</p>\n<pre class=\"lang-rust prettyprint-override\"><code>// from\nlet vb_m = vb.pp(&quot;model&quot;);\n//to\nlet vb_m = vb.pp(&quot;embedding_model&quot;);\n</code></pre>\n",
         "2.0",
         "NV-Embed-v2\n---\nModel Details / Base Decoder-only LLM: Mistral-7B-v0.1\n---\nnvidia/NV-Embed-v2\n---\nError: missing field `vocab_size` at line 101 column 1\n---\ncandle_transformers::models::mistral::Config\n---\nMistral::new(&config, vb)\n---\nError: cannot find tensor model.embed_tokens.weight",
         "model.embed_tokens.weight\n---\nembedding_model.embed_tokens.weight\n---\nmistral.rs\n---\n// from\nlet vb_m = vb.pp(\"model\");\n//to\nlet vb_m = vb.pp(\"embedding_model\");",
         "Is it possible to get embeddings from NVEmbed using Candle",
         "What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed",
         "candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed",
         "possible get embeddings nvembed using candle want cli program outputs embeddings arbitrary input want inference embeddings model chose framework choice candle also looked mistralrs basically im trying code fragment rust candle tried start mistral candles example nvembeds hf page says replaced model id original code able download weights hugging face upon loading config got hardcoded values json config loaded hf newly created instance fails way around maybe candlebased open source works could use inspiration maybe thats common mistake could easily diagnosed",
         "possible get embedding nvembe use candle want cli program output embedding arbitrary input want inference embedding model choose framework choice candle also look mistralrs basically I m try code fragment rust candle try start mistral candle example nvembed hf page say replace model i d original code able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail way around maybe candlebase open source work could use inspiration maybe that s common mistake could easily diagnose",
         "possible get embedding nvembe candle cli program embedding arbitrary input inference embedding choose framework choice candle also mistralrs basically I fragment rust candle start mistral candle nvembed hf page say replace i d original able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail around maybe candlebase open source could inspiration maybe that s common mistake could easily diagnose",
         "2"
        ],
        [
         "25",
         "79111733",
         "How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)",
         "<p>How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)</p>\n<p>I have short product descriptions that I’d like to transform into structured attributes.</p>\n<p>Example:</p>\n<p>Input:</p>\n<pre><code>“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n</code></pre>\n<p>Output:</p>\n<pre><code>Year = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml\n</code></pre>\n<p>If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:</p>\n<ol>\n<li><p>There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.</p>\n</li>\n<li><p>There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.</p>\n</li>\n<li><p>Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.</p>\n</li>\n<li><p>I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.</p>\n</li>\n</ol>\n<p>I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.</p>\n<p>I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.</p>\n<p>Appreciate any insight into approaches or suggested learning resources.</p>\n<p></p>\n<p>I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.</p>\n",
         "2024-10-21 20:54:56",
         "0",
         "166",
         "1",
         "79113907.0",
         "<p>LLM would work nicely for this.  I'v done similar tasks before and it worked nicely with minimal training.  Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate,  but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions.</p>\n<p>For you task I would use a framework like Langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  When run with a model it will create an XML output which would be trivial to parse.  You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me.</p>\n<pre><code>You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like &quot;Red&quot; or &quot;White&quot; (e.g., &quot;Red Head Vineyard&quot;) which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., &quot;1500 ml&quot;) rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n&lt;Wine&gt;\n&lt;Year&gt;{{Year}}&lt;/Year&gt;\n&lt;Color&gt;{{Color}}&lt;/Color&gt;\n&lt;Weight&gt;{{Weight}}&lt;/Weight&gt;\n&lt;WeightUnit&gt;{{WeightUnit}}&lt;/WeightUnit&gt;\n&lt;Brand&gt;{{Brand}}&lt;/Brand&gt;\n&lt;GrapeVariety&gt;{{GrapeVariety}}&lt;/GrapeVariety&gt;\n&lt;/Wine&gt;\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   &lt;Wine&gt;\n     &lt;Year&gt;2017&lt;/Year&gt;\n     &lt;Color&gt;Red&lt;/Color&gt;\n     &lt;Weight&gt;750&lt;/Weight&gt;\n     &lt;WeightUnit&gt;ml&lt;/WeightUnit&gt;\n     &lt;Brand&gt;La Lecciaia&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Cabernet Sauvignon&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   &lt;Wine&gt;\n     &lt;Year&gt;2020&lt;/Year&gt;\n     &lt;Color&gt;&lt;/Color&gt;\n     &lt;Weight&gt;1.5&lt;/Weight&gt;\n     &lt;WeightUnit&gt;L&lt;/WeightUnit&gt;\n     &lt;Brand&gt;Red Head Vineyard&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Chardonnay&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}\n</code></pre>\n<p>Keep in mind that LLMs are not cheap to run.  But for this tasks given ambiguousness of the domain it is most likely the best choice.  For this particular task it would be 1/1000 of a penny per label using OpenAI service.  You might find a cheaper model / provider.  However when working with LLM it is very important to ensure accuracy first,  then optimize for costs.</p>\n<p>The whole thing will probably take 1-2 hours to build for the intermediate LLM developer.  If you are learning it may vary.  But this is a perfect project to learn about LLMs</p>\n",
         "1.0",
         "“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n---\nYear = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml",
         "You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like \"Red\" or \"White\" (e.g., \"Red Head Vineyard\") which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., \"1500 ml\") rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n<Wine>\n<Year>{{Year}}</Year>\n<Color>{{Color}}</Color>\n<Weight>{{Weight}}</Weight>\n<WeightUnit>{{WeightUnit}}</WeightUnit>\n<Brand>{{Brand}}</Brand>\n<GrapeVariety>{{GrapeVariety}}</GrapeVariety>\n</Wine>\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   <Wine>\n     <Year>2017</Year>\n     <Color>Red</Color>\n     <Weight>750</Weight>\n     <WeightUnit>ml</WeightUnit>\n     <Brand>La Lecciaia</Brand>\n     <GrapeVariety>Cabernet Sauvignon</GrapeVariety>\n   </Wine>\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   <Wine>\n     <Year>2020</Year>\n     <Color></Color>\n     <Weight>1.5</Weight>\n     <WeightUnit>L</WeightUnit>\n     <Brand>Red Head Vineyard</Brand>\n     <GrapeVariety>Chardonnay</GrapeVariety>\n   </Wine>\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}",
         "How to derive attributes/labels from short plain text descriptions NER LLM",
         "How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense",
         "LLM would work nicely for this Iv done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense LLM would work nicely for this Iv done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense",
         "derive attributes/labels short plain text descriptions ner llm derive attributes/labels short plain text descriptions ner llm short product descriptions id like transform structured attributes example input output everything format would trivial write regular expression done many different formats nuances increasingly cumbersome hardcode logic format trying create generic solution immediately run issues basic approach several different data providers format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even given provider may multiple formats may change time formats always strictly followed many ways expressing particular components example weight might expressed one 15l 1 1/2 liters 1500ml etc parts description may confused target components may white wine brand called red head vineyard weight 2000 ml may confused year etc im using wine examples sake simplicity general audience product domain conceptual issues id consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work harder get right theres straightforward method would good know id like develop generalpurpose function accept description format little experience nlp/artificial intelligence suspect useful tools/algos leverage 1000+ example records could potentially use train model something run locally would preferred necessary im looking specific implementation guidance anyone whos worked similar problem open hybrid approaches additional logic manual oversight could account initial inaccuracies appreciate insight approaches suggested learning resources ive looked online information many approaches involve significant amount front work unclear theyll work practical sense",
         "derive attribute / label short plain text description ner llm derive attribute / label short plain text description ner llm short product description i d like transform structure attribute example input output everything format would trivial write regular expression do many different format nuance increasingly cumbersome hardcode logic format try create generic solution immediately run issue basic approach several different datum provider format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even give provider may multiple format may change time format always strictly follow many way express particular component example weight might express one 15l 1 1/2 liter 1500ml etc part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year etc I m use wine example sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work hard get right there s straightforward method would good know I d like develop generalpurpose function accept description format little experience nlp / artificial intelligence suspect useful tool / algo leverage 1000 + example record could potentially use train model something run locally would prefer necessary I m look specific implementation guidance anyone who s work similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource I ve look online information many approach involve significant amount front work unclear they ll work practical sense",
         "derive attribute label short plain description ner llm derive attribute label short plain description ner llm short product description i d like transform structure attribute input everything format would trivial write regular expression do many format nuance increasingly cumbersome hardcode logic format create generic solution immediately run issue basic approach several datum provider format another provider might red 2017 la lecciaia cabernet sauvignon 750 ml even provider may multiple format may change time format always strictly many express particular component weight might express 15l 1 12 liter 1500ml part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year I wine sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough la lecciaia brand cabernet sauvignon grape variety assuming would take front hard get right there s straightforward method would good I d like develop generalpurpose function accept description format little experience nlp artificial intelligence suspect useful tool algo leverage 1000 record could potentially train something run locally would prefer necessary I specific implementation guidance anyone who s similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource I ve online information many approach involve significant amount front unclear they ll practical sense",
         "1"
        ],
        [
         "26",
         "79102797",
         "Varying embedding dim due to changing padding in batch size",
         "<p>I want to train a simple neural network, which has <strong>embedding_dim</strong> as a parameter:</p>\n<pre><code>class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n</code></pre>\n<p>To load the data I used torchs DataLoader with a custom collate_fn.</p>\n<pre><code>train_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n</code></pre>\n<p>The collate_fn_padd function looks the following:</p>\n<pre><code>def collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels\n\n</code></pre>\n<p><strong>The problem:</strong> For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch).</p>\n<p>That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch.</p>\n<p>Due to that, I receive errors like that: <strong>mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64)</strong></p>\n<p>Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?</p>\n",
         "2024-10-18 15:54:51",
         "0",
         "42",
         "1",
         "79105117.0",
         "<p>You can add a maximum length argument set to <code>embedding_dim</code> to pad and truncate all the data to a fixed length:</p>\n<pre><code>padded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]\n</code></pre>\n",
         "1.0",
         "class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n---\ntrain_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n---\ndef collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels",
         "embedding_dim\n---\npadded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]",
         "Varying embedding dim due to changing padding in batch size",
         "I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size",
         "You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size",
         "varying embedding dim due changing padding batch size want train simple neural network embedding_dim parameter load data used torchs dataloader custom collate_fn collate_fn_padd function looks following problem every batch want train model embedded text gets padded differently long takes longest sequence current batch means embedding dim/input size linear layer neural network changes batch batch althoug want size every batch due receive errors like mat1 mat2 shapes multiplied 16x182 301x64 possible adjust collate_fn_pad function padds sequence size independet batch size",
         "vary embed dim due change padding batch size want train simple neural network embedding_dim parameter load datum use torchs dataloader custom collate_fn collate_fn_padd function look follow problem every batch want train model embed text get pad differently long take long sequence current batch mean embed dim / input size linear layer neural network change batch batch althoug want size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collate_fn_pad function padd sequence size independet batch size",
         "vary embed dim due change padding batch size train simple neural network embeddingdim parameter load datum torchs dataloader custom collatefn collatefnpadd function problem every batch train embed get pad differently long take long sequence current batch mean embed dim input size linear layer neural network change batch batch althoug size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collatefnpad function padd sequence size independet batch size",
         "7"
        ],
        [
         "27",
         "79100835",
         "How can I adjust the performance of tokenizer?",
         "<p>Working with the tokenizer from the <code>transformers</code> library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not.</p>\n<p>I'm wondering if I can <strong>&quot;adjust&quot;</strong> (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to.</p>\n<p>To be more specific, the type of tokenizer is <code>transformers.XLMRobertaTokenizerFast</code>, which is a unigram tokenizer, and the model is <code>paraphrase-multilingual-mpnet-base-v2</code>.</p>\n",
         "2024-10-18 06:45:15",
         "0",
         "45",
         "1",
         "79107575.0",
         "<p>You can change the tokenizer's vocabulary:</p>\n<pre><code>tokenizer.add_tokens([&quot;asadaf&quot;, &quot;sdfsaf&quot;])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = &quot;This is asadaf and sdfsaf&quot;\nprint(tokenizer(input_text))\n</code></pre>\n<p>As a result, <em>asadaf</em> and <em>sdfsaf</em> would be tokenized as unique words.</p>\n",
         "1.0",
         "transformers\n---\ntransformers.XLMRobertaTokenizerFast\n---\nparaphrase-multilingual-mpnet-base-v2",
         "tokenizer.add_tokens([\"asadaf\", \"sdfsaf\"])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = \"This is asadaf and sdfsaf\"\nprint(tokenizer(input_text))",
         "How can I adjust the performance of tokenizer",
         "Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is",
         "You can change the tokenizers vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is You can change the tokenizers vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is",
         "adjust performance tokenizer working tokenizer library hugging face tokenizer works fine cases cases im wondering adjust train new tokenizer scratch performance tokenizer handle bad cases still maintaining good performance cases used specific type tokenizer unigram tokenizer model",
         "adjust performance tokenizer working tokenizer library hug face tokenizer work fine case case I m wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case use specific type tokenizer unigram tokenizer model",
         "adjust performance tokenizer working tokenizer library hug face tokenizer fine case case I wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case specific type tokenizer unigram tokenizer",
         "2"
        ],
        [
         "28",
         "79081924",
         "With spaCy, how can I get all lemmas from a string?",
         "<p>I have a pandas data frame with a column of text values (documents).  I want to apply lemmatization on these values with the spaCy library using the pandas <code>apply</code> function.  I've defined my <code>to_lemma</code> function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow.  Is there a way to extract the lemmatized form of a document in spaCy?</p>\n<pre><code>def to_lemma(text):\n    tp = nlp(text)\n    line = &quot;&quot;\n    for word in tp:\n        line = line + word.lemma_ + &quot; &quot;\n    return line\n</code></pre>\n",
         "2024-10-12 21:03:21",
         "-1",
         "97",
         "2",
         "79086290.0",
         "<p>There are many ways to speed up SpaCy processing. The question which of them make sense for you depends mostly on the size of your input.</p>\n<ol>\n<li>The most obvious one is not individually apply the model to every single row, but rather use batch processing. Use <code>nlp.pipe()</code> with an Iterable of strings. This means it is easier to not use apply.</li>\n<li>Disable components that you do not use. For token level processing where you need the lemmas this would be <code>'parser'</code> (the dependency parser) and <code>'ner'</code> (the Named Entity Recognition component).</li>\n<li>Increase the <code>batch_size</code> (objects to buffer) in pipe(). The default is 1000. Obviously this only makes sense to touch if you have the memory to increase it a lot.</li>\n<li>Increase the number of processors used using <code>n_process</code>. This will increase the time it takes to initially load the model but decrease the processing time. In my experience this starts making sense at about 500k+ texts. Note that this also requires the code to be run in an <code>if __name__ == '__main__':</code> wrapper.</li>\n</ol>\n<p>Basic example with 1. and 2.:</p>\n<pre><code>texts = df[&quot;column_name&quot;]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\ndf[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n<p>Advanced example for all four:</p>\n<pre><code>if __name__ == '__main__':\n    texts = df[&quot;column_name&quot;]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\n    df[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n",
         "2.0",
         "apply\n---\nto_lemma\n---\ndef to_lemma(text):\n    tp = nlp(text)\n    line = \"\"\n    for word in tp:\n        line = line + word.lemma_ + \" \"\n    return line",
         "nlp.pipe()\n---\n'parser'\n---\n'ner'\n---\nbatch_size\n---\nn_process\n---\nif __name__ == '__main__':\n---\ntexts = df[\"column_name\"]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\ndf[\"column_name_lemmas\"] = lemmas\n---\nif __name__ == '__main__':\n    texts = df[\"column_name\"]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\n    df[\"column_name_lemmas\"] = lemmas",
         "With spaCy how can I get all lemmas from a string",
         "I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy",
         "There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy",
         "spacy get lemmas string pandas data frame column text values documents want apply lemmatization values spacy library using pandas function ive defined function iterate words document concatenate corresponding lemmas output string however slow way extract lemmatized form document spacy",
         "spacy get lemmas string panda datum frame column text value document want apply lemmatization value spacy library use panda function I ve define function iterate word document concatenate correspond lemmas output string however slow way extract lemmatize form document spacy",
         "spacy get lemmas panda datum frame column value document apply lemmatization value spacy library panda function I ve define function iterate document concatenate correspond lemmas however slow extract lemmatize form document spacy",
         "6"
        ],
        [
         "29",
         "79057082",
         "Avoiding overlap in frequency and document frequency count in Quanteda",
         "<p>Below is a dummy corpus of 4 documents.</p>\n<p>The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in.</p>\n<p>The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive.</p>\n<p>Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each,\nbut are intended to be counted once, according to the dictionary.</p>\n<p>The expected overall frequency count is (extracted from the 'pattern&quot; column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key.</p>\n<p>Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key.</p>\n<p>I have three questions:</p>\n<ol>\n<li>are there any problems/issues that could affect output accuracy using this approach?</li>\n<li>is there a better/more parsimonius approach to achieve what I am trying to do?</li>\n<li>what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table?</li>\n</ol>\n<pre><code>        library (quanteda)\n        library(quanteda.textstats)\n\n        txt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\n\n\n         x &lt;- x %&gt;%\n         tokens_compound(phrase(&quot;dairy farmers&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farms&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farm&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farming&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy industry&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;indigenous australians&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;australia post&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farmer&quot;), concatenator = &quot; &quot;)\n              x\n\n         dict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign- \n         owned&quot;, &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, &quot;multinational \n         oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n         dairy = c(&quot;dairy farmers&quot;, &quot;dairy farms&quot;,&quot;dairy farm&quot;,&quot;dairy farming&quot;,&quot;dairy \n         industry&quot;, &quot;dairy farmer&quot;,&quot;dairy&quot;, &quot;milk&quot;),\n         auspost = &quot;australia post&quot;,\n         oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n         farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n         foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n         business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n         indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres \n         strait&quot;),\n         peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, &quot;people of \n         this nation&quot;, &quot;people of this country&quot;),\n         industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\n        kwicdict &lt;- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, &quot;D:/Output/TEST.csv&quot;)\n\n       DF &lt;- read.csv(&quot;D://Output/TEST.csv&quot;,header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       &gt; x2 &lt;- DF[,8]\n       &gt; \n       &gt; table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2 \n</code></pre>\n",
         "2024-10-05 12:43:52",
         "1",
         "57",
         "1",
         "79058791.0",
         "<p>I don't think that <code>kwic()</code> is what you want here. <code>tokens_lookup()</code> lets you specify that the nested scope should be mutually exclusive across keys, not just within keys. Observe the difference below. (And note the use of wildcarding for dairy key.)</p>\n<pre class=\"lang-r prettyprint-override\"><code>library(quanteda)\n#&gt; Package version: 4.1.0\n#&gt; Unicode version: 14.0\n#&gt; ICU version: 71.1\n#&gt; Parallel computing: 10 of 10 threads used.\n#&gt; See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\ndict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign-owned&quot;, \n                                     &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, \n                                     &quot;multinational oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n                        dairy = c(&quot;dairy farm*&quot;, &quot;dairy industry&quot;, &quot;dairy&quot;, &quot;milk&quot;),\n                        auspost = &quot;australia post&quot;,\n                        oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n                        farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n                        foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n                        business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n                        indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres strait&quot;),\n                        peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, \n                                 &quot;people of this nation&quot;, &quot;people of this country&quot;),\n                        industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\nx &lt;- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       5       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        1\n#&gt;   doc3        1     1       0  0       1       0        1     1    1        0\n#&gt;   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = &quot;dictionary&quot;) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       3       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        0\n#&gt;   doc3        1     1       0  0       1       0        1     1    0        0\n#&gt;   doc4        0     0       1  1       1       1        0     0    0        0\n</code></pre>\n<p><sup>Created on 2024-10-06 with <a href=\"https://reprex.tidyverse.org\" rel=\"nofollow noreferrer\">reprex v2.1.1</a></sup></p>\n",
         "0.0",
         "library (quanteda)\n        library(quanteda.textstats)\n\n        txt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\n\n\n         x <- x %>%\n         tokens_compound(phrase(\"dairy farmers\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farms\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farm\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farming\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy industry\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"indigenous australians\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"australia post\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farmer\"), concatenator = \" \")\n              x\n\n         dict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign- \n         owned\", \"foreign owned\", \"foreign companies\", \"multinational\", \"multinational \n         oil companies\", \"multinationals\", \"transnational\"),\n         dairy = c(\"dairy farmers\", \"dairy farms\",\"dairy farm\",\"dairy farming\",\"dairy \n         industry\", \"dairy farmer\",\"dairy\", \"milk\"),\n         auspost = \"australia post\",\n         oz = c(\"australia\", \"this country\", \"our country\"),\n         farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n         foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n         business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n         indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres \n         strait\"),\n         peep = c(\"australians\", \"people of australia\", \"australian people\", \"people of \n         this nation\", \"people of this country\"),\n         industry = c(\"industry\", \"industries\")))\n\n        kwicdict <- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, \"D:/Output/TEST.csv\")\n\n       DF <- read.csv(\"D://Output/TEST.csv\",header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       > x2 <- DF[,8]\n       > \n       > table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2",
         "kwic()\n---\ntokens_lookup()\n---\nlibrary(quanteda)\n#> Package version: 4.1.0\n#> Unicode version: 14.0\n#> ICU version: 71.1\n#> Parallel computing: 10 of 10 threads used.\n#> See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\ndict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign-owned\", \n                                     \"foreign owned\", \"foreign companies\", \"multinational\", \n                                     \"multinational oil companies\", \"multinationals\", \"transnational\"),\n                        dairy = c(\"dairy farm*\", \"dairy industry\", \"dairy\", \"milk\"),\n                        auspost = \"australia post\",\n                        oz = c(\"australia\", \"this country\", \"our country\"),\n                        farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n                        foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n                        business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n                        indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres strait\"),\n                        peep = c(\"australians\", \"people of australia\", \"australian people\", \n                                 \"people of this nation\", \"people of this country\"),\n                        industry = c(\"industry\", \"industries\")))\n\nx <- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       5       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        1\n#>   doc3        1     1       0  0       1       0        1     1    1        0\n#>   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = \"dictionary\") |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       3       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        0\n#>   doc3        1     1       0  0       1       0        1     1    0        0\n#>   doc4        0     0       1  1       1       1        0     0    0        0",
         "Avoiding overlap in frequency and document frequency count in Quanteda",
         "Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table",
         "I dont think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table I dont think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table",
         "avoiding overlap frequency document frequency count quanteda dummy corpus 4 documents dictionary developed identify frequency words phrases corpus well number documents word phrases occurs world australians occurs two dictionary keys peep indig key content intended mutually exclusive similarly australia oz australia post foreign foreign multinat farm/farmers dairy farmers occur two dictionary keys intended counted according dictionary expected overall frequency count extracted pattern column kwic table reported x2 note word industry appears allocated industry define din indig key dairy frequency occuring key occuring three documents calculated unique rows kwic table doc names column key three questions problems/issues could affect output accuracy using approach better/more parsimonius approach achieve trying would best way extract equivalent tetxstat frequency count data kwic table",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency word phrase corpus well number document word phrase occur world australian occur two dictionary key peep indig key content intend mutually exclusive similarly australia oz australia post foreign foreign multinat farm / farmer dairy farmer occur two dictionary key intend count accord dictionary expect overall frequency count extract pattern column kwic table report x2 note word industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table doc name column key three question problem / issue could affect output accuracy use approach well / more parsimonius approach achieve trying would well way extract equivalent tetxstat frequency count datum kwic table",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency phrase corpus number document phrase occur world australian occur dictionary key peep indig key content intend mutually exclusive similarly australia oz australia post foreign foreign multinat farm farmer dairy farmer occur dictionary key intend count accord dictionary expect overall frequency count extract pattern column kwic table report x2 note industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table doc name column key three question problem issue could affect accuracy approach more parsimonius approach achieve trying would extract equivalent tetxstat frequency count datum kwic table",
         "8"
        ],
        [
         "30",
         "79005985",
         "Seq2Seq trainer.train() keeps giving indexing error",
         "<p>I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error:</p>\n<blockquote>\n<p>IndexError: Invalid key: 39463 is out of bounds for size 0.</p>\n</blockquote>\n<ul>\n<li>The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B</li>\n<li>The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also.</li>\n</ul>\n<p>Detailed error message:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;nllbtrain.py&quot;, line 273, in &lt;module&gt;\n    print(trainer.train())\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1645, in train\n    return inner_training_loop(\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 631, in __next__\n    data = self._next_data()\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2810, in __getitem__\n    return self._getitem(key)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 583, in query_table\n    _check_valid_index_key(key, size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 526, in _check_valid_index_key\n    raise IndexError(f&quot;Invalid key: {key} is out of bounds for size {size}&quot;)\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n</code></pre>\n<p>The code of the preprocessing done for the data:</p>\n<pre><code>def preprocess_function(examples):\n        inputs = [example + ' &lt;/s&gt;' + f' &lt;2{s_lang}&gt;' for example in examples[source_lang]]\n        targets = [f'&lt;2{t_lang}&gt; ' + example + ' &lt;/s&gt;' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n</code></pre>\n<p>Data after preprocessing:</p>\n<pre><code>DatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n</code></pre>\n<p>The code of model params and training:</p>\n<pre><code>model_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=&quot;hin_Deva&quot;, tgt_lang=&quot;san_Deva&quot;, max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=&quot;epoch&quot;,\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=&quot;./output_dir&quot;,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())\n\n</code></pre>\n<p>Any idea why this error is persisting?</p>\n",
         "2024-09-20 08:43:32",
         "0",
         "54",
         "1",
         "79007590.0",
         "<p><code>size 0</code> indicates that the dataset your trainer gets when the fine-tuning starts is empty. Looking at this (<a href=\"https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25</a>) and this (<a href=\"https://github.com/huggingface/datasets/issues/6535\" rel=\"nofollow noreferrer\">https://github.com/huggingface/datasets/issues/6535</a>) thread suggests adding <code>remove_unused_columns = False</code> to your <code>training_args</code> might resolve the issue, so you could give that a try.</p>\n",
         "0.0",
         "Traceback (most recent call last):\n  File \"nllbtrain.py\", line 273, in <module>\n    print(trainer.train())\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2810, in __getitem__\n    return self._getitem(key)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\n    _check_valid_index_key(key, size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n---\ndef preprocess_function(examples):\n        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]\n        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n---\nDatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n---\nmodel_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=\"hin_Deva\", tgt_lang=\"san_Deva\", max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=\"epoch\",\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=\"./output_dir\",\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())",
         "size 0\n---\nremove_unused_columns = False\n---\ntraining_args",
         "Seq2Seq trainertrain keeps giving indexing error",
         "I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting",
         "indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting",
         "seq2seq trainertrain keeps giving indexing error trying machine translation hindi sanskrit using nllb model keep getting error indexerror invalid key 39463 bounds size 0 error coming training pretrained nllb model ` facebook/nllb20013b input data ~40k hindi sentences error arises tried training sample data also detailed error message code preprocessing done data data preprocessing code model params training idea error persisting",
         "seq2seq trainertrain keep give indexing error try machine translation hindi sanskrit use nllb model keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb model ` facebook / nllb20013b input datum ~40k hindi sentence error arises try train sample datum also detail error message code preprocessing do data datum preprocesse code model param train idea error persist",
         "seq2seq trainertrain keep indexing error machine translation hindi sanskrit nllb keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb facebook nllb20013b input datum 40k hindi error arises train sample datum also detail error message preprocessing do data datum preprocesse param train idea error persist",
         "7"
        ],
        [
         "31",
         "78985137",
         "Alternative to device_map = \"auto\" in Huggingface Pretrained",
         "<p>I have a model that I was reading from huggingface using the following code:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n</code></pre>\n<p>Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model.</p>\n<p>Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like <code>device_map=&quot;auto&quot;</code> but after reading the model.</p>\n<p>So simply something like</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\nmodel.device_map = &quot;auto&quot;\n</code></pre>\n",
         "2024-09-14 12:42:03",
         "2",
         "1034",
         "1",
         "79007343.0",
         "<p>I found out that there are actually several methods in <code>accelerate</code> for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map</a></p>\n<p>The second one is used to match your model with the devices:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model</a></p>\n<p>So basically, in your case, you can use the following code:</p>\n<pre><code>from accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)\n</code></pre>\n<p>P.S. This code still needs to be tested on fine-tuning.</p>\n",
         "2.0",
         "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n---\ndevice_map=\"auto\"\n---\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\nmodel.device_map = \"auto\"",
         "accelerate\n---\nfrom accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)",
         "Alternative to device_map = auto in Huggingface Pretrained",
         "I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like",
         "I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like",
         "alternative device_map = auto huggingface pretrained model reading huggingface using following code read model modifications internal layers added layers started training/finetuning get everything model investigations found custom layers arent distributed multi gpus original model need something like reading model simply something like",
         "alternative device_map = auto huggingface pretraine model reading huggingface use follow code read model modification internal layer add layer start training / finetune get everything model investigation find custom layer be not distribute multi gpus original model need something like read model simply something like",
         "alternative devicemap auto huggingface pretraine reading huggingface read modification internal layer add layer start training finetune get everything investigation custom layer be not distribute multi gpus original something like read simply something like",
         "7"
        ],
        [
         "32",
         "78966943",
         "How are the weights of the Mistral models reinitialized in Huggingface?",
         "<p>From <a href=\"https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic\">How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model?</a> and <a href=\"https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4</a> there's different suggestions to reinitialize the model.</p>\n<p>When I tried this, it seems to work.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;, token=&quot;hf_*****&quot;)\n\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n</code></pre>\n<p>[out]:</p>\n<pre><code>OrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n</code></pre>\n<p>How are the layers re-initialized through the <code>from_config</code> function? Is it using <a href=\"https://cs230.stanford.edu/section/4/\" rel=\"nofollow noreferrer\">Xaiver/He initialization</a> or just random initialization?</p>\n",
         "2024-09-09 19:25:52",
         "3",
         "175",
         "2",
         "78969695.0",
         "<p><a href=\"https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralConfig\" rel=\"nofollow noreferrer\">MistralConfig</a> has a default parameter <code>initializer_range</code> which is set to 0.02 and described as <code>The standard deviation of the truncated_normal_initializer for initializing all weight matrices</code>, so one can assume they use a truncated normal distribution with a standard deviation of 0.02.</p>\n<p>If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 0.02 looks like, it seems like a fit to me:</p>\n<pre><code>import numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/M67S0rKp.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/M67S0rKp.png\" alt=\"model_weights\" /></a></p>\n",
         "2.0",
         "from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", token=\"hf_*****\")\n\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n---\nOrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n---\nfrom_config",
         "initializer_range\n---\nThe standard deviation of the truncated_normal_initializer for initializing all weight matrices\n---\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()",
         "How are the weights of the Mistral models reinitialized in Huggingface",
         "From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization",
         "MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization",
         "weights mistral models reinitialized huggingface one reinitialize weights hugging face llama v2 model official way original model theres different suggestions reinitialize model tried seems work layers reinitialized function using xaiver/he initialization random initialization",
         "weight mistral model reinitialize huggingface one reinitialize weight hug face llama v2 model official way original model there s different suggestion reinitialize model try seem work layer reinitialize function use xaiver / he initialization random initialization",
         "weight mistral reinitialize huggingface reinitialize weight hug face llama v2 official original there s suggestion reinitialize layer reinitialize function xaiver he initialization random initialization",
         "7"
        ],
        [
         "33",
         "78957322",
         "Break after first PER sequence found with Spacy",
         "<p>I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all &quot;PER&quot; tags, but I want to reduce the overhead and get only the first contiguous sequence of &quot;PER&quot; entities. Here’s the example output I get:</p>\n<pre><code>Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n</code></pre>\n<p>But I want the result to be:</p>\n<pre><code>Detected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n</code></pre>\n<p>Here is the code I am currently using:</p>\n<pre><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}]\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == &quot;PER&quot;:\n            per_entities.append(ent.text)\n        elif ent.label_ == &quot;MISC&quot; and misc_count &lt; 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n<p>What I'm looking for:</p>\n<p>I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of &quot;PER&quot; entities in the text, ignoring any subsequent &quot;PER&quot; entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?</p>\n",
         "2024-09-06 13:14:32",
         "0",
         "39",
         "1",
         "78957722.0",
         "<p>The issues is that <code>doc[start_idx:].ents</code> is <a href=\"https://spacy.io/api/doc#ents\" rel=\"nofollow noreferrer\">only the named entities</a> in that slice of the doc. Thus, you will never process &quot;habló&quot; for the first entry, you will just go straight from &quot;García&quot; to &quot;López&quot;. To actually iterate over the tokens so that you see when the PER sequence ends, you have to leave out the <code>.ents</code> part. Then you just wait until you see the first token with <code>ent_type_</code> PER and start appending, then break after one of your conditions is met. I ended up refactoring your code a little as I debugged this, but here's an edited version of your program that produces the desired outputs:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;,\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}],\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == &quot;PER&quot;:\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == &quot;MISC&quot; and misc_count &lt; 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n",
         "1.0",
         "Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n---\nDetected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\"\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}]\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == \"PER\":\n            per_entities.append(ent.text)\n        elif ent.label_ == \"MISC\" and misc_count < 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "doc[start_idx:].ents\n---\n.ents\n---\nent_type_\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\",\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}],\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == \"PER\":\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == \"MISC\" and misc_count < 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "Break after first PER sequence found with Spacy",
         "I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this",
         "The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but heres an edited version of your program that produces the desired outputs",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but heres an edited version of your program that produces the desired outputs",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this",
         "break first per sequence found spacy trying extract first speakers name list texts using spacy currently function returns per tags want reduce overhead get first contiguous sequence per entities heres example output get want result code currently using im looking want modify find_per_sequence function returns first contiguous sequence per entities text ignoring subsequent per entities encountering different type entity provided function returns multiple names partial names need way ensure first name sequence included achieve",
         "break first per sequence find spacy try extract first speaker name list text use spacy currently function return per tag want reduce overhead get first contiguous sequence per entity here example output get want result code currently use I m look want modify find_per_sequence function return first contiguous sequence per entity text ignore subsequent per entity encounter different type entity provide function return multiple name partial name need way ensure first name sequence include achieve",
         "break first per sequence spacy extract first speaker name spacy currently function return per tag reduce overhead get first contiguous sequence per entity here get currently I modify findpersequence function return first contiguous sequence per entity ignore subsequent per entity encounter type entity provide function return multiple name partial name ensure first name sequence include achieve",
         "5"
        ],
        [
         "34",
         "78949607",
         "Trainer huggingface - RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned",
         "<p>I recently got the following error:\n<code>RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned</code>\nwhen doing LoRA on a small LLM.</p>\n<p>I saw on a discord someone saying:</p>\n<blockquote>\n<p>The issue likely stems from the fact that you are manually placing\nyour inputs on the GPU (with to(model.device)), but the Trainer\nexpects data to be on the CPU and will handle the transfer to the GPU\ninternally.</p>\n</blockquote>\n<p>I can't find anything of the sort written in the Trainer documentation of huggingface <a href=\"https://huggingface.co/docs/transformers/en/main_classes/trainer\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/en/main_classes/trainer</a>.</p>\n<p>Is it true? If not, how can I get rid of that error?</p>\n<p>MRE:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = &quot;croissantllm/CroissantLLMBase&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=&quot;auto&quot;)\n\ntexts = [\n    &quot;The first sentence for fine-tuning. &lt;/s&gt;&quot;,\n    &quot;The second sentence for fine-tuning. &lt;/s&gt;&quot;\n]\n\ninputs = [tokenizer(text, return_tensors=&quot;pt&quot;).to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {&quot;input_ids&quot;: input_ids, &quot;labels&quot;: labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n</code></pre>\n<p>The issue is fairly easy to reproduce directly on colab (run <code>%pip install --upgrade torch transformers peft</code> in the first cell).</p>\n",
         "2024-09-04 16:09:18",
         "2",
         "1370",
         "1",
         "79112186.0",
         "<p>Since pinning memory is only available on CPU and not GPU, when running on GPU on Colab, you can just disable it by setting <code>dataloader_pin_memory</code> to <code>False</code> for <code>TrainingArguments</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n</code></pre>\n",
         "3.0",
         "RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned\n---\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = \"croissantllm/CroissantLLMBase\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\ntexts = [\n    \"The first sentence for fine-tuning. </s>\",\n    \"The second sentence for fine-tuning. </s>\"\n]\n\ninputs = [tokenizer(text, return_tensors=\"pt\").to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {\"input_ids\": input_ids, \"labels\": labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n---\n%pip install --upgrade torch transformers peft",
         "dataloader_pin_memory\n---\nFalse\n---\nTrainingArguments\n---\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned",
         "I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell",
         "Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensors pinned recently got following error lora small llm saw discord someone saying issue likely stems fact manually placing inputs gpu tomodeldevice trainer expects data cpu handle transfer gpu internally cant find anything sort written trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin recently get follow error lora small llm see discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not find anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin recently get error lora small llm discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "7"
        ],
        [
         "35",
         "78943401",
         "Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error \"Attempting to Unscale FP16 Gradients\"",
         "<p>I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?</p>\n<p>Here is a minimal example:</p>\n<pre class=\"lang-none prettyprint-override\"><code>import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = &quot;facebook/opt-1.3b&quot;\ncache_dir = './models'\nos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;7&quot;\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=&quot;nf4&quot;,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n</code></pre>\n<p>At the line <code>scaler.step(optimizer)</code>, an error occurs:</p>\n<pre><code>Exception has occurred: ValueError: Attempting to unscale FP16 gradients.\n\n</code></pre>\n",
         "2024-09-03 08:38:23",
         "1",
         "497",
         "1",
         "78945455.0",
         "<p>You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32.</p>\n<p>You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training.</p>\n<p>What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details <a href=\"https://huggingface.co/docs/peft/en/developer_guides/quantization\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "1.0",
         "import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = \"facebook/opt-1.3b\"\ncache_dir = './models'\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n---\nscaler.step(optimizer)\n---\nException has occurred: ValueError: Attempting to unscale FP16 gradients.",
         "",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients",
         "I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs",
         "You cant finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldnt finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs You cant finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldnt finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs",
         "finetuning pretrained model quantization amp scaler error attempting unscale fp16 gradients trying finetune pretrained model limited vram achieve using quantization automatic mixed precision amp however encountering issue cant seem resolve could please help identify problem minimal example line error occurs",
         "finetune pretraine model quantization amp scaler error attempt unscale fp16 gradient try finetune pretraine model limited vram achieve use quantization automatic mixed precision amp however encounter issue can not seem resolve could please help identify problem minimal example line error occur",
         "finetune pretraine quantization amp scaler error attempt unscale fp16 gradient finetune pretraine limited vram achieve quantization automatic mixed precision amp however encounter issue can not resolve could please help identify problem minimal line error occur",
         "7"
        ],
        [
         "36",
         "78933232",
         "Keep training pytorch model on new data",
         "<p>I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps:</p>\n<ol>\n<li>Load and process the text.</li>\n<li>Use a TF-IDF Vectorizer.</li>\n<li>Build the neural network and save the TF-IDF Vectorizer and model to predict new data.</li>\n</ol>\n<p>However, every day I need to classify new comments and correct any wrong classifications.</p>\n<p>Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct).</p>\n<p>Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen.</p>\n<p>So the mains questions are:</p>\n<ol>\n<li>How could i check if the propossed way to solve this problem work as i expect?</li>\n<li>What can i do with the vectorizer when it face new tokens, can i just do a <code>.fit_transform()</code> or i would loose the original vectorizer?</li>\n</ol>\n<p>Here its the full training process:</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        &quot;set1.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n&quot;&quot;&quot;\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n&quot;&quot;&quot;\n\nlabel_mapping = {\n    &quot;anger&quot;: 0,\n    &quot;fear&quot;: 1,\n    &quot;joy&quot;: 2,\n    &quot;sadness&quot;: 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[&quot;label&quot;])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[&quot;label&quot;]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = &quot;modelbest.pth&quot;\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\ntorch.save(model.state_dict(), model_path)\n\n</code></pre>\n<p>Proposed code:</p>\n<pre><code>import torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\nsentiments = [&quot;fear&quot;,&quot;joy&quot;,&quot;sadness&quot;,&quot;anger&quot;]\n\nnew_data = (\n    pl\n    .read_csv(\n        &quot;set2.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n    .filter(pl.col(&quot;label&quot;).is_in(sentiments))\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = &quot;modelbest.pth&quot;\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = &quot;new_moedl.pth&quot;\ntorch.save(model.state_dict(), new_best_model_path)\n</code></pre>\n<p>The dataset can be found <a href=\"https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "2024-08-30 17:47:59",
         "2",
         "271",
         "2",
         "78934212.0",
         "<p>use  pre-trained word embeddings like BertForSequenceClassification.  These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words.</p>\n<p><strong>Model Training with BERT</strong></p>\n<pre><code>import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(&quot;set1.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\ntrain_df = train_df.with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[&quot;label&quot;])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[&quot;label&quot;])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy: {results['eval_accuracy']:.4f}&quot;)\n\n# Save the model and tokenizer\nmodel.save_pretrained(&quot;saved_model&quot;)\ntokenizer.save_pretrained(&quot;saved_tokenizer&quot;)\n</code></pre>\n<p><strong>Incremental training with least effort</strong></p>\n<pre><code># Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(&quot;saved_model&quot;)\ntokenizer = BertTokenizer.from_pretrained(&quot;saved_tokenizer&quot;)\n\n# Load new data\nnew_data = (\n    pl.read_csv(&quot;set2.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n    .filter(pl.col(&quot;label&quot;).is_in([&quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;anger&quot;]))\n    .with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}&quot;)\n\n# Save the updated model\nmodel.save_pretrained(&quot;saved_model_incremental&quot;)\n</code></pre>\n",
         "3.0",
         ".fit_transform()\n---\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        \"set1.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n\"\"\"\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n\"\"\"\n\nlabel_mapping = {\n    \"anger\": 0,\n    \"fear\": 1,\n    \"joy\": 2,\n    \"sadness\": 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[\"label\"])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[\"label\"]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = \"modelbest.pth\"\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = \"text_classification_model.pth\"\ntorch.save(model.state_dict(), model_path)\n---\nimport torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = \"text_classification_model.pth\"\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\nsentiments = [\"fear\",\"joy\",\"sadness\",\"anger\"]\n\nnew_data = (\n    pl\n    .read_csv(\n        \"set2.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n    .filter(pl.col(\"label\").is_in(sentiments))\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = \"modelbest.pth\"\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = \"new_moedl.pth\"\ntorch.save(model.state_dict(), new_best_model_path)",
         "import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(\"set1.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\ntrain_df = train_df.with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[\"label\"])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[\"label\"])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")\n\n# Save the model and tokenizer\nmodel.save_pretrained(\"saved_model\")\ntokenizer.save_pretrained(\"saved_tokenizer\")\n---\n# Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"saved_model\")\ntokenizer = BertTokenizer.from_pretrained(\"saved_tokenizer\")\n\n# Load new data\nnew_data = (\n    pl.read_csv(\"set2.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n    .filter(pl.col(\"label\").is_in([\"fear\", \"joy\", \"sadness\", \"anger\"]))\n    .with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}\")\n\n# Save the updated model\nmodel.save_pretrained(\"saved_model_incremental\")",
         "Keep training pytorch model on new data",
         "Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here",
         "use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "Keep training pytorch model on new data Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "Keep training pytorch model on new data Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here",
         "keep training pytorch model new data im working text classification task decided use pytorch model purpose process mainly involves following steps load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new data however every day need classify new comments correct wrong classifications currently approach add new comments correct classification dataset retrain entire model process timeconsuming new comments lost validation would like create new dataset newly classified texts continue training new data new comments classified manually label correct using gpt online code write desired process however im sure working expected im making silly mistakes happen mains questions could check propossed way solve problem work expect vectorizer face new tokens would loose original vectorizer full training process proposed code dataset found",
         "keep train pytorch model new datum I m work text classification task decide use pytorch model purpose process mainly involve follow step load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new datum however every day need classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire model process timeconsume new comment lose validation would like create new dataset newly classify text continue train new datum new comment classify manually label correct use gpt online code write desire process however I m sure working expect I m make silly mistake happen main question could check proposse way solve problem work expect vectorizer face new token would loose original vectorizer full training process propose code dataset find",
         "keep train pytorch new datum I classification task decide pytorch purpose process mainly involve step load process tfidf vectorizer build neural network save tfidf vectorizer predict new datum however every day classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire process timeconsume new comment lose validation would like create new dataset newly classify continue train new datum new comment classify manually label correct gpt online write desire process however I sure working expect I make silly mistake happen main question could check proposse solve problem expect vectorizer face new token would loose original vectorizer full training process propose dataset",
         "7"
        ],
        [
         "37",
         "78932356",
         "Capitalized words in sentiment analysis",
         "<p>I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive .\nA common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .</p>\n<p>this is my current code :</p>\n<pre><code>from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)\n</code></pre>\n",
         "2024-08-30 13:49:56",
         "-1",
         "128",
         "1",
         "78933236.0",
         "<p>If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data.</p>\n<p>If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis.</p>\n<p>If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out <a href=\"https://github.com/EliasK93/transformer-model-comparison-for-review-sentiment-regression\" rel=\"nofollow noreferrer\">this repository</a> as an example.</p>\n",
         "0.0",
         "from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)",
         "",
         "Capitalized words in sentiment analysis",
         "Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code",
         "If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "Capitalized words in sentiment analysis Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "Capitalized words in sentiment analysis Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code",
         "capitalized words sentiment analysis im currently working data customers reviews products sephora task classify sentiments negative neutral positive common technique text preprocessing lower case words situation upper case words like amazing hide significant emotion behind turning word lower case cause information loss would happy opinion subject still lower case words personally think creating classes distinction sentiments good good positive include importance upper case words current code",
         "capitalize word sentiment analysis I m currently work datum customer review product sephora task classify sentiment negative neutral positive common technique text preprocesse low case word situation upper case word like amazing hide significant emotion behind turn word low case cause information loss would happy opinion subject still low case word personally think create class distinction sentiment good good positive include importance upper case word current code",
         "capitalize sentiment analysis I currently datum customer review product sephora task classify sentiment negative neutral positive common technique preprocesse low case situation upper case like amazing hide significant emotion behind turn low case cause information loss would happy opinion subject still low case personally think create class distinction sentiment good good positive include importance upper case current",
         "0"
        ],
        [
         "38",
         "78920095",
         "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub'",
         "<p>I've been using LLAMA 2 for research for a few months now and I import as follows:</p>\n<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(&quot;cuda&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;,torch_dtype=&quot;auto&quot;)\nmodel = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;, torch_dtype=&quot;auto&quot;, load_in_4bit=True)\n</code></pre>\n<p>It has always worked. However, today it is showing the following error:\n<strong>RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/<strong>init</strong>.py)</strong></p>\n<p>Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.</p>\n",
         "2024-08-27 17:20:42",
         "1",
         "5871",
         "1",
         "78920098.0",
         "<p>The error you're encountering is due to the <code>split_torch_state_dict_into_shards</code> function not being available in <code>huggingface-hub version &lt; 0.23.0</code>.</p>\n<p>This function is included starting from version <code>0.23.0</code>.</p>\n<p><strong>To resolve this issue, update the <code>huggingface-hub</code> library to version 0.23.0 or later</strong></p>\n<p>Also please install accelerate:</p>\n<pre><code>pip install accelerate==0.31.0\n</code></pre>\n<p>here is a git link: <strong><a href=\"https://github.com/run-llama/llama_index/discussions/14605\" rel=\"nofollow noreferrer\">https://github.com/run-llama/llama_index/discussions/14605</a></strong></p>\n",
         "4.0",
         "from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\",torch_dtype=\"auto\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\", torch_dtype=\"auto\", load_in_4bit=True)",
         "split_torch_state_dict_into_shards\n---\nhuggingface-hub version < 0.23.0\n---\n0.23.0\n---\nhuggingface-hub\n---\npip install accelerate==0.31.0",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub",
         "Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook",
         "The error youre encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook The error youre encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook",
         "import name split_torch_state_dict_into_shards huggingface_hub ive using llama 2 research months import follows always worked however today showing following error runtimeerror failed import transformersmodelsllamamodeling_llama following error look see traceback failed import transformersgenerationutils following error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py recreated hugging face token didnt work using google colab kaggle notebook",
         "import name split_torch_state_dict_into_shard huggingface_hub I ve use llama 2 research month import follows always work however today show follow error runtimeerror fail import transformersmodelsllamamodeling_llama follow error look see traceback fail import transformersgenerationutil follow error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt / conda / lib / python310 / sitepackage / huggingface_hub/ init py recreate hug face token do not work use google colab kaggle notebook",
         "import name splittorchstatedictintoshard huggingfacehub I ve llama 2 research month import follows always however today show error runtimeerror fail import transformersmodelsllamamodelingllama error traceback fail import transformersgenerationutil error traceback import name splittorchstatedictintoshards huggingfacehub opt conda lib python310 sitepackage huggingfacehub init py recreate hug face token do not google colab kaggle notebook",
         "4"
        ],
        [
         "39",
         "78917743",
         "How to Process Data on GPU Instead of RAM for This Python Code?",
         "<p>I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance.\nmy code :</p>\n<pre><code>def prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n    batch[&quot;input_features&quot;] = feature_extractor(\n        audio[&quot;array&quot;], \n        sampling_rate=audio[&quot;sampling_rate&quot;]\n    ).input_features[0]\n    batch[&quot;labels&quot;] = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[&quot;train&quot;], \n    num_proc=1\n)\n</code></pre>\n<p>How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!</p>\n",
         "2024-08-27 08:03:28",
         "1",
         "61",
         "1",
         "78918851.0",
         "<p>you can using the following code to process audio data on GPU</p>\n<pre><code>import torch\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n\n    input_features = feature_extractor(audio[&quot;array&quot;], sampling_rate=audio[&quot;sampling_rate&quot;]).input_features[0]\n    batch[&quot;input_features&quot;] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    batch[&quot;labels&quot;] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[&quot;train&quot;])\n</code></pre>\n",
         "2.0",
         "def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n    batch[\"input_features\"] = feature_extractor(\n        audio[\"array\"], \n        sampling_rate=audio[\"sampling_rate\"]\n    ).input_features[0]\n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[\"train\"], \n    num_proc=1\n)",
         "import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    input_features = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n    batch[\"input_features\"] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[\"sentence\"]).input_ids\n    batch[\"labels\"] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"])",
         "How to Process Data on GPU Instead of RAM for This Python Code",
         "Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated",
         "you can using the following code to process audio data on GPU",
         "How to Process Data on GPU Instead of RAM for This Python Code Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated you can using the following code to process audio data on GPU",
         "How to Process Data on GPU Instead of RAM for This Python Code Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated",
         "process data gpu instead ram python code im currently using following code process audio data runs ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific changes much appreciated",
         "process datum gpu instead ram python code I m currently use follow code process audio data run ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific change much appreciate",
         "process datum gpu instead ram python I currently process audio data run ram offload processing gpu improve performance modify utilize gpu processing instead ram guidance specific change much appreciate",
         "8"
        ],
        [
         "40",
         "78912171",
         "How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation",
         "<p>I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this.</p>\n<p><strong>What I’ve Tried:</strong></p>\n<ul>\n<li><p><strong>Initial Attempt:</strong> I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me.</p>\n</li>\n<li><p><strong>Using Forward Hooks:</strong> To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation:</p>\n</li>\n</ul>\n<pre class=\"lang-py prettyprint-override\"><code># VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = &quot;Helsinki-NLP/opus-mt-en-de&quot;\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = &quot;Please translate this to German.&quot;\ninputs = tokenizer(input_text, return_tensors=&quot;pt&quot;)\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(&quot;layers, heads, output tokens, input tokens&quot;)\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(&quot;Input Tokens&quot;)\nplt.ylabel(&quot;Output Tokens&quot;)\nplt.title(&quot;Cross-Attention Matrix&quot;)\nplt.show()\n</code></pre>\n<p>This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation.</p>\n<p><strong>My Question</strong></p>\n<p>Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8?</p>\n<p>I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.</p>\n",
         "2024-08-25 20:13:54",
         "1",
         "380",
         "1",
         "78915504.0",
         "<p>Huggingface has built in methods to return attention weights</p>\n<pre class=\"lang-py prettyprint-override\"><code>translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n&gt; odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n</code></pre>\n<p>With <code>return_dict_in_generate=True</code>, <code>model.generate</code> returns a dict-like object. With <code>output_attentions=True</code>, the output dict will contain all attention weights.</p>\n<p>For this model, it will include encoder attentions, decoder attentions and cross attentions.</p>\n",
         "3.0",
         "# VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = \"Helsinki-NLP/opus-mt-en-de\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = \"Please translate this to German.\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(\"layers, heads, output tokens, input tokens\")\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(\"Input Tokens\")\nplt.ylabel(\"Output Tokens\")\nplt.title(\"Cross-Attention Matrix\")\nplt.show()",
         "translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n> odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n---\nreturn_dict_in_generate=True\n---\nmodel.generate\n---\noutput_attentions=True",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation",
         "I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this",
         "Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this",
         "visualize crossattention matrices marianmtmodel output generation working machine translation task using marianmtmodel hugging face transformers library specifically want visualize crossattention matrices models translation process however encountered difficulties achieving ive tried initial attempt noticed crossattention matrices directly returned model generates translation example found involved feeding source text translation model however goal access crossattention matrices model generates output translation given using forward hooks achieve implemented forward hooks key query projections attention mechanism disabling keyvalue caching use_cache=false capture full matrices last step heres implementation approach seemed work capturing crossattention matrices however observed matrices 4 attention heads instead expected 8 makes question correctness implementation question given issues ive encountered reliable method extract visualize crossattention matrices translation process additionally current approach fundamentally okay resolve issue capturing 4 attention heads instead 8 suspect issue might related im currently reshaping key k query q tensors head dimension multiplication wanted ask advice case theres easier effective way",
         "visualize crossattention matrix marianmtmodel output generation work machine translation task use marianmtmodel hug face transformer library specifically want visualize crossattention matrix model translation process however encounter difficulty achieve I ve try initial attempt notice crossattention matrix directly return model generate translation example find involved feeding source text translation model however goal access crossattention matrix model generate output translation give use forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache use_cache = false capture full matrix last step here implementation approach seem work capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question give issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate I m currently reshape key k query q tensor head dimension multiplication want ask advice case there s easy effective way",
         "visualize crossattention matrix marianmtmodel generation machine translation task marianmtmodel hug face transformer library specifically visualize crossattention matrix translation process however encounter difficulty achieve I ve initial attempt notice crossattention matrix directly return generate translation involved feeding source translation however goal access crossattention matrix generate translation forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache usecache false capture full matrix last step here implementation approach capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate I currently reshape key k query q tensor head dimension multiplication ask advice case there s easy effective",
         "7"
        ],
        [
         "41",
         "78905614",
         "Why doesn't permuting positional encodings in BERT affect the output as expected?",
         "<p>I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another <a href=\"https://stackoverflow.com/questions/78902301/why-doesnt-permuting-positional-encodings-in-gpt-2-affect-the-output-as-expecte/78903454#78903454\">question</a> I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results.</p>\n<p><strong>What I expected to happen:</strong></p>\n<ul>\n<li>No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary.</li>\n<li>Permuting only the input IDs should return distribution B.</li>\n<li>Permuting only the positional embeddings should return distribution B.</li>\n<li>Permuting both the input IDs and positional embeddings should return distribution A.</li>\n</ul>\n<p><strong>What actually happens:</strong>\nSometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.</p>\n<p><strong>My question is:</strong> Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding?</p>\n<p>For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in <code>masked_prediction</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(&quot;transformers.modeling_utils&quot;).setLevel(logging.ERROR)\nwarnings.simplefilter(&quot;ignore&quot;, FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) &gt; 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(&quot;You can't use an empty sequence for prediction&quot;)\n            return\n        \n        model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(&quot;You need to include a [MASK] token for prediction&quot;)\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=&quot;Run Prediction&quot;)\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)\n</code></pre>\n",
         "2024-08-23 11:12:08",
         "1",
         "74",
         "1",
         "78906902.0",
         "<p>The model inputs have token ids and position ids. There are four scenarios to consider:</p>\n<ol>\n<li>Baseline. Correct order for tokens and positions</li>\n<li>Permute position ids only</li>\n<li>Permute token ids only</li>\n<li>Permute position ids and token ids</li>\n</ol>\n<p>You are correct that scenario 1 and 4 should produce the same results. However you are incorrect in assuming that permuting tokens or positions separately should give the same result. Consider:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n</code></pre>\n<p>In <code>Ex1</code>, the model is told that token <code>2</code> occurs at position <code>0</code>. In <code>Ex2</code>, the model is told that token <code>2</code> occurs at position <code>1</code>. Even though we used the same permutation, the mapping of tokens to positions is different. This results in different model outputs.</p>\n<p>The reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. This is luck - the average case produces different results.</p>\n<p>It is simple to test this. Huggingface models take a <code>position_ids</code> input parameter. We can use this to test permutations of the input ids without messing with the weight matrices.</p>\n<p>To test this, we'll create input data, permute as needed, compute logits and compare logits.</p>\n<p>When comparing logits, we will permute or depermute as needed to compare on a token to token basis. For example if token <code>i</code> in scenario 1 is permuted to token <code>j</code> in scenario 3, we want to compare logits <code>i</code> from scenario 1 to logits <code>j</code> in scenario 3.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f&quot;Comparison {sa}, {sb}: {val.item():.6f}&quot;)\n</code></pre>\n<p>The code should produce an output like:</p>\n<pre><code>Comparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n</code></pre>\n<p>Run the code a bunch of times. You will find that the <code>S1, S4</code> comparison <em>always</em> has a small deviation. This is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues.</p>\n<p>You will find the <code>S2, S3</code> comparison generally has a large deviation, but <em>sometimes</em> has a small deviation. As discussed, this is due to getting a lucky permutation where positions and ids mostly line up.</p>\n",
         "2.0",
         "masked_prediction\n---\nimport torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) > 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(\"You can't use an empty sequence for prediction\")\n            return\n        \n        model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(\"You need to include a [MASK] token for prediction\")\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=\"Run Prediction\")\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)",
         "# Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n---\nEx1\n---\n2\n---\n0\n---\nEx2\n---\n2\n---\n1\n---\nposition_ids\n---\ni\n---\nj\n---\ni\n---\nj\n---\nimport torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f\"Comparison {sa}, {sb}: {val.item():.6f}\")\n---\nComparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n---\nS1, S4\n---\nS2, S3",
         "Why doesnt permuting positional encodings in BERT affect the output as expected",
         "I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in",
         "The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this well create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "Why doesnt permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this well create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "Why doesnt permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in",
         "doesnt permuting positional encodings bert affect output expected working jupyter notebook transformers section positional encodings want demonstrate transformer relies entirely positional encoding understand order sequence previously learned another question posted concept applies models dont use masked attention like gpt2 however attempted approach bert model uses crossattention predict mask token encountered unexpected results expected happen permutation cause model predict different token ie distribution consistent vocabulary permuting input ids return distribution b permuting positional embeddings return distribution b permuting input ids positional embeddings return distribution actually happens sometimes results align expectations times permuting one aspect either input ids positional embeddings leads different outcomes even though occasionally produce result question something else hugging faces bert model might influenced position beyond positional encoding completeness included full code part notebook tried directly important part happens",
         "do not permute positional encoding bert affect output expect work jupyter notebook transformer section positional encoding want demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply model do not use mask attention like gpt2 however attempt approach bert model use crossattention predict mask token encounter unexpected result expect happen permutation cause model predict different token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes result align expectation time permute one aspect either input ids positional embedding lead different outcome even though occasionally produce result question something else hug face bert model might influence position beyond positional encoding completeness include full code part notebook try directly important part happen",
         "do not permute positional encoding bert affect expect jupyter notebook transformer section positional encoding demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply do not mask attention like gpt2 however attempt approach bert crossattention predict mask token encounter unexpected expect happen permutation cause predict token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes align expectation time permute aspect either input ids positional embedding lead outcome even though occasionally produce question something else hug face bert might influence position beyond positional encoding completeness include full part notebook directly important part happen",
         "2"
        ],
        [
         "42",
         "78901998",
         "How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus?",
         "<p>I'm working with the <code>OpenAIEmbeddings()</code> class from <code>OpenAI</code>, which uses the <code>text-embedding-3-small</code> model. According to the <a href=\"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\" rel=\"nofollow noreferrer\">documentation</a>, it generates a 1536-dimensional vector for any input text.</p>\n<p>However, I'm a bit confused about how this works:</p>\n<ul>\n<li>Is the 1536-dimensional vector generated for the entire input text?</li>\n<li>If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</li>\n</ul>\n<p><strong>I was expecting this:</strong></p>\n<p>If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536.</p>\n<p>But the output is a single vector of size 1536 for the whole input text.</p>\n<p>Why I expected this?</p>\n<p>Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings?</p>\n<p>I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input.</p>\n<p>Any insights or examples would be greatly appreciated!</p>\n",
         "2024-08-22 14:09:25",
         "2",
         "580",
         "1",
         "78902136.0",
         "<p>Everything you described is 100% expected.</p>\n<h3>Q: Is the 1536-dimensional vector generated for the entire input text?</h3>\n<p>A: Yes.</p>\n<h3>Q: If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</h3>\n<p>A: First, the OpenAI Embeddings model doesn't handle a single word any different than a long text. For the model, it's an input. The input can be even a single character (e.g., &quot;a&quot;), but it doesn't make sense to calculate an embedding vector out of it since &quot;a&quot; doesn't semantically mean anything to us humans.</p>\n<p>Second, what you probably meant with this question is what happens when you do a similarity search with these embeddings. In other words, what happens when you <em>use</em> them? What happens if you use embeddings of words, sentences, paragraphs, or the whole text? Does it matter? Yes!</p>\n<p>This is called chunking. The decision about how to chunk your text depends on the use case. The best thing is probably to simply try and see. If you get meaningful results after doing a similarity search, then this means that chunking is appropriate (even if this means chunking the whole text). If you don't get meaningful results after doing a similarity search, then this means that chunking isn't appropriate (e.g., instead of chunking by paragraph, try chunking by sentences).</p>\n<p>There's an excellent Stack Overflow <a href=\"https://stackoverflow.blog/2024/06/06/breaking-up-is-hard-to-do-chunking-in-rag-applications/\">blog post</a> about this topic you should read (pay attention to the bolded text because this is the best explanation):</p>\n<blockquote>\n<p>With RAG, you create text embeddings of the pieces of data that you\nwant to draw from and retrieve. That allows you to place a piece of\nthe source text within the semantic space that LLMs use to create\nresponses.</p>\n<p>/.../</p>\n<p>When it comes to RAG systems, you’ll need to pay special attention to\nhow big the individual pieces of data are. How you divide your data up\nis called chunking, and it’s more complex than embedding whole\ndocuments.</p>\n<p>/.../</p>\n<p>The size of the chunked data is going to make a huge difference in\nwhat information comes up in a search. When you embed a piece of data,\nthe whole thing is converted into a vector. <strong>Include too much in a\nchunk and the vector loses the ability to be specific to anything it\ndiscusses. Include too little and you lose the context of the data.</strong></p>\n</blockquote>\n",
         "3.0",
         "OpenAIEmbeddings()\n---\nOpenAI\n---\ntext-embedding-3-small",
         "",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus",
         "Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated",
         "Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesnt handle a single word any different than a long text For the model its an input The input can be even a single character eg a but it doesnt make sense to calculate an embedding vector out of it since a doesnt semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you dont get meaningful results after doing a similarity search then this means that chunking isnt appropriate eg instead of chunking by paragraph try chunking by sentences Theres an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesnt handle a single word any different than a long text For the model its an input The input can be even a single character eg a but it doesnt make sense to calculate an embedding vector out of it since a doesnt semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you dont get meaningful results after doing a similarity search then this means that chunking isnt appropriate eg instead of chunking by paragraph try chunking by sentences Theres an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated",
         "openaiembeddings work creating single vector size 1536 whole text corpus im working class uses model according documentation generates 1536dimensional vector input text however im bit confused works 1536dimensional vector generated entire input text 1536dimensional vector represents entire input text model handle individual words versus longer texts like sentences paragraphs expecting 100 words input text expected openaiembeddings would output 100 vectors size 1536 output single vector size 1536 whole input text expected learning ive understood embeddings like word2vec glove provide vectors word corpus differ approach taken openaiembeddings im trying understand whether theres way extract embeddings individual words using model output always single vector representing whole input insights examples would greatly appreciated",
         "openaiembedding work create single vector size 1536 whole text corpus I m work class use model accord documentation generate 1536dimensional vector input text however I m bit confused work 1536dimensional vector generate entire input text 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph expect 100 word input text expect openaiembedding would output 100 vector size 1536 output single vector size 1536 whole input text expect learning I ve understand embedding like word2vec glove provide vector word corpus differ approach take openaiembedding I m try understand whether there s way extract embedding individual word use model output always single vector represent whole input insight example would greatly appreciate",
         "openaiembedding create single vector size 1536 whole corpus I class accord documentation generate 1536dimensional vector input however I bit confused 1536dimensional vector generate entire input 1536dimensional vector represent entire input handle individual versus long like paragraph expect 100 input expect openaiembedding would 100 vector size 1536 single vector size 1536 whole input expect learning I ve understand embedding like word2vec glove provide vector corpus differ approach take openaiembedding I understand whether there s extract embedding individual always single vector represent whole input insight would greatly appreciate",
         "2"
        ],
        [
         "43",
         "78895710",
         "NER versus LLM to extract name, gender, role and company from text",
         "<p>I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons.</p>\n<p>I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them.</p>\n<p>Is there another, smaller LLM that might be good at this while using fewer processing resources?</p>\n<p>Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.)</p>\n<p>Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass?</p>\n<p>Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction.</p>\n<p>Thanks in advance!</p>\n",
         "2024-08-21 07:39:13",
         "1",
         "1524",
         "2",
         "78896098.0",
         "<p>Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. <code>NER</code> is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use <code>NLTK</code> or <code>SpaCy</code> for this matter. My personal choice is <code>SpaCy</code>, however a <code>gender</code> as you defined is not a known named entity. you can see a list of named entities in <a href=\"https://github.com/explosion/spaCy/discussions/9147\" rel=\"nofollow noreferrer\">this doc</a>.</p>\n<p>I guess what you mean by <code>gender</code> is the possible <code>gender</code> associated with the names of a <code>PERSON</code> mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use <a href=\"https://pypi.org/project/gender-guesser/\" rel=\"nofollow noreferrer\"><code>gender-guesser</code> package</a>.</p>\n<p>A possible solution would be like this:</p>\n<pre><code>import spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == &quot;PERSON&quot;:\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n</code></pre>\n<p>Note that <code>en_core_web_sm</code> is the small model available via spaCy, you can use the large model by specifying <code>en_core_web_lg</code>, just make sure that the model is downloaded before running your code. here's how you can download the model:</p>\n<pre><code>python -m spacy download en_core_web_sm\n</code></pre>\n",
         "1.0",
         "",
         "NER\n---\nNLTK\n---\nSpaCy\n---\nSpaCy\n---\ngender\n---\ngender\n---\ngender\n---\nPERSON\n---\ngender-guesser\n---\nimport spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == \"PERSON\":\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n---\nen_core_web_sm\n---\nen_core_web_lg\n---\npython -m spacy download en_core_web_sm",
         "NER versus LLM to extract name gender role and company from text",
         "I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance",
         "Apart from your limitations I wouldnt recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code heres how you can download the model",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance Apart from your limitations I wouldnt recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code heres how you can download the model",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance",
         "ner versus llm extract name gender role company text need extract name gender job title employer/company name newspaper articles running process local hardware cloud allowed due copyright reasons ive playing around llama 31 im finding dont get useable results models smaller 70b parameters size models run much slowly best hardware throw another smaller llm might good using fewer processing resources ner use extract data ners ive looked extract name gender dont know extract data gender showstopper alternatively approach take first pass ner pass names llm together original newspaper article extract data get better results faster single llm pass answer training model good model use starting point im much beginning machine learning journey would love pointed right direction thanks advance",
         "ner versus llm extract name gender role company text need extract name gender job title employer / company name newspaper article running process local hardware cloud allow due copyright reason I ve play around llama 31 I m find do not get useable result model small 70b parameter size model run much slowly good hardware throw another small llm might good use few processing resource ner use extract data ner I ve look extract name gender do not know extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get well result fast single llm pass answer training model good model use start point I m much begin machine learn journey would love point right direction thank advance",
         "ner versus llm extract name gender role company extract name gender job title employer company name newspaper article running process local hardware cloud allow due copyright reason I ve play around llama 31 I do not get useable small 70b parameter size run much slowly good hardware throw another small llm might good few processing resource ner extract data ner I ve extract name gender do not extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get fast single llm pass answer training good start point I much begin machine learn journey would love point right direction thank advance",
         "1"
        ],
        [
         "44",
         "78887743",
         "Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask?",
         "<p>In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%).</p>\n<p>I am curious about the following:</p>\n<p>When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively?\nDoes the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact?</p>\n<p>Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%?</p>\n<p>Thank you for your insights!</p>\n",
         "2024-08-19 11:49:06",
         "1",
         "525",
         "1",
         "78890409.0",
         "<p>Attention is computed on a tensor of shape <code>(batch_size, sequence_length, embedding_dimension)</code>. The compute and memory requirements scale with the size of those dimensions.</p>\n<p>For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size.</p>\n<p>With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length <code>8</code> and 10 sequences of length <code>128</code>. Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of <code>128</code>. If you sort by length before batching, you get one batch with sequence length of <code>8</code> and another with length <code>128</code>. The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128).</p>\n<p>That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to &quot;skip over&quot; padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.</p>\n",
         "2.0",
         "",
         "(batch_size, sequence_length, embedding_dimension)\n---\n8\n---\n128\n---\n128\n---\n8\n---\n128",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask",
         "In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights",
         "Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding youre not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you arent going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesnt work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding youre not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you arent going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesnt work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights",
         "padding batch sequences affect performance effective attention mask transformer models sequences variable lengths typically padded maximum length batch however sequence lengths vary batch may contain substantial amount padding potentially 50 % curious following pytorch computes transformer padding tokens impact calculation speed negatively presence attention mask allow model effectively skip padding tokens resulting minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero values computation effectively reduce approximately 10 % thank insights",
         "padding batch sequence affect performance effective attention mask transformer model sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 % curious follow pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow model effectively skip padding token result minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero value computation effectively reduce approximately 10 % thank insight",
         "padding batch sequence affect performance effective attention mask transformer sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 curious pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow effectively skip padding token minimal performance impact overall effective attention mask sparse attention mask 10 nonzero value computation effectively reduce approximately 10 thank insight",
         "2"
        ],
        [
         "45",
         "78865486",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text",
         "<p>Using the following Matcher rule:</p>\n<pre><code>{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }\n</code></pre>\n<p>on the text: 'MyLabel: Some Value'</p>\n<p>I get <strong>two</strong> matches: 'MyLabel' and 'MyLabel:'</p>\n<p>For me, that was quite surprising - I was expecting a single match on 'MyLabel:'.\nAdding the new greedy flag didn't make any difference.</p>\n<ul>\n<li>Is this the intended behavior or is it a bug?</li>\n<li>How should I determine that the second match really is just a subset of the first match?</li>\n<li>Will the shorter match always be reported before the longer match?</li>\n</ul>\n<p>SpaCy version 3.7.5</p>\n",
         "2024-08-13 09:37:23",
         "1",
         "34",
         "1",
         "78870921.0",
         "<p>i will say that the behavior you're observing with the SpaCy <code>Matcher</code> is expected, and it is not a bug. When you use the <code>{'TEXT': ':', 'OP': '?'}</code> pattern, the <code>OP: '?'</code> operator means that the colon is optional, so the matcher will generate both the shorter and the longer match, as you've seen.</p>\n<h3>Explanation:</h3>\n<ul>\n<li><strong>Pattern</strong>: <code>{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}</code>.</li>\n<li><strong>Text</strong>: <code>'MyLabel: Some Value'</code>.</li>\n</ul>\n<p>So for this pattern, SpaCy  will try to match:</p>\n<ol>\n<li><code>'MyLabel'</code> alone (because the colon is optional).</li>\n<li><code>'MyLabel:'</code> (because the colon can be included).</li>\n</ol>\n<p>Therefore, you will get two matches: <code>'MyLabel'</code> and <code>'MyLabel:'</code>.</p>\n<h3>Now to  Answer Your Questions:</h3>\n<ol>\n<li><p><strong>Is this the intended behavior or is it a bug?</strong></p>\n<ul>\n<li>This is intended behavior. The <code>OP: '?'</code> operator allows the colon to be optionally matched, leading to multiple matches.</li>\n</ul>\n</li>\n<li><p><strong>How should I determine that the second match really is just a subset of the first match?</strong></p>\n<ul>\n<li>To determine if one match is a subset of another, you can compare the start and end indices of the matches. The longer match will have the same start index but a different end index. Now i wrote a code below even using spacy version 3.7.5, see details below</li>\n</ul>\n</li>\n</ol>\n<pre><code>pip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n</code></pre>\n<p>Now Example in code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\ndoc = nlp(&quot;MyLabel: Some Value&quot;)\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(&quot;R-1&quot;, [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f&quot;Match: {span.text}, Start: {start}, End: {end}&quot;)\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start &gt;= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f&quot;Filtered Match: {span.text}&quot;)\n</code></pre>\n<p>Now, This code will filter out the shorter match and your output will be</p>\n<pre><code>Match: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n\n</code></pre>\n<ol start=\"3\">\n<li><strong>Now Will the shorter match always be reported before the longer match?</strong>\n<ul>\n<li>I don't think the matches are not guaranteed to be reported in a specific order. so to handle this, you can sort the matches by their start and end indices as shown in the code example above.Now, After sorting, you can now filter out matches that are subsets of longer matches.</li>\n</ul>\n</li>\n</ol>\n<h3>Another Alternative Solution:</h3>\n<p>If you want to ensure that only the longest match is returned, you can change the way you define the pattern:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n</code></pre>\n<p>note that the <code>greedy</code> flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings.</p>\n<h3>Now back to the Summary of what i explained:</h3>\n<ul>\n<li>The behavior you're seeing is by design, due to the optional <code>OP: '?'</code> operator.</li>\n<li>in addition, you can filter out the shorter match by comparing start and end indices of the matches.</li>\n<li>furthermore, Sorting the matches by start and end indices allows you to keep only the longest, non-overlapping matches.</li>\n</ul>\n",
         "1.0",
         "{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }",
         "Matcher\n---\n{'TEXT': ':', 'OP': '?'}\n---\nOP: '?'\n---\n{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}\n---\n'MyLabel: Some Value'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\nOP: '?'\n---\npip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"MyLabel: Some Value\")\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(\"R-1\", [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f\"Match: {span.text}, Start: {start}, End: {end}\")\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start >= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f\"Filtered Match: {span.text}\")\n---\nMatch: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n---\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n---\ngreedy\n---\nOP: '?'",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text",
         "Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375",
         "i will say that the behavior youre observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as youve seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I dont think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesnt change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior youre seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375 i will say that the behavior youre observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as youve seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I dont think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesnt change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior youre seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375",
         "spacy matcher optional suffix pattern reports multiple matches text using following matcher rule text mylabel value get two matches mylabel mylabel quite surprising expecting single match mylabel adding new greedy flag didnt make difference intended behavior bug determine second match subset first match shorter match always reported longer match spacy version 375",
         "spacy matcher optional suffix pattern report multiple match text use follow matcher rule text mylabel value get two match mylabel mylabel quite surprising expect single match mylabel add new greedy flag do not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375",
         "spacy matcher optional suffix pattern report multiple match matcher rule mylabel value get match mylabel mylabel quite surprising expect single match mylabel add new greedy flag do not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375",
         "9"
        ],
        [
         "46",
         "78862691",
         "`mlflow.transformers.log_model()` does not finish",
         "<h3>Problem</h3>\n<p>I want to use <code>mlflow.transformers.log_model()</code> to log a finetuned huggingface model.</p>\n<p><strong>However, when the <code>mlflow.transformers.log_model</code> method is running, it simply does not finish - runs forever - throws no errors.</strong></p>\n<p>I suspect my configuration is not right, the model is too big?\nThe output says <code>Skipping saving pretrained model weights to disk</code> so that should not be the problem.</p>\n<p>Any ideas how to do this properly?</p>\n<h3>Example</h3>\n<p>This is more or less how my setup looks like, you cannot run this, it includes some pseudocode...</p>\n<p>I am on python 3.11.9 with <code>transformers = &quot;^4.41.2&quot;</code> &amp; <code>mlflow = &quot;^2.15.1&quot;</code>.</p>\n<pre><code>import mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = &quot;LeoLM/leo-hessianai-7b-chat-bilingual&quot;\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=&quot;auto&quot;,\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n<p>The last output I get in the console is:</p>\n<pre><code>INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(&quot;Setuptools is replacing distutils.&quot;)\n</code></pre>\n",
         "2024-08-12 16:27:32",
         "0",
         "337",
         "1",
         "78877979.0",
         "<p>Before defining the trainer, the model has be turned into a Peft model object via <code>get_peft_model</code>, then the <code>mlflow.transformers.log_model</code> works:</p>\n<pre><code>from peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n",
         "0.0",
         "mlflow.transformers.log_model()\n---\nmlflow.transformers.log_model\n---\nSkipping saving pretrained model weights to disk\n---\ntransformers = \"^4.41.2\"\n---\nmlflow = \"^2.15.1\"\n---\nimport mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = \"LeoLM/leo-hessianai-7b-chat-bilingual\"\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )\n---\nINFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")",
         "get_peft_model\n---\nmlflow.transformers.log_model\n---\nfrom peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )",
         "`mlflowtransformerslog_model` does not finish",
         "Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is",
         "Before defining the trainer the model has be turned into a Peft model object via then the works",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is Before defining the trainer the model has be turned into a Peft model object via then the works",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is",
         "` mlflowtransformerslog_model ` finish problem want use log finetuned huggingface model however method running simply finish runs forever throws errors suspect configuration right model big output says problem ideas properly example less setup looks like run includes pseudocode python 3119 & last output get console",
         "` mlflowtransformerslog_model ` finish problem want use log finetune huggingface model however method run simply finish run forever throw error suspect configuration right model big output say problem idea properly example less setup look like run include pseudocode python 3119 & last output get console",
         "mlflowtransformerslogmodel finish problem log finetune huggingface however method run simply finish run forever throw error suspect configuration right big say problem idea properly less setup like run include pseudocode python 3119 last get console",
         "4"
        ],
        [
         "47",
         "78853409",
         "NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation)",
         "<p>I'm attempting to fine-tune the NLLB model <code>&quot;facebook/nllb-200-distilled-600M&quot;</code> for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb.</p>\n<p>Documentation: <a href=\"https://github.com/facebookresearch/fairseq/tree/nllb?tab=readme-ov-file\" rel=\"nofollow noreferrer\">link</a></p>\n<p>This is the code block which is giving error:</p>\n<pre><code>DATA_CONFIG = &quot;/content/sample_data/data_config.json&quot;\nOUTPUT_DIR = &quot;/content/outputs&quot;\nMODEL_FOLDER = &quot;/content/drive/MyDrive/Thesis/nllb-checkpoints&quot;\nDROP = 0.1\nSRC = &quot;eng_Latn&quot;\nTGT = &quot;deu_Latn&quot;\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=&quot;$SRC-$TGT&quot; \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n</code></pre>\n<p>This is the error:</p>\n<pre><code>/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=&quot;conf&quot;, config_name=&quot;base_config&quot;)\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 289, in main\n    train_module = TrainModule(config)\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n</code></pre>\n<p>So far, I understand there is a <code>Missing data_prefix configuration</code>. I created a demo custom data_config.json. Which looks like this:</p>\n<pre><code>{\n    &quot;data_prefix&quot;: &quot;/content/sample_data&quot;,\n    &quot;train_data&quot;: &quot;train_demo.json&quot;,\n    &quot;test_data&quot;: &quot;test_demo.json&quot;,\n    &quot;lang_pairs&quot;: &quot;eng_Latn-deu_Latn&quot;\n}\n</code></pre>\n<p>While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?</p>\n",
         "2024-08-09 14:46:54",
         "1",
         "148",
         "1",
         "78854613.0",
         "<p>While I can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided JSON files), my personal recommendation would be to fine-tune NLLB in the <code>transformers</code> library, specifically using the <code>Seq2SeqTrainer</code>.</p>\n<p>I did this before for multiple models, including NLLB, check out this repository: <a href=\"https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/\" rel=\"nofollow noreferrer\">https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/</a></p>\n<p>This way the fine-tuning and inference process for the NLLB model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so:</p>\n<pre><code>tokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=&quot;eng_Latn&quot;, tgt_lang=&quot;deu_Latn&quot;)\n</code></pre>\n<p>and generate translations like this:</p>\n<pre><code>model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(&quot;deu_Latn&quot;)[1], max_length=512)\n</code></pre>\n",
         "0.0",
         "\"facebook/nllb-200-distilled-600M\"\n---\nDATA_CONFIG = \"/content/sample_data/data_config.json\"\nOUTPUT_DIR = \"/content/outputs\"\nMODEL_FOLDER = \"/content/drive/MyDrive/Thesis/nllb-checkpoints\"\nDROP = 0.1\nSRC = \"eng_Latn\"\nTGT = \"deu_Latn\"\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=\"$SRC-$TGT\" \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n---\n/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=\"conf\", config_name=\"base_config\")\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 289, in main\n    train_module = TrainModule(config)\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n---\nMissing data_prefix configuration\n---\n{\n    \"data_prefix\": \"/content/sample_data\",\n    \"train_data\": \"train_demo.json\",\n    \"test_data\": \"test_demo.json\",\n    \"lang_pairs\": \"eng_Latn-deu_Latn\"\n}",
         "transformers\n---\nSeq2SeqTrainer\n---\ntokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=\"eng_Latn\", tgt_lang=\"deu_Latn\")\n---\nmodel.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(\"deu_Latn\")[1], max_length=512)",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation",
         "Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB",
         "While I cant help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB While I cant help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB",
         "nllb finetuning error missing data_prefix configuration englishgerman translation im attempting finetune nllb model scientific translation task english eng_latn german deu_latn followed official guidelines finetuning authors nllb documentation link code block giving error error far understand created demo custom data_configjson looks like official documentation provides information im encountering difficulties applying specific use case someone share detailed guide point helpful resources finetuning nllb",
         "nllb finetune error miss data_prefix configuration englishgerman translation I m attempt finetune nllb model scientific translation task english eng_latn german deu_latn follow official guideline finetune author nllb documentation link code block give error error far understand create demo custom data_configjson look like official documentation provide information I m encounter difficulty apply specific use case someone share detailed guide point helpful resource finetune nllb",
         "nllb finetune error miss dataprefix configuration englishgerman translation I attempt finetune nllb scientific translation task english englatn german deulatn official guideline finetune author nllb documentation link block error error far understand create demo custom dataconfigjson like official documentation provide information I encounter difficulty apply specific case someone share detailed guide point helpful resource finetune nllb",
         "4"
        ],
        [
         "48",
         "78846004",
         "How can I use structured_output with Azure OpenAI with the openai Python library?",
         "<p>I want to use structured output with Azure OpenAI.</p>\n<p>I tried the following code, based on the code given in <a href=\"https://openai.com/index/introducing-structured-outputs-in-the-api/\" rel=\"nofollow noreferrer\">https://openai.com/index/introducing-structured-outputs-in-the-api/</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=&quot;gpt-4omini-2024-07-18-name&quot;,\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful math tutor.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;solve 8x + 31 = 2&quot;},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n</code></pre>\n<p>I get the error:</p>\n<pre><code>openai.BadRequestError: Error code: 400:\n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Invalid parameter: response_format must be one of json_object, text.&quot;,\n        &quot;type&quot;: &quot;invalid_request_error&quot;,\n        &quot;param&quot;: &quot;response_format&quot;,\n        &quot;code&quot;: &quot;None&quot;\n    }\n}\n</code></pre>\n<p>How to fix it?</p>\n<p>I ran <code>pip install -U openai</code>: I use <code>openai==1.40.1</code> and Python 3.11.</p>\n<hr />\n<p>I also tried <a href=\"https://cookbook.openai.com/examples/structured_outputs_intro\" rel=\"nofollow noreferrer\">https://cookbook.openai.com/examples/structured_outputs_intro</a> using  using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message:</p>\n<pre><code>from openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = &quot;https://&lt;your-resource-name&gt;.openai.azure.com&quot;\napi_key = &quot;&lt;your-api-key&gt;&quot;\ndeployment_name = &quot;&lt;your-deployment-name&gt;&quot; # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f&quot;{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01&quot;\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            &quot;role&quot;: &quot;system&quot;,\n            &quot;content&quot;: math_tutor_prompt\n        },\n        {\n            &quot;role&quot;: &quot;user&quot;,\n            &quot;content&quot;: question\n        }\n    ],\n    response_format={\n        &quot;type&quot;: &quot;json_schema&quot;,\n        &quot;json_schema&quot;: {\n            &quot;name&quot;: &quot;math_reasoning&quot;,\n            &quot;schema&quot;: {\n                &quot;type&quot;: &quot;object&quot;,\n                &quot;properties&quot;: {\n                    &quot;steps&quot;: {\n                        &quot;type&quot;: &quot;array&quot;,\n                        &quot;items&quot;: {\n                            &quot;type&quot;: &quot;object&quot;,\n                            &quot;properties&quot;: {\n                                &quot;explanation&quot;: {&quot;type&quot;: &quot;string&quot;},\n                                &quot;output&quot;: {&quot;type&quot;: &quot;string&quot;}\n                            },\n                            &quot;required&quot;: [&quot;explanation&quot;, &quot;output&quot;],\n                            &quot;additionalProperties&quot;: False\n                        }\n                    },\n                    &quot;final_answer&quot;: {&quot;type&quot;: &quot;string&quot;}\n                },\n                &quot;required&quot;: [&quot;steps&quot;, &quot;final_answer&quot;],\n                &quot;additionalProperties&quot;: False\n            },\n            &quot;strict&quot;: True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = &quot;how can I solve 8x + 7 = -23&quot;\n\nresult = get_math_solution(question)\n\nprint(result.content)\n</code></pre>\n",
         "2024-08-07 23:14:19",
         "0",
         "1201",
         "2",
         "78946352.0",
         "<p>Using <code>gpt-4o-2024-08-06</code>, which finally got deployed today (2024-09-03) on Azure, made it work. Code example from <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python-secure\" rel=\"nofollow noreferrer\">learn.microsoft.com</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = &quot;https://your-azure-openai-endpoint.com&quot;\napi_key = &quot;your-azure-openai-key&quot;\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Extract the event information.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Alice and Bob are going to a science fair on Friday.&quot;},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n</code></pre>\n<p>output:</p>\n<pre><code>name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  &quot;id&quot;: &quot;chatcmpl-A3XDRVolXpjeAAQIGddswI990weid&quot;,\n  &quot;choices&quot;: [\n    {\n      &quot;finish_reason&quot;: &quot;stop&quot;,\n      &quot;index&quot;: 0,\n      &quot;logprobs&quot;: null,\n      &quot;message&quot;: {\n        &quot;content&quot;: &quot;{\\&quot;name\\&quot;:\\&quot;Science Fair\\&quot;,\\&quot;date\\&quot;:\\&quot;Friday\\&quot;,\\&quot;participants\\&quot;:[\\&quot;Alice\\&quot;,\\&quot;Bob\\&quot;]}&quot;,\n        &quot;refusal&quot;: null,\n        &quot;role&quot;: &quot;assistant&quot;,\n        &quot;function_call&quot;: null,\n        &quot;tool_calls&quot;: [],\n        &quot;parsed&quot;: {\n          &quot;name&quot;: &quot;Science Fair&quot;,\n          &quot;date&quot;: &quot;Friday&quot;,\n          &quot;participants&quot;: [\n            &quot;Alice&quot;,\n            &quot;Bob&quot;\n          ]\n        }\n      },\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ],\n  &quot;created&quot;: 1725406029,\n  &quot;model&quot;: &quot;gpt-4o-2024-08-06&quot;,\n  &quot;object&quot;: &quot;chat.completion&quot;,\n  &quot;service_tier&quot;: null,\n  &quot;system_fingerprint&quot;: &quot;fp_b2ffeb31ff&quot;,\n  &quot;usage&quot;: {\n    &quot;completion_tokens&quot;: 17,\n    &quot;prompt_tokens&quot;: 32,\n    &quot;total_tokens&quot;: 49\n  },\n  &quot;prompt_filter_results&quot;: [\n    {\n      &quot;prompt_index&quot;: 0,\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ]\n}\n</code></pre>\n<p>Tested with Python 3.11.7 and openai==1.43.0.</p>\n",
         "0.0",
         "from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4omini-2024-07-18-name\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n        {\"role\": \"user\", \"content\": \"solve 8x + 31 = 2\"},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n---\nopenai.BadRequestError: Error code: 400:\n{\n    \"error\": {\n        \"message\": \"Invalid parameter: response_format must be one of json_object, text.\",\n        \"type\": \"invalid_request_error\",\n        \"param\": \"response_format\",\n        \"code\": \"None\"\n    }\n}\n---\npip install -U openai\n---\nopenai==1.40.1\n---\nfrom openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = \"https://<your-resource-name>.openai.azure.com\"\napi_key = \"<your-api-key>\"\ndeployment_name = \"<your-deployment-name>\" # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01\"\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": math_tutor_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": question\n        }\n    ],\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"math_reasoning\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"steps\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"explanation\": {\"type\": \"string\"},\n                                \"output\": {\"type\": \"string\"}\n                            },\n                            \"required\": [\"explanation\", \"output\"],\n                            \"additionalProperties\": False\n                        }\n                    },\n                    \"final_answer\": {\"type\": \"string\"}\n                },\n                \"required\": [\"steps\", \"final_answer\"],\n                \"additionalProperties\": False\n            },\n            \"strict\": True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = \"how can I solve 8x + 7 = -23\"\n\nresult = get_math_solution(question)\n\nprint(result.content)",
         "gpt-4o-2024-08-06\n---\nfrom pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = \"https://your-azure-openai-endpoint.com\"\napi_key = \"your-azure-openai-key\"\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n---\nname='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  \"id\": \"chatcmpl-A3XDRVolXpjeAAQIGddswI990weid\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"{\\\"name\\\":\\\"Science Fair\\\",\\\"date\\\":\\\"Friday\\\",\\\"participants\\\":[\\\"Alice\\\",\\\"Bob\\\"]}\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": [],\n        \"parsed\": {\n          \"name\": \"Science Fair\",\n          \"date\": \"Friday\",\n          \"participants\": [\n            \"Alice\",\n            \"Bob\"\n          ]\n        }\n      },\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"created\": 1725406029,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": \"fp_b2ffeb31ff\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 32,\n    \"total_tokens\": 49\n  },\n  \"prompt_filter_results\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ]\n}",
         "How can I use structured_output with Azure OpenAI with the openai Python library",
         "I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message",
         "Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message",
         "use structured_output azure openai openai python library want use structured output azure openai tried following code based code given get error fix ran use python 311 also tried using using azure+ gpt4o mini 20240718 didnt work either error message",
         "use structured_output azure openai openai python library want use structured output azure openai try follow code base code given get error fix run use python 311 also try use use azure+ gpt4o mini 20240718 do not work either error message",
         "structuredoutput azure openai openai python library structured azure openai base given get error fix run python 311 also azure gpt4o mini 20240718 do not either error message",
         "4"
        ],
        [
         "49",
         "78836208",
         "Removing bi-grams after tokenization for TfidfVectorizer",
         "<p>I'm attempting to remove bi-grams that are created by <code>TfidfVectorizer</code>.  I'm using <code>text.TfidfVectorizer</code> so that I can use my own preprocessor function.</p>\n<p>Test strings and preprocessor function:</p>\n<pre><code>doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, &quot;&quot;)\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n</code></pre>\n<p>My <code>TfidfVectorizer</code> instantiation and <code>fit_transform</code>:</p>\n<pre><code>from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n</code></pre>\n<p>Here is my error:</p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in &lt;cell line: 5&gt;()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----&gt; 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-&gt; 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 &quot;Upper case characters found in&quot;\n   1332                 &quot; vocabulary while 'lowercase'&quot;\n   1333                 &quot; is True. These entries will not&quot;\n   1334                 &quot; be matched with any documents&quot;\n   1335             )\n   1336             break\n-&gt; 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-&gt; 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--&gt; 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object\n</code></pre>\n<p>How to resolve it?</p>\n",
         "2024-08-05 19:46:40",
         "1",
         "38",
         "1",
         "78837616.0",
         "<p>The preprocessor should handle documents, not the whole corpus. (The clues are the &quot;expected string&quot; in the error, and the fact that <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"nofollow noreferrer\">the <code>TfidfVectorizer</code> docs</a> refer to &quot;the preprocessing (string transformation) stage&quot;. The docs could definitely be clearer.)</p>\n<p>This should fix it:</p>\n<pre><code>def remove_bigrams(doc: str) -&gt; str:\n    &quot;&quot;&quot;Remove certain bi-grams from a document.&quot;&quot;&quot;\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, &quot;&quot;)\n    return doc\n</code></pre>\n",
         "2.0",
         "TfidfVectorizer\n---\ntext.TfidfVectorizer\n---\ndoc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, \"\")\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n---\nTfidfVectorizer\n---\nfit_transform\n---\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n---\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in <cell line: 5>()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----> 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-> 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 \"Upper case characters found in\"\n   1332                 \" vocabulary while 'lowercase'\"\n   1333                 \" is True. These entries will not\"\n   1334                 \" be matched with any documents\"\n   1335             )\n   1336             break\n-> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-> 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--> 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object",
         "TfidfVectorizer\n---\ndef remove_bigrams(doc: str) -> str:\n    \"\"\"Remove certain bi-grams from a document.\"\"\"\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, \"\")\n    return doc",
         "Removing bigrams after tokenization for TfidfVectorizer",
         "Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it",
         "The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "Removing bigrams after tokenization for TfidfVectorizer Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "Removing bigrams after tokenization for TfidfVectorizer Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it",
         "removing bigrams tokenization tfidfvectorizer im attempting remove bigrams created im using use preprocessor function test strings preprocessor function instantiation error resolve",
         "remove bigrams tokenization tfidfvectorizer I m attempt remove bigram create I m use use preprocessor function test string preprocessor function instantiation error resolve",
         "remove bigrams tokenization tfidfvectorizer I attempt remove bigram create I preprocessor function test preprocessor function instantiation error resolve",
         "4"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 8510
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AcceptedAnswerBody</th>\n",
       "      <th>AcceptedAnswerScore</th>\n",
       "      <th>...</th>\n",
       "      <th>Answer_Code</th>\n",
       "      <th>Title_Clean</th>\n",
       "      <th>Body_Clean</th>\n",
       "      <th>AcceptedAnswerBody_Clean</th>\n",
       "      <th>combination_text</th>\n",
       "      <th>combination_text_only_question</th>\n",
       "      <th>combination_text_only_question_no_stopw</th>\n",
       "      <th>combination_text_only_question_lemma</th>\n",
       "      <th>combination_text_only_question_lemma_no_noise</th>\n",
       "      <th>cluster_kmean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>2025-04-02 05:56:11</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>79552218.0</td>\n",
       "      <td>&lt;p&gt;The configuration file is missing the 'labe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>labels_to_ignore:\\n    - O\\n---\\nnlp_engine_na...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>Im using spaCy with the pl_core_news_lg model ...</td>\n",
       "      <td>The configuration file is missing the labels_t...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79548202</td>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>&lt;p&gt;I understand the -100 label id is used so t...</td>\n",
       "      <td>2025-04-01 09:21:17</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>79551169.0</td>\n",
       "      <td>&lt;p&gt;The author of the tutorial you mentioned se...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100\\n---\\nignore_index\\n---\\nignore_index\\n--...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>I understand the 100 label id is used so that ...</td>\n",
       "      <td>The author of the tutorial you mentioned sets ...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>gpt2 models huggingface 100 label index traini...</td>\n",
       "      <td>gpt2 model huggingface 100 label index trainin...</td>\n",
       "      <td>gpt2 huggingface 100 label index training inst...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79523269</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>&lt;p&gt;I am trying to import gensim into colab.&lt;/p...</td>\n",
       "      <td>2025-03-20 14:36:02</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>79523777.0</td>\n",
       "      <td>&lt;p&gt;You have to restart the session for the und...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>numpy\\n---\\nnumpy\\n---\\nscipy</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>I am trying to import gensim into colab I get ...</td>\n",
       "      <td>You have to restart the session for the underl...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "      <td>trouble getting importing gensim work colab tr...</td>\n",
       "      <td>trouble getting import gensim work colab try i...</td>\n",
       "      <td>trouble getting import gensim colab import gen...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this &lt;a href...</td>\n",
       "      <td>2025-03-11 14:50:31</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>79501337.0</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>I am running the code found on this site in my...</td>\n",
       "      <td>I cant test it but I checked source code and i...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>store images instead showing server running co...</td>\n",
       "      <td>store image instead show server run code find ...</td>\n",
       "      <td>store image instead show server run site serve...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79482283</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>&lt;p&gt;I am using presidio/langchain_experimental ...</td>\n",
       "      <td>2025-03-03 22:27:07</td>\n",
       "      <td>4</td>\n",
       "      <td>230</td>\n",
       "      <td>2</td>\n",
       "      <td>79495969.0</td>\n",
       "      <td>&lt;p&gt;After some test I was able to find the solu...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>config = {\\n    \"nlp_engine_name\": \"spacy\",\\n ...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>I am using presidio/langchain_experimental to ...</td>\n",
       "      <td>After some test I was able to find the solutio...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>62328</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>&lt;p&gt;input: phrase 1, phrase 2&lt;/p&gt;\\n\\n&lt;p&gt;output:...</td>\n",
       "      <td>2008-09-15 12:26:42</td>\n",
       "      <td>65</td>\n",
       "      <td>49889</td>\n",
       "      <td>11</td>\n",
       "      <td>63076.0</td>\n",
       "      <td>&lt;hr&gt;\\n\\n&lt;p&gt;You might want to check out this pa...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>input phrase 1 phrase 2 output semantic simila...</td>\n",
       "      <td>You might want to check out this paper Sentenc...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>algorithm tells semantic similarity two phrase...</td>\n",
       "      <td>algorithm tell semantic similarity two phrase ...</td>\n",
       "      <td>algorithm tell semantic similarity phrase inpu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8506</th>\n",
       "      <td>42489</td>\n",
       "      <td>How to implement a \"related\" degree measure al...</td>\n",
       "      <td>&lt;p&gt;I was going to Ask a Question earlier today...</td>\n",
       "      <td>2008-09-03 20:21:04</td>\n",
       "      <td>8</td>\n",
       "      <td>456</td>\n",
       "      <td>2</td>\n",
       "      <td>42532.0</td>\n",
       "      <td>&lt;p&gt;One such way to implement such an algorithm...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>I was going to Ask a Question earlier today wh...</td>\n",
       "      <td>One such way to implement such an algorithm wo...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>implement related degree measure algorithm goi...</td>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03 10:36:13</td>\n",
       "      <td>118</td>\n",
       "      <td>33200</td>\n",
       "      <td>11</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>&lt;p&gt;Actually what Google does is very much non-...</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>How do you implement a Did you mean</td>\n",
       "      <td>Possible Duplicate How does the Google Did you...</td>\n",
       "      <td>Actually what Google does is much nontrivial a...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8508</th>\n",
       "      <td>36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31 01:08:48</td>\n",
       "      <td>3</td>\n",
       "      <td>5661</td>\n",
       "      <td>6</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>&lt;p&gt;Citation from Vista &lt;a href=\"http://blogs.m...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>my primary language is spanish but I use all m...</td>\n",
       "      <td>Citation from Vista speech recognition blog In...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>vista speech recognition multiple languages pr...</td>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22 22:45:10</td>\n",
       "      <td>27</td>\n",
       "      <td>6484</td>\n",
       "      <td>9</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>&lt;p&gt;We developed exactly what you are looking f...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>Natural language date/time parser for NET</td>\n",
       "      <td>Does anyone know of a NET date/time parser sim...</td>\n",
       "      <td>We developed exactly what you are looking for ...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "      <td>natural language date/time parser net anyone k...</td>\n",
       "      <td>natural language date / time parser net anyone...</td>\n",
       "      <td>natural language date time parser net anyone n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8510 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      QuestionId                                              Title  \\\n",
       "0       79549787  Why does Presidio with spacy nlp engine not re...   \n",
       "1       79548202  GPT-2 and other models from huggingface -100 l...   \n",
       "2       79523269  Trouble getting importing gensim to work in colab   \n",
       "3       79501178        Store images instead of showing in a server   \n",
       "4       79482283  Presidio with Langchain Experimental does not ...   \n",
       "...          ...                                                ...   \n",
       "8505       62328  Is there an algorithm that tells the semantic ...   \n",
       "8506       42489  How to implement a \"related\" degree measure al...   \n",
       "8507       41424             How do you implement a \"Did you mean\"?   \n",
       "8508       36533     Vista speech recognition in multiple languages   \n",
       "8509       23689        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                   Body         CreationDate  \\\n",
       "0     <p>I'm using spaCy with the pl_core_news_lg mo...  2025-04-02 05:56:11   \n",
       "1     <p>I understand the -100 label id is used so t...  2025-04-01 09:21:17   \n",
       "2     <p>I am trying to import gensim into colab.</p...  2025-03-20 14:36:02   \n",
       "3     <p>I am running the code found on this <a href...  2025-03-11 14:50:31   \n",
       "4     <p>I am using presidio/langchain_experimental ...  2025-03-03 22:27:07   \n",
       "...                                                 ...                  ...   \n",
       "8505  <p>input: phrase 1, phrase 2</p>\\n\\n<p>output:...  2008-09-15 12:26:42   \n",
       "8506  <p>I was going to Ask a Question earlier today...  2008-09-03 20:21:04   \n",
       "8507  <blockquote>\\n  <p><strong>Possible Duplicate:...  2008-09-03 10:36:13   \n",
       "8508  <p>my primary language is spanish, but I use a...  2008-08-31 01:08:48   \n",
       "8509  <p>Does anyone know of a .NET date/time parser...  2008-08-22 22:45:10   \n",
       "\n",
       "      Score  ViewCount  AnswerCount  AcceptedAnswerId  \\\n",
       "0         0         68            1        79552218.0   \n",
       "1         0         46            1        79551169.0   \n",
       "2         0        125            1        79523777.0   \n",
       "3         0         36            1        79501337.0   \n",
       "4         4        230            2        79495969.0   \n",
       "...     ...        ...          ...               ...   \n",
       "8505     65      49889           11           63076.0   \n",
       "8506      8        456            2           42532.0   \n",
       "8507    118      33200           11           41448.0   \n",
       "8508      3       5661            6           36684.0   \n",
       "8509     27       6484            9          631134.0   \n",
       "\n",
       "                                     AcceptedAnswerBody  AcceptedAnswerScore  \\\n",
       "0     <p>The configuration file is missing the 'labe...                  1.0   \n",
       "1     <p>The author of the tutorial you mentioned se...                  1.0   \n",
       "2     <p>You have to restart the session for the und...                  1.0   \n",
       "3     <p>I can't test it but ...</p>\\n<p>I checked <...                  1.0   \n",
       "4     <p>After some test I was able to find the solu...                 -2.0   \n",
       "...                                                 ...                  ...   \n",
       "8505  <hr>\\n\\n<p>You might want to check out this pa...                 44.0   \n",
       "8506  <p>One such way to implement such an algorithm...                  5.0   \n",
       "8507  <p>Actually what Google does is very much non-...                 87.0   \n",
       "8508  <p>Citation from Vista <a href=\"http://blogs.m...                  8.0   \n",
       "8509  <p>We developed exactly what you are looking f...                 12.0   \n",
       "\n",
       "      ...                                        Answer_Code  \\\n",
       "0     ...  labels_to_ignore:\\n    - O\\n---\\nnlp_engine_na...   \n",
       "1     ...  -100\\n---\\nignore_index\\n---\\nignore_index\\n--...   \n",
       "2     ...                      numpy\\n---\\nnumpy\\n---\\nscipy   \n",
       "3     ...  matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...   \n",
       "4     ...  config = {\\n    \"nlp_engine_name\": \"spacy\",\\n ...   \n",
       "...   ...                                                ...   \n",
       "8505  ...                                                      \n",
       "8506  ...                                                      \n",
       "8507  ...                                                      \n",
       "8508  ...                                                      \n",
       "8509  ...                                                      \n",
       "\n",
       "                                            Title_Clean  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in colab   \n",
       "3           Store images instead of showing in a server   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507                How do you implement a Did you mean   \n",
       "8508     Vista speech recognition in multiple languages   \n",
       "8509          Natural language date/time parser for NET   \n",
       "\n",
       "                                             Body_Clean  \\\n",
       "0     Im using spaCy with the pl_core_news_lg model ...   \n",
       "1     I understand the 100 label id is used so that ...   \n",
       "2     I am trying to import gensim into colab I get ...   \n",
       "3     I am running the code found on this site in my...   \n",
       "4     I am using presidio/langchain_experimental to ...   \n",
       "...                                                 ...   \n",
       "8505  input phrase 1 phrase 2 output semantic simila...   \n",
       "8506  I was going to Ask a Question earlier today wh...   \n",
       "8507  Possible Duplicate How does the Google Did you...   \n",
       "8508  my primary language is spanish but I use all m...   \n",
       "8509  Does anyone know of a NET date/time parser sim...   \n",
       "\n",
       "                               AcceptedAnswerBody_Clean  \\\n",
       "0     The configuration file is missing the labels_t...   \n",
       "1     The author of the tutorial you mentioned sets ...   \n",
       "2     You have to restart the session for the underl...   \n",
       "3     I cant test it but I checked source code and i...   \n",
       "4     After some test I was able to find the solutio...   \n",
       "...                                                 ...   \n",
       "8505  You might want to check out this paper Sentenc...   \n",
       "8506  One such way to implement such an algorithm wo...   \n",
       "8507  Actually what Google does is much nontrivial a...   \n",
       "8508  Citation from Vista speech recognition blog In...   \n",
       "8509  We developed exactly what you are looking for ...   \n",
       "\n",
       "                                       combination_text  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in co...   \n",
       "3     Store images instead of showing in a server I ...   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507  How do you implement a Did you mean Possible D...   \n",
       "8508  Vista speech recognition in multiple languages...   \n",
       "8509  Natural language date/time parser for NET Does...   \n",
       "\n",
       "                         combination_text_only_question  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in co...   \n",
       "3     Store images instead of showing in a server I ...   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507  How do you implement a Did you mean Possible D...   \n",
       "8508  Vista speech recognition in multiple languages...   \n",
       "8509  Natural language date/time parser for NET Does...   \n",
       "\n",
       "                combination_text_only_question_no_stopw  \\\n",
       "0     presidio spacy nlp engine recognize organizati...   \n",
       "1     gpt2 models huggingface 100 label index traini...   \n",
       "2     trouble getting importing gensim work colab tr...   \n",
       "3     store images instead showing server running co...   \n",
       "4     presidio langchain experimental detect polish ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tells semantic similarity two phrase...   \n",
       "8506  implement related degree measure algorithm goi...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple languages pr...   \n",
       "8509  natural language date/time parser net anyone k...   \n",
       "\n",
       "                   combination_text_only_question_lemma  \\\n",
       "0     presidio spacy nlp engine recognize organizati...   \n",
       "1     gpt2 model huggingface 100 label index trainin...   \n",
       "2     trouble getting import gensim work colab try i...   \n",
       "3     store image instead show server run code find ...   \n",
       "4     presidio langchain experimental detect polish ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tell semantic similarity two phrase ...   \n",
       "8506  implement relate degree measure algorithm go a...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple language pri...   \n",
       "8509  natural language date / time parser net anyone...   \n",
       "\n",
       "          combination_text_only_question_lemma_no_noise cluster_kmean  \n",
       "0     presidio spacy nlp engine recognize organizati...             9  \n",
       "1     gpt2 huggingface 100 label index training inst...             2  \n",
       "2     trouble getting import gensim colab import gen...             4  \n",
       "3     store image instead show server run site serve...             4  \n",
       "4     presidio langchain experimental detect polish ...             9  \n",
       "...                                                 ...           ...  \n",
       "8505  algorithm tell semantic similarity phrase inpu...             0  \n",
       "8506  implement relate degree measure algorithm go a...             1  \n",
       "8507  implement mean possible duplicate google mean ...             8  \n",
       "8508  vista speech recognition multiple language pri...             1  \n",
       "8509  natural language date time parser net anyone n...             1  \n",
       "\n",
       "[8510 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_questions['keywords_fromBert'] = df_post_questions['combination_text_only_question_lemma_no_noise'].apply(extract_keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerScore",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Question_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Answer_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerBody_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question_no_stopw",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question_lemma",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_only_question_lemma_no_noise",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cluster_kmean",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "keywords_fromBert",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a69f5542-3d2f-4705-b824-d1780711c8da",
       "rows": [
        [
         "0",
         "79549787",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?",
         "<p>I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER):</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;pl_core_news_lg&quot;)\ntext = &quot;Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.&quot;\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n</code></pre>\n<p>Output:</p>\n<pre><code>[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n</code></pre>\n<p>However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities.</p>\n<pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=&quot;path_to_my_file/nlp_config.yaml&quot;) \nnlp_engine = provider.create_engine()\n\nprint(f&quot;Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}&quot;)\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[&quot;pl&quot;])\nregistry.load_predefined_recognizers([&quot;pl&quot;])\n\nprint(f&quot;Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}&quot;)\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, &quot;pl&quot;)\n\nfor entity in results:\n    print(f&quot;Found entity: {entity.entity_type} with score {entity.score}&quot;)\n</code></pre>\n<p>Output:</p>\n<pre><code>Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n</code></pre>\n<p>Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text.</p>\n<p>My config file:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n<p>Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?</p>\n",
         "2025-04-02 05:56:11",
         "0",
         "68",
         "1",
         "79552218.0",
         "<p>The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine :</p>\n<pre><code>  labels_to_ignore:\n    - O\n</code></pre>\n<p>On your configuration it would look like this:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n",
         "1.0",
         "import spacy\n\nnlp = spacy.load(\"pl_core_news_lg\")\ntext = \"Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.\"\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n---\n[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n---\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=\"path_to_my_file/nlp_config.yaml\") \nnlp_engine = provider.create_engine()\n\nprint(f\"Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}\")\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[\"pl\"])\nregistry.load_predefined_recognizers([\"pl\"])\n\nprint(f\"Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}\")\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, \"pl\")\n\nfor entity in results:\n    print(f\"Found entity: {entity.entity_type} with score {entity.score}\")\n---\nSupported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "labels_to_ignore:\n    - O\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does",
         "Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them",
         "The configuration file is missing the labels_to_ignore field stating that no entities should be ignored in the nlp engine On your configuration it would look like this",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them The configuration file is missing the labels_to_ignore field stating that no entities should be ignored in the nlp engine On your configuration it would look like this",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does Im using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and peoples names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though ORGANIZATION and PL_PESEL are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them",
         "presidio spacy nlp engine recognize organizations pesel spacy im using spacy pl_core_news_lg model extract named entities polish text correctly detects organizations org peoples names per output however use presidio pl_core_news_lg model configuration file recognizers correctly detect organizations org pesel numbers even though appear list supported entities output even though organization pl_pesel listed org listed nlp engine supported recognizers presidio detect correctly text config file presidio fail detect organizations org pesel numbers pl_pesel spacy correctly detects",
         "presidio spacy nlp engine recognize organization pesel spacy I m use spacy pl_core_news_lg model extract name entity polish text correctly detect organization org people name per output however use presidio pl_core_news_lg model configuration file recognizer correctly detect organization org pesel number even though appear list support entity output even though organization pl_pesel list org list nlp engine support recognizer presidio detect correctly text config file presidio fail detect organization org pesel number pl_pesel spacy correctly detect",
         "presidio spacy nlp engine recognize organization pesel spacy I spacy plcorenewslg extract name entity polish correctly detect organization org people name per however presidio plcorenewslg configuration recognizer correctly detect organization org pesel number even though appear support entity even though organization plpesel org nlp engine support recognizer presidio detect correctly config presidio fail detect organization org pesel number plpesel spacy correctly detect",
         "9",
         "organization pesel,recognizer presidio,spacy plcorenewslg,detect organization,nlp engine"
        ],
        [
         "1",
         "79548202",
         "GPT-2 and other models from huggingface -100 label index for training, instead of pad token",
         "<p>I understand the -100 label id is used so that the predictions for these are not included when calculating the loss.</p>\n<p>However on <a href=\"https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16#bert2gpt2-summarization-with-%F0%9F%A4%97-encoderdecoder-framework\" rel=\"nofollow noreferrer\">huggingface</a>, they state\n&quot;complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not&quot;, when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument &quot;ignore_index&quot;.</p>\n<p>Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same?</p>\n<p>The way it is written makes me think there is some benefit, but the description of &quot;ignore_index&quot; appears to achieve what is wanted.</p>\n",
         "2025-04-01 09:21:17",
         "0",
         "46",
         "1",
         "79551169.0",
         "<p>The author of the tutorial you mentioned sets it to <code>-100</code> <strong>and</strong> uses <code>ignore_index</code> to save a few lines of code. You don't see the line where the author pass something to <code>ignore_index</code> because it has a default value. The default value of <code>ignore_index</code> for <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\" rel=\"nofollow noreferrer\">nn.CrossEntropyLoss</a> is <code>-100</code>. Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.</p>\n",
         "1.0",
         "",
         "-100\n---\nignore_index\n---\nignore_index\n---\nignore_index\n---\n-100",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token",
         "I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted",
         "The author of the tutorial you mentioned sets it to and uses to save a few lines of code You dont see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you dont have to pass the pad token id from tokenizer down to the loss function",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted The author of the tutorial you mentioned sets it to and uses to save a few lines of code You dont see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you dont have to pass the pad token id from tokenizer down to the loss function",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted",
         "gpt2 models huggingface 100 label index training instead pad token understand 100 label id used predictions included calculating loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label excluded replacing pad tokens implementation use nncrossentropyloss argument ignore_index benefit changing id 100 opposed adding argument ignore_index loss setting pad token id results way written makes think benefit description ignore_index appears achieve wanted",
         "gpt2 model huggingface 100 label index training instead pad token understand 100 label i d use prediction include calculate loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label exclude replace pad token implementation use nncrossentropyloss argument ignore_index benefit change i d 100 oppose add argument ignore_index loss set pad token i d result way write make think benefit description ignore_index appear achieve wanted",
         "gpt2 huggingface 100 label index training instead pad token understand 100 label i d prediction include calculate loss however huggingface state complicated comprehension padtokenid alone good enough whether label exclude replace pad token implementation nncrossentropyloss argument ignoreindex benefit change i d 100 oppose add argument ignoreindex loss set pad token i d write make think benefit description ignoreindex appear achieve wanted",
         "2",
         "ignoreindex loss,label prediction,pad token,description ignoreindex,gpt2"
        ],
        [
         "2",
         "79523269",
         "Trouble getting importing gensim to work in colab",
         "<p>I am trying to import gensim into colab.</p>\n<pre><code>!pip install gensim\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--&gt; 367             msg = (&quot;The current Numpy installation ({!r}) fails to &quot;\n    368                    &quot;pass simple sanity checks. This can be caused for example &quot;\n    369                    &quot;by incorrect BLAS library being linked in, or by mixing &quot;\n\nModuleNotFoundError: No module named 'numpy.char'\n</code></pre>\n<p>my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks</p>\n",
         "2025-03-20 14:36:02",
         "0",
         "125",
         "1",
         "79523777.0",
         "<p>You have to restart the session for the underlying runtime to notice the package changes. See: <a href=\"https://stackoverflow.com/a/79518359/130288\">https://stackoverflow.com/a/79518359/130288</a></p>\n<p>I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded <code>numpy</code>/etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade.</p>\n<p>But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two.</p>\n<p>Explicitly restarting after the Gensim-install &amp; <code>numpy</code>/<code>scipy</code> downgrades resolves the errors.</p>\n",
         "1.0",
         "!pip install gensim\n---\n/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--> 367             msg = (\"The current Numpy installation ({!r}) fails to \"\n    368                    \"pass simple sanity checks. This can be caused for example \"\n    369                    \"by incorrect BLAS library being linked in, or by mixing \"\n\nModuleNotFoundError: No module named 'numpy.char'",
         "numpy\n---\nnumpy\n---\nscipy",
         "Trouble getting importing gensim to work in colab",
         "I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks",
         "You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadnt yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadnt yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks",
         "trouble getting importing gensim work colab trying import gensim colab get following error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string related issue thanks",
         "trouble getting import gensim work colab try import gensim colab get follow error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string relate issue thank",
         "trouble getting import gensim colab import gensim colab get error numpy version 202 downgrade numpy another version like say 1264 get error always numpy relate issue thank",
         "4",
         "getting import,numpy relate,gensim colab,colab error,downgrade numpy"
        ],
        [
         "3",
         "79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">site</a> in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?</p>\n",
         "2025-03-11 14:50:31",
         "0",
         "36",
         "1",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n",
         "1.0",
         "server\n---\nSSH\n---\nskip_tokens = [1]  # skip the special token for the start of the text <s>\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = \"playing guitar, hiking, and spending time with his family.\"\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)",
         "matplotlib\n---\nshow=True\n---\nfig, ax\n---\nimport matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(\"output.png\")\n#plt.show()  # eventually show it after saving\n---\nfig\n---\nfig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(\"output.png\")",
         "Store images instead of showing in a server",
         "I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them",
         "I cant test it but I checked source code and it uses for this If you remove then it shouldnt show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them I cant test it but I checked source code and it uses for this If you remove then it shouldnt show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them",
         "store images instead showing server running code found site server would like store images instead showing since connected remotely ssh connection via connection code instance one store files locally instead showing",
         "store image instead show server run code find site server would like store image instead show since connect remotely ssh connection via connection code instance one store file locally instead show",
         "store image instead show server run site server would like store image instead show since connect remotely ssh connection via connection instance store locally instead show",
         "4",
         "connection instance,image instead,store locally,remotely ssh,store image"
        ],
        [
         "4",
         "79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03 22:27:07",
         "4",
         "230",
         "2",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n",
         "-2.0",
         "from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"pl\", \"model_name\": \"pl_core_news_lg\"}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                               languages_config=config)\n\ntext = \"Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\"\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(\"Anonymized text:\", anonymized_result)\nprint(\"Deanonymized text:\", deanonymized_result)\nprint(\"Map:\", anonymizer_tool.deanonymizer_mapping)\nprint(\"Anonymized text:\", anon_result)\n---\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\npython -m spacy download pl_core_news_lg\n---\nanonymizer_tool = PresidioReversibleAnonymizer()\n---\nAnonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n---\nnlp = spacy.load(\"pl_core_news_lg\")\ndoc = nlp(text)\n---\nJan Kowalski persName\nWarszawie placeName",
         "config = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": 'pl', \"model_name\": \"pl_core_news_lg\"}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=\"pl\",\n    supported_entities=[\"persName\"]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"CREDIT_CARD\"], languages_config=config)\n---\nAnonymized text: <persName> mieszka w Warszawie i ma e-mail glenn58@example.org.\n---\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\nMap: {'persName': {'<persName>': 'Jan Kowalski', '<persName_2>': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}\n---\nSpacyRecognizer\n---\nsupported_entities",
         "Presidio with Langchain Experimental does not detect Polish names",
         "I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected",
         "After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCys requirements I believe theres something missing or unclear in the documentation which is causing the misunderstanding",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCys requirements I believe theres something missing or unclear in the documentation which is causing the misunderstanding",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that its the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected",
         "presidio langchain experimental detect polish names using presidio/langchain_experimental anonymize text polish detect names eg jan kowalski code output expected name jan kowalski email address anonymized output remains unchanged installed pl_core_news_lg model using missing something configuration presidio support polish entity recognition properly suggestions make detect names polish interesting thing use output look like mentioned use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio works expected",
         "presidio langchain experimental detect polish name use presidio / langchain_experimental anonymize text polish detect name eg jan kowalski code output expect name jan kowalski email address anonymize output remain unchanged instal pl_core_news_lg model use miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing use output look like mention use spacy output correct guess problem presidio output spacy would like create custom analyzer use spacy presidio work expect",
         "presidio langchain experimental detect polish name presidio langchainexperimental anonymize polish detect name eg jan kowalski expect name jan kowalski email address anonymize remain unchanged instal plcorenewslg miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing like mention spacy correct guess problem presidio spacy would like create custom analyzer spacy presidio expect",
         "9",
         "presidio langchain,anonymize,presidio spacy,polish entity,detect polish"
        ],
        [
         "5",
         "79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22 16:06:11",
         "-1",
         "40",
         "1",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n",
         "1.0",
         "",
         "",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall",
         "Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isnt The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "OpenNLP POSTaggerME and ChunkerME synergy Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isnt The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "OpenNLP POSTaggerME and ChunkerME synergy Im trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasnt produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall",
         "opennlp postaggerme chunkerme synergy im trying use opennlp chunking api chunk portuguese sentence first tokenized sentence using tokenizerme tagged postaggerme used readymade models provided project sentence ivo viu uva postaggerme returns tags propn verb det noun model seems using ud pos tags readymade model chunkerme portuguese followed instructions training first using chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything worked well sentence chunker produced correct tags bnp bvp bnp inp complex sentences hasnt produced good results trying identify could improve chunker training one things noticed difference types tags portuguese corpus bosque 80 seems using portuguese tags example instead propn corpus uses prop instead det uses art seems could lead problems especially since one parameters chunker receives array ud tags trained another type tag writing code creating routine convert portuguese notation ud penn wanted ask indeed impact tool already translation suggestions improving chunker precision/recall",
         "opennlp postaggerme chunkerme synergy I m try use opennlp chunk api chunk portuguese sentence first tokenized sentence use tokenizerme tag postaggerme use readymade model provide project sentence ivo viu uva postaggerme return tag propn verb det noun model seem use ud pos tag readymade model chunkerme portuguese follow instruction training first use chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything work well sentence chunker produce correct tag bnp bvp bnp inp complex sentence have not produce good result try identify could improve chunker training one thing notice difference type tag portuguese corpus bosque 80 seem use portuguese tag example instead propn corpus use prop instead det use art seem could lead problem especially since one parameter chunker receive array ud tag train another type tag write code create routine convert portuguese notation ud penn want ask indeed impact tool already translation suggestion improve chunker precision / recall",
         "opennlp postaggerme chunkerme synergy I opennlp chunk api chunk portuguese first tokenized tokenizerme tag postaggerme readymade provide project ivo viu uva postaggerme return tag propn verb det noun ud pos tag readymade chunkerme portuguese instruction training first chunkerconverter tool convert arvore deitada conll2000 generating chunkertrainerme tool everything chunker produce correct tag bnp bvp bnp inp complex have not produce good identify could improve chunker training thing notice difference type tag portuguese corpus bosque 80 portuguese tag instead propn corpus prop instead det art could lead problem especially since parameter chunker receive array ud tag train another type tag write create routine convert portuguese notation ud penn ask indeed impact tool already translation suggestion improve chunker precision recall",
         "3",
         "postaggerme chunkerme,training chunkerconverter,chunkerme portuguese,parameter chunker,opennlp chunk"
        ],
        [
         "6",
         "79451974",
         "word/ sentence similarities",
         "<p>I am trying to find if a given word/ set of words are similar to a definition.</p>\n<p>Example - Definition - &quot;vegetarian User&quot;</p>\n<p>Now, if I want to check a set of sentences like below</p>\n<pre><code>sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n</code></pre>\n<p>I tried using some sentence transformer like below</p>\n<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n</code></pre>\n<p>But this is not giving me expected results.</p>\n<p>What is the best approach to achieve results like below?</p>\n<pre><code>[False,True,True,False]\n</code></pre>\n<p>Is it doable with nlp/ some other technique?</p>\n",
         "2025-02-19 15:47:45",
         "1",
         "50",
         "1",
         "79461281.0",
         "<p>Yes, it’s definitely doable using NLP! The key here is that you don’t need a full similarity matrix; you want to check if each sentence is semantically similar to the given definition.</p>\n<p>✅ Better Approach:\nEncode both the definition and sentences using a sentence transformer.\nCompute cosine similarity between the definition embedding and each sentence embedding.\nSet a threshold (e.g., 0.6 or 0.7) to determine if they are &quot;similar enough.&quot;</p>\n<pre><code>from sentence_transformers import SentenceTransformer, util\n# Load the pre-trained model\nmodel = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\n\n# Definition and sentences\ndefinition = &quot;vegetarian User&quot;\nsentences = [\n  'vegetarian User',\n  'user sometimes eats chicken',\n  'user is vegetarian',\n  'user only eats fruits',\n  'user likes fish'\n]\n\n# Encode the definition and sentences\ndefinition_embedding = model.encode(definition, convert_to_tensor=True)\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine similarities\nsimilarities = util.cos_sim(definition_embedding, sentence_embeddings)[0]\n\n# Set a threshold for similarity (tune this value as needed)\nthreshold = 0.6\nresults = [sim &gt;= threshold for sim in similarities]\n\n# Print results\nprint(results)  # Example output: [True, False, True, False, False]\n</code></pre>\n<p>💡 Explanation:\nutil.cos_sim computes the cosine similarity between the definition and each sentence.\nThreshold tuning:\nIf the similarity is above the threshold, consider it True.\nAdjust the threshold based on how strict you want the matching.</p>\n<p>🔍 Why the original approach didn’t work:\nmodel.similarity doesn’t exist in the SentenceTransformers API.\nYou were computing a sentence-to-sentence matrix, not definition-to-sentence comparisons.</p>\n",
         "1.0",
         "sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n---\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n---\n[False,True,True,False]",
         "from sentence_transformers import SentenceTransformer, util\n# Load the pre-trained model\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\n\n# Definition and sentences\ndefinition = \"vegetarian User\"\nsentences = [\n  'vegetarian User',\n  'user sometimes eats chicken',\n  'user is vegetarian',\n  'user only eats fruits',\n  'user likes fish'\n]\n\n# Encode the definition and sentences\ndefinition_embedding = model.encode(definition, convert_to_tensor=True)\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine similarities\nsimilarities = util.cos_sim(definition_embedding, sentence_embeddings)[0]\n\n# Set a threshold for similarity (tune this value as needed)\nthreshold = 0.6\nresults = [sim >= threshold for sim in similarities]\n\n# Print results\nprint(results)  # Example output: [True, False, True, False, False]",
         "word/ sentence similarities",
         "I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique",
         "Yes its definitely doable using NLP The key here is that you dont need a full similarity matrix you want to check if each sentence is semantically similar to the given definition Better Approach Encode both the definition and sentences using a sentence transformer Compute cosine similarity between the definition embedding and each sentence embedding Set a threshold eg 06 or 07 to determine if they are similar enough Explanation utilcos_sim computes the cosine similarity between the definition and each sentence Threshold tuning If the similarity is above the threshold consider it True Adjust the threshold based on how strict you want the matching Why the original approach didnt work modelsimilarity doesnt exist in the SentenceTransformers API You were computing a sentencetosentence matrix not definitiontosentence comparisons",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique Yes its definitely doable using NLP The key here is that you dont need a full similarity matrix you want to check if each sentence is semantically similar to the given definition Better Approach Encode both the definition and sentences using a sentence transformer Compute cosine similarity between the definition embedding and each sentence embedding Set a threshold eg 06 or 07 to determine if they are similar enough Explanation utilcos_sim computes the cosine similarity between the definition and each sentence Threshold tuning If the similarity is above the threshold consider it True Adjust the threshold based on how strict you want the matching Why the original approach didnt work modelsimilarity doesnt exist in the SentenceTransformers API You were computing a sentencetosentence matrix not definitiontosentence comparisons",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition Example Definition vegetarian User Now if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results What is the best approach to achieve results like below Is it doable with nlp/ some other technique",
         "word/ sentence similarities trying find given word/ set words similar definition example definition vegetarian user want check set sentences like tried using sentence transformer like giving expected results best approach achieve results like doable nlp/ technique",
         "word/ sentence similarity try find give word/ set word similar definition example definition vegetarian user want check set sentence like try use sentence transformer like give expect result good approach achieve result like doable nlp/ technique",
         "similarity set similar definition definition vegetarian user check set like transformer like expect good approach achieve like doable nlp technique",
         "0",
         "achieve like,check set,nlp technique,doable nlp,similar definition"
        ],
        [
         "7",
         "79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07 02:48:25",
         "-1",
         "45",
         "1",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n",
         "0.0",
         "lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()",
         "",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged",
         "I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue",
         "Based on extra information in the comments Im going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort Im fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldnt find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they dont handle multiple meanings of the word So queen might be a female king as in embeddings party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers dont have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue Based on extra information in the comments Im going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort Im fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldnt find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they dont handle multiple meanings of the word So queen might be a female king as in embeddings party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers dont have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {positive 58749 negative 26643 neutral 9106} Label distribution after random over sampling positive 58749 negative 26643 neutral 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue",
         "underfitting pretrained glove + lstm model accurcacy unchanged sentiment classification using pretrained glove lstm model use google play review scrap resulting 50k++ texts implement random sampling minority classes however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embedding size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set padded 140997 200 total x validation set padded 17625 200 total x testing set padded 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue",
         "underfitte pretraine glove + lstm model accurcacy unchanged sentiment classification use pretraine glove lstm model use google play review scrap result 50k++ text implement random sampling minority class however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling { positive 58749 negative 26643 neutral 9106 } label distribution random sampling positive 58749 negative 26643 neutral 9106 } total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue",
         "underfitte pretraine glove lstm accurcacy unchanged sentiment classification pretraine glove lstm google play review scrap 50k implement random sampling minority class however train lstm training accuracy remain unchanged several epoch insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling positive 58749 negative 26643 neutral 9106 label distribution random sampling positive 58749 negative 26643 neutral 9106 total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set hot 140997 3 total validation set hot 17625 3 total testing set hot 17625 2003 full enter link description highlight issue",
         "7",
         "training accuracy,training set,lstm google,underfitte pretraine,glove lstm"
        ],
        [
         "8",
         "79330283",
         "Can't compile Marian NMT",
         "<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=\"https://marian-nmt.github.io/docs/#installation\" rel=\"nofollow noreferrer\">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>\n<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>\n<p>These are the steps that I followed:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n</code></pre>\n<p>This is the result I had:</p>\n<pre><code>➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2\n</code></pre>\n<p>Please help.</p>\n",
         "2025-01-05 06:04:59",
         "4",
         "68",
         "1",
         "79332711.0",
         "<p>The diagnostic that your build is tripping, <code>Wtemplate-id-cdtor</code>, was introduced\nwith GCC 14.1. It is a warning, not an error, but your build promotes all warnings to\nerrors, so it breaks your build.</p>\n<p>Although your build specifies <code>-std=c++11</code> in <code>src/3rd_party/spdlog/CMakeLists.txt</code>, which\ngenerates the failure, g++-14 emits <code>Wtemplate-id-cdtor</code> to warn you that the code <em>would be</em>\nillegal under the more recent standard c++20 (and later). Then the warning is made an error.</p>\n<p>The warning is made an error by the compile option <code>-Werror</code>. This option is included in the list\nof compile options <code>ALL_WARNINGS</code>, which is created in the top-level <code>marian/CMakeLists.txt</code>\nat line 227 <em>et seq</em>:</p>\n<pre><code># These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n</code></pre>\n<p>and then applied as compile options for the <code>marian</code> library target in <code>src/CMakeLists.txt</code>\nat line 133:</p>\n<pre><code>target_compile_options(marian PRIVATE ${ALL_WARNINGS})\n</code></pre>\n<p>whence the options are operative for the failing compilation of <code>src/CMakeFiles/marian.dir/common/logging.cpp</code>.</p>\n<p>This failure is a bug in the <code>marian</code> repo which you should <a href=\"https://github.com/marian-nmt/marian/issues\" rel=\"nofollow noreferrer\">report to the maintainers</a>, as\nit does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14.</p>\n<p>Pending a fix, you seem to have three interim options to get your build done. Either:</p>\n<ul>\n<li><p>Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:</p>\n<pre><code>/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\n</code></pre>\n</li>\n</ul>\n<p>e.g. make it <code>registry_t(const registry_t&lt;Mutex&gt;&amp;) = delete;</code> in this occurrence.</p>\n<p>Or:</p>\n<ul>\n<li><p>Locally disable <code>-Wtemplate-id-cdtor</code> at each occurrence, e.g:</p>\n<pre><code>#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored &quot;-Wtemplate-id-cdtor&quot;\nregistry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n#pragma GCC diagnostic pop\n</code></pre>\n</li>\n</ul>\n<p>Or:</p>\n<ul>\n<li>Remove <code>-Werror</code> from the <code>ALL_WARNINGS</code> list in <code>marian/CMakeLists.txt</code> so that <code>Wtemplate-id-cdtor</code> remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status).</li>\n</ul>\n<p>I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.</p>\n",
         "4.0",
         "CMakeLists.txt\n---\nset (CMAKE_CXX_STANDARD 11)\n---\ngit clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n---\n➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2",
         "Wtemplate-id-cdtor\n---\n-std=c++11\n---\nsrc/3rd_party/spdlog/CMakeLists.txt\n---\nWtemplate-id-cdtor\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\n# These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n---\nmarian\n---\nsrc/CMakeLists.txt\n---\ntarget_compile_options(marian PRIVATE ${ALL_WARNINGS})\n---\nsrc/CMakeFiles/marian.dir/common/logging.cpp\n---\nmarian\n---\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\n---\nregistry_t(const registry_t<Mutex>&) = delete;\n---\n-Wtemplate-id-cdtor\n---\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wtemplate-id-cdtor\"\nregistry_t<Mutex>(const registry_t<Mutex>&) = delete;\n#pragma GCC diagnostic pop\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\nWtemplate-id-cdtor",
         "Cant compile Marian NMT",
         "Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help",
         "The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I havent tested any of these options as Id need to go to the trouble of installing CUDA",
         "Cant compile Marian NMT Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I havent tested any of these options as Id need to go to the trouble of installing CUDA",
         "Cant compile Marian NMT Im using endeavouros Im trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help",
         "cant compile marian nmt im using endeavouros im trying compile marian instructions fails error message seemingly indicates conflict code c++20 files repo line steps followed result please help",
         "can not compile marian nmt I m use endeavouros I m try compile marian instruction fail error message seemingly indicate conflict code c++20 file repo line step follow result please help",
         "can not compile marian nmt I endeavouros I compile marian instruction fail error message seemingly indicate conflict c20 repo line step please help",
         "4",
         "error message,endeavouros compile,c20 repo,marian instruction,marian nmt"
        ],
        [
         "9",
         "79328514",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )\n</code></pre>\n<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>\n",
         "2025-01-04 08:57:44",
         "2",
         "34",
         "1",
         "79328698.0",
         "<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>\n<p>To modify the <strong>data collator</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>class CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])\n        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])\n        return batch\n</code></pre>\n<p>then pass it to <code>Trainer</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n</code></pre>\n<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>\n<p>Hope, it will work!</p>\n",
         "1.0",
         "input_ids\n---\nlabels\n---\ninteract_ids\n---\ncandidate_ids\n---\nclass LLMWithCustomLayer(LlamaForCausalLM)\n---\ndef forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )",
         "interact_ids\n---\ncandidate_ids\n---\nclass CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[\"interact_ids\"] = torch.tensor([f[\"interact_ids\"] for f in features])\n        batch[\"candidate_ids\"] = torch.tensor([f[\"candidate_ids\"] for f in features])\n        return batch\n---\nTrainer\n---\ntrainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(\"your-llama-model\"),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n---\nforward()\n---\ninteract_ids\n---\ncandidate_ids",
         "how to get custom column in the models forward function when training with Huggingface Trainer",
         "I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much",
         "You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "how to get custom column in the models forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "how to get custom column in the models forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields and so on and I additionally add 2 custom colunms and But i cant get these custom fields in the forward function of my Model I an new in LLM fine tuning Can anyone help me I would be grateful so much",
         "get custom column models forward function training huggingface trainer using huggingface trainer train cumstom model subclassing llama llm tokenized tokenizer dataset fields additionally add 2 custom colunms cant get custom fields forward function model new llm fine tuning anyone help would grateful much",
         "get custom column model forward function training huggingface trainer use huggingface trainer train cumstom model subclasse llama llm tokenized tokenizer dataset field additionally add 2 custom colunms can not get custom field forward function model new llm fine tuning anyone help would grateful much",
         "get custom column forward function training huggingface trainer huggingface trainer train cumstom subclasse llama llm tokenized tokenizer dataset field additionally add 2 custom colunms can not get custom field forward function new llm fine tuning anyone help would grateful much",
         "7",
         "field forward,custom column,custom colunms,training huggingface,trainer train"
        ],
        [
         "10",
         "79312133",
         "Getting all leaf words (reverse stemming) into one Python List",
         "<p>On the same lines as the solution provided <a href=\"https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword\">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>\n<p>Imagine I have a shorter sample word list as follows:</p>\n<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n</code></pre>\n<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>\n<pre><code>get_word_forms(&quot;command&quot;)\n</code></pre>\n<p>and get the following output:</p>\n<pre><code>{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n</code></pre>\n<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>\n<p>If I try to reverse-stem the entire list in one go:</p>\n<pre><code>[get_word_forms(word) for word in sample]\n</code></pre>\n<p>I fail at getting any output:</p>\n<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n</code></pre>\n<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>\n<p>something like:</p>\n<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. \n</code></pre>\n",
         "2024-12-27 15:04:05",
         "1",
         "47",
         "1",
         "79312987.0",
         "<p>One solution using nested list comprehensions after stripping forgotten spaces:</p>\n<pre><code>all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']\n</code></pre>\n",
         "1.0",
         "get_word_forms\n---\nmy_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n---\nget_word_forms(\"command\")\n---\n{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n---\n[get_word_forms(word) for word in sample]\n---\n[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n---\n['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.",
         "all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']",
         "Getting all leaf words reverse stemming into one Python List",
         "On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like",
         "One solution using nested list comprehensions after stripping forgotten spaces",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like One solution using nested list comprehensions after stripping forgotten spaces",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output n is noun a is adjective v is verb and r is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like",
         "getting leaf words reverse stemming one python list lines solution provided link trying get leaf words one stem word using communitycontributed srivastava package imagine shorter sample word list follows work manually following go wordbyword timeconsuming list 200 words get following output n noun adjective v verb r adverb try reversestem entire list one go fail getting output think failing saving output dictionary eventually would like output list without breaking noun adjective adverb verb something like",
         "get leaf word reverse stem one python list line solution provide link try get leaf word one stem word use communitycontributed srivastava package imagine short sample word list follow work manually follow go wordbyword timeconsuming list 200 word get follow output n noun adjective v verb r adverb try reversestem entire list one go fail get output think fail save output dictionary eventually would like output list without break noun adjective adverb verb something like",
         "get leaf reverse stem python line solution provide link get leaf stem communitycontributed srivastava package imagine short sample manually go wordbyword timeconsuming 200 get n noun adjective v verb r adverb reversestem entire go fail get think fail save dictionary eventually would like without break noun adjective adverb verb something like",
         "3",
         "adverb verb,break noun,stem python,manually wordbyword,leaf reverse"
        ],
        [
         "11",
         "79298368",
         "Inspect all probabilities of BERTopic model",
         "<p>Say I build a BERTopic model using</p>\n<pre><code>from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre>\n<p>Inspecting <code>probs</code> gives me just a single value for each item in <code>docs</code>.</p>\n<pre><code>probs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n</code></pre>\n<p>I would like the entire probability vector across all topics (so in this case, where <code>nr_topics=20</code>, I want a vector of 20 probabilities for each item in <code>docs</code>). In other words, if I have N items in <code>docs</code> and K topics, I would like an NxK output.</p>\n",
         "2024-12-20 20:49:34",
         "1",
         "52",
         "1",
         "79299703.0",
         "<p>For individual topic probability across each document you need to add one more argument.</p>\n<pre><code>topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n</code></pre>\n<p>Note: This calculate_probabilities = True will only work if you are using <strong><code>HDBSCAN</code></strong> clustering embedding model. And Bertopic by default uses <code>all-MiniLM-L6-v2</code>.</p>\n<p><strong>Official documentation:</strong> <a href=\"https://maartengr.github.io/BERTopic/api/bertopic.html\" rel=\"nofollow noreferrer\">https://maartengr.github.io/BERTopic/api/bertopic.html</a></p>\n<p>They have mentioned the same in document as well.</p>\n",
         "1.0",
         "from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n---\nprobs\n---\ndocs\n---\nprobs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n---\nnr_topics=20\n---\ndocs\n---\ndocs",
         "topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n---\nHDBSCAN\n---\nall-MiniLM-L6-v2",
         "Inspect all probabilities of BERTopic model",
         "Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output",
         "For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output",
         "inspect probabilities bertopic model say build bertopic model using inspecting gives single value item would like entire probability vector across topics case want vector 20 probabilities item words n items k topics would like nxk output",
         "inspect probability bertopic model say build bertopic model use inspecting give single value item would like entire probability vector across topic case want vector 20 probability item word n item k topic would like nxk output",
         "inspect probability bertopic say build bertopic inspecting single value item would like entire probability vector across topic case vector 20 probability item n item k topic would like nxk",
         "0",
         "item topic,20 probability,probability vector,build bertopic,bertopic inspecting"
        ],
        [
         "12",
         "79293919",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>\n<p>For example:\nA sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>\n<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>\n<p>Any and all help is welcome and appreciated.</p>\n<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>\n",
         "2024-12-19 10:24:04",
         "0",
         "63",
         "2",
         "79294074.0",
         "<p>You need a external dataset for this task. You can try dataset such as google n gram dataset.</p>\n<p>Here is the breakdown of the problem statement:</p>\n<ol>\n<li>Input: &quot;I enjoy a cold glass of water on a hot day&quot;. <code>Output</code>: &quot;water&quot;.</li>\n<li>Split the sentences into words list.</li>\n</ol>\n<blockquote>\n<p>Example: [&quot;I&quot;, &quot;enjoy&quot;, &quot;a&quot;, &quot;cold&quot;, &quot;glass&quot;, &quot;of&quot;, &quot;water&quot;, &quot;on&quot;,\n&quot;a&quot;, &quot;hot&quot;, &quot;day&quot;]</p>\n</blockquote>\n<ol start=\"3\">\n<li>First loop in through all the word of the sentences. so let say you are at first word &quot;I&quot;.</li>\n<li>Now you will look the same word &quot;I&quot; in external dataset and will look for the frequency of that word.\nLet say the word &quot;I&quot; in external dataset is repeated <code>5000000</code> times</li>\n<li>Repeat this task for all the word.</li>\n<li>Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.\nFrequency in the below example is random value not exact value.</li>\n</ol>\n<blockquote>\n<pre><code>{\n    &quot;I&quot;: 5000000,\n    &quot;enjoy&quot;: 50000,\n    &quot;a&quot;: 10000000,\n    &quot;cold&quot;: 30000,\n    &quot;glass&quot;: 100000,\n    &quot;of&quot;: 8000000,\n    &quot;water&quot;: 1200000,\n    &quot;on&quot;: 6000000,\n    &quot;hot&quot;: 700000,\n    &quot;day&quot;: 400000\n}\n</code></pre>\n</blockquote>\n<ol start=\"7\">\n<li>Pick the word with highest frequency.</li>\n</ol>\n<p>Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself</p>\n",
         "2.0",
         "",
         "Output\n---\n5000000\n---\n{\n    \"I\": 5000000,\n    \"enjoy\": 50000,\n    \"a\": 10000000,\n    \"cold\": 30000,\n    \"glass\": 100000,\n    \"of\": 8000000,\n    \"water\": 1200000,\n    \"on\": 6000000,\n    \"hot\": 700000,\n    \"day\": 400000\n}",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess",
         "You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but Im trying to figure out how to determine the most used words in the English language from a set of words in a dictionary Ive made Ive done some research on NLTK but cant seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because its the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure Ill likely have to involve AI but any time Ive tried to use AI I wind up copy and pasting code because I just dont understand it so Im trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesnt have from the computers guess",
         "determining popular words english dictionary within dictionary words forgive wording awful im trying figure determine used words english language set words dictionary ive made ive done research nltk cant seem find function within library matter help need example sentence enjoy cold glass water hot day would return water used word day day conversation sentence essentially need returned value frequently used word conversations figure ill likely involve ai time ive tried use ai wind copy pasting code dont understand im trying avoid going route help welcome appreciated context decided start project would essentially guess predetermined word based characters user says doesnt computers guess",
         "determine popular word english dictionary within dictionary word forgive word awful I m try figure determine use word english language set word dictionary I ve make I ve do research nltk can not seem find function within library matter help need example sentence enjoy cold glass water hot day would return water use word day day conversation sentence essentially need return value frequently use word conversation figure ill likely involve ai time I ve try use ai wind copy paste code do not understand I m try avoid go route help welcome appreciated context decide start project would essentially guess predetermine word base character user say do not computer guess",
         "determine popular english dictionary within dictionary forgive awful I figure determine english language set dictionary I ve make I ve do research nltk can not function within library matter help enjoy cold glass water hot day would return water day day conversation essentially return value frequently conversation figure ill likely involve ai time I ve ai wind copy paste do not understand I avoid go route help welcome appreciated context decide start project would essentially guess predetermine base character user say do not computer guess",
         "1",
         "nltk function,determine english,popular english,set dictionary,dictionary dictionary"
        ],
        [
         "13",
         "79293889",
         "catelog sentences into 5 words that represent them",
         "<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>\n<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>\n<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>\n<p>I will glad for recomendations how to do that</p>\n<p><strong>edit</strong></p>\n<p>represnt = the semantic distance between the word to the text.</p>\n<p>for example -\nlets say in row 1 the text is &quot;i want to eat&quot;\nand I have 2 words : food and house.</p>\n<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>\n",
         "2024-12-19 10:16:47",
         "0",
         "54",
         "1",
         "79294099.0",
         "<p>You could use a pre-trained sentence transformer model from <a href=\"https://pypi.org/project/sentence-transformers/\" rel=\"nofollow noreferrer\"><code>sentence_transformers</code></a>:</p>\n<pre><code>import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -&gt; None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -&gt; list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -&gt; pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838\n</code></pre>\n",
         "0.0",
         "df['text']\n---\ndf[\"word1\"]\n---\ndf[\"word2\"]\n---\ndf[\"food \"]\n---\ndf[\"house\"]",
         "sentence_transformers\n---\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -> None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -> list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -> pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n---\ntext  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838",
         "catelog sentences into 5 words that represent them",
         "I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in",
         "You could use a pretrained sentence transformer model from Output",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in You could use a pretrained sentence transformer model from Output",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in",
         "catelog sentences 5 words represent dataframe 1000 text rows also 5 words want know one much represnt text 0 1 every score etc glad recomendations edit represnt = semantic distance word text example lets say row 1 text want eat 2 words food house would higher score",
         "catelog sentence 5 word represent dataframe 1000 text row also 5 word want know one much represnt text 0 1 every score etc glad recomendation edit represnt = semantic distance word text example let say row 1 text want eat 2 word food house would high score",
         "catelog 5 represent dataframe 1000 row also 5 much represnt 0 1 every score glad recomendation edit represnt semantic distance let say row 1 eat 2 food house would high score",
         "0",
         "represent dataframe,row eat,catelog,semantic distance,dataframe 1000"
        ],
        [
         "14",
         "79253283",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>\n<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>\n<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n</code></pre>\n<p>A small Example -</p>\n<pre><code>text_dict = {\n    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]\n}\n\ndf = pd.DataFrame(text_dict)\n</code></pre>\n<p>My expected outcome is -</p>\n<pre><code>                 ITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n</code></pre>\n<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>\n<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>\n",
         "2024-12-05 03:05:06",
         "0",
         "81",
         "1",
         "79263000.0",
         "<p>You need to process each row of text by identifying occurrences of <code>key_words</code> and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique <code>search_word</code> found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique <code>search_words</code> near <code>key_words</code>.</p>\n<pre><code>import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        &quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process &quot;\n        &quot;of adopting Symantec. We believe these technologies will improve our access control. &quot;\n        &quot;Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = &quot; &quot;.join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n</code></pre>\n<p>Which returns</p>\n<pre><code>                                               ITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2\n</code></pre>\n",
         "0.0",
         "search words\n---\nkey words\n---\nsearch words\n---\nkey words\n---\nkey words\n---\nsearch words\n---\nsearch word\n---\nkey word\n---\nsearch_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n---\ntext_dict = {\n    'ITEM7':[\"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"]\n}\n\ndf = pd.DataFrame(text_dict)\n---\nITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n---\ndf\n---\nAVG\n---\nAlibaba Security\n---\nsearch_words\n---\nkey_words\n---\nSymantec\n---\nAccess Control\n---\ndata security\n---\ndiagnostic program\n---\nsearch_words\n---\nadopting\n---\nimprove\n---\nintegrated\n---\nkey_words\n---\nFrequency\n---\ndf\n---\nkey_words",
         "key_words\n---\nsearch_word\n---\nsearch_words\n---\nkey_words\n---\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        \"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \"\n        \"of adopting Symantec. We believe these technologies will improve our access control. \"\n        \"Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = \" \".join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n---\nITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also",
         "You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also",
         "counting frequency words within key words text two sets word lists first one called second one called goal calculate frequency within 10 words example assume word acquire list look words list within 10 words acquire within 10 words mean 10 words forward key words 10 words backward key words meaning forward backward movement lists small example expected outcome first row see word list around word acquired base form acquire list similarly list words within 10 words list total search words 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note words basically base form variation like adopted adopting counted key words also",
         "count frequency word within key word text two set word list first one call second one call goal calculate frequency within 10 word example assume word acquire list look word list within 10 word acquire within 10 word mean 10 word forward key word 10 word backward key word mean forward backward movement list small example expect outcome first row see word list around word acquire base form acquire list similarly list word within 10 word list total search word 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note word basically base form variation like adopt adopt count key word also",
         "count frequency within key set first call second call goal calculate frequency within 10 assume acquire within 10 acquire within 10 mean 10 forward key 10 backward key mean forward backward movement small expect outcome first row around acquire base form acquire similarly within 10 total search 6 avgalibaba securitysymantecaccess controldata securitydiagnostic program therefore column value 6 please note basically base form variation like adopt adopt count key also",
         "8",
         "outcome row,program column,frequency 10,calculate frequency,count key"
        ],
        [
         "15",
         "79247672",
         "Error in getting Captum text explanations for text classification",
         "<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>\n<pre><code>import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f&quot;Test Accuracy: {accuracy:.2f}&quot;)\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n<p>But I am getting the following error in running the above.</p>\n<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n</code></pre>\n",
         "2024-12-03 12:47:45",
         "2",
         "84",
         "1",
         "79248379.0",
         "<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>\n<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=\"https://captum.ai/tutorials/Bert_SQUAD_Interpret\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>\n<p>Below please find snippet that works:</p>\n<pre><code>from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n",
         "1.0",
         "import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)\n---\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-f047b509c98d> in <cell line: 90>()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
         "from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    # print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)",
         "Error in getting Captum text explanations for text classification",
         "I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above",
         "You need to slightly change the gradients calculation class Also you didnt include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above You need to slightly change the gradients calculation class Also you didnt include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above",
         "error getting captum text explanations text classification following code using identify influential words used correctly predict text test dataset getting following error running",
         "error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset get follow error run",
         "error get captum explanation classification identify influential correctly predict test dataset get error run",
         "4",
         "influential correctly,explanation classification,predict test,captum explanation,dataset error"
        ],
        [
         "16",
         "79247594",
         "euclidian distance from word to sentence after doing Vectorizer",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did TfidfVectorizer.</p>\n<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>\n<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n",
         "2024-12-03 12:25:05",
         "1",
         "44",
         "1",
         "79248087.0",
         "<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([&quot;king&quot;]).toarray()\nexcept KeyError:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n</code></pre>\n<p>which would give you</p>\n<pre><code>                                                text  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0  \n</code></pre>\n<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif &quot;king&quot; in feature_names:\n    king_index = np.where(feature_names == &quot;king&quot;)[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n\n</code></pre>\n<p>which gives</p>\n<pre><code>                                                text  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214\n</code></pre>\n",
         "1.0",
         "",
         "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([\"king\"]).toarray()\nexcept KeyError:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n---\ntext  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0\n---\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif \"king\" in feature_names:\n    king_index = np.where(feature_names == \"king\")[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n---\ntext  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214",
         "euclidian distance from word to sentence after doing Vectorizer",
         "I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method",
         "I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king dfking I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method",
         "euclidian distance word sentence vectorizer dataframe 1000 text rows tfidfvectorizer want create new field give distance sentence word want lets say word king dfking thought taking sentence 5 closet words word king make average glad know hear another method",
         "euclidian distance word sentence vectorizer dataframe 1000 text row tfidfvectorizer want create new field give distance sentence word want let say word king dfke think take sentence 5 closet word word king make average glad know hear another method",
         "euclidian distance vectorizer dataframe 1000 row tfidfvectorizer create new field distance let say king dfke think take 5 closet king make average glad hear another method",
         "6",
         "euclidian,row tfidfvectorizer,distance let,dataframe 1000,field distance"
        ],
        [
         "17",
         "79234004",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "<p>I want to use <code>Llama-3.2-1B-Instruct</code> model, and although I have set <code>&quot;temperature&quot;: 0.0, &quot;top_p&quot;:0.0 and &quot;top_k&quot;:0</code>, it still generates inconsistent output. This is how my pipeline looks like:</p>\n<pre><code>pipe = pipeline(\n    &quot;text-generation&quot;,\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=&quot;mps&quot;,\n        model_kwargs={&quot;temperature&quot;: 0.0,\n                  &quot;do_sample&quot;:True,\n                              &quot;top_p&quot;:0.0,\n                              &quot;top_k&quot;:0,},\n)\n</code></pre>\n<p>Any idea how to solve this issue?</p>\n",
         "2024-11-28 13:02:37",
         "1",
         "641",
         "2",
         "79246602.0",
         "<p>The model inconsistent output can be due to two main factors:</p>\n<p><strong>1. Temperature:</strong></p>\n<p>setting temperature to zero give more inconsistent result. You can refer <a href=\"https://community.openai.com/t/why-the-api-output-is-inconsistent-even-after-the-temperature-is-set-to-0/329541/2\" rel=\"nofollow noreferrer\">Opeani discussion page</a> for detail.</p>\n<p>So the best option is to set temperature to very low values such as 0.00001 instead of zero.</p>\n<p><strong>2. do_sample</strong></p>\n<p>You already set it false, and it should remain that way only.</p>\n",
         "1.0",
         "Llama-3.2-1B-Instruct\n---\n\"temperature\": 0.0, \"top_p\":0.0 and \"top_k\":0\n---\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"mps\",\n        model_kwargs={\"temperature\": 0.0,\n                  \"do_sample\":True,\n                              \"top_p\":0.0,\n                              \"top_k\":0,},\n)",
         "",
         "Llama321BInstruct generate inconsistent output",
         "I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue",
         "The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue",
         "llama321binstruct generate inconsistent output want use model although set still generates inconsistent output pipeline looks like idea solve issue",
         "llama321binstruct generate inconsistent output want use model although set still generate inconsistent output pipeline look like idea solve issue",
         "llama321binstruct generate inconsistent although set still generate inconsistent pipeline like idea solve issue",
         "4",
         "set,pipeline like,inconsistent,set generate,llama321binstruct"
        ],
        [
         "18",
         "79192130",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>\n<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>\n",
         "2024-11-15 11:13:36",
         "1",
         "56",
         "2",
         "79192427.0",
         "<p>In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:</p>\n<ol>\n<li>Package your KeyBERT script in a lambda and easily deploy it with a container.</li>\n<li>Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.</li>\n</ol>\n",
         "1.0",
         "",
         "",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT",
         "I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks",
         "In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model Im using is resourceintensive and requires GPU acceleration Id go with SageMaker otherwise Lambda is a good solution So for your case heres what Id do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model Im using is resourceintensive and requires GPU acceleration Id go with SageMaker otherwise Lambda is a good solution So for your case heres what Id do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I dont want to integrate it into my other projects as they dont use python Ive attempted to use lambda but Im concerned about the potential cost of running this Thanks",
         "using aws service execute python script extract keywords text using keybert simple python script given two blocks text extracts keywords using keybert compares lists keywords sort two lists depending lists share keywords aws service would best fit needs want able esentially spin needed give blocks text execute return results dont want integrate projects dont use python ive attempted use lambda im concerned potential cost running thanks",
         "use aws service execute python script extract keyword text use keybert simple python script give two block text extract keyword use keybert compare list keyword sort two list depend list share keyword aws service would well fit need want able esentially spin need give block text execute return result do not want integrate project do not use python I ve attempt use lambda I m concerned potential cost run thank",
         "aws service execute python script extract keyword keybert simple python script block extract keyword keybert compare keyword sort depend share keyword aws service would fit able esentially spin block execute return do not integrate project do not python I ve attempt lambda I concerned potential cost run thank",
         "8",
         "extract keyword,lambda,service execute,aws,python script"
        ],
        [
         "19",
         "79178041",
         "Normalization of token embeddings in BERT encoder blocks",
         "<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>\n",
         "2024-11-11 14:30:31",
         "2",
         "162",
         "2",
         "79238393.0",
         "<p>I tracked down full details of layer normalization (LN) in BERT <a href=\"https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para\">here</a>.</p>\n<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>\n",
         "0.0",
         "",
         "",
         "Normalization of token embeddings in BERT encoder blocks",
         "Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings",
         "I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token its per embedding dimension",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token its per embedding dimension",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings",
         "normalization token embeddings bert encoder blocks following multiheaded attention layer bert encoder block layer normalization done separately embedding token ie one mean variance per token embedding concatenated vector token embeddings mean variance embeddings",
         "normalization token embedding bert encoder block follow multiheade attention layer bert encoder block layer normalization done separately embed token ie one mean variance per token embed concatenate vector token embedding mean variance embedding",
         "normalization token embedding bert encoder block multiheade attention layer bert encoder block layer normalization done separately embed token ie mean variance per token embed concatenate vector token embedding mean variance embedding",
         "2",
         "attention layer,variance embedding,token embed,normalization token,layer bert"
        ],
        [
         "20",
         "79173053",
         "How to convert character indices to BERT token indices",
         "<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>\n<pre><code>from datasets import load_dataset\nds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\n</code></pre>\n<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>\n<pre><code>d0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n</code></pre>\n<p>After tokenization, the answer indices are 56  and 16:</p>\n<pre><code>from transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'\n</code></pre>\n<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>\n<p>This is from a <a href=\"https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false\" rel=\"nofollow noreferrer\">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=\"https://i.sstatic.net/GsZ6mfcQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/GsZ6mfcQ.png\" alt=\"QA dataset with token answer indices\" /></a></p>\n",
         "2024-11-09 15:15:33",
         "2",
         "33",
         "1",
         "79175157.0",
         "<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>\n<p>The following function does the above for you:</p>\n<pre><code>def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start &lt;= char_start &lt; end: \n            start_token_idx = i\n        if start &lt; char_end &lt;= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n</code></pre>\n<p>Here's how you can use and test this function:</p>\n<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])\nprint(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(&quot;Tokenized answer:&quot;, answer_tokens)\n</code></pre>\n<p>Output:</p>\n<pre><code>Tokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream\n</code></pre>\n",
         "2.0",
         "UCLNLP/adversarial_qa\n---\nfrom datasets import load_dataset\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\n---\nd0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n---\nfrom transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'",
         "def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start <= char_start < end: \n            start_token_idx = i\n        if start < char_end <= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n---\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(\"Tokenized start index:\", d0_tokenized['answer_start_token_idx'])\nprint(\"Tokenized end index:\", d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(\"Tokenized answer:\", answer_tokens)\n---\nTokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream",
         "How to convert character indices to BERT token indices",
         "I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result",
         "You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Heres how you can use and test this function Output",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Heres how you can use and test this function Output",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Heres an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answers token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result",
         "convert character indices bert token indices working questionanswer dataset map characterbased answer indices tokenbased indices tokenizing context question together using tokenizer like bert heres example row dataset tokenization answer indices 56 16 want create new dataset answers token indices eg 56 ad 60 linkedin learning class instructor conversion created csv file share code expected result",
         "convert character index bert token indices work questionanswer dataset map characterbase answer index tokenbase index tokenize context question together use tokenizer like bert heres example row dataset tokenization answer indice 56 16 want create new dataset answer token indices eg 56 ad 60 linkedin learn class instructor conversion create csv file share code expect result",
         "convert character index bert token indices questionanswer dataset map characterbase answer index tokenbase index tokenize context question together tokenizer like bert heres row dataset tokenization answer indice 56 16 create new dataset answer token indices eg 56 ad 60 linkedin learn class instructor conversion create csv share expect",
         "2",
         "convert character,index tokenbase,question tokenizer,dataset tokenization,index bert"
        ],
        [
         "21",
         "79159805",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage?",
         "<p>I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage.</p>\n<p>I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing.</p>\n<ul>\n<li><p>Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process?</p>\n</li>\n<li><p>Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes?</p>\n</li>\n<li><p>If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects?</p>\n</li>\n</ul>\n<p>Any suggestions or examples would be greatly appreciated! Thank you!</p>\n",
         "2024-11-05 15:49:33",
         "3",
         "93",
         "2",
         "79162232.0",
         "<p>I would strongly advise you not to treat NLP models like any other Python object. I would always prefer to load an NLP model using a microservice approach, which is more aligned with ML/software engineering best practices by separating the model logic from the main application.</p>\n<p>Instead of loading the model in each process (which can be memory-intensive), the model is loaded just once in a dedicated service. This setup allows the model to be used by multiple parts of the application without duplicating memory usage, making it efficient, modular, and scalable. Not only is your concern about memory efficiency addressed, but scalability and modularity are also improved.</p>\n<p>An example of implementing such a microservice using FastAPI + Docker could look like this:</p>\n<pre><code># main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(&quot;en_core_web_lg&quot;)  # Load model once\n\n@app.post(&quot;/process/&quot;)\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {&quot;tokens&quot;: [(token.text, token.pos_) for token in doc]}\n</code></pre>\n<p>To containerize above FastAPI service:</p>\n<pre><code># Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt &amp;&amp; python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [&quot;gunicorn&quot;, &quot;-w&quot;, &quot;4&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;main:app&quot;]\n</code></pre>\n",
         "3.0",
         "",
         "# main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(\"en_core_web_lg\")  # Load model once\n\n@app.post(\"/process/\")\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {\"tokens\": [(token.text, token.pos_) for token in doc]}\n---\n# Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt && python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [\"gunicorn\", \"-w\", \"4\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"main:app\"]",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage",
         "Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you",
         "I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage Im working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPIs MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you",
         "share complex spacy nlp model across multiple python processes minimize memory usage im working multiprocessing python application multiple processes need access large preloaded spacy nlp model eg en_core_web_lg since model memoryintensive want avoid loading separately process since quickly run main memory object readonly instead id like load shared location processes read without duplicating memory usage looked multiprocessingmanager multiprocessingshared_memory approaches seem better suited numpy arrays raw data buffers simple objects complex objects internal references like nlp model also looked mpis mpiwinallocate_shared ran issues using redis server make rank 0 processing works mpi since processing done single rank defeats propose using multiprocessing efficient way share spacy model instance across multiple processes python avoid reloading process libraries techniques specifically suited sharing complex readonly objects like nlp models memory across processes multiprocessingmanager shared_memory viable ways improve performance reduce memory overhead working complex objects suggestions examples would greatly appreciated thank",
         "share complex spacy nlp model across multiple python process minimize memory usage I m work multiprocesse python application multiple process need access large preloade spacy nlp model eg en_core_web_lg since model memoryintensive want avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage look multiprocessingmanager multiprocessingshared_memory approach seem well suited numpy array raw datum buffer simple object complex object internal reference like nlp model also look mpis mpiwinallocate_shared run issue use redis server make rank 0 processing work mpi since process do single rank defeat propose use multiprocesse efficient way share spacy model instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp model memory across process multiprocessingmanager shared_memory viable way improve performance reduce memory overhead work complex object suggestion example would greatly appreciate thank",
         "share complex spacy nlp across multiple python process minimize memory usage I multiprocesse python application multiple process access large preloade spacy nlp eg encoreweblg since memoryintensive avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage multiprocessingmanager multiprocessingsharedmemory approach suited numpy array raw datum buffer simple object complex object internal reference like nlp also mpis mpiwinallocateshared run issue redis server make rank 0 processing mpi since process do single rank defeat propose multiprocesse efficient share spacy instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp memory across process multiprocessingmanager sharedmemory viable improve performance reduce memory overhead complex object suggestion would greatly appreciate thank",
         "8",
         "nlp memory,multiprocesse python,multiprocessingsharedmemory approach,multiprocessingmanager sharedmemory,multiprocesse efficient"
        ],
        [
         "22",
         "79155290",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing",
         "<p>When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data.</p>\n<p><a href=\"https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment\" rel=\"nofollow noreferrer\">https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment</a></p>\n<p>There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative.</p>\n<p><strong>Is there a way to get neutral labels for such examples in RobBERTje?</strong></p>\n<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = &quot;DTAI-KULeuven/robbert-v2-dutch-sentiment&quot;\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n</code></pre>\n<p>Output:</p>\n<pre><code>[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]\n</code></pre>\n",
         "2024-11-04 11:36:35",
         "2",
         "54",
         "1",
         "79155380.0",
         "<p>This model was trained only on <code>negative</code> and <code>positive</code> labels. Therefore, it will try to categorize every input as positive or negative, even if it is nonsensical or neutral.</p>\n<p>what you can do is to:\n1- Find other models that was trained to include <code>neutral</code> label.\n2- Fine-tune this model on a dataset that includes <code>neutral</code> label.\n3- Empirically define a threshold based on the confidence outputs and interpret it as <code>neutral</code>.</p>\n<p>The first 2 choices are extensive in effort. I would suggest you go with the third option for a quick workaround. Try feeding the model with a few neutral input and observe the range of confidence score in the output. then use that threshold to classify as <code>neutral</code>.</p>\n<p>Here's a sample:</p>\n<pre><code>def classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] &lt; threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result\n</code></pre>\n",
         "3.0",
         "from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n---\n[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]",
         "negative\n---\npositive\n---\nneutral\n---\nneutral\n---\nneutral\n---\nneutral\n---\ndef classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] < threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing",
         "When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output",
         "This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Heres a sample",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Heres a sample",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg Fhdf nonsense and Als gisteren inclusief blauw neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output",
         "dutch sentiment analysis robbertje outputs positive/negative labels netural label missing run dutch sentiment analysis robbertje outputs positive/negative labels netural label missing data obvious neutral sentences/words eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral labels examples robbertje output",
         "dutch sentiment analysis robbertje outputs positive / negative label netural label miss run dutch sentiment analysis robbertje outputs positive / negative label netural label miss datum obvious neutral sentence / word eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative way get neutral label example robbertje output",
         "dutch sentiment analysis robbertje outputs positive negative label netural label miss run dutch sentiment analysis robbertje outputs positive negative label netural label miss datum obvious neutral eg fhdf nonsense als gisteren inclusief blauw neutral evaluate positive negative get neutral label robbertje",
         "0",
         "negative neutral,dutch,robbertje outputs,neutral label,sentiment analysis"
        ],
        [
         "23",
         "79148979",
         "Finding Root Form of Verbs using Curiosity-AI/Catalyst",
         "<p>I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match <code>PartOfSpeech.VERB</code> but I don't know how to continue from there.</p>\n<p>This is what I have so far:</p>\n<pre><code>const string text = &quot;The disastrous cat runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}\n</code></pre>\n<p>Any help is greatly appreciated.</p>\n",
         "2024-11-01 17:58:01",
         "2",
         "135",
         "1",
         "79160163.0",
         "<p>The following code (targeting .NET 8.0) illustrates one method to obtain the root form of a verb from an inflected form.</p>\n<p>(I have annonoted, as code comments, the three NuGet packages (with versions) required. Most of the code is identical to your original sample above.)</p>\n<pre><code>//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = &quot;The disastrous cat quickly runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($&quot;Root of the verb '{token.Value}' is '{token.Lemma}'.&quot;);\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(&quot;Complete; press any key.&quot;);\nConsole.ReadKey();\n</code></pre>\n<p><strong>Note:</strong> For this specific sentence, I have added an adverb (&quot;quickly&quot;) before the verb (&quot;runs&quot;). Without this, the library incorrectly interprets &quot;runs&quot; as a noun. Depending on your source text, this might be an issue for you, but I believe it is separate from the question being asked.</p>\n",
         "1.0",
         "PartOfSpeech.VERB\n---\nconst string text = \"The disastrous cat runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}",
         "//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = \"The disastrous cat quickly runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($\"Root of the verb '{token.Value}' is '{token.Lemma}'.\");\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(\"Complete; press any key.\");\nConsole.ReadKey();",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst",
         "Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated",
         "The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst Im trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I dont know how to continue from there This is what I have so far Any help is greatly appreciated",
         "finding root form verbs using curiosityai/catalyst im trying find root form verb run text pipeline identify tokens match dont know continue far help greatly appreciated",
         "find root form verb use curiosityai / catalyst I m try find root form verb run text pipeline identify tokens match do not know continue far help greatly appreciate",
         "root form verb curiosityai catalyst I root form verb run pipeline identify tokens match do not continue far help greatly appreciate",
         "5",
         "curiosityai catalyst,run pipeline,root form,identify tokens,tokens match"
        ],
        [
         "24",
         "79145419",
         "Is it possible to get embeddings from NV-Embed using Candle?",
         "<p>What I want to do is a CLI program that outputs embeddings of an arbitrary input.\nTo do that, I want to do an inference with an embeddings model, and I chose <code>NV-Embed-v2</code>. My framework of choice is <a href=\"https://github.com/huggingface/candle\" rel=\"nofollow noreferrer\">Candle</a>, but I also looked at <a href=\"https://github.com/EricLBuehler/mistral.rs\" rel=\"nofollow noreferrer\">Mistral-RS</a>.</p>\n<p>Basically, what I'm trying to do is this code fragment:\n<a href=\"https://huggingface.co/nvidia/NV-Embed-v2\" rel=\"nofollow noreferrer\">https://huggingface.co/nvidia/NV-Embed-v2</a>\nbut with Rust and Candle.</p>\n<p>What I tried is to start off with <a href=\"https://github.com/huggingface/candle/blob/main/candle-examples/examples/mistral/main.rs\" rel=\"nofollow noreferrer\">Mistral Candle's example</a> because the NV-Embed's HF page says: <code>Model Details / Base Decoder-only LLM: Mistral-7B-v0.1</code>.</p>\n<p>I replaced the model id in the original code with <code>nvidia/NV-Embed-v2</code>, and was able to download the weights from Hugging Face, but upon loading the config, I got this:</p>\n<pre><code>Error: missing field `vocab_size` at line 101 column 1\n</code></pre>\n<p>Then I hardcoded the values from the JSON config loaded from HF to a newly created <code>candle_transformers::models::mistral::Config</code> instance. And after that, <code>Mistral::new(&amp;config, vb)</code> fails with:</p>\n<pre><code>Error: cannot find tensor model.embed_tokens.weight\n</code></pre>\n<p>Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?</p>\n",
         "2024-10-31 15:55:49",
         "0",
         "329",
         "1",
         "79156470.0",
         "<p>candle looking for <code>model.embed_tokens.weight</code> whereas the original tensor name is <code>embedding_model.embed_tokens.weight</code>. You just have to change this line of <code>mistral.rs</code> in candle_transformers.</p>\n<pre class=\"lang-rust prettyprint-override\"><code>// from\nlet vb_m = vb.pp(&quot;model&quot;);\n//to\nlet vb_m = vb.pp(&quot;embedding_model&quot;);\n</code></pre>\n",
         "2.0",
         "NV-Embed-v2\n---\nModel Details / Base Decoder-only LLM: Mistral-7B-v0.1\n---\nnvidia/NV-Embed-v2\n---\nError: missing field `vocab_size` at line 101 column 1\n---\ncandle_transformers::models::mistral::Config\n---\nMistral::new(&config, vb)\n---\nError: cannot find tensor model.embed_tokens.weight",
         "model.embed_tokens.weight\n---\nembedding_model.embed_tokens.weight\n---\nmistral.rs\n---\n// from\nlet vb_m = vb.pp(\"model\");\n//to\nlet vb_m = vb.pp(\"embedding_model\");",
         "Is it possible to get embeddings from NVEmbed using Candle",
         "What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed",
         "candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what Im trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candles example because the NVEmbeds HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe thats a common mistake that could easily be diagnosed",
         "possible get embeddings nvembed using candle want cli program outputs embeddings arbitrary input want inference embeddings model chose framework choice candle also looked mistralrs basically im trying code fragment rust candle tried start mistral candles example nvembeds hf page says replaced model id original code able download weights hugging face upon loading config got hardcoded values json config loaded hf newly created instance fails way around maybe candlebased open source works could use inspiration maybe thats common mistake could easily diagnosed",
         "possible get embedding nvembe use candle want cli program output embedding arbitrary input want inference embedding model choose framework choice candle also look mistralrs basically I m try code fragment rust candle try start mistral candle example nvembed hf page say replace model i d original code able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail way around maybe candlebase open source work could use inspiration maybe that s common mistake could easily diagnose",
         "possible get embedding nvembe candle cli program embedding arbitrary input inference embedding choose framework choice candle also mistralrs basically I fragment rust candle start mistral candle nvembed hf page say replace i d original able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail around maybe candlebase open source could inspiration maybe that s common mistake could easily diagnose",
         "2",
         "nvembe candle,possible embedding,program embedding,rust candle,candlebase open"
        ],
        [
         "25",
         "79111733",
         "How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)",
         "<p>How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)</p>\n<p>I have short product descriptions that I’d like to transform into structured attributes.</p>\n<p>Example:</p>\n<p>Input:</p>\n<pre><code>“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n</code></pre>\n<p>Output:</p>\n<pre><code>Year = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml\n</code></pre>\n<p>If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:</p>\n<ol>\n<li><p>There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.</p>\n</li>\n<li><p>There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.</p>\n</li>\n<li><p>Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.</p>\n</li>\n<li><p>I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.</p>\n</li>\n</ol>\n<p>I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.</p>\n<p>I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.</p>\n<p>Appreciate any insight into approaches or suggested learning resources.</p>\n<p></p>\n<p>I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.</p>\n",
         "2024-10-21 20:54:56",
         "0",
         "166",
         "1",
         "79113907.0",
         "<p>LLM would work nicely for this.  I'v done similar tasks before and it worked nicely with minimal training.  Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate,  but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions.</p>\n<p>For you task I would use a framework like Langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  When run with a model it will create an XML output which would be trivial to parse.  You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me.</p>\n<pre><code>You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like &quot;Red&quot; or &quot;White&quot; (e.g., &quot;Red Head Vineyard&quot;) which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., &quot;1500 ml&quot;) rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n&lt;Wine&gt;\n&lt;Year&gt;{{Year}}&lt;/Year&gt;\n&lt;Color&gt;{{Color}}&lt;/Color&gt;\n&lt;Weight&gt;{{Weight}}&lt;/Weight&gt;\n&lt;WeightUnit&gt;{{WeightUnit}}&lt;/WeightUnit&gt;\n&lt;Brand&gt;{{Brand}}&lt;/Brand&gt;\n&lt;GrapeVariety&gt;{{GrapeVariety}}&lt;/GrapeVariety&gt;\n&lt;/Wine&gt;\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   &lt;Wine&gt;\n     &lt;Year&gt;2017&lt;/Year&gt;\n     &lt;Color&gt;Red&lt;/Color&gt;\n     &lt;Weight&gt;750&lt;/Weight&gt;\n     &lt;WeightUnit&gt;ml&lt;/WeightUnit&gt;\n     &lt;Brand&gt;La Lecciaia&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Cabernet Sauvignon&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   &lt;Wine&gt;\n     &lt;Year&gt;2020&lt;/Year&gt;\n     &lt;Color&gt;&lt;/Color&gt;\n     &lt;Weight&gt;1.5&lt;/Weight&gt;\n     &lt;WeightUnit&gt;L&lt;/WeightUnit&gt;\n     &lt;Brand&gt;Red Head Vineyard&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Chardonnay&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}\n</code></pre>\n<p>Keep in mind that LLMs are not cheap to run.  But for this tasks given ambiguousness of the domain it is most likely the best choice.  For this particular task it would be 1/1000 of a penny per label using OpenAI service.  You might find a cheaper model / provider.  However when working with LLM it is very important to ensure accuracy first,  then optimize for costs.</p>\n<p>The whole thing will probably take 1-2 hours to build for the intermediate LLM developer.  If you are learning it may vary.  But this is a perfect project to learn about LLMs</p>\n",
         "1.0",
         "“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n---\nYear = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml",
         "You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like \"Red\" or \"White\" (e.g., \"Red Head Vineyard\") which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., \"1500 ml\") rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n<Wine>\n<Year>{{Year}}</Year>\n<Color>{{Color}}</Color>\n<Weight>{{Weight}}</Weight>\n<WeightUnit>{{WeightUnit}}</WeightUnit>\n<Brand>{{Brand}}</Brand>\n<GrapeVariety>{{GrapeVariety}}</GrapeVariety>\n</Wine>\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   <Wine>\n     <Year>2017</Year>\n     <Color>Red</Color>\n     <Weight>750</Weight>\n     <WeightUnit>ml</WeightUnit>\n     <Brand>La Lecciaia</Brand>\n     <GrapeVariety>Cabernet Sauvignon</GrapeVariety>\n   </Wine>\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   <Wine>\n     <Year>2020</Year>\n     <Color></Color>\n     <Weight>1.5</Weight>\n     <WeightUnit>L</WeightUnit>\n     <Brand>Red Head Vineyard</Brand>\n     <GrapeVariety>Chardonnay</GrapeVariety>\n   </Wine>\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}",
         "How to derive attributes/labels from short plain text descriptions NER LLM",
         "How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense",
         "LLM would work nicely for this Iv done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense LLM would work nicely for this Iv done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources Ive looked online for information but many approaches involve significant amount of up front work and unclear if theyll work in a practical sense",
         "derive attributes/labels short plain text descriptions ner llm derive attributes/labels short plain text descriptions ner llm short product descriptions id like transform structured attributes example input output everything format would trivial write regular expression done many different formats nuances increasingly cumbersome hardcode logic format trying create generic solution immediately run issues basic approach several different data providers format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even given provider may multiple formats may change time formats always strictly followed many ways expressing particular components example weight might expressed one 15l 1 1/2 liters 1500ml etc parts description may confused target components may white wine brand called red head vineyard weight 2000 ml may confused year etc im using wine examples sake simplicity general audience product domain conceptual issues id consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work harder get right theres straightforward method would good know id like develop generalpurpose function accept description format little experience nlp/artificial intelligence suspect useful tools/algos leverage 1000+ example records could potentially use train model something run locally would preferred necessary im looking specific implementation guidance anyone whos worked similar problem open hybrid approaches additional logic manual oversight could account initial inaccuracies appreciate insight approaches suggested learning resources ive looked online information many approaches involve significant amount front work unclear theyll work practical sense",
         "derive attribute / label short plain text description ner llm derive attribute / label short plain text description ner llm short product description i d like transform structure attribute example input output everything format would trivial write regular expression do many different format nuance increasingly cumbersome hardcode logic format try create generic solution immediately run issue basic approach several different datum provider format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even give provider may multiple format may change time format always strictly follow many way express particular component example weight might express one 15l 1 1/2 liter 1500ml etc part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year etc I m use wine example sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work hard get right there s straightforward method would good know I d like develop generalpurpose function accept description format little experience nlp / artificial intelligence suspect useful tool / algo leverage 1000 + example record could potentially use train model something run locally would prefer necessary I m look specific implementation guidance anyone who s work similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource I ve look online information many approach involve significant amount front work unclear they ll work practical sense",
         "derive attribute label short plain description ner llm derive attribute label short plain description ner llm short product description i d like transform structure attribute input everything format would trivial write regular expression do many format nuance increasingly cumbersome hardcode logic format create generic solution immediately run issue basic approach several datum provider format another provider might red 2017 la lecciaia cabernet sauvignon 750 ml even provider may multiple format may change time format always strictly many express particular component weight might express 15l 1 12 liter 1500ml part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year I wine sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough la lecciaia brand cabernet sauvignon grape variety assuming would take front hard get right there s straightforward method would good I d like develop generalpurpose function accept description format little experience nlp artificial intelligence suspect useful tool algo leverage 1000 record could potentially train something run locally would prefer necessary I specific implementation guidance anyone who s similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource I ve online information many approach involve significant amount front unclear they ll practical sense",
         "1",
         "hardcode logic,structure attribute,multiple format,nlp artificial,attribute label"
        ],
        [
         "26",
         "79102797",
         "Varying embedding dim due to changing padding in batch size",
         "<p>I want to train a simple neural network, which has <strong>embedding_dim</strong> as a parameter:</p>\n<pre><code>class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n</code></pre>\n<p>To load the data I used torchs DataLoader with a custom collate_fn.</p>\n<pre><code>train_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n</code></pre>\n<p>The collate_fn_padd function looks the following:</p>\n<pre><code>def collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels\n\n</code></pre>\n<p><strong>The problem:</strong> For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch).</p>\n<p>That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch.</p>\n<p>Due to that, I receive errors like that: <strong>mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64)</strong></p>\n<p>Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?</p>\n",
         "2024-10-18 15:54:51",
         "0",
         "42",
         "1",
         "79105117.0",
         "<p>You can add a maximum length argument set to <code>embedding_dim</code> to pad and truncate all the data to a fixed length:</p>\n<pre><code>padded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]\n</code></pre>\n",
         "1.0",
         "class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n---\ntrain_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n---\ndef collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels",
         "embedding_dim\n---\npadded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]",
         "Varying embedding dim due to changing padding in batch size",
         "I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size",
         "You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size",
         "varying embedding dim due changing padding batch size want train simple neural network embedding_dim parameter load data used torchs dataloader custom collate_fn collate_fn_padd function looks following problem every batch want train model embedded text gets padded differently long takes longest sequence current batch means embedding dim/input size linear layer neural network changes batch batch althoug want size every batch due receive errors like mat1 mat2 shapes multiplied 16x182 301x64 possible adjust collate_fn_pad function padds sequence size independet batch size",
         "vary embed dim due change padding batch size want train simple neural network embedding_dim parameter load datum use torchs dataloader custom collate_fn collate_fn_padd function look follow problem every batch want train model embed text get pad differently long take long sequence current batch mean embed dim / input size linear layer neural network change batch batch althoug want size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collate_fn_pad function padd sequence size independet batch size",
         "vary embed dim due change padding batch size train simple neural network embeddingdim parameter load datum torchs dataloader custom collatefn collatefnpadd function problem every batch train embed get pad differently long take long sequence current batch mean embed dim input size linear layer neural network change batch batch althoug size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collatefnpad function padd sequence size independet batch size",
         "7",
         "adjust collatefnpad,problem batch,network embeddingdim,padding batch,embed dim"
        ],
        [
         "27",
         "79100835",
         "How can I adjust the performance of tokenizer?",
         "<p>Working with the tokenizer from the <code>transformers</code> library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not.</p>\n<p>I'm wondering if I can <strong>&quot;adjust&quot;</strong> (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to.</p>\n<p>To be more specific, the type of tokenizer is <code>transformers.XLMRobertaTokenizerFast</code>, which is a unigram tokenizer, and the model is <code>paraphrase-multilingual-mpnet-base-v2</code>.</p>\n",
         "2024-10-18 06:45:15",
         "0",
         "45",
         "1",
         "79107575.0",
         "<p>You can change the tokenizer's vocabulary:</p>\n<pre><code>tokenizer.add_tokens([&quot;asadaf&quot;, &quot;sdfsaf&quot;])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = &quot;This is asadaf and sdfsaf&quot;\nprint(tokenizer(input_text))\n</code></pre>\n<p>As a result, <em>asadaf</em> and <em>sdfsaf</em> would be tokenized as unique words.</p>\n",
         "1.0",
         "transformers\n---\ntransformers.XLMRobertaTokenizerFast\n---\nparaphrase-multilingual-mpnet-base-v2",
         "tokenizer.add_tokens([\"asadaf\", \"sdfsaf\"])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = \"This is asadaf and sdfsaf\"\nprint(tokenizer(input_text))",
         "How can I adjust the performance of tokenizer",
         "Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is",
         "You can change the tokenizers vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is You can change the tokenizers vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not Im wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is",
         "adjust performance tokenizer working tokenizer library hugging face tokenizer works fine cases cases im wondering adjust train new tokenizer scratch performance tokenizer handle bad cases still maintaining good performance cases used specific type tokenizer unigram tokenizer model",
         "adjust performance tokenizer working tokenizer library hug face tokenizer work fine case case I m wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case use specific type tokenizer unigram tokenizer model",
         "adjust performance tokenizer working tokenizer library hug face tokenizer fine case case I wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case specific type tokenizer unigram tokenizer",
         "2",
         "tokenizer handle,tokenizer scratch,type tokenizer,face tokenizer,performance tokenizer"
        ],
        [
         "28",
         "79081924",
         "With spaCy, how can I get all lemmas from a string?",
         "<p>I have a pandas data frame with a column of text values (documents).  I want to apply lemmatization on these values with the spaCy library using the pandas <code>apply</code> function.  I've defined my <code>to_lemma</code> function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow.  Is there a way to extract the lemmatized form of a document in spaCy?</p>\n<pre><code>def to_lemma(text):\n    tp = nlp(text)\n    line = &quot;&quot;\n    for word in tp:\n        line = line + word.lemma_ + &quot; &quot;\n    return line\n</code></pre>\n",
         "2024-10-12 21:03:21",
         "-1",
         "97",
         "2",
         "79086290.0",
         "<p>There are many ways to speed up SpaCy processing. The question which of them make sense for you depends mostly on the size of your input.</p>\n<ol>\n<li>The most obvious one is not individually apply the model to every single row, but rather use batch processing. Use <code>nlp.pipe()</code> with an Iterable of strings. This means it is easier to not use apply.</li>\n<li>Disable components that you do not use. For token level processing where you need the lemmas this would be <code>'parser'</code> (the dependency parser) and <code>'ner'</code> (the Named Entity Recognition component).</li>\n<li>Increase the <code>batch_size</code> (objects to buffer) in pipe(). The default is 1000. Obviously this only makes sense to touch if you have the memory to increase it a lot.</li>\n<li>Increase the number of processors used using <code>n_process</code>. This will increase the time it takes to initially load the model but decrease the processing time. In my experience this starts making sense at about 500k+ texts. Note that this also requires the code to be run in an <code>if __name__ == '__main__':</code> wrapper.</li>\n</ol>\n<p>Basic example with 1. and 2.:</p>\n<pre><code>texts = df[&quot;column_name&quot;]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\ndf[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n<p>Advanced example for all four:</p>\n<pre><code>if __name__ == '__main__':\n    texts = df[&quot;column_name&quot;]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\n    df[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n",
         "2.0",
         "apply\n---\nto_lemma\n---\ndef to_lemma(text):\n    tp = nlp(text)\n    line = \"\"\n    for word in tp:\n        line = line + word.lemma_ + \" \"\n    return line",
         "nlp.pipe()\n---\n'parser'\n---\n'ner'\n---\nbatch_size\n---\nn_process\n---\nif __name__ == '__main__':\n---\ntexts = df[\"column_name\"]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\ndf[\"column_name_lemmas\"] = lemmas\n---\nif __name__ == '__main__':\n    texts = df[\"column_name\"]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\n    df[\"column_name_lemmas\"] = lemmas",
         "With spaCy how can I get all lemmas from a string",
         "I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy",
         "There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function Ive defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy",
         "spacy get lemmas string pandas data frame column text values documents want apply lemmatization values spacy library using pandas function ive defined function iterate words document concatenate corresponding lemmas output string however slow way extract lemmatized form document spacy",
         "spacy get lemmas string panda datum frame column text value document want apply lemmatization value spacy library use panda function I ve define function iterate word document concatenate correspond lemmas output string however slow way extract lemmatize form document spacy",
         "spacy get lemmas panda datum frame column value document apply lemmatization value spacy library panda function I ve define function iterate document concatenate correspond lemmas however slow extract lemmatize form document spacy",
         "6",
         "lemmatize form,panda function,library panda,spacy lemmas,extract lemmatize"
        ],
        [
         "29",
         "79057082",
         "Avoiding overlap in frequency and document frequency count in Quanteda",
         "<p>Below is a dummy corpus of 4 documents.</p>\n<p>The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in.</p>\n<p>The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive.</p>\n<p>Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each,\nbut are intended to be counted once, according to the dictionary.</p>\n<p>The expected overall frequency count is (extracted from the 'pattern&quot; column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key.</p>\n<p>Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key.</p>\n<p>I have three questions:</p>\n<ol>\n<li>are there any problems/issues that could affect output accuracy using this approach?</li>\n<li>is there a better/more parsimonius approach to achieve what I am trying to do?</li>\n<li>what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table?</li>\n</ol>\n<pre><code>        library (quanteda)\n        library(quanteda.textstats)\n\n        txt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\n\n\n         x &lt;- x %&gt;%\n         tokens_compound(phrase(&quot;dairy farmers&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farms&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farm&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farming&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy industry&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;indigenous australians&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;australia post&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farmer&quot;), concatenator = &quot; &quot;)\n              x\n\n         dict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign- \n         owned&quot;, &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, &quot;multinational \n         oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n         dairy = c(&quot;dairy farmers&quot;, &quot;dairy farms&quot;,&quot;dairy farm&quot;,&quot;dairy farming&quot;,&quot;dairy \n         industry&quot;, &quot;dairy farmer&quot;,&quot;dairy&quot;, &quot;milk&quot;),\n         auspost = &quot;australia post&quot;,\n         oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n         farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n         foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n         business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n         indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres \n         strait&quot;),\n         peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, &quot;people of \n         this nation&quot;, &quot;people of this country&quot;),\n         industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\n        kwicdict &lt;- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, &quot;D:/Output/TEST.csv&quot;)\n\n       DF &lt;- read.csv(&quot;D://Output/TEST.csv&quot;,header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       &gt; x2 &lt;- DF[,8]\n       &gt; \n       &gt; table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2 \n</code></pre>\n",
         "2024-10-05 12:43:52",
         "1",
         "57",
         "1",
         "79058791.0",
         "<p>I don't think that <code>kwic()</code> is what you want here. <code>tokens_lookup()</code> lets you specify that the nested scope should be mutually exclusive across keys, not just within keys. Observe the difference below. (And note the use of wildcarding for dairy key.)</p>\n<pre class=\"lang-r prettyprint-override\"><code>library(quanteda)\n#&gt; Package version: 4.1.0\n#&gt; Unicode version: 14.0\n#&gt; ICU version: 71.1\n#&gt; Parallel computing: 10 of 10 threads used.\n#&gt; See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\ndict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign-owned&quot;, \n                                     &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, \n                                     &quot;multinational oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n                        dairy = c(&quot;dairy farm*&quot;, &quot;dairy industry&quot;, &quot;dairy&quot;, &quot;milk&quot;),\n                        auspost = &quot;australia post&quot;,\n                        oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n                        farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n                        foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n                        business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n                        indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres strait&quot;),\n                        peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, \n                                 &quot;people of this nation&quot;, &quot;people of this country&quot;),\n                        industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\nx &lt;- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       5       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        1\n#&gt;   doc3        1     1       0  0       1       0        1     1    1        0\n#&gt;   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = &quot;dictionary&quot;) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       3       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        0\n#&gt;   doc3        1     1       0  0       1       0        1     1    0        0\n#&gt;   doc4        0     0       1  1       1       1        0     0    0        0\n</code></pre>\n<p><sup>Created on 2024-10-06 with <a href=\"https://reprex.tidyverse.org\" rel=\"nofollow noreferrer\">reprex v2.1.1</a></sup></p>\n",
         "0.0",
         "library (quanteda)\n        library(quanteda.textstats)\n\n        txt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\n\n\n         x <- x %>%\n         tokens_compound(phrase(\"dairy farmers\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farms\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farm\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farming\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy industry\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"indigenous australians\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"australia post\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farmer\"), concatenator = \" \")\n              x\n\n         dict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign- \n         owned\", \"foreign owned\", \"foreign companies\", \"multinational\", \"multinational \n         oil companies\", \"multinationals\", \"transnational\"),\n         dairy = c(\"dairy farmers\", \"dairy farms\",\"dairy farm\",\"dairy farming\",\"dairy \n         industry\", \"dairy farmer\",\"dairy\", \"milk\"),\n         auspost = \"australia post\",\n         oz = c(\"australia\", \"this country\", \"our country\"),\n         farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n         foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n         business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n         indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres \n         strait\"),\n         peep = c(\"australians\", \"people of australia\", \"australian people\", \"people of \n         this nation\", \"people of this country\"),\n         industry = c(\"industry\", \"industries\")))\n\n        kwicdict <- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, \"D:/Output/TEST.csv\")\n\n       DF <- read.csv(\"D://Output/TEST.csv\",header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       > x2 <- DF[,8]\n       > \n       > table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2",
         "kwic()\n---\ntokens_lookup()\n---\nlibrary(quanteda)\n#> Package version: 4.1.0\n#> Unicode version: 14.0\n#> ICU version: 71.1\n#> Parallel computing: 10 of 10 threads used.\n#> See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\ndict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign-owned\", \n                                     \"foreign owned\", \"foreign companies\", \"multinational\", \n                                     \"multinational oil companies\", \"multinationals\", \"transnational\"),\n                        dairy = c(\"dairy farm*\", \"dairy industry\", \"dairy\", \"milk\"),\n                        auspost = \"australia post\",\n                        oz = c(\"australia\", \"this country\", \"our country\"),\n                        farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n                        foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n                        business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n                        indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres strait\"),\n                        peep = c(\"australians\", \"people of australia\", \"australian people\", \n                                 \"people of this nation\", \"people of this country\"),\n                        industry = c(\"industry\", \"industries\")))\n\nx <- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       5       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        1\n#>   doc3        1     1       0  0       1       0        1     1    1        0\n#>   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = \"dictionary\") |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       3       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        0\n#>   doc3        1     1       0  0       1       0        1     1    0        0\n#>   doc4        0     0       1  1       1       1        0     0    0        0",
         "Avoiding overlap in frequency and document frequency count in Quanteda",
         "Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table",
         "I dont think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table I dont think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world Australians occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly Australia oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table doc names column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table",
         "avoiding overlap frequency document frequency count quanteda dummy corpus 4 documents dictionary developed identify frequency words phrases corpus well number documents word phrases occurs world australians occurs two dictionary keys peep indig key content intended mutually exclusive similarly australia oz australia post foreign foreign multinat farm/farmers dairy farmers occur two dictionary keys intended counted according dictionary expected overall frequency count extracted pattern column kwic table reported x2 note word industry appears allocated industry define din indig key dairy frequency occuring key occuring three documents calculated unique rows kwic table doc names column key three questions problems/issues could affect output accuracy using approach better/more parsimonius approach achieve trying would best way extract equivalent tetxstat frequency count data kwic table",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency word phrase corpus well number document word phrase occur world australian occur two dictionary key peep indig key content intend mutually exclusive similarly australia oz australia post foreign foreign multinat farm / farmer dairy farmer occur two dictionary key intend count accord dictionary expect overall frequency count extract pattern column kwic table report x2 note word industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table doc name column key three question problem / issue could affect output accuracy use approach well / more parsimonius approach achieve trying would well way extract equivalent tetxstat frequency count datum kwic table",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency phrase corpus number document phrase occur world australian occur dictionary key peep indig key content intend mutually exclusive similarly australia oz australia post foreign foreign multinat farm farmer dairy farmer occur dictionary key intend count accord dictionary expect overall frequency count extract pattern column kwic table report x2 note industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table doc name column key three question problem issue could affect accuracy approach more parsimonius approach achieve trying would extract equivalent tetxstat frequency count datum kwic table",
         "8",
         "overlap frequency,occur dictionary,document frequency,dummy corpus,count extract"
        ],
        [
         "30",
         "79005985",
         "Seq2Seq trainer.train() keeps giving indexing error",
         "<p>I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error:</p>\n<blockquote>\n<p>IndexError: Invalid key: 39463 is out of bounds for size 0.</p>\n</blockquote>\n<ul>\n<li>The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B</li>\n<li>The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also.</li>\n</ul>\n<p>Detailed error message:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;nllbtrain.py&quot;, line 273, in &lt;module&gt;\n    print(trainer.train())\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1645, in train\n    return inner_training_loop(\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 631, in __next__\n    data = self._next_data()\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2810, in __getitem__\n    return self._getitem(key)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 583, in query_table\n    _check_valid_index_key(key, size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 526, in _check_valid_index_key\n    raise IndexError(f&quot;Invalid key: {key} is out of bounds for size {size}&quot;)\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n</code></pre>\n<p>The code of the preprocessing done for the data:</p>\n<pre><code>def preprocess_function(examples):\n        inputs = [example + ' &lt;/s&gt;' + f' &lt;2{s_lang}&gt;' for example in examples[source_lang]]\n        targets = [f'&lt;2{t_lang}&gt; ' + example + ' &lt;/s&gt;' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n</code></pre>\n<p>Data after preprocessing:</p>\n<pre><code>DatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n</code></pre>\n<p>The code of model params and training:</p>\n<pre><code>model_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=&quot;hin_Deva&quot;, tgt_lang=&quot;san_Deva&quot;, max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=&quot;epoch&quot;,\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=&quot;./output_dir&quot;,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())\n\n</code></pre>\n<p>Any idea why this error is persisting?</p>\n",
         "2024-09-20 08:43:32",
         "0",
         "54",
         "1",
         "79007590.0",
         "<p><code>size 0</code> indicates that the dataset your trainer gets when the fine-tuning starts is empty. Looking at this (<a href=\"https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25</a>) and this (<a href=\"https://github.com/huggingface/datasets/issues/6535\" rel=\"nofollow noreferrer\">https://github.com/huggingface/datasets/issues/6535</a>) thread suggests adding <code>remove_unused_columns = False</code> to your <code>training_args</code> might resolve the issue, so you could give that a try.</p>\n",
         "0.0",
         "Traceback (most recent call last):\n  File \"nllbtrain.py\", line 273, in <module>\n    print(trainer.train())\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2810, in __getitem__\n    return self._getitem(key)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\n    _check_valid_index_key(key, size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n---\ndef preprocess_function(examples):\n        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]\n        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n---\nDatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n---\nmodel_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=\"hin_Deva\", tgt_lang=\"san_Deva\", max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=\"epoch\",\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=\"./output_dir\",\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())",
         "size 0\n---\nremove_unused_columns = False\n---\ntraining_args",
         "Seq2Seq trainertrain keeps giving indexing error",
         "I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting",
         "indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting",
         "seq2seq trainertrain keeps giving indexing error trying machine translation hindi sanskrit using nllb model keep getting error indexerror invalid key 39463 bounds size 0 error coming training pretrained nllb model ` facebook/nllb20013b input data ~40k hindi sentences error arises tried training sample data also detailed error message code preprocessing done data data preprocessing code model params training idea error persisting",
         "seq2seq trainertrain keep give indexing error try machine translation hindi sanskrit use nllb model keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb model ` facebook / nllb20013b input datum ~40k hindi sentence error arises try train sample datum also detail error message code preprocessing do data datum preprocesse code model param train idea error persist",
         "seq2seq trainertrain keep indexing error machine translation hindi sanskrit nllb keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb facebook nllb20013b input datum 40k hindi error arises train sample datum also detail error message preprocessing do data datum preprocesse param train idea error persist",
         "7",
         "error indexerror,hindi sanskrit,seq2seq,nllb error,trainertrain indexing"
        ],
        [
         "31",
         "78985137",
         "Alternative to device_map = \"auto\" in Huggingface Pretrained",
         "<p>I have a model that I was reading from huggingface using the following code:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n</code></pre>\n<p>Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model.</p>\n<p>Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like <code>device_map=&quot;auto&quot;</code> but after reading the model.</p>\n<p>So simply something like</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\nmodel.device_map = &quot;auto&quot;\n</code></pre>\n",
         "2024-09-14 12:42:03",
         "2",
         "1034",
         "1",
         "79007343.0",
         "<p>I found out that there are actually several methods in <code>accelerate</code> for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map</a></p>\n<p>The second one is used to match your model with the devices:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model</a></p>\n<p>So basically, in your case, you can use the following code:</p>\n<pre><code>from accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)\n</code></pre>\n<p>P.S. This code still needs to be tested on fine-tuning.</p>\n",
         "2.0",
         "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n---\ndevice_map=\"auto\"\n---\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\nmodel.device_map = \"auto\"",
         "accelerate\n---\nfrom accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)",
         "Alternative to device_map = auto in Huggingface Pretrained",
         "I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like",
         "I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers arent distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like",
         "alternative device_map = auto huggingface pretrained model reading huggingface using following code read model modifications internal layers added layers started training/finetuning get everything model investigations found custom layers arent distributed multi gpus original model need something like reading model simply something like",
         "alternative device_map = auto huggingface pretraine model reading huggingface use follow code read model modification internal layer add layer start training / finetune get everything model investigation find custom layer be not distribute multi gpus original model need something like read model simply something like",
         "alternative devicemap auto huggingface pretraine reading huggingface read modification internal layer add layer start training finetune get everything investigation custom layer be not distribute multi gpus original something like read simply something like",
         "7",
         "huggingface read,gpus original,pretraine reading,multi gpus,devicemap auto"
        ],
        [
         "32",
         "78966943",
         "How are the weights of the Mistral models reinitialized in Huggingface?",
         "<p>From <a href=\"https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic\">How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model?</a> and <a href=\"https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4</a> there's different suggestions to reinitialize the model.</p>\n<p>When I tried this, it seems to work.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;, token=&quot;hf_*****&quot;)\n\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n</code></pre>\n<p>[out]:</p>\n<pre><code>OrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n</code></pre>\n<p>How are the layers re-initialized through the <code>from_config</code> function? Is it using <a href=\"https://cs230.stanford.edu/section/4/\" rel=\"nofollow noreferrer\">Xaiver/He initialization</a> or just random initialization?</p>\n",
         "2024-09-09 19:25:52",
         "3",
         "175",
         "2",
         "78969695.0",
         "<p><a href=\"https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralConfig\" rel=\"nofollow noreferrer\">MistralConfig</a> has a default parameter <code>initializer_range</code> which is set to 0.02 and described as <code>The standard deviation of the truncated_normal_initializer for initializing all weight matrices</code>, so one can assume they use a truncated normal distribution with a standard deviation of 0.02.</p>\n<p>If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 0.02 looks like, it seems like a fit to me:</p>\n<pre><code>import numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/M67S0rKp.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/M67S0rKp.png\" alt=\"model_weights\" /></a></p>\n",
         "2.0",
         "from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", token=\"hf_*****\")\n\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n---\nOrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n---\nfrom_config",
         "initializer_range\n---\nThe standard deviation of the truncated_normal_initializer for initializing all weight matrices\n---\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()",
         "How are the weights of the Mistral models reinitialized in Huggingface",
         "From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization",
         "MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and theres different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization",
         "weights mistral models reinitialized huggingface one reinitialize weights hugging face llama v2 model official way original model theres different suggestions reinitialize model tried seems work layers reinitialized function using xaiver/he initialization random initialization",
         "weight mistral model reinitialize huggingface one reinitialize weight hug face llama v2 model official way original model there s different suggestion reinitialize model try seem work layer reinitialize function use xaiver / he initialization random initialization",
         "weight mistral reinitialize huggingface reinitialize weight hug face llama v2 official original there s suggestion reinitialize layer reinitialize function xaiver he initialization random initialization",
         "7",
         "random initialization,weight hug,reinitialize layer,weight mistral,reinitialize huggingface"
        ],
        [
         "33",
         "78957322",
         "Break after first PER sequence found with Spacy",
         "<p>I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all &quot;PER&quot; tags, but I want to reduce the overhead and get only the first contiguous sequence of &quot;PER&quot; entities. Here’s the example output I get:</p>\n<pre><code>Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n</code></pre>\n<p>But I want the result to be:</p>\n<pre><code>Detected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n</code></pre>\n<p>Here is the code I am currently using:</p>\n<pre><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}]\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == &quot;PER&quot;:\n            per_entities.append(ent.text)\n        elif ent.label_ == &quot;MISC&quot; and misc_count &lt; 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n<p>What I'm looking for:</p>\n<p>I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of &quot;PER&quot; entities in the text, ignoring any subsequent &quot;PER&quot; entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?</p>\n",
         "2024-09-06 13:14:32",
         "0",
         "39",
         "1",
         "78957722.0",
         "<p>The issues is that <code>doc[start_idx:].ents</code> is <a href=\"https://spacy.io/api/doc#ents\" rel=\"nofollow noreferrer\">only the named entities</a> in that slice of the doc. Thus, you will never process &quot;habló&quot; for the first entry, you will just go straight from &quot;García&quot; to &quot;López&quot;. To actually iterate over the tokens so that you see when the PER sequence ends, you have to leave out the <code>.ents</code> part. Then you just wait until you see the first token with <code>ent_type_</code> PER and start appending, then break after one of your conditions is met. I ended up refactoring your code a little as I debugged this, but here's an edited version of your program that produces the desired outputs:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;,\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}],\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == &quot;PER&quot;:\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == &quot;MISC&quot; and misc_count &lt; 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n",
         "1.0",
         "Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n---\nDetected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\"\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}]\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == \"PER\":\n            per_entities.append(ent.text)\n        elif ent.label_ == \"MISC\" and misc_count < 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "doc[start_idx:].ents\n---\n.ents\n---\nent_type_\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\",\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}],\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == \"PER\":\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == \"MISC\" and misc_count < 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "Break after first PER sequence found with Spacy",
         "I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this",
         "The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but heres an edited version of your program that produces the desired outputs",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but heres an edited version of your program that produces the desired outputs",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speakers name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What Im looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this",
         "break first per sequence found spacy trying extract first speakers name list texts using spacy currently function returns per tags want reduce overhead get first contiguous sequence per entities heres example output get want result code currently using im looking want modify find_per_sequence function returns first contiguous sequence per entities text ignoring subsequent per entities encountering different type entity provided function returns multiple names partial names need way ensure first name sequence included achieve",
         "break first per sequence find spacy try extract first speaker name list text use spacy currently function return per tag want reduce overhead get first contiguous sequence per entity here example output get want result code currently use I m look want modify find_per_sequence function return first contiguous sequence per entity text ignore subsequent per entity encounter different type entity provide function return multiple name partial name need way ensure first name sequence include achieve",
         "break first per sequence spacy extract first speaker name spacy currently function return per tag reduce overhead get first contiguous sequence per entity here get currently I modify findpersequence function return first contiguous sequence per entity ignore subsequent per entity encounter type entity provide function return multiple name partial name ensure first name sequence include achieve",
         "5",
         "speaker spacy,extract speaker,modify findpersequence,break sequence,sequence entity"
        ],
        [
         "34",
         "78949607",
         "Trainer huggingface - RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned",
         "<p>I recently got the following error:\n<code>RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned</code>\nwhen doing LoRA on a small LLM.</p>\n<p>I saw on a discord someone saying:</p>\n<blockquote>\n<p>The issue likely stems from the fact that you are manually placing\nyour inputs on the GPU (with to(model.device)), but the Trainer\nexpects data to be on the CPU and will handle the transfer to the GPU\ninternally.</p>\n</blockquote>\n<p>I can't find anything of the sort written in the Trainer documentation of huggingface <a href=\"https://huggingface.co/docs/transformers/en/main_classes/trainer\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/en/main_classes/trainer</a>.</p>\n<p>Is it true? If not, how can I get rid of that error?</p>\n<p>MRE:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = &quot;croissantllm/CroissantLLMBase&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=&quot;auto&quot;)\n\ntexts = [\n    &quot;The first sentence for fine-tuning. &lt;/s&gt;&quot;,\n    &quot;The second sentence for fine-tuning. &lt;/s&gt;&quot;\n]\n\ninputs = [tokenizer(text, return_tensors=&quot;pt&quot;).to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {&quot;input_ids&quot;: input_ids, &quot;labels&quot;: labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n</code></pre>\n<p>The issue is fairly easy to reproduce directly on colab (run <code>%pip install --upgrade torch transformers peft</code> in the first cell).</p>\n",
         "2024-09-04 16:09:18",
         "2",
         "1370",
         "1",
         "79112186.0",
         "<p>Since pinning memory is only available on CPU and not GPU, when running on GPU on Colab, you can just disable it by setting <code>dataloader_pin_memory</code> to <code>False</code> for <code>TrainingArguments</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n</code></pre>\n",
         "3.0",
         "RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned\n---\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = \"croissantllm/CroissantLLMBase\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\ntexts = [\n    \"The first sentence for fine-tuning. </s>\",\n    \"The second sentence for fine-tuning. </s>\"\n]\n\ninputs = [tokenizer(text, return_tensors=\"pt\").to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {\"input_ids\": input_ids, \"labels\": labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n---\n%pip install --upgrade torch transformers peft",
         "dataloader_pin_memory\n---\nFalse\n---\nTrainingArguments\n---\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned",
         "I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell",
         "Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "Trainer huggingface RuntimeError cannot pin torchcudaFloatTensor only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I cant find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensors pinned recently got following error lora small llm saw discord someone saying issue likely stems fact manually placing inputs gpu tomodeldevice trainer expects data cpu handle transfer gpu internally cant find anything sort written trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin recently get follow error lora small llm see discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not find anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin recently get error lora small llm discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "7",
         "write trainer,torchcudafloattensor dense,transfer gpu,huggingface runtimeerror,runtimeerror pin"
        ],
        [
         "35",
         "78943401",
         "Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error \"Attempting to Unscale FP16 Gradients\"",
         "<p>I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?</p>\n<p>Here is a minimal example:</p>\n<pre class=\"lang-none prettyprint-override\"><code>import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = &quot;facebook/opt-1.3b&quot;\ncache_dir = './models'\nos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;7&quot;\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=&quot;nf4&quot;,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n</code></pre>\n<p>At the line <code>scaler.step(optimizer)</code>, an error occurs:</p>\n<pre><code>Exception has occurred: ValueError: Attempting to unscale FP16 gradients.\n\n</code></pre>\n",
         "2024-09-03 08:38:23",
         "1",
         "497",
         "1",
         "78945455.0",
         "<p>You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32.</p>\n<p>You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training.</p>\n<p>What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details <a href=\"https://huggingface.co/docs/peft/en/developer_guides/quantization\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "1.0",
         "import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = \"facebook/opt-1.3b\"\ncache_dir = './models'\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n---\nscaler.step(optimizer)\n---\nException has occurred: ValueError: Attempting to unscale FP16 gradients.",
         "",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients",
         "I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs",
         "You cant finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldnt finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs You cant finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldnt finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I cant seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs",
         "finetuning pretrained model quantization amp scaler error attempting unscale fp16 gradients trying finetune pretrained model limited vram achieve using quantization automatic mixed precision amp however encountering issue cant seem resolve could please help identify problem minimal example line error occurs",
         "finetune pretraine model quantization amp scaler error attempt unscale fp16 gradient try finetune pretraine model limited vram achieve use quantization automatic mixed precision amp however encounter issue can not seem resolve could please help identify problem minimal example line error occur",
         "finetune pretraine quantization amp scaler error attempt unscale fp16 gradient finetune pretraine limited vram achieve quantization automatic mixed precision amp however encounter issue can not resolve could please help identify problem minimal line error occur",
         "7",
         "pretraine limited,fp16 gradient,line error,quantization amp,scaler error"
        ],
        [
         "36",
         "78933232",
         "Keep training pytorch model on new data",
         "<p>I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps:</p>\n<ol>\n<li>Load and process the text.</li>\n<li>Use a TF-IDF Vectorizer.</li>\n<li>Build the neural network and save the TF-IDF Vectorizer and model to predict new data.</li>\n</ol>\n<p>However, every day I need to classify new comments and correct any wrong classifications.</p>\n<p>Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct).</p>\n<p>Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen.</p>\n<p>So the mains questions are:</p>\n<ol>\n<li>How could i check if the propossed way to solve this problem work as i expect?</li>\n<li>What can i do with the vectorizer when it face new tokens, can i just do a <code>.fit_transform()</code> or i would loose the original vectorizer?</li>\n</ol>\n<p>Here its the full training process:</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        &quot;set1.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n&quot;&quot;&quot;\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n&quot;&quot;&quot;\n\nlabel_mapping = {\n    &quot;anger&quot;: 0,\n    &quot;fear&quot;: 1,\n    &quot;joy&quot;: 2,\n    &quot;sadness&quot;: 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[&quot;label&quot;])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[&quot;label&quot;]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = &quot;modelbest.pth&quot;\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\ntorch.save(model.state_dict(), model_path)\n\n</code></pre>\n<p>Proposed code:</p>\n<pre><code>import torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\nsentiments = [&quot;fear&quot;,&quot;joy&quot;,&quot;sadness&quot;,&quot;anger&quot;]\n\nnew_data = (\n    pl\n    .read_csv(\n        &quot;set2.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n    .filter(pl.col(&quot;label&quot;).is_in(sentiments))\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = &quot;modelbest.pth&quot;\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = &quot;new_moedl.pth&quot;\ntorch.save(model.state_dict(), new_best_model_path)\n</code></pre>\n<p>The dataset can be found <a href=\"https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "2024-08-30 17:47:59",
         "2",
         "271",
         "2",
         "78934212.0",
         "<p>use  pre-trained word embeddings like BertForSequenceClassification.  These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words.</p>\n<p><strong>Model Training with BERT</strong></p>\n<pre><code>import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(&quot;set1.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\ntrain_df = train_df.with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[&quot;label&quot;])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[&quot;label&quot;])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy: {results['eval_accuracy']:.4f}&quot;)\n\n# Save the model and tokenizer\nmodel.save_pretrained(&quot;saved_model&quot;)\ntokenizer.save_pretrained(&quot;saved_tokenizer&quot;)\n</code></pre>\n<p><strong>Incremental training with least effort</strong></p>\n<pre><code># Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(&quot;saved_model&quot;)\ntokenizer = BertTokenizer.from_pretrained(&quot;saved_tokenizer&quot;)\n\n# Load new data\nnew_data = (\n    pl.read_csv(&quot;set2.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n    .filter(pl.col(&quot;label&quot;).is_in([&quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;anger&quot;]))\n    .with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}&quot;)\n\n# Save the updated model\nmodel.save_pretrained(&quot;saved_model_incremental&quot;)\n</code></pre>\n",
         "3.0",
         ".fit_transform()\n---\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        \"set1.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n\"\"\"\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n\"\"\"\n\nlabel_mapping = {\n    \"anger\": 0,\n    \"fear\": 1,\n    \"joy\": 2,\n    \"sadness\": 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[\"label\"])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[\"label\"]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = \"modelbest.pth\"\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = \"text_classification_model.pth\"\ntorch.save(model.state_dict(), model_path)\n---\nimport torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = \"text_classification_model.pth\"\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\nsentiments = [\"fear\",\"joy\",\"sadness\",\"anger\"]\n\nnew_data = (\n    pl\n    .read_csv(\n        \"set2.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n    .filter(pl.col(\"label\").is_in(sentiments))\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = \"modelbest.pth\"\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = \"new_moedl.pth\"\ntorch.save(model.state_dict(), new_best_model_path)",
         "import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(\"set1.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\ntrain_df = train_df.with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[\"label\"])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[\"label\"])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")\n\n# Save the model and tokenizer\nmodel.save_pretrained(\"saved_model\")\ntokenizer.save_pretrained(\"saved_tokenizer\")\n---\n# Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"saved_model\")\ntokenizer = BertTokenizer.from_pretrained(\"saved_tokenizer\")\n\n# Load new data\nnew_data = (\n    pl.read_csv(\"set2.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n    .filter(pl.col(\"label\").is_in([\"fear\", \"joy\", \"sadness\", \"anger\"]))\n    .with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}\")\n\n# Save the updated model\nmodel.save_pretrained(\"saved_model_incremental\")",
         "Keep training pytorch model on new data",
         "Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here",
         "use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "Keep training pytorch model on new data Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "Keep training pytorch model on new data Im working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here",
         "keep training pytorch model new data im working text classification task decided use pytorch model purpose process mainly involves following steps load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new data however every day need classify new comments correct wrong classifications currently approach add new comments correct classification dataset retrain entire model process timeconsuming new comments lost validation would like create new dataset newly classified texts continue training new data new comments classified manually label correct using gpt online code write desired process however im sure working expected im making silly mistakes happen mains questions could check propossed way solve problem work expect vectorizer face new tokens would loose original vectorizer full training process proposed code dataset found",
         "keep train pytorch model new datum I m work text classification task decide use pytorch model purpose process mainly involve follow step load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new datum however every day need classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire model process timeconsume new comment lose validation would like create new dataset newly classify text continue train new datum new comment classify manually label correct use gpt online code write desire process however I m sure working expect I m make silly mistake happen main question could check proposse way solve problem work expect vectorizer face new token would loose original vectorizer full training process propose code dataset find",
         "keep train pytorch new datum I classification task decide pytorch purpose process mainly involve step load process tfidf vectorizer build neural network save tfidf vectorizer predict new datum however every day classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire process timeconsume new comment lose validation would like create new dataset newly classify continue train new datum new comment classify manually label correct gpt online write desire process however I sure working expect I make silly mistake happen main question could check proposse solve problem expect vectorizer face new token would loose original vectorizer full training process propose dataset",
         "7",
         "new dataset,classify new,retrain,vectorizer training,train pytorch"
        ],
        [
         "37",
         "78932356",
         "Capitalized words in sentiment analysis",
         "<p>I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive .\nA common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .</p>\n<p>this is my current code :</p>\n<pre><code>from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)\n</code></pre>\n",
         "2024-08-30 13:49:56",
         "-1",
         "128",
         "1",
         "78933236.0",
         "<p>If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data.</p>\n<p>If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis.</p>\n<p>If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out <a href=\"https://github.com/EliasK93/transformer-model-comparison-for-review-sentiment-regression\" rel=\"nofollow noreferrer\">this repository</a> as an example.</p>\n",
         "0.0",
         "from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)",
         "",
         "Capitalized words in sentiment analysis",
         "Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code",
         "If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "Capitalized words in sentiment analysis Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "Capitalized words in sentiment analysis Im currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like AMAZING can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code",
         "capitalized words sentiment analysis im currently working data customers reviews products sephora task classify sentiments negative neutral positive common technique text preprocessing lower case words situation upper case words like amazing hide significant emotion behind turning word lower case cause information loss would happy opinion subject still lower case words personally think creating classes distinction sentiments good good positive include importance upper case words current code",
         "capitalize word sentiment analysis I m currently work datum customer review product sephora task classify sentiment negative neutral positive common technique text preprocesse low case word situation upper case word like amazing hide significant emotion behind turn word low case cause information loss would happy opinion subject still low case word personally think create class distinction sentiment good good positive include importance upper case word current code",
         "capitalize sentiment analysis I currently datum customer review product sephora task classify sentiment negative neutral positive common technique preprocesse low case situation upper case like amazing hide significant emotion behind turn low case cause information loss would happy opinion subject still low case personally think create class distinction sentiment good good positive include importance upper case current",
         "0",
         "case like,opinion subject,sentiment negative,classify sentiment,capitalize"
        ],
        [
         "38",
         "78920095",
         "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub'",
         "<p>I've been using LLAMA 2 for research for a few months now and I import as follows:</p>\n<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(&quot;cuda&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;,torch_dtype=&quot;auto&quot;)\nmodel = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;, torch_dtype=&quot;auto&quot;, load_in_4bit=True)\n</code></pre>\n<p>It has always worked. However, today it is showing the following error:\n<strong>RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/<strong>init</strong>.py)</strong></p>\n<p>Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.</p>\n",
         "2024-08-27 17:20:42",
         "1",
         "5871",
         "1",
         "78920098.0",
         "<p>The error you're encountering is due to the <code>split_torch_state_dict_into_shards</code> function not being available in <code>huggingface-hub version &lt; 0.23.0</code>.</p>\n<p>This function is included starting from version <code>0.23.0</code>.</p>\n<p><strong>To resolve this issue, update the <code>huggingface-hub</code> library to version 0.23.0 or later</strong></p>\n<p>Also please install accelerate:</p>\n<pre><code>pip install accelerate==0.31.0\n</code></pre>\n<p>here is a git link: <strong><a href=\"https://github.com/run-llama/llama_index/discussions/14605\" rel=\"nofollow noreferrer\">https://github.com/run-llama/llama_index/discussions/14605</a></strong></p>\n",
         "4.0",
         "from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\",torch_dtype=\"auto\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\", torch_dtype=\"auto\", load_in_4bit=True)",
         "split_torch_state_dict_into_shards\n---\nhuggingface-hub version < 0.23.0\n---\n0.23.0\n---\nhuggingface-hub\n---\npip install accelerate==0.31.0",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub",
         "Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook",
         "The error youre encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook The error youre encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "cannot import name split_torch_state_dict_into_shards from huggingface_hub Ive been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name split_torch_state_dict_into_shards from huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didnt work I am using Google Colab and Kaggle Notebook",
         "import name split_torch_state_dict_into_shards huggingface_hub ive using llama 2 research months import follows always worked however today showing following error runtimeerror failed import transformersmodelsllamamodeling_llama following error look see traceback failed import transformersgenerationutils following error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py recreated hugging face token didnt work using google colab kaggle notebook",
         "import name split_torch_state_dict_into_shard huggingface_hub I ve use llama 2 research month import follows always work however today show follow error runtimeerror fail import transformersmodelsllamamodeling_llama follow error look see traceback fail import transformersgenerationutil follow error look see traceback import name split_torch_state_dict_into_shards huggingface_hub /opt / conda / lib / python310 / sitepackage / huggingface_hub/ init py recreate hug face token do not work use google colab kaggle notebook",
         "import name splittorchstatedictintoshard huggingfacehub I ve llama 2 research month import follows always however today show error runtimeerror fail import transformersmodelsllamamodelingllama error traceback fail import transformersgenerationutil error traceback import name splittorchstatedictintoshards huggingfacehub opt conda lib python310 sitepackage huggingfacehub init py recreate hug face token do not google colab kaggle notebook",
         "4",
         "sitepackage huggingfacehub,transformersgenerationutil error,transformersmodelsllamamodelingllama error,fail import,splittorchstatedictintoshards huggingfacehub"
        ],
        [
         "39",
         "78917743",
         "How to Process Data on GPU Instead of RAM for This Python Code?",
         "<p>I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance.\nmy code :</p>\n<pre><code>def prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n    batch[&quot;input_features&quot;] = feature_extractor(\n        audio[&quot;array&quot;], \n        sampling_rate=audio[&quot;sampling_rate&quot;]\n    ).input_features[0]\n    batch[&quot;labels&quot;] = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[&quot;train&quot;], \n    num_proc=1\n)\n</code></pre>\n<p>How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!</p>\n",
         "2024-08-27 08:03:28",
         "1",
         "61",
         "1",
         "78918851.0",
         "<p>you can using the following code to process audio data on GPU</p>\n<pre><code>import torch\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n\n    input_features = feature_extractor(audio[&quot;array&quot;], sampling_rate=audio[&quot;sampling_rate&quot;]).input_features[0]\n    batch[&quot;input_features&quot;] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    batch[&quot;labels&quot;] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[&quot;train&quot;])\n</code></pre>\n",
         "2.0",
         "def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n    batch[\"input_features\"] = feature_extractor(\n        audio[\"array\"], \n        sampling_rate=audio[\"sampling_rate\"]\n    ).input_features[0]\n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[\"train\"], \n    num_proc=1\n)",
         "import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    input_features = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n    batch[\"input_features\"] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[\"sentence\"]).input_ids\n    batch[\"labels\"] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"])",
         "How to Process Data on GPU Instead of RAM for This Python Code",
         "Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated",
         "you can using the following code to process audio data on GPU",
         "How to Process Data on GPU Instead of RAM for This Python Code Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated you can using the following code to process audio data on GPU",
         "How to Process Data on GPU Instead of RAM for This Python Code Im currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated",
         "process data gpu instead ram python code im currently using following code process audio data runs ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific changes much appreciated",
         "process datum gpu instead ram python code I m currently use follow code process audio data run ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific change much appreciate",
         "process datum gpu instead ram python I currently process audio data run ram offload processing gpu improve performance modify utilize gpu processing instead ram guidance specific change much appreciate",
         "8",
         "process audio,gpu improve,run ram,datum gpu,ram python"
        ],
        [
         "40",
         "78912171",
         "How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation",
         "<p>I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this.</p>\n<p><strong>What I’ve Tried:</strong></p>\n<ul>\n<li><p><strong>Initial Attempt:</strong> I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me.</p>\n</li>\n<li><p><strong>Using Forward Hooks:</strong> To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation:</p>\n</li>\n</ul>\n<pre class=\"lang-py prettyprint-override\"><code># VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = &quot;Helsinki-NLP/opus-mt-en-de&quot;\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = &quot;Please translate this to German.&quot;\ninputs = tokenizer(input_text, return_tensors=&quot;pt&quot;)\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(&quot;layers, heads, output tokens, input tokens&quot;)\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(&quot;Input Tokens&quot;)\nplt.ylabel(&quot;Output Tokens&quot;)\nplt.title(&quot;Cross-Attention Matrix&quot;)\nplt.show()\n</code></pre>\n<p>This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation.</p>\n<p><strong>My Question</strong></p>\n<p>Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8?</p>\n<p>I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.</p>\n",
         "2024-08-25 20:13:54",
         "1",
         "380",
         "1",
         "78915504.0",
         "<p>Huggingface has built in methods to return attention weights</p>\n<pre class=\"lang-py prettyprint-override\"><code>translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n&gt; odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n</code></pre>\n<p>With <code>return_dict_in_generate=True</code>, <code>model.generate</code> returns a dict-like object. With <code>output_attentions=True</code>, the output dict will contain all attention weights.</p>\n<p>For this model, it will include encoder attentions, decoder attentions and cross attentions.</p>\n",
         "3.0",
         "# VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = \"Helsinki-NLP/opus-mt-en-de\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = \"Please translate this to German.\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(\"layers, heads, output tokens, input tokens\")\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(\"Input Tokens\")\nplt.ylabel(\"Output Tokens\")\nplt.title(\"Cross-Attention Matrix\")\nplt.show()",
         "translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n> odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n---\nreturn_dict_in_generate=True\n---\nmodel.generate\n---\noutput_attentions=True",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation",
         "I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this",
         "Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the models translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that Im currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this",
         "visualize crossattention matrices marianmtmodel output generation working machine translation task using marianmtmodel hugging face transformers library specifically want visualize crossattention matrices models translation process however encountered difficulties achieving ive tried initial attempt noticed crossattention matrices directly returned model generates translation example found involved feeding source text translation model however goal access crossattention matrices model generates output translation given using forward hooks achieve implemented forward hooks key query projections attention mechanism disabling keyvalue caching use_cache=false capture full matrices last step heres implementation approach seemed work capturing crossattention matrices however observed matrices 4 attention heads instead expected 8 makes question correctness implementation question given issues ive encountered reliable method extract visualize crossattention matrices translation process additionally current approach fundamentally okay resolve issue capturing 4 attention heads instead 8 suspect issue might related im currently reshaping key k query q tensors head dimension multiplication wanted ask advice case theres easier effective way",
         "visualize crossattention matrix marianmtmodel output generation work machine translation task use marianmtmodel hug face transformer library specifically want visualize crossattention matrix model translation process however encounter difficulty achieve I ve try initial attempt notice crossattention matrix directly return model generate translation example find involved feeding source text translation model however goal access crossattention matrix model generate output translation give use forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache use_cache = false capture full matrix last step here implementation approach seem work capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question give issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate I m currently reshape key k query q tensor head dimension multiplication want ask advice case there s easy effective way",
         "visualize crossattention matrix marianmtmodel generation machine translation task marianmtmodel hug face transformer library specifically visualize crossattention matrix translation process however encounter difficulty achieve I ve initial attempt notice crossattention matrix directly return generate translation involved feeding source translation however goal access crossattention matrix generate translation forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache usecache false capture full matrix last step here implementation approach capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate I currently reshape key k query q tensor head dimension multiplication ask advice case there s easy effective",
         "7",
         "generate translation,matrix attention,translation process,visualize crossattention,crossattention matrix"
        ],
        [
         "41",
         "78905614",
         "Why doesn't permuting positional encodings in BERT affect the output as expected?",
         "<p>I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another <a href=\"https://stackoverflow.com/questions/78902301/why-doesnt-permuting-positional-encodings-in-gpt-2-affect-the-output-as-expecte/78903454#78903454\">question</a> I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results.</p>\n<p><strong>What I expected to happen:</strong></p>\n<ul>\n<li>No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary.</li>\n<li>Permuting only the input IDs should return distribution B.</li>\n<li>Permuting only the positional embeddings should return distribution B.</li>\n<li>Permuting both the input IDs and positional embeddings should return distribution A.</li>\n</ul>\n<p><strong>What actually happens:</strong>\nSometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.</p>\n<p><strong>My question is:</strong> Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding?</p>\n<p>For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in <code>masked_prediction</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(&quot;transformers.modeling_utils&quot;).setLevel(logging.ERROR)\nwarnings.simplefilter(&quot;ignore&quot;, FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) &gt; 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(&quot;You can't use an empty sequence for prediction&quot;)\n            return\n        \n        model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(&quot;You need to include a [MASK] token for prediction&quot;)\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=&quot;Run Prediction&quot;)\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)\n</code></pre>\n",
         "2024-08-23 11:12:08",
         "1",
         "74",
         "1",
         "78906902.0",
         "<p>The model inputs have token ids and position ids. There are four scenarios to consider:</p>\n<ol>\n<li>Baseline. Correct order for tokens and positions</li>\n<li>Permute position ids only</li>\n<li>Permute token ids only</li>\n<li>Permute position ids and token ids</li>\n</ol>\n<p>You are correct that scenario 1 and 4 should produce the same results. However you are incorrect in assuming that permuting tokens or positions separately should give the same result. Consider:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n</code></pre>\n<p>In <code>Ex1</code>, the model is told that token <code>2</code> occurs at position <code>0</code>. In <code>Ex2</code>, the model is told that token <code>2</code> occurs at position <code>1</code>. Even though we used the same permutation, the mapping of tokens to positions is different. This results in different model outputs.</p>\n<p>The reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. This is luck - the average case produces different results.</p>\n<p>It is simple to test this. Huggingface models take a <code>position_ids</code> input parameter. We can use this to test permutations of the input ids without messing with the weight matrices.</p>\n<p>To test this, we'll create input data, permute as needed, compute logits and compare logits.</p>\n<p>When comparing logits, we will permute or depermute as needed to compare on a token to token basis. For example if token <code>i</code> in scenario 1 is permuted to token <code>j</code> in scenario 3, we want to compare logits <code>i</code> from scenario 1 to logits <code>j</code> in scenario 3.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f&quot;Comparison {sa}, {sb}: {val.item():.6f}&quot;)\n</code></pre>\n<p>The code should produce an output like:</p>\n<pre><code>Comparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n</code></pre>\n<p>Run the code a bunch of times. You will find that the <code>S1, S4</code> comparison <em>always</em> has a small deviation. This is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues.</p>\n<p>You will find the <code>S2, S3</code> comparison generally has a large deviation, but <em>sometimes</em> has a small deviation. As discussed, this is due to getting a lucky permutation where positions and ids mostly line up.</p>\n",
         "2.0",
         "masked_prediction\n---\nimport torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) > 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(\"You can't use an empty sequence for prediction\")\n            return\n        \n        model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(\"You need to include a [MASK] token for prediction\")\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=\"Run Prediction\")\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)",
         "# Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n---\nEx1\n---\n2\n---\n0\n---\nEx2\n---\n2\n---\n1\n---\nposition_ids\n---\ni\n---\nj\n---\ni\n---\nj\n---\nimport torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f\"Comparison {sa}, {sb}: {val.item():.6f}\")\n---\nComparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n---\nS1, S4\n---\nS2, S3",
         "Why doesnt permuting positional encodings in BERT affect the output as expected",
         "I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in",
         "The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this well create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "Why doesnt permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this well create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "Why doesnt permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that dont use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Faces BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in",
         "doesnt permuting positional encodings bert affect output expected working jupyter notebook transformers section positional encodings want demonstrate transformer relies entirely positional encoding understand order sequence previously learned another question posted concept applies models dont use masked attention like gpt2 however attempted approach bert model uses crossattention predict mask token encountered unexpected results expected happen permutation cause model predict different token ie distribution consistent vocabulary permuting input ids return distribution b permuting positional embeddings return distribution b permuting input ids positional embeddings return distribution actually happens sometimes results align expectations times permuting one aspect either input ids positional embeddings leads different outcomes even though occasionally produce result question something else hugging faces bert model might influenced position beyond positional encoding completeness included full code part notebook tried directly important part happens",
         "do not permute positional encoding bert affect output expect work jupyter notebook transformer section positional encoding want demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply model do not use mask attention like gpt2 however attempt approach bert model use crossattention predict mask token encounter unexpected result expect happen permutation cause model predict different token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes result align expectation time permute one aspect either input ids positional embedding lead different outcome even though occasionally produce result question something else hug face bert model might influence position beyond positional encoding completeness include full code part notebook try directly important part happen",
         "do not permute positional encoding bert affect expect jupyter notebook transformer section positional encoding demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply do not mask attention like gpt2 however attempt approach bert crossattention predict mask token encounter unexpected expect happen permutation cause predict token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes align expectation time permute aspect either input ids positional embedding lead outcome even though occasionally produce question something else hug face bert might influence position beyond positional encoding completeness include full part notebook directly important part happen",
         "2",
         "positional embedding,permute positional,bert influence,bert crossattention,positional encoding"
        ],
        [
         "42",
         "78901998",
         "How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus?",
         "<p>I'm working with the <code>OpenAIEmbeddings()</code> class from <code>OpenAI</code>, which uses the <code>text-embedding-3-small</code> model. According to the <a href=\"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\" rel=\"nofollow noreferrer\">documentation</a>, it generates a 1536-dimensional vector for any input text.</p>\n<p>However, I'm a bit confused about how this works:</p>\n<ul>\n<li>Is the 1536-dimensional vector generated for the entire input text?</li>\n<li>If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</li>\n</ul>\n<p><strong>I was expecting this:</strong></p>\n<p>If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536.</p>\n<p>But the output is a single vector of size 1536 for the whole input text.</p>\n<p>Why I expected this?</p>\n<p>Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings?</p>\n<p>I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input.</p>\n<p>Any insights or examples would be greatly appreciated!</p>\n",
         "2024-08-22 14:09:25",
         "2",
         "580",
         "1",
         "78902136.0",
         "<p>Everything you described is 100% expected.</p>\n<h3>Q: Is the 1536-dimensional vector generated for the entire input text?</h3>\n<p>A: Yes.</p>\n<h3>Q: If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</h3>\n<p>A: First, the OpenAI Embeddings model doesn't handle a single word any different than a long text. For the model, it's an input. The input can be even a single character (e.g., &quot;a&quot;), but it doesn't make sense to calculate an embedding vector out of it since &quot;a&quot; doesn't semantically mean anything to us humans.</p>\n<p>Second, what you probably meant with this question is what happens when you do a similarity search with these embeddings. In other words, what happens when you <em>use</em> them? What happens if you use embeddings of words, sentences, paragraphs, or the whole text? Does it matter? Yes!</p>\n<p>This is called chunking. The decision about how to chunk your text depends on the use case. The best thing is probably to simply try and see. If you get meaningful results after doing a similarity search, then this means that chunking is appropriate (even if this means chunking the whole text). If you don't get meaningful results after doing a similarity search, then this means that chunking isn't appropriate (e.g., instead of chunking by paragraph, try chunking by sentences).</p>\n<p>There's an excellent Stack Overflow <a href=\"https://stackoverflow.blog/2024/06/06/breaking-up-is-hard-to-do-chunking-in-rag-applications/\">blog post</a> about this topic you should read (pay attention to the bolded text because this is the best explanation):</p>\n<blockquote>\n<p>With RAG, you create text embeddings of the pieces of data that you\nwant to draw from and retrieve. That allows you to place a piece of\nthe source text within the semantic space that LLMs use to create\nresponses.</p>\n<p>/.../</p>\n<p>When it comes to RAG systems, you’ll need to pay special attention to\nhow big the individual pieces of data are. How you divide your data up\nis called chunking, and it’s more complex than embedding whole\ndocuments.</p>\n<p>/.../</p>\n<p>The size of the chunked data is going to make a huge difference in\nwhat information comes up in a search. When you embed a piece of data,\nthe whole thing is converted into a vector. <strong>Include too much in a\nchunk and the vector loses the ability to be specific to anything it\ndiscusses. Include too little and you lose the context of the data.</strong></p>\n</blockquote>\n",
         "3.0",
         "OpenAIEmbeddings()\n---\nOpenAI\n---\ntext-embedding-3-small",
         "",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus",
         "Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated",
         "Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesnt handle a single word any different than a long text For the model its an input The input can be even a single character eg a but it doesnt make sense to calculate an embedding vector out of it since a doesnt semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you dont get meaningful results after doing a similarity search then this means that chunking isnt appropriate eg instead of chunking by paragraph try chunking by sentences Theres an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesnt handle a single word any different than a long text For the model its an input The input can be even a single character eg a but it doesnt make sense to calculate an embedding vector out of it since a doesnt semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you dont get meaningful results after doing a similarity search then this means that chunking isnt appropriate eg instead of chunking by paragraph try chunking by sentences Theres an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus Im working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However Im a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning ive understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings Im trying to understand whether theres a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated",
         "openaiembeddings work creating single vector size 1536 whole text corpus im working class uses model according documentation generates 1536dimensional vector input text however im bit confused works 1536dimensional vector generated entire input text 1536dimensional vector represents entire input text model handle individual words versus longer texts like sentences paragraphs expecting 100 words input text expected openaiembeddings would output 100 vectors size 1536 output single vector size 1536 whole input text expected learning ive understood embeddings like word2vec glove provide vectors word corpus differ approach taken openaiembeddings im trying understand whether theres way extract embeddings individual words using model output always single vector representing whole input insights examples would greatly appreciated",
         "openaiembedding work create single vector size 1536 whole text corpus I m work class use model accord documentation generate 1536dimensional vector input text however I m bit confused work 1536dimensional vector generate entire input text 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph expect 100 word input text expect openaiembedding would output 100 vector size 1536 output single vector size 1536 whole input text expect learning I ve understand embedding like word2vec glove provide vector word corpus differ approach take openaiembedding I m try understand whether there s way extract embedding individual word use model output always single vector represent whole input insight example would greatly appreciate",
         "openaiembedding create single vector size 1536 whole corpus I class accord documentation generate 1536dimensional vector input however I bit confused 1536dimensional vector generate entire input 1536dimensional vector represent entire input handle individual versus long like paragraph expect 100 input expect openaiembedding would 100 vector size 1536 single vector size 1536 whole input expect learning I ve understand embedding like word2vec glove provide vector corpus differ approach take openaiembedding I understand whether there s extract embedding individual always single vector represent whole input insight would greatly appreciate",
         "2",
         "vector size,vector corpus,expect openaiembedding,openaiembedding create,understand embedding"
        ],
        [
         "43",
         "78895710",
         "NER versus LLM to extract name, gender, role and company from text",
         "<p>I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons.</p>\n<p>I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them.</p>\n<p>Is there another, smaller LLM that might be good at this while using fewer processing resources?</p>\n<p>Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.)</p>\n<p>Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass?</p>\n<p>Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction.</p>\n<p>Thanks in advance!</p>\n",
         "2024-08-21 07:39:13",
         "1",
         "1524",
         "2",
         "78896098.0",
         "<p>Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. <code>NER</code> is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use <code>NLTK</code> or <code>SpaCy</code> for this matter. My personal choice is <code>SpaCy</code>, however a <code>gender</code> as you defined is not a known named entity. you can see a list of named entities in <a href=\"https://github.com/explosion/spaCy/discussions/9147\" rel=\"nofollow noreferrer\">this doc</a>.</p>\n<p>I guess what you mean by <code>gender</code> is the possible <code>gender</code> associated with the names of a <code>PERSON</code> mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use <a href=\"https://pypi.org/project/gender-guesser/\" rel=\"nofollow noreferrer\"><code>gender-guesser</code> package</a>.</p>\n<p>A possible solution would be like this:</p>\n<pre><code>import spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == &quot;PERSON&quot;:\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n</code></pre>\n<p>Note that <code>en_core_web_sm</code> is the small model available via spaCy, you can use the large model by specifying <code>en_core_web_lg</code>, just make sure that the model is downloaded before running your code. here's how you can download the model:</p>\n<pre><code>python -m spacy download en_core_web_sm\n</code></pre>\n",
         "1.0",
         "",
         "NER\n---\nNLTK\n---\nSpaCy\n---\nSpaCy\n---\ngender\n---\ngender\n---\ngender\n---\nPERSON\n---\ngender-guesser\n---\nimport spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == \"PERSON\":\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n---\nen_core_web_sm\n---\nen_core_web_lg\n---\npython -m spacy download en_core_web_sm",
         "NER versus LLM to extract name gender role and company from text",
         "I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance",
         "Apart from your limitations I wouldnt recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code heres how you can download the model",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance Apart from your limitations I wouldnt recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code heres how you can download the model",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons Ive been playing around with Llama 31 but Im finding I dont get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs Ive looked into extract name but not gender I dont know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point Im much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance",
         "ner versus llm extract name gender role company text need extract name gender job title employer/company name newspaper articles running process local hardware cloud allowed due copyright reasons ive playing around llama 31 im finding dont get useable results models smaller 70b parameters size models run much slowly best hardware throw another smaller llm might good using fewer processing resources ner use extract data ners ive looked extract name gender dont know extract data gender showstopper alternatively approach take first pass ner pass names llm together original newspaper article extract data get better results faster single llm pass answer training model good model use starting point im much beginning machine learning journey would love pointed right direction thanks advance",
         "ner versus llm extract name gender role company text need extract name gender job title employer / company name newspaper article running process local hardware cloud allow due copyright reason I ve play around llama 31 I m find do not get useable result model small 70b parameter size model run much slowly good hardware throw another small llm might good use few processing resource ner use extract data ner I ve look extract name gender do not know extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get well result fast single llm pass answer training model good model use start point I m much begin machine learn journey would love point right direction thank advance",
         "ner versus llm extract name gender role company extract name gender job title employer company name newspaper article running process local hardware cloud allow due copyright reason I ve play around llama 31 I do not get useable small 70b parameter size run much slowly good hardware throw another small llm might good few processing resource ner extract data ner I ve extract name gender do not extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get fast single llm pass answer training good start point I much begin machine learn journey would love point right direction thank advance",
         "1",
         "ner pass,ner versus,extract data,llm extract,extract gender"
        ],
        [
         "44",
         "78887743",
         "Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask?",
         "<p>In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%).</p>\n<p>I am curious about the following:</p>\n<p>When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively?\nDoes the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact?</p>\n<p>Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%?</p>\n<p>Thank you for your insights!</p>\n",
         "2024-08-19 11:49:06",
         "1",
         "525",
         "1",
         "78890409.0",
         "<p>Attention is computed on a tensor of shape <code>(batch_size, sequence_length, embedding_dimension)</code>. The compute and memory requirements scale with the size of those dimensions.</p>\n<p>For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size.</p>\n<p>With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length <code>8</code> and 10 sequences of length <code>128</code>. Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of <code>128</code>. If you sort by length before batching, you get one batch with sequence length of <code>8</code> and another with length <code>128</code>. The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128).</p>\n<p>That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to &quot;skip over&quot; padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.</p>\n",
         "2.0",
         "",
         "(batch_size, sequence_length, embedding_dimension)\n---\n8\n---\n128\n---\n128\n---\n8\n---\n128",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask",
         "In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights",
         "Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding youre not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you arent going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesnt work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding youre not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you arent going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesnt work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights",
         "padding batch sequences affect performance effective attention mask transformer models sequences variable lengths typically padded maximum length batch however sequence lengths vary batch may contain substantial amount padding potentially 50 % curious following pytorch computes transformer padding tokens impact calculation speed negatively presence attention mask allow model effectively skip padding tokens resulting minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero values computation effectively reduce approximately 10 % thank insights",
         "padding batch sequence affect performance effective attention mask transformer model sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 % curious follow pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow model effectively skip padding token result minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero value computation effectively reduce approximately 10 % thank insight",
         "padding batch sequence affect performance effective attention mask transformer sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 curious pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow effectively skip padding token minimal performance impact overall effective attention mask sparse attention mask 10 nonzero value computation effectively reduce approximately 10 thank insight",
         "2",
         "typically pad,transformer sequence,padding token,effective attention,length batch"
        ],
        [
         "45",
         "78865486",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text",
         "<p>Using the following Matcher rule:</p>\n<pre><code>{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }\n</code></pre>\n<p>on the text: 'MyLabel: Some Value'</p>\n<p>I get <strong>two</strong> matches: 'MyLabel' and 'MyLabel:'</p>\n<p>For me, that was quite surprising - I was expecting a single match on 'MyLabel:'.\nAdding the new greedy flag didn't make any difference.</p>\n<ul>\n<li>Is this the intended behavior or is it a bug?</li>\n<li>How should I determine that the second match really is just a subset of the first match?</li>\n<li>Will the shorter match always be reported before the longer match?</li>\n</ul>\n<p>SpaCy version 3.7.5</p>\n",
         "2024-08-13 09:37:23",
         "1",
         "34",
         "1",
         "78870921.0",
         "<p>i will say that the behavior you're observing with the SpaCy <code>Matcher</code> is expected, and it is not a bug. When you use the <code>{'TEXT': ':', 'OP': '?'}</code> pattern, the <code>OP: '?'</code> operator means that the colon is optional, so the matcher will generate both the shorter and the longer match, as you've seen.</p>\n<h3>Explanation:</h3>\n<ul>\n<li><strong>Pattern</strong>: <code>{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}</code>.</li>\n<li><strong>Text</strong>: <code>'MyLabel: Some Value'</code>.</li>\n</ul>\n<p>So for this pattern, SpaCy  will try to match:</p>\n<ol>\n<li><code>'MyLabel'</code> alone (because the colon is optional).</li>\n<li><code>'MyLabel:'</code> (because the colon can be included).</li>\n</ol>\n<p>Therefore, you will get two matches: <code>'MyLabel'</code> and <code>'MyLabel:'</code>.</p>\n<h3>Now to  Answer Your Questions:</h3>\n<ol>\n<li><p><strong>Is this the intended behavior or is it a bug?</strong></p>\n<ul>\n<li>This is intended behavior. The <code>OP: '?'</code> operator allows the colon to be optionally matched, leading to multiple matches.</li>\n</ul>\n</li>\n<li><p><strong>How should I determine that the second match really is just a subset of the first match?</strong></p>\n<ul>\n<li>To determine if one match is a subset of another, you can compare the start and end indices of the matches. The longer match will have the same start index but a different end index. Now i wrote a code below even using spacy version 3.7.5, see details below</li>\n</ul>\n</li>\n</ol>\n<pre><code>pip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n</code></pre>\n<p>Now Example in code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\ndoc = nlp(&quot;MyLabel: Some Value&quot;)\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(&quot;R-1&quot;, [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f&quot;Match: {span.text}, Start: {start}, End: {end}&quot;)\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start &gt;= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f&quot;Filtered Match: {span.text}&quot;)\n</code></pre>\n<p>Now, This code will filter out the shorter match and your output will be</p>\n<pre><code>Match: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n\n</code></pre>\n<ol start=\"3\">\n<li><strong>Now Will the shorter match always be reported before the longer match?</strong>\n<ul>\n<li>I don't think the matches are not guaranteed to be reported in a specific order. so to handle this, you can sort the matches by their start and end indices as shown in the code example above.Now, After sorting, you can now filter out matches that are subsets of longer matches.</li>\n</ul>\n</li>\n</ol>\n<h3>Another Alternative Solution:</h3>\n<p>If you want to ensure that only the longest match is returned, you can change the way you define the pattern:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n</code></pre>\n<p>note that the <code>greedy</code> flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings.</p>\n<h3>Now back to the Summary of what i explained:</h3>\n<ul>\n<li>The behavior you're seeing is by design, due to the optional <code>OP: '?'</code> operator.</li>\n<li>in addition, you can filter out the shorter match by comparing start and end indices of the matches.</li>\n<li>furthermore, Sorting the matches by start and end indices allows you to keep only the longest, non-overlapping matches.</li>\n</ul>\n",
         "1.0",
         "{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }",
         "Matcher\n---\n{'TEXT': ':', 'OP': '?'}\n---\nOP: '?'\n---\n{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}\n---\n'MyLabel: Some Value'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\nOP: '?'\n---\npip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"MyLabel: Some Value\")\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(\"R-1\", [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f\"Match: {span.text}, Start: {start}, End: {end}\")\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start >= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f\"Filtered Match: {span.text}\")\n---\nMatch: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n---\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n---\ngreedy\n---\nOP: '?'",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text",
         "Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375",
         "i will say that the behavior youre observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as youve seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I dont think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesnt change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior youre seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375 i will say that the behavior youre observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as youve seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I dont think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesnt change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior youre seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text MyLabel Some Value I get two matches MyLabel and MyLabel For me that was quite surprising I was expecting a single match on MyLabel Adding the new greedy flag didnt make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375",
         "spacy matcher optional suffix pattern reports multiple matches text using following matcher rule text mylabel value get two matches mylabel mylabel quite surprising expecting single match mylabel adding new greedy flag didnt make difference intended behavior bug determine second match subset first match shorter match always reported longer match spacy version 375",
         "spacy matcher optional suffix pattern report multiple match text use follow matcher rule text mylabel value get two match mylabel mylabel quite surprising expect single match mylabel add new greedy flag do not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375",
         "spacy matcher optional suffix pattern report multiple match matcher rule mylabel value get match mylabel mylabel quite surprising expect single match mylabel add new greedy flag do not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375",
         "9",
         "match short,rule mylabel,match report,spacy matcher,match mylabel"
        ],
        [
         "46",
         "78862691",
         "`mlflow.transformers.log_model()` does not finish",
         "<h3>Problem</h3>\n<p>I want to use <code>mlflow.transformers.log_model()</code> to log a finetuned huggingface model.</p>\n<p><strong>However, when the <code>mlflow.transformers.log_model</code> method is running, it simply does not finish - runs forever - throws no errors.</strong></p>\n<p>I suspect my configuration is not right, the model is too big?\nThe output says <code>Skipping saving pretrained model weights to disk</code> so that should not be the problem.</p>\n<p>Any ideas how to do this properly?</p>\n<h3>Example</h3>\n<p>This is more or less how my setup looks like, you cannot run this, it includes some pseudocode...</p>\n<p>I am on python 3.11.9 with <code>transformers = &quot;^4.41.2&quot;</code> &amp; <code>mlflow = &quot;^2.15.1&quot;</code>.</p>\n<pre><code>import mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = &quot;LeoLM/leo-hessianai-7b-chat-bilingual&quot;\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=&quot;auto&quot;,\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n<p>The last output I get in the console is:</p>\n<pre><code>INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(&quot;Setuptools is replacing distutils.&quot;)\n</code></pre>\n",
         "2024-08-12 16:27:32",
         "0",
         "337",
         "1",
         "78877979.0",
         "<p>Before defining the trainer, the model has be turned into a Peft model object via <code>get_peft_model</code>, then the <code>mlflow.transformers.log_model</code> works:</p>\n<pre><code>from peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n",
         "0.0",
         "mlflow.transformers.log_model()\n---\nmlflow.transformers.log_model\n---\nSkipping saving pretrained model weights to disk\n---\ntransformers = \"^4.41.2\"\n---\nmlflow = \"^2.15.1\"\n---\nimport mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = \"LeoLM/leo-hessianai-7b-chat-bilingual\"\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )\n---\nINFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")",
         "get_peft_model\n---\nmlflow.transformers.log_model\n---\nfrom peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )",
         "`mlflowtransformerslog_model` does not finish",
         "Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is",
         "Before defining the trainer the model has be turned into a Peft model object via then the works",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is Before defining the trainer the model has be turned into a Peft model object via then the works",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is",
         "` mlflowtransformerslog_model ` finish problem want use log finetuned huggingface model however method running simply finish runs forever throws errors suspect configuration right model big output says problem ideas properly example less setup looks like run includes pseudocode python 3119 & last output get console",
         "` mlflowtransformerslog_model ` finish problem want use log finetune huggingface model however method run simply finish run forever throw error suspect configuration right model big output say problem idea properly example less setup look like run include pseudocode python 3119 & last output get console",
         "mlflowtransformerslogmodel finish problem log finetune huggingface however method run simply finish run forever throw error suspect configuration right big say problem idea properly less setup like run include pseudocode python 3119 last get console",
         "4",
         "python 3119,simply finish,run forever,problem log,mlflowtransformerslogmodel"
        ],
        [
         "47",
         "78853409",
         "NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation)",
         "<p>I'm attempting to fine-tune the NLLB model <code>&quot;facebook/nllb-200-distilled-600M&quot;</code> for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb.</p>\n<p>Documentation: <a href=\"https://github.com/facebookresearch/fairseq/tree/nllb?tab=readme-ov-file\" rel=\"nofollow noreferrer\">link</a></p>\n<p>This is the code block which is giving error:</p>\n<pre><code>DATA_CONFIG = &quot;/content/sample_data/data_config.json&quot;\nOUTPUT_DIR = &quot;/content/outputs&quot;\nMODEL_FOLDER = &quot;/content/drive/MyDrive/Thesis/nllb-checkpoints&quot;\nDROP = 0.1\nSRC = &quot;eng_Latn&quot;\nTGT = &quot;deu_Latn&quot;\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=&quot;$SRC-$TGT&quot; \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n</code></pre>\n<p>This is the error:</p>\n<pre><code>/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=&quot;conf&quot;, config_name=&quot;base_config&quot;)\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 289, in main\n    train_module = TrainModule(config)\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n</code></pre>\n<p>So far, I understand there is a <code>Missing data_prefix configuration</code>. I created a demo custom data_config.json. Which looks like this:</p>\n<pre><code>{\n    &quot;data_prefix&quot;: &quot;/content/sample_data&quot;,\n    &quot;train_data&quot;: &quot;train_demo.json&quot;,\n    &quot;test_data&quot;: &quot;test_demo.json&quot;,\n    &quot;lang_pairs&quot;: &quot;eng_Latn-deu_Latn&quot;\n}\n</code></pre>\n<p>While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?</p>\n",
         "2024-08-09 14:46:54",
         "1",
         "148",
         "1",
         "78854613.0",
         "<p>While I can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided JSON files), my personal recommendation would be to fine-tune NLLB in the <code>transformers</code> library, specifically using the <code>Seq2SeqTrainer</code>.</p>\n<p>I did this before for multiple models, including NLLB, check out this repository: <a href=\"https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/\" rel=\"nofollow noreferrer\">https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/</a></p>\n<p>This way the fine-tuning and inference process for the NLLB model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so:</p>\n<pre><code>tokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=&quot;eng_Latn&quot;, tgt_lang=&quot;deu_Latn&quot;)\n</code></pre>\n<p>and generate translations like this:</p>\n<pre><code>model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(&quot;deu_Latn&quot;)[1], max_length=512)\n</code></pre>\n",
         "0.0",
         "\"facebook/nllb-200-distilled-600M\"\n---\nDATA_CONFIG = \"/content/sample_data/data_config.json\"\nOUTPUT_DIR = \"/content/outputs\"\nMODEL_FOLDER = \"/content/drive/MyDrive/Thesis/nllb-checkpoints\"\nDROP = 0.1\nSRC = \"eng_Latn\"\nTGT = \"deu_Latn\"\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=\"$SRC-$TGT\" \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n---\n/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=\"conf\", config_name=\"base_config\")\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 289, in main\n    train_module = TrainModule(config)\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n---\nMissing data_prefix configuration\n---\n{\n    \"data_prefix\": \"/content/sample_data\",\n    \"train_data\": \"train_demo.json\",\n    \"test_data\": \"test_demo.json\",\n    \"lang_pairs\": \"eng_Latn-deu_Latn\"\n}",
         "transformers\n---\nSeq2SeqTrainer\n---\ntokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=\"eng_Latn\", tgt_lang=\"deu_Latn\")\n---\nmodel.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(\"deu_Latn\")[1], max_length=512)",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation",
         "Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB",
         "While I cant help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB While I cant help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation Im attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information Im encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB",
         "nllb finetuning error missing data_prefix configuration englishgerman translation im attempting finetune nllb model scientific translation task english eng_latn german deu_latn followed official guidelines finetuning authors nllb documentation link code block giving error error far understand created demo custom data_configjson looks like official documentation provides information im encountering difficulties applying specific use case someone share detailed guide point helpful resources finetuning nllb",
         "nllb finetune error miss data_prefix configuration englishgerman translation I m attempt finetune nllb model scientific translation task english eng_latn german deu_latn follow official guideline finetune author nllb documentation link code block give error error far understand create demo custom data_configjson look like official documentation provide information I m encounter difficulty apply specific use case someone share detailed guide point helpful resource finetune nllb",
         "nllb finetune error miss dataprefix configuration englishgerman translation I attempt finetune nllb scientific translation task english englatn german deulatn official guideline finetune author nllb documentation link block error error far understand create demo custom dataconfigjson like official documentation provide information I encounter difficulty apply specific case someone share detailed guide point helpful resource finetune nllb",
         "4",
         "custom dataconfigjson,miss dataprefix,finetune error,nllb documentation,configuration englishgerman"
        ],
        [
         "48",
         "78846004",
         "How can I use structured_output with Azure OpenAI with the openai Python library?",
         "<p>I want to use structured output with Azure OpenAI.</p>\n<p>I tried the following code, based on the code given in <a href=\"https://openai.com/index/introducing-structured-outputs-in-the-api/\" rel=\"nofollow noreferrer\">https://openai.com/index/introducing-structured-outputs-in-the-api/</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=&quot;gpt-4omini-2024-07-18-name&quot;,\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful math tutor.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;solve 8x + 31 = 2&quot;},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n</code></pre>\n<p>I get the error:</p>\n<pre><code>openai.BadRequestError: Error code: 400:\n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Invalid parameter: response_format must be one of json_object, text.&quot;,\n        &quot;type&quot;: &quot;invalid_request_error&quot;,\n        &quot;param&quot;: &quot;response_format&quot;,\n        &quot;code&quot;: &quot;None&quot;\n    }\n}\n</code></pre>\n<p>How to fix it?</p>\n<p>I ran <code>pip install -U openai</code>: I use <code>openai==1.40.1</code> and Python 3.11.</p>\n<hr />\n<p>I also tried <a href=\"https://cookbook.openai.com/examples/structured_outputs_intro\" rel=\"nofollow noreferrer\">https://cookbook.openai.com/examples/structured_outputs_intro</a> using  using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message:</p>\n<pre><code>from openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = &quot;https://&lt;your-resource-name&gt;.openai.azure.com&quot;\napi_key = &quot;&lt;your-api-key&gt;&quot;\ndeployment_name = &quot;&lt;your-deployment-name&gt;&quot; # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f&quot;{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01&quot;\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            &quot;role&quot;: &quot;system&quot;,\n            &quot;content&quot;: math_tutor_prompt\n        },\n        {\n            &quot;role&quot;: &quot;user&quot;,\n            &quot;content&quot;: question\n        }\n    ],\n    response_format={\n        &quot;type&quot;: &quot;json_schema&quot;,\n        &quot;json_schema&quot;: {\n            &quot;name&quot;: &quot;math_reasoning&quot;,\n            &quot;schema&quot;: {\n                &quot;type&quot;: &quot;object&quot;,\n                &quot;properties&quot;: {\n                    &quot;steps&quot;: {\n                        &quot;type&quot;: &quot;array&quot;,\n                        &quot;items&quot;: {\n                            &quot;type&quot;: &quot;object&quot;,\n                            &quot;properties&quot;: {\n                                &quot;explanation&quot;: {&quot;type&quot;: &quot;string&quot;},\n                                &quot;output&quot;: {&quot;type&quot;: &quot;string&quot;}\n                            },\n                            &quot;required&quot;: [&quot;explanation&quot;, &quot;output&quot;],\n                            &quot;additionalProperties&quot;: False\n                        }\n                    },\n                    &quot;final_answer&quot;: {&quot;type&quot;: &quot;string&quot;}\n                },\n                &quot;required&quot;: [&quot;steps&quot;, &quot;final_answer&quot;],\n                &quot;additionalProperties&quot;: False\n            },\n            &quot;strict&quot;: True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = &quot;how can I solve 8x + 7 = -23&quot;\n\nresult = get_math_solution(question)\n\nprint(result.content)\n</code></pre>\n",
         "2024-08-07 23:14:19",
         "0",
         "1201",
         "2",
         "78946352.0",
         "<p>Using <code>gpt-4o-2024-08-06</code>, which finally got deployed today (2024-09-03) on Azure, made it work. Code example from <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python-secure\" rel=\"nofollow noreferrer\">learn.microsoft.com</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = &quot;https://your-azure-openai-endpoint.com&quot;\napi_key = &quot;your-azure-openai-key&quot;\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Extract the event information.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Alice and Bob are going to a science fair on Friday.&quot;},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n</code></pre>\n<p>output:</p>\n<pre><code>name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  &quot;id&quot;: &quot;chatcmpl-A3XDRVolXpjeAAQIGddswI990weid&quot;,\n  &quot;choices&quot;: [\n    {\n      &quot;finish_reason&quot;: &quot;stop&quot;,\n      &quot;index&quot;: 0,\n      &quot;logprobs&quot;: null,\n      &quot;message&quot;: {\n        &quot;content&quot;: &quot;{\\&quot;name\\&quot;:\\&quot;Science Fair\\&quot;,\\&quot;date\\&quot;:\\&quot;Friday\\&quot;,\\&quot;participants\\&quot;:[\\&quot;Alice\\&quot;,\\&quot;Bob\\&quot;]}&quot;,\n        &quot;refusal&quot;: null,\n        &quot;role&quot;: &quot;assistant&quot;,\n        &quot;function_call&quot;: null,\n        &quot;tool_calls&quot;: [],\n        &quot;parsed&quot;: {\n          &quot;name&quot;: &quot;Science Fair&quot;,\n          &quot;date&quot;: &quot;Friday&quot;,\n          &quot;participants&quot;: [\n            &quot;Alice&quot;,\n            &quot;Bob&quot;\n          ]\n        }\n      },\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ],\n  &quot;created&quot;: 1725406029,\n  &quot;model&quot;: &quot;gpt-4o-2024-08-06&quot;,\n  &quot;object&quot;: &quot;chat.completion&quot;,\n  &quot;service_tier&quot;: null,\n  &quot;system_fingerprint&quot;: &quot;fp_b2ffeb31ff&quot;,\n  &quot;usage&quot;: {\n    &quot;completion_tokens&quot;: 17,\n    &quot;prompt_tokens&quot;: 32,\n    &quot;total_tokens&quot;: 49\n  },\n  &quot;prompt_filter_results&quot;: [\n    {\n      &quot;prompt_index&quot;: 0,\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ]\n}\n</code></pre>\n<p>Tested with Python 3.11.7 and openai==1.43.0.</p>\n",
         "0.0",
         "from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4omini-2024-07-18-name\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n        {\"role\": \"user\", \"content\": \"solve 8x + 31 = 2\"},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n---\nopenai.BadRequestError: Error code: 400:\n{\n    \"error\": {\n        \"message\": \"Invalid parameter: response_format must be one of json_object, text.\",\n        \"type\": \"invalid_request_error\",\n        \"param\": \"response_format\",\n        \"code\": \"None\"\n    }\n}\n---\npip install -U openai\n---\nopenai==1.40.1\n---\nfrom openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = \"https://<your-resource-name>.openai.azure.com\"\napi_key = \"<your-api-key>\"\ndeployment_name = \"<your-deployment-name>\" # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01\"\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": math_tutor_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": question\n        }\n    ],\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"math_reasoning\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"steps\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"explanation\": {\"type\": \"string\"},\n                                \"output\": {\"type\": \"string\"}\n                            },\n                            \"required\": [\"explanation\", \"output\"],\n                            \"additionalProperties\": False\n                        }\n                    },\n                    \"final_answer\": {\"type\": \"string\"}\n                },\n                \"required\": [\"steps\", \"final_answer\"],\n                \"additionalProperties\": False\n            },\n            \"strict\": True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = \"how can I solve 8x + 7 = -23\"\n\nresult = get_math_solution(question)\n\nprint(result.content)",
         "gpt-4o-2024-08-06\n---\nfrom pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = \"https://your-azure-openai-endpoint.com\"\napi_key = \"your-azure-openai-key\"\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n---\nname='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  \"id\": \"chatcmpl-A3XDRVolXpjeAAQIGddswI990weid\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"{\\\"name\\\":\\\"Science Fair\\\",\\\"date\\\":\\\"Friday\\\",\\\"participants\\\":[\\\"Alice\\\",\\\"Bob\\\"]}\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": [],\n        \"parsed\": {\n          \"name\": \"Science Fair\",\n          \"date\": \"Friday\",\n          \"participants\": [\n            \"Alice\",\n            \"Bob\"\n          ]\n        }\n      },\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"created\": 1725406029,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": \"fp_b2ffeb31ff\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 32,\n    \"total_tokens\": 49\n  },\n  \"prompt_filter_results\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ]\n}",
         "How can I use structured_output with Azure OpenAI with the openai Python library",
         "I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message",
         "Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didnt work either same error message",
         "use structured_output azure openai openai python library want use structured output azure openai tried following code based code given get error fix ran use python 311 also tried using using azure+ gpt4o mini 20240718 didnt work either error message",
         "use structured_output azure openai openai python library want use structured output azure openai try follow code base code given get error fix run use python 311 also try use use azure+ gpt4o mini 20240718 do not work either error message",
         "structuredoutput azure openai openai python library structured azure openai base given get error fix run python 311 also azure gpt4o mini 20240718 do not either error message",
         "4",
         "311 azure,structuredoutput,openai base,azure gpt4o,openai python"
        ],
        [
         "49",
         "78836208",
         "Removing bi-grams after tokenization for TfidfVectorizer",
         "<p>I'm attempting to remove bi-grams that are created by <code>TfidfVectorizer</code>.  I'm using <code>text.TfidfVectorizer</code> so that I can use my own preprocessor function.</p>\n<p>Test strings and preprocessor function:</p>\n<pre><code>doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, &quot;&quot;)\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n</code></pre>\n<p>My <code>TfidfVectorizer</code> instantiation and <code>fit_transform</code>:</p>\n<pre><code>from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n</code></pre>\n<p>Here is my error:</p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in &lt;cell line: 5&gt;()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----&gt; 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-&gt; 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 &quot;Upper case characters found in&quot;\n   1332                 &quot; vocabulary while 'lowercase'&quot;\n   1333                 &quot; is True. These entries will not&quot;\n   1334                 &quot; be matched with any documents&quot;\n   1335             )\n   1336             break\n-&gt; 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-&gt; 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--&gt; 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object\n</code></pre>\n<p>How to resolve it?</p>\n",
         "2024-08-05 19:46:40",
         "1",
         "38",
         "1",
         "78837616.0",
         "<p>The preprocessor should handle documents, not the whole corpus. (The clues are the &quot;expected string&quot; in the error, and the fact that <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"nofollow noreferrer\">the <code>TfidfVectorizer</code> docs</a> refer to &quot;the preprocessing (string transformation) stage&quot;. The docs could definitely be clearer.)</p>\n<p>This should fix it:</p>\n<pre><code>def remove_bigrams(doc: str) -&gt; str:\n    &quot;&quot;&quot;Remove certain bi-grams from a document.&quot;&quot;&quot;\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, &quot;&quot;)\n    return doc\n</code></pre>\n",
         "2.0",
         "TfidfVectorizer\n---\ntext.TfidfVectorizer\n---\ndoc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, \"\")\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n---\nTfidfVectorizer\n---\nfit_transform\n---\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n---\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in <cell line: 5>()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----> 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-> 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 \"Upper case characters found in\"\n   1332                 \" vocabulary while 'lowercase'\"\n   1333                 \" is True. These entries will not\"\n   1334                 \" be matched with any documents\"\n   1335             )\n   1336             break\n-> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-> 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--> 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object",
         "TfidfVectorizer\n---\ndef remove_bigrams(doc: str) -> str:\n    \"\"\"Remove certain bi-grams from a document.\"\"\"\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, \"\")\n    return doc",
         "Removing bigrams after tokenization for TfidfVectorizer",
         "Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it",
         "The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "Removing bigrams after tokenization for TfidfVectorizer Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "Removing bigrams after tokenization for TfidfVectorizer Im attempting to remove bigrams that are created by Im using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it",
         "removing bigrams tokenization tfidfvectorizer im attempting remove bigrams created im using use preprocessor function test strings preprocessor function instantiation error resolve",
         "remove bigrams tokenization tfidfvectorizer I m attempt remove bigram create I m use use preprocessor function test string preprocessor function instantiation error resolve",
         "remove bigrams tokenization tfidfvectorizer I attempt remove bigram create I preprocessor function test preprocessor function instantiation error resolve",
         "4",
         "instantiation error,bigrams,tfidfvectorizer,tokenization tfidfvectorizer,remove bigram"
        ]
       ],
       "shape": {
        "columns": 22,
        "rows": 8510
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AcceptedAnswerBody</th>\n",
       "      <th>AcceptedAnswerScore</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_Clean</th>\n",
       "      <th>Body_Clean</th>\n",
       "      <th>AcceptedAnswerBody_Clean</th>\n",
       "      <th>combination_text</th>\n",
       "      <th>combination_text_only_question</th>\n",
       "      <th>combination_text_only_question_no_stopw</th>\n",
       "      <th>combination_text_only_question_lemma</th>\n",
       "      <th>combination_text_only_question_lemma_no_noise</th>\n",
       "      <th>cluster_kmean</th>\n",
       "      <th>keywords_fromBert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>2025-04-02 05:56:11</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>79552218.0</td>\n",
       "      <td>&lt;p&gt;The configuration file is missing the 'labe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>Im using spaCy with the pl_core_news_lg model ...</td>\n",
       "      <td>The configuration file is missing the labels_t...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>9</td>\n",
       "      <td>organization pesel,recognizer presidio,spacy p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79548202</td>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>&lt;p&gt;I understand the -100 label id is used so t...</td>\n",
       "      <td>2025-04-01 09:21:17</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>79551169.0</td>\n",
       "      <td>&lt;p&gt;The author of the tutorial you mentioned se...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>I understand the 100 label id is used so that ...</td>\n",
       "      <td>The author of the tutorial you mentioned sets ...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>gpt2 models huggingface 100 label index traini...</td>\n",
       "      <td>gpt2 model huggingface 100 label index trainin...</td>\n",
       "      <td>gpt2 huggingface 100 label index training inst...</td>\n",
       "      <td>2</td>\n",
       "      <td>ignoreindex loss,label prediction,pad token,de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79523269</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>&lt;p&gt;I am trying to import gensim into colab.&lt;/p...</td>\n",
       "      <td>2025-03-20 14:36:02</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>79523777.0</td>\n",
       "      <td>&lt;p&gt;You have to restart the session for the und...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>I am trying to import gensim into colab I get ...</td>\n",
       "      <td>You have to restart the session for the underl...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "      <td>trouble getting importing gensim work colab tr...</td>\n",
       "      <td>trouble getting import gensim work colab try i...</td>\n",
       "      <td>trouble getting import gensim colab import gen...</td>\n",
       "      <td>4</td>\n",
       "      <td>getting import,numpy relate,gensim colab,colab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this &lt;a href...</td>\n",
       "      <td>2025-03-11 14:50:31</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>79501337.0</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>I am running the code found on this site in my...</td>\n",
       "      <td>I cant test it but I checked source code and i...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>store images instead showing server running co...</td>\n",
       "      <td>store image instead show server run code find ...</td>\n",
       "      <td>store image instead show server run site serve...</td>\n",
       "      <td>4</td>\n",
       "      <td>connection instance,image instead,store locall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79482283</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>&lt;p&gt;I am using presidio/langchain_experimental ...</td>\n",
       "      <td>2025-03-03 22:27:07</td>\n",
       "      <td>4</td>\n",
       "      <td>230</td>\n",
       "      <td>2</td>\n",
       "      <td>79495969.0</td>\n",
       "      <td>&lt;p&gt;After some test I was able to find the solu...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>I am using presidio/langchain_experimental to ...</td>\n",
       "      <td>After some test I was able to find the solutio...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>9</td>\n",
       "      <td>presidio langchain,anonymize,presidio spacy,po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>62328</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>&lt;p&gt;input: phrase 1, phrase 2&lt;/p&gt;\\n\\n&lt;p&gt;output:...</td>\n",
       "      <td>2008-09-15 12:26:42</td>\n",
       "      <td>65</td>\n",
       "      <td>49889</td>\n",
       "      <td>11</td>\n",
       "      <td>63076.0</td>\n",
       "      <td>&lt;hr&gt;\\n\\n&lt;p&gt;You might want to check out this pa...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>input phrase 1 phrase 2 output semantic simila...</td>\n",
       "      <td>You might want to check out this paper Sentenc...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>algorithm tells semantic similarity two phrase...</td>\n",
       "      <td>algorithm tell semantic similarity two phrase ...</td>\n",
       "      <td>algorithm tell semantic similarity phrase inpu...</td>\n",
       "      <td>0</td>\n",
       "      <td>tell semantic,input phrase,similarity value,si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8506</th>\n",
       "      <td>42489</td>\n",
       "      <td>How to implement a \"related\" degree measure al...</td>\n",
       "      <td>&lt;p&gt;I was going to Ask a Question earlier today...</td>\n",
       "      <td>2008-09-03 20:21:04</td>\n",
       "      <td>8</td>\n",
       "      <td>456</td>\n",
       "      <td>2</td>\n",
       "      <td>42532.0</td>\n",
       "      <td>&lt;p&gt;One such way to implement such an algorithm...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>I was going to Ask a Question earlier today wh...</td>\n",
       "      <td>One such way to implement such an algorithm wo...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>implement related degree measure algorithm goi...</td>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "      <td>1</td>\n",
       "      <td>functionality,relate degree,implement stackove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03 10:36:13</td>\n",
       "      <td>118</td>\n",
       "      <td>33200</td>\n",
       "      <td>11</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>&lt;p&gt;Actually what Google does is very much non-...</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>How do you implement a Did you mean</td>\n",
       "      <td>Possible Duplicate How does the Google Did you...</td>\n",
       "      <td>Actually what Google does is much nontrivial a...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>8</td>\n",
       "      <td>mean possible,website implement,search website...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8508</th>\n",
       "      <td>36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31 01:08:48</td>\n",
       "      <td>3</td>\n",
       "      <td>5661</td>\n",
       "      <td>6</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>&lt;p&gt;Citation from Vista &lt;a href=\"http://blogs.m...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>my primary language is spanish but I use all m...</td>\n",
       "      <td>Citation from Vista speech recognition blog In...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>vista speech recognition multiple languages pr...</td>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "      <td>1</td>\n",
       "      <td>software english,multiple language,speech reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22 22:45:10</td>\n",
       "      <td>27</td>\n",
       "      <td>6484</td>\n",
       "      <td>9</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>&lt;p&gt;We developed exactly what you are looking f...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Natural language date/time parser for NET</td>\n",
       "      <td>Does anyone know of a NET date/time parser sim...</td>\n",
       "      <td>We developed exactly what you are looking for ...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "      <td>natural language date/time parser net anyone k...</td>\n",
       "      <td>natural language date / time parser net anyone...</td>\n",
       "      <td>natural language date time parser net anyone n...</td>\n",
       "      <td>1</td>\n",
       "      <td>language date,parser similar,date time,write r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8510 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      QuestionId                                              Title  \\\n",
       "0       79549787  Why does Presidio with spacy nlp engine not re...   \n",
       "1       79548202  GPT-2 and other models from huggingface -100 l...   \n",
       "2       79523269  Trouble getting importing gensim to work in colab   \n",
       "3       79501178        Store images instead of showing in a server   \n",
       "4       79482283  Presidio with Langchain Experimental does not ...   \n",
       "...          ...                                                ...   \n",
       "8505       62328  Is there an algorithm that tells the semantic ...   \n",
       "8506       42489  How to implement a \"related\" degree measure al...   \n",
       "8507       41424             How do you implement a \"Did you mean\"?   \n",
       "8508       36533     Vista speech recognition in multiple languages   \n",
       "8509       23689        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                   Body         CreationDate  \\\n",
       "0     <p>I'm using spaCy with the pl_core_news_lg mo...  2025-04-02 05:56:11   \n",
       "1     <p>I understand the -100 label id is used so t...  2025-04-01 09:21:17   \n",
       "2     <p>I am trying to import gensim into colab.</p...  2025-03-20 14:36:02   \n",
       "3     <p>I am running the code found on this <a href...  2025-03-11 14:50:31   \n",
       "4     <p>I am using presidio/langchain_experimental ...  2025-03-03 22:27:07   \n",
       "...                                                 ...                  ...   \n",
       "8505  <p>input: phrase 1, phrase 2</p>\\n\\n<p>output:...  2008-09-15 12:26:42   \n",
       "8506  <p>I was going to Ask a Question earlier today...  2008-09-03 20:21:04   \n",
       "8507  <blockquote>\\n  <p><strong>Possible Duplicate:...  2008-09-03 10:36:13   \n",
       "8508  <p>my primary language is spanish, but I use a...  2008-08-31 01:08:48   \n",
       "8509  <p>Does anyone know of a .NET date/time parser...  2008-08-22 22:45:10   \n",
       "\n",
       "      Score  ViewCount  AnswerCount  AcceptedAnswerId  \\\n",
       "0         0         68            1        79552218.0   \n",
       "1         0         46            1        79551169.0   \n",
       "2         0        125            1        79523777.0   \n",
       "3         0         36            1        79501337.0   \n",
       "4         4        230            2        79495969.0   \n",
       "...     ...        ...          ...               ...   \n",
       "8505     65      49889           11           63076.0   \n",
       "8506      8        456            2           42532.0   \n",
       "8507    118      33200           11           41448.0   \n",
       "8508      3       5661            6           36684.0   \n",
       "8509     27       6484            9          631134.0   \n",
       "\n",
       "                                     AcceptedAnswerBody  AcceptedAnswerScore  \\\n",
       "0     <p>The configuration file is missing the 'labe...                  1.0   \n",
       "1     <p>The author of the tutorial you mentioned se...                  1.0   \n",
       "2     <p>You have to restart the session for the und...                  1.0   \n",
       "3     <p>I can't test it but ...</p>\\n<p>I checked <...                  1.0   \n",
       "4     <p>After some test I was able to find the solu...                 -2.0   \n",
       "...                                                 ...                  ...   \n",
       "8505  <hr>\\n\\n<p>You might want to check out this pa...                 44.0   \n",
       "8506  <p>One such way to implement such an algorithm...                  5.0   \n",
       "8507  <p>Actually what Google does is very much non-...                 87.0   \n",
       "8508  <p>Citation from Vista <a href=\"http://blogs.m...                  8.0   \n",
       "8509  <p>We developed exactly what you are looking f...                 12.0   \n",
       "\n",
       "      ...                                        Title_Clean  \\\n",
       "0     ...  Why does Presidio with spacy nlp engine not re...   \n",
       "1     ...  GPT2 and other models from huggingface 100 lab...   \n",
       "2     ...  Trouble getting importing gensim to work in colab   \n",
       "3     ...        Store images instead of showing in a server   \n",
       "4     ...  Presidio with Langchain Experimental does not ...   \n",
       "...   ...                                                ...   \n",
       "8505  ...  Is there an algorithm that tells the semantic ...   \n",
       "8506  ...  How to implement a related degree measure algo...   \n",
       "8507  ...                How do you implement a Did you mean   \n",
       "8508  ...     Vista speech recognition in multiple languages   \n",
       "8509  ...          Natural language date/time parser for NET   \n",
       "\n",
       "                                             Body_Clean  \\\n",
       "0     Im using spaCy with the pl_core_news_lg model ...   \n",
       "1     I understand the 100 label id is used so that ...   \n",
       "2     I am trying to import gensim into colab I get ...   \n",
       "3     I am running the code found on this site in my...   \n",
       "4     I am using presidio/langchain_experimental to ...   \n",
       "...                                                 ...   \n",
       "8505  input phrase 1 phrase 2 output semantic simila...   \n",
       "8506  I was going to Ask a Question earlier today wh...   \n",
       "8507  Possible Duplicate How does the Google Did you...   \n",
       "8508  my primary language is spanish but I use all m...   \n",
       "8509  Does anyone know of a NET date/time parser sim...   \n",
       "\n",
       "                               AcceptedAnswerBody_Clean  \\\n",
       "0     The configuration file is missing the labels_t...   \n",
       "1     The author of the tutorial you mentioned sets ...   \n",
       "2     You have to restart the session for the underl...   \n",
       "3     I cant test it but I checked source code and i...   \n",
       "4     After some test I was able to find the solutio...   \n",
       "...                                                 ...   \n",
       "8505  You might want to check out this paper Sentenc...   \n",
       "8506  One such way to implement such an algorithm wo...   \n",
       "8507  Actually what Google does is much nontrivial a...   \n",
       "8508  Citation from Vista speech recognition blog In...   \n",
       "8509  We developed exactly what you are looking for ...   \n",
       "\n",
       "                                       combination_text  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in co...   \n",
       "3     Store images instead of showing in a server I ...   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507  How do you implement a Did you mean Possible D...   \n",
       "8508  Vista speech recognition in multiple languages...   \n",
       "8509  Natural language date/time parser for NET Does...   \n",
       "\n",
       "                         combination_text_only_question  \\\n",
       "0     Why does Presidio with spacy nlp engine not re...   \n",
       "1     GPT2 and other models from huggingface 100 lab...   \n",
       "2     Trouble getting importing gensim to work in co...   \n",
       "3     Store images instead of showing in a server I ...   \n",
       "4     Presidio with Langchain Experimental does not ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507  How do you implement a Did you mean Possible D...   \n",
       "8508  Vista speech recognition in multiple languages...   \n",
       "8509  Natural language date/time parser for NET Does...   \n",
       "\n",
       "                combination_text_only_question_no_stopw  \\\n",
       "0     presidio spacy nlp engine recognize organizati...   \n",
       "1     gpt2 models huggingface 100 label index traini...   \n",
       "2     trouble getting importing gensim work colab tr...   \n",
       "3     store images instead showing server running co...   \n",
       "4     presidio langchain experimental detect polish ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tells semantic similarity two phrase...   \n",
       "8506  implement related degree measure algorithm goi...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple languages pr...   \n",
       "8509  natural language date/time parser net anyone k...   \n",
       "\n",
       "                   combination_text_only_question_lemma  \\\n",
       "0     presidio spacy nlp engine recognize organizati...   \n",
       "1     gpt2 model huggingface 100 label index trainin...   \n",
       "2     trouble getting import gensim work colab try i...   \n",
       "3     store image instead show server run code find ...   \n",
       "4     presidio langchain experimental detect polish ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tell semantic similarity two phrase ...   \n",
       "8506  implement relate degree measure algorithm go a...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple language pri...   \n",
       "8509  natural language date / time parser net anyone...   \n",
       "\n",
       "          combination_text_only_question_lemma_no_noise cluster_kmean  \\\n",
       "0     presidio spacy nlp engine recognize organizati...             9   \n",
       "1     gpt2 huggingface 100 label index training inst...             2   \n",
       "2     trouble getting import gensim colab import gen...             4   \n",
       "3     store image instead show server run site serve...             4   \n",
       "4     presidio langchain experimental detect polish ...             9   \n",
       "...                                                 ...           ...   \n",
       "8505  algorithm tell semantic similarity phrase inpu...             0   \n",
       "8506  implement relate degree measure algorithm go a...             1   \n",
       "8507  implement mean possible duplicate google mean ...             8   \n",
       "8508  vista speech recognition multiple language pri...             1   \n",
       "8509  natural language date time parser net anyone n...             1   \n",
       "\n",
       "                                      keywords_fromBert  \n",
       "0     organization pesel,recognizer presidio,spacy p...  \n",
       "1     ignoreindex loss,label prediction,pad token,de...  \n",
       "2     getting import,numpy relate,gensim colab,colab...  \n",
       "3     connection instance,image instead,store locall...  \n",
       "4     presidio langchain,anonymize,presidio spacy,po...  \n",
       "...                                                 ...  \n",
       "8505  tell semantic,input phrase,similarity value,si...  \n",
       "8506  functionality,relate degree,implement stackove...  \n",
       "8507  mean possible,website implement,search website...  \n",
       "8508  software english,multiple language,speech reco...  \n",
       "8509  language date,parser similar,date time,write r...  \n",
       "\n",
       "[8510 rows x 22 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_post_questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m frequency_questions \u001b[38;5;241m=\u001b[39m category_frequency(\u001b[43mdf_post_questions\u001b[49m,keyword_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords_fromBert\u001b[39m\u001b[38;5;124m\"\u001b[39m,cluster_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_kmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_post_questions' is not defined"
     ]
    }
   ],
   "source": [
    "frequency_questions = category_frequency(df_post_questions,keyword_col=\"keywords_fromBert\",cluster_col=\"cluster_kmean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "dee4551c-3e42-417d-8019-34c423ce33ec",
       "rows": [
        [
         "0",
         "0.012658227848101266",
         "frozenset({'panda', 'panda dataframe'})",
         "6"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012658</td>\n",
       "      <td>(panda, panda dataframe)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score                   pattern  cluster\n",
       "0  0.012658  (panda, panda dataframe)        6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only using Title to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop word\n",
    "df_post_answer['Title_Clean'] = df_post_answer['Title_Clean'].apply(remove_stopwords)\n",
    "#Lemma Text\n",
    "title_text_for_lemma = df_post_answer['Title_Clean'].tolist()\n",
    "title_lemma = lemma_texts_parallel(title_text_for_lemma)\n",
    "df_post_answer['Title_Lemma'] = title_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_embedding(df_post_answer,'Title_Clean',model_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['keywords_fromBert'] = df_post_answer['Title_Clean'].apply(extract_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_title = category_frequency(df_post_answer,keyword_col=\"keywords_fromBert\",cluster_col=\"cluster_kmean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "578384bc-46f3-4ea2-92ad-47d5d5386dd4",
       "rows": [
        [
         "0",
         "0.005063291139240506",
         "frozenset({'strings', 'based'})",
         "0"
        ],
        [
         "1",
         "0.005063291139240506",
         "frozenset({'remove', 'characters'})",
         "0"
        ],
        [
         "2",
         "0.005063291139240506",
         "frozenset({'words', 'characters'})",
         "0"
        ],
        [
         "3",
         "0.005063291139240506",
         "frozenset({'strings', 'convert'})",
         "0"
        ],
        [
         "4",
         "0.010126582278481013",
         "frozenset({'regular', 'expression'})",
         "0"
        ],
        [
         "5",
         "0.006329113924050633",
         "frozenset({'extract', 'string'})",
         "0"
        ],
        [
         "6",
         "0.012658227848101266",
         "frozenset({'extract', 'text'})",
         "0"
        ],
        [
         "7",
         "0.008860759493670886",
         "frozenset({'strings', 'list'})",
         "0"
        ],
        [
         "8",
         "0.005063291139240506",
         "frozenset({'pattern', 'text'})",
         "0"
        ],
        [
         "9",
         "0.006329113924050633",
         "frozenset({'remove', 'text'})",
         "0"
        ],
        [
         "10",
         "0.005063291139240506",
         "frozenset({'strings', 'tokenize'})",
         "0"
        ],
        [
         "11",
         "0.005876591576885406",
         "frozenset({'algorithm', 'words'})",
         "1"
        ],
        [
         "12",
         "0.0068560235063663075",
         "frozenset({'common', 'words'})",
         "1"
        ],
        [
         "13",
         "0.00881488736532811",
         "frozenset({'words', 'corpus'})",
         "1"
        ],
        [
         "14",
         "0.005876591576885406",
         "frozenset({'count', 'word'})",
         "1"
        ],
        [
         "15",
         "0.012732615083251714",
         "frozenset({'words', 'english'})",
         "1"
        ],
        [
         "16",
         "0.005876591576885406",
         "frozenset({'finding', 'word'})",
         "1"
        ],
        [
         "17",
         "0.005876591576885406",
         "frozenset({'finding', 'words'})",
         "1"
        ],
        [
         "18",
         "0.010773751224289911",
         "frozenset({'frequency', 'word'})",
         "1"
        ],
        [
         "19",
         "0.00881488736532811",
         "frozenset({'words', 'frequency'})",
         "1"
        ],
        [
         "20",
         "0.005876591576885406",
         "frozenset({'frequent', 'words'})",
         "1"
        ],
        [
         "21",
         "0.019588638589618023",
         "frozenset({'words', 'list'})",
         "1"
        ],
        [
         "22",
         "0.005876591576885406",
         "frozenset({'nlp', 'python'})",
         "1"
        ],
        [
         "23",
         "0.0068560235063663075",
         "frozenset({'nlp', 'word'})",
         "1"
        ],
        [
         "24",
         "0.005876591576885406",
         "frozenset({'pos', 'words'})",
         "1"
        ],
        [
         "25",
         "0.019588638589618023",
         "frozenset({'python', 'word'})",
         "1"
        ],
        [
         "26",
         "0.021547502448579822",
         "frozenset({'python', 'words'})",
         "1"
        ],
        [
         "27",
         "0.005876591576885406",
         "frozenset({'remove', 'words'})",
         "1"
        ],
        [
         "28",
         "0.007835455435847209",
         "frozenset({'sentence', 'word'})",
         "1"
        ],
        [
         "29",
         "0.009794319294809012",
         "frozenset({'sentence', 'words'})",
         "1"
        ],
        [
         "30",
         "0.007835455435847209",
         "frozenset({'similar', 'words'})",
         "1"
        ],
        [
         "31",
         "0.005876591576885406",
         "frozenset({'similarity', 'words'})",
         "1"
        ],
        [
         "32",
         "0.0068560235063663075",
         "frozenset({'words', 'specific'})",
         "1"
        ],
        [
         "33",
         "0.005876591576885406",
         "frozenset({'words', 'stemming'})",
         "1"
        ],
        [
         "34",
         "0.009794319294809012",
         "frozenset({'stop', 'words'})",
         "1"
        ],
        [
         "35",
         "0.012732615083251714",
         "frozenset({'using', 'words'})",
         "1"
        ],
        [
         "36",
         "0.0068560235063663075",
         "frozenset({'words', 'using python'})",
         "1"
        ],
        [
         "37",
         "0.005376344086021506",
         "frozenset({'allocation', 'dirichlet'})",
         "2"
        ],
        [
         "38",
         "0.008602150537634409",
         "frozenset({'semantic', 'analysis'})",
         "2"
        ],
        [
         "39",
         "0.01935483870967742",
         "frozenset({'sentiment', 'analysis'})",
         "2"
        ],
        [
         "40",
         "0.01935483870967742",
         "frozenset({'sentiment analysis', 'analysis'})",
         "2"
        ],
        [
         "41",
         "0.016129032258064516",
         "frozenset({'classification', 'text'})",
         "2"
        ],
        [
         "42",
         "0.00967741935483871",
         "frozenset({'classification', 'text classification'})",
         "2"
        ],
        [
         "43",
         "0.005376344086021506",
         "frozenset({'classify', 'text'})",
         "2"
        ],
        [
         "44",
         "0.005376344086021506",
         "frozenset({'entity recognition', 'entity'})",
         "2"
        ],
        [
         "45",
         "0.011827956989247311",
         "frozenset({'named', 'entity'})",
         "2"
        ],
        [
         "46",
         "0.011827956989247311",
         "frozenset({'recognition', 'entity'})",
         "2"
        ],
        [
         "47",
         "0.0064516129032258064",
         "frozenset({'named', 'entity recognition'})",
         "2"
        ],
        [
         "48",
         "0.0064516129032258064",
         "frozenset({'recognition', 'entity recognition'})",
         "2"
        ],
        [
         "49",
         "0.005376344086021506",
         "frozenset({'extraction', 'information extraction'})",
         "2"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 415
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005063</td>\n",
       "      <td>(strings, based)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005063</td>\n",
       "      <td>(remove, characters)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005063</td>\n",
       "      <td>(words, characters)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005063</td>\n",
       "      <td>(strings, convert)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010127</td>\n",
       "      <td>(regular, expression)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.005172</td>\n",
       "      <td>(language, natural language, processing)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.006034</td>\n",
       "      <td>(language processing, natural language, natural)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.010345</td>\n",
       "      <td>(language processing, natural, processing)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.010345</td>\n",
       "      <td>(natural, natural language, processing)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.005172</td>\n",
       "      <td>(language, natural, natural language, processing)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                                            pattern  cluster\n",
       "0    0.005063                                   (strings, based)        0\n",
       "1    0.005063                               (remove, characters)        0\n",
       "2    0.005063                                (words, characters)        0\n",
       "3    0.005063                                 (strings, convert)        0\n",
       "4    0.010127                              (regular, expression)        0\n",
       "..        ...                                                ...      ...\n",
       "410  0.005172           (language, natural language, processing)        9\n",
       "411  0.006034   (language processing, natural language, natural)        9\n",
       "412  0.010345         (language processing, natural, processing)        9\n",
       "413  0.010345            (natural, natural language, processing)        9\n",
       "414  0.005172  (language, natural, natural language, processing)        9\n",
       "\n",
       "[415 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
