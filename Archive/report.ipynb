{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Lib\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import swifter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\n",
    "\n",
    "\n",
    "color_pal = sns.color_palette()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will load this when we have json structure. For now to start coding we will use from csv\n",
    "# df = pd.read_json(\"nlp_questions.json\") \n",
    "\n",
    "df = pd.read_csv(\"QueryResults.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AllTags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "06405732-7f51-4b2e-bbbe-135e8025d17a",
       "rows": [
        [
         "0",
         "79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this [site][1] in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?\n[1]: <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Llama2_LLM_Attribution</a></p>\n",
         "2025-03-11 14:50:31",
         "0",
         "23",
         "1",
         "<python><nlp><large-language-model>",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n"
        ],
        [
         "1",
         "79498915",
         "Comparing the similarity of spoken and written form text",
         "<p>I'm converting spoken form text to its written form. For example, &quot;he owes me two-thousand dollars&quot; should be converted to &quot;he owes me $2,000&quot; . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of &quot;two-thousand dollars&quot; to &quot;$2,000&quot; to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this?</p>\n",
         "2025-03-10 18:55:59",
         "0",
         "20",
         "1",
         "<nlp><large-language-model>",
         null,
         null
        ],
        [
         "2",
         "79488426",
         "Upserting in Pinecone takes too long",
         "<p>I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using <code>jina-embedding-v3</code>. For 204 reviews this takes around <strong>2.5 hours!</strong> in Colab. Tried using GPU but the embeddings arent using GPU.\nAm i doing something wrong? Is there a way that i can speed up the process? The code is below:</p>\n<p>Initialising DB:</p>\n<pre><code>if index_name not in pc.list_indexes().names():\n  pc.create_index(\n    name=index_name,\n    dimension=1024,\n    metric=&quot;cosine&quot;,\n    spec=ServerlessSpec(\n        cloud=&quot;aws&quot;,\n        region=&quot;us-east-1&quot;\n    )\n)\n</code></pre>\n<p>Embedding &amp; Upserting:</p>\n<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the Jina embedding model and tokenizer from Hugging Face\nmodel_name = &quot;jinaai/jina-embeddings-v3&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\nfrom langchain.text_splitter import SpacyTextSplitter\ntext_splitter = SpacyTextSplitter(chunk_size=500)\n\n# Function to generate embeddings\ndef generate_embeddings(text, task='retrieval.passage'):\n    return model.encode(text, convert_to_tensor=True, task=task).numpy()\n\nfor review_id, review in enumerate(all_reviews[:2]):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n\n# Generate and store embeddings\nfor review_id, review in enumerate(all_reviews):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n</code></pre>\n",
         "2025-03-06 06:22:35",
         "1",
         "37",
         "1",
         "<python><nlp><rag><pinecone>",
         null,
         null
        ],
        [
         "3",
         "79484448",
         "How does ELMo generate words for training ? Is it autoregressive?",
         "<p>I'm confused about using Bidirectional LM for words prediction and loss computing  while training.</p>\n<p>At first we have a sequence of tokens X1, ..., Xn.</p>\n<p>After gathering their context independent embeddings via CNN... we pass them to 2-layer Stacked BiLSTM.</p>\n<p>BiLSTM works as a tagger here, and for every input token X1, ...,Xn we get probabilities of the next token (for X1 it <strong>should</strong> be X2, for X2 - X3 and etc.). For Xn it will be a probability of the next word in the sentence - Y_n.</p>\n<p>Now, we can compute loss for every token.</p>\n<p>So, I don't understand what we do next. Does ELMo works like an autoregressive LM?</p>\n<p>If it does and we put the newly predicted token Y_n right after X1, ...,Xn and feed it to the BiLSTM to predict the second new word - Y_{n+1}, won't predictions for all previous tokens change ?</p>\n<p>I guess they should because of the Bidirectional nature of the model. There will be new context from the right, and the model can change everything.</p>\n<p>Can we simply compute loss for every token again, not just for a new one ?</p>\n<p>But, if we want to use this model like LM in inference, and predict next <strong>several</strong> words, newly predicted words will affect previous. What predictions should we use ?</p>\n<p>We pass X1, ..., Xn to the ElMo. We get new word Y1. We pass X1, ..., Xn, Y1 to the ELMo. We get another new word Y2 and the previous word Y1 changes to Z1. Should we take Z1, Y2 as an answer, or we freeze Y1, ignore Z1 and use Y1, Y2 as an answer?</p>\n<p>In Transformer this problem is solved by Masked Self-Attention in decoder, and newly predicted words won't affect previous.</p>\n<p>I tried looking for the answer in the original paper, but didn't find anything about the training process.</p>\n",
         "2025-03-04 17:32:14",
         "0",
         "28",
         "1",
         "<nlp><language-model><autoregressive-models><elmo>",
         null,
         null
        ],
        [
         "4",
         "79482290",
         "How to handle German language specific characters like (ä, ö, ü, ß) while tokenizing using GPT2Tokenizer?",
         "<p>I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer.</p>\n<p>To tokenize the text, I wrote the implementation as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import GPT2Tokenizer\n\ntext = &quot;zügiger Transport des ABCD stabilen Kindes in die Notaufnahme UKA&quot;\ntext = text.encode(&quot;utf-8&quot;).decode(&quot;utf-8&quot;)  # Re-encode to fix encoding issues\n\n# Load GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\n\nprint(tokens)  # Should properly tokenize &quot;zügiger&quot; instead of splitting &quot;ü&quot;\n</code></pre>\n<p>Now, when I execute this code snippet I get output as follows:</p>\n<pre><code>['z', 'Ã¼', 'g', 'iger', 'ĠTransport', 'Ġdes', 'ĠABC', 'D', 'Ġstabil', 'en', 'ĠKind', 'es', 'Ġin', 'Ġdie', 'ĠNot', 'au', 'fn', 'ah', 'me', 'ĠUK', 'A']\n</code></pre>\n<p>After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below.</p>\n<pre class=\"lang-markdown prettyprint-override\"><code>| Character | UTF-8 Bytes | Misdecoded as Latin-1 | Resulting String |\n|-----------|-------------|-----------------------|------------------|\n| ä         | C3 A4       | Ã + ¤                 | Ã¤               |\n| ö         | C3 B6       | Ã + ¶                 | Ã¶               |\n| ü         | C3 BC       | Ã + ¼                 | Ã¼               |\n| ß         | C3 9F       | Ã + Ÿ                 | ÃŸ               |\n</code></pre>\n<p>Now, how I can keep German language specific characters like (ä, ö, ü, ß) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. &quot;zügiger&quot; becomes something like ['z', 'ü', 'g', 'iger'].</p>\n",
         "2025-03-03 22:32:36",
         "1",
         "59",
         "1",
         "<python><nlp><tokenize><large-language-model><gpt-2>",
         null,
         null
        ],
        [
         "5",
         "79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03 22:27:07",
         "4",
         "182",
         "2",
         "<python><nlp><spacy><langchain><presidio>",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n"
        ],
        [
         "6",
         "79465047",
         "Where is the HuggingFace model saved in when loading a model on colab?",
         "<p>I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.).</p>\n<pre><code>model_id = &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;\n\n\npipeline = transformers.pipeline(\n            &quot;text-generation&quot;,\n            model=model_id,\n            #model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16, &quot;cache_dir&quot;: cache_dir},\n            device_map=&quot;auto&quot;)\n</code></pre>\n",
         "2025-02-24 23:25:42",
         "1",
         "46",
         "1",
         "<nlp><huggingface-transformers><huggingface>",
         null,
         null
        ],
        [
         "7",
         "79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22 16:06:11",
         "1",
         "31",
         "1",
         "<nlp><opennlp>",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n"
        ],
        [
         "8",
         "79451974",
         "word/ sentence similarities",
         "<p>I am trying to find if a given word/ set of words are similar to a definition.</p>\n<p>Example - Definition - &quot;vegetarian User&quot;</p>\n<p>Now, if I want to check a set of sentences like below</p>\n<pre><code>sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n</code></pre>\n<p>I tried using some sentence transformer like below</p>\n<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n</code></pre>\n<p>But this is not giving me expected results.</p>\n<p>What is the best approach to achieve results like below?</p>\n<pre><code>[False,True,True,False]\n</code></pre>\n<p>Is it doable with nlp/ some other technique?</p>\n",
         "2025-02-19 15:47:45",
         "1",
         "43",
         "1",
         "<python><python-3.x><nlp>",
         null,
         null
        ],
        [
         "9",
         "79449476",
         "How do I remove escape characters from output of nltk.word_tokenize?",
         "<p>How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output.</p>\n<p>Here's my code:</p>\n<pre><code>from __future__ import division\nimport nltk, re, pprint\nfrom urllib.request import urlopen\n\nurl = &quot;https://www.gutenberg.org/cache/epub/75394/pg75394.txt&quot;\nraw = urlopen(url).read()\nraw = raw.decode('utf-8')\ntokens = nltk.word_tokenize(raw)\nprint(type(tokens))\nprint(len(tokens))\nprint(tokens[:10])\n</code></pre>\n<p>And the output, with the escapes visible in the first list item:\n<a href=\"https://i.sstatic.net/L1QJ1Mdr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/L1QJ1Mdr.png\" alt=\"enter image description here\" /></a></p>\n<p>I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong?</p>\n<p>I'm using Python 3.12.1 on Windows 11.</p>\n<p>Thanks in advance - please do let me know if I can provide any further helpful information.</p>\n",
         "2025-02-18 20:10:13",
         "1",
         "59",
         "1",
         "<python><nlp><nltk><tokenize><text-processing>",
         null,
         null
        ],
        [
         "10",
         "79448878",
         "Python Farm-haystack Dependencies",
         "<p>i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these</p>\n<p>from this implementation:</p>\n<pre><code>import os\nfrom typing import List\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes import PreProcessor, BM25Retriever, FARMReader\n\n# Initialize an in-memory document store (replaceable with Elasticsearch)\ndocument_store = InMemoryDocumentStore()\n\n# Folder where uploaded documents are stored\nUPLOAD_FOLDER = &quot;uploaded_docs&quot;\n\n# Ensure the upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\n\ndef list_documents() -&gt; List[str]:\n    &quot;&quot;&quot;List all uploaded documents.&quot;&quot;&quot;\n    try:\n        return os.listdir(UPLOAD_FOLDER)\n    except FileNotFoundError:\n        raise RuntimeError(f&quot;Upload folder '{UPLOAD_FOLDER}' not found. Please create it.&quot;)\n\n\ndef read_document(file_path: str) -&gt; str:\n    &quot;&quot;&quot;Read the content of a document.&quot;&quot;&quot;\n    try:\n        with open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:\n            return f.read()\n    except Exception as e:\n        raise RuntimeError(f&quot;Error reading file '{file_path}': {str(e)}&quot;)\n\n\ndef preprocess_document(content: str) -&gt; List[dict]:\n    &quot;&quot;&quot;Preprocess the document content into smaller chunks for indexing.&quot;&quot;&quot;\n    preprocessor = PreProcessor(\n        split_by=&quot;word&quot;,  # Split the content into chunks by word count\n        split_length=200,  # Chunk size\n        split_overlap=20,  # Overlap between chunks\n        split_respect_sentence_boundary=True,\n    )\n    return preprocessor.process({&quot;content&quot;: content})\n\n\ndef index_document(file_name: str):\n    &quot;&quot;&quot;Read, preprocess, and index a document.&quot;&quot;&quot;\n    file_path = os.path.join(UPLOAD_FOLDER, file_name)\n    if not os.path.isfile(file_path):\n        raise RuntimeError(f&quot;File '{file_name}' not found in '{UPLOAD_FOLDER}'.&quot;)\n\n    content = read_document(file_path)\n    chunks = preprocess_document(content)\n\n    # Prepare chunks in Haystack-compatible format\n    formatted_chunks = [{&quot;content&quot;: chunk[&quot;content&quot;]} for chunk in chunks]\n    document_store.write_documents(formatted_chunks)\n\n    return {\n        &quot;message&quot;: f&quot;Document '{file_name}' indexed successfully.&quot;,\n        &quot;chunks_count&quot;: len(formatted_chunks),\n    }\n\n\ndef search_documents(query: str):\n    &quot;&quot;&quot;Search indexed documents using a query.&quot;&quot;&quot;\n    retriever = BM25Retriever(document_store=document_store)\n    reader = FARMReader(model_name_or_path=&quot;deepset/roberta-base-squad2&quot;, use_gpu=False)\n    \n    # Retrieve documents\n    retrieved_docs = retriever.retrieve(query)\n    if not retrieved_docs:\n        return {&quot;message&quot;: &quot;No relevant documents found.&quot;}\n\n    # Reader to predict answers from retrieved documents\n    answers = reader.predict(query=query, documents=retrieved_docs, top_k=3)\n\n    # Serialize the results to avoid unsupported types\n    results = [\n        {\n            &quot;answer&quot;: ans.answer,\n            &quot;score&quot;: ans.score,\n            &quot;context&quot;: ans.context,\n            &quot;document_id&quot;: ans.document_id,\n        }\n        for ans in answers[&quot;answers&quot;]\n    ]\n\n    return {&quot;results&quot;: results}\n</code></pre>\n<p>But i keep getting this error:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 122, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 131, in _main\n    prepare(preparation_data)\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 244, in prepare\n    _fixup_main_from_name(data['init_main_from_name'])\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 268, in _fixup_main_from_name\n    main_content = runpy.run_module(mod_name,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;&lt;frozen runpy&gt;&quot;, line 226, in run_module\n  File &quot;&lt;frozen runpy&gt;&quot;, line 98, in _run_module_code\n  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/main.py&quot;, line 7, in &lt;module&gt;\n    from app.views.routes import router\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/views/routes.py&quot;, line 2, in &lt;module&gt;\n    from app.services.document_service import list_documents, index_document, search_documents\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/services/document_service.py&quot;, line 3, in &lt;module&gt;\n    from haystack.document_stores import InMemoryDocumentStore\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/__init__.py&quot;, line 8, in &lt;module&gt;\n    from haystack.schema import Document, Answer, Label, MultiLabel, Span, EvaluationResult, TableCell\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/schema.py&quot;, line 42, in &lt;module&gt;\n    @dataclass\n     ^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 250, in dataclass\n    return create_dataclass(_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 241, in create_dataclass\n    pydantic_complete = _pydantic_dataclasses.complete_dataclass(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py&quot;, line 159, in complete_dataclass\n    schema = gen_schema.generate_schema(cls, from_dunder_get_core_schema=False)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 832, in match_type\n    return self._dataclass_schema(obj, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1561, in _dataclass_schema\n    args = sorted(\n           ^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1562, in &lt;genexpr&gt;\n    (self._generate_dc_field_schema(k, v, decorators) for k, v in fields.items()),\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 933, in _generate_dc_field_schema\n    common_field = self._common_field_schema(name, field_info, decorators)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1081, in _common_field_schema\n    schema = self._apply_annotations(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1825, in _apply_annotations\n    schema = get_inner_schema(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py&quot;, line 82, in __call__\n    schema = self._handler(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1806, in inner_handler\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 840, in match_type\n    return self._match_generic_type(obj, origin)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 864, in _match_generic_type\n    return self._union_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1152, in _union_schema\n    choices.append(self.generate_schema(arg))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 844, in match_type\n    return self._unknown_type_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 405, in _unknown_type_schema\n    raise PydanticSchemaGenerationError(\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for &lt;class 'pandas.core.frame.DataFrame'&gt;. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(&lt;some type&gt;) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(&lt;some type&gt;)` since we do not call `__get_pydantic_core_schema__` on `&lt;some type&gt;` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type\n</code></pre>\n",
         "2025-02-18 16:05:55",
         "-1",
         "29",
         "1",
         "<python-3.x><nlp><artificial-intelligence><fastapi><haystack>",
         null,
         null
        ],
        [
         "11",
         "79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07 02:48:25",
         "-1",
         "43",
         "1",
         "<keras><deep-learning><nlp><lstm><sentiment-analysis>",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n"
        ],
        [
         "12",
         "79406743",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text",
         "<p>I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns &quot;UNK&quot;. Here is my code:</p>\n<pre><code>from quickumls import QuickUMLS\n\nquickumls_fp = &quot;med7_en/lib/python3.10/site-packages/quickumls&quot;\nmatcher = QuickUMLS(quickumls_fp)\n\ndef extract_umls_cuis(text):\n    &quot;&quot;&quot;Extract UMLS CUIs using QuickUMLS.&quot;&quot;&quot;\n    if isinstance(text, str):\n        matches = matcher.match(text)\n        if matches:\n            return [match['cui'] for match in matches[0]]\n        else:\n            return &quot;UNK&quot;\n\nsample_text = &quot;diclofenac.&quot;\nprint(extract_umls_cuis(sample_text))\n</code></pre>\n<p>What I Have Checked:</p>\n<ul>\n<li>QuickUMLS Installation: I have installed QuickUMLS correctly.</li>\n<li>UMLS Data Availability: I have set the correct path to QuickUMLS.</li>\n<li>Different Input Words: I tried various medical terms, but all return &quot;UNK&quot;.</li>\n</ul>\n",
         "2025-02-02 14:12:55",
         "0",
         "26",
         "1",
         "<python><machine-learning><deep-learning><nlp>",
         null,
         null
        ],
        [
         "13",
         "79402035",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query",
         "<p>How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation.\nI have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n",
         "2025-01-31 07:26:35",
         "1",
         "50",
         "1",
         "<nlp><large-language-model><rag><vectara><enterprise-rag>",
         null,
         null
        ],
        [
         "14",
         "79399448",
         "LLM Model Lacking Confidence and Changing Answers Based on User Input",
         "<p>I've trained a Large Language Model (LLM) using the RAG method to answer user queries. However, I'm facing an issue where the model lacks confidence in its answers and changes them based on user input, even when the initial response is correct.</p>\n<p>For example, when asked &quot;What is the capital of France?&quot;, the model correctly responds with &quot;Paris.&quot; However, if the user replies &quot;No, it's Berlin,&quot; the model accepts this incorrect response and later provides &quot;Berlin&quot; as the capital of France when asked again.</p>\n<p>I've tried using different prompt templates to reinforce answer consistency, but the issue persists. How can I improve the model’s robustness and prevent it from altering correct answers based on user responses? Any suggestions or guidance would be greatly appreciated.</p>\n",
         "2025-01-30 09:49:37",
         "3",
         "60",
         "1",
         "<nlp><langchain><large-language-model><aiml>",
         null,
         null
        ],
        [
         "15",
         "79393930",
         "Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data?",
         "<p>I’m running into a frustrating issue while training a BERT-based multi-label text classification model on an imbalanced dataset. After a few epochs, the training loss suddenly becomes NaN, and I can’t seem to figure out why. I’ve tried a bunch of different things, but nothing has worked so far. Hoping someone here has dealt with this before.\n<strong>Dataset Setup</strong>\nNumber of samples is around 100K\nNumber of labels is around 50\nImbalance: Some labels are super common (appear in 80% of samples), while others are barely there (less than 0.5%)</p>\n<p><strong>My Setup</strong>\nI’m using Hugging Face’s bert-base-uncased with a custom classification head.</p>\n<pre><code>from transformers import BertModel\nimport torch.nn as nn\n\nclass MultiLabelClassifier(nn.Module):\n    def __init__(self, num_labels):\n        super(MultiLabelClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.classifier(outputs.pooler_output)  # Using the [CLS] token\n        return logits\n\n</code></pre>\n<p>I’m using ***BCEWithLogitsLoss ***with a weighted loss function to deal with the imbalance:</p>\n<pre><code>class_weights = torch.tensor([1.0 / (freq + 1e-5) for freq in label_frequencies]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n\n</code></pre>\n<p>after 2 or 3 epochs;</p>\n<ul>\n<li>The loss starts off fine but becomes NaN</li>\n<li>Some logits are ridiculously large or small (1e20, -1e20) before the NaN happens.</li>\n<li>Gradients also seem to explode right before the NaN loss kicks in.</li>\n</ul>\n<p>to solve this issue,</p>\n<ol>\n<li>Added gradient clipping, which helps a bit but doesn’t fully fix the issue\n<code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code></li>\n<li>Tried reducing the learning rate to 5e-6, which delays the NaN issue but doesn’t stop it completely</li>\n<li>Thought the issue might be with the classifier weights, so I reinitialized them like this,</li>\n</ol>\n<pre><code>for layer in model.classifier.parameters():\n    if isinstance(layer, nn.Linear):\n        nn.init.xavier_normal_(layer.weight)\n\n</code></pre>\n<ol start=\"4\">\n<li>Rewrote the loss calculation using torch.logsigmoid for numerical stability...</li>\n</ol>\n<pre><code>loss = -labels * torch.logsigmoid(logits) - (1 - labels) * torch.logsigmoid(-logits)\nloss = loss.mean()\n</code></pre>\n<p>Nothing seems to solve the problem completely.</p>\n<p><strong>------my questions---</strong></p>\n<ol>\n<li>Why is this happening? Is it because of the extreme imbalance in my dataset or something else?</li>\n<li>How can I fix it? Should I try something like label smoothing, or is there a better way to stabilize the training?</li>\n</ol>\n<p>here is a snippet of my training loop for context.</p>\n<pre><code>optimizer = AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in dataloader:\n        input_ids, attention_mask, labels = batch[&quot;input_ids&quot;], batch[&quot;attention_mask&quot;], batch[&quot;labels&quot;]\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n</code></pre>\n<p>---<strong>I Need</strong>--\nI’m looking for practical suggestions to <strong>prevent the NaN loss issue entirely</strong> and <strong>stabilize training when working with imbalanced multi-label datasets</strong>.</p>\n<p>If anyone has faced this issue or knows a good fix, I’d really appreciate the help. Thanks!</p>\n",
         "2025-01-28 13:03:15",
         "0",
         "64",
         "1",
         "<deep-learning><nlp><bert-language-model><multilabel-classification><imbalanced-data>",
         null,
         null
        ],
        [
         "16",
         "79385917",
         "torch.OutOfMemoryError: CUDA out of memory. (Google Colab)",
         "<p>I tried to adapt the mBERT model to an existing code. However, I received the following issue even though I tried different solutions.</p>\n<pre><code>torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 9.06 MiB is free. Process 84806 has 14.74 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 129.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n</code></pre>\n<p>Here's the news model that I'm trying to adapt to DST-MetaASSIST(STAR), which you can find it here:</p>\n<pre><code>### For DST-MetaASSIST\n!git clone https://github.com/smartyfh/DST-MetaASSIST\n</code></pre>\n<p>These are the new models:</p>\n<pre><code># First model mBERT \nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n# Load the model with the correct number of labels\nmodel = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n\n# Second model XLM-R\n\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n\n# Load the tokenizer and model\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nmodel = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=number_of_labels)\n\n\n# Third model mT5\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the mT5 tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained('google/mt5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('google/mt5-small')\n</code></pre>\n<p><strong>I changed the 'train-S1.py' file and then run it</strong></p>\n<pre><code>import torch\nimport os\nimport torch\n\n# Set environment variable for CUDA memory management\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ntorch.cuda.empty_cache()  # Clear CUDA cache\ntorch.backends.cudnn.benchmark = True\n\n# Check for GPU availability\nif torch.cuda.is_available():\n    device = torch.device(&quot;cuda&quot;)\n    print(&quot;Using GPU:&quot;, torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(&quot;cpu&quot;)\n    print(&quot;Using CPU&quot;)\n\n# Reduce batch sizes for memory optimization\ntrain_batch_size = 2  # Reduced from 4 or 16\nmeta_batch_size = 1    # Reduced from 2 or 8\n\n# Run your training script with reduced batch sizes\n!python3 /content/DST-MetaASSIST/STAR/train-S1.py --data_dir data/mwz2.4 --save_dir output-meta24-S1/exp --train_batch_size 2 --meta_batch_size 1 --enc_lr 4e-5 --dec_lr 1e-4 --sw_lr 5e-5 --init_weight 0.5 --n_epochs 1 --do_train\n</code></pre>\n<p>The new script:</p>\n<pre><code># import faulthandler\n# faulthandler.enable()\n# learn slot-wise weight\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport argparse\nimport random\nimport json\nimport time\nimport logging\nfrom tqdm import tqdm, trange\n\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom utils.data_utils import Processor, MultiWozDataset\nfrom utils.eval_utils import model_evaluation\nfrom utils.loss_utils import *\nfrom utils.label_lookup import get_label_lookup_from_first_token\nfrom models.DST import UtteranceEncoding, BeliefTracker\n# Import necessary libraries\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n\n\n#====================================\nimport higher\nimport itertools\nfrom models.WeightNet import SlotWeight\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR\n#====================================\n\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup as get_linear_schedule_with_warmup_T\n\nos.environ['CUDA_VISIBLE_DEVICES']='0'\n# torch.cuda.set_device(0)\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                    datefmt='%m/%d/%Y %H:%M:%S',\n                    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, dec_num_warmup_steps, num_training_steps, last_epoch=-1):\n    &quot;&quot;&quot;\n    see https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/optimization.py#L75\n    &quot;&quot;&quot;\n    def enc_lr_lambda(current_step: int):\n        if current_step &lt; enc_num_warmup_steps:\n            return float(current_step) / float(max(1, enc_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - enc_num_warmup_steps))\n        )\n    \n    def dec_lr_lambda(current_step: int):\n        if current_step &lt; dec_num_warmup_steps:\n            return float(current_step) / float(max(1, dec_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - dec_num_warmup_steps))\n        )\n\n    return LambdaLR(optimizer, [enc_lr_lambda, enc_lr_lambda, dec_lr_lambda], last_epoch)\n\ndef set_seed(args, device):\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n    torch.manual_seed(args.random_seed)\n    if device == &quot;cuda&quot;:\n        torch.cuda.manual_seed(args.random_seed)\n        torch.cuda.manual_seed_all(args.random_seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        \ndef get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device):\n    slot_lookup = get_label_lookup_from_first_token(slot_meta, tokenizer, sv_encoder, device)\n    value_lookup = []\n    for slot in ontology.keys():\n        value_lookup.append(get_label_lookup_from_first_token(ontology[slot], tokenizer, sv_encoder, device))\n    return slot_lookup, value_lookup\n\ndef prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    enc_param_optimizer = list(model.encoder.named_parameters())\n    dec_param_optimizer = list(model.decoder.parameters())\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in enc_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in enc_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n        {'params': dec_param_optimizer, 'lr': dec_learning_rate}\n        ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    scheduler = get_linear_schedule_with_warmup(optimizer, int(num_train_steps * enc_warmup_ratio),\n                                                int(num_train_steps * dec_warmup_ratio), num_train_steps)\n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n\n'''def prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    \n    # Access all parameters of the model\n    param_optimizer = list(model.named_parameters())\n    \n    # Group parameters for the optimizer\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    \n    # Calculate warmup steps\n    enc_num_warmup_steps = int(num_train_steps * enc_warmup_ratio)\n    \n    # Create the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, num_train_steps)\n    \n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n'''\n\ndef get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids):\n    _, pred_all_distance = slot_value_matching(slot_output, value_lookup)\n                \n    loss_slot_gt = unreduced_cross_entropy_loss(pred_all_distance, label_ids)\n    loss_slot_pseudo = unreduced_cross_entropy_loss(pred_all_distance, pseudo_label_ids)\n    \n    return loss_slot_gt, loss_slot_pseudo\n\ndef main(args):\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n        \n    # logger\n    logger_file_name = args.save_dir.split('/')[1]\n    fileHandler = logging.FileHandler(os.path.join(args.save_dir, &quot;%s.txt&quot;%(logger_file_name)))\n    logger.addHandler(fileHandler)\n    logger.info(args)\n    \n    # cuda setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logger.info(&quot;device: {}&quot;.format(device))\n    \n    # set random seed\n    set_seed(args, device)\n\n    #******************************************************\n    # load data\n    #******************************************************\n    processor = Processor(args)\n    slot_meta = processor.slot_meta\n    ontology = processor.ontology\n    logger.info(slot_meta)\n   \n    # Load the mBERT tokenizer\n    #tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n    tokenizer = BertTokenizer.from_pretrained(args.pretrained_model)\n\n    \n    \n    if args.do_train:\n        train_data_raw = processor.get_instances(args.data_dir, args.train_data, tokenizer, True)\n        print(&quot;# train examples %d&quot; % len(train_data_raw))\n\n        # Get unique labels from the training data\n        unique_labels = set(label for instance in train_data_raw for label in instance.label_ids)  # Flatten the list\n        num_labels = len(unique_labels)  # Count unique labels\n        print(f&quot;Number of unique labels: {num_labels}&quot;)\n        \n        meta_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(&quot;# meta examples %d&quot; % len(meta_data_raw))\n        \n        dev_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(&quot;# dev examples %d&quot; % len(dev_data_raw))\n    \n    test_data_raw = processor.get_instances(args.data_dir, args.test_data, tokenizer)\n    print(&quot;# test examples %d&quot; % len(test_data_raw))\n    logger.info(&quot;Data loaded!&quot;)\n    \n    ## Initialize slot and value embeddings\n    sv_encoder = UtteranceEncoding.from_pretrained(args.pretrained_model)\n    for p in sv_encoder.bert.parameters():\n        p.requires_grad = False  \n    slot_lookup, value_lookup = get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device)\n\n    #num_labels = len(value_lookup)\n    # Load the mBERT model with the correct number of labels\n    #model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n    # Clear unused variables and cache\n    torch.cuda.empty_cache()\n    if args.do_train:\n        train_data = MultiWozDataset(train_data_raw,\n                                     tokenizer,\n                                     word_dropout=args.word_dropout,\n                                     max_seq_length=args.max_seq_length,\n                                     use_pseudo_label=True)\n        meta_data = MultiWozDataset(meta_data_raw,\n                                    tokenizer,\n                                    word_dropout=0.0, # do word dropout here???\n                                    max_seq_length=args.max_seq_length)\n\n        num_train_steps = int(len(train_data_raw) / args.train_batch_size * args.n_epochs)\n        logger.info(&quot;***** Run training *****&quot;)\n        logger.info(&quot; Num examples = %d&quot;, len(train_data_raw))\n        logger.info(&quot; Batch size = %d&quot;, args.train_batch_size)\n        logger.info(&quot; Num steps = %d&quot;, num_train_steps)\n\n        train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data,\n                                      sampler=train_sampler,\n                                      batch_size=args.train_batch_size,\n                                      collate_fn=train_data.collate_fn)\n        \n        meta_sampler = RandomSampler(meta_data)\n        meta_dataloader = DataLoader(meta_data,\n                                     sampler=meta_sampler,\n                                     batch_size=args.meta_batch_size,\n                                     collate_fn=meta_data.collate_fn)\n        meta_dataloader = itertools.cycle(meta_dataloader)\n        \n        #******************************************************\n        # build model\n        #******************************************************\n        ## model initialization\n        base_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        # Load the model without unsupported arguments\n        # Load the mBERT model with the correct number of labels\n        #base_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        base_model.to(device)\n\n\n\n\n        '''\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        meta_model.to(device)\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                    num_self_attention_layer=args.num_self_attention_layer)\n        #\n        '''\n        meta_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        meta_model.to(device)\n        \n        # Number of slots\n        SW = SlotWeight(len(slot_meta), init_val=np.log(args.init_weight/(1.0 - args.init_weight)))\n        SW.to(device)\n\n        ## prepare optimizer\n        # Prepare optimizer\n        # Prepare optimizer\n        #base_optimizer, base_scheduler = prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n    \n\n        base_optimizer, base_scheduler = \\\n        prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n        \n        logger.info(base_optimizer)\n        # meta model is a copy of the base model, thus shares the optimizer and scheduler\n        meta_optimizer, meta_scheduler = \\\n        prepare_optimizer(meta_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n\n        sw_param_optimizer = list(SW.parameters())\n        sw_optimizer = optim.AdamW(sw_param_optimizer, lr=args.sw_lr)\n        sw_scheduler = get_linear_schedule_with_warmup_T(sw_optimizer,\n                                                         int(num_train_steps * args.sw_warmup),\n                                                         num_train_steps)\n        \n        #******************************************************\n        # training\n        #******************************************************\n        logger.info(&quot;Training...&quot;)\n\n        best_loss = None\n        best_acc = None\n        last_update = None\n\n        for epoch in trange(int(args.n_epochs), desc=&quot;Epoch&quot;):       \n            batch_loss, meta_batch_loss = [], []\n            for step, batch in enumerate(tqdm(train_dataloader)):\n                base_model.train()\n\n                batch = [b.to(device) for b in batch]\n                input_ids, segment_ids, input_mask, label_ids, pseudo_label_ids = batch\n                \n                # forward (meta model)\n                meta_model.load_state_dict(base_model.state_dict())\n                meta_optimizer.load_state_dict(base_optimizer.state_dict())\n                meta_optimizer.zero_grad()\n                with higher.innerloop_ctx(meta_model, meta_optimizer) as (meta_m, meta_opt):\n                    meta_m.train()\n                    slot_output = meta_m(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    \n                    loss_slot_gt, loss_slot_pseudo = \\\n                    get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                    \n                    s_weight = SW()\n                \n                    meta_loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                    # first backward\n                    meta_opt.step(meta_loss)\n                    \n                    # compute on the meta validation set\n                    batch_v = next(meta_dataloader)\n                    batch_v = [b.to(device) for b in batch_v]\n                    input_ids_v, segment_ids_v, input_mask_v, label_ids_v = batch_v\n                    # second forward\n                    meta_m.eval() # disable dropout\n                    slot_output_v = meta_m(input_ids=input_ids_v,\n                                           attention_mask=input_mask_v,\n                                           token_type_ids=segment_ids_v,\n                                           slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    _, pred_all_distance = slot_value_matching(slot_output_v, value_lookup)\n                    loss_v, _, _ = hard_cross_entropy_loss(pred_all_distance, label_ids_v)\n                    # backward over backward\n                    sw_optimizer.zero_grad()\n                    loss_v.backward()\n                    sw_optimizer.step()\n                    sw_scheduler.step()\n                    meta_batch_loss.append(loss_v.item())\n                \n                # Now we have the updated weight net  \n                # forward (base model)\n                slot_output = base_model(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n\n                loss_slot_gt, loss_slot_pseudo = \\\n                get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                with torch.no_grad():    \n                    s_weight = SW()\n\n                loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                # backward (base model)\n                base_optimizer.zero_grad()\n                loss.backward()\n                base_optimizer.step()\n                base_scheduler.step()\n\n                batch_loss.append(loss.item())\n                if step % 300 == 0:\n                    logger.info(&quot;[%d/%d] [%d/%d] mean_loss: %.6f mean_meta_loss: %.6f&quot; % \\\n                               (epoch+1, args.n_epochs, step, len(train_dataloader),\n                                np.mean(batch_loss), np.mean(meta_batch_loss)))\n                    batch_loss, meta_batch_loss = [], []\n                    logger.info(f'Slot weights: {s_weight.cpu().numpy()}')\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, dev_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n                if last_update is None or best_loss &gt; eval_res['loss']:\n                    best_loss = eval_res['loss']\n#                     save_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#                     torch.save(base_model.state_dict(), save_path)\n                    print(&quot;Best Loss : &quot;, best_loss)\n                    print(&quot;\\n&quot;)\n                if last_update is None or best_acc &lt; eval_res['joint_acc']:\n                    best_acc = eval_res['joint_acc']\n                    save_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n                    save_path_w = os.path.join(args.save_dir, 'sw.bin')\n                    torch.save(base_model.state_dict(), save_path)\n                    torch.save(SW.state_dict(), save_path_w)\n                    last_update = epoch\n                    print(&quot;Best Acc : &quot;, best_acc)\n                    print(&quot;\\n&quot;)\n\n                logger.info(&quot;*** Epoch=%d, Last Update=%d, Dev Loss=%.6f, Dev Acc=%.6f, Dev Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***&quot; % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, test_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n\n                logger.info(&quot;*** Epoch=%d, Last Update=%d, Tes Loss=%.6f, Tes Acc=%.6f, Tes Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***&quot; % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if last_update + args.patience &lt;= epoch:\n                    break\n            torch.cuda.empty_cache()\n\n#         print(&quot;Test using best loss model...&quot;)\n#         best_epoch = 0\n#         ckpt_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#         model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n#                               num_self_attention_layer=args.num_self_attention_layer)\n#         ckpt = torch.load(ckpt_path, map_location='cpu')\n#         model.load_state_dict(ckpt)\n#         model.to(device)\n\n#         test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n#                                     ontology, best_epoch, is_gt_p_state=False)\n#         logger.info(&quot;Results based on best loss: &quot;)\n#         logger.info(test_res)\n    #----------------------------------------------------------------------\n    print(&quot;Test using best acc model...&quot;)\n    best_epoch = 1\n    ckpt_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n    model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                          num_self_attention_layer=args.num_self_attention_layer)\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    model.load_state_dict(ckpt)\n    model.to(device)\n\n    test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n                                ontology, best_epoch, is_gt_p_state=False)\n    logger.info(&quot;Results based on best acc: &quot;)\n    logger.info(test_res)\n    \n\nif __name__ == &quot;__main__&quot;:\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(&quot;--data_dir&quot;, default='data/mwz2.4', type=str)\n    parser.add_argument(&quot;--train_data&quot;, default='train_dials_v2.json', type=str)\n    parser.add_argument(&quot;--dev_data&quot;, default='dev_dials_v2.json', type=str)\n    parser.add_argument(&quot;--test_data&quot;, default='test_dials_v2.json', type=str)\n    #parser.add_argument(&quot;--pretrained_model&quot;, default='bert-base-uncased', type=str)\n    parser.add_argument(&quot;--pretrained_model&quot;, default='bert-base-multilingual-cased', type=str)\n\n    parser.add_argument(&quot;--save_dir&quot;, default='output-meta24-S1/exp', type=str)\n\n    parser.add_argument(&quot;--random_seed&quot;, default=42, type=int)\n\n    #parser.add_argument(&quot;--train_batch_size&quot;, default=16, type=int)\n    #parser.add_argument(&quot;--meta_batch_size&quot;, default=8, type=int)\n    parser.add_argument(&quot;--train_batch_size&quot;, default=2, type=int)  # Reduce from 16 to 8 to 2\n    parser.add_argument(&quot;--meta_batch_size&quot;, default=1, type=int)   # Reduce from 8 to 4 to 1\n    \n    parser.add_argument(&quot;--enc_warmup&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--dec_warmup&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--sw_warmup&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--enc_lr&quot;, default=4e-5, type=float)\n    parser.add_argument(&quot;--dec_lr&quot;, default=1e-4, type=float)\n    parser.add_argument(&quot;--sw_lr&quot;, default=5e-5, type=float)\n    parser.add_argument(&quot;--init_weight&quot;, default=0.5, type=float)\n    parser.add_argument(&quot;--n_epochs&quot;, default=15, type=int)\n    parser.add_argument(&quot;--eval_epoch&quot;, default=1, type=int)\n    parser.add_argument(&quot;--eval_step&quot;, default=100000, type=int)\n\n    parser.add_argument(&quot;--dropout_prob&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--word_dropout&quot;, default=0.1, type=float)\n    \n    parser.add_argument(&quot;--max_seq_length&quot;, default=512, type=int)\n    parser.add_argument(&quot;--patience&quot;, default=6, type=int)\n    parser.add_argument(&quot;--attn_head&quot;, default=4, type=int)\n    parser.add_argument(&quot;--num_history&quot;, default=20, type=int)\n    parser.add_argument(&quot;--num_self_attention_layer&quot;, default=6, type=int)\n    \n    parser.add_argument(&quot;--do_train&quot;, action='store_true')\n       \n    args = parser.parse_args()\n    \n    print('pytorch version: ', torch.__version__)\n    args.torch_version = torch.__version__\n    args.transformers_version = transformers.__version__\n    args.save_dir = args.save_dir + \\\n    f'-sd{args.random_seed}-bz{args.train_batch_size}-{args.meta_batch_size}-lr{args.enc_lr}-{args.dec_lr}-{args.sw_lr}-ep{args.n_epochs}'\n\n    main(args)\n\n</code></pre>\n<p>Any suggestions?! Thanks in advance.</p>\n",
         "2025-01-24 23:41:19",
         "0",
         "81",
         "1",
         "<python><nlp><google-colaboratory><bert-language-model>",
         null,
         null
        ],
        [
         "17",
         "79377676",
         "how to create a Natural Language Inference pipeline in haystack",
         "<p>Could anyone help me with some advice on how to create a Natural Language Inference pipeline in haystack</p>\n<p>I want to use the Haystack framework to create a pipeline for Natural Language Inference on the response from a Retrieval-Augmented Generation (RAG) application</p>\n<p>Because I'm using haystack-ai , I cannot use farm-haystack. If I could use farm-haystack (v1.0) I believe I could do something like below:</p>\n<pre><code>from haystack import Pipeline\nfrom haystack_ai.nodes import HuggingFaceTextClassifier\n\nclassifier = HuggingFaceTextClassifier(\n    model_name_or_path=entailment_model,\n    task=&quot;text-classification&quot;,  # Task type: text classification\n    labels=[\n        &quot;entailment&quot;,\n        &quot;contradiction&quot;,\n        &quot;neutral&quot;,\n    ],  # Define the labels your model is trained on\n)\n\nclassifier_pipeline = Pipeline()\nclassifier_pipeline.add_cmponent(&quot;classifier_llm&quot;, classifier)\npremise = &quot;The sun rises in the east and sets in the west.&quot;\nhypothesis = &quot;The sun rises in the east.&quot;\n\nclassifier_pipeline.run({&quot;classifier_llm&quot;: {&quot;text&quot;: premise, &quot;text_pair&quot;: hypothesis}})\n</code></pre>\n<p>However I cannot see how to achieve the same in haystack v2.0 (haystack-ai) .</p>\n<p>Any comments or pointers welcome.</p>\n",
         "2025-01-22 12:17:47",
         "0",
         "44",
         "1",
         "<nlp><huggingface><haystack>",
         null,
         null
        ],
        [
         "18",
         "79372521",
         "TypeError: isinstance() arg 2 must be a type or tuple of types with collections search in Weaviate",
         "<p>I have the following code:</p>\n<pre><code>from weaviate.classes.query import MetadataQuery\nimport weaviate\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\nclient = weaviate.connect_to_local()\nauto_finance= client.collections.get(&quot;AutomotiveFinance&quot;)\n\nquery_vector = embedding_model.embed_documents(&quot;what is Honda Cash and cash equivalents?&quot;)\nprint(len(query_vector[0]))\n#query_vector=[float(i) for i in query_vector[0]]\nquery_vector = [0.023]*768\nresponse = auto_finance.query.hybrid(\n    query = &quot;what is Honda Cash and cash equivalents?&quot;,\n    vector = query_vector, alpha = 0.25, limit = 4  \n)\n</code></pre>\n<p>It queries from a collections called AutomotiveFinance, which is locally hosted.\nI have replaced the query_vec with a dummy vec just to debug, since that example is used in the official document.</p>\n<p>The Automotive Finance collections seems to be ok: below is the inspection code and output:</p>\n<pre><code>import weaviate\nimport weaviate.classes.config as wc\n\n\nclient = weaviate.connect_to_local()\n\nresult = client.collections.get('AutomotiveFinance')\nfor item in result.iterator():\n    print(item.uuid, item.properties)\n\ndata_object = result.query.fetch_object_by_id(\n    &quot;ffdc789b-b188-4fcf-94d5-2e4ad6be37ec&quot;,\n    include_vector=True\n)\n\nprint(len(data_object.vector[&quot;default&quot;]))\n\nclient.close()\n</code></pre>\n<p>output from the AutomotiveFinance inspection code has all the metadata and the id as expected:</p>\n<pre><code>faab711b-f714-457c-a547-ff755b2de4d3 {'page1': 2, 'company': 'honda', 'doc_type': '10q', 'page2': 3, 'raw_text': 'THIS IS PAGE 3\\nBased on the provided text from the SEC Form 10-Q, here is a structured summary of the key sections and items:\\n\\n### Company Information\\n- **Company Name:** American Honda Finance Corporation\\n- **Report \nType:** Quarterly Report on Form 10-Q\\n- **Reporting Period:** For the quarter ended June 30, 2024'}\n</code></pre>\n<p>And the length of the vector is 768 -&gt; this matches the vector embedding model's expected behavior.</p>\n<p>However, when I run the query, I am keep getting the following error:</p>\n<pre><code>PS C:\\Users\\ikim1&gt; &amp; C:/Users/ikim1/RAG-blog/Scripts/python.exe &quot;c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py&quot;\n768\nTraceback (most recent call last):\n  File &quot;c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py&quot;, line 13, in &lt;module&gt;\n    response = auto_finance.query.hybrid(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\syncify.py&quot;, line 23, in sync_method\n    return _EventLoopSingleton.get_instance().run_until_complete(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\event_loop.py&quot;, line 40, in run_until_complete       \n    return fut.result()\n  File &quot;C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result    \n    return self.__get_result()\n  File &quot;C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\queries\\hybrid\\query.py&quot;, line 107, in hybrid\n    res = await self._query.hybrid(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\grpc\\query.py&quot;, line 189, in hybrid      \n    _validate_input(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 31, in _validate_input\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 31, in &lt;genexpr&gt;\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 61, in _is_valid\n    return all(isinstance(val, args[0]) for val in value)\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 61, in &lt;genexpr&gt;\n    return all(isinstance(val, args[0]) for val in value)\nTypeError: isinstance() arg 2 must be a type or tuple of types\n</code></pre>\n<p>I am not completely sure as to what is going on. I think I inputted all the parameters as described in this: <a href=\"https://weaviate.io/developers/weaviate/search/hybrid#specify-a-search-vector\" rel=\"nofollow noreferrer\">https://weaviate.io/developers/weaviate/search/hybrid#specify-a-search-vector</a>.</p>\n<p>EDIT: I figured I should also provide on how I am doing the embedding and all:</p>\n<pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nimport weaviate\nimport weaviate.classes.config as wc\nimport json\nimport os\n\nclient = weaviate.connect_to_local()\n# check if the client is alive\nassert client.is_live()\n\n# delete the existing schema + create schema \nclient.collections.delete(&quot;AutomotiveFinance&quot;)\nclient.collections.create(\n    name = 'AutomotiveFinance',\n    properties=[\n        wc.Property(name = 'page1', data_type = wc.DataType.INT),\n        wc.Property(name = 'page2', data_type = wc.DataType.INT),\n        wc.Property(name = 'company', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'doc_type', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'raw_text', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'embedding', data_type = wc.DataType.BLOB) # here BLOB stands for binary large object.\n    ]\n)\nauto_finance = client.collections.get(&quot;AutomotiveFinance&quot;)\n\n# get the path of each json file\njson_top_file_path = r'C:\\Users\\ikim1\\OneDrive\\Desktop\\RAG file'\njson_file_path = []\nfor file in os.listdir(json_top_file_path):\n    if file.endswith('.json'):\n        file_path = os.path.join(json_top_file_path, file)\n        json_file_path.append(file_path)\n\n# Initialize the text splitter +  embedding model\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,  # Maximum size of each chunk\n    chunk_overlap=100  # Overlap between consecutive chunks\n)\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\n# for each file path of json get the chunks. \n\nchunks_with_metadata_list = []\nfor one_json_path in json_file_path: \n    # each json had the following structure\n    # json['pages'], json['file_path'], json['company'] json['doc_type']\n    # in json['pages'], there is list of each page as element \n    \n    # open the json file \n    with open(one_json_path, 'r') as file: \n        json_data = json.load(file)\n        pages = json_data['pages']\n        company = json_data['company']\n        doc_type = json_data['doc_type']\n        \n        # make the entire string from the pages\n        # make sure to insert the page numbers as well. \n        old_page_num = 0; old_md = ''; old_raw_txt = ''\n        json_string = '' \n        for i, page in enumerate(pages): \n            md = page['md']\n            raw_txt = page['text']\n            page_num = page['page']\n            print(i)\n            # if this is the second one, then start the chunking process\n            if i &gt; 0: \n                old_combined_str = &quot;THIS IS PAGE &quot; + str(old_page_num) + '\\n' + old_md + '\\n' + old_raw_txt\n                new_combined_str = &quot;THIS IS PAGE &quot; + str(page_num) + '\\n' + md + '\\n' + raw_txt\n                combined_str = new_combined_str + '\\n' + old_combined_str\n                # chunk the combined_str using recursive splittin,g but inject the metadata. \n                chunks = text_splitter.split_text(combined_str)\n                # inject the metadata into the chunks\n                for chunk in chunks: \n                    # embed the chunk : output is already a list. so no need for conversion for Weaviate\n                    embedded_chunk = embedding_model.embed_documents(chunk)[0]\n                    chunk_metadata = {\n                        &quot;page1&quot; : old_page_num, &quot;page2&quot; : page_num, \n                        &quot;company&quot; : company, &quot;doc_type&quot; : doc_type, \n                        'raw_text' : chunk\n                        }\n                    # put the chunk data into the vector database\n                    auto_finance.data.insert(\n                        properties = chunk_metadata, \n                        vector = [float(i) for i in embedded_chunk]\n                    )\n            # cache the previous one\n            old_md = md\n            old_raw_txt = raw_txt\n            old_page_num = page_num\n            \n\nclient.close()\n\n</code></pre>\n<p>Thanks!</p>\n",
         "2025-01-20 20:00:44",
         "1",
         "35",
         "1",
         "<python-3.x><nlp><vector-database><rag><weaviate>",
         null,
         null
        ],
        [
         "19",
         "79367089",
         "Is it possible to Fine-Tune TinyBERT on Mac (M1 chip)?",
         "<p>Does fine-tuning require huge resources, or is it possible to do this task on the local machine as well? I have an Apple M1 (8GB RAM). I know bigger models GPU access but what about TinyBERT? My training dataset has 100,000 samples for your reference.</p>\n",
         "2025-01-18 11:58:02",
         "-1",
         "53",
         "1",
         "<deep-learning><nlp><huggingface-transformers><text-classification><fine-tuning>",
         null,
         null
        ],
        [
         "20",
         "79330283",
         "Can't compile Marian NMT",
         "<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=\"https://marian-nmt.github.io/docs/#installation\" rel=\"nofollow noreferrer\">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>\n<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>\n<p>These are the steps that I followed:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n</code></pre>\n<p>This is the result I had:</p>\n<pre><code>➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2\n</code></pre>\n<p>Please help.</p>\n",
         "2025-01-05 06:04:59",
         "4",
         "62",
         "1",
         "<gcc><cmake><nlp><g++>",
         "79332711.0",
         "<p>The diagnostic that your build is tripping, <code>Wtemplate-id-cdtor</code>, was introduced\nwith GCC 14.1. It is a warning, not an error, but your build promotes all warnings to\nerrors, so it breaks your build.</p>\n<p>Although your build specifies <code>-std=c++11</code> in <code>src/3rd_party/spdlog/CMakeLists.txt</code>, which\ngenerates the failure, g++-14 emits <code>Wtemplate-id-cdtor</code> to warn you that the code <em>would be</em>\nillegal under the more recent standard c++20 (and later). Then the warning is made an error.</p>\n<p>The warning is made an error by the compile option <code>-Werror</code>. This option is included in the list\nof compile options <code>ALL_WARNINGS</code>, which is created in the top-level <code>marian/CMakeLists.txt</code>\nat line 227 <em>et seq</em>:</p>\n<pre><code># These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n</code></pre>\n<p>and then applied as compile options for the <code>marian</code> library target in <code>src/CMakeLists.txt</code>\nat line 133:</p>\n<pre><code>target_compile_options(marian PRIVATE ${ALL_WARNINGS})\n</code></pre>\n<p>whence the options are operative for the failing compilation of <code>src/CMakeFiles/marian.dir/common/logging.cpp</code>.</p>\n<p>This failure is a bug in the <code>marian</code> repo which you should <a href=\"https://github.com/marian-nmt/marian/issues\" rel=\"nofollow noreferrer\">report to the maintainers</a>, as\nit does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14.</p>\n<p>Pending a fix, you seem to have three interim options to get your build done. Either:</p>\n<ul>\n<li><p>Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:</p>\n<pre><code>/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\n</code></pre>\n</li>\n</ul>\n<p>e.g. make it <code>registry_t(const registry_t&lt;Mutex&gt;&amp;) = delete;</code> in this occurrence.</p>\n<p>Or:</p>\n<ul>\n<li><p>Locally disable <code>-Wtemplate-id-cdtor</code> at each occurrence, e.g:</p>\n<pre><code>#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored &quot;-Wtemplate-id-cdtor&quot;\nregistry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n#pragma GCC diagnostic pop\n</code></pre>\n</li>\n</ul>\n<p>Or:</p>\n<ul>\n<li>Remove <code>-Werror</code> from the <code>ALL_WARNINGS</code> list in <code>marian/CMakeLists.txt</code> so that <code>Wtemplate-id-cdtor</code> remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status).</li>\n</ul>\n<p>I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.</p>\n"
        ],
        [
         "21",
         "79328514",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )\n</code></pre>\n<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>\n",
         "2025-01-04 08:57:44",
         "2",
         "32",
         "1",
         "<pytorch><nlp><large-language-model><huggingface-trainer>",
         "79328698.0",
         "<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>\n<p>To modify the <strong>data collator</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>class CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])\n        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])\n        return batch\n</code></pre>\n<p>then pass it to <code>Trainer</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n</code></pre>\n<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>\n<p>Hope, it will work!</p>\n"
        ],
        [
         "22",
         "79321551",
         "getting an error: object of type 'float' has no len()",
         "<p>I am a naive in python and started learning python few months ago\nI am working on twitter data in my local, it has 4 columns. ID, brand, sentiment, comment</p>\n<pre><code>def data_clean_pipeline(text):\n    #removing html tags\n    text = str(BeautifulSoup(text).get_text())\n    #removing any nonletter words \n    text = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, text)\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    SW = stopwords.words('english')\n    text = [t for t in text if not t in set(SW)]\n    #Then we can apply stemming and then lemmitize the data\n    SS_stem = SnowballStemmer(language='english')\n    text = [SS_stem.stem(t) for t in text]\n    word_lemmitize = WordNetLemmatizer()\n    text = [word_lemmitize.lemmatize(t) for t in text]\n    return &quot; &quot;.join(text)\n</code></pre>\n<p>When I apply this function to one of the review in twitter data it works, but when i apply to the entire column<br />\nfor example</p>\n<pre><code>data_clean_pipeline(twitter_data['comment'][0] #it works fine and return the output\n</code></pre>\n<p>but the error occurs when i apply this function to a column</p>\n<pre><code>twitter_data['clean'] = twitter_data['comment'].apply(data_clean_pipeline)  \n</code></pre>\n<p>Any feedback would be helpful, thank you:)</p>\n<p>i have attached an image of my error code in the image description for the python error window</p>\n<p><a href=\"https://i.sstatic.net/JpUuncF2.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<p>I was expecting that it will apply the function to the entire comment column which is not happening.<br />\nI have tried to make multiple unsuccessful attempts</p>\n",
         "2025-01-01 12:01:20",
         "0",
         "48",
         "1",
         "<python><python-3.x><twitter><nlp><project>",
         null,
         null
        ],
        [
         "23",
         "79315936",
         "Is n-gram precision the number of elements in the intersection of one hypothesis and possibly many references?",
         "<p>I was trying to understand how BLEU score works and noticed that if I had to compute the n-gram precisions and have multiple reference sentences, it makes more sense to turn everything into sets to remove duplicates. Order in terms of n-grams does not seem to matter, the order of n-tokens is persisted by n-grams but the order each individial n-gram is irrelevant, so we can indeed just use sets to compute n-gram precision of the hypothesis with respect to many references.</p>\n<p>In other words, if I'm given many references and one hypothesis, I compute the n-gram precision by:</p>\n<ol>\n<li>Forming two sets, one set is the union of all n-gram sets of all references, the other set consists of all n-grams in the hypothesis. Duplicates are eliminated.</li>\n<li>Look for the intersection between both sets. The number of n-grams in that intersection divided by the number of n-grams in the hypothesis set is the n-gram precision.</li>\n</ol>\n<p>I think this idea is also backed up by the <a href=\"https://www.nltk.org/_modules/nltk/translate/bleu_score.html\" rel=\"nofollow noreferrer\">nltk implementation</a> of BLEU scores.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    # Extracts all ngrams in hypothesis\n    # Set an empty Counter if hypothesis is empty.\n    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) &gt;= n else Counter()\n    \n    # Extract a union of references' counts.\n    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n    max_counts = {}\n    for reference in references:\n        reference_counts = (\n            Counter(ngrams(reference, n)) if len(reference) &gt;= n else Counter()\n        )\n        for ngram in counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n    # Assigns the intersection between hypothesis and references' counts.\n    clipped_counts = {\n        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n    }\n</code></pre>\n<p><strong>Am I misunderstanding something or this a correct way of computing n-gram precision?</strong></p>\n",
         "2024-12-29 16:25:12",
         "2",
         "52",
         "2",
         "<nlp><nltk><bleu>",
         null,
         null
        ],
        [
         "24",
         "79312133",
         "Getting all leaf words (reverse stemming) into one Python List",
         "<p>On the same lines as the solution provided <a href=\"https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword\">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>\n<p>Imagine I have a shorter sample word list as follows:</p>\n<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n</code></pre>\n<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>\n<pre><code>get_word_forms(&quot;command&quot;)\n</code></pre>\n<p>and get the following output:</p>\n<pre><code>{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n</code></pre>\n<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>\n<p>If I try to reverse-stem the entire list in one go:</p>\n<pre><code>[get_word_forms(word) for word in sample]\n</code></pre>\n<p>I fail at getting any output:</p>\n<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n</code></pre>\n<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>\n<p>something like:</p>\n<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. \n</code></pre>\n",
         "2024-12-27 15:04:05",
         "1",
         "46",
         "1",
         "<python><nlp><nltk>",
         "79312987.0",
         "<p>One solution using nested list comprehensions after stripping forgotten spaces:</p>\n<pre><code>all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']\n</code></pre>\n"
        ],
        [
         "25",
         "79302218",
         "torch.nn.functional.softmax giving inaccurate softmax output",
         "<p>I am trying to implement masked self-attention from scratch but when calculating the softmax for the similarity scores I get odd results. I looked at the documentation and other questions posted on here but I still cant figure out what I am doing wrong. Below is a test I set up with the results.</p>\n<p>What I tried:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(sims)\nprint(torch.nn.functional.softmax(sims, dim=1))\n</code></pre>\n<p>Which gives the following output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n        [ 7.,  8.,  9., 10., 11., 12.],\n        [13., 14., 15., 16., 17., 18.],\n        [19., 20., 21., 22., 23., 24.],\n        [25., 26., 27., 28., 29., 30.],\n        [31., 32., 33., 34., 35., 36.],\n        [37., 38., 39., 40., 41., 42.],\n        [43., 44., 45., 46., 47., 48.],\n        [49., 50., 51., 52., 53., 54.],\n        [55., 56., 57., 58., 59., 60.]])\n\ntensor([[0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337]])\n</code></pre>\n<p>As an example, I am expecting the output of the <code>softmax</code> function on the first row &quot;sims&quot;</p>\n<pre><code>[ 1.,  2.,  3.,  4.,  5.,  6.]\n</code></pre>\n<p>to show</p>\n<pre><code>[ .047,  .095,  .142,  .19,  .238,  .285]\n</code></pre>\n<p>which would be the accurate softmax attention percentages needed to apply to my value tensor</p>\n",
         "2024-12-23 05:18:17",
         "0",
         "51",
         "1",
         "<python><pytorch><nlp><softmax>",
         null,
         null
        ],
        [
         "26",
         "79300156",
         "Why does my contrastive learning model's loss and gradients explode during training?",
         "<p>I am fine-tuning an embedding model using contrastive learning. For the loss function, I’m using <code>torch.nn.CrossEntropyLoss</code>.</p>\n<p>The training process initially seems to work fine — the loss decreases steadily on average. However, at some point during training (usually around step 16,000 in this case), the loss and gradients explode. After this, the model is unable to stabilize, and training becomes unusable.</p>\n<p>Here is a graph showing the behavior:</p>\n<p><a href=\"https://i.sstatic.net/o0g7mnA4.png\" rel=\"nofollow noreferrer\">Tensorboard loss by step graph</a></p>\n<h4>What I have tried so far:</h4>\n<ol>\n<li><p><strong>Data preprocessing</strong>:</p>\n<ul>\n<li><p>Removed outliers (e.g., very long texts).</p>\n</li>\n<li><p>Cleaned and filtered the dataset for consistency.</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Hyperparameter tuning</strong>:</p>\n<ul>\n<li><p>Adjusted the learning rate and tried different values.</p>\n</li>\n<li><p>Changed the optimizer (e.g., switching from Adam to SGD)</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Gradient clipping</strong>:</p>\n<ul>\n<li>Clipped gradients to a max norm of 1 using <code>torch.nn.utils.clip_grad_norm_</code>.</li>\n</ul>\n</li>\n</ol>\n<h4>My setup:</h4>\n<ul>\n<li><p><strong>Dataset size</strong>: ~14,000 samples</p>\n</li>\n<li><p><strong>Model architecture</strong>: Transformer-based embedding model</p>\n</li>\n<li><p><strong>Batch size</strong>: 1 (given my gpu capacity)</p>\n</li>\n<li><p><strong>Learning rate</strong>: 1e-5</p>\n</li>\n<li><p><strong>Optimizer</strong>: Adam with weight decay</p>\n</li>\n</ul>\n<p><strong>Training loop (relevant part):</strong></p>\n<pre><code>for epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    for step, batch in enumerate(dataset_train):\n        temperature = max(0.1, 0.05 * (1 - step / num_training_steps))\n\n        # Move data to device\n        anchor_input_ids = batch[&quot;anchor_input_ids&quot;].to(device)\n        anchor_attention_mask = batch[&quot;anchor_attention_mask&quot;].to(device)\n        positive_input_ids = batch[&quot;positive_input_ids&quot;].to(device)\n        positive_attention_mask = batch[&quot;positive_attention_mask&quot;].to(device)\n        negative_input_ids = batch[&quot;negative_input_ids&quot;].to(device)\n        negative_attention_mask = batch[&quot;negative_attention_mask&quot;].to(device)\n\n        anchor_input_ids = anchor_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        anchor_attention_mask = anchor_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        positive_input_ids = positive_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        positive_attention_mask = positive_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        negative_input_ids = negative_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        negative_attention_mask = negative_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n\n        optimizer.zero_grad()\n\n        # Generate embeddings\n        anchor_embeddings = model.forward(anchor_input_ids, anchor_attention_mask)\n        positive_embeddings = model.forward(positive_input_ids, positive_attention_mask)\n        negative_embeddings = model.forward(negative_input_ids, negative_attention_mask)\n\n        # Calculate cosine similarities\n        pos_sim = cosine_similarity(anchor_embeddings, positive_embeddings)\n        neg_sim = cosine_similarity(anchor_embeddings, negative_embeddings)\n\n        # Calculate logits\n        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim.unsqueeze(1)], dim=1) / temperature\n        labels = torch.zeros(logits.size(0), dtype=torch.long).to(device)  # The positive class is always the first\n\n        # Calculate InfoNCE loss\n        loss = torch.nn.CrossEntropyLoss()(logits, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n</code></pre>\n<p><strong>Dataset generation:</strong></p>\n<pre><code>import torch\nfrom torch.utils.data import Dataset\nimport random\n\nclass FineTuneContrastiveDataset(Dataset):\n    def __init__(self, pairs_data, df_1, df_2, tokenizer, max_tokens=512):\n        &quot;&quot;&quot;\n        pairs_data: List of tuples (id_1, id_2, label).\n        df_1: DataFrame containing text data associated with id_1.\n        df_2: DataFrame containing text data associated with id_2.\n        tokenizer: Hugging Face tokenizer.\n        max_tokens: Maximum allowed length for the tokenized text.\n        &quot;&quot;&quot;\n        self.pairs_data = pairs_data\n        self.df_1 = df_1.set_index(&quot;id_1&quot;)\n        self.df_2 = df_2.set_index(&quot;id_2&quot;)\n        self.tokenizer = tokenizer\n        self.max_tokens = max_tokens\n        self.id_2_list = list(self.df_2.index)  # For selecting negative samples\n\n    def __len__(self):\n        return len(self.pairs_data)\n\n    def __getitem__(self, idx):\n        # Retrieve data from the pair\n        id_1, id_2_positive, label = self.pairs_data[idx]\n        \n        # Text associated with id_1 (anchor)\n        text_1 = &quot; &quot;.join(self.df_1.loc[id_1][&quot;chunks&quot;])\n\n        # Positive text associated with id_2\n        text_2_positive = &quot; &quot;.join(self.df_2.loc[id_2_positive][&quot;chunks&quot;])\n\n        # Generate a negative sample from id_2\n        id_2_negative = random.choice(\n            [candidate_id for candidate_id in self.id_2_list if candidate_id != id_2_positive]\n        )\n        text_2_negative = &quot; &quot;.join(self.df_2.loc[id_2_negative][&quot;chunks&quot;])\n\n        # Tokenize inputs\n        inputs_anchor = self.tokenizer(\n            text_1, truncation=True, max_length=self.max_tokens, \n            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;\n        )\n        inputs_positive = self.tokenizer(\n            text_2_positive, truncation=True, max_length=self.max_tokens, \n            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;\n        )\n        inputs_negative = self.tokenizer(\n            text_2_negative, truncation=True, max_length=self.max_tokens, \n            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;\n        )\n\n        return {\n            &quot;anchor_input_ids&quot;: inputs_anchor[&quot;input_ids&quot;].squeeze(0),\n            &quot;anchor_attention_mask&quot;: inputs_anchor[&quot;attention_mask&quot;].squeeze(0),\n            &quot;positive_input_ids&quot;: inputs_positive[&quot;input_ids&quot;].squeeze(0),\n            &quot;positive_attention_mask&quot;: inputs_positive[&quot;attention_mask&quot;].squeeze(0),\n            &quot;negative_input_ids&quot;: inputs_negative[&quot;input_ids&quot;].squeeze(0),\n            &quot;negative_attention_mask&quot;: inputs_negative[&quot;attention_mask&quot;].squeeze(0),\n            &quot;label&quot;: torch.tensor(label, dtype=torch.float),\n            &quot;id_1&quot;: id_1,\n        }\n</code></pre>\n",
         "2024-12-21 21:46:25",
         "1",
         "72",
         "1",
         "<deep-learning><pytorch><nlp><word-embedding>",
         null,
         null
        ],
        [
         "27",
         "79298368",
         "Inspect all probabilities of BERTopic model",
         "<p>Say I build a BERTopic model using</p>\n<pre><code>from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre>\n<p>Inspecting <code>probs</code> gives me just a single value for each item in <code>docs</code>.</p>\n<pre><code>probs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n</code></pre>\n<p>I would like the entire probability vector across all topics (so in this case, where <code>nr_topics=20</code>, I want a vector of 20 probabilities for each item in <code>docs</code>). In other words, if I have N items in <code>docs</code> and K topics, I would like an NxK output.</p>\n",
         "2024-12-20 20:49:34",
         "1",
         "41",
         "1",
         "<python><nlp><topic-modeling>",
         "79299703.0",
         "<p>For individual topic probability across each document you need to add one more argument.</p>\n<pre><code>topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n</code></pre>\n<p>Note: This calculate_probabilities = True will only work if you are using <strong><code>HDBSCAN</code></strong> clustering embedding model. And Bertopic by default uses <code>all-MiniLM-L6-v2</code>.</p>\n<p><strong>Official documentation:</strong> <a href=\"https://maartengr.github.io/BERTopic/api/bertopic.html\" rel=\"nofollow noreferrer\">https://maartengr.github.io/BERTopic/api/bertopic.html</a></p>\n<p>They have mentioned the same in document as well.</p>\n"
        ],
        [
         "28",
         "79293919",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>\n<p>For example:\nA sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>\n<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>\n<p>Any and all help is welcome and appreciated.</p>\n<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>\n",
         "2024-12-19 10:24:04",
         "0",
         "62",
         "2",
         "<python><nlp><nltk><detection>",
         "79294074.0",
         "<p>You need a external dataset for this task. You can try dataset such as google n gram dataset.</p>\n<p>Here is the breakdown of the problem statement:</p>\n<ol>\n<li>Input: &quot;I enjoy a cold glass of water on a hot day&quot;. <code>Output</code>: &quot;water&quot;.</li>\n<li>Split the sentences into words list.</li>\n</ol>\n<blockquote>\n<p>Example: [&quot;I&quot;, &quot;enjoy&quot;, &quot;a&quot;, &quot;cold&quot;, &quot;glass&quot;, &quot;of&quot;, &quot;water&quot;, &quot;on&quot;,\n&quot;a&quot;, &quot;hot&quot;, &quot;day&quot;]</p>\n</blockquote>\n<ol start=\"3\">\n<li>First loop in through all the word of the sentences. so let say you are at first word &quot;I&quot;.</li>\n<li>Now you will look the same word &quot;I&quot; in external dataset and will look for the frequency of that word.\nLet say the word &quot;I&quot; in external dataset is repeated <code>5000000</code> times</li>\n<li>Repeat this task for all the word.</li>\n<li>Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.\nFrequency in the below example is random value not exact value.</li>\n</ol>\n<blockquote>\n<pre><code>{\n    &quot;I&quot;: 5000000,\n    &quot;enjoy&quot;: 50000,\n    &quot;a&quot;: 10000000,\n    &quot;cold&quot;: 30000,\n    &quot;glass&quot;: 100000,\n    &quot;of&quot;: 8000000,\n    &quot;water&quot;: 1200000,\n    &quot;on&quot;: 6000000,\n    &quot;hot&quot;: 700000,\n    &quot;day&quot;: 400000\n}\n</code></pre>\n</blockquote>\n<ol start=\"7\">\n<li>Pick the word with highest frequency.</li>\n</ol>\n<p>Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself</p>\n"
        ],
        [
         "29",
         "79293889",
         "catelog sentences into 5 words that represent them",
         "<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>\n<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>\n<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>\n<p>I will glad for recomendations how to do that</p>\n<p><strong>edit</strong></p>\n<p>represnt = the semantic distance between the word to the text.</p>\n<p>for example -\nlets say in row 1 the text is &quot;i want to eat&quot;\nand I have 2 words : food and house.</p>\n<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>\n",
         "2024-12-19 10:16:47",
         "0",
         "53",
         "1",
         "<python><pandas><nlp><text-mining><similarity>",
         "79294099.0",
         "<p>You could use a pre-trained sentence transformer model from <a href=\"https://pypi.org/project/sentence-transformers/\" rel=\"nofollow noreferrer\"><code>sentence_transformers</code></a>:</p>\n<pre><code>import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -&gt; None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -&gt; list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -&gt; pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838\n</code></pre>\n"
        ],
        [
         "30",
         "79287799",
         "How to correctly identify entity types for tokens using spaCy using python?",
         "<p>I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this.</p>\n<p>Here is the code I am using:</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\ndef getPayeeName(description):\n    description = description.replace(&quot;-&quot;, &quot; &quot;).replace(&quot;/&quot;, &quot; &quot;).strip()\n    doc = nlp(description)\n\n    for token in doc:\n        print(f&quot;Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else 'None'}&quot;)\n\n# Example input\ndescription = &quot;UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 MEDICAL LTD HDFC 50200&quot;\ngetPayeeName(description)\n</code></pre>\n<p>Token: UPI, Entity: ORG</p>\n<p>Token: DR, Entity: ORG</p>\n<p>Token: 400874707203, Entity: None</p>\n<p>Token: BENGALORE, Entity: None</p>\n<p>Token: 08, Entity: DATE</p>\n<p>Token: JAN, Entity: DATE</p>\n<p>Token: 2024, Entity: DATE</p>\n<p>Token: 14:38:56, Entity: None</p>\n<p>Token: MEDICAL, Entity: ORG</p>\n<p>Token: LTD, Entity: ORG</p>\n<p>Token: HDFC, Entity: ORG</p>\n<p>Token: 50200, Entity: ORG</p>\n<ul>\n<li><p>50200 is identified as ORG, but it is just a number.</p>\n</li>\n<li><p>BENGALORE is a city, but it is not recognized as a GPE or location\n(returns None).</p>\n</li>\n<li><p>UPI and DR are acronyms/abbreviations, but they are incorrectly\nidentified as ORG.</p>\n</li>\n</ul>\n<p>I want the entity recognition to be more accurate and reliable.\nHow can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition?</p>\n<p>Note: I tried ChatGPT as well, but still this issue is not solved.</p>\n",
         "2024-12-17 12:09:49",
         "1",
         "44",
         "1",
         "<python><machine-learning><nlp><spacy>",
         null,
         null
        ],
        [
         "31",
         "79283846",
         "--user-dir in Fairseq in failing",
         "<p>I’m trying to fine-tune the IndicTrans2 model using fairseq-train, but I keep encountering the following error:</p>\n<p>fairseq-train: error: argument --user-dir: invalid Optional value: 'C:/Users/sasid/Downloads/en-indic-exp/model_configs'</p>\n<p>I’ve provided the --user-dir argument as the path to the model_configs directory (e.g., --user-dir C:/Users/sasid/Downloads/IndicTrans2/model_configs), but the training script fails with the above error.</p>\n<p>I figured out that the problem is it's not taking path in windows convention with backslashes instead it is taking forward slashes so it is failing with that error.</p>\n<p>So how can I make it take the path in windows convention?</p>\n",
         "2024-12-16 07:03:15",
         "1",
         "30",
         "1",
         "<python><nlp><huggingface-transformers><large-language-model><fairseq>",
         null,
         null
        ],
        [
         "32",
         "79279045",
         "Recommending a pre-train NER model for geospatial entities",
         "<p>I am trying to find the best pre-trained Hugging Face Transformer model exclusively dedicated to geospatial or location entities to extract location entities in English from a text. Does it work way better than roberta-large?</p>\n",
         "2024-12-13 16:53:40",
         "1",
         "34",
         "1",
         "<nlp><geolocation><pipeline><geospatial><huggingface-transformers>",
         null,
         null
        ],
        [
         "33",
         "79274184",
         "How to split and spelling correct arabic text without spaces into list of words",
         "<p>I'm looking for a way to split the Arabic text and correct the spelling. Whitespace is the first splitting criterion, then, Maybe based on a dictionary of correct words the splitting should done, considering spelling issues:</p>\n<pre><code>تشرابالقطط الحليب =&gt; [تشرب، القطط، الحليب]\n\nمخمديوسف =&gt; [&quot;محمد&quot;, &quot;يوسف&quot;]\n\nانا ارييدان اشراب =&gt; [&quot;انا&quot;, &quot;أريد&quot;, &quot;أن&quot;, &quot;أشرب&quot;]\n\nجملة صحيحة =&gt; [&quot;جملة&quot;, &quot;صحيحة&quot;]\n</code></pre>\n<p>And if there are multiple correct splitting ways, return them all:</p>\n<pre><code>مخمديوسف =&gt; [ [&quot;محمد&quot;, &quot;يوسف&quot;] , [&quot;احمد&quot;, &quot;يوسف&quot;] ]\n</code></pre>\n<p>If any libraries can do the same. Otherwise, A custom algorithm/code that we can implement?</p>\n",
         "2024-12-12 07:34:49",
         "3",
         "69",
         "2",
         "<algorithm><deep-learning><nlp>",
         null,
         null
        ],
        [
         "34",
         "79264247",
         "similarity from word to sentence after doing words Embedding",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did word2vec .</p>\n<p>Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word &quot;king&quot;.</p>\n<p>I thought about taking in each sentence the 4 closet words to the word king  and make average of them.\nmaybe by using <code>model.wv.similarity</code>.\nthe avg of each sentnce will be in the field df['king']</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n<p>example data:</p>\n<pre><code>    data = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\ndf['text']=df['text'].str.split()    \n\nmodel = Word2Vec(df['text'], vector_size=100, window=2, min_count=1 )\n\nmodel.wv.similarity('Knights','king')\n</code></pre>\n<p><strong>edit</strong>:</p>\n<p>My mission is:</p>\n<p>I have 1000 text rows (people that complain about something)\nI want to catalog them into 4 words.\nLets say that word 1 is king. Word 2 is castle…\nI want to know about each sentence which word from the  4 words most represent the sentence.\nIn order to do that I thought about taking each word from the 4 words and calculate <code>model.wv.similarity</code> to all of the words in  df['text'].\nAfter that, for each sentence, take the 3  words that have  the highest score to word king  (and to the word  castle and ets..)  .\ncalculate mean of the 3 highest score and that would be the value of df['king'] for the sentence</p>\n",
         "2024-12-09 08:14:04",
         "0",
         "64",
         "1",
         "<python><nlp><text-mining><word2vec><similarity>",
         null,
         null
        ],
        [
         "35",
         "79260748",
         "How to install spacy?",
         "<p>I am using trying to install spacy library using 'pip install -U spacy' in the command prompt (run as admin) in Windows-11 O.S., but it shows some error I don't understand. I am using Python 3.13.0, gcc 13.2.0 and make 4.4.1. What could be the problem? Or is there any other way to install spacy?</p>\n<pre class=\"lang-none prettyprint-override\"><code>C:\\&gt;pip install -U spacy\nCollecting spacy\n  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n\n  × pip subprocess to install build dependencies did not run successfully.\n  │ exit code: 1\n  ╰─&gt; [113 lines of output]\n      Ignoring numpy: markers 'python_version &lt; &quot;3.9&quot;' don't match your environment\n      Collecting setuptools\n        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n      Collecting cython&lt;3.0,&gt;=0.25\n        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n      Collecting cymem&lt;2.1.0,&gt;=2.0.2\n        Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n      Collecting preshed&lt;3.1.0,&gt;=3.0.2\n        Using cached preshed-3.0.9.tar.gz (14 kB)\n        Installing build dependencies: started\n        Installing build dependencies: finished with status 'done'\n        Getting requirements to build wheel: started\n        Getting requirements to build wheel: finished with status 'done'\n        Preparing metadata (pyproject.toml): started\n        Preparing metadata (pyproject.toml): finished with status 'done'\n      Collecting murmurhash&lt;1.1.0,&gt;=0.28.0\n        Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n      Collecting thinc&lt;8.4.0,&gt;=8.3.0\n        Using cached thinc-8.3.2.tar.gz (193 kB)\n        Installing build dependencies: started\n        Installing build dependencies: still running...\n        Installing build dependencies: finished with status 'error'\n        error: subprocess-exited-with-error\n\n        pip subprocess to install build dependencies did not run successfully.\n        exit code: 1\n\n        [74 lines of output]\n        Ignoring numpy: markers 'python_version &lt; &quot;3.9&quot;' don't match your environment\n        Collecting setuptools\n          Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n        Collecting cython&lt;3.0,&gt;=0.25\n          Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n        Collecting murmurhash&lt;1.1.0,&gt;=1.0.2\n          Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n        Collecting cymem&lt;2.1.0,&gt;=2.0.2\n          Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n        Collecting preshed&lt;3.1.0,&gt;=3.0.2\n          Using cached preshed-3.0.9.tar.gz (14 kB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting blis&lt;1.1.0,&gt;=1.0.0\n          Using cached blis-1.0.1.tar.gz (3.6 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting numpy&lt;2.1.0,&gt;=2.0.0\n          Using cached numpy-2.0.2.tar.gz (18.9 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Installing backend dependencies: started\n          Installing backend dependencies: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'error'\n          error: subprocess-exited-with-error\n\n          Preparing metadata (pyproject.toml) did not run successfully.\n          exit code: 1\n\n          [22 lines of output]\n          + C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\vendored-meson\\meson\\meson.py setup C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-python-native-file.ini\n          The Meson build system\n          Version: 1.4.99\n          Source dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\n          Build dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\n          Build type: native build\n          Project name: NumPy\n          Project version: 2.0.2\n          C compiler for the host machine: gcc (gcc 13.2.0 &quot;gcc (GCC) 13.2.0&quot;)\n          C linker for the host machine: gcc ld.bfd 2.41\n          C++ compiler for the host machine: c++ (gcc 6.3.0 &quot;c++ (MinGW.org GCC-6.3.0-1) 6.3.0&quot;)\n          C++ linker for the host machine: c++ ld.bfd 2.28\n          Cython compiler for the host machine: cython (cython 3.0.11)\n          Host machine cpu family: x86\n          Host machine cpu: x86\n          Program python found: YES (C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe)\n          Need python for x86, but found x86_64\n          Run-time dependency python found: NO (tried sysconfig)\n\n          ..\\meson.build:41:12: ERROR: Python dependency not found\n\n          A full log can be found at C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-logs\\meson-log.txt\n          [end of output]\n\n          note: This error originates from a subprocess, and is likely not a problem with pip.\n        error: metadata-generation-failed\n\n        Encountered error while generating package metadata.\n\n        See above for output.\n\n        note: This is an issue with the package mentioned above, not pip.\n        hint: See above for details.\n        [end of output]\n\n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: subprocess-exited-with-error\n\n      pip subprocess to install build dependencies did not run successfully.\n      exit code: 1\n\n      See above for output.\n\n      note: This error originates from a subprocess, and is likely not a problem with pip.\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n× pip subprocess to install build dependencies did not run successfully.\n│ exit code: 1\n╰─&gt; See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n</code></pre>\n",
         "2024-12-07 14:07:54",
         "0",
         "718",
         "4",
         "<python><pip><nlp><spacy>",
         null,
         null
        ],
        [
         "36",
         "79256112",
         "How to log only the current script file to W&B code panel immediately?",
         "<p>How can I ensure that only the current script file (e.g., <code>train.py</code>) is logged to the W&amp;B Code panel when running a script, without logging the entire directory?</p>\n<p>Currently, I'm using:</p>\n<pre class=\"lang-py prettyprint-override\"><code>wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;)\n</code></pre>\n<p>I want to confirm if this approach works reliably across different environments and if there are better practices for this use case.</p>\n<p>Main part of the code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;) # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)\n    print(f'{run.get_url()=}')\n    wandb.finish()\n</code></pre>\n<p>All the code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\nfrom typing import Optional\nimport random\nimport torch\nfrom transformers import PushToHubCallback\nfrom transformers import get_cosine_schedule_with_warmup\nfrom trl import SFTConfig, SFTTrainer\nimport os\nimport fire\nimport wandb\nimport sys\n\nfrom train.callbacks import GenCallbackWithHFGenerate\nfrom train.data import load_math_style_dataset, print_first_example_after_decode\nimport train.models\n\nfrom train.utils import seed_everything\n\ndef main(**config):\n    # -- Seed everything\n    seed_everything(seed=config.get('seed', 0))\n    \n    # -- HF login\n    from huggingface_hub import login\n    token = open(os.path.expanduser(&quot;~/keys/master_hf_token.txt&quot;)).read().strip()\n    login(token=token)\n\n    # -- Get model\n    model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'google/gemma-2-2b')) \n    # model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'meta-llama/Llama-3.1-8B')) \n\n    # -- Load datasets\n    ds_name_or_path = config.get('ds_name_or_path', 'Putnam-AXIOM/putnam-axiom-dataset')\n    train_split, val_split = config.get('train_split', 'func_original_53_10_30_2024'), config.get('val_split', 'func_variations_265_11_23_2024')\n    print(f'\\n---&gt; {ds_name_or_path=} {train_split=} {val_split=}\\n')\n    train_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=1, split=train_split)\n    print_first_example_after_decode(train_dataset, tok)\n    # eval_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=15, split=val_split)\n    eval_dataset = train_dataset\n    print(f'{len(train_dataset)=}\\n{len(eval_dataset)=}')\n    wandb.config.update({'dataset': f'{ds_name_or_path} ({train_split=} {val_split=})'})\n\n    # -- Prepare output directory\n    today: str = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss')\n    output_dir: str = os.path.expanduser(f&quot;~/data/runs_logic_cont/run_{config.get('today', today)}&quot;)\n    print(f'{output_dir=}')\n    \n    # Save the initial model and tokenizer as checkpoint-0\n    initial_checkpoint_dir = os.path.join(output_dir, &quot;checkpoint-0&quot;)\n    os.makedirs(initial_checkpoint_dir, exist_ok=True)\n    print(f&quot;Saving initial checkpoint and tokenizer at {initial_checkpoint_dir}&quot;)\n    model.save_pretrained(initial_checkpoint_dir)\n    tok.save_pretrained(initial_checkpoint_dir)\n\n    # -- Train model\n    # max_steps = 50  # Limit fine-tuning to a few steps\n    # os.environ['CUDA_VISIBLE_DEVICES'] = str(random.randint(0, 7))\n    # config = {'max_steps': 2, 'eval_steps': 1, 'logging_steps': 1, \n    #           'save_strategy': 'steps', 'save_steps': 1, 'eval_strategy': 'steps'}\n    # config = config | {'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'maybe 0')}\n    training_args = SFTConfig(\n        max_steps=config.get('max_steps', 30),\n        # --\n        output_dir=output_dir,\n        bf16=torch.cuda.is_bf16_supported(),\n        fp16=not torch.cuda.is_bf16_supported(),\n        # -- logging opts\n        save_steps=config.get('save_steps', 5), \n        save_strategy=config.get('save_strategy', 'steps'),\n        eval_on_start=config.get('eval_on_start', True),\n        evaluation_strategy=config.get('eval_strategy', 'steps'), \n        eval_steps=config.get('eval_steps', 1), \n        logging_first_step=config.get('logging_first_step', True), # Default to False, unsure 100% what this does but looks like a good idea\n        logging_strategy=config.get('logging_strategy', 'steps'),\n        logging_steps=config.get('logging_steps', 1),\n        # --\n        num_train_epochs=config.get('num_train_epochs', 10),\n        max_seq_length=config.get('max_seq_length', 512),\n        per_device_train_batch_size=config.get('batch_size', 2),\n        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 2),\n    )\n    # Calculate Total Steps\n    steps_per_epoch = (len(train_dataset) // training_args.per_device_train_batch_size) // training_args.gradient_accumulation_steps\n    total_steps = steps_per_epoch * training_args.num_train_epochs\n    print(f'{steps_per_epoch=}')\n\n    # Optimizer and Scheduler\n    # optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 1e-4}]\n    optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.get('learning_rate', 1e-5))\n\n    # Add Cosine Learning Rate Scheduler\n    # warmup_steps = int(0.01 * total_steps)  # Warm-up for 1% of total steps\n    warmup_steps = 0\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps,\n    )\n    scheduler = None\n    print(f'{total_steps=} {warmup_steps=}')\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tok,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        args=training_args,\n        optimizers=(optimizer, scheduler),\n        callbacks=[GenCallbackWithHFGenerate(model, tok)]\n    )\n    print(f&quot;\\nStarting fine-tuning...&quot;)\n    trainer.train()\n    # - end run\n    return os.path.expanduser(output_dir)\n\ndef run_eval_logic_contamination(output_dir: str):\n    &quot;&quot;&quot;\n    Runs the eval_logic_contamination.py script with the specified output directory.\n\n    Args:\n        output_dir (str): The directory where the model is saved, expanded using `os.path.expanduser`.\n    &quot;&quot;&quot;\n    import gc\n    torch.cuda.empty_cache()\n    gc.collect()\n    output_dir = os.path.expanduser(output_dir)  # Ensure `output_dir` is expanded \n    from eval_logic_contamination import main\n    task='putnam_axiom_53'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # task='putnam_axiom_53' # for debugging\n    task='putnam_axiom_variations'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # wandb.run.define_metric(&quot;eval/accuracy&quot;, step_metric=&quot;eval/checkpoint_idx&quot;)\n    # wandb.run.define_metric(&quot;eval/checkpoint_idx&quot;) \n    # for idx, acc in [(10,5), (20,10), (30,15)]:\n    #     wandb.log({'eval/accuracy': acc, 'eval/checkpoint_idx': idx})\n\ndef _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;) # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)\n    print(f'{run.get_url()=}')\n    wandb.finish()\n\nif __name__ == &quot;__main__&quot;:\n    import time\n    start_time = time.time()\n    fire.Fire(_main)\n    print(f&quot;Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\\a&quot;)\n</code></pre>\n<p>Cross: <a href=\"https://community.wandb.ai/t/how-to-log-only-the-current-script-file-to-w-b-code-panel-immediately/8537\" rel=\"nofollow noreferrer\">https://community.wandb.ai/t/how-to-log-only-the-current-script-file-to-w-b-code-panel-immediately/8537</a></p>\n",
         "2024-12-05 20:11:21",
         "0",
         "18",
         "1",
         "<deep-learning><nlp><wandb>",
         null,
         null
        ],
        [
         "37",
         "79253283",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>\n<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>\n<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n</code></pre>\n<p>A small Example -</p>\n<pre><code>text_dict = {\n    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]\n}\n\ndf = pd.DataFrame(text_dict)\n</code></pre>\n<p>My expected outcome is -</p>\n<pre><code>                 ITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n</code></pre>\n<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>\n<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>\n",
         "2024-12-05 03:05:06",
         "0",
         "81",
         "1",
         "<python><pandas><nlp>",
         "79263000.0",
         "<p>You need to process each row of text by identifying occurrences of <code>key_words</code> and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique <code>search_word</code> found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique <code>search_words</code> near <code>key_words</code>.</p>\n<pre><code>import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        &quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process &quot;\n        &quot;of adopting Symantec. We believe these technologies will improve our access control. &quot;\n        &quot;Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = &quot; &quot;.join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n</code></pre>\n<p>Which returns</p>\n<pre><code>                                               ITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2\n</code></pre>\n"
        ],
        [
         "38",
         "79247672",
         "Error in getting Captum text explanations for text classification",
         "<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>\n<pre><code>import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f&quot;Test Accuracy: {accuracy:.2f}&quot;)\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n<p>But I am getting the following error in running the above.</p>\n<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n</code></pre>\n",
         "2024-12-03 12:47:45",
         "2",
         "73",
         "1",
         "<machine-learning><pytorch><nlp><huggingface-transformers><text-classification>",
         "79248379.0",
         "<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>\n<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=\"https://captum.ai/tutorials/Bert_SQUAD_Interpret\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>\n<p>Below please find snippet that works:</p>\n<pre><code>from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n"
        ],
        [
         "39",
         "79247594",
         "euclidian distance from word to sentence after doing Vectorizer",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did TfidfVectorizer.</p>\n<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>\n<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n",
         "2024-12-03 12:25:05",
         "1",
         "43",
         "1",
         "<pandas><dataframe><nlp><text-classification><tf-idf>",
         "79248087.0",
         "<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([&quot;king&quot;]).toarray()\nexcept KeyError:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n</code></pre>\n<p>which would give you</p>\n<pre><code>                                                text  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0  \n</code></pre>\n<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif &quot;king&quot; in feature_names:\n    king_index = np.where(feature_names == &quot;king&quot;)[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n\n</code></pre>\n<p>which gives</p>\n<pre><code>                                                text  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214\n</code></pre>\n"
        ],
        [
         "40",
         "79234004",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "<p>I want to use <code>Llama-3.2-1B-Instruct</code> model, and although I have set <code>&quot;temperature&quot;: 0.0, &quot;top_p&quot;:0.0 and &quot;top_k&quot;:0</code>, it still generates inconsistent output. This is how my pipeline looks like:</p>\n<pre><code>pipe = pipeline(\n    &quot;text-generation&quot;,\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=&quot;mps&quot;,\n        model_kwargs={&quot;temperature&quot;: 0.0,\n                  &quot;do_sample&quot;:True,\n                              &quot;top_p&quot;:0.0,\n                              &quot;top_k&quot;:0,},\n)\n</code></pre>\n<p>Any idea how to solve this issue?</p>\n",
         "2024-11-28 13:02:37",
         "1",
         "532",
         "2",
         "<python><nlp><huggingface-transformers><large-language-model>",
         "79246602.0",
         "<p>The model inconsistent output can be due to two main factors:</p>\n<p><strong>1. Temperature:</strong></p>\n<p>setting temperature to zero give more inconsistent result. You can refer <a href=\"https://community.openai.com/t/why-the-api-output-is-inconsistent-even-after-the-temperature-is-set-to-0/329541/2\" rel=\"nofollow noreferrer\">Opeani discussion page</a> for detail.</p>\n<p>So the best option is to set temperature to very low values such as 0.00001 instead of zero.</p>\n<p><strong>2. do_sample</strong></p>\n<p>You already set it false, and it should remain that way only.</p>\n"
        ],
        [
         "41",
         "79227390",
         "How to extract specific entities from unstructured text",
         "<p>Given a generic text sentence (in a specific context) how can I extract word/entities of interest belonging to a specific &quot;category&quot; using python and any NLP library?</p>\n<p>For example given a step for a culinary recipe <code>Add an onion to a bowl of carrots</code> as input text, I'd like to retrive <code>onion</code> and <code>carrots</code> while given <code>Sprinkle with paprika.</code> should return <code>paprika</code>.\nBut this should also work with sentences like <code>stir well, and cook an additional minute.</code> that do not contain any food entity in them.</p>\n<p>So far, what I was able to achieve is using the <code>spacy</code> library for training a NER module to parse sentences. The problem with the NER pipeline is that it is a rule-based parsing, it is trained providing a set of sentences and entities/matches/labels to learn, which works fine as expeted on sentences similar to the one used during train, but performs bad on new and different sentences:</p>\n<pre class=\"lang-py prettyprint-override\"><code>nlp = spacy.load('trained_model')\n\ndocument = nlp('Add flour, mustard, and salt')\n[(ent.text, ent.label_) for ent in document.ents]\n# &gt;&gt; [('Add flour', 'FOOD'), ('mustard', 'FOOD'), ('salt', 'FOOD')]\n# (quite) correct output\n\ndocument = nlp('I took a building, car and squirrel on the weekend')\n[(ent.text, ent.label_) for ent in document.ents]\n# &gt;&gt; [('building', 'FOOD'), ('car', 'FOOD'), ('squirrel', 'FOOD')]\n# wrong output\n\ndocument = nlp('stir well, and cook an additional minute.')\n[(ent.text, ent.label_) for ent in document.ents]\n# &gt;&gt; [('stir well', 'FOOD'), ('cook', 'FOOD'), ('additional minute.', 'FOOD')]\n# wrong output\n</code></pre>\n<p>I am aware that there are several similar questions and posts, but I have found only solutions working for &quot;semi-structured&quot; text, i.e. list of ingredients as <code>1 tsp. of sugar, 1 cup of milk, ...</code> which can be easily solved using the previous rule-based approach. Also <code>nltk</code> and part-of-speech (POS) are an option, but I'd prefer an alternative solution rather than having to compare each noun with an exhaustive list of foods.</p>\n<p>What instead I am looking for is a way of to extract specific entities or at least to classify words in generic text with additional categories beyond those of the basic parsing.\nWhich methods should I use/look at to achieve this?</p>\n",
         "2024-11-26 15:46:48",
         "1",
         "92",
         "1",
         "<python><machine-learning><nlp><nltk><spacy>",
         null,
         null
        ],
        [
         "42",
         "79202614",
         "Understanding byte-pair encoding tokenization for Greek characters",
         "<p>I am trying to train a new tokenizer with Greek text to later add the new tokens into the Llama 3.1 tokenizer using</p>\n<pre class=\"lang-py prettyprint-override\"><code>tokenizer.add_tokens(list(new_tokens)).\n</code></pre>\n<p>However, upon training the byte-pair encoding tokenizer on Greek and Spanish text, the result looks something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>\\['Translate', 'Ġfrom', 'ĠGreek', 'Ġto', 'ĠSpanish', ':', 'ĠÎĿÎ±', 'ĠÎŃÏĥÎ¹', 'ĠÎ¿Î³Î¯', 'ĠÎ³ÎŃÏģÎ¿Ïħ'\\]\n</code></pre>\n<p>When extending the token vocabulary in the tokenizer, it seems that those encoded tokens are being passed literally, not as encodings of Greek characters, and they are not recognized by the tokenizer to encode a sentence. However, when using the same method and new tokens are hardcoded, such as in</p>\n<pre class=\"lang-py prettyprint-override\"><code>extender_tokenizer.add_tokens(['Αυτό', 'είναι'])\n</code></pre>\n<p>it does work.</p>\n<p>I assume this is an encoding issue or it is related to BPE inner workings.\nWhy are Greek characters shown that way? Is it related to encoding, BPE or both? How to obtain a list of Greek character tokens that can be added to the tokenizer?</p>\n<p>Reference code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n\ntokenizer = Tokenizer(models.BPE())\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\ntrainer = trainers.BpeTrainer(vocab_size = 2000, min_frequency = 3, show_progress = True)\ntokenizer.train_from_iterator(training_corpus, trainer = trainer)\n</code></pre>\n",
         "2024-11-19 08:31:26",
         "0",
         "74",
         "1",
         "<encoding><nlp><tokenize><byte-pair-encoding>",
         null,
         null
        ],
        [
         "43",
         "79192130",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>\n<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>\n",
         "2024-11-15 11:13:36",
         "1",
         "52",
         "2",
         "<python><amazon-web-services><aws-lambda><nlp><large-language-model>",
         "79192427.0",
         "<p>In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:</p>\n<ol>\n<li>Package your KeyBERT script in a lambda and easily deploy it with a container.</li>\n<li>Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.</li>\n</ol>\n"
        ],
        [
         "44",
         "79190601",
         "Pyspark sentiment analysis invalid output",
         "<p>I am trying to perform sentiment analysis for a use case. Most of the time, it is giving correct results, but in some cases, even positive comments are being marked as negative. How can I fix my code to achieve better accuracy?</p>\n<p>My code</p>\n<pre><code>from pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import StringType\nfrom transformers import pipeline\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Define the filter_stopwords function\ndef filter_stopwords(sentence):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(sentence)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n    return &quot; &quot;.join(filtered_sentence)\n\n# Initialize the sentiment analysis pipeline with a different model\nsentiment_pipeline = pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)\n\n# Define a function to get sentiment using the pipeline\ndef get_sentiment(text):\n    filtered_text = filter_stopwords(text)\n    result = sentiment_pipeline(filtered_text)[0]\n    return result['label'].lower()  # returns 'positive', 'negative', etc.\n\n# Register the function as a UDF\nsentiment_udf = udf(get_sentiment, StringType())\n\n# df = df.withColumn(&quot;sentiment&quot;, sentiment_udf(col(&quot;text_column&quot;)))\n</code></pre>\n<p>Input data</p>\n<ol>\n<li><p>Didn't get it right the first 2 times but when it was fixed it was fixed well.</p>\n</li>\n<li><p>The Response time was grate -- <strong>Note</strong> Looks like spelling mistake but his review is positive</p>\n</li>\n<li><p>The initial agent contact could not resolve my issue but escalated it quickly to someone who could.</p>\n</li>\n</ol>\n<p>for this inputs i am expecting all should be <strong>positive</strong> instead i am getting <strong>negative</strong></p>\n",
         "2024-11-14 22:24:54",
         "3",
         "60",
         "2",
         "<pyspark><nlp><nltk><huggingface-transformers>",
         null,
         null
        ],
        [
         "45",
         "79180131",
         "How to parse a resume with few shot method using the specified models from HuggingFace and Langchain?",
         "<h1>Model selection confusion and some errors while trying to parse a resume with the following codes</h1>\n<ul>\n<li>Trying to do a few shot prompting with a google flan t5 base model</li>\n<li>While Doing so I am getting an error\n<code>ERROR:Service.service:Error parsing resume: &quot;'ContactInformation'&quot;</code></li>\n<li>exception as <code>&quot;detail&quot;: &quot;Failed to parse the file: An error occurred in parsed_resume: \\&quot;'ContactInformation'\\&quot;&quot;</code></li>\n</ul>\n<h2>The code is given below</h2>\n<pre class=\"lang-none prettyprint-override\"><code>from typing import List, Optional, Union, Any, re\nimport json\n\n\n\nclass ContactInformation(BaseModel):\n    Name: Optional[str] = None\n    Email: Optional[str] = None\n    Contact: Optional[str] = None\n    Links: Optional[List[str]] = None\n\n\n\nclass Experience(BaseModel):\n    title: Optional[str] = None\n    company: Optional[str] = None\n    duration: Optional[str] = None\n\n\nclass Education(BaseModel):\n    course: Optional[str] = None\n    branch: Optional[str] = None\n    institute: Optional[str] = None\n\n\nclass Projects(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n    link: Optional[str] = None\n\nclass OutputFormat(BaseModel):\n    ContactInformation: Optional[Any] = None\n    AboutMe: Optional[Any] = None\n    Experiences: Optional[List[Any]] = None\n    Educations: Optional[List[Any]] = None\n    Skills: Optional[List[Any]] = None\n    Certificates: Optional[List[Any]] = None\n    Projects: Optional[List[Any]] = None\n    Achievements: Optional[List[Any]] = None\n    Volunteer: Optional[List[Any]] = None\n\n</code></pre>\n<pre class=\"lang-none prettyprint-override\"><code>    def __init__(self, model_name=model_1, fine_tune_model_path: str = None):\n        # Initialize LLM service with specified model\n\n\n        if fine_tune_model_path:\n            # Load fine-tuned model from local directory\n            self.tokenizer = AutoTokenizer.from_pretrained(fine_tune_model_path)\n            self.model = AutoModel.from_pretrained(fine_tune_model_path)\n        else:\n            # Load base model\n            self.llm_service = HuggingFaceHub(\n                repo_id=&quot;google/flan-t5-base&quot;,\n                huggingfacehub_api_token=huggingface_api_key,\n                model_kwargs={\n                    &quot;temperature&quot;: 0.5,\n                    &quot;max_new_tokens&quot;: 200\n                }  # Model parameters for consistent output\n            )\n    def parsed_resume(self, resume_txt: str):\n        df = pd.read_csv(r&quot;C:\\Users\\Sarthak\\PycharmProjects\\JobAxle\\Service\\data\\for_model_resume_dataset.csv&quot;)\n        print(df['prompt'][0])\n        examples = [\n            {'prompt':df['prompt'][0], &quot;completion&quot;:df['completion'][0]},\n            {'prompt': df['prompt'][1], &quot;completion&quot;: df['completion'][1]}\n        ]\n        print('Examples:',examples[0])\n        example_formatter_template = &quot;&quot;&quot;\n        {prompt}\n        {completion}\\n\n        &quot;&quot;&quot;\n        example_prompt = PromptTemplate(\n            input_variables=[&quot;prompt&quot;, &quot;completion&quot;],\n            template=example_formatter_template,\n        )\n        parser = PydanticOutputParser(pydantic_object=OutputFormat)\n        few_shot_prompt_template = FewShotPromptTemplate(\n            examples=examples,\n            example_prompt=example_prompt,\n            suffix=&quot;&quot;&quot;\n                Parse the given resume text, ensuring the output in JSON format:\n        \n                Resume:\n                {resume}\n        \n                {format_instructions}\n        \n                Output as JSON below:\n                completion:&quot;&quot;&quot;,\n            input_variables=[&quot;resume&quot;],\n            example_separator=&quot;\\n&quot;,\n            partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}\n        )\n        print(&quot;Few-Shot Prompt Template with Examples and JSON Instructions:\\n&quot;, few_shot_prompt_template)\n\n        prompt_template = PromptTemplate(\n            input_variables=['resume'],\n            template=Prompt_2\n        )\n        # print(few_shot_prompt_template)\n        # Initialize the LLM chain\n        chain = LLMChain(\n            llm=self.llm_service,\n            prompt=few_shot_prompt_template,\n            verbose=True\n        )\n        print(&quot;Chain:&quot;, chain)\n        print(resume_txt)\n        try:\n            response = chain.invoke({'resume': resume_txt}, verbose=True)\n            print(response)\n            logger.info('Model Response: %s', response)\n            print(&quot;Type of Response:&quot;,type(response))\n            # Invoke the chain and get a response\n            response_json = self.process_response(response)\n            parsed_json = self.structure_response(response_json)\n            return OutputFormat(**parsed_json)  # Return as OutputFormat object\n\n        except Exception as e:\n            logger.error(&quot;Error parsing resume: %s&quot;, e)\n            raise Exception(f&quot;An error occurred in parsed_resume: {e}&quot;)\n\n    def process_response(self, response_text: str) -&gt; Dict:\n        &quot;&quot;&quot;Process LLM response into JSON format.&quot;&quot;&quot;\n        response_json = json.dumps(response_text).strip()\n\n        # Remove extraneous characters\n        if response_json.startswith(&quot;```json&quot;):\n            response_json = response_json[len(&quot;```json&quot;):].strip()\n        if response_json.endswith(&quot;```&quot;):\n            response_json = response_json[:-len(&quot;```&quot;)].strip()\n\n        response_json = remove_trailing_commas(response_json)\n\n        try:\n\n            return json.loads(response_json)\n        except json.JSONDecodeError as e:\n            logger.error(&quot;JSON decoding error: %s. Response text: %s&quot;, e, response_json)\n            raise ValueError(&quot;Failed to parse response as valid JSON&quot;)\n\n    def structure_response(self, parsed_json: Dict) -&gt; Dict:\n        &quot;&quot;&quot;\n        Structure response JSON to match the OutputFormat schema, ensuring lists for 'Experiences' and 'Educations'.\n        &quot;&quot;&quot;\n        # Ensure ContactInformation, Experiences, and Educations are present in the expected structure\n        parsed_json[&quot;ContactInformation&quot;] = parsed_json.get(&quot;ContactInformation&quot;, {})\n        if isinstance(parsed_json.get(&quot;Experiences&quot;), dict):\n            parsed_json[&quot;Experiences&quot;] = [parsed_json[&quot;Experiences&quot;]]\n        else:\n            parsed_json[&quot;Experiences&quot;] = parsed_json.get(&quot;Experiences&quot;, [])\n\n        if isinstance(parsed_json.get(&quot;Educations&quot;), dict):\n            parsed_json[&quot;Educations&quot;] = [parsed_json[&quot;Educations&quot;]]\n        else:\n            parsed_json[&quot;Educations&quot;] = parsed_json.get(&quot;Educations&quot;, [])\n\n        parsed_json[&quot;Projects&quot;] = parsed_json.get(&quot;Projects&quot;, [])\n\n        return parsed_json\n</code></pre>\n<p>The df contains prompt, completion feature with resume_text as a prompt and json output as completion.</p>\n<p>Or Is there any Alternative to doing the task on a low-end specs laptop? I am in need of help because I can't access big parameter models. anybody??</p>\n",
         "2024-11-12 07:20:00",
         "2",
         "59",
         "1",
         "<nlp><prompt><langchain><large-language-model><huggingface>",
         null,
         null
        ],
        [
         "46",
         "79178041",
         "Normalization of token embeddings in BERT encoder blocks",
         "<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>\n",
         "2024-11-11 14:30:31",
         "2",
         "141",
         "2",
         "<nlp><normalization><bert-language-model><attention-model>",
         "79238393.0",
         "<p>I tracked down full details of layer normalization (LN) in BERT <a href=\"https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para\">here</a>.</p>\n<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>\n"
        ],
        [
         "47",
         "79177228",
         "DASK to_csv() problems due to memory",
         "<p>I'm cleaning my text data and afterwards want to save it to csv. Defined cleaning functions work fine, but when to_csv() part comes, here come the problems as well.\nMaybe someone have faced similar problem and has a trick to share with me how it would be possible to solve it? Maybe saving data to csv in chunks or something?</p>\n<p>`if <strong>name</strong> == '<strong>main</strong>':\n# Initialize the Dask client\nclient = Client(n_workers=3, threads_per_worker=1,\nmemory_limit='1.5GB')<br />\nprint('Dask client created')</p>\n<pre><code>PATH = &quot;C:\\\\Users\\\\el ruchenzo\\\\jobsproject\\\\jobsproject\\\\lt_data.csv&quot;\nreqd = ['description', 'title', 'code']\nblocksize = 25e6  # REDUCED FROM 100 GB TO 25 GB\n\n# Load the CSV with Dask\ndf = dd.read_csv(PATH,\n                 usecols=reqd,\n                 blocksize=blocksize,\n                 dtype={'Code': 'float'},\n                 engine='python',\n                 encoding='utf-8',\n                 on_bad_lines='skip')\n\n# Apply the cleaning function to the 'title' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_title'] = df['title'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('title', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f&quot;1. Processing time: {end_time - start_time:.2f} seconds&quot;)\n\n# Apply the cleaning function to the 'description' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_description'] = df['description'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('description', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f&quot;2. Processing time: {end_time - start_time:.2f} seconds&quot;)\n\ndf.to_csv('cleaned_lemma_*.csv', index=False, encoding='utf-8', single_file = False)\nprint('Saved to csv successfully')\n\nprint('Work ended successfully')`\n</code></pre>\n<p>Tried changing workers number, blocksize, memory limit and etc., but nothing seems to work. Also change map() to apply(), tried using df = df.persist(), writing data to csv with pandas in chunks instead with dask, and so on.</p>\n",
         "2024-11-11 10:13:33",
         "0",
         "38",
         "1",
         "<csv><text><nlp><export-to-csv><dask>",
         null,
         null
        ],
        [
         "48",
         "79173053",
         "How to convert character indices to BERT token indices",
         "<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>\n<pre><code>from datasets import load_dataset\nds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\n</code></pre>\n<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>\n<pre><code>d0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n</code></pre>\n<p>After tokenization, the answer indices are 56  and 16:</p>\n<pre><code>from transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'\n</code></pre>\n<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>\n<p>This is from a <a href=\"https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false\" rel=\"nofollow noreferrer\">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=\"https://i.sstatic.net/GsZ6mfcQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/GsZ6mfcQ.png\" alt=\"QA dataset with token answer indices\" /></a></p>\n",
         "2024-11-09 15:15:33",
         "2",
         "33",
         "1",
         "<python><nlp><dataset><large-language-model><bert-language-model>",
         "79175157.0",
         "<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>\n<p>The following function does the above for you:</p>\n<pre><code>def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start &lt;= char_start &lt; end: \n            start_token_idx = i\n        if start &lt; char_end &lt;= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n</code></pre>\n<p>Here's how you can use and test this function:</p>\n<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])\nprint(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(&quot;Tokenized answer:&quot;, answer_tokens)\n</code></pre>\n<p>Output:</p>\n<pre><code>Tokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream\n</code></pre>\n"
        ],
        [
         "49",
         "79165649",
         "Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy",
         "<p>I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity.</p>\n<p>My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold.</p>\n<p>For instance, the text &quot;Standard PRS for Alzheimer's&quot; incorrectly links entities for &quot;Standard&quot; and &quot;PRS,&quot; in addition to &quot;Alzheimer's.&quot; Another example, &quot;Other diseases of respiratory system, NEC,&quot; captures &quot;respiratory&quot; and &quot;diseases&quot; but misses &quot;NEC&quot; (Necrotizing enterocolitis), which should be prioritized.</p>\n<p>I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms.</p>\n<p>I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well).</p>\n<p>I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd).</p>\n<p>I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must.</p>\n",
         "2024-11-07 08:52:14",
         "2",
         "34",
         "1",
         "<nlp><data-science><spacy><named-entity-recognition><entity-linking>",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 16679
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AllTags</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AcceptedAnswerBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this [site][...</td>\n",
       "      <td>2025-03-11 14:50:31</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;python&gt;&lt;nlp&gt;&lt;large-language-model&gt;</td>\n",
       "      <td>79501337.0</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79498915</td>\n",
       "      <td>Comparing the similarity of spoken and written...</td>\n",
       "      <td>&lt;p&gt;I'm converting spoken form text to its writ...</td>\n",
       "      <td>2025-03-10 18:55:59</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;nlp&gt;&lt;large-language-model&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79488426</td>\n",
       "      <td>Upserting in Pinecone takes too long</td>\n",
       "      <td>&lt;p&gt;I'm trying to upsert reviews that i've scra...</td>\n",
       "      <td>2025-03-06 06:22:35</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;python&gt;&lt;nlp&gt;&lt;rag&gt;&lt;pinecone&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79484448</td>\n",
       "      <td>How does ELMo generate words for training ? Is...</td>\n",
       "      <td>&lt;p&gt;I'm confused about using Bidirectional LM f...</td>\n",
       "      <td>2025-03-04 17:32:14</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;nlp&gt;&lt;language-model&gt;&lt;autoregressive-models&gt;&lt;e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79482290</td>\n",
       "      <td>How to handle German language specific charact...</td>\n",
       "      <td>&lt;p&gt;I am working with German Texts, where I nee...</td>\n",
       "      <td>2025-03-03 22:32:36</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;python&gt;&lt;nlp&gt;&lt;tokenize&gt;&lt;large-language-model&gt;&lt;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>62328</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>&lt;p&gt;input: phrase 1, phrase 2&lt;/p&gt;\\n\\n&lt;p&gt;output:...</td>\n",
       "      <td>2008-09-15 12:26:42</td>\n",
       "      <td>65</td>\n",
       "      <td>49872</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;algorithm&gt;&lt;nlp&gt;&lt;semantics&gt;</td>\n",
       "      <td>63076.0</td>\n",
       "      <td>&lt;hr&gt;\\n\\n&lt;p&gt;You might want to check out this pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16675</th>\n",
       "      <td>41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03 10:36:13</td>\n",
       "      <td>118</td>\n",
       "      <td>33149</td>\n",
       "      <td>17</td>\n",
       "      <td>&lt;nlp&gt;</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>&lt;p&gt;Actually what Google does is very much non-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16676</th>\n",
       "      <td>36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31 01:08:48</td>\n",
       "      <td>3</td>\n",
       "      <td>5658</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;windows-vista&gt;&lt;nlp&gt;&lt;speech-recognition&gt;&lt;multi...</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>&lt;p&gt;Citation from Vista &lt;a href=\"http://blogs.m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16677</th>\n",
       "      <td>25332</td>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>&lt;p&gt;I'm looking for an existing library to summ...</td>\n",
       "      <td>2008-08-24 20:57:33</td>\n",
       "      <td>14</td>\n",
       "      <td>6483</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;language-agnostic&gt;&lt;nlp&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16678</th>\n",
       "      <td>23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22 22:45:10</td>\n",
       "      <td>27</td>\n",
       "      <td>6462</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;.net&gt;&lt;datetime&gt;&lt;nlp&gt;</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>&lt;p&gt;We developed exactly what you are looking f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16679 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       QuestionId                                              Title  \\\n",
       "0        79501178        Store images instead of showing in a server   \n",
       "1        79498915  Comparing the similarity of spoken and written...   \n",
       "2        79488426               Upserting in Pinecone takes too long   \n",
       "3        79484448  How does ELMo generate words for training ? Is...   \n",
       "4        79482290  How to handle German language specific charact...   \n",
       "...           ...                                                ...   \n",
       "16674       62328  Is there an algorithm that tells the semantic ...   \n",
       "16675       41424             How do you implement a \"Did you mean\"?   \n",
       "16676       36533     Vista speech recognition in multiple languages   \n",
       "16677       25332  What's a good natural language library to use ...   \n",
       "16678       23689        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                    Body         CreationDate  \\\n",
       "0      <p>I am running the code found on this [site][...  2025-03-11 14:50:31   \n",
       "1      <p>I'm converting spoken form text to its writ...  2025-03-10 18:55:59   \n",
       "2      <p>I'm trying to upsert reviews that i've scra...  2025-03-06 06:22:35   \n",
       "3      <p>I'm confused about using Bidirectional LM f...  2025-03-04 17:32:14   \n",
       "4      <p>I am working with German Texts, where I nee...  2025-03-03 22:32:36   \n",
       "...                                                  ...                  ...   \n",
       "16674  <p>input: phrase 1, phrase 2</p>\\n\\n<p>output:...  2008-09-15 12:26:42   \n",
       "16675  <blockquote>\\n  <p><strong>Possible Duplicate:...  2008-09-03 10:36:13   \n",
       "16676  <p>my primary language is spanish, but I use a...  2008-08-31 01:08:48   \n",
       "16677  <p>I'm looking for an existing library to summ...  2008-08-24 20:57:33   \n",
       "16678  <p>Does anyone know of a .NET date/time parser...  2008-08-22 22:45:10   \n",
       "\n",
       "       Score  ViewCount  AnswerCount  \\\n",
       "0          0         23            1   \n",
       "1          0         20            1   \n",
       "2          1         37            1   \n",
       "3          0         28            1   \n",
       "4          1         59            1   \n",
       "...      ...        ...          ...   \n",
       "16674     65      49872           11   \n",
       "16675    118      33149           17   \n",
       "16676      3       5658            6   \n",
       "16677     14       6483            4   \n",
       "16678     27       6462            9   \n",
       "\n",
       "                                                 AllTags  AcceptedAnswerId  \\\n",
       "0                    <python><nlp><large-language-model>        79501337.0   \n",
       "1                            <nlp><large-language-model>               NaN   \n",
       "2                           <python><nlp><rag><pinecone>               NaN   \n",
       "3      <nlp><language-model><autoregressive-models><e...               NaN   \n",
       "4      <python><nlp><tokenize><large-language-model><...               NaN   \n",
       "...                                                  ...               ...   \n",
       "16674                        <algorithm><nlp><semantics>           63076.0   \n",
       "16675                                              <nlp>           41448.0   \n",
       "16676  <windows-vista><nlp><speech-recognition><multi...           36684.0   \n",
       "16677                           <language-agnostic><nlp>               NaN   \n",
       "16678                              <.net><datetime><nlp>          631134.0   \n",
       "\n",
       "                                      AcceptedAnswerBody  \n",
       "0      <p>I can't test it but ...</p>\\n<p>I checked <...  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "16674  <hr>\\n\\n<p>You might want to check out this pa...  \n",
       "16675  <p>Actually what Google does is very much non-...  \n",
       "16676  <p>Citation from Vista <a href=\"http://blogs.m...  \n",
       "16677                                                NaN  \n",
       "16678  <p>We developed exactly what you are looking f...  \n",
       "\n",
       "[16679 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79       <p><a href=\"https://huggingface.co/docs/transf...\n",
      "195      <h3>Contextual embedding models are computatio...\n",
      "209      <p>So found this in StanfordNLP Javadoc of <em...\n",
      "405      <p>Latest release of deepspeech was 3 years ba...\n",
      "423      <p>The first issue with your code is that you ...\n",
      "                               ...                        \n",
      "15613    <p>If all you want is adjective frequencies, t...\n",
      "15804    <p>The type of plot you referred to in the OP ...\n",
      "15816    <p>This problem is not that stupid. Norvig wro...\n",
      "15858    <p>For example - you might use some classes fr...\n",
      "16478    <p>The basic idea of doing something like this...\n",
      "Name: AcceptedAnswerBody, Length: 238, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_df = df.loc[df['AcceptedAnswerBody'].str.contains('<img', na=False), 'AcceptedAnswerBody']\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graphs using popular python libraries to visualise the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preporcess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensifiers = {\n",
    "    \"very\", \"really\", \"extremely\", \"absolutely\", \"totally\", \"highly\", \"deeply\", \n",
    "    \"strongly\", \"incredibly\", \"exceptionally\", \"remarkably\", \"unbelievably\", \n",
    "    \"insanely\", \"awfully\", \"horribly\", \"hugely\", \"immensely\", \"overly\", \n",
    "    \"particularly\", \"significantly\", \"seriously\", \"tremendously\", \"wildly\",\n",
    "    \"super\", \"ultra\", \"crazy\", \"majorly\"\n",
    "}\n",
    "def clean_text(text,remove_code=True):\n",
    "\n",
    "    # Check if text is None, empty, or NaN\n",
    "    if text is None or text == \"\" or (isinstance(text, float) and np.isnan(text)):\n",
    "        return \"\"\n",
    "    # Convert to string if it's not already (handles numbers, etc.)\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Remove code blocks if requested\n",
    "    if remove_code:\n",
    "        for code in soup.find_all(['code', 'pre']):\n",
    "            code.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    # Clean up excessive whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Remove urls\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    # Remove '@' character using regex\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    text = ''.join([char for char in text if ord(char) < 128])\n",
    "    text = re.sub(r'\\[|\\]', '', text)\n",
    "    \n",
    "    text = text.split()\n",
    "    text = [word for word in text if word.lower() not in intensifiers]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Return the cleaned text as a string\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code(html_content):\n",
    "    # Handle None or empty content\n",
    "    if html_content is None or html_content == \"\" or (isinstance(html_content, float) and np.isnan(html_content)):\n",
    "        return \"\"  # Return empty string for consistency\n",
    "\n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract code blocks\n",
    "    code_blocks = []\n",
    "    for code in soup.find_all(['code', 'pre']):\n",
    "        code_text = code.get_text(strip=True)\n",
    "        if code_text:  # Only add non-empty code blocks\n",
    "            code_blocks.append(code_text)\n",
    "        # Remove code blocks from the soup to avoid duplication\n",
    "        code.decompose()\n",
    "    code_content = \"\\n---\\n\".join(code_blocks) if code_blocks else \"\"\n",
    "    return code_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # Handle non-string inputs\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = df['AcceptedAnswerBody'].loc[24]\n",
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Question_Code'] = df['Body'].apply(get_code)\n",
    "df['Answer_Code'] = df['AcceptedAnswerBody'].apply(get_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_Clean'] = df['Title'].apply(clean_text) \n",
    "df['Body_Clean'] = df['Body'].apply(clean_text)\n",
    "df['AcceptedAnswerBody_Clean'] = df['AcceptedAnswerBody'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combination_text'] = df['Title_Clean'] + \" \" + df['Body_Clean'] + \" \" +  df['AcceptedAnswerBody_Clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with remove stop word for all combination text\n",
    "df['combination_text_no_stopw'] = df['combination_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''parameter:\n",
    "- data will be a single dataframe columns where we want to combine all the text and provide wordcloud\n",
    "- Title will be the output name of wordcloud make sure it's meaningful'''\n",
    "def wc_generating(data,title):\n",
    "    #Define the model\n",
    "    stopwords = STOPWORDS\n",
    "    wc = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        height=600,\n",
    "        width=400\n",
    "    )\n",
    "    #Combine all text of each rows to big text\n",
    "    all_text = ' '.join(data.fillna(''))\n",
    "    wc.generate(all_text)\n",
    "    wc.to_file(f'{title}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_generating(df['Title_Clean'],\"First Attempt for Title\")\n",
    "wc_generating(df['combination_text_no_stopw'],\"First Attempt Combination with no stopword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can do more text cleaning later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy:\n",
    "\n",
    "- Remove some intensifier words (from Assignment1), noise word (could use wordcloud to detect them, define some lists)\n",
    "- Clustering them by embedding first then DBSCAN\n",
    "- Then could try regex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model_embedding = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "def cluster_embedding(dframe,column,model_embedding,cluster_type=\"kmean\",):\n",
    "    embeddings_title = model_embedding.encode(dframe[column].tolist())\n",
    "    dbscan = DBSCAN(eps=0.5,min_samples=5,metric='euclidean')\n",
    "    k = 10  # or whatever value you want to test\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    if cluster_type == \"kmean\":\n",
    "        cluster_kmean = kmeans.fit_predict(embeddings_title)\n",
    "        dframe['cluster_kmean'] = cluster_kmean\n",
    "    elif cluster_type == \"dbscan\":\n",
    "        clusters_dbscan = dbscan.fit_predict(embeddings_title)\n",
    "        dframe['cluster_kmean'] = cluster_kmean\n",
    "    else:\n",
    "        cluster_kmean = kmeans.fit_predict(embeddings_title)\n",
    "        dframe['cluster_kmean'] = cluster_kmean\n",
    "        clusters_dbscan = dbscan.fit_predict(embeddings_title)\n",
    "        dframe['cluster_kmean'] = cluster_kmean\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_embedding(df,'Title_Clean',model_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AllTags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Question_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Answer_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerBody_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_no_stopw",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cluster_kmean",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f063d6e8-819a-473d-90cb-87a3b9558bad",
       "rows": [
        [
         "0",
         "79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this [site][1] in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?\n[1]: <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Llama2_LLM_Attribution</a></p>\n",
         "2025-03-11 14:50:31",
         "0",
         "23",
         "1",
         "<python><nlp><large-language-model>",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n",
         "server\n---\nSSH\n---\nskip_tokens = [1]  # skip the special token for the start of the text <s>\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = \"playing guitar, hiking, and spending time with his family.\"\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)",
         "matplotlib\n---\nshow=True\n---\nfig, ax\n---\nimport matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(\"output.png\")\n#plt.show()  # eventually show it after saving\n---\nfig\n---\nfig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(\"output.png\")",
         "Store images instead of showing in a server",
         "I am running the code found on this site1 in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection. The code is for instance this one: How to store the files locally instead of showing them? 1:",
         "I can't test it but ... I checked source code and it uses for this. If you remove then it shouldn't show it but it should only get . I think you could use matplotlib.pyplot.savefig(filename) to save it in file. Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site1 in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection. The code is for instance this one: How to store the files locally instead of showing them? 1: I can't test it but ... I checked source code and it uses for this. If you remove then it shouldn't show it but it should only get . I think you could use matplotlib.pyplot.savefig(filename) to save it in file. Probably you can also use for this",
         "store images instead showing server running code found site1 server would like store images instead showing since connected remotely ssh connection via connection . code instance one : store files locally instead showing ? 1 : ca n't test ... checked source code uses . remove n't show get . think could use matplotlib.pyplot.savefig ( filename ) save file . probably also use",
         "8"
        ],
        [
         "1",
         "79498915",
         "Comparing the similarity of spoken and written form text",
         "<p>I'm converting spoken form text to its written form. For example, &quot;he owes me two-thousand dollars&quot; should be converted to &quot;he owes me $2,000&quot; . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of &quot;two-thousand dollars&quot; to &quot;$2,000&quot; to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this?</p>\n",
         "2025-03-10 18:55:59",
         "0",
         "20",
         "1",
         "<nlp><large-language-model>",
         null,
         null,
         "",
         "",
         "Comparing the similarity of spoken and written form text",
         "I'm converting spoken form text to its written form. For example, \"he owes me two-thousand dollars\" should be converted to \"he owes me $2,000\" . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of \"two-thousand dollars\" to \"$2,000\" to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this?",
         "",
         "Comparing the similarity of spoken and written form text I'm converting spoken form text to its written form. For example, \"he owes me two-thousand dollars\" should be converted to \"he owes me $2,000\" . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of \"two-thousand dollars\" to \"$2,000\" to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this? ",
         "comparing similarity spoken written form text 'm converting spoken form text written form . example , `` owes two-thousand dollars '' converted `` owes $ 2,000 '' . want automatic check , judge conversion right . use sentence transformers compare embeddings `` two-thousand dollars '' `` $ 2,000 '' check spoken written conversion right ? example , cosine similarity embeddings close 1 , would mean right conversion . better way ?",
         "3"
        ],
        [
         "2",
         "79488426",
         "Upserting in Pinecone takes too long",
         "<p>I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using <code>jina-embedding-v3</code>. For 204 reviews this takes around <strong>2.5 hours!</strong> in Colab. Tried using GPU but the embeddings arent using GPU.\nAm i doing something wrong? Is there a way that i can speed up the process? The code is below:</p>\n<p>Initialising DB:</p>\n<pre><code>if index_name not in pc.list_indexes().names():\n  pc.create_index(\n    name=index_name,\n    dimension=1024,\n    metric=&quot;cosine&quot;,\n    spec=ServerlessSpec(\n        cloud=&quot;aws&quot;,\n        region=&quot;us-east-1&quot;\n    )\n)\n</code></pre>\n<p>Embedding &amp; Upserting:</p>\n<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the Jina embedding model and tokenizer from Hugging Face\nmodel_name = &quot;jinaai/jina-embeddings-v3&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\nfrom langchain.text_splitter import SpacyTextSplitter\ntext_splitter = SpacyTextSplitter(chunk_size=500)\n\n# Function to generate embeddings\ndef generate_embeddings(text, task='retrieval.passage'):\n    return model.encode(text, convert_to_tensor=True, task=task).numpy()\n\nfor review_id, review in enumerate(all_reviews[:2]):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n\n# Generate and store embeddings\nfor review_id, review in enumerate(all_reviews):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n</code></pre>\n",
         "2025-03-06 06:22:35",
         "1",
         "37",
         "1",
         "<python><nlp><rag><pinecone>",
         null,
         null,
         "jina-embedding-v3\n---\nif index_name not in pc.list_indexes().names():\n  pc.create_index(\n    name=index_name,\n    dimension=1024,\n    metric=\"cosine\",\n    spec=ServerlessSpec(\n        cloud=\"aws\",\n        region=\"us-east-1\"\n    )\n)\n---\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the Jina embedding model and tokenizer from Hugging Face\nmodel_name = \"jinaai/jina-embeddings-v3\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\nfrom langchain.text_splitter import SpacyTextSplitter\ntext_splitter = SpacyTextSplitter(chunk_size=500)\n\n# Function to generate embeddings\ndef generate_embeddings(text, task='retrieval.passage'):\n    return model.encode(text, convert_to_tensor=True, task=task).numpy()\n\nfor review_id, review in enumerate(all_reviews[:2]):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f\"{review_id}_{chunk_index}\"\n\n        metadata = {\"review_id\": review_id, \"chunk_index\": chunk_index, \"text\": chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n\n# Generate and store embeddings\nfor review_id, review in enumerate(all_reviews):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f\"{review_id}_{chunk_index}\"\n\n        metadata = {\"review_id\": review_id, \"chunk_index\": chunk_index, \"text\": chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])",
         "",
         "Upserting in Pinecone takes too long",
         "I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using . For 204 reviews this takes around 2.5 hours! in Colab. Tried using GPU but the embeddings arent using GPU. Am i doing something wrong? Is there a way that i can speed up the process? The code is below: Initialising DB: Embedding & Upserting:",
         "",
         "Upserting in Pinecone takes too long I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using . For 204 reviews this takes around 2.5 hours! in Colab. Tried using GPU but the embeddings arent using GPU. Am i doing something wrong? Is there a way that i can speed up the process? The code is below: Initialising DB: Embedding & Upserting: ",
         "upserting pinecone takes long 'm trying upsert reviews 've scraped pinecone . embedding model im using . 204 reviews takes around 2.5 hours ! colab . tried using gpu embeddings arent using gpu . something wrong ? way speed process ? code : initialising db : embedding & upserting :",
         "8"
        ],
        [
         "3",
         "79484448",
         "How does ELMo generate words for training ? Is it autoregressive?",
         "<p>I'm confused about using Bidirectional LM for words prediction and loss computing  while training.</p>\n<p>At first we have a sequence of tokens X1, ..., Xn.</p>\n<p>After gathering their context independent embeddings via CNN... we pass them to 2-layer Stacked BiLSTM.</p>\n<p>BiLSTM works as a tagger here, and for every input token X1, ...,Xn we get probabilities of the next token (for X1 it <strong>should</strong> be X2, for X2 - X3 and etc.). For Xn it will be a probability of the next word in the sentence - Y_n.</p>\n<p>Now, we can compute loss for every token.</p>\n<p>So, I don't understand what we do next. Does ELMo works like an autoregressive LM?</p>\n<p>If it does and we put the newly predicted token Y_n right after X1, ...,Xn and feed it to the BiLSTM to predict the second new word - Y_{n+1}, won't predictions for all previous tokens change ?</p>\n<p>I guess they should because of the Bidirectional nature of the model. There will be new context from the right, and the model can change everything.</p>\n<p>Can we simply compute loss for every token again, not just for a new one ?</p>\n<p>But, if we want to use this model like LM in inference, and predict next <strong>several</strong> words, newly predicted words will affect previous. What predictions should we use ?</p>\n<p>We pass X1, ..., Xn to the ElMo. We get new word Y1. We pass X1, ..., Xn, Y1 to the ELMo. We get another new word Y2 and the previous word Y1 changes to Z1. Should we take Z1, Y2 as an answer, or we freeze Y1, ignore Z1 and use Y1, Y2 as an answer?</p>\n<p>In Transformer this problem is solved by Masked Self-Attention in decoder, and newly predicted words won't affect previous.</p>\n<p>I tried looking for the answer in the original paper, but didn't find anything about the training process.</p>\n",
         "2025-03-04 17:32:14",
         "0",
         "28",
         "1",
         "<nlp><language-model><autoregressive-models><elmo>",
         null,
         null,
         "",
         "",
         "How does ELMo generate words for training ? Is it autoregressive?",
         "I'm confused about using Bidirectional LM for words prediction and loss computing while training. At first we have a sequence of tokens X1, ..., Xn. After gathering their context independent embeddings via CNN... we pass them to 2-layer Stacked BiLSTM. BiLSTM works as a tagger here, and for every input token X1, ...,Xn we get probabilities of the next token (for X1 it should be X2, for X2 - X3 and etc.). For Xn it will be a probability of the next word in the sentence - Y_n. Now, we can compute loss for every token. So, I don't understand what we do next. Does ELMo works like an autoregressive LM? If it does and we put the newly predicted token Y_n right after X1, ...,Xn and feed it to the BiLSTM to predict the second new word - Y_{n+1}, won't predictions for all previous tokens change ? I guess they should because of the Bidirectional nature of the model. There will be new context from the right, and the model can change everything. Can we simply compute loss for every token again, not just for a new one ? But, if we want to use this model like LM in inference, and predict next several words, newly predicted words will affect previous. What predictions should we use ? We pass X1, ..., Xn to the ElMo. We get new word Y1. We pass X1, ..., Xn, Y1 to the ELMo. We get another new word Y2 and the previous word Y1 changes to Z1. Should we take Z1, Y2 as an answer, or we freeze Y1, ignore Z1 and use Y1, Y2 as an answer? In Transformer this problem is solved by Masked Self-Attention in decoder, and newly predicted words won't affect previous. I tried looking for the answer in the original paper, but didn't find anything about the training process.",
         "",
         "How does ELMo generate words for training ? Is it autoregressive? I'm confused about using Bidirectional LM for words prediction and loss computing while training. At first we have a sequence of tokens X1, ..., Xn. After gathering their context independent embeddings via CNN... we pass them to 2-layer Stacked BiLSTM. BiLSTM works as a tagger here, and for every input token X1, ...,Xn we get probabilities of the next token (for X1 it should be X2, for X2 - X3 and etc.). For Xn it will be a probability of the next word in the sentence - Y_n. Now, we can compute loss for every token. So, I don't understand what we do next. Does ELMo works like an autoregressive LM? If it does and we put the newly predicted token Y_n right after X1, ...,Xn and feed it to the BiLSTM to predict the second new word - Y_{n+1}, won't predictions for all previous tokens change ? I guess they should because of the Bidirectional nature of the model. There will be new context from the right, and the model can change everything. Can we simply compute loss for every token again, not just for a new one ? But, if we want to use this model like LM in inference, and predict next several words, newly predicted words will affect previous. What predictions should we use ? We pass X1, ..., Xn to the ElMo. We get new word Y1. We pass X1, ..., Xn, Y1 to the ELMo. We get another new word Y2 and the previous word Y1 changes to Z1. Should we take Z1, Y2 as an answer, or we freeze Y1, ignore Z1 and use Y1, Y2 as an answer? In Transformer this problem is solved by Masked Self-Attention in decoder, and newly predicted words won't affect previous. I tried looking for the answer in the original paper, but didn't find anything about the training process. ",
         "elmo generate words training ? autoregressive ? 'm confused using bidirectional lm words prediction loss computing training . first sequence tokens x1 , ... , xn . gathering context independent embeddings via cnn ... pass 2-layer stacked bilstm . bilstm works tagger , every input token x1 , ... , xn get probabilities next token ( x1 x2 , x2 - x3 etc. ) . xn probability next word sentence - y_n . , compute loss every token . , n't understand next . elmo works like autoregressive lm ? put newly predicted token y_n right x1 , ... , xn feed bilstm predict second new word - y_ { n+1 } , wo n't predictions previous tokens change ? guess bidirectional nature model . new context right , model change everything . simply compute loss every token , new one ? , want use model like lm inference , predict next several words , newly predicted words affect previous . predictions use ? pass x1 , ... , xn elmo . get new word y1 . pass x1 , ... , xn , y1 elmo . get another new word y2 previous word y1 changes z1 . take z1 , y2 answer , freeze y1 , ignore z1 use y1 , y2 answer ? transformer problem solved masked self-attention decoder , newly predicted words wo n't affect previous . tried looking answer original paper , n't find anything training process .",
         "7"
        ],
        [
         "4",
         "79482290",
         "How to handle German language specific characters like (ä, ö, ü, ß) while tokenizing using GPT2Tokenizer?",
         "<p>I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer.</p>\n<p>To tokenize the text, I wrote the implementation as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import GPT2Tokenizer\n\ntext = &quot;zügiger Transport des ABCD stabilen Kindes in die Notaufnahme UKA&quot;\ntext = text.encode(&quot;utf-8&quot;).decode(&quot;utf-8&quot;)  # Re-encode to fix encoding issues\n\n# Load GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\n\nprint(tokens)  # Should properly tokenize &quot;zügiger&quot; instead of splitting &quot;ü&quot;\n</code></pre>\n<p>Now, when I execute this code snippet I get output as follows:</p>\n<pre><code>['z', 'Ã¼', 'g', 'iger', 'ĠTransport', 'Ġdes', 'ĠABC', 'D', 'Ġstabil', 'en', 'ĠKind', 'es', 'Ġin', 'Ġdie', 'ĠNot', 'au', 'fn', 'ah', 'me', 'ĠUK', 'A']\n</code></pre>\n<p>After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below.</p>\n<pre class=\"lang-markdown prettyprint-override\"><code>| Character | UTF-8 Bytes | Misdecoded as Latin-1 | Resulting String |\n|-----------|-------------|-----------------------|------------------|\n| ä         | C3 A4       | Ã + ¤                 | Ã¤               |\n| ö         | C3 B6       | Ã + ¶                 | Ã¶               |\n| ü         | C3 BC       | Ã + ¼                 | Ã¼               |\n| ß         | C3 9F       | Ã + Ÿ                 | ÃŸ               |\n</code></pre>\n<p>Now, how I can keep German language specific characters like (ä, ö, ü, ß) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. &quot;zügiger&quot; becomes something like ['z', 'ü', 'g', 'iger'].</p>\n",
         "2025-03-03 22:32:36",
         "1",
         "59",
         "1",
         "<python><nlp><tokenize><large-language-model><gpt-2>",
         null,
         null,
         "from transformers import GPT2Tokenizer\n\ntext = \"zügiger Transport des ABCD stabilen Kindes in die Notaufnahme UKA\"\ntext = text.encode(\"utf-8\").decode(\"utf-8\")  # Re-encode to fix encoding issues\n\n# Load GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\n\nprint(tokens)  # Should properly tokenize \"zügiger\" instead of splitting \"ü\"\n---\n['z', 'Ã¼', 'g', 'iger', 'ĠTransport', 'Ġdes', 'ĠABC', 'D', 'Ġstabil', 'en', 'ĠKind', 'es', 'Ġin', 'Ġdie', 'ĠNot', 'au', 'fn', 'ah', 'me', 'ĠUK', 'A']\n---\n| Character | UTF-8 Bytes | Misdecoded as Latin-1 | Resulting String |\n|-----------|-------------|-----------------------|------------------|\n| ä         | C3 A4       | Ã + ¤                 | Ã¤               |\n| ö         | C3 B6       | Ã + ¶                 | Ã¶               |\n| ü         | C3 BC       | Ã + ¼                 | Ã¼               |\n| ß         | C3 9F       | Ã + Ÿ                 | ÃŸ               |",
         "",
         "How to handle German language specific characters like (, , , ) while tokenizing using GPT2Tokenizer?",
         "I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer. To tokenize the text, I wrote the implementation as follows: Now, when I execute this code snippet I get output as follows: After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below. Now, how I can keep German language specific characters like (, , , ) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. \"zgiger\" becomes something like 'z', '', 'g', 'iger'.",
         "",
         "How to handle German language specific characters like (, , , ) while tokenizing using GPT2Tokenizer? I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer. To tokenize the text, I wrote the implementation as follows: Now, when I execute this code snippet I get output as follows: After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below. Now, how I can keep German language specific characters like (, , , ) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. \"zgiger\" becomes something like 'z', '', 'g', 'iger'. ",
         "handle german language specific characters like ( , , , ) tokenizing using gpt2tokenizer ? working german texts , need tokenize texts using gpt2tokenizer . tokenize text , wrote implementation follows : , execute code snippet get output follows : bit analysis , found german language specific characters mis-decoded latin-1 see table . , keep german language specific characters like ( , , , ) inside tokens tokenization process , avoiding unintentional misdecodeding , i.e . `` zgiger '' becomes something like ' z ' , `` , ' g ' , 'iger ' .",
         "9"
        ],
        [
         "5",
         "79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03 22:27:07",
         "4",
         "182",
         "2",
         "<python><nlp><spacy><langchain><presidio>",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n",
         "from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"pl\", \"model_name\": \"pl_core_news_lg\"}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                               languages_config=config)\n\ntext = \"Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\"\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(\"Anonymized text:\", anonymized_result)\nprint(\"Deanonymized text:\", deanonymized_result)\nprint(\"Map:\", anonymizer_tool.deanonymizer_mapping)\nprint(\"Anonymized text:\", anon_result)\n---\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\npython -m spacy download pl_core_news_lg\n---\nanonymizer_tool = PresidioReversibleAnonymizer()\n---\nAnonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n---\nnlp = spacy.load(\"pl_core_news_lg\")\ndoc = nlp(text)\n---\nJan Kowalski persName\nWarszawie placeName",
         "config = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": 'pl', \"model_name\": \"pl_core_news_lg\"}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=\"pl\",\n    supported_entities=[\"persName\"]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"CREDIT_CARD\"], languages_config=config)\n---\nAnonymized text: <persName> mieszka w Warszawie i ma e-mail glenn58@example.org.\n---\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\nMap: {'persName': {'<persName>': 'Jan Kowalski', '<persName_2>': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}\n---\nSpacyRecognizer\n---\nsupported_entities",
         "Presidio with Langchain Experimental does not detect Polish names",
         "I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., \"Jan Kowalski\"). Here is my code: Output: I expected the name \"Jan Kowalski\" and the email address to be anonymized, but the output remains unchanged. I have installed the pl_core_news_lg model using: Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly? Any suggestions on how to make it detect names in Polish? The interesting thing is that when I use only Then the output look like this: As mentioned below if I use only spaCy: Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy: So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected.",
         "After some test I was able to find the solution: The output look like this: You need to directly add with formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., \"Jan Kowalski\"). Here is my code: Output: I expected the name \"Jan Kowalski\" and the email address to be anonymized, but the output remains unchanged. I have installed the pl_core_news_lg model using: Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly? Any suggestions on how to make it detect names in Polish? The interesting thing is that when I use only Then the output look like this: As mentioned below if I use only spaCy: Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy: So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected. After some test I was able to find the solution: The output look like this: You need to directly add with formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.",
         "presidio langchain experimental detect polish names using presidio/langchain_experimental anonymize text polish , detect names ( e.g. , `` jan kowalski '' ) . code : output : expected name `` jan kowalski '' email address anonymized , output remains unchanged . installed pl_core_news_lg model using : missing something configuration , presidio support polish entity recognition properly ? suggestions make detect names polish ? interesting thing use output look like : mentioned use spacy : output correct guess 's problem presidio . output spacy : would like create custom analyzer use spacy presidio works expected . test able find solution : output look like : need directly add formatted according spacy 's requirements . believe 's something missing unclear documentation , causing misunderstanding .",
         "8"
        ],
        [
         "6",
         "79465047",
         "Where is the HuggingFace model saved in when loading a model on colab?",
         "<p>I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.).</p>\n<pre><code>model_id = &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;\n\n\npipeline = transformers.pipeline(\n            &quot;text-generation&quot;,\n            model=model_id,\n            #model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16, &quot;cache_dir&quot;: cache_dir},\n            device_map=&quot;auto&quot;)\n</code></pre>\n",
         "2025-02-24 23:25:42",
         "1",
         "46",
         "1",
         "<nlp><huggingface-transformers><huggingface>",
         null,
         null,
         "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n\n\npipeline = transformers.pipeline(\n            \"text-generation\",\n            model=model_id,\n            #model_kwargs={\"torch_dtype\": torch.bfloat16, \"cache_dir\": cache_dir},\n            device_map=\"auto\")",
         "",
         "Where is the HuggingFace model saved in when loading a model on colab?",
         "I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.).",
         "",
         "Where is the HuggingFace model saved in when loading a model on colab? I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.). ",
         "huggingface model saved loading model colab ? code loading generative model . 'm sure see model files colab ( i.e. , config.json etc . ) .",
         "8"
        ],
        [
         "7",
         "79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22 16:06:11",
         "1",
         "31",
         "1",
         "<nlp><opennlp>",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n",
         "",
         "",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using TokenizerME , then I tagged it with POSTaggerME . For both I used the ready-made models provided by the project here . For the sentence Ivo viu a uva, POSTaggerME returns the tags PROPN, VERB, DET, NOUN. The model seems to be using the UD POS Tags . As there is no ready-made model for ChunkerME in portuguese, I followed the instructions and did the training first using the ChunkerConverter tool (to convert from \"arvore deitada\" to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags (B-NP, B-VP, B-NP, I-NP). But, for more complex sentences, it hasn't produced such good results. I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus ( Bosque 8.0 ) seems to be using portuguese tags. For example, instead of PROPN , the corpus uses prop and instead of DET , it uses art . It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag... But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if this does indeed have an impact, there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall.",
         "Q1 Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner: Penn -> UD should work well. UD -> Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the \"classic' Penn tag set. Using a custom, language specific tag-set can work, but it is a matter of \"mapping\" from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy. Q2 No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang. Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists . Alternatively, feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition.",
         "OpenNLP POSTaggerME and ChunkerME synergy I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using TokenizerME , then I tagged it with POSTaggerME . For both I used the ready-made models provided by the project here . For the sentence Ivo viu a uva, POSTaggerME returns the tags PROPN, VERB, DET, NOUN. The model seems to be using the UD POS Tags . As there is no ready-made model for ChunkerME in portuguese, I followed the instructions and did the training first using the ChunkerConverter tool (to convert from \"arvore deitada\" to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags (B-NP, B-VP, B-NP, I-NP). But, for more complex sentences, it hasn't produced such good results. I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus ( Bosque 8.0 ) seems to be using portuguese tags. For example, instead of PROPN , the corpus uses prop and instead of DET , it uses art . It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag... But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if this does indeed have an impact, there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall. Q1 Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner: Penn -> UD should work well. UD -> Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the \"classic' Penn tag set. Using a custom, language specific tag-set can work, but it is a matter of \"mapping\" from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy. Q2 No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang. Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists . Alternatively, feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition.",
         "opennlp postaggerme chunkerme synergy 'm trying use opennlp chunking api chunk portuguese sentence . , first tokenized sentence using tokenizerme , tagged postaggerme . used ready-made models provided project . sentence ivo viu uva , postaggerme returns tags propn , verb , det , noun . model seems using ud pos tags . ready-made model chunkerme portuguese , followed instructions training first using chunkerconverter tool ( convert `` arvore deitada '' conll2000 ) generating model chunkertrainerme tool . everything worked well . sentence , chunker produced correct tags ( b-np , b-vp , b-np , i-np ) . , complex sentences , n't produced good results . trying identify could improve chunker training , one things noticed difference types tags . portuguese corpus ( bosque 8.0 ) seems using portuguese tags . example , instead propn , corpus uses prop instead det , uses art . seems could lead problems , especially since one parameters chunker receives array ud tags , trained another type tag ... writing code creating routine convert portuguese notation ud ( penn ) wanted ask , indeed impact , tool already translation suggestions improving chunker precision/recall . q1 yes , chosen tag set ( ud , penn , custom ) impact . conversion possible bi-directional manner : penn - > ud work well . ud - > penn good idea lossy conversion . ud tag set less detailed compared `` classic ' penn tag set . using custom , language specific tag-set work , matter `` mapping '' from/to ud correctly . might work tag sets languages , others might complicated / lossy . q2 , n't . opennlp project takes code donations upcoming releases , want provide mapping/translation pt lang . q3 needs details/discussion apache opennlp user and/or dev mailing lists . alternatively , feel free open jira issue drill topic clear idea proposed code addition .",
         "1"
        ],
        [
         "8",
         "79451974",
         "word/ sentence similarities",
         "<p>I am trying to find if a given word/ set of words are similar to a definition.</p>\n<p>Example - Definition - &quot;vegetarian User&quot;</p>\n<p>Now, if I want to check a set of sentences like below</p>\n<pre><code>sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n</code></pre>\n<p>I tried using some sentence transformer like below</p>\n<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n</code></pre>\n<p>But this is not giving me expected results.</p>\n<p>What is the best approach to achieve results like below?</p>\n<pre><code>[False,True,True,False]\n</code></pre>\n<p>Is it doable with nlp/ some other technique?</p>\n",
         "2025-02-19 15:47:45",
         "1",
         "43",
         "1",
         "<python><python-3.x><nlp>",
         null,
         null,
         "sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n---\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n---\n[False,True,True,False]",
         "",
         "word/ sentence similarities",
         "I am trying to find if a given word/ set of words are similar to a definition. Example - Definition - \"vegetarian User\" Now, if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results. What is the best approach to achieve results like below? Is it doable with nlp/ some other technique?",
         "",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition. Example - Definition - \"vegetarian User\" Now, if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results. What is the best approach to achieve results like below? Is it doable with nlp/ some other technique? ",
         "word/ sentence similarities trying find given word/ set words similar definition . example - definition - `` vegetarian user '' , want check set sentences like tried using sentence transformer like giving expected results . best approach achieve results like ? doable nlp/ technique ?",
         "3"
        ],
        [
         "9",
         "79449476",
         "How do I remove escape characters from output of nltk.word_tokenize?",
         "<p>How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output.</p>\n<p>Here's my code:</p>\n<pre><code>from __future__ import division\nimport nltk, re, pprint\nfrom urllib.request import urlopen\n\nurl = &quot;https://www.gutenberg.org/cache/epub/75394/pg75394.txt&quot;\nraw = urlopen(url).read()\nraw = raw.decode('utf-8')\ntokens = nltk.word_tokenize(raw)\nprint(type(tokens))\nprint(len(tokens))\nprint(tokens[:10])\n</code></pre>\n<p>And the output, with the escapes visible in the first list item:\n<a href=\"https://i.sstatic.net/L1QJ1Mdr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/L1QJ1Mdr.png\" alt=\"enter image description here\" /></a></p>\n<p>I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong?</p>\n<p>I'm using Python 3.12.1 on Windows 11.</p>\n<p>Thanks in advance - please do let me know if I can provide any further helpful information.</p>\n",
         "2025-02-18 20:10:13",
         "1",
         "59",
         "1",
         "<python><nlp><nltk><tokenize><text-processing>",
         null,
         null,
         "from __future__ import division\nimport nltk, re, pprint\nfrom urllib.request import urlopen\n\nurl = \"https://www.gutenberg.org/cache/epub/75394/pg75394.txt\"\nraw = urlopen(url).read()\nraw = raw.decode('utf-8')\ntokens = nltk.word_tokenize(raw)\nprint(type(tokens))\nprint(len(tokens))\nprint(tokens[:10])",
         "",
         "How do I remove escape characters from output of nltk.word_tokenize?",
         "How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output. Here's my code: And the output, with the escapes visible in the first list item: I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong? I'm using Python 3.12.1 on Windows 11. Thanks in advance - please do let me know if I can provide any further helpful information.",
         "",
         "How do I remove escape characters from output of nltk.word_tokenize? How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output. Here's my code: And the output, with the escapes visible in the first list item: I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong? I'm using Python 3.12.1 on Windows 11. Thanks in advance - please do let me know if I can provide any further helpful information. ",
         "remove escape characters output nltk.word_tokenize ? get rid non-printing ( escaped ) characters output nltk.word_tokenize method ? working book 'natural language processing python ' following code examples , inform output consist words punctuation , however 'm still getting escapes output . 's code : output , escapes visible first list item : 've poked around online suspicion may fact book 's sample code written python 2 , already caused encoding issues ( needed add line convert output bytes string ) . right track ? , wrong ? 'm using python 3.12.1 windows 11. thanks advance - please let know provide helpful information .",
         "1"
        ],
        [
         "10",
         "79448878",
         "Python Farm-haystack Dependencies",
         "<p>i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these</p>\n<p>from this implementation:</p>\n<pre><code>import os\nfrom typing import List\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes import PreProcessor, BM25Retriever, FARMReader\n\n# Initialize an in-memory document store (replaceable with Elasticsearch)\ndocument_store = InMemoryDocumentStore()\n\n# Folder where uploaded documents are stored\nUPLOAD_FOLDER = &quot;uploaded_docs&quot;\n\n# Ensure the upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\n\ndef list_documents() -&gt; List[str]:\n    &quot;&quot;&quot;List all uploaded documents.&quot;&quot;&quot;\n    try:\n        return os.listdir(UPLOAD_FOLDER)\n    except FileNotFoundError:\n        raise RuntimeError(f&quot;Upload folder '{UPLOAD_FOLDER}' not found. Please create it.&quot;)\n\n\ndef read_document(file_path: str) -&gt; str:\n    &quot;&quot;&quot;Read the content of a document.&quot;&quot;&quot;\n    try:\n        with open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:\n            return f.read()\n    except Exception as e:\n        raise RuntimeError(f&quot;Error reading file '{file_path}': {str(e)}&quot;)\n\n\ndef preprocess_document(content: str) -&gt; List[dict]:\n    &quot;&quot;&quot;Preprocess the document content into smaller chunks for indexing.&quot;&quot;&quot;\n    preprocessor = PreProcessor(\n        split_by=&quot;word&quot;,  # Split the content into chunks by word count\n        split_length=200,  # Chunk size\n        split_overlap=20,  # Overlap between chunks\n        split_respect_sentence_boundary=True,\n    )\n    return preprocessor.process({&quot;content&quot;: content})\n\n\ndef index_document(file_name: str):\n    &quot;&quot;&quot;Read, preprocess, and index a document.&quot;&quot;&quot;\n    file_path = os.path.join(UPLOAD_FOLDER, file_name)\n    if not os.path.isfile(file_path):\n        raise RuntimeError(f&quot;File '{file_name}' not found in '{UPLOAD_FOLDER}'.&quot;)\n\n    content = read_document(file_path)\n    chunks = preprocess_document(content)\n\n    # Prepare chunks in Haystack-compatible format\n    formatted_chunks = [{&quot;content&quot;: chunk[&quot;content&quot;]} for chunk in chunks]\n    document_store.write_documents(formatted_chunks)\n\n    return {\n        &quot;message&quot;: f&quot;Document '{file_name}' indexed successfully.&quot;,\n        &quot;chunks_count&quot;: len(formatted_chunks),\n    }\n\n\ndef search_documents(query: str):\n    &quot;&quot;&quot;Search indexed documents using a query.&quot;&quot;&quot;\n    retriever = BM25Retriever(document_store=document_store)\n    reader = FARMReader(model_name_or_path=&quot;deepset/roberta-base-squad2&quot;, use_gpu=False)\n    \n    # Retrieve documents\n    retrieved_docs = retriever.retrieve(query)\n    if not retrieved_docs:\n        return {&quot;message&quot;: &quot;No relevant documents found.&quot;}\n\n    # Reader to predict answers from retrieved documents\n    answers = reader.predict(query=query, documents=retrieved_docs, top_k=3)\n\n    # Serialize the results to avoid unsupported types\n    results = [\n        {\n            &quot;answer&quot;: ans.answer,\n            &quot;score&quot;: ans.score,\n            &quot;context&quot;: ans.context,\n            &quot;document_id&quot;: ans.document_id,\n        }\n        for ans in answers[&quot;answers&quot;]\n    ]\n\n    return {&quot;results&quot;: results}\n</code></pre>\n<p>But i keep getting this error:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 122, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 131, in _main\n    prepare(preparation_data)\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 244, in prepare\n    _fixup_main_from_name(data['init_main_from_name'])\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 268, in _fixup_main_from_name\n    main_content = runpy.run_module(mod_name,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;&lt;frozen runpy&gt;&quot;, line 226, in run_module\n  File &quot;&lt;frozen runpy&gt;&quot;, line 98, in _run_module_code\n  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/main.py&quot;, line 7, in &lt;module&gt;\n    from app.views.routes import router\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/views/routes.py&quot;, line 2, in &lt;module&gt;\n    from app.services.document_service import list_documents, index_document, search_documents\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/services/document_service.py&quot;, line 3, in &lt;module&gt;\n    from haystack.document_stores import InMemoryDocumentStore\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/__init__.py&quot;, line 8, in &lt;module&gt;\n    from haystack.schema import Document, Answer, Label, MultiLabel, Span, EvaluationResult, TableCell\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/schema.py&quot;, line 42, in &lt;module&gt;\n    @dataclass\n     ^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 250, in dataclass\n    return create_dataclass(_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 241, in create_dataclass\n    pydantic_complete = _pydantic_dataclasses.complete_dataclass(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py&quot;, line 159, in complete_dataclass\n    schema = gen_schema.generate_schema(cls, from_dunder_get_core_schema=False)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 832, in match_type\n    return self._dataclass_schema(obj, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1561, in _dataclass_schema\n    args = sorted(\n           ^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1562, in &lt;genexpr&gt;\n    (self._generate_dc_field_schema(k, v, decorators) for k, v in fields.items()),\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 933, in _generate_dc_field_schema\n    common_field = self._common_field_schema(name, field_info, decorators)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1081, in _common_field_schema\n    schema = self._apply_annotations(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1825, in _apply_annotations\n    schema = get_inner_schema(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py&quot;, line 82, in __call__\n    schema = self._handler(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1806, in inner_handler\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 840, in match_type\n    return self._match_generic_type(obj, origin)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 864, in _match_generic_type\n    return self._union_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1152, in _union_schema\n    choices.append(self.generate_schema(arg))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 844, in match_type\n    return self._unknown_type_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 405, in _unknown_type_schema\n    raise PydanticSchemaGenerationError(\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for &lt;class 'pandas.core.frame.DataFrame'&gt;. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(&lt;some type&gt;) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(&lt;some type&gt;)` since we do not call `__get_pydantic_core_schema__` on `&lt;some type&gt;` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type\n</code></pre>\n",
         "2025-02-18 16:05:55",
         "-1",
         "29",
         "1",
         "<python-3.x><nlp><artificial-intelligence><fastapi><haystack>",
         null,
         null,
         "import os\nfrom typing import List\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes import PreProcessor, BM25Retriever, FARMReader\n\n# Initialize an in-memory document store (replaceable with Elasticsearch)\ndocument_store = InMemoryDocumentStore()\n\n# Folder where uploaded documents are stored\nUPLOAD_FOLDER = \"uploaded_docs\"\n\n# Ensure the upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\n\ndef list_documents() -> List[str]:\n    \"\"\"List all uploaded documents.\"\"\"\n    try:\n        return os.listdir(UPLOAD_FOLDER)\n    except FileNotFoundError:\n        raise RuntimeError(f\"Upload folder '{UPLOAD_FOLDER}' not found. Please create it.\")\n\n\ndef read_document(file_path: str) -> str:\n    \"\"\"Read the content of a document.\"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n    except Exception as e:\n        raise RuntimeError(f\"Error reading file '{file_path}': {str(e)}\")\n\n\ndef preprocess_document(content: str) -> List[dict]:\n    \"\"\"Preprocess the document content into smaller chunks for indexing.\"\"\"\n    preprocessor = PreProcessor(\n        split_by=\"word\",  # Split the content into chunks by word count\n        split_length=200,  # Chunk size\n        split_overlap=20,  # Overlap between chunks\n        split_respect_sentence_boundary=True,\n    )\n    return preprocessor.process({\"content\": content})\n\n\ndef index_document(file_name: str):\n    \"\"\"Read, preprocess, and index a document.\"\"\"\n    file_path = os.path.join(UPLOAD_FOLDER, file_name)\n    if not os.path.isfile(file_path):\n        raise RuntimeError(f\"File '{file_name}' not found in '{UPLOAD_FOLDER}'.\")\n\n    content = read_document(file_path)\n    chunks = preprocess_document(content)\n\n    # Prepare chunks in Haystack-compatible format\n    formatted_chunks = [{\"content\": chunk[\"content\"]} for chunk in chunks]\n    document_store.write_documents(formatted_chunks)\n\n    return {\n        \"message\": f\"Document '{file_name}' indexed successfully.\",\n        \"chunks_count\": len(formatted_chunks),\n    }\n\n\ndef search_documents(query: str):\n    \"\"\"Search indexed documents using a query.\"\"\"\n    retriever = BM25Retriever(document_store=document_store)\n    reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)\n    \n    # Retrieve documents\n    retrieved_docs = retriever.retrieve(query)\n    if not retrieved_docs:\n        return {\"message\": \"No relevant documents found.\"}\n\n    # Reader to predict answers from retrieved documents\n    answers = reader.predict(query=query, documents=retrieved_docs, top_k=3)\n\n    # Serialize the results to avoid unsupported types\n    results = [\n        {\n            \"answer\": ans.answer,\n            \"score\": ans.score,\n            \"context\": ans.context,\n            \"document_id\": ans.document_id,\n        }\n        for ans in answers[\"answers\"]\n    ]\n\n    return {\"results\": results}\n---\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 131, in _main\n    prepare(preparation_data)\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 244, in prepare\n    _fixup_main_from_name(data['init_main_from_name'])\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 268, in _fixup_main_from_name\n    main_content = runpy.run_module(mod_name,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen runpy>\", line 226, in run_module\n  File \"<frozen runpy>\", line 98, in _run_module_code\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/main.py\", line 7, in <module>\n    from app.views.routes import router\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/views/routes.py\", line 2, in <module>\n    from app.services.document_service import list_documents, index_document, search_documents\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/services/document_service.py\", line 3, in <module>\n    from haystack.document_stores import InMemoryDocumentStore\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/__init__.py\", line 8, in <module>\n    from haystack.schema import Document, Answer, Label, MultiLabel, Span, EvaluationResult, TableCell\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/schema.py\", line 42, in <module>\n    @dataclass\n     ^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py\", line 250, in dataclass\n    return create_dataclass(_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py\", line 241, in create_dataclass\n    pydantic_complete = _pydantic_dataclasses.complete_dataclass(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 159, in complete_dataclass\n    schema = gen_schema.generate_schema(cls, from_dunder_get_core_schema=False)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 832, in match_type\n    return self._dataclass_schema(obj, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1561, in _dataclass_schema\n    args = sorted(\n           ^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1562, in <genexpr>\n    (self._generate_dc_field_schema(k, v, decorators) for k, v in fields.items()),\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 933, in _generate_dc_field_schema\n    common_field = self._common_field_schema(name, field_info, decorators)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1081, in _common_field_schema\n    schema = self._apply_annotations(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1825, in _apply_annotations\n    schema = get_inner_schema(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py\", line 82, in __call__\n    schema = self._handler(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1806, in inner_handler\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 840, in match_type\n    return self._match_generic_type(obj, origin)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 864, in _match_generic_type\n    return self._union_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1152, in _union_schema\n    choices.append(self.generate_schema(arg))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 844, in match_type\n    return self._unknown_type_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 405, in _unknown_type_schema\n    raise PydanticSchemaGenerationError(\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'pandas.core.frame.DataFrame'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type",
         "",
         "Python Farm-haystack Dependencies",
         "i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these from this implementation: But i keep getting this error:",
         "",
         "Python Farm-haystack Dependencies i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these from this implementation: But i keep getting this error: ",
         "python farm-haystack dependencies trying implement model using farm-haystack , however dependency mismatch following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart , currently 2 versions python installed machine ( 3.12 3.11.10 ) , facing challenges . need help proper version dependencies python version works better implementation : keep getting error :",
         "2"
        ],
        [
         "11",
         "79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07 02:48:25",
         "-1",
         "43",
         "1",
         "<keras><deep-learning><nlp><lstm><sentiment-analysis>",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n",
         "lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()",
         "",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes. However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue. This is several information about the dataset: Embedding size: (41151, 100) Maximum sequence length: 731 Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106} Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106} Total x training set (padded): (140997, 200) Total x validation set (padded): (17625, 200) Total x testing set (padded): (17625, 200) Total y training set (one hot): (140997, 3) Total y validation set (one hot): (17625, 3) Total y testing set (one hot): (17625, 2003 This is my full code: enter link description here This is my highlight code for this issue:",
         "Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort. I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then. Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So \"queen\" might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece. This is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context. (It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling with 20+ layers, whereas LSTMs tend to choke after two layers.)",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes. However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue. This is several information about the dataset: Embedding size: (41151, 100) Maximum sequence length: 731 Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106} Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106} Total x training set (padded): (140997, 200) Total x validation set (padded): (17625, 200) Total x testing set (padded): (17625, 200) Total y training set (one hot): (140997, 3) Total y validation set (one hot): (17625, 3) Total y testing set (one hot): (17625, 2003 This is my full code: enter link description here This is my highlight code for this issue: Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort. I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then. Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So \"queen\" might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece. This is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context. (It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling with 20+ layers, whereas LSTMs tend to choke after two layers.)",
         "underfitting pre-trained glove + lstm model : accurcacy unchanged sentiment classification using pre-trained glove lstm model . use google play review scrap , resulting 50k++ texts . implement random sampling minority classes . however , train lstm model , training accuracy remain unchanged several epoch , need insight fix issue . several information dataset : embedding size : ( 41151 , 100 ) maximum sequence length : 731 label distribution random sampling : { 'positive ' : 58749 , 'negative ' : 26643 , 'neutral ' : 9106 } label distribution random sampling : ( 'positive ' : 58749 , 'negative ' : 26643 , 'neutral ' : 9106 } total x training set ( padded ) : ( 140997 , 200 ) total x validation set ( padded ) : ( 17625 , 200 ) total x testing set ( padded ) : ( 17625 , 200 ) total training set ( one hot ) : ( 140997 , 3 ) total validation set ( one hot ) : ( 17625 , 3 ) total testing set ( one hot ) : ( 17625 , 2003 full code : enter link description highlight code issue : based extra information comments , 'm going say reason lstm model hits wall ( unspecified ) lower accuracy 85 % trying reach best type model problem . case tweaking parameters likely wasted effort . 'm fairly sure encoder transformers ( e.g . bert ) surpassed sentiment analysis benchmarks number years back ( sorry , quick search could n't find killer reference insert ) , transformers got bigger better since . extra thought : building top glove embeddings presents problem n't handle multiple meanings word . `` queen '' might female king ( embedding 's party trick : king - male + female = queen ) might pop group , might gay man , might chess piece . going put limit accuracy models built , whereas transformers n't limitation look whole string see words context . ( possible argue , course , bringing context lstm comes . transformers still scaling 20+ layers , whereas lstms tend choke two layers . )",
         "7"
        ],
        [
         "12",
         "79406743",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text",
         "<p>I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns &quot;UNK&quot;. Here is my code:</p>\n<pre><code>from quickumls import QuickUMLS\n\nquickumls_fp = &quot;med7_en/lib/python3.10/site-packages/quickumls&quot;\nmatcher = QuickUMLS(quickumls_fp)\n\ndef extract_umls_cuis(text):\n    &quot;&quot;&quot;Extract UMLS CUIs using QuickUMLS.&quot;&quot;&quot;\n    if isinstance(text, str):\n        matches = matcher.match(text)\n        if matches:\n            return [match['cui'] for match in matches[0]]\n        else:\n            return &quot;UNK&quot;\n\nsample_text = &quot;diclofenac.&quot;\nprint(extract_umls_cuis(sample_text))\n</code></pre>\n<p>What I Have Checked:</p>\n<ul>\n<li>QuickUMLS Installation: I have installed QuickUMLS correctly.</li>\n<li>UMLS Data Availability: I have set the correct path to QuickUMLS.</li>\n<li>Different Input Words: I tried various medical terms, but all return &quot;UNK&quot;.</li>\n</ul>\n",
         "2025-02-02 14:12:55",
         "0",
         "26",
         "1",
         "<python><machine-learning><deep-learning><nlp>",
         null,
         null,
         "from quickumls import QuickUMLS\n\nquickumls_fp = \"med7_en/lib/python3.10/site-packages/quickumls\"\nmatcher = QuickUMLS(quickumls_fp)\n\ndef extract_umls_cuis(text):\n    \"\"\"Extract UMLS CUIs using QuickUMLS.\"\"\"\n    if isinstance(text, str):\n        matches = matcher.match(text)\n        if matches:\n            return [match['cui'] for match in matches[0]]\n        else:\n            return \"UNK\"\n\nsample_text = \"diclofenac.\"\nprint(extract_umls_cuis(sample_text))",
         "",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text",
         "I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns \"UNK\". Here is my code: What I Have Checked: QuickUMLS Installation: I have installed QuickUMLS correctly. UMLS Data Availability: I have set the correct path to QuickUMLS. Different Input Words: I tried various medical terms, but all return \"UNK\".",
         "",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns \"UNK\". Here is my code: What I Have Checked: QuickUMLS Installation: I have installed QuickUMLS correctly. UMLS Data Availability: I have set the correct path to QuickUMLS. Different Input Words: I tried various medical terms, but all return \"UNK\". ",
         "quickumls always returns `` unk '' input text using quickumls extract umls concept unique identifiers ( cuis ) text , matter word input , always returns `` unk '' . code : checked : quickumls installation : installed quickumls correctly . umls data availability : set correct path quickumls . different input words : tried various medical terms , return `` unk '' .",
         "8"
        ],
        [
         "13",
         "79402035",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query",
         "<p>How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation.\nI have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n",
         "2025-01-31 07:26:35",
         "1",
         "50",
         "1",
         "<nlp><large-language-model><rag><vectara><enterprise-rag>",
         null,
         null,
         "mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n---\nmockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)",
         "",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation. I have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working. The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset: I have used these models, but still, the citation is not showing in the response. The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset: I have used these models, but still, the citation is not showing in the response.",
         "",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation. I have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working. The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset: I have used these models, but still, the citation is not showing in the response. The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset: I have used these models, but still, the citation is not showing in the response. ",
         "add citations response vectara ? testing multiple corpora query add citations response vectara ? testing multiple corpora query , updated citation payload . followed approach mentioned vectara documentation . tried models mentioned documentation followed instructions provide citation style , still working . documentation states use citations , one must specify one following summarizers generation_preset : used models , still , citation showing response . documentation states use citations , one must specify one following summarizers generation_preset : used models , still , citation showing response .",
         "8"
        ],
        [
         "14",
         "79399448",
         "LLM Model Lacking Confidence and Changing Answers Based on User Input",
         "<p>I've trained a Large Language Model (LLM) using the RAG method to answer user queries. However, I'm facing an issue where the model lacks confidence in its answers and changes them based on user input, even when the initial response is correct.</p>\n<p>For example, when asked &quot;What is the capital of France?&quot;, the model correctly responds with &quot;Paris.&quot; However, if the user replies &quot;No, it's Berlin,&quot; the model accepts this incorrect response and later provides &quot;Berlin&quot; as the capital of France when asked again.</p>\n<p>I've tried using different prompt templates to reinforce answer consistency, but the issue persists. How can I improve the model’s robustness and prevent it from altering correct answers based on user responses? Any suggestions or guidance would be greatly appreciated.</p>\n",
         "2025-01-30 09:49:37",
         "3",
         "60",
         "1",
         "<nlp><langchain><large-language-model><aiml>",
         null,
         null,
         "",
         "",
         "LLM Model Lacking Confidence and Changing Answers Based on User Input",
         "I've trained a Large Language Model (LLM) using the RAG method to answer user queries. However, I'm facing an issue where the model lacks confidence in its answers and changes them based on user input, even when the initial response is correct. For example, when asked \"What is the capital of France?\", the model correctly responds with \"Paris.\" However, if the user replies \"No, it's Berlin,\" the model accepts this incorrect response and later provides \"Berlin\" as the capital of France when asked again. I've tried using different prompt templates to reinforce answer consistency, but the issue persists. How can I improve the models robustness and prevent it from altering correct answers based on user responses? Any suggestions or guidance would be greatly appreciated.",
         "",
         "LLM Model Lacking Confidence and Changing Answers Based on User Input I've trained a Large Language Model (LLM) using the RAG method to answer user queries. However, I'm facing an issue where the model lacks confidence in its answers and changes them based on user input, even when the initial response is correct. For example, when asked \"What is the capital of France?\", the model correctly responds with \"Paris.\" However, if the user replies \"No, it's Berlin,\" the model accepts this incorrect response and later provides \"Berlin\" as the capital of France when asked again. I've tried using different prompt templates to reinforce answer consistency, but the issue persists. How can I improve the models robustness and prevent it from altering correct answers based on user responses? Any suggestions or guidance would be greatly appreciated. ",
         "llm model lacking confidence changing answers based user input 've trained large language model ( llm ) using rag method answer user queries . however , 'm facing issue model lacks confidence answers changes based user input , even initial response correct . example , asked `` capital france ? `` , model correctly responds `` paris . '' however , user replies `` , 's berlin , '' model accepts incorrect response later provides `` berlin '' capital france asked . 've tried using different prompt templates reinforce answer consistency , issue persists . improve models robustness prevent altering correct answers based user responses ? suggestions guidance would greatly appreciated .",
         "8"
        ],
        [
         "15",
         "79393930",
         "Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data?",
         "<p>I’m running into a frustrating issue while training a BERT-based multi-label text classification model on an imbalanced dataset. After a few epochs, the training loss suddenly becomes NaN, and I can’t seem to figure out why. I’ve tried a bunch of different things, but nothing has worked so far. Hoping someone here has dealt with this before.\n<strong>Dataset Setup</strong>\nNumber of samples is around 100K\nNumber of labels is around 50\nImbalance: Some labels are super common (appear in 80% of samples), while others are barely there (less than 0.5%)</p>\n<p><strong>My Setup</strong>\nI’m using Hugging Face’s bert-base-uncased with a custom classification head.</p>\n<pre><code>from transformers import BertModel\nimport torch.nn as nn\n\nclass MultiLabelClassifier(nn.Module):\n    def __init__(self, num_labels):\n        super(MultiLabelClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.classifier(outputs.pooler_output)  # Using the [CLS] token\n        return logits\n\n</code></pre>\n<p>I’m using ***BCEWithLogitsLoss ***with a weighted loss function to deal with the imbalance:</p>\n<pre><code>class_weights = torch.tensor([1.0 / (freq + 1e-5) for freq in label_frequencies]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n\n</code></pre>\n<p>after 2 or 3 epochs;</p>\n<ul>\n<li>The loss starts off fine but becomes NaN</li>\n<li>Some logits are ridiculously large or small (1e20, -1e20) before the NaN happens.</li>\n<li>Gradients also seem to explode right before the NaN loss kicks in.</li>\n</ul>\n<p>to solve this issue,</p>\n<ol>\n<li>Added gradient clipping, which helps a bit but doesn’t fully fix the issue\n<code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code></li>\n<li>Tried reducing the learning rate to 5e-6, which delays the NaN issue but doesn’t stop it completely</li>\n<li>Thought the issue might be with the classifier weights, so I reinitialized them like this,</li>\n</ol>\n<pre><code>for layer in model.classifier.parameters():\n    if isinstance(layer, nn.Linear):\n        nn.init.xavier_normal_(layer.weight)\n\n</code></pre>\n<ol start=\"4\">\n<li>Rewrote the loss calculation using torch.logsigmoid for numerical stability...</li>\n</ol>\n<pre><code>loss = -labels * torch.logsigmoid(logits) - (1 - labels) * torch.logsigmoid(-logits)\nloss = loss.mean()\n</code></pre>\n<p>Nothing seems to solve the problem completely.</p>\n<p><strong>------my questions---</strong></p>\n<ol>\n<li>Why is this happening? Is it because of the extreme imbalance in my dataset or something else?</li>\n<li>How can I fix it? Should I try something like label smoothing, or is there a better way to stabilize the training?</li>\n</ol>\n<p>here is a snippet of my training loop for context.</p>\n<pre><code>optimizer = AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in dataloader:\n        input_ids, attention_mask, labels = batch[&quot;input_ids&quot;], batch[&quot;attention_mask&quot;], batch[&quot;labels&quot;]\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n</code></pre>\n<p>---<strong>I Need</strong>--\nI’m looking for practical suggestions to <strong>prevent the NaN loss issue entirely</strong> and <strong>stabilize training when working with imbalanced multi-label datasets</strong>.</p>\n<p>If anyone has faced this issue or knows a good fix, I’d really appreciate the help. Thanks!</p>\n",
         "2025-01-28 13:03:15",
         "0",
         "64",
         "1",
         "<deep-learning><nlp><bert-language-model><multilabel-classification><imbalanced-data>",
         null,
         null,
         "from transformers import BertModel\nimport torch.nn as nn\n\nclass MultiLabelClassifier(nn.Module):\n    def __init__(self, num_labels):\n        super(MultiLabelClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.classifier(outputs.pooler_output)  # Using the [CLS] token\n        return logits\n---\nclass_weights = torch.tensor([1.0 / (freq + 1e-5) for freq in label_frequencies]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n---\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n---\nfor layer in model.classifier.parameters():\n    if isinstance(layer, nn.Linear):\n        nn.init.xavier_normal_(layer.weight)\n---\nloss = -labels * torch.logsigmoid(logits) - (1 - labels) * torch.logsigmoid(-logits)\nloss = loss.mean()\n---\noptimizer = AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in dataloader:\n        input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()",
         "",
         "Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data?",
         "Im running into a frustrating issue while training a BERT-based multi-label text classification model on an imbalanced dataset. After a few epochs, the training loss suddenly becomes NaN, and I cant seem to figure out why. Ive tried a bunch of different things, but nothing has worked so far. Hoping someone here has dealt with this before. Dataset Setup Number of samples is around 100K Number of labels is around 50 Imbalance: Some labels are common (appear in 80% of samples), while others are barely there (less than 0.5%) My Setup Im using Hugging Faces bert-base-uncased with a custom classification head. Im using ***BCEWithLogitsLoss ***with a weighted loss function to deal with the imbalance: after 2 or 3 epochs; The loss starts off fine but becomes NaN Some logits are ridiculously large or small (1e20, -1e20) before the NaN happens. Gradients also seem to explode right before the NaN loss kicks in. to solve this issue, Added gradient clipping, which helps a bit but doesnt fully fix the issue Tried reducing the learning rate to 5e-6, which delays the NaN issue but doesnt stop it completely Thought the issue might be with the classifier weights, so I reinitialized them like this, Rewrote the loss calculation using torch.logsigmoid for numerical stability... Nothing seems to solve the problem completely. ------my questions--- Why is this happening? Is it because of the extreme imbalance in my dataset or something else? How can I fix it? Should I try something like label smoothing, or is there a better way to stabilize the training? here is a snippet of my training loop for context. --- I Need -- Im looking for practical suggestions to prevent the NaN loss issue entirely and stabilize training when working with imbalanced multi-label datasets . If anyone has faced this issue or knows a good fix, Id appreciate the help. Thanks!",
         "",
         "Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data? Im running into a frustrating issue while training a BERT-based multi-label text classification model on an imbalanced dataset. After a few epochs, the training loss suddenly becomes NaN, and I cant seem to figure out why. Ive tried a bunch of different things, but nothing has worked so far. Hoping someone here has dealt with this before. Dataset Setup Number of samples is around 100K Number of labels is around 50 Imbalance: Some labels are common (appear in 80% of samples), while others are barely there (less than 0.5%) My Setup Im using Hugging Faces bert-base-uncased with a custom classification head. Im using ***BCEWithLogitsLoss ***with a weighted loss function to deal with the imbalance: after 2 or 3 epochs; The loss starts off fine but becomes NaN Some logits are ridiculously large or small (1e20, -1e20) before the NaN happens. Gradients also seem to explode right before the NaN loss kicks in. to solve this issue, Added gradient clipping, which helps a bit but doesnt fully fix the issue Tried reducing the learning rate to 5e-6, which delays the NaN issue but doesnt stop it completely Thought the issue might be with the classifier weights, so I reinitialized them like this, Rewrote the loss calculation using torch.logsigmoid for numerical stability... Nothing seems to solve the problem completely. ------my questions--- Why is this happening? Is it because of the extreme imbalance in my dataset or something else? How can I fix it? Should I try something like label smoothing, or is there a better way to stabilize the training? here is a snippet of my training loop for context. --- I Need -- Im looking for practical suggestions to prevent the NaN loss issue entirely and stabilize training when working with imbalanced multi-label datasets . If anyone has faced this issue or knows a good fix, Id appreciate the help. Thanks! ",
         "bert model producing nan loss training multi-label classification imbalanced data ? im running frustrating issue training bert-based multi-label text classification model imbalanced dataset . epochs , training loss suddenly becomes nan , cant seem figure . ive tried bunch different things , nothing worked far . hoping someone dealt . dataset setup number samples around 100k number labels around 50 imbalance : labels common ( appear 80 % samples ) , others barely ( less 0.5 % ) setup im using hugging faces bert-base-uncased custom classification head . im using * * * bcewithlogitsloss * * * weighted loss function deal imbalance : 2 3 epochs ; loss starts fine becomes nan logits ridiculously large small ( 1e20 , -1e20 ) nan happens . gradients also seem explode right nan loss kicks . solve issue , added gradient clipping , helps bit doesnt fully fix issue tried reducing learning rate 5e-6 , delays nan issue doesnt stop completely thought issue might classifier weights , reinitialized like , rewrote loss calculation using torch.logsigmoid numerical stability ... nothing seems solve problem completely . -- -- -- questions -- - happening ? extreme imbalance dataset something else ? fix ? try something like label smoothing , better way stabilize training ? snippet training loop context . -- - need -- im looking practical suggestions prevent nan loss issue entirely stabilize training working imbalanced multi-label datasets . anyone faced issue knows good fix , id appreciate help . thanks !",
         "7"
        ],
        [
         "16",
         "79385917",
         "torch.OutOfMemoryError: CUDA out of memory. (Google Colab)",
         "<p>I tried to adapt the mBERT model to an existing code. However, I received the following issue even though I tried different solutions.</p>\n<pre><code>torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 9.06 MiB is free. Process 84806 has 14.74 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 129.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n</code></pre>\n<p>Here's the news model that I'm trying to adapt to DST-MetaASSIST(STAR), which you can find it here:</p>\n<pre><code>### For DST-MetaASSIST\n!git clone https://github.com/smartyfh/DST-MetaASSIST\n</code></pre>\n<p>These are the new models:</p>\n<pre><code># First model mBERT \nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n# Load the model with the correct number of labels\nmodel = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n\n# Second model XLM-R\n\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n\n# Load the tokenizer and model\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nmodel = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=number_of_labels)\n\n\n# Third model mT5\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the mT5 tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained('google/mt5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('google/mt5-small')\n</code></pre>\n<p><strong>I changed the 'train-S1.py' file and then run it</strong></p>\n<pre><code>import torch\nimport os\nimport torch\n\n# Set environment variable for CUDA memory management\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ntorch.cuda.empty_cache()  # Clear CUDA cache\ntorch.backends.cudnn.benchmark = True\n\n# Check for GPU availability\nif torch.cuda.is_available():\n    device = torch.device(&quot;cuda&quot;)\n    print(&quot;Using GPU:&quot;, torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(&quot;cpu&quot;)\n    print(&quot;Using CPU&quot;)\n\n# Reduce batch sizes for memory optimization\ntrain_batch_size = 2  # Reduced from 4 or 16\nmeta_batch_size = 1    # Reduced from 2 or 8\n\n# Run your training script with reduced batch sizes\n!python3 /content/DST-MetaASSIST/STAR/train-S1.py --data_dir data/mwz2.4 --save_dir output-meta24-S1/exp --train_batch_size 2 --meta_batch_size 1 --enc_lr 4e-5 --dec_lr 1e-4 --sw_lr 5e-5 --init_weight 0.5 --n_epochs 1 --do_train\n</code></pre>\n<p>The new script:</p>\n<pre><code># import faulthandler\n# faulthandler.enable()\n# learn slot-wise weight\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport argparse\nimport random\nimport json\nimport time\nimport logging\nfrom tqdm import tqdm, trange\n\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom utils.data_utils import Processor, MultiWozDataset\nfrom utils.eval_utils import model_evaluation\nfrom utils.loss_utils import *\nfrom utils.label_lookup import get_label_lookup_from_first_token\nfrom models.DST import UtteranceEncoding, BeliefTracker\n# Import necessary libraries\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n\n\n#====================================\nimport higher\nimport itertools\nfrom models.WeightNet import SlotWeight\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR\n#====================================\n\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup as get_linear_schedule_with_warmup_T\n\nos.environ['CUDA_VISIBLE_DEVICES']='0'\n# torch.cuda.set_device(0)\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                    datefmt='%m/%d/%Y %H:%M:%S',\n                    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, dec_num_warmup_steps, num_training_steps, last_epoch=-1):\n    &quot;&quot;&quot;\n    see https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/optimization.py#L75\n    &quot;&quot;&quot;\n    def enc_lr_lambda(current_step: int):\n        if current_step &lt; enc_num_warmup_steps:\n            return float(current_step) / float(max(1, enc_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - enc_num_warmup_steps))\n        )\n    \n    def dec_lr_lambda(current_step: int):\n        if current_step &lt; dec_num_warmup_steps:\n            return float(current_step) / float(max(1, dec_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - dec_num_warmup_steps))\n        )\n\n    return LambdaLR(optimizer, [enc_lr_lambda, enc_lr_lambda, dec_lr_lambda], last_epoch)\n\ndef set_seed(args, device):\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n    torch.manual_seed(args.random_seed)\n    if device == &quot;cuda&quot;:\n        torch.cuda.manual_seed(args.random_seed)\n        torch.cuda.manual_seed_all(args.random_seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        \ndef get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device):\n    slot_lookup = get_label_lookup_from_first_token(slot_meta, tokenizer, sv_encoder, device)\n    value_lookup = []\n    for slot in ontology.keys():\n        value_lookup.append(get_label_lookup_from_first_token(ontology[slot], tokenizer, sv_encoder, device))\n    return slot_lookup, value_lookup\n\ndef prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    enc_param_optimizer = list(model.encoder.named_parameters())\n    dec_param_optimizer = list(model.decoder.parameters())\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in enc_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in enc_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n        {'params': dec_param_optimizer, 'lr': dec_learning_rate}\n        ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    scheduler = get_linear_schedule_with_warmup(optimizer, int(num_train_steps * enc_warmup_ratio),\n                                                int(num_train_steps * dec_warmup_ratio), num_train_steps)\n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n\n'''def prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    \n    # Access all parameters of the model\n    param_optimizer = list(model.named_parameters())\n    \n    # Group parameters for the optimizer\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    \n    # Calculate warmup steps\n    enc_num_warmup_steps = int(num_train_steps * enc_warmup_ratio)\n    \n    # Create the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, num_train_steps)\n    \n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n'''\n\ndef get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids):\n    _, pred_all_distance = slot_value_matching(slot_output, value_lookup)\n                \n    loss_slot_gt = unreduced_cross_entropy_loss(pred_all_distance, label_ids)\n    loss_slot_pseudo = unreduced_cross_entropy_loss(pred_all_distance, pseudo_label_ids)\n    \n    return loss_slot_gt, loss_slot_pseudo\n\ndef main(args):\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n        \n    # logger\n    logger_file_name = args.save_dir.split('/')[1]\n    fileHandler = logging.FileHandler(os.path.join(args.save_dir, &quot;%s.txt&quot;%(logger_file_name)))\n    logger.addHandler(fileHandler)\n    logger.info(args)\n    \n    # cuda setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logger.info(&quot;device: {}&quot;.format(device))\n    \n    # set random seed\n    set_seed(args, device)\n\n    #******************************************************\n    # load data\n    #******************************************************\n    processor = Processor(args)\n    slot_meta = processor.slot_meta\n    ontology = processor.ontology\n    logger.info(slot_meta)\n   \n    # Load the mBERT tokenizer\n    #tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n    tokenizer = BertTokenizer.from_pretrained(args.pretrained_model)\n\n    \n    \n    if args.do_train:\n        train_data_raw = processor.get_instances(args.data_dir, args.train_data, tokenizer, True)\n        print(&quot;# train examples %d&quot; % len(train_data_raw))\n\n        # Get unique labels from the training data\n        unique_labels = set(label for instance in train_data_raw for label in instance.label_ids)  # Flatten the list\n        num_labels = len(unique_labels)  # Count unique labels\n        print(f&quot;Number of unique labels: {num_labels}&quot;)\n        \n        meta_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(&quot;# meta examples %d&quot; % len(meta_data_raw))\n        \n        dev_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(&quot;# dev examples %d&quot; % len(dev_data_raw))\n    \n    test_data_raw = processor.get_instances(args.data_dir, args.test_data, tokenizer)\n    print(&quot;# test examples %d&quot; % len(test_data_raw))\n    logger.info(&quot;Data loaded!&quot;)\n    \n    ## Initialize slot and value embeddings\n    sv_encoder = UtteranceEncoding.from_pretrained(args.pretrained_model)\n    for p in sv_encoder.bert.parameters():\n        p.requires_grad = False  \n    slot_lookup, value_lookup = get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device)\n\n    #num_labels = len(value_lookup)\n    # Load the mBERT model with the correct number of labels\n    #model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n    # Clear unused variables and cache\n    torch.cuda.empty_cache()\n    if args.do_train:\n        train_data = MultiWozDataset(train_data_raw,\n                                     tokenizer,\n                                     word_dropout=args.word_dropout,\n                                     max_seq_length=args.max_seq_length,\n                                     use_pseudo_label=True)\n        meta_data = MultiWozDataset(meta_data_raw,\n                                    tokenizer,\n                                    word_dropout=0.0, # do word dropout here???\n                                    max_seq_length=args.max_seq_length)\n\n        num_train_steps = int(len(train_data_raw) / args.train_batch_size * args.n_epochs)\n        logger.info(&quot;***** Run training *****&quot;)\n        logger.info(&quot; Num examples = %d&quot;, len(train_data_raw))\n        logger.info(&quot; Batch size = %d&quot;, args.train_batch_size)\n        logger.info(&quot; Num steps = %d&quot;, num_train_steps)\n\n        train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data,\n                                      sampler=train_sampler,\n                                      batch_size=args.train_batch_size,\n                                      collate_fn=train_data.collate_fn)\n        \n        meta_sampler = RandomSampler(meta_data)\n        meta_dataloader = DataLoader(meta_data,\n                                     sampler=meta_sampler,\n                                     batch_size=args.meta_batch_size,\n                                     collate_fn=meta_data.collate_fn)\n        meta_dataloader = itertools.cycle(meta_dataloader)\n        \n        #******************************************************\n        # build model\n        #******************************************************\n        ## model initialization\n        base_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        # Load the model without unsupported arguments\n        # Load the mBERT model with the correct number of labels\n        #base_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        base_model.to(device)\n\n\n\n\n        '''\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        meta_model.to(device)\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                    num_self_attention_layer=args.num_self_attention_layer)\n        #\n        '''\n        meta_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        meta_model.to(device)\n        \n        # Number of slots\n        SW = SlotWeight(len(slot_meta), init_val=np.log(args.init_weight/(1.0 - args.init_weight)))\n        SW.to(device)\n\n        ## prepare optimizer\n        # Prepare optimizer\n        # Prepare optimizer\n        #base_optimizer, base_scheduler = prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n    \n\n        base_optimizer, base_scheduler = \\\n        prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n        \n        logger.info(base_optimizer)\n        # meta model is a copy of the base model, thus shares the optimizer and scheduler\n        meta_optimizer, meta_scheduler = \\\n        prepare_optimizer(meta_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n\n        sw_param_optimizer = list(SW.parameters())\n        sw_optimizer = optim.AdamW(sw_param_optimizer, lr=args.sw_lr)\n        sw_scheduler = get_linear_schedule_with_warmup_T(sw_optimizer,\n                                                         int(num_train_steps * args.sw_warmup),\n                                                         num_train_steps)\n        \n        #******************************************************\n        # training\n        #******************************************************\n        logger.info(&quot;Training...&quot;)\n\n        best_loss = None\n        best_acc = None\n        last_update = None\n\n        for epoch in trange(int(args.n_epochs), desc=&quot;Epoch&quot;):       \n            batch_loss, meta_batch_loss = [], []\n            for step, batch in enumerate(tqdm(train_dataloader)):\n                base_model.train()\n\n                batch = [b.to(device) for b in batch]\n                input_ids, segment_ids, input_mask, label_ids, pseudo_label_ids = batch\n                \n                # forward (meta model)\n                meta_model.load_state_dict(base_model.state_dict())\n                meta_optimizer.load_state_dict(base_optimizer.state_dict())\n                meta_optimizer.zero_grad()\n                with higher.innerloop_ctx(meta_model, meta_optimizer) as (meta_m, meta_opt):\n                    meta_m.train()\n                    slot_output = meta_m(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    \n                    loss_slot_gt, loss_slot_pseudo = \\\n                    get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                    \n                    s_weight = SW()\n                \n                    meta_loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                    # first backward\n                    meta_opt.step(meta_loss)\n                    \n                    # compute on the meta validation set\n                    batch_v = next(meta_dataloader)\n                    batch_v = [b.to(device) for b in batch_v]\n                    input_ids_v, segment_ids_v, input_mask_v, label_ids_v = batch_v\n                    # second forward\n                    meta_m.eval() # disable dropout\n                    slot_output_v = meta_m(input_ids=input_ids_v,\n                                           attention_mask=input_mask_v,\n                                           token_type_ids=segment_ids_v,\n                                           slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    _, pred_all_distance = slot_value_matching(slot_output_v, value_lookup)\n                    loss_v, _, _ = hard_cross_entropy_loss(pred_all_distance, label_ids_v)\n                    # backward over backward\n                    sw_optimizer.zero_grad()\n                    loss_v.backward()\n                    sw_optimizer.step()\n                    sw_scheduler.step()\n                    meta_batch_loss.append(loss_v.item())\n                \n                # Now we have the updated weight net  \n                # forward (base model)\n                slot_output = base_model(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n\n                loss_slot_gt, loss_slot_pseudo = \\\n                get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                with torch.no_grad():    \n                    s_weight = SW()\n\n                loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                # backward (base model)\n                base_optimizer.zero_grad()\n                loss.backward()\n                base_optimizer.step()\n                base_scheduler.step()\n\n                batch_loss.append(loss.item())\n                if step % 300 == 0:\n                    logger.info(&quot;[%d/%d] [%d/%d] mean_loss: %.6f mean_meta_loss: %.6f&quot; % \\\n                               (epoch+1, args.n_epochs, step, len(train_dataloader),\n                                np.mean(batch_loss), np.mean(meta_batch_loss)))\n                    batch_loss, meta_batch_loss = [], []\n                    logger.info(f'Slot weights: {s_weight.cpu().numpy()}')\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, dev_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n                if last_update is None or best_loss &gt; eval_res['loss']:\n                    best_loss = eval_res['loss']\n#                     save_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#                     torch.save(base_model.state_dict(), save_path)\n                    print(&quot;Best Loss : &quot;, best_loss)\n                    print(&quot;\\n&quot;)\n                if last_update is None or best_acc &lt; eval_res['joint_acc']:\n                    best_acc = eval_res['joint_acc']\n                    save_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n                    save_path_w = os.path.join(args.save_dir, 'sw.bin')\n                    torch.save(base_model.state_dict(), save_path)\n                    torch.save(SW.state_dict(), save_path_w)\n                    last_update = epoch\n                    print(&quot;Best Acc : &quot;, best_acc)\n                    print(&quot;\\n&quot;)\n\n                logger.info(&quot;*** Epoch=%d, Last Update=%d, Dev Loss=%.6f, Dev Acc=%.6f, Dev Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***&quot; % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, test_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n\n                logger.info(&quot;*** Epoch=%d, Last Update=%d, Tes Loss=%.6f, Tes Acc=%.6f, Tes Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***&quot; % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if last_update + args.patience &lt;= epoch:\n                    break\n            torch.cuda.empty_cache()\n\n#         print(&quot;Test using best loss model...&quot;)\n#         best_epoch = 0\n#         ckpt_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#         model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n#                               num_self_attention_layer=args.num_self_attention_layer)\n#         ckpt = torch.load(ckpt_path, map_location='cpu')\n#         model.load_state_dict(ckpt)\n#         model.to(device)\n\n#         test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n#                                     ontology, best_epoch, is_gt_p_state=False)\n#         logger.info(&quot;Results based on best loss: &quot;)\n#         logger.info(test_res)\n    #----------------------------------------------------------------------\n    print(&quot;Test using best acc model...&quot;)\n    best_epoch = 1\n    ckpt_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n    model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                          num_self_attention_layer=args.num_self_attention_layer)\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    model.load_state_dict(ckpt)\n    model.to(device)\n\n    test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n                                ontology, best_epoch, is_gt_p_state=False)\n    logger.info(&quot;Results based on best acc: &quot;)\n    logger.info(test_res)\n    \n\nif __name__ == &quot;__main__&quot;:\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(&quot;--data_dir&quot;, default='data/mwz2.4', type=str)\n    parser.add_argument(&quot;--train_data&quot;, default='train_dials_v2.json', type=str)\n    parser.add_argument(&quot;--dev_data&quot;, default='dev_dials_v2.json', type=str)\n    parser.add_argument(&quot;--test_data&quot;, default='test_dials_v2.json', type=str)\n    #parser.add_argument(&quot;--pretrained_model&quot;, default='bert-base-uncased', type=str)\n    parser.add_argument(&quot;--pretrained_model&quot;, default='bert-base-multilingual-cased', type=str)\n\n    parser.add_argument(&quot;--save_dir&quot;, default='output-meta24-S1/exp', type=str)\n\n    parser.add_argument(&quot;--random_seed&quot;, default=42, type=int)\n\n    #parser.add_argument(&quot;--train_batch_size&quot;, default=16, type=int)\n    #parser.add_argument(&quot;--meta_batch_size&quot;, default=8, type=int)\n    parser.add_argument(&quot;--train_batch_size&quot;, default=2, type=int)  # Reduce from 16 to 8 to 2\n    parser.add_argument(&quot;--meta_batch_size&quot;, default=1, type=int)   # Reduce from 8 to 4 to 1\n    \n    parser.add_argument(&quot;--enc_warmup&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--dec_warmup&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--sw_warmup&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--enc_lr&quot;, default=4e-5, type=float)\n    parser.add_argument(&quot;--dec_lr&quot;, default=1e-4, type=float)\n    parser.add_argument(&quot;--sw_lr&quot;, default=5e-5, type=float)\n    parser.add_argument(&quot;--init_weight&quot;, default=0.5, type=float)\n    parser.add_argument(&quot;--n_epochs&quot;, default=15, type=int)\n    parser.add_argument(&quot;--eval_epoch&quot;, default=1, type=int)\n    parser.add_argument(&quot;--eval_step&quot;, default=100000, type=int)\n\n    parser.add_argument(&quot;--dropout_prob&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--word_dropout&quot;, default=0.1, type=float)\n    \n    parser.add_argument(&quot;--max_seq_length&quot;, default=512, type=int)\n    parser.add_argument(&quot;--patience&quot;, default=6, type=int)\n    parser.add_argument(&quot;--attn_head&quot;, default=4, type=int)\n    parser.add_argument(&quot;--num_history&quot;, default=20, type=int)\n    parser.add_argument(&quot;--num_self_attention_layer&quot;, default=6, type=int)\n    \n    parser.add_argument(&quot;--do_train&quot;, action='store_true')\n       \n    args = parser.parse_args()\n    \n    print('pytorch version: ', torch.__version__)\n    args.torch_version = torch.__version__\n    args.transformers_version = transformers.__version__\n    args.save_dir = args.save_dir + \\\n    f'-sd{args.random_seed}-bz{args.train_batch_size}-{args.meta_batch_size}-lr{args.enc_lr}-{args.dec_lr}-{args.sw_lr}-ep{args.n_epochs}'\n\n    main(args)\n\n</code></pre>\n<p>Any suggestions?! Thanks in advance.</p>\n",
         "2025-01-24 23:41:19",
         "0",
         "81",
         "1",
         "<python><nlp><google-colaboratory><bert-language-model>",
         null,
         null,
         "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 9.06 MiB is free. Process 84806 has 14.74 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 129.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n---\n### For DST-MetaASSIST\n!git clone https://github.com/smartyfh/DST-MetaASSIST\n---\n# First model mBERT \nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n# Load the model with the correct number of labels\nmodel = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n\n# Second model XLM-R\n\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n\n# Load the tokenizer and model\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nmodel = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=number_of_labels)\n\n\n# Third model mT5\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the mT5 tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained('google/mt5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('google/mt5-small')\n---\nimport torch\nimport os\nimport torch\n\n# Set environment variable for CUDA memory management\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ntorch.cuda.empty_cache()  # Clear CUDA cache\ntorch.backends.cudnn.benchmark = True\n\n# Check for GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using GPU:\", torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\n# Reduce batch sizes for memory optimization\ntrain_batch_size = 2  # Reduced from 4 or 16\nmeta_batch_size = 1    # Reduced from 2 or 8\n\n# Run your training script with reduced batch sizes\n!python3 /content/DST-MetaASSIST/STAR/train-S1.py --data_dir data/mwz2.4 --save_dir output-meta24-S1/exp --train_batch_size 2 --meta_batch_size 1 --enc_lr 4e-5 --dec_lr 1e-4 --sw_lr 5e-5 --init_weight 0.5 --n_epochs 1 --do_train\n---\n# import faulthandler\n# faulthandler.enable()\n# learn slot-wise weight\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport argparse\nimport random\nimport json\nimport time\nimport logging\nfrom tqdm import tqdm, trange\n\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom utils.data_utils import Processor, MultiWozDataset\nfrom utils.eval_utils import model_evaluation\nfrom utils.loss_utils import *\nfrom utils.label_lookup import get_label_lookup_from_first_token\nfrom models.DST import UtteranceEncoding, BeliefTracker\n# Import necessary libraries\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n\n\n#====================================\nimport higher\nimport itertools\nfrom models.WeightNet import SlotWeight\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR\n#====================================\n\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup as get_linear_schedule_with_warmup_T\n\nos.environ['CUDA_VISIBLE_DEVICES']='0'\n# torch.cuda.set_device(0)\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                    datefmt='%m/%d/%Y %H:%M:%S',\n                    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, dec_num_warmup_steps, num_training_steps, last_epoch=-1):\n    \"\"\"\n    see https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/optimization.py#L75\n    \"\"\"\n    def enc_lr_lambda(current_step: int):\n        if current_step < enc_num_warmup_steps:\n            return float(current_step) / float(max(1, enc_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - enc_num_warmup_steps))\n        )\n    \n    def dec_lr_lambda(current_step: int):\n        if current_step < dec_num_warmup_steps:\n            return float(current_step) / float(max(1, dec_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - dec_num_warmup_steps))\n        )\n\n    return LambdaLR(optimizer, [enc_lr_lambda, enc_lr_lambda, dec_lr_lambda], last_epoch)\n\ndef set_seed(args, device):\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n    torch.manual_seed(args.random_seed)\n    if device == \"cuda\":\n        torch.cuda.manual_seed(args.random_seed)\n        torch.cuda.manual_seed_all(args.random_seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        \ndef get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device):\n    slot_lookup = get_label_lookup_from_first_token(slot_meta, tokenizer, sv_encoder, device)\n    value_lookup = []\n    for slot in ontology.keys():\n        value_lookup.append(get_label_lookup_from_first_token(ontology[slot], tokenizer, sv_encoder, device))\n    return slot_lookup, value_lookup\n\ndef prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    enc_param_optimizer = list(model.encoder.named_parameters())\n    dec_param_optimizer = list(model.decoder.parameters())\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in enc_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in enc_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n        {'params': dec_param_optimizer, 'lr': dec_learning_rate}\n        ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    scheduler = get_linear_schedule_with_warmup(optimizer, int(num_train_steps * enc_warmup_ratio),\n                                                int(num_train_steps * dec_warmup_ratio), num_train_steps)\n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n\n'''def prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    \n    # Access all parameters of the model\n    param_optimizer = list(model.named_parameters())\n    \n    # Group parameters for the optimizer\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    \n    # Calculate warmup steps\n    enc_num_warmup_steps = int(num_train_steps * enc_warmup_ratio)\n    \n    # Create the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, num_train_steps)\n    \n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n'''\n\ndef get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids):\n    _, pred_all_distance = slot_value_matching(slot_output, value_lookup)\n                \n    loss_slot_gt = unreduced_cross_entropy_loss(pred_all_distance, label_ids)\n    loss_slot_pseudo = unreduced_cross_entropy_loss(pred_all_distance, pseudo_label_ids)\n    \n    return loss_slot_gt, loss_slot_pseudo\n\ndef main(args):\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n        \n    # logger\n    logger_file_name = args.save_dir.split('/')[1]\n    fileHandler = logging.FileHandler(os.path.join(args.save_dir, \"%s.txt\"%(logger_file_name)))\n    logger.addHandler(fileHandler)\n    logger.info(args)\n    \n    # cuda setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logger.info(\"device: {}\".format(device))\n    \n    # set random seed\n    set_seed(args, device)\n\n    #******************************************************\n    # load data\n    #******************************************************\n    processor = Processor(args)\n    slot_meta = processor.slot_meta\n    ontology = processor.ontology\n    logger.info(slot_meta)\n   \n    # Load the mBERT tokenizer\n    #tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n    tokenizer = BertTokenizer.from_pretrained(args.pretrained_model)\n\n    \n    \n    if args.do_train:\n        train_data_raw = processor.get_instances(args.data_dir, args.train_data, tokenizer, True)\n        print(\"# train examples %d\" % len(train_data_raw))\n\n        # Get unique labels from the training data\n        unique_labels = set(label for instance in train_data_raw for label in instance.label_ids)  # Flatten the list\n        num_labels = len(unique_labels)  # Count unique labels\n        print(f\"Number of unique labels: {num_labels}\")\n        \n        meta_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(\"# meta examples %d\" % len(meta_data_raw))\n        \n        dev_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(\"# dev examples %d\" % len(dev_data_raw))\n    \n    test_data_raw = processor.get_instances(args.data_dir, args.test_data, tokenizer)\n    print(\"# test examples %d\" % len(test_data_raw))\n    logger.info(\"Data loaded!\")\n    \n    ## Initialize slot and value embeddings\n    sv_encoder = UtteranceEncoding.from_pretrained(args.pretrained_model)\n    for p in sv_encoder.bert.parameters():\n        p.requires_grad = False  \n    slot_lookup, value_lookup = get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device)\n\n    #num_labels = len(value_lookup)\n    # Load the mBERT model with the correct number of labels\n    #model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n    # Clear unused variables and cache\n    torch.cuda.empty_cache()\n    if args.do_train:\n        train_data = MultiWozDataset(train_data_raw,\n                                     tokenizer,\n                                     word_dropout=args.word_dropout,\n                                     max_seq_length=args.max_seq_length,\n                                     use_pseudo_label=True)\n        meta_data = MultiWozDataset(meta_data_raw,\n                                    tokenizer,\n                                    word_dropout=0.0, # do word dropout here???\n                                    max_seq_length=args.max_seq_length)\n\n        num_train_steps = int(len(train_data_raw) / args.train_batch_size * args.n_epochs)\n        logger.info(\"***** Run training *****\")\n        logger.info(\" Num examples = %d\", len(train_data_raw))\n        logger.info(\" Batch size = %d\", args.train_batch_size)\n        logger.info(\" Num steps = %d\", num_train_steps)\n\n        train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data,\n                                      sampler=train_sampler,\n                                      batch_size=args.train_batch_size,\n                                      collate_fn=train_data.collate_fn)\n        \n        meta_sampler = RandomSampler(meta_data)\n        meta_dataloader = DataLoader(meta_data,\n                                     sampler=meta_sampler,\n                                     batch_size=args.meta_batch_size,\n                                     collate_fn=meta_data.collate_fn)\n        meta_dataloader = itertools.cycle(meta_dataloader)\n        \n        #******************************************************\n        # build model\n        #******************************************************\n        ## model initialization\n        base_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        # Load the model without unsupported arguments\n        # Load the mBERT model with the correct number of labels\n        #base_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        base_model.to(device)\n\n\n\n\n        '''\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        meta_model.to(device)\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                    num_self_attention_layer=args.num_self_attention_layer)\n        #\n        '''\n        meta_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        meta_model.to(device)\n        \n        # Number of slots\n        SW = SlotWeight(len(slot_meta), init_val=np.log(args.init_weight/(1.0 - args.init_weight)))\n        SW.to(device)\n\n        ## prepare optimizer\n        # Prepare optimizer\n        # Prepare optimizer\n        #base_optimizer, base_scheduler = prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n    \n\n        base_optimizer, base_scheduler = \\\n        prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n        \n        logger.info(base_optimizer)\n        # meta model is a copy of the base model, thus shares the optimizer and scheduler\n        meta_optimizer, meta_scheduler = \\\n        prepare_optimizer(meta_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n\n        sw_param_optimizer = list(SW.parameters())\n        sw_optimizer = optim.AdamW(sw_param_optimizer, lr=args.sw_lr)\n        sw_scheduler = get_linear_schedule_with_warmup_T(sw_optimizer,\n                                                         int(num_train_steps * args.sw_warmup),\n                                                         num_train_steps)\n        \n        #******************************************************\n        # training\n        #******************************************************\n        logger.info(\"Training...\")\n\n        best_loss = None\n        best_acc = None\n        last_update = None\n\n        for epoch in trange(int(args.n_epochs), desc=\"Epoch\"):       \n            batch_loss, meta_batch_loss = [], []\n            for step, batch in enumerate(tqdm(train_dataloader)):\n                base_model.train()\n\n                batch = [b.to(device) for b in batch]\n                input_ids, segment_ids, input_mask, label_ids, pseudo_label_ids = batch\n                \n                # forward (meta model)\n                meta_model.load_state_dict(base_model.state_dict())\n                meta_optimizer.load_state_dict(base_optimizer.state_dict())\n                meta_optimizer.zero_grad()\n                with higher.innerloop_ctx(meta_model, meta_optimizer) as (meta_m, meta_opt):\n                    meta_m.train()\n                    slot_output = meta_m(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    \n                    loss_slot_gt, loss_slot_pseudo = \\\n                    get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                    \n                    s_weight = SW()\n                \n                    meta_loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                    # first backward\n                    meta_opt.step(meta_loss)\n                    \n                    # compute on the meta validation set\n                    batch_v = next(meta_dataloader)\n                    batch_v = [b.to(device) for b in batch_v]\n                    input_ids_v, segment_ids_v, input_mask_v, label_ids_v = batch_v\n                    # second forward\n                    meta_m.eval() # disable dropout\n                    slot_output_v = meta_m(input_ids=input_ids_v,\n                                           attention_mask=input_mask_v,\n                                           token_type_ids=segment_ids_v,\n                                           slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    _, pred_all_distance = slot_value_matching(slot_output_v, value_lookup)\n                    loss_v, _, _ = hard_cross_entropy_loss(pred_all_distance, label_ids_v)\n                    # backward over backward\n                    sw_optimizer.zero_grad()\n                    loss_v.backward()\n                    sw_optimizer.step()\n                    sw_scheduler.step()\n                    meta_batch_loss.append(loss_v.item())\n                \n                # Now we have the updated weight net  \n                # forward (base model)\n                slot_output = base_model(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n\n                loss_slot_gt, loss_slot_pseudo = \\\n                get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                with torch.no_grad():    \n                    s_weight = SW()\n\n                loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                # backward (base model)\n                base_optimizer.zero_grad()\n                loss.backward()\n                base_optimizer.step()\n                base_scheduler.step()\n\n                batch_loss.append(loss.item())\n                if step % 300 == 0:\n                    logger.info(\"[%d/%d] [%d/%d] mean_loss: %.6f mean_meta_loss: %.6f\" % \\\n                               (epoch+1, args.n_epochs, step, len(train_dataloader),\n                                np.mean(batch_loss), np.mean(meta_batch_loss)))\n                    batch_loss, meta_batch_loss = [], []\n                    logger.info(f'Slot weights: {s_weight.cpu().numpy()}')\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, dev_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n                if last_update is None or best_loss > eval_res['loss']:\n                    best_loss = eval_res['loss']\n#                     save_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#                     torch.save(base_model.state_dict(), save_path)\n                    print(\"Best Loss : \", best_loss)\n                    print(\"\\n\")\n                if last_update is None or best_acc < eval_res['joint_acc']:\n                    best_acc = eval_res['joint_acc']\n                    save_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n                    save_path_w = os.path.join(args.save_dir, 'sw.bin')\n                    torch.save(base_model.state_dict(), save_path)\n                    torch.save(SW.state_dict(), save_path_w)\n                    last_update = epoch\n                    print(\"Best Acc : \", best_acc)\n                    print(\"\\n\")\n\n                logger.info(\"*** Epoch=%d, Last Update=%d, Dev Loss=%.6f, Dev Acc=%.6f, Dev Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***\" % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, test_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n\n                logger.info(\"*** Epoch=%d, Last Update=%d, Tes Loss=%.6f, Tes Acc=%.6f, Tes Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***\" % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if last_update + args.patience <= epoch:\n                    break\n            torch.cuda.empty_cache()\n\n#         print(\"Test using best loss model...\")\n#         best_epoch = 0\n#         ckpt_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#         model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n#                               num_self_attention_layer=args.num_self_attention_layer)\n#         ckpt = torch.load(ckpt_path, map_location='cpu')\n#         model.load_state_dict(ckpt)\n#         model.to(device)\n\n#         test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n#                                     ontology, best_epoch, is_gt_p_state=False)\n#         logger.info(\"Results based on best loss: \")\n#         logger.info(test_res)\n    #----------------------------------------------------------------------\n    print(\"Test using best acc model...\")\n    best_epoch = 1\n    ckpt_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n    model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                          num_self_attention_layer=args.num_self_attention_layer)\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    model.load_state_dict(ckpt)\n    model.to(device)\n\n    test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n                                ontology, best_epoch, is_gt_p_state=False)\n    logger.info(\"Results based on best acc: \")\n    logger.info(test_res)\n    \n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(\"--data_dir\", default='data/mwz2.4', type=str)\n    parser.add_argument(\"--train_data\", default='train_dials_v2.json', type=str)\n    parser.add_argument(\"--dev_data\", default='dev_dials_v2.json', type=str)\n    parser.add_argument(\"--test_data\", default='test_dials_v2.json', type=str)\n    #parser.add_argument(\"--pretrained_model\", default='bert-base-uncased', type=str)\n    parser.add_argument(\"--pretrained_model\", default='bert-base-multilingual-cased', type=str)\n\n    parser.add_argument(\"--save_dir\", default='output-meta24-S1/exp', type=str)\n\n    parser.add_argument(\"--random_seed\", default=42, type=int)\n\n    #parser.add_argument(\"--train_batch_size\", default=16, type=int)\n    #parser.add_argument(\"--meta_batch_size\", default=8, type=int)\n    parser.add_argument(\"--train_batch_size\", default=2, type=int)  # Reduce from 16 to 8 to 2\n    parser.add_argument(\"--meta_batch_size\", default=1, type=int)   # Reduce from 8 to 4 to 1\n    \n    parser.add_argument(\"--enc_warmup\", default=0.1, type=float)\n    parser.add_argument(\"--dec_warmup\", default=0.1, type=float)\n    parser.add_argument(\"--sw_warmup\", default=0.1, type=float)\n    parser.add_argument(\"--enc_lr\", default=4e-5, type=float)\n    parser.add_argument(\"--dec_lr\", default=1e-4, type=float)\n    parser.add_argument(\"--sw_lr\", default=5e-5, type=float)\n    parser.add_argument(\"--init_weight\", default=0.5, type=float)\n    parser.add_argument(\"--n_epochs\", default=15, type=int)\n    parser.add_argument(\"--eval_epoch\", default=1, type=int)\n    parser.add_argument(\"--eval_step\", default=100000, type=int)\n\n    parser.add_argument(\"--dropout_prob\", default=0.1, type=float)\n    parser.add_argument(\"--word_dropout\", default=0.1, type=float)\n    \n    parser.add_argument(\"--max_seq_length\", default=512, type=int)\n    parser.add_argument(\"--patience\", default=6, type=int)\n    parser.add_argument(\"--attn_head\", default=4, type=int)\n    parser.add_argument(\"--num_history\", default=20, type=int)\n    parser.add_argument(\"--num_self_attention_layer\", default=6, type=int)\n    \n    parser.add_argument(\"--do_train\", action='store_true')\n       \n    args = parser.parse_args()\n    \n    print('pytorch version: ', torch.__version__)\n    args.torch_version = torch.__version__\n    args.transformers_version = transformers.__version__\n    args.save_dir = args.save_dir + \\\n    f'-sd{args.random_seed}-bz{args.train_batch_size}-{args.meta_batch_size}-lr{args.enc_lr}-{args.dec_lr}-{args.sw_lr}-ep{args.n_epochs}'\n\n    main(args)",
         "",
         "torch.OutOfMemoryError: CUDA out of memory. (Google Colab)",
         "I tried to adapt the mBERT model to an existing code. However, I received the following issue even though I tried different solutions. Here's the news model that I'm trying to adapt to DST-MetaASSIST(STAR), which you can find it here: These are the new models: I changed the 'train-S1.py' file and then run it The new script: Any suggestions?! Thanks in advance.",
         "",
         "torch.OutOfMemoryError: CUDA out of memory. (Google Colab) I tried to adapt the mBERT model to an existing code. However, I received the following issue even though I tried different solutions. Here's the news model that I'm trying to adapt to DST-MetaASSIST(STAR), which you can find it here: These are the new models: I changed the 'train-S1.py' file and then run it The new script: Any suggestions?! Thanks in advance. ",
         "torch.outofmemoryerror : cuda memory . ( google colab ) tried adapt mbert model existing code . however , received following issue even though tried different solutions . 's news model 'm trying adapt dst-metaassist ( star ) , find : new models : changed 'train-s1.py ' file run new script : suggestions ? ! thanks advance .",
         "2"
        ],
        [
         "17",
         "79377676",
         "how to create a Natural Language Inference pipeline in haystack",
         "<p>Could anyone help me with some advice on how to create a Natural Language Inference pipeline in haystack</p>\n<p>I want to use the Haystack framework to create a pipeline for Natural Language Inference on the response from a Retrieval-Augmented Generation (RAG) application</p>\n<p>Because I'm using haystack-ai , I cannot use farm-haystack. If I could use farm-haystack (v1.0) I believe I could do something like below:</p>\n<pre><code>from haystack import Pipeline\nfrom haystack_ai.nodes import HuggingFaceTextClassifier\n\nclassifier = HuggingFaceTextClassifier(\n    model_name_or_path=entailment_model,\n    task=&quot;text-classification&quot;,  # Task type: text classification\n    labels=[\n        &quot;entailment&quot;,\n        &quot;contradiction&quot;,\n        &quot;neutral&quot;,\n    ],  # Define the labels your model is trained on\n)\n\nclassifier_pipeline = Pipeline()\nclassifier_pipeline.add_cmponent(&quot;classifier_llm&quot;, classifier)\npremise = &quot;The sun rises in the east and sets in the west.&quot;\nhypothesis = &quot;The sun rises in the east.&quot;\n\nclassifier_pipeline.run({&quot;classifier_llm&quot;: {&quot;text&quot;: premise, &quot;text_pair&quot;: hypothesis}})\n</code></pre>\n<p>However I cannot see how to achieve the same in haystack v2.0 (haystack-ai) .</p>\n<p>Any comments or pointers welcome.</p>\n",
         "2025-01-22 12:17:47",
         "0",
         "44",
         "1",
         "<nlp><huggingface><haystack>",
         null,
         null,
         "from haystack import Pipeline\nfrom haystack_ai.nodes import HuggingFaceTextClassifier\n\nclassifier = HuggingFaceTextClassifier(\n    model_name_or_path=entailment_model,\n    task=\"text-classification\",  # Task type: text classification\n    labels=[\n        \"entailment\",\n        \"contradiction\",\n        \"neutral\",\n    ],  # Define the labels your model is trained on\n)\n\nclassifier_pipeline = Pipeline()\nclassifier_pipeline.add_cmponent(\"classifier_llm\", classifier)\npremise = \"The sun rises in the east and sets in the west.\"\nhypothesis = \"The sun rises in the east.\"\n\nclassifier_pipeline.run({\"classifier_llm\": {\"text\": premise, \"text_pair\": hypothesis}})",
         "",
         "how to create a Natural Language Inference pipeline in haystack",
         "Could anyone help me with some advice on how to create a Natural Language Inference pipeline in haystack I want to use the Haystack framework to create a pipeline for Natural Language Inference on the response from a Retrieval-Augmented Generation (RAG) application Because I'm using haystack-ai , I cannot use farm-haystack. If I could use farm-haystack (v1.0) I believe I could do something like below: However I cannot see how to achieve the same in haystack v2.0 (haystack-ai) . Any comments or pointers welcome.",
         "",
         "how to create a Natural Language Inference pipeline in haystack Could anyone help me with some advice on how to create a Natural Language Inference pipeline in haystack I want to use the Haystack framework to create a pipeline for Natural Language Inference on the response from a Retrieval-Augmented Generation (RAG) application Because I'm using haystack-ai , I cannot use farm-haystack. If I could use farm-haystack (v1.0) I believe I could do something like below: However I cannot see how to achieve the same in haystack v2.0 (haystack-ai) . Any comments or pointers welcome. ",
         "create natural language inference pipeline haystack could anyone help advice create natural language inference pipeline haystack want use haystack framework create pipeline natural language inference response retrieval-augmented generation ( rag ) application 'm using haystack-ai , use farm-haystack . could use farm-haystack ( v1.0 ) believe could something like : however see achieve haystack v2.0 ( haystack-ai ) . comments pointers welcome .",
         "3"
        ],
        [
         "18",
         "79372521",
         "TypeError: isinstance() arg 2 must be a type or tuple of types with collections search in Weaviate",
         "<p>I have the following code:</p>\n<pre><code>from weaviate.classes.query import MetadataQuery\nimport weaviate\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\nclient = weaviate.connect_to_local()\nauto_finance= client.collections.get(&quot;AutomotiveFinance&quot;)\n\nquery_vector = embedding_model.embed_documents(&quot;what is Honda Cash and cash equivalents?&quot;)\nprint(len(query_vector[0]))\n#query_vector=[float(i) for i in query_vector[0]]\nquery_vector = [0.023]*768\nresponse = auto_finance.query.hybrid(\n    query = &quot;what is Honda Cash and cash equivalents?&quot;,\n    vector = query_vector, alpha = 0.25, limit = 4  \n)\n</code></pre>\n<p>It queries from a collections called AutomotiveFinance, which is locally hosted.\nI have replaced the query_vec with a dummy vec just to debug, since that example is used in the official document.</p>\n<p>The Automotive Finance collections seems to be ok: below is the inspection code and output:</p>\n<pre><code>import weaviate\nimport weaviate.classes.config as wc\n\n\nclient = weaviate.connect_to_local()\n\nresult = client.collections.get('AutomotiveFinance')\nfor item in result.iterator():\n    print(item.uuid, item.properties)\n\ndata_object = result.query.fetch_object_by_id(\n    &quot;ffdc789b-b188-4fcf-94d5-2e4ad6be37ec&quot;,\n    include_vector=True\n)\n\nprint(len(data_object.vector[&quot;default&quot;]))\n\nclient.close()\n</code></pre>\n<p>output from the AutomotiveFinance inspection code has all the metadata and the id as expected:</p>\n<pre><code>faab711b-f714-457c-a547-ff755b2de4d3 {'page1': 2, 'company': 'honda', 'doc_type': '10q', 'page2': 3, 'raw_text': 'THIS IS PAGE 3\\nBased on the provided text from the SEC Form 10-Q, here is a structured summary of the key sections and items:\\n\\n### Company Information\\n- **Company Name:** American Honda Finance Corporation\\n- **Report \nType:** Quarterly Report on Form 10-Q\\n- **Reporting Period:** For the quarter ended June 30, 2024'}\n</code></pre>\n<p>And the length of the vector is 768 -&gt; this matches the vector embedding model's expected behavior.</p>\n<p>However, when I run the query, I am keep getting the following error:</p>\n<pre><code>PS C:\\Users\\ikim1&gt; &amp; C:/Users/ikim1/RAG-blog/Scripts/python.exe &quot;c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py&quot;\n768\nTraceback (most recent call last):\n  File &quot;c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py&quot;, line 13, in &lt;module&gt;\n    response = auto_finance.query.hybrid(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\syncify.py&quot;, line 23, in sync_method\n    return _EventLoopSingleton.get_instance().run_until_complete(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\event_loop.py&quot;, line 40, in run_until_complete       \n    return fut.result()\n  File &quot;C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result    \n    return self.__get_result()\n  File &quot;C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\queries\\hybrid\\query.py&quot;, line 107, in hybrid\n    res = await self._query.hybrid(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\grpc\\query.py&quot;, line 189, in hybrid      \n    _validate_input(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 31, in _validate_input\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 31, in &lt;genexpr&gt;\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 61, in _is_valid\n    return all(isinstance(val, args[0]) for val in value)\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 61, in &lt;genexpr&gt;\n    return all(isinstance(val, args[0]) for val in value)\nTypeError: isinstance() arg 2 must be a type or tuple of types\n</code></pre>\n<p>I am not completely sure as to what is going on. I think I inputted all the parameters as described in this: <a href=\"https://weaviate.io/developers/weaviate/search/hybrid#specify-a-search-vector\" rel=\"nofollow noreferrer\">https://weaviate.io/developers/weaviate/search/hybrid#specify-a-search-vector</a>.</p>\n<p>EDIT: I figured I should also provide on how I am doing the embedding and all:</p>\n<pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nimport weaviate\nimport weaviate.classes.config as wc\nimport json\nimport os\n\nclient = weaviate.connect_to_local()\n# check if the client is alive\nassert client.is_live()\n\n# delete the existing schema + create schema \nclient.collections.delete(&quot;AutomotiveFinance&quot;)\nclient.collections.create(\n    name = 'AutomotiveFinance',\n    properties=[\n        wc.Property(name = 'page1', data_type = wc.DataType.INT),\n        wc.Property(name = 'page2', data_type = wc.DataType.INT),\n        wc.Property(name = 'company', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'doc_type', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'raw_text', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'embedding', data_type = wc.DataType.BLOB) # here BLOB stands for binary large object.\n    ]\n)\nauto_finance = client.collections.get(&quot;AutomotiveFinance&quot;)\n\n# get the path of each json file\njson_top_file_path = r'C:\\Users\\ikim1\\OneDrive\\Desktop\\RAG file'\njson_file_path = []\nfor file in os.listdir(json_top_file_path):\n    if file.endswith('.json'):\n        file_path = os.path.join(json_top_file_path, file)\n        json_file_path.append(file_path)\n\n# Initialize the text splitter +  embedding model\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,  # Maximum size of each chunk\n    chunk_overlap=100  # Overlap between consecutive chunks\n)\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\n# for each file path of json get the chunks. \n\nchunks_with_metadata_list = []\nfor one_json_path in json_file_path: \n    # each json had the following structure\n    # json['pages'], json['file_path'], json['company'] json['doc_type']\n    # in json['pages'], there is list of each page as element \n    \n    # open the json file \n    with open(one_json_path, 'r') as file: \n        json_data = json.load(file)\n        pages = json_data['pages']\n        company = json_data['company']\n        doc_type = json_data['doc_type']\n        \n        # make the entire string from the pages\n        # make sure to insert the page numbers as well. \n        old_page_num = 0; old_md = ''; old_raw_txt = ''\n        json_string = '' \n        for i, page in enumerate(pages): \n            md = page['md']\n            raw_txt = page['text']\n            page_num = page['page']\n            print(i)\n            # if this is the second one, then start the chunking process\n            if i &gt; 0: \n                old_combined_str = &quot;THIS IS PAGE &quot; + str(old_page_num) + '\\n' + old_md + '\\n' + old_raw_txt\n                new_combined_str = &quot;THIS IS PAGE &quot; + str(page_num) + '\\n' + md + '\\n' + raw_txt\n                combined_str = new_combined_str + '\\n' + old_combined_str\n                # chunk the combined_str using recursive splittin,g but inject the metadata. \n                chunks = text_splitter.split_text(combined_str)\n                # inject the metadata into the chunks\n                for chunk in chunks: \n                    # embed the chunk : output is already a list. so no need for conversion for Weaviate\n                    embedded_chunk = embedding_model.embed_documents(chunk)[0]\n                    chunk_metadata = {\n                        &quot;page1&quot; : old_page_num, &quot;page2&quot; : page_num, \n                        &quot;company&quot; : company, &quot;doc_type&quot; : doc_type, \n                        'raw_text' : chunk\n                        }\n                    # put the chunk data into the vector database\n                    auto_finance.data.insert(\n                        properties = chunk_metadata, \n                        vector = [float(i) for i in embedded_chunk]\n                    )\n            # cache the previous one\n            old_md = md\n            old_raw_txt = raw_txt\n            old_page_num = page_num\n            \n\nclient.close()\n\n</code></pre>\n<p>Thanks!</p>\n",
         "2025-01-20 20:00:44",
         "1",
         "35",
         "1",
         "<python-3.x><nlp><vector-database><rag><weaviate>",
         null,
         null,
         "from weaviate.classes.query import MetadataQuery\nimport weaviate\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\nclient = weaviate.connect_to_local()\nauto_finance= client.collections.get(\"AutomotiveFinance\")\n\nquery_vector = embedding_model.embed_documents(\"what is Honda Cash and cash equivalents?\")\nprint(len(query_vector[0]))\n#query_vector=[float(i) for i in query_vector[0]]\nquery_vector = [0.023]*768\nresponse = auto_finance.query.hybrid(\n    query = \"what is Honda Cash and cash equivalents?\",\n    vector = query_vector, alpha = 0.25, limit = 4  \n)\n---\nimport weaviate\nimport weaviate.classes.config as wc\n\n\nclient = weaviate.connect_to_local()\n\nresult = client.collections.get('AutomotiveFinance')\nfor item in result.iterator():\n    print(item.uuid, item.properties)\n\ndata_object = result.query.fetch_object_by_id(\n    \"ffdc789b-b188-4fcf-94d5-2e4ad6be37ec\",\n    include_vector=True\n)\n\nprint(len(data_object.vector[\"default\"]))\n\nclient.close()\n---\nfaab711b-f714-457c-a547-ff755b2de4d3 {'page1': 2, 'company': 'honda', 'doc_type': '10q', 'page2': 3, 'raw_text': 'THIS IS PAGE 3\\nBased on the provided text from the SEC Form 10-Q, here is a structured summary of the key sections and items:\\n\\n### Company Information\\n- **Company Name:** American Honda Finance Corporation\\n- **Report \nType:** Quarterly Report on Form 10-Q\\n- **Reporting Period:** For the quarter ended June 30, 2024'}\n---\nPS C:\\Users\\ikim1> & C:/Users/ikim1/RAG-blog/Scripts/python.exe \"c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py\"\n768\nTraceback (most recent call last):\n  File \"c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py\", line 13, in <module>\n    response = auto_finance.query.hybrid(\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\syncify.py\", line 23, in sync_method\n    return _EventLoopSingleton.get_instance().run_until_complete(\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\event_loop.py\", line 40, in run_until_complete       \n    return fut.result()\n  File \"C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py\", line 439, in result    \n    return self.__get_result()\n  File \"C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py\", line 388, in __get_result\n    raise self._exception\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\queries\\hybrid\\query.py\", line 107, in hybrid\n    res = await self._query.hybrid(\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\grpc\\query.py\", line 189, in hybrid      \n    _validate_input(\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py\", line 31, in _validate_input\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py\", line 31, in <genexpr>\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py\", line 61, in _is_valid\n    return all(isinstance(val, args[0]) for val in value)\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py\", line 61, in <genexpr>\n    return all(isinstance(val, args[0]) for val in value)\nTypeError: isinstance() arg 2 must be a type or tuple of types\n---\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nimport weaviate\nimport weaviate.classes.config as wc\nimport json\nimport os\n\nclient = weaviate.connect_to_local()\n# check if the client is alive\nassert client.is_live()\n\n# delete the existing schema + create schema \nclient.collections.delete(\"AutomotiveFinance\")\nclient.collections.create(\n    name = 'AutomotiveFinance',\n    properties=[\n        wc.Property(name = 'page1', data_type = wc.DataType.INT),\n        wc.Property(name = 'page2', data_type = wc.DataType.INT),\n        wc.Property(name = 'company', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'doc_type', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'raw_text', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'embedding', data_type = wc.DataType.BLOB) # here BLOB stands for binary large object.\n    ]\n)\nauto_finance = client.collections.get(\"AutomotiveFinance\")\n\n# get the path of each json file\njson_top_file_path = r'C:\\Users\\ikim1\\OneDrive\\Desktop\\RAG file'\njson_file_path = []\nfor file in os.listdir(json_top_file_path):\n    if file.endswith('.json'):\n        file_path = os.path.join(json_top_file_path, file)\n        json_file_path.append(file_path)\n\n# Initialize the text splitter +  embedding model\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,  # Maximum size of each chunk\n    chunk_overlap=100  # Overlap between consecutive chunks\n)\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\n# for each file path of json get the chunks. \n\nchunks_with_metadata_list = []\nfor one_json_path in json_file_path: \n    # each json had the following structure\n    # json['pages'], json['file_path'], json['company'] json['doc_type']\n    # in json['pages'], there is list of each page as element \n    \n    # open the json file \n    with open(one_json_path, 'r') as file: \n        json_data = json.load(file)\n        pages = json_data['pages']\n        company = json_data['company']\n        doc_type = json_data['doc_type']\n        \n        # make the entire string from the pages\n        # make sure to insert the page numbers as well. \n        old_page_num = 0; old_md = ''; old_raw_txt = ''\n        json_string = '' \n        for i, page in enumerate(pages): \n            md = page['md']\n            raw_txt = page['text']\n            page_num = page['page']\n            print(i)\n            # if this is the second one, then start the chunking process\n            if i > 0: \n                old_combined_str = \"THIS IS PAGE \" + str(old_page_num) + '\\n' + old_md + '\\n' + old_raw_txt\n                new_combined_str = \"THIS IS PAGE \" + str(page_num) + '\\n' + md + '\\n' + raw_txt\n                combined_str = new_combined_str + '\\n' + old_combined_str\n                # chunk the combined_str using recursive splittin,g but inject the metadata. \n                chunks = text_splitter.split_text(combined_str)\n                # inject the metadata into the chunks\n                for chunk in chunks: \n                    # embed the chunk : output is already a list. so no need for conversion for Weaviate\n                    embedded_chunk = embedding_model.embed_documents(chunk)[0]\n                    chunk_metadata = {\n                        \"page1\" : old_page_num, \"page2\" : page_num, \n                        \"company\" : company, \"doc_type\" : doc_type, \n                        'raw_text' : chunk\n                        }\n                    # put the chunk data into the vector database\n                    auto_finance.data.insert(\n                        properties = chunk_metadata, \n                        vector = [float(i) for i in embedded_chunk]\n                    )\n            # cache the previous one\n            old_md = md\n            old_raw_txt = raw_txt\n            old_page_num = page_num\n            \n\nclient.close()",
         "",
         "TypeError: isinstance() arg 2 must be a type or tuple of types with collections search in Weaviate",
         "I have the following code: It queries from a collections called AutomotiveFinance, which is locally hosted. I have replaced the query_vec with a dummy vec just to debug, since that example is used in the official document. The Automotive Finance collections seems to be ok: below is the inspection code and output: output from the AutomotiveFinance inspection code has all the metadata and the id as expected: And the length of the vector is 768 -> this matches the vector embedding model's expected behavior. However, when I run the query, I am keep getting the following error: I am not completely sure as to what is going on. I think I inputted all the parameters as described in this: . EDIT: I figured I should also provide on how I am doing the embedding and all: Thanks!",
         "",
         "TypeError: isinstance() arg 2 must be a type or tuple of types with collections search in Weaviate I have the following code: It queries from a collections called AutomotiveFinance, which is locally hosted. I have replaced the query_vec with a dummy vec just to debug, since that example is used in the official document. The Automotive Finance collections seems to be ok: below is the inspection code and output: output from the AutomotiveFinance inspection code has all the metadata and the id as expected: And the length of the vector is 768 -> this matches the vector embedding model's expected behavior. However, when I run the query, I am keep getting the following error: I am not completely sure as to what is going on. I think I inputted all the parameters as described in this: . EDIT: I figured I should also provide on how I am doing the embedding and all: Thanks! ",
         "typeerror : isinstance ( ) arg 2 must type tuple types collections search weaviate following code : queries collections called automotivefinance , locally hosted . replaced query_vec dummy vec debug , since example used official document . automotive finance collections seems ok : inspection code output : output automotivefinance inspection code metadata id expected : length vector 768 - > matches vector embedding model 's expected behavior . however , run query , keep getting following error : completely sure going . think inputted parameters described : . edit : figured also provide embedding : thanks !",
         "2"
        ],
        [
         "19",
         "79367089",
         "Is it possible to Fine-Tune TinyBERT on Mac (M1 chip)?",
         "<p>Does fine-tuning require huge resources, or is it possible to do this task on the local machine as well? I have an Apple M1 (8GB RAM). I know bigger models GPU access but what about TinyBERT? My training dataset has 100,000 samples for your reference.</p>\n",
         "2025-01-18 11:58:02",
         "-1",
         "53",
         "1",
         "<deep-learning><nlp><huggingface-transformers><text-classification><fine-tuning>",
         null,
         null,
         "",
         "",
         "Is it possible to Fine-Tune TinyBERT on Mac (M1 chip)?",
         "Does fine-tuning require huge resources, or is it possible to do this task on the local machine as well? I have an Apple M1 (8GB RAM). I know bigger models GPU access but what about TinyBERT? My training dataset has 100,000 samples for your reference.",
         "",
         "Is it possible to Fine-Tune TinyBERT on Mac (M1 chip)? Does fine-tuning require huge resources, or is it possible to do this task on the local machine as well? I have an Apple M1 (8GB RAM). I know bigger models GPU access but what about TinyBERT? My training dataset has 100,000 samples for your reference. ",
         "possible fine-tune tinybert mac ( m1 chip ) ? fine-tuning require huge resources , possible task local machine well ? apple m1 ( 8gb ram ) . know bigger models gpu access tinybert ? training dataset 100,000 samples reference .",
         "8"
        ],
        [
         "20",
         "79330283",
         "Can't compile Marian NMT",
         "<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=\"https://marian-nmt.github.io/docs/#installation\" rel=\"nofollow noreferrer\">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>\n<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>\n<p>These are the steps that I followed:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n</code></pre>\n<p>This is the result I had:</p>\n<pre><code>➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2\n</code></pre>\n<p>Please help.</p>\n",
         "2025-01-05 06:04:59",
         "4",
         "62",
         "1",
         "<gcc><cmake><nlp><g++>",
         "79332711.0",
         "<p>The diagnostic that your build is tripping, <code>Wtemplate-id-cdtor</code>, was introduced\nwith GCC 14.1. It is a warning, not an error, but your build promotes all warnings to\nerrors, so it breaks your build.</p>\n<p>Although your build specifies <code>-std=c++11</code> in <code>src/3rd_party/spdlog/CMakeLists.txt</code>, which\ngenerates the failure, g++-14 emits <code>Wtemplate-id-cdtor</code> to warn you that the code <em>would be</em>\nillegal under the more recent standard c++20 (and later). Then the warning is made an error.</p>\n<p>The warning is made an error by the compile option <code>-Werror</code>. This option is included in the list\nof compile options <code>ALL_WARNINGS</code>, which is created in the top-level <code>marian/CMakeLists.txt</code>\nat line 227 <em>et seq</em>:</p>\n<pre><code># These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n</code></pre>\n<p>and then applied as compile options for the <code>marian</code> library target in <code>src/CMakeLists.txt</code>\nat line 133:</p>\n<pre><code>target_compile_options(marian PRIVATE ${ALL_WARNINGS})\n</code></pre>\n<p>whence the options are operative for the failing compilation of <code>src/CMakeFiles/marian.dir/common/logging.cpp</code>.</p>\n<p>This failure is a bug in the <code>marian</code> repo which you should <a href=\"https://github.com/marian-nmt/marian/issues\" rel=\"nofollow noreferrer\">report to the maintainers</a>, as\nit does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14.</p>\n<p>Pending a fix, you seem to have three interim options to get your build done. Either:</p>\n<ul>\n<li><p>Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:</p>\n<pre><code>/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\n</code></pre>\n</li>\n</ul>\n<p>e.g. make it <code>registry_t(const registry_t&lt;Mutex&gt;&amp;) = delete;</code> in this occurrence.</p>\n<p>Or:</p>\n<ul>\n<li><p>Locally disable <code>-Wtemplate-id-cdtor</code> at each occurrence, e.g:</p>\n<pre><code>#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored &quot;-Wtemplate-id-cdtor&quot;\nregistry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n#pragma GCC diagnostic pop\n</code></pre>\n</li>\n</ul>\n<p>Or:</p>\n<ul>\n<li>Remove <code>-Werror</code> from the <code>ALL_WARNINGS</code> list in <code>marian/CMakeLists.txt</code> so that <code>Wtemplate-id-cdtor</code> remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status).</li>\n</ul>\n<p>I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.</p>\n",
         "CMakeLists.txt\n---\nset (CMAKE_CXX_STANDARD 11)\n---\ngit clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n---\n➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2",
         "Wtemplate-id-cdtor\n---\n-std=c++11\n---\nsrc/3rd_party/spdlog/CMakeLists.txt\n---\nWtemplate-id-cdtor\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\n# These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n---\nmarian\n---\nsrc/CMakeLists.txt\n---\ntarget_compile_options(marian PRIVATE ${ALL_WARNINGS})\n---\nsrc/CMakeFiles/marian.dir/common/logging.cpp\n---\nmarian\n---\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\n---\nregistry_t(const registry_t<Mutex>&) = delete;\n---\n-Wtemplate-id-cdtor\n---\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wtemplate-id-cdtor\"\nregistry_t<Mutex>(const registry_t<Mutex>&) = delete;\n#pragma GCC diagnostic pop\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\nWtemplate-id-cdtor",
         "Can't compile Marian NMT",
         "I'm using endeavouros. I'm trying to compile Marian with these instructions: . But it fails. The error message seemingly indicates a conflict between the code and c++20. But in all the files of the repo, there is the line . These are the steps that I followed: This is the result I had: Please help.",
         "The diagnostic that your build is tripping, , was introduced with GCC 14.1. It is a warning, not an error, but your build promotes all warnings to errors, so it breaks your build. Although your build specifies in , which generates the failure, g++-14 emits to warn you that the code would be illegal under the more recent standard c++20 (and later). Then the warning is made an error. The warning is made an error by the compile option . This option is included in the list of compile options , which is created in the top-level at line 227 et seq : and then applied as compile options for the library target in at line 133: whence the options are operative for the failing compilation of . This failure is a bug in the repo which you should report to the maintainers , as it does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14. Pending a fix, you seem to have three interim options to get your build done. Either: Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence: e.g. make it in this occurrence. Or: Locally disable at each occurrence, e.g: Or: Remove from the list in so that remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status). I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.",
         "Can't compile Marian NMT I'm using endeavouros. I'm trying to compile Marian with these instructions: . But it fails. The error message seemingly indicates a conflict between the code and c++20. But in all the files of the repo, there is the line . These are the steps that I followed: This is the result I had: Please help. The diagnostic that your build is tripping, , was introduced with GCC 14.1. It is a warning, not an error, but your build promotes all warnings to errors, so it breaks your build. Although your build specifies in , which generates the failure, g++-14 emits to warn you that the code would be illegal under the more recent standard c++20 (and later). Then the warning is made an error. The warning is made an error by the compile option . This option is included in the list of compile options , which is created in the top-level at line 227 et seq : and then applied as compile options for the library target in at line 133: whence the options are operative for the failing compilation of . This failure is a bug in the repo which you should report to the maintainers , as it does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14. Pending a fix, you seem to have three interim options to get your build done. Either: Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence: e.g. make it in this occurrence. Or: Locally disable at each occurrence, e.g: Or: Remove from the list in so that remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status). I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.",
         "ca n't compile marian nmt 'm using endeavouros . 'm trying compile marian instructions : . fails . error message seemingly indicates conflict code c++20 . files repo , line . steps followed : result : please help . diagnostic build tripping , , introduced gcc 14.1. warning , error , build promotes warnings errors , breaks build . although build specifies , generates failure , g++-14 emits warn code would illegal recent standard c++20 ( later ) . warning made error . warning made error compile option . option included list compile options , created top-level line 227 et seq : applied compile options library target line 133 : whence options operative failing compilation . failure bug repo report maintainers , seem reported already . head revision v1.12.0 year older gcc 14. pending fix , seem three interim options get build done . either : make code legal c++11 c++20 diagnostic advice says occurrence : e.g . make occurrence . : locally disable occurrence , e.g : : remove list remains warning . may result diagnostics demoted errors warnings ( default status ) . n't tested options 'd need go trouble installing cuda .",
         "2"
        ],
        [
         "21",
         "79328514",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )\n</code></pre>\n<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>\n",
         "2025-01-04 08:57:44",
         "2",
         "32",
         "1",
         "<pytorch><nlp><large-language-model><huggingface-trainer>",
         "79328698.0",
         "<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>\n<p>To modify the <strong>data collator</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>class CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])\n        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])\n        return batch\n</code></pre>\n<p>then pass it to <code>Trainer</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n</code></pre>\n<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>\n<p>Hope, it will work!</p>\n",
         "input_ids\n---\nlabels\n---\ninteract_ids\n---\ncandidate_ids\n---\nclass LLMWithCustomLayer(LlamaForCausalLM)\n---\ndef forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )",
         "interact_ids\n---\ncandidate_ids\n---\nclass CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[\"interact_ids\"] = torch.tensor([f[\"interact_ids\"] for f in features])\n        batch[\"candidate_ids\"] = torch.tensor([f[\"candidate_ids\"] for f in features])\n        return batch\n---\nTrainer\n---\ntrainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(\"your-llama-model\"),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n---\nforward()\n---\ninteract_ids\n---\ncandidate_ids",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields ' ', ' ' and so on, and I additionally add 2 custom colunms ' ' and ' '. But i can't get these custom fields in the forward() function of my Model ' '. I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.",
         "You need to modify the data collator to pass and to your model, as Trainer ignores extra columns by default. To modify the data collator then pass it to Now, your method will receive and . Hope, it will work!",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer? I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields ' ', ' ' and so on, and I additionally add 2 custom colunms ' ' and ' '. But i can't get these custom fields in the forward() function of my Model ' '. I an new in LLM fine tuning. Can anyone help me? I would be grateful so much. You need to modify the data collator to pass and to your model, as Trainer ignores extra columns by default. To modify the data collator then pass it to Now, your method will receive and . Hope, it will work!",
         "get custom column model 's forward ( ) function training huggingface trainer ? using huggingface trainer train cumstom model subclassing llama llm . tokenized tokenizer , dataset fields ' ' , ' ' , additionally add 2 custom colunms ' ' ' ' . ca n't get custom fields forward ( ) function model ' ' . new llm fine tuning . anyone help ? would grateful much . need modify data collator pass model , trainer ignores extra columns default . modify data collator pass , method receive . hope , work !",
         "7"
        ],
        [
         "22",
         "79321551",
         "getting an error: object of type 'float' has no len()",
         "<p>I am a naive in python and started learning python few months ago\nI am working on twitter data in my local, it has 4 columns. ID, brand, sentiment, comment</p>\n<pre><code>def data_clean_pipeline(text):\n    #removing html tags\n    text = str(BeautifulSoup(text).get_text())\n    #removing any nonletter words \n    text = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, text)\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    SW = stopwords.words('english')\n    text = [t for t in text if not t in set(SW)]\n    #Then we can apply stemming and then lemmitize the data\n    SS_stem = SnowballStemmer(language='english')\n    text = [SS_stem.stem(t) for t in text]\n    word_lemmitize = WordNetLemmatizer()\n    text = [word_lemmitize.lemmatize(t) for t in text]\n    return &quot; &quot;.join(text)\n</code></pre>\n<p>When I apply this function to one of the review in twitter data it works, but when i apply to the entire column<br />\nfor example</p>\n<pre><code>data_clean_pipeline(twitter_data['comment'][0] #it works fine and return the output\n</code></pre>\n<p>but the error occurs when i apply this function to a column</p>\n<pre><code>twitter_data['clean'] = twitter_data['comment'].apply(data_clean_pipeline)  \n</code></pre>\n<p>Any feedback would be helpful, thank you:)</p>\n<p>i have attached an image of my error code in the image description for the python error window</p>\n<p><a href=\"https://i.sstatic.net/JpUuncF2.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<p>I was expecting that it will apply the function to the entire comment column which is not happening.<br />\nI have tried to make multiple unsuccessful attempts</p>\n",
         "2025-01-01 12:01:20",
         "0",
         "48",
         "1",
         "<python><python-3.x><twitter><nlp><project>",
         null,
         null,
         "def data_clean_pipeline(text):\n    #removing html tags\n    text = str(BeautifulSoup(text).get_text())\n    #removing any nonletter words \n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    SW = stopwords.words('english')\n    text = [t for t in text if not t in set(SW)]\n    #Then we can apply stemming and then lemmitize the data\n    SS_stem = SnowballStemmer(language='english')\n    text = [SS_stem.stem(t) for t in text]\n    word_lemmitize = WordNetLemmatizer()\n    text = [word_lemmitize.lemmatize(t) for t in text]\n    return \" \".join(text)\n---\ndata_clean_pipeline(twitter_data['comment'][0] #it works fine and return the output\n---\ntwitter_data['clean'] = twitter_data['comment'].apply(data_clean_pipeline)",
         "",
         "getting an error: object of type 'float' has no len()",
         "I am a naive in python and started learning python few months ago I am working on twitter data in my local, it has 4 columns. ID, brand, sentiment, comment When I apply this function to one of the review in twitter data it works, but when i apply to the entire column for example but the error occurs when i apply this function to a column Any feedback would be helpful, thank you:) i have attached an image of my error code in the image description for the python error window enter image description here I was expecting that it will apply the function to the entire comment column which is not happening. I have tried to make multiple unsuccessful attempts",
         "",
         "getting an error: object of type 'float' has no len() I am a naive in python and started learning python few months ago I am working on twitter data in my local, it has 4 columns. ID, brand, sentiment, comment When I apply this function to one of the review in twitter data it works, but when i apply to the entire column for example but the error occurs when i apply this function to a column Any feedback would be helpful, thank you:) i have attached an image of my error code in the image description for the python error window enter image description here I was expecting that it will apply the function to the entire comment column which is not happening. I have tried to make multiple unsuccessful attempts ",
         "getting error : object type 'float ' len ( ) naive python started learning python months ago working twitter data local , 4 columns . id , brand , sentiment , comment apply function one review twitter data works , apply entire column example error occurs apply function column feedback would helpful , thank : ) attached image error code image description python error window enter image description expecting apply function entire comment column happening . tried make multiple unsuccessful attempts",
         "2"
        ],
        [
         "23",
         "79315936",
         "Is n-gram precision the number of elements in the intersection of one hypothesis and possibly many references?",
         "<p>I was trying to understand how BLEU score works and noticed that if I had to compute the n-gram precisions and have multiple reference sentences, it makes more sense to turn everything into sets to remove duplicates. Order in terms of n-grams does not seem to matter, the order of n-tokens is persisted by n-grams but the order each individial n-gram is irrelevant, so we can indeed just use sets to compute n-gram precision of the hypothesis with respect to many references.</p>\n<p>In other words, if I'm given many references and one hypothesis, I compute the n-gram precision by:</p>\n<ol>\n<li>Forming two sets, one set is the union of all n-gram sets of all references, the other set consists of all n-grams in the hypothesis. Duplicates are eliminated.</li>\n<li>Look for the intersection between both sets. The number of n-grams in that intersection divided by the number of n-grams in the hypothesis set is the n-gram precision.</li>\n</ol>\n<p>I think this idea is also backed up by the <a href=\"https://www.nltk.org/_modules/nltk/translate/bleu_score.html\" rel=\"nofollow noreferrer\">nltk implementation</a> of BLEU scores.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    # Extracts all ngrams in hypothesis\n    # Set an empty Counter if hypothesis is empty.\n    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) &gt;= n else Counter()\n    \n    # Extract a union of references' counts.\n    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n    max_counts = {}\n    for reference in references:\n        reference_counts = (\n            Counter(ngrams(reference, n)) if len(reference) &gt;= n else Counter()\n        )\n        for ngram in counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n    # Assigns the intersection between hypothesis and references' counts.\n    clipped_counts = {\n        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n    }\n</code></pre>\n<p><strong>Am I misunderstanding something or this a correct way of computing n-gram precision?</strong></p>\n",
         "2024-12-29 16:25:12",
         "2",
         "52",
         "2",
         "<nlp><nltk><bleu>",
         null,
         null,
         "# Extracts all ngrams in hypothesis\n    # Set an empty Counter if hypothesis is empty.\n    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n    \n    # Extract a union of references' counts.\n    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n    max_counts = {}\n    for reference in references:\n        reference_counts = (\n            Counter(ngrams(reference, n)) if len(reference) >= n else Counter()\n        )\n        for ngram in counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n    # Assigns the intersection between hypothesis and references' counts.\n    clipped_counts = {\n        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n    }",
         "",
         "Is n-gram precision the number of elements in the intersection of one hypothesis and possibly many references?",
         "I was trying to understand how BLEU score works and noticed that if I had to compute the n-gram precisions and have multiple reference sentences, it makes more sense to turn everything into sets to remove duplicates. Order in terms of n-grams does not seem to matter, the order of n-tokens is persisted by n-grams but the order each individial n-gram is irrelevant, so we can indeed just use sets to compute n-gram precision of the hypothesis with respect to many references. In other words, if I'm given many references and one hypothesis, I compute the n-gram precision by: Forming two sets, one set is the union of all n-gram sets of all references, the other set consists of all n-grams in the hypothesis. Duplicates are eliminated. Look for the intersection between both sets. The number of n-grams in that intersection divided by the number of n-grams in the hypothesis set is the n-gram precision. I think this idea is also backed up by the nltk implementation of BLEU scores. Am I misunderstanding something or this a correct way of computing n-gram precision?",
         "",
         "Is n-gram precision the number of elements in the intersection of one hypothesis and possibly many references? I was trying to understand how BLEU score works and noticed that if I had to compute the n-gram precisions and have multiple reference sentences, it makes more sense to turn everything into sets to remove duplicates. Order in terms of n-grams does not seem to matter, the order of n-tokens is persisted by n-grams but the order each individial n-gram is irrelevant, so we can indeed just use sets to compute n-gram precision of the hypothesis with respect to many references. In other words, if I'm given many references and one hypothesis, I compute the n-gram precision by: Forming two sets, one set is the union of all n-gram sets of all references, the other set consists of all n-grams in the hypothesis. Duplicates are eliminated. Look for the intersection between both sets. The number of n-grams in that intersection divided by the number of n-grams in the hypothesis set is the n-gram precision. I think this idea is also backed up by the nltk implementation of BLEU scores. Am I misunderstanding something or this a correct way of computing n-gram precision? ",
         "n-gram precision number elements intersection one hypothesis possibly many references ? trying understand bleu score works noticed compute n-gram precisions multiple reference sentences , makes sense turn everything sets remove duplicates . order terms n-grams seem matter , order n-tokens persisted n-grams order individial n-gram irrelevant , indeed use sets compute n-gram precision hypothesis respect many references . words , 'm given many references one hypothesis , compute n-gram precision : forming two sets , one set union n-gram sets references , set consists n-grams hypothesis . duplicates eliminated . look intersection sets . number n-grams intersection divided number n-grams hypothesis set n-gram precision . think idea also backed nltk implementation bleu scores . misunderstanding something correct way computing n-gram precision ?",
         "8"
        ],
        [
         "24",
         "79312133",
         "Getting all leaf words (reverse stemming) into one Python List",
         "<p>On the same lines as the solution provided <a href=\"https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword\">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>\n<p>Imagine I have a shorter sample word list as follows:</p>\n<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n</code></pre>\n<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>\n<pre><code>get_word_forms(&quot;command&quot;)\n</code></pre>\n<p>and get the following output:</p>\n<pre><code>{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n</code></pre>\n<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>\n<p>If I try to reverse-stem the entire list in one go:</p>\n<pre><code>[get_word_forms(word) for word in sample]\n</code></pre>\n<p>I fail at getting any output:</p>\n<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n</code></pre>\n<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>\n<p>something like:</p>\n<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. \n</code></pre>\n",
         "2024-12-27 15:04:05",
         "1",
         "46",
         "1",
         "<python><nlp><nltk>",
         "79312987.0",
         "<p>One solution using nested list comprehensions after stripping forgotten spaces:</p>\n<pre><code>all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']\n</code></pre>\n",
         "get_word_forms\n---\nmy_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n---\nget_word_forms(\"command\")\n---\n{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n---\n[get_word_forms(word) for word in sample]\n---\n[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n---\n['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.",
         "all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']",
         "Getting all leaf words (reverse stemming) into one Python List",
         "On the same lines as the solution provided in this link , I am trying to get all leaf words of one stem word. I am using the community-contributed ( Srivastava) package Imagine I have a shorter sample word list as follows: If I work it manually, I do the following (which is go word-by-word, which is time-consuming if I have a list of 200 words): and get the following output: 'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb. If I try to reverse-stem the entire list in one go: I fail at getting any output: I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb: something like:",
         "One solution using nested list comprehensions after stripping forgotten spaces:",
         "Getting all leaf words (reverse stemming) into one Python List On the same lines as the solution provided in this link , I am trying to get all leaf words of one stem word. I am using the community-contributed ( Srivastava) package Imagine I have a shorter sample word list as follows: If I work it manually, I do the following (which is go word-by-word, which is time-consuming if I have a list of 200 words): and get the following output: 'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb. If I try to reverse-stem the entire list in one go: I fail at getting any output: I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb: something like: One solution using nested list comprehensions after stripping forgotten spaces:",
         "getting leaf words ( reverse stemming ) one python list lines solution provided link , trying get leaf words one stem word . using community-contributed ( srivastava ) package imagine shorter sample word list follows : work manually , following ( go word-by-word , time-consuming list 200 words ) : get following output : 'n ' noun , ' ' adjective , ' v ' verb , ' r ' adverb . try reverse-stem entire list one go : fail getting output : think failing saving output dictionary . eventually , would like output list without breaking noun , adjective , adverb , verb : something like : one solution using nested list comprehensions stripping forgotten spaces :",
         "6"
        ],
        [
         "25",
         "79302218",
         "torch.nn.functional.softmax giving inaccurate softmax output",
         "<p>I am trying to implement masked self-attention from scratch but when calculating the softmax for the similarity scores I get odd results. I looked at the documentation and other questions posted on here but I still cant figure out what I am doing wrong. Below is a test I set up with the results.</p>\n<p>What I tried:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(sims)\nprint(torch.nn.functional.softmax(sims, dim=1))\n</code></pre>\n<p>Which gives the following output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n        [ 7.,  8.,  9., 10., 11., 12.],\n        [13., 14., 15., 16., 17., 18.],\n        [19., 20., 21., 22., 23., 24.],\n        [25., 26., 27., 28., 29., 30.],\n        [31., 32., 33., 34., 35., 36.],\n        [37., 38., 39., 40., 41., 42.],\n        [43., 44., 45., 46., 47., 48.],\n        [49., 50., 51., 52., 53., 54.],\n        [55., 56., 57., 58., 59., 60.]])\n\ntensor([[0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337]])\n</code></pre>\n<p>As an example, I am expecting the output of the <code>softmax</code> function on the first row &quot;sims&quot;</p>\n<pre><code>[ 1.,  2.,  3.,  4.,  5.,  6.]\n</code></pre>\n<p>to show</p>\n<pre><code>[ .047,  .095,  .142,  .19,  .238,  .285]\n</code></pre>\n<p>which would be the accurate softmax attention percentages needed to apply to my value tensor</p>\n",
         "2024-12-23 05:18:17",
         "0",
         "51",
         "1",
         "<python><pytorch><nlp><softmax>",
         null,
         null,
         "print(sims)\nprint(torch.nn.functional.softmax(sims, dim=1))\n---\ntensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n        [ 7.,  8.,  9., 10., 11., 12.],\n        [13., 14., 15., 16., 17., 18.],\n        [19., 20., 21., 22., 23., 24.],\n        [25., 26., 27., 28., 29., 30.],\n        [31., 32., 33., 34., 35., 36.],\n        [37., 38., 39., 40., 41., 42.],\n        [43., 44., 45., 46., 47., 48.],\n        [49., 50., 51., 52., 53., 54.],\n        [55., 56., 57., 58., 59., 60.]])\n\ntensor([[0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337]])\n---\nsoftmax\n---\n[ 1.,  2.,  3.,  4.,  5.,  6.]\n---\n[ .047,  .095,  .142,  .19,  .238,  .285]",
         "",
         "torch.nn.functional.softmax giving inaccurate softmax output",
         "I am trying to implement masked self-attention from scratch but when calculating the softmax for the similarity scores I get odd results. I looked at the documentation and other questions posted on here but I still cant figure out what I am doing wrong. Below is a test I set up with the results. What I tried: Which gives the following output: As an example, I am expecting the output of the function on the first row \"sims\" to show which would be the accurate softmax attention percentages needed to apply to my value tensor",
         "",
         "torch.nn.functional.softmax giving inaccurate softmax output I am trying to implement masked self-attention from scratch but when calculating the softmax for the similarity scores I get odd results. I looked at the documentation and other questions posted on here but I still cant figure out what I am doing wrong. Below is a test I set up with the results. What I tried: Which gives the following output: As an example, I am expecting the output of the function on the first row \"sims\" to show which would be the accurate softmax attention percentages needed to apply to my value tensor ",
         "torch.nn.functional.softmax giving inaccurate softmax output trying implement masked self-attention scratch calculating softmax similarity scores get odd results . looked documentation questions posted still cant figure wrong . test set results . tried : gives following output : example , expecting output function first row `` sims '' show would accurate softmax attention percentages needed apply value tensor",
         "7"
        ],
        [
         "26",
         "79300156",
         "Why does my contrastive learning model's loss and gradients explode during training?",
         "<p>I am fine-tuning an embedding model using contrastive learning. For the loss function, I’m using <code>torch.nn.CrossEntropyLoss</code>.</p>\n<p>The training process initially seems to work fine — the loss decreases steadily on average. However, at some point during training (usually around step 16,000 in this case), the loss and gradients explode. After this, the model is unable to stabilize, and training becomes unusable.</p>\n<p>Here is a graph showing the behavior:</p>\n<p><a href=\"https://i.sstatic.net/o0g7mnA4.png\" rel=\"nofollow noreferrer\">Tensorboard loss by step graph</a></p>\n<h4>What I have tried so far:</h4>\n<ol>\n<li><p><strong>Data preprocessing</strong>:</p>\n<ul>\n<li><p>Removed outliers (e.g., very long texts).</p>\n</li>\n<li><p>Cleaned and filtered the dataset for consistency.</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Hyperparameter tuning</strong>:</p>\n<ul>\n<li><p>Adjusted the learning rate and tried different values.</p>\n</li>\n<li><p>Changed the optimizer (e.g., switching from Adam to SGD)</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Gradient clipping</strong>:</p>\n<ul>\n<li>Clipped gradients to a max norm of 1 using <code>torch.nn.utils.clip_grad_norm_</code>.</li>\n</ul>\n</li>\n</ol>\n<h4>My setup:</h4>\n<ul>\n<li><p><strong>Dataset size</strong>: ~14,000 samples</p>\n</li>\n<li><p><strong>Model architecture</strong>: Transformer-based embedding model</p>\n</li>\n<li><p><strong>Batch size</strong>: 1 (given my gpu capacity)</p>\n</li>\n<li><p><strong>Learning rate</strong>: 1e-5</p>\n</li>\n<li><p><strong>Optimizer</strong>: Adam with weight decay</p>\n</li>\n</ul>\n<p><strong>Training loop (relevant part):</strong></p>\n<pre><code>for epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    for step, batch in enumerate(dataset_train):\n        temperature = max(0.1, 0.05 * (1 - step / num_training_steps))\n\n        # Move data to device\n        anchor_input_ids = batch[&quot;anchor_input_ids&quot;].to(device)\n        anchor_attention_mask = batch[&quot;anchor_attention_mask&quot;].to(device)\n        positive_input_ids = batch[&quot;positive_input_ids&quot;].to(device)\n        positive_attention_mask = batch[&quot;positive_attention_mask&quot;].to(device)\n        negative_input_ids = batch[&quot;negative_input_ids&quot;].to(device)\n        negative_attention_mask = batch[&quot;negative_attention_mask&quot;].to(device)\n\n        anchor_input_ids = anchor_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        anchor_attention_mask = anchor_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        positive_input_ids = positive_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        positive_attention_mask = positive_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        negative_input_ids = negative_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        negative_attention_mask = negative_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n\n        optimizer.zero_grad()\n\n        # Generate embeddings\n        anchor_embeddings = model.forward(anchor_input_ids, anchor_attention_mask)\n        positive_embeddings = model.forward(positive_input_ids, positive_attention_mask)\n        negative_embeddings = model.forward(negative_input_ids, negative_attention_mask)\n\n        # Calculate cosine similarities\n        pos_sim = cosine_similarity(anchor_embeddings, positive_embeddings)\n        neg_sim = cosine_similarity(anchor_embeddings, negative_embeddings)\n\n        # Calculate logits\n        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim.unsqueeze(1)], dim=1) / temperature\n        labels = torch.zeros(logits.size(0), dtype=torch.long).to(device)  # The positive class is always the first\n\n        # Calculate InfoNCE loss\n        loss = torch.nn.CrossEntropyLoss()(logits, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n</code></pre>\n<p><strong>Dataset generation:</strong></p>\n<pre><code>import torch\nfrom torch.utils.data import Dataset\nimport random\n\nclass FineTuneContrastiveDataset(Dataset):\n    def __init__(self, pairs_data, df_1, df_2, tokenizer, max_tokens=512):\n        &quot;&quot;&quot;\n        pairs_data: List of tuples (id_1, id_2, label).\n        df_1: DataFrame containing text data associated with id_1.\n        df_2: DataFrame containing text data associated with id_2.\n        tokenizer: Hugging Face tokenizer.\n        max_tokens: Maximum allowed length for the tokenized text.\n        &quot;&quot;&quot;\n        self.pairs_data = pairs_data\n        self.df_1 = df_1.set_index(&quot;id_1&quot;)\n        self.df_2 = df_2.set_index(&quot;id_2&quot;)\n        self.tokenizer = tokenizer\n        self.max_tokens = max_tokens\n        self.id_2_list = list(self.df_2.index)  # For selecting negative samples\n\n    def __len__(self):\n        return len(self.pairs_data)\n\n    def __getitem__(self, idx):\n        # Retrieve data from the pair\n        id_1, id_2_positive, label = self.pairs_data[idx]\n        \n        # Text associated with id_1 (anchor)\n        text_1 = &quot; &quot;.join(self.df_1.loc[id_1][&quot;chunks&quot;])\n\n        # Positive text associated with id_2\n        text_2_positive = &quot; &quot;.join(self.df_2.loc[id_2_positive][&quot;chunks&quot;])\n\n        # Generate a negative sample from id_2\n        id_2_negative = random.choice(\n            [candidate_id for candidate_id in self.id_2_list if candidate_id != id_2_positive]\n        )\n        text_2_negative = &quot; &quot;.join(self.df_2.loc[id_2_negative][&quot;chunks&quot;])\n\n        # Tokenize inputs\n        inputs_anchor = self.tokenizer(\n            text_1, truncation=True, max_length=self.max_tokens, \n            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;\n        )\n        inputs_positive = self.tokenizer(\n            text_2_positive, truncation=True, max_length=self.max_tokens, \n            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;\n        )\n        inputs_negative = self.tokenizer(\n            text_2_negative, truncation=True, max_length=self.max_tokens, \n            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;\n        )\n\n        return {\n            &quot;anchor_input_ids&quot;: inputs_anchor[&quot;input_ids&quot;].squeeze(0),\n            &quot;anchor_attention_mask&quot;: inputs_anchor[&quot;attention_mask&quot;].squeeze(0),\n            &quot;positive_input_ids&quot;: inputs_positive[&quot;input_ids&quot;].squeeze(0),\n            &quot;positive_attention_mask&quot;: inputs_positive[&quot;attention_mask&quot;].squeeze(0),\n            &quot;negative_input_ids&quot;: inputs_negative[&quot;input_ids&quot;].squeeze(0),\n            &quot;negative_attention_mask&quot;: inputs_negative[&quot;attention_mask&quot;].squeeze(0),\n            &quot;label&quot;: torch.tensor(label, dtype=torch.float),\n            &quot;id_1&quot;: id_1,\n        }\n</code></pre>\n",
         "2024-12-21 21:46:25",
         "1",
         "72",
         "1",
         "<deep-learning><pytorch><nlp><word-embedding>",
         null,
         null,
         "torch.nn.CrossEntropyLoss\n---\ntorch.nn.utils.clip_grad_norm_\n---\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    for step, batch in enumerate(dataset_train):\n        temperature = max(0.1, 0.05 * (1 - step / num_training_steps))\n\n        # Move data to device\n        anchor_input_ids = batch[\"anchor_input_ids\"].to(device)\n        anchor_attention_mask = batch[\"anchor_attention_mask\"].to(device)\n        positive_input_ids = batch[\"positive_input_ids\"].to(device)\n        positive_attention_mask = batch[\"positive_attention_mask\"].to(device)\n        negative_input_ids = batch[\"negative_input_ids\"].to(device)\n        negative_attention_mask = batch[\"negative_attention_mask\"].to(device)\n\n        anchor_input_ids = anchor_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        anchor_attention_mask = anchor_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        positive_input_ids = positive_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        positive_attention_mask = positive_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        negative_input_ids = negative_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        negative_attention_mask = negative_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n\n        optimizer.zero_grad()\n\n        # Generate embeddings\n        anchor_embeddings = model.forward(anchor_input_ids, anchor_attention_mask)\n        positive_embeddings = model.forward(positive_input_ids, positive_attention_mask)\n        negative_embeddings = model.forward(negative_input_ids, negative_attention_mask)\n\n        # Calculate cosine similarities\n        pos_sim = cosine_similarity(anchor_embeddings, positive_embeddings)\n        neg_sim = cosine_similarity(anchor_embeddings, negative_embeddings)\n\n        # Calculate logits\n        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim.unsqueeze(1)], dim=1) / temperature\n        labels = torch.zeros(logits.size(0), dtype=torch.long).to(device)  # The positive class is always the first\n\n        # Calculate InfoNCE loss\n        loss = torch.nn.CrossEntropyLoss()(logits, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n---\nimport torch\nfrom torch.utils.data import Dataset\nimport random\n\nclass FineTuneContrastiveDataset(Dataset):\n    def __init__(self, pairs_data, df_1, df_2, tokenizer, max_tokens=512):\n        \"\"\"\n        pairs_data: List of tuples (id_1, id_2, label).\n        df_1: DataFrame containing text data associated with id_1.\n        df_2: DataFrame containing text data associated with id_2.\n        tokenizer: Hugging Face tokenizer.\n        max_tokens: Maximum allowed length for the tokenized text.\n        \"\"\"\n        self.pairs_data = pairs_data\n        self.df_1 = df_1.set_index(\"id_1\")\n        self.df_2 = df_2.set_index(\"id_2\")\n        self.tokenizer = tokenizer\n        self.max_tokens = max_tokens\n        self.id_2_list = list(self.df_2.index)  # For selecting negative samples\n\n    def __len__(self):\n        return len(self.pairs_data)\n\n    def __getitem__(self, idx):\n        # Retrieve data from the pair\n        id_1, id_2_positive, label = self.pairs_data[idx]\n        \n        # Text associated with id_1 (anchor)\n        text_1 = \" \".join(self.df_1.loc[id_1][\"chunks\"])\n\n        # Positive text associated with id_2\n        text_2_positive = \" \".join(self.df_2.loc[id_2_positive][\"chunks\"])\n\n        # Generate a negative sample from id_2\n        id_2_negative = random.choice(\n            [candidate_id for candidate_id in self.id_2_list if candidate_id != id_2_positive]\n        )\n        text_2_negative = \" \".join(self.df_2.loc[id_2_negative][\"chunks\"])\n\n        # Tokenize inputs\n        inputs_anchor = self.tokenizer(\n            text_1, truncation=True, max_length=self.max_tokens, \n            padding=\"max_length\", return_tensors=\"pt\"\n        )\n        inputs_positive = self.tokenizer(\n            text_2_positive, truncation=True, max_length=self.max_tokens, \n            padding=\"max_length\", return_tensors=\"pt\"\n        )\n        inputs_negative = self.tokenizer(\n            text_2_negative, truncation=True, max_length=self.max_tokens, \n            padding=\"max_length\", return_tensors=\"pt\"\n        )\n\n        return {\n            \"anchor_input_ids\": inputs_anchor[\"input_ids\"].squeeze(0),\n            \"anchor_attention_mask\": inputs_anchor[\"attention_mask\"].squeeze(0),\n            \"positive_input_ids\": inputs_positive[\"input_ids\"].squeeze(0),\n            \"positive_attention_mask\": inputs_positive[\"attention_mask\"].squeeze(0),\n            \"negative_input_ids\": inputs_negative[\"input_ids\"].squeeze(0),\n            \"negative_attention_mask\": inputs_negative[\"attention_mask\"].squeeze(0),\n            \"label\": torch.tensor(label, dtype=torch.float),\n            \"id_1\": id_1,\n        }",
         "",
         "Why does my contrastive learning model's loss and gradients explode during training?",
         "I am fine-tuning an embedding model using contrastive learning. For the loss function, Im using . The training process initially seems to work fine the loss decreases steadily on average. However, at some point during training (usually around step 16,000 in this case), the loss and gradients explode. After this, the model is unable to stabilize, and training becomes unusable. Here is a graph showing the behavior: Tensorboard loss by step graph What I have tried so far: Data preprocessing : Removed outliers (e.g., long texts). Cleaned and filtered the dataset for consistency. Hyperparameter tuning : Adjusted the learning rate and tried different values. Changed the optimizer (e.g., switching from Adam to SGD) Gradient clipping : Clipped gradients to a max norm of 1 using . My setup: Dataset size : ~14,000 samples Model architecture : Transformer-based embedding model Batch size : 1 (given my gpu capacity) Learning rate : 1e-5 Optimizer : Adam with weight decay Training loop (relevant part): Dataset generation:",
         "",
         "Why does my contrastive learning model's loss and gradients explode during training? I am fine-tuning an embedding model using contrastive learning. For the loss function, Im using . The training process initially seems to work fine the loss decreases steadily on average. However, at some point during training (usually around step 16,000 in this case), the loss and gradients explode. After this, the model is unable to stabilize, and training becomes unusable. Here is a graph showing the behavior: Tensorboard loss by step graph What I have tried so far: Data preprocessing : Removed outliers (e.g., long texts). Cleaned and filtered the dataset for consistency. Hyperparameter tuning : Adjusted the learning rate and tried different values. Changed the optimizer (e.g., switching from Adam to SGD) Gradient clipping : Clipped gradients to a max norm of 1 using . My setup: Dataset size : ~14,000 samples Model architecture : Transformer-based embedding model Batch size : 1 (given my gpu capacity) Learning rate : 1e-5 Optimizer : Adam with weight decay Training loop (relevant part): Dataset generation: ",
         "contrastive learning model 's loss gradients explode training ? fine-tuning embedding model using contrastive learning . loss function , im using . training process initially seems work fine loss decreases steadily average . however , point training ( usually around step 16,000 case ) , loss gradients explode . , model unable stabilize , training becomes unusable . graph showing behavior : tensorboard loss step graph tried far : data preprocessing : removed outliers ( e.g. , long texts ) . cleaned filtered dataset consistency . hyperparameter tuning : adjusted learning rate tried different values . changed optimizer ( e.g. , switching adam sgd ) gradient clipping : clipped gradients max norm 1 using . setup : dataset size : ~14,000 samples model architecture : transformer-based embedding model batch size : 1 ( given gpu capacity ) learning rate : 1e-5 optimizer : adam weight decay training loop ( relevant part ) : dataset generation :",
         "7"
        ],
        [
         "27",
         "79298368",
         "Inspect all probabilities of BERTopic model",
         "<p>Say I build a BERTopic model using</p>\n<pre><code>from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre>\n<p>Inspecting <code>probs</code> gives me just a single value for each item in <code>docs</code>.</p>\n<pre><code>probs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n</code></pre>\n<p>I would like the entire probability vector across all topics (so in this case, where <code>nr_topics=20</code>, I want a vector of 20 probabilities for each item in <code>docs</code>). In other words, if I have N items in <code>docs</code> and K topics, I would like an NxK output.</p>\n",
         "2024-12-20 20:49:34",
         "1",
         "41",
         "1",
         "<python><nlp><topic-modeling>",
         "79299703.0",
         "<p>For individual topic probability across each document you need to add one more argument.</p>\n<pre><code>topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n</code></pre>\n<p>Note: This calculate_probabilities = True will only work if you are using <strong><code>HDBSCAN</code></strong> clustering embedding model. And Bertopic by default uses <code>all-MiniLM-L6-v2</code>.</p>\n<p><strong>Official documentation:</strong> <a href=\"https://maartengr.github.io/BERTopic/api/bertopic.html\" rel=\"nofollow noreferrer\">https://maartengr.github.io/BERTopic/api/bertopic.html</a></p>\n<p>They have mentioned the same in document as well.</p>\n",
         "from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n---\nprobs\n---\ndocs\n---\nprobs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n---\nnr_topics=20\n---\ndocs\n---\ndocs",
         "topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n---\nHDBSCAN\n---\nall-MiniLM-L6-v2",
         "Inspect all probabilities of BERTopic model",
         "Say I build a BERTopic model using Inspecting gives me just a single value for each item in . I would like the entire probability vector across all topics (so in this case, where , I want a vector of 20 probabilities for each item in ). In other words, if I have N items in and K topics, I would like an NxK output.",
         "For individual topic probability across each document you need to add one more argument. Note: This calculate_probabilities = True will only work if you are using clustering embedding model. And Bertopic by default uses . Official documentation: They have mentioned the same in document as well.",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in . I would like the entire probability vector across all topics (so in this case, where , I want a vector of 20 probabilities for each item in ). In other words, if I have N items in and K topics, I would like an NxK output. For individual topic probability across each document you need to add one more argument. Note: This calculate_probabilities = True will only work if you are using clustering embedding model. And Bertopic by default uses . Official documentation: They have mentioned the same in document as well.",
         "inspect probabilities bertopic model say build bertopic model using inspecting gives single value item . would like entire probability vector across topics ( case , , want vector 20 probabilities item ) . words , n items k topics , would like nxk output . individual topic probability across document need add one argument . note : calculate_probabilities = true work using clustering embedding model . bertopic default uses . official documentation : mentioned document well .",
         "7"
        ],
        [
         "28",
         "79293919",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>\n<p>For example:\nA sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>\n<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>\n<p>Any and all help is welcome and appreciated.</p>\n<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>\n",
         "2024-12-19 10:24:04",
         "0",
         "62",
         "2",
         "<python><nlp><nltk><detection>",
         "79294074.0",
         "<p>You need a external dataset for this task. You can try dataset such as google n gram dataset.</p>\n<p>Here is the breakdown of the problem statement:</p>\n<ol>\n<li>Input: &quot;I enjoy a cold glass of water on a hot day&quot;. <code>Output</code>: &quot;water&quot;.</li>\n<li>Split the sentences into words list.</li>\n</ol>\n<blockquote>\n<p>Example: [&quot;I&quot;, &quot;enjoy&quot;, &quot;a&quot;, &quot;cold&quot;, &quot;glass&quot;, &quot;of&quot;, &quot;water&quot;, &quot;on&quot;,\n&quot;a&quot;, &quot;hot&quot;, &quot;day&quot;]</p>\n</blockquote>\n<ol start=\"3\">\n<li>First loop in through all the word of the sentences. so let say you are at first word &quot;I&quot;.</li>\n<li>Now you will look the same word &quot;I&quot; in external dataset and will look for the frequency of that word.\nLet say the word &quot;I&quot; in external dataset is repeated <code>5000000</code> times</li>\n<li>Repeat this task for all the word.</li>\n<li>Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.\nFrequency in the below example is random value not exact value.</li>\n</ol>\n<blockquote>\n<pre><code>{\n    &quot;I&quot;: 5000000,\n    &quot;enjoy&quot;: 50000,\n    &quot;a&quot;: 10000000,\n    &quot;cold&quot;: 30000,\n    &quot;glass&quot;: 100000,\n    &quot;of&quot;: 8000000,\n    &quot;water&quot;: 1200000,\n    &quot;on&quot;: 6000000,\n    &quot;hot&quot;: 700000,\n    &quot;day&quot;: 400000\n}\n</code></pre>\n</blockquote>\n<ol start=\"7\">\n<li>Pick the word with highest frequency.</li>\n</ol>\n<p>Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself</p>\n",
         "",
         "Output\n---\n5000000\n---\n{\n    \"I\": 5000000,\n    \"enjoy\": 50000,\n    \"a\": 10000000,\n    \"cold\": 30000,\n    \"glass\": 100000,\n    \"of\": 8000000,\n    \"water\": 1200000,\n    \"on\": 6000000,\n    \"hot\": 700000,\n    \"day\": 400000\n}",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do. For example: A sentence \"I enjoy a cold glass of water on a hot day\" would return \"water\" because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations. I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route Any and all help is welcome and appreciated. For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.",
         "You need a external dataset for this task. You can try dataset such as google n gram dataset. Here is the breakdown of the problem statement: Input: \"I enjoy a cold glass of water on a hot day\". : \"water\". Split the sentences into words list. Example: \"I\", \"enjoy\", \"a\", \"cold\", \"glass\", \"of\", \"water\", \"on\", \"a\", \"hot\", \"day\" First loop in through all the word of the sentences. so let say you are at first word \"I\". Now you will look the same word \"I\" in external dataset and will look for the frequency of that word. Let say the word \"I\" in external dataset is repeated times Repeat this task for all the word. Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data. Frequency in the below example is random value not exact value. Pick the word with highest frequency. Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do. For example: A sentence \"I enjoy a cold glass of water on a hot day\" would return \"water\" because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations. I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route Any and all help is welcome and appreciated. For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess. You need a external dataset for this task. You can try dataset such as google n gram dataset. Here is the breakdown of the problem statement: Input: \"I enjoy a cold glass of water on a hot day\". : \"water\". Split the sentences into words list. Example: \"I\", \"enjoy\", \"a\", \"cold\", \"glass\", \"of\", \"water\", \"on\", \"a\", \"hot\", \"day\" First loop in through all the word of the sentences. so let say you are at first word \"I\". Now you will look the same word \"I\" in external dataset and will look for the frequency of that word. Let say the word \"I\" in external dataset is repeated times Repeat this task for all the word. Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data. Frequency in the below example is random value not exact value. Pick the word with highest frequency. Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself",
         "determining popular words english dictionary within dictionary words forgive wording awful , 'm trying figure determine used words english language set words dictionary 've made . 've done research nltk ca n't seem find function within ( library matter ) help need . example : sentence `` enjoy cold glass water hot day '' would return `` water '' 's used word day day conversation sentence . essentially need returned value frequently used word conversations . figure 'll likely involve ai , time 've tried use ai wind copy pasting code n't understand , 'm trying avoid going route help welcome appreciated . context , decided start project would essentially guess predetermined word based characters user says n't computers guess . need external dataset task . try dataset google n gram dataset . breakdown problem statement : input : `` enjoy cold glass water hot day '' . : `` water '' . split sentences words list . example : `` '' , `` enjoy '' , `` '' , `` cold '' , `` glass '' , `` '' , `` water '' , `` '' , `` '' , `` hot '' , `` day '' first loop word sentences . let say first word `` '' . look word `` '' external dataset look frequency word . let say word `` '' external dataset repeated times repeat task word . dictionary word sentence key value frequency word get external data . frequency example random value exact value . pick word highest frequency . note : try big corpus external data . using big corpus english word used conversation . even frequency mentioned create",
         "3"
        ],
        [
         "29",
         "79293889",
         "catelog sentences into 5 words that represent them",
         "<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>\n<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>\n<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>\n<p>I will glad for recomendations how to do that</p>\n<p><strong>edit</strong></p>\n<p>represnt = the semantic distance between the word to the text.</p>\n<p>for example -\nlets say in row 1 the text is &quot;i want to eat&quot;\nand I have 2 words : food and house.</p>\n<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>\n",
         "2024-12-19 10:16:47",
         "0",
         "53",
         "1",
         "<python><pandas><nlp><text-mining><similarity>",
         "79294099.0",
         "<p>You could use a pre-trained sentence transformer model from <a href=\"https://pypi.org/project/sentence-transformers/\" rel=\"nofollow noreferrer\"><code>sentence_transformers</code></a>:</p>\n<pre><code>import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -&gt; None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -&gt; list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -&gt; pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838\n</code></pre>\n",
         "df['text']\n---\ndf[\"word1\"]\n---\ndf[\"word2\"]\n---\ndf[\"food \"]\n---\ndf[\"house\"]",
         "sentence_transformers\n---\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -> None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -> list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -> pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n---\ntext  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838",
         "catelog sentences into 5 words that represent them",
         "I have dataframe with 1000 text rows. I also have 5 words that I want to know for each one of them how much they represnt the text (between 0 to 1) every score will be in , and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text. for example - lets say in row 1 the text is \"i want to eat\" and I have 2 words : food and house. so in it would be higher score than in",
         "You could use a pre-trained sentence transformer model from : Output:",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows. I also have 5 words that I want to know for each one of them how much they represnt the text (between 0 to 1) every score will be in , and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text. for example - lets say in row 1 the text is \"i want to eat\" and I have 2 words : food and house. so in it would be higher score than in You could use a pre-trained sentence transformer model from : Output:",
         "catelog sentences 5 words represent dataframe 1000 text rows . also 5 words want know one much represnt text ( 0 1 ) every score , etc glad recomendations edit represnt = semantic distance word text . example - lets say row 1 text `` want eat '' 2 words : food house . would higher score could use pre-trained sentence transformer model : output :",
         "3"
        ],
        [
         "30",
         "79287799",
         "How to correctly identify entity types for tokens using spaCy using python?",
         "<p>I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this.</p>\n<p>Here is the code I am using:</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\ndef getPayeeName(description):\n    description = description.replace(&quot;-&quot;, &quot; &quot;).replace(&quot;/&quot;, &quot; &quot;).strip()\n    doc = nlp(description)\n\n    for token in doc:\n        print(f&quot;Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else 'None'}&quot;)\n\n# Example input\ndescription = &quot;UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 MEDICAL LTD HDFC 50200&quot;\ngetPayeeName(description)\n</code></pre>\n<p>Token: UPI, Entity: ORG</p>\n<p>Token: DR, Entity: ORG</p>\n<p>Token: 400874707203, Entity: None</p>\n<p>Token: BENGALORE, Entity: None</p>\n<p>Token: 08, Entity: DATE</p>\n<p>Token: JAN, Entity: DATE</p>\n<p>Token: 2024, Entity: DATE</p>\n<p>Token: 14:38:56, Entity: None</p>\n<p>Token: MEDICAL, Entity: ORG</p>\n<p>Token: LTD, Entity: ORG</p>\n<p>Token: HDFC, Entity: ORG</p>\n<p>Token: 50200, Entity: ORG</p>\n<ul>\n<li><p>50200 is identified as ORG, but it is just a number.</p>\n</li>\n<li><p>BENGALORE is a city, but it is not recognized as a GPE or location\n(returns None).</p>\n</li>\n<li><p>UPI and DR are acronyms/abbreviations, but they are incorrectly\nidentified as ORG.</p>\n</li>\n</ul>\n<p>I want the entity recognition to be more accurate and reliable.\nHow can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition?</p>\n<p>Note: I tried ChatGPT as well, but still this issue is not solved.</p>\n",
         "2024-12-17 12:09:49",
         "1",
         "44",
         "1",
         "<python><machine-learning><nlp><spacy>",
         null,
         null,
         "import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef getPayeeName(description):\n    description = description.replace(\"-\", \" \").replace(\"/\", \" \").strip()\n    doc = nlp(description)\n\n    for token in doc:\n        print(f\"Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else 'None'}\")\n\n# Example input\ndescription = \"UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 MEDICAL LTD HDFC 50200\"\ngetPayeeName(description)",
         "",
         "How to correctly identify entity types for tokens using spaCy using python?",
         "I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this. Here is the code I am using: Token: UPI, Entity: ORG Token: DR, Entity: ORG Token: 400874707203, Entity: None Token: BENGALORE, Entity: None Token: 08, Entity: DATE Token: JAN, Entity: DATE Token: 2024, Entity: DATE Token: 14:38:56, Entity: None Token: MEDICAL, Entity: ORG Token: LTD, Entity: ORG Token: HDFC, Entity: ORG Token: 50200, Entity: ORG 50200 is identified as ORG, but it is just a number. BENGALORE is a city, but it is not recognized as a GPE or location (returns None). UPI and DR are acronyms/abbreviations, but they are incorrectly identified as ORG. I want the entity recognition to be more accurate and reliable. How can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition? Note: I tried ChatGPT as well, but still this issue is not solved.",
         "",
         "How to correctly identify entity types for tokens using spaCy using python? I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this. Here is the code I am using: Token: UPI, Entity: ORG Token: DR, Entity: ORG Token: 400874707203, Entity: None Token: BENGALORE, Entity: None Token: 08, Entity: DATE Token: JAN, Entity: DATE Token: 2024, Entity: DATE Token: 14:38:56, Entity: None Token: MEDICAL, Entity: ORG Token: LTD, Entity: ORG Token: HDFC, Entity: ORG Token: 50200, Entity: ORG 50200 is identified as ORG, but it is just a number. BENGALORE is a city, but it is not recognized as a GPE or location (returns None). UPI and DR are acronyms/abbreviations, but they are incorrectly identified as ORG. I want the entity recognition to be more accurate and reliable. How can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition? Note: I tried ChatGPT as well, but still this issue is not solved. ",
         "correctly identify entity types tokens using spacy using python ? 'm using spacy extract identify entity types ( like org , gpe , date , etc . ) text description . however , noticing incorrect results , 'm unsure fix . code using : token : upi , entity : org token : dr , entity : org token : 400874707203 , entity : none token : bengalore , entity : none token : 08 , entity : date token : jan , entity : date token : 2024 , entity : date token : 14:38:56 , entity : none token : medical , entity : org token : ltd , entity : org token : hdfc , entity : org token : 50200 , entity : org 50200 identified org , number . bengalore city , recognized gpe location ( returns none ) . upi dr acronyms/abbreviations , incorrectly identified org . want entity recognition accurate reliable . fix issues ? additional spacy configurations , custom rules , pre-trained models use improve entity recognition ? note : tried chatgpt well , still issue solved .",
         "0"
        ],
        [
         "31",
         "79283846",
         "--user-dir in Fairseq in failing",
         "<p>I’m trying to fine-tune the IndicTrans2 model using fairseq-train, but I keep encountering the following error:</p>\n<p>fairseq-train: error: argument --user-dir: invalid Optional value: 'C:/Users/sasid/Downloads/en-indic-exp/model_configs'</p>\n<p>I’ve provided the --user-dir argument as the path to the model_configs directory (e.g., --user-dir C:/Users/sasid/Downloads/IndicTrans2/model_configs), but the training script fails with the above error.</p>\n<p>I figured out that the problem is it's not taking path in windows convention with backslashes instead it is taking forward slashes so it is failing with that error.</p>\n<p>So how can I make it take the path in windows convention?</p>\n",
         "2024-12-16 07:03:15",
         "1",
         "30",
         "1",
         "<python><nlp><huggingface-transformers><large-language-model><fairseq>",
         null,
         null,
         "",
         "",
         "--user-dir in Fairseq in failing",
         "Im trying to fine-tune the IndicTrans2 model using fairseq-train, but I keep encountering the following error: fairseq-train: error: argument --user-dir: invalid Optional value: 'C:/Users/sasid/Downloads/en-indic-exp/model_configs' Ive provided the --user-dir argument as the path to the model_configs directory (e.g., --user-dir C:/Users/sasid/Downloads/IndicTrans2/model_configs), but the training script fails with the above error. I figured out that the problem is it's not taking path in windows convention with backslashes instead it is taking forward slashes so it is failing with that error. So how can I make it take the path in windows convention?",
         "",
         "--user-dir in Fairseq in failing Im trying to fine-tune the IndicTrans2 model using fairseq-train, but I keep encountering the following error: fairseq-train: error: argument --user-dir: invalid Optional value: 'C:/Users/sasid/Downloads/en-indic-exp/model_configs' Ive provided the --user-dir argument as the path to the model_configs directory (e.g., --user-dir C:/Users/sasid/Downloads/IndicTrans2/model_configs), but the training script fails with the above error. I figured out that the problem is it's not taking path in windows convention with backslashes instead it is taking forward slashes so it is failing with that error. So how can I make it take the path in windows convention? ",
         "-- user-dir fairseq failing im trying fine-tune indictrans2 model using fairseq-train , keep encountering following error : fairseq-train : error : argument -- user-dir : invalid optional value : ' c : /users/sasid/downloads/en-indic-exp/model_configs ' ive provided -- user-dir argument path model_configs directory ( e.g. , -- user-dir c : /users/sasid/downloads/indictrans2/model_configs ) , training script fails error . figured problem 's taking path windows convention backslashes instead taking forward slashes failing error . make take path windows convention ?",
         "2"
        ],
        [
         "32",
         "79279045",
         "Recommending a pre-train NER model for geospatial entities",
         "<p>I am trying to find the best pre-trained Hugging Face Transformer model exclusively dedicated to geospatial or location entities to extract location entities in English from a text. Does it work way better than roberta-large?</p>\n",
         "2024-12-13 16:53:40",
         "1",
         "34",
         "1",
         "<nlp><geolocation><pipeline><geospatial><huggingface-transformers>",
         null,
         null,
         "",
         "",
         "Recommending a pre-train NER model for geospatial entities",
         "I am trying to find the best pre-trained Hugging Face Transformer model exclusively dedicated to geospatial or location entities to extract location entities in English from a text. Does it work way better than roberta-large?",
         "",
         "Recommending a pre-train NER model for geospatial entities I am trying to find the best pre-trained Hugging Face Transformer model exclusively dedicated to geospatial or location entities to extract location entities in English from a text. Does it work way better than roberta-large? ",
         "recommending pre-train ner model geospatial entities trying find best pre-trained hugging face transformer model exclusively dedicated geospatial location entities extract location entities english text . work way better roberta-large ?",
         "8"
        ],
        [
         "33",
         "79274184",
         "How to split and spelling correct arabic text without spaces into list of words",
         "<p>I'm looking for a way to split the Arabic text and correct the spelling. Whitespace is the first splitting criterion, then, Maybe based on a dictionary of correct words the splitting should done, considering spelling issues:</p>\n<pre><code>تشرابالقطط الحليب =&gt; [تشرب، القطط، الحليب]\n\nمخمديوسف =&gt; [&quot;محمد&quot;, &quot;يوسف&quot;]\n\nانا ارييدان اشراب =&gt; [&quot;انا&quot;, &quot;أريد&quot;, &quot;أن&quot;, &quot;أشرب&quot;]\n\nجملة صحيحة =&gt; [&quot;جملة&quot;, &quot;صحيحة&quot;]\n</code></pre>\n<p>And if there are multiple correct splitting ways, return them all:</p>\n<pre><code>مخمديوسف =&gt; [ [&quot;محمد&quot;, &quot;يوسف&quot;] , [&quot;احمد&quot;, &quot;يوسف&quot;] ]\n</code></pre>\n<p>If any libraries can do the same. Otherwise, A custom algorithm/code that we can implement?</p>\n",
         "2024-12-12 07:34:49",
         "3",
         "69",
         "2",
         "<algorithm><deep-learning><nlp>",
         null,
         null,
         "تشرابالقطط الحليب => [تشرب، القطط، الحليب]\n\nمخمديوسف => [\"محمد\", \"يوسف\"]\n\nانا ارييدان اشراب => [\"انا\", \"أريد\", \"أن\", \"أشرب\"]\n\nجملة صحيحة => [\"جملة\", \"صحيحة\"]\n---\nمخمديوسف => [ [\"محمد\", \"يوسف\"] , [\"احمد\", \"يوسف\"] ]",
         "",
         "How to split and spelling correct arabic text without spaces into list of words",
         "I'm looking for a way to split the Arabic text and correct the spelling. Whitespace is the first splitting criterion, then, Maybe based on a dictionary of correct words the splitting should done, considering spelling issues: And if there are multiple correct splitting ways, return them all: If any libraries can do the same. Otherwise, A custom algorithm/code that we can implement?",
         "",
         "How to split and spelling correct arabic text without spaces into list of words I'm looking for a way to split the Arabic text and correct the spelling. Whitespace is the first splitting criterion, then, Maybe based on a dictionary of correct words the splitting should done, considering spelling issues: And if there are multiple correct splitting ways, return them all: If any libraries can do the same. Otherwise, A custom algorithm/code that we can implement? ",
         "split spelling correct arabic text without spaces list words 'm looking way split arabic text correct spelling . whitespace first splitting criterion , , maybe based dictionary correct words splitting done , considering spelling issues : multiple correct splitting ways , return : libraries . otherwise , custom algorithm/code implement ?",
         "9"
        ],
        [
         "34",
         "79264247",
         "similarity from word to sentence after doing words Embedding",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did word2vec .</p>\n<p>Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word &quot;king&quot;.</p>\n<p>I thought about taking in each sentence the 4 closet words to the word king  and make average of them.\nmaybe by using <code>model.wv.similarity</code>.\nthe avg of each sentnce will be in the field df['king']</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n<p>example data:</p>\n<pre><code>    data = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\ndf['text']=df['text'].str.split()    \n\nmodel = Word2Vec(df['text'], vector_size=100, window=2, min_count=1 )\n\nmodel.wv.similarity('Knights','king')\n</code></pre>\n<p><strong>edit</strong>:</p>\n<p>My mission is:</p>\n<p>I have 1000 text rows (people that complain about something)\nI want to catalog them into 4 words.\nLets say that word 1 is king. Word 2 is castle…\nI want to know about each sentence which word from the  4 words most represent the sentence.\nIn order to do that I thought about taking each word from the 4 words and calculate <code>model.wv.similarity</code> to all of the words in  df['text'].\nAfter that, for each sentence, take the 3  words that have  the highest score to word king  (and to the word  castle and ets..)  .\ncalculate mean of the 3 highest score and that would be the value of df['king'] for the sentence</p>\n",
         "2024-12-09 08:14:04",
         "0",
         "64",
         "1",
         "<python><nlp><text-mining><word2vec><similarity>",
         null,
         null,
         "model.wv.similarity\n---\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\ndf['text']=df['text'].str.split()    \n\nmodel = Word2Vec(df['text'], vector_size=100, window=2, min_count=1 )\n\nmodel.wv.similarity('Knights','king')\n---\nmodel.wv.similarity",
         "",
         "similarity from word to sentence after doing words Embedding",
         "I have dataframe with 1000 text rows. I did word2vec . Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word \"king\". I thought about taking in each sentence the 4 closet words to the word king and make average of them. maybe by using . the avg of each sentnce will be in the field df'king' I will glad to know how to do that or to hear about another method. example data: edit : My mission is: I have 1000 text rows (people that complain about something) I want to catalog them into 4 words. Lets say that word 1 is king. Word 2 is castle I want to know about each sentence which word from the 4 words most represent the sentence. In order to do that I thought about taking each word from the 4 words and calculate to all of the words in df'text'. After that, for each sentence, take the 3 words that have the highest score to word king (and to the word castle and ets..) . calculate mean of the 3 highest score and that would be the value of df'king' for the sentence",
         "",
         "similarity from word to sentence after doing words Embedding I have dataframe with 1000 text rows. I did word2vec . Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word \"king\". I thought about taking in each sentence the 4 closet words to the word king and make average of them. maybe by using . the avg of each sentnce will be in the field df'king' I will glad to know how to do that or to hear about another method. example data: edit : My mission is: I have 1000 text rows (people that complain about something) I want to catalog them into 4 words. Lets say that word 1 is king. Word 2 is castle I want to know about each sentence which word from the 4 words most represent the sentence. In order to do that I thought about taking each word from the 4 words and calculate to all of the words in df'text'. After that, for each sentence, take the 3 words that have the highest score to word king (and to the word castle and ets..) . calculate mean of the 3 highest score and that would be the value of df'king' for the sentence ",
         "similarity word sentence words embedding dataframe 1000 text rows . word2vec . want create new field give distance sentence word want , lets say word `` king '' . thought taking sentence 4 closet words word king make average . maybe using . avg sentnce field df'king ' glad know hear another method . example data : edit : mission : 1000 text rows ( people complain something ) want catalog 4 words . lets say word 1 king . word 2 castle want know sentence word 4 words represent sentence . order thought taking word 4 words calculate words df'text ' . , sentence , take 3 words highest score word king ( word castle ets .. ) . calculate mean 3 highest score would value df'king ' sentence",
         "5"
        ],
        [
         "35",
         "79260748",
         "How to install spacy?",
         "<p>I am using trying to install spacy library using 'pip install -U spacy' in the command prompt (run as admin) in Windows-11 O.S., but it shows some error I don't understand. I am using Python 3.13.0, gcc 13.2.0 and make 4.4.1. What could be the problem? Or is there any other way to install spacy?</p>\n<pre class=\"lang-none prettyprint-override\"><code>C:\\&gt;pip install -U spacy\nCollecting spacy\n  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n\n  × pip subprocess to install build dependencies did not run successfully.\n  │ exit code: 1\n  ╰─&gt; [113 lines of output]\n      Ignoring numpy: markers 'python_version &lt; &quot;3.9&quot;' don't match your environment\n      Collecting setuptools\n        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n      Collecting cython&lt;3.0,&gt;=0.25\n        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n      Collecting cymem&lt;2.1.0,&gt;=2.0.2\n        Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n      Collecting preshed&lt;3.1.0,&gt;=3.0.2\n        Using cached preshed-3.0.9.tar.gz (14 kB)\n        Installing build dependencies: started\n        Installing build dependencies: finished with status 'done'\n        Getting requirements to build wheel: started\n        Getting requirements to build wheel: finished with status 'done'\n        Preparing metadata (pyproject.toml): started\n        Preparing metadata (pyproject.toml): finished with status 'done'\n      Collecting murmurhash&lt;1.1.0,&gt;=0.28.0\n        Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n      Collecting thinc&lt;8.4.0,&gt;=8.3.0\n        Using cached thinc-8.3.2.tar.gz (193 kB)\n        Installing build dependencies: started\n        Installing build dependencies: still running...\n        Installing build dependencies: finished with status 'error'\n        error: subprocess-exited-with-error\n\n        pip subprocess to install build dependencies did not run successfully.\n        exit code: 1\n\n        [74 lines of output]\n        Ignoring numpy: markers 'python_version &lt; &quot;3.9&quot;' don't match your environment\n        Collecting setuptools\n          Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n        Collecting cython&lt;3.0,&gt;=0.25\n          Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n        Collecting murmurhash&lt;1.1.0,&gt;=1.0.2\n          Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n        Collecting cymem&lt;2.1.0,&gt;=2.0.2\n          Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n        Collecting preshed&lt;3.1.0,&gt;=3.0.2\n          Using cached preshed-3.0.9.tar.gz (14 kB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting blis&lt;1.1.0,&gt;=1.0.0\n          Using cached blis-1.0.1.tar.gz (3.6 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting numpy&lt;2.1.0,&gt;=2.0.0\n          Using cached numpy-2.0.2.tar.gz (18.9 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Installing backend dependencies: started\n          Installing backend dependencies: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'error'\n          error: subprocess-exited-with-error\n\n          Preparing metadata (pyproject.toml) did not run successfully.\n          exit code: 1\n\n          [22 lines of output]\n          + C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\vendored-meson\\meson\\meson.py setup C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-python-native-file.ini\n          The Meson build system\n          Version: 1.4.99\n          Source dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\n          Build dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\n          Build type: native build\n          Project name: NumPy\n          Project version: 2.0.2\n          C compiler for the host machine: gcc (gcc 13.2.0 &quot;gcc (GCC) 13.2.0&quot;)\n          C linker for the host machine: gcc ld.bfd 2.41\n          C++ compiler for the host machine: c++ (gcc 6.3.0 &quot;c++ (MinGW.org GCC-6.3.0-1) 6.3.0&quot;)\n          C++ linker for the host machine: c++ ld.bfd 2.28\n          Cython compiler for the host machine: cython (cython 3.0.11)\n          Host machine cpu family: x86\n          Host machine cpu: x86\n          Program python found: YES (C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe)\n          Need python for x86, but found x86_64\n          Run-time dependency python found: NO (tried sysconfig)\n\n          ..\\meson.build:41:12: ERROR: Python dependency not found\n\n          A full log can be found at C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-logs\\meson-log.txt\n          [end of output]\n\n          note: This error originates from a subprocess, and is likely not a problem with pip.\n        error: metadata-generation-failed\n\n        Encountered error while generating package metadata.\n\n        See above for output.\n\n        note: This is an issue with the package mentioned above, not pip.\n        hint: See above for details.\n        [end of output]\n\n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: subprocess-exited-with-error\n\n      pip subprocess to install build dependencies did not run successfully.\n      exit code: 1\n\n      See above for output.\n\n      note: This error originates from a subprocess, and is likely not a problem with pip.\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n× pip subprocess to install build dependencies did not run successfully.\n│ exit code: 1\n╰─&gt; See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n</code></pre>\n",
         "2024-12-07 14:07:54",
         "0",
         "718",
         "4",
         "<python><pip><nlp><spacy>",
         null,
         null,
         "C:\\>pip install -U spacy\nCollecting spacy\n  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n\n  × pip subprocess to install build dependencies did not run successfully.\n  │ exit code: 1\n  ╰─> [113 lines of output]\n      Ignoring numpy: markers 'python_version < \"3.9\"' don't match your environment\n      Collecting setuptools\n        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n      Collecting cython<3.0,>=0.25\n        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n      Collecting cymem<2.1.0,>=2.0.2\n        Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n      Collecting preshed<3.1.0,>=3.0.2\n        Using cached preshed-3.0.9.tar.gz (14 kB)\n        Installing build dependencies: started\n        Installing build dependencies: finished with status 'done'\n        Getting requirements to build wheel: started\n        Getting requirements to build wheel: finished with status 'done'\n        Preparing metadata (pyproject.toml): started\n        Preparing metadata (pyproject.toml): finished with status 'done'\n      Collecting murmurhash<1.1.0,>=0.28.0\n        Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n      Collecting thinc<8.4.0,>=8.3.0\n        Using cached thinc-8.3.2.tar.gz (193 kB)\n        Installing build dependencies: started\n        Installing build dependencies: still running...\n        Installing build dependencies: finished with status 'error'\n        error: subprocess-exited-with-error\n\n        pip subprocess to install build dependencies did not run successfully.\n        exit code: 1\n\n        [74 lines of output]\n        Ignoring numpy: markers 'python_version < \"3.9\"' don't match your environment\n        Collecting setuptools\n          Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n        Collecting cython<3.0,>=0.25\n          Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n        Collecting murmurhash<1.1.0,>=1.0.2\n          Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n        Collecting cymem<2.1.0,>=2.0.2\n          Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n        Collecting preshed<3.1.0,>=3.0.2\n          Using cached preshed-3.0.9.tar.gz (14 kB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting blis<1.1.0,>=1.0.0\n          Using cached blis-1.0.1.tar.gz (3.6 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting numpy<2.1.0,>=2.0.0\n          Using cached numpy-2.0.2.tar.gz (18.9 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Installing backend dependencies: started\n          Installing backend dependencies: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'error'\n          error: subprocess-exited-with-error\n\n          Preparing metadata (pyproject.toml) did not run successfully.\n          exit code: 1\n\n          [22 lines of output]\n          + C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\vendored-meson\\meson\\meson.py setup C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-python-native-file.ini\n          The Meson build system\n          Version: 1.4.99\n          Source dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\n          Build dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\n          Build type: native build\n          Project name: NumPy\n          Project version: 2.0.2\n          C compiler for the host machine: gcc (gcc 13.2.0 \"gcc (GCC) 13.2.0\")\n          C linker for the host machine: gcc ld.bfd 2.41\n          C++ compiler for the host machine: c++ (gcc 6.3.0 \"c++ (MinGW.org GCC-6.3.0-1) 6.3.0\")\n          C++ linker for the host machine: c++ ld.bfd 2.28\n          Cython compiler for the host machine: cython (cython 3.0.11)\n          Host machine cpu family: x86\n          Host machine cpu: x86\n          Program python found: YES (C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe)\n          Need python for x86, but found x86_64\n          Run-time dependency python found: NO (tried sysconfig)\n\n          ..\\meson.build:41:12: ERROR: Python dependency not found\n\n          A full log can be found at C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-logs\\meson-log.txt\n          [end of output]\n\n          note: This error originates from a subprocess, and is likely not a problem with pip.\n        error: metadata-generation-failed\n\n        Encountered error while generating package metadata.\n\n        See above for output.\n\n        note: This is an issue with the package mentioned above, not pip.\n        hint: See above for details.\n        [end of output]\n\n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: subprocess-exited-with-error\n\n      pip subprocess to install build dependencies did not run successfully.\n      exit code: 1\n\n      See above for output.\n\n      note: This error originates from a subprocess, and is likely not a problem with pip.\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n× pip subprocess to install build dependencies did not run successfully.\n│ exit code: 1\n╰─> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.",
         "",
         "How to install spacy?",
         "I am using trying to install spacy library using 'pip install -U spacy' in the command prompt (run as admin) in Windows-11 O.S., but it shows some error I don't understand. I am using Python 3.13.0, gcc 13.2.0 and make 4.4.1. What could be the problem? Or is there any other way to install spacy?",
         "",
         "How to install spacy? I am using trying to install spacy library using 'pip install -U spacy' in the command prompt (run as admin) in Windows-11 O.S., but it shows some error I don't understand. I am using Python 3.13.0, gcc 13.2.0 and make 4.4.1. What could be the problem? Or is there any other way to install spacy? ",
         "install spacy ? using trying install spacy library using 'pip install -u spacy ' command prompt ( run admin ) windows-11 o.s. , shows error n't understand . using python 3.13.0 , gcc 13.2.0 make 4.4.1. could problem ? way install spacy ?",
         "0"
        ],
        [
         "36",
         "79256112",
         "How to log only the current script file to W&B code panel immediately?",
         "<p>How can I ensure that only the current script file (e.g., <code>train.py</code>) is logged to the W&amp;B Code panel when running a script, without logging the entire directory?</p>\n<p>Currently, I'm using:</p>\n<pre class=\"lang-py prettyprint-override\"><code>wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;)\n</code></pre>\n<p>I want to confirm if this approach works reliably across different environments and if there are better practices for this use case.</p>\n<p>Main part of the code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;) # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)\n    print(f'{run.get_url()=}')\n    wandb.finish()\n</code></pre>\n<p>All the code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\nfrom typing import Optional\nimport random\nimport torch\nfrom transformers import PushToHubCallback\nfrom transformers import get_cosine_schedule_with_warmup\nfrom trl import SFTConfig, SFTTrainer\nimport os\nimport fire\nimport wandb\nimport sys\n\nfrom train.callbacks import GenCallbackWithHFGenerate\nfrom train.data import load_math_style_dataset, print_first_example_after_decode\nimport train.models\n\nfrom train.utils import seed_everything\n\ndef main(**config):\n    # -- Seed everything\n    seed_everything(seed=config.get('seed', 0))\n    \n    # -- HF login\n    from huggingface_hub import login\n    token = open(os.path.expanduser(&quot;~/keys/master_hf_token.txt&quot;)).read().strip()\n    login(token=token)\n\n    # -- Get model\n    model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'google/gemma-2-2b')) \n    # model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'meta-llama/Llama-3.1-8B')) \n\n    # -- Load datasets\n    ds_name_or_path = config.get('ds_name_or_path', 'Putnam-AXIOM/putnam-axiom-dataset')\n    train_split, val_split = config.get('train_split', 'func_original_53_10_30_2024'), config.get('val_split', 'func_variations_265_11_23_2024')\n    print(f'\\n---&gt; {ds_name_or_path=} {train_split=} {val_split=}\\n')\n    train_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=1, split=train_split)\n    print_first_example_after_decode(train_dataset, tok)\n    # eval_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=15, split=val_split)\n    eval_dataset = train_dataset\n    print(f'{len(train_dataset)=}\\n{len(eval_dataset)=}')\n    wandb.config.update({'dataset': f'{ds_name_or_path} ({train_split=} {val_split=})'})\n\n    # -- Prepare output directory\n    today: str = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss')\n    output_dir: str = os.path.expanduser(f&quot;~/data/runs_logic_cont/run_{config.get('today', today)}&quot;)\n    print(f'{output_dir=}')\n    \n    # Save the initial model and tokenizer as checkpoint-0\n    initial_checkpoint_dir = os.path.join(output_dir, &quot;checkpoint-0&quot;)\n    os.makedirs(initial_checkpoint_dir, exist_ok=True)\n    print(f&quot;Saving initial checkpoint and tokenizer at {initial_checkpoint_dir}&quot;)\n    model.save_pretrained(initial_checkpoint_dir)\n    tok.save_pretrained(initial_checkpoint_dir)\n\n    # -- Train model\n    # max_steps = 50  # Limit fine-tuning to a few steps\n    # os.environ['CUDA_VISIBLE_DEVICES'] = str(random.randint(0, 7))\n    # config = {'max_steps': 2, 'eval_steps': 1, 'logging_steps': 1, \n    #           'save_strategy': 'steps', 'save_steps': 1, 'eval_strategy': 'steps'}\n    # config = config | {'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'maybe 0')}\n    training_args = SFTConfig(\n        max_steps=config.get('max_steps', 30),\n        # --\n        output_dir=output_dir,\n        bf16=torch.cuda.is_bf16_supported(),\n        fp16=not torch.cuda.is_bf16_supported(),\n        # -- logging opts\n        save_steps=config.get('save_steps', 5), \n        save_strategy=config.get('save_strategy', 'steps'),\n        eval_on_start=config.get('eval_on_start', True),\n        evaluation_strategy=config.get('eval_strategy', 'steps'), \n        eval_steps=config.get('eval_steps', 1), \n        logging_first_step=config.get('logging_first_step', True), # Default to False, unsure 100% what this does but looks like a good idea\n        logging_strategy=config.get('logging_strategy', 'steps'),\n        logging_steps=config.get('logging_steps', 1),\n        # --\n        num_train_epochs=config.get('num_train_epochs', 10),\n        max_seq_length=config.get('max_seq_length', 512),\n        per_device_train_batch_size=config.get('batch_size', 2),\n        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 2),\n    )\n    # Calculate Total Steps\n    steps_per_epoch = (len(train_dataset) // training_args.per_device_train_batch_size) // training_args.gradient_accumulation_steps\n    total_steps = steps_per_epoch * training_args.num_train_epochs\n    print(f'{steps_per_epoch=}')\n\n    # Optimizer and Scheduler\n    # optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 1e-4}]\n    optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.get('learning_rate', 1e-5))\n\n    # Add Cosine Learning Rate Scheduler\n    # warmup_steps = int(0.01 * total_steps)  # Warm-up for 1% of total steps\n    warmup_steps = 0\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps,\n    )\n    scheduler = None\n    print(f'{total_steps=} {warmup_steps=}')\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tok,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        args=training_args,\n        optimizers=(optimizer, scheduler),\n        callbacks=[GenCallbackWithHFGenerate(model, tok)]\n    )\n    print(f&quot;\\nStarting fine-tuning...&quot;)\n    trainer.train()\n    # - end run\n    return os.path.expanduser(output_dir)\n\ndef run_eval_logic_contamination(output_dir: str):\n    &quot;&quot;&quot;\n    Runs the eval_logic_contamination.py script with the specified output directory.\n\n    Args:\n        output_dir (str): The directory where the model is saved, expanded using `os.path.expanduser`.\n    &quot;&quot;&quot;\n    import gc\n    torch.cuda.empty_cache()\n    gc.collect()\n    output_dir = os.path.expanduser(output_dir)  # Ensure `output_dir` is expanded \n    from eval_logic_contamination import main\n    task='putnam_axiom_53'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # task='putnam_axiom_53' # for debugging\n    task='putnam_axiom_variations'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # wandb.run.define_metric(&quot;eval/accuracy&quot;, step_metric=&quot;eval/checkpoint_idx&quot;)\n    # wandb.run.define_metric(&quot;eval/checkpoint_idx&quot;) \n    # for idx, acc in [(10,5), (20,10), (30,15)]:\n    #     wandb.log({'eval/accuracy': acc, 'eval/checkpoint_idx': idx})\n\ndef _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;) # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)\n    print(f'{run.get_url()=}')\n    wandb.finish()\n\nif __name__ == &quot;__main__&quot;:\n    import time\n    start_time = time.time()\n    fire.Fire(_main)\n    print(f&quot;Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\\a&quot;)\n</code></pre>\n<p>Cross: <a href=\"https://community.wandb.ai/t/how-to-log-only-the-current-script-file-to-w-b-code-panel-immediately/8537\" rel=\"nofollow noreferrer\">https://community.wandb.ai/t/how-to-log-only-the-current-script-file-to-w-b-code-panel-immediately/8537</a></p>\n",
         "2024-12-05 20:11:21",
         "0",
         "18",
         "1",
         "<deep-learning><nlp><wandb>",
         null,
         null,
         "train.py\n---\nwandb.run.log_code(f\"./{os.path.basename(__file__)}\")\n---\ndef _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=\"putnam-axiom\", name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=\"putnam-axiom\", name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f\"./{os.path.basename(__file__)}\") # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=\"Run Completed\", text=f\"Run finished, run url: {run.get_url()}\")\n    print(f'{run.get_url()=}')\n    wandb.finish()\n---\nfrom datetime import datetime\nfrom typing import Optional\nimport random\nimport torch\nfrom transformers import PushToHubCallback\nfrom transformers import get_cosine_schedule_with_warmup\nfrom trl import SFTConfig, SFTTrainer\nimport os\nimport fire\nimport wandb\nimport sys\n\nfrom train.callbacks import GenCallbackWithHFGenerate\nfrom train.data import load_math_style_dataset, print_first_example_after_decode\nimport train.models\n\nfrom train.utils import seed_everything\n\ndef main(**config):\n    # -- Seed everything\n    seed_everything(seed=config.get('seed', 0))\n    \n    # -- HF login\n    from huggingface_hub import login\n    token = open(os.path.expanduser(\"~/keys/master_hf_token.txt\")).read().strip()\n    login(token=token)\n\n    # -- Get model\n    model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'google/gemma-2-2b')) \n    # model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'meta-llama/Llama-3.1-8B')) \n\n    # -- Load datasets\n    ds_name_or_path = config.get('ds_name_or_path', 'Putnam-AXIOM/putnam-axiom-dataset')\n    train_split, val_split = config.get('train_split', 'func_original_53_10_30_2024'), config.get('val_split', 'func_variations_265_11_23_2024')\n    print(f'\\n---> {ds_name_or_path=} {train_split=} {val_split=}\\n')\n    train_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=1, split=train_split)\n    print_first_example_after_decode(train_dataset, tok)\n    # eval_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=15, split=val_split)\n    eval_dataset = train_dataset\n    print(f'{len(train_dataset)=}\\n{len(eval_dataset)=}')\n    wandb.config.update({'dataset': f'{ds_name_or_path} ({train_split=} {val_split=})'})\n\n    # -- Prepare output directory\n    today: str = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss')\n    output_dir: str = os.path.expanduser(f\"~/data/runs_logic_cont/run_{config.get('today', today)}\")\n    print(f'{output_dir=}')\n    \n    # Save the initial model and tokenizer as checkpoint-0\n    initial_checkpoint_dir = os.path.join(output_dir, \"checkpoint-0\")\n    os.makedirs(initial_checkpoint_dir, exist_ok=True)\n    print(f\"Saving initial checkpoint and tokenizer at {initial_checkpoint_dir}\")\n    model.save_pretrained(initial_checkpoint_dir)\n    tok.save_pretrained(initial_checkpoint_dir)\n\n    # -- Train model\n    # max_steps = 50  # Limit fine-tuning to a few steps\n    # os.environ['CUDA_VISIBLE_DEVICES'] = str(random.randint(0, 7))\n    # config = {'max_steps': 2, 'eval_steps': 1, 'logging_steps': 1, \n    #           'save_strategy': 'steps', 'save_steps': 1, 'eval_strategy': 'steps'}\n    # config = config | {'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'maybe 0')}\n    training_args = SFTConfig(\n        max_steps=config.get('max_steps', 30),\n        # --\n        output_dir=output_dir,\n        bf16=torch.cuda.is_bf16_supported(),\n        fp16=not torch.cuda.is_bf16_supported(),\n        # -- logging opts\n        save_steps=config.get('save_steps', 5), \n        save_strategy=config.get('save_strategy', 'steps'),\n        eval_on_start=config.get('eval_on_start', True),\n        evaluation_strategy=config.get('eval_strategy', 'steps'), \n        eval_steps=config.get('eval_steps', 1), \n        logging_first_step=config.get('logging_first_step', True), # Default to False, unsure 100% what this does but looks like a good idea\n        logging_strategy=config.get('logging_strategy', 'steps'),\n        logging_steps=config.get('logging_steps', 1),\n        # --\n        num_train_epochs=config.get('num_train_epochs', 10),\n        max_seq_length=config.get('max_seq_length', 512),\n        per_device_train_batch_size=config.get('batch_size', 2),\n        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 2),\n    )\n    # Calculate Total Steps\n    steps_per_epoch = (len(train_dataset) // training_args.per_device_train_batch_size) // training_args.gradient_accumulation_steps\n    total_steps = steps_per_epoch * training_args.num_train_epochs\n    print(f'{steps_per_epoch=}')\n\n    # Optimizer and Scheduler\n    # optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 1e-4}]\n    optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.get('learning_rate', 1e-5))\n\n    # Add Cosine Learning Rate Scheduler\n    # warmup_steps = int(0.01 * total_steps)  # Warm-up for 1% of total steps\n    warmup_steps = 0\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps,\n    )\n    scheduler = None\n    print(f'{total_steps=} {warmup_steps=}')\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tok,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        args=training_args,\n        optimizers=(optimizer, scheduler),\n        callbacks=[GenCallbackWithHFGenerate(model, tok)]\n    )\n    print(f\"\\nStarting fine-tuning...\")\n    trainer.train()\n    # - end run\n    return os.path.expanduser(output_dir)\n\ndef run_eval_logic_contamination(output_dir: str):\n    \"\"\"\n    Runs the eval_logic_contamination.py script with the specified output directory.\n\n    Args:\n        output_dir (str): The directory where the model is saved, expanded using `os.path.expanduser`.\n    \"\"\"\n    import gc\n    torch.cuda.empty_cache()\n    gc.collect()\n    output_dir = os.path.expanduser(output_dir)  # Ensure `output_dir` is expanded \n    from eval_logic_contamination import main\n    task='putnam_axiom_53'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # task='putnam_axiom_53' # for debugging\n    task='putnam_axiom_variations'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # wandb.run.define_metric(\"eval/accuracy\", step_metric=\"eval/checkpoint_idx\")\n    # wandb.run.define_metric(\"eval/checkpoint_idx\") \n    # for idx, acc in [(10,5), (20,10), (30,15)]:\n    #     wandb.log({'eval/accuracy': acc, 'eval/checkpoint_idx': idx})\n\ndef _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=\"putnam-axiom\", name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=\"putnam-axiom\", name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f\"./{os.path.basename(__file__)}\") # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=\"Run Completed\", text=f\"Run finished, run url: {run.get_url()}\")\n    print(f'{run.get_url()=}')\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    import time\n    start_time = time.time()\n    fire.Fire(_main)\n    print(f\"Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\\a\")",
         "",
         "How to log only the current script file to W&B code panel immediately?",
         "How can I ensure that only the current script file (e.g., ) is logged to the W&B Code panel when running a script, without logging the entire directory? Currently, I'm using: I want to confirm if this approach works reliably across different environments and if there are better practices for this use case. Main part of the code: All the code: Cross:",
         "",
         "How to log only the current script file to W&B code panel immediately? How can I ensure that only the current script file (e.g., ) is logged to the W&B Code panel when running a script, without logging the entire directory? Currently, I'm using: I want to confirm if this approach works reliably across different environments and if there are better practices for this use case. Main part of the code: All the code: Cross: ",
         "log current script file w & b code panel immediately ? ensure current script file ( e.g. , ) logged w & b code panel running script , without logging entire directory ? currently , 'm using : want confirm approach works reliably across different environments better practices use case . main part code : code : cross :",
         "8"
        ],
        [
         "37",
         "79253283",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>\n<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>\n<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n</code></pre>\n<p>A small Example -</p>\n<pre><code>text_dict = {\n    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]\n}\n\ndf = pd.DataFrame(text_dict)\n</code></pre>\n<p>My expected outcome is -</p>\n<pre><code>                 ITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n</code></pre>\n<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>\n<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>\n",
         "2024-12-05 03:05:06",
         "0",
         "81",
         "1",
         "<python><pandas><nlp>",
         "79263000.0",
         "<p>You need to process each row of text by identifying occurrences of <code>key_words</code> and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique <code>search_word</code> found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique <code>search_words</code> near <code>key_words</code>.</p>\n<pre><code>import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        &quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process &quot;\n        &quot;of adopting Symantec. We believe these technologies will improve our access control. &quot;\n        &quot;Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = &quot; &quot;.join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n</code></pre>\n<p>Which returns</p>\n<pre><code>                                               ITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2\n</code></pre>\n",
         "search words\n---\nkey words\n---\nsearch words\n---\nkey words\n---\nkey words\n---\nsearch words\n---\nsearch word\n---\nkey word\n---\nsearch_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n---\ntext_dict = {\n    'ITEM7':[\"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"]\n}\n\ndf = pd.DataFrame(text_dict)\n---\nITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n---\ndf\n---\nAVG\n---\nAlibaba Security\n---\nsearch_words\n---\nkey_words\n---\nSymantec\n---\nAccess Control\n---\ndata security\n---\ndiagnostic program\n---\nsearch_words\n---\nadopting\n---\nimprove\n---\nintegrated\n---\nkey_words\n---\nFrequency\n---\ndf\n---\nkey_words",
         "key_words\n---\nsearch_word\n---\nsearch_words\n---\nkey_words\n---\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        \"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \"\n        \"of adopting Symantec. We believe these technologies will improve our access control. \"\n        \"Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = \" \".join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n---\nITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "I have two sets of word lists - first one I called and the second one I called . My goal is to calculate the frequency of within 10 words of . For example, assume that the word - acquire - is in list, then I will look for the words in list within 10 words of acquire . Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement. Below is my and lists - A small Example - My expected outcome is - For the first row in , we see the word and are from list and around the word acquired , the base form of which - acquire - is in the list. Similarly, , , , are from list and these words are within 10 words of , , from list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the column of , the value is 6. Please note that the words in are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.",
         "You need to process each row of text by identifying occurrences of and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique near . Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists - first one I called and the second one I called . My goal is to calculate the frequency of within 10 words of . For example, assume that the word - acquire - is in list, then I will look for the words in list within 10 words of acquire . Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement. Below is my and lists - A small Example - My expected outcome is - For the first row in , we see the word and are from list and around the word acquired , the base form of which - acquire - is in the list. Similarly, , , , are from list and these words are within 10 words of , , from list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the column of , the value is 6. Please note that the words in are in basically base form, so their variation (like adopted, adopting) should be counted as key words also. You need to process each row of text by identifying occurrences of and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique near . Which returns",
         "counting frequency words within key words text two sets word lists - first one called second one called . goal calculate frequency within 10 words . example , assume word - acquire - list , look words list within 10 words acquire . within 10 words mean , 10 words forward key words 10 words backward key words , meaning forward backward movement . lists - small example - expected outcome - first row , see word list around word acquired , base form - acquire - list . similarly , , , , list words within 10 words , , list . , total search words 6 ( avg+alibaba security+symantec+access control+data security+diagnostic program ) . therefore , column , value 6. please note words basically base form , variation ( like adopted , adopting ) counted key words also . need process row text identifying occurrences capturing 10-word window around . within window , need check multi-word search_words , ensuring matched phrases . unique found within windows needs counted , avoiding double-counting across row . stored results frequency count row , accurately reflecting number unique near . returns",
         "9"
        ],
        [
         "38",
         "79247672",
         "Error in getting Captum text explanations for text classification",
         "<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>\n<pre><code>import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f&quot;Test Accuracy: {accuracy:.2f}&quot;)\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n<p>But I am getting the following error in running the above.</p>\n<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n</code></pre>\n",
         "2024-12-03 12:47:45",
         "2",
         "73",
         "1",
         "<machine-learning><pytorch><nlp><huggingface-transformers><text-classification>",
         "79248379.0",
         "<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>\n<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=\"https://captum.ai/tutorials/Bert_SQUAD_Interpret\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>\n<p>Below please find snippet that works:</p>\n<pre><code>from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n",
         "import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)\n---\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-f047b509c98d> in <cell line: 90>()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
         "from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    # print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)",
         "Error in getting Captum text explanations for text classification",
         "I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above.",
         "You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly. I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial Below please find snippet that works:",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above. You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly. I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial Below please find snippet that works:",
         "error getting captum text explanations text classification following code using identify influential words used correctly predict text test dataset getting following error running . need slightly change gradients calculation class . also , n't include forward_func gradients class constructor , attribute method able launch stuff properly . think using layerintegratedgradients better debugging bert - line tutorial please find snippet works :",
         "2"
        ],
        [
         "39",
         "79247594",
         "euclidian distance from word to sentence after doing Vectorizer",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did TfidfVectorizer.</p>\n<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>\n<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n",
         "2024-12-03 12:25:05",
         "1",
         "43",
         "1",
         "<pandas><dataframe><nlp><text-classification><tf-idf>",
         "79248087.0",
         "<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([&quot;king&quot;]).toarray()\nexcept KeyError:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n</code></pre>\n<p>which would give you</p>\n<pre><code>                                                text  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0  \n</code></pre>\n<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif &quot;king&quot; in feature_names:\n    king_index = np.where(feature_names == &quot;king&quot;)[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n\n</code></pre>\n<p>which gives</p>\n<pre><code>                                                text  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214\n</code></pre>\n",
         "",
         "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([\"king\"]).toarray()\nexcept KeyError:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n---\ntext  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0\n---\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif \"king\" in feature_names:\n    king_index = np.where(feature_names == \"king\")[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n---\ntext  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214",
         "euclidian distance from word to sentence after doing Vectorizer",
         "I have dataframe with 1000 text rows. I did TfidfVectorizer. Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word \"king\". df'king' I thought about taking in each sentence the 5 closet words to the word king and make average of them. I will glad to know how to do that or to hear about another method.",
         "I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores: which would give you That being said, if you want to focus on Euclidean distance, here is a method: which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows. I did TfidfVectorizer. Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word \"king\". df'king' I thought about taking in each sentence the 5 closet words to the word king and make average of them. I will glad to know how to do that or to hear about another method. I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores: which would give you That being said, if you want to focus on Euclidean distance, here is a method: which gives",
         "euclidian distance word sentence vectorizer dataframe 1000 text rows . tfidfvectorizer . want create new field give distance sentence word want , lets say word `` king '' . df'king ' thought taking sentence 5 closet words word king make average . glad know hear another method . convinced euclidean distance would optimal measure . would actually look similarity scores : would give said , want focus euclidean distance , method : gives",
         "5"
        ],
        [
         "40",
         "79234004",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "<p>I want to use <code>Llama-3.2-1B-Instruct</code> model, and although I have set <code>&quot;temperature&quot;: 0.0, &quot;top_p&quot;:0.0 and &quot;top_k&quot;:0</code>, it still generates inconsistent output. This is how my pipeline looks like:</p>\n<pre><code>pipe = pipeline(\n    &quot;text-generation&quot;,\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=&quot;mps&quot;,\n        model_kwargs={&quot;temperature&quot;: 0.0,\n                  &quot;do_sample&quot;:True,\n                              &quot;top_p&quot;:0.0,\n                              &quot;top_k&quot;:0,},\n)\n</code></pre>\n<p>Any idea how to solve this issue?</p>\n",
         "2024-11-28 13:02:37",
         "1",
         "532",
         "2",
         "<python><nlp><huggingface-transformers><large-language-model>",
         "79246602.0",
         "<p>The model inconsistent output can be due to two main factors:</p>\n<p><strong>1. Temperature:</strong></p>\n<p>setting temperature to zero give more inconsistent result. You can refer <a href=\"https://community.openai.com/t/why-the-api-output-is-inconsistent-even-after-the-temperature-is-set-to-0/329541/2\" rel=\"nofollow noreferrer\">Opeani discussion page</a> for detail.</p>\n<p>So the best option is to set temperature to very low values such as 0.00001 instead of zero.</p>\n<p><strong>2. do_sample</strong></p>\n<p>You already set it false, and it should remain that way only.</p>\n",
         "Llama-3.2-1B-Instruct\n---\n\"temperature\": 0.0, \"top_p\":0.0 and \"top_k\":0\n---\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"mps\",\n        model_kwargs={\"temperature\": 0.0,\n                  \"do_sample\":True,\n                              \"top_p\":0.0,\n                              \"top_k\":0,},\n)",
         "",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "I want to use model, and although I have set , it still generates inconsistent output. This is how my pipeline looks like: Any idea how to solve this issue?",
         "The model inconsistent output can be due to two main factors: 1. Temperature: setting temperature to zero give more inconsistent result. You can refer Opeani discussion page for detail. So the best option is to set temperature to low values such as 0.00001 instead of zero. 2. do_sample You already set it false, and it should remain that way only.",
         "Llama-3.2-1B-Instruct generate inconsistent output I want to use model, and although I have set , it still generates inconsistent output. This is how my pipeline looks like: Any idea how to solve this issue? The model inconsistent output can be due to two main factors: 1. Temperature: setting temperature to zero give more inconsistent result. You can refer Opeani discussion page for detail. So the best option is to set temperature to low values such as 0.00001 instead of zero. 2. do_sample You already set it false, and it should remain that way only.",
         "llama-3.2-1b-instruct generate inconsistent output want use model , although set , still generates inconsistent output . pipeline looks like : idea solve issue ? model inconsistent output due two main factors : 1. temperature : setting temperature zero give inconsistent result . refer opeani discussion page detail . best option set temperature low values 0.00001 instead zero . 2. do_sample already set false , remain way .",
         "8"
        ],
        [
         "41",
         "79227390",
         "How to extract specific entities from unstructured text",
         "<p>Given a generic text sentence (in a specific context) how can I extract word/entities of interest belonging to a specific &quot;category&quot; using python and any NLP library?</p>\n<p>For example given a step for a culinary recipe <code>Add an onion to a bowl of carrots</code> as input text, I'd like to retrive <code>onion</code> and <code>carrots</code> while given <code>Sprinkle with paprika.</code> should return <code>paprika</code>.\nBut this should also work with sentences like <code>stir well, and cook an additional minute.</code> that do not contain any food entity in them.</p>\n<p>So far, what I was able to achieve is using the <code>spacy</code> library for training a NER module to parse sentences. The problem with the NER pipeline is that it is a rule-based parsing, it is trained providing a set of sentences and entities/matches/labels to learn, which works fine as expeted on sentences similar to the one used during train, but performs bad on new and different sentences:</p>\n<pre class=\"lang-py prettyprint-override\"><code>nlp = spacy.load('trained_model')\n\ndocument = nlp('Add flour, mustard, and salt')\n[(ent.text, ent.label_) for ent in document.ents]\n# &gt;&gt; [('Add flour', 'FOOD'), ('mustard', 'FOOD'), ('salt', 'FOOD')]\n# (quite) correct output\n\ndocument = nlp('I took a building, car and squirrel on the weekend')\n[(ent.text, ent.label_) for ent in document.ents]\n# &gt;&gt; [('building', 'FOOD'), ('car', 'FOOD'), ('squirrel', 'FOOD')]\n# wrong output\n\ndocument = nlp('stir well, and cook an additional minute.')\n[(ent.text, ent.label_) for ent in document.ents]\n# &gt;&gt; [('stir well', 'FOOD'), ('cook', 'FOOD'), ('additional minute.', 'FOOD')]\n# wrong output\n</code></pre>\n<p>I am aware that there are several similar questions and posts, but I have found only solutions working for &quot;semi-structured&quot; text, i.e. list of ingredients as <code>1 tsp. of sugar, 1 cup of milk, ...</code> which can be easily solved using the previous rule-based approach. Also <code>nltk</code> and part-of-speech (POS) are an option, but I'd prefer an alternative solution rather than having to compare each noun with an exhaustive list of foods.</p>\n<p>What instead I am looking for is a way of to extract specific entities or at least to classify words in generic text with additional categories beyond those of the basic parsing.\nWhich methods should I use/look at to achieve this?</p>\n",
         "2024-11-26 15:46:48",
         "1",
         "92",
         "1",
         "<python><machine-learning><nlp><nltk><spacy>",
         null,
         null,
         "Add an onion to a bowl of carrots\n---\nonion\n---\ncarrots\n---\nSprinkle with paprika.\n---\npaprika\n---\nstir well, and cook an additional minute.\n---\nspacy\n---\nnlp = spacy.load('trained_model')\n\ndocument = nlp('Add flour, mustard, and salt')\n[(ent.text, ent.label_) for ent in document.ents]\n# >> [('Add flour', 'FOOD'), ('mustard', 'FOOD'), ('salt', 'FOOD')]\n# (quite) correct output\n\ndocument = nlp('I took a building, car and squirrel on the weekend')\n[(ent.text, ent.label_) for ent in document.ents]\n# >> [('building', 'FOOD'), ('car', 'FOOD'), ('squirrel', 'FOOD')]\n# wrong output\n\ndocument = nlp('stir well, and cook an additional minute.')\n[(ent.text, ent.label_) for ent in document.ents]\n# >> [('stir well', 'FOOD'), ('cook', 'FOOD'), ('additional minute.', 'FOOD')]\n# wrong output\n---\n1 tsp. of sugar, 1 cup of milk, ...\n---\nnltk",
         "",
         "How to extract specific entities from unstructured text",
         "Given a generic text sentence (in a specific context) how can I extract word/entities of interest belonging to a specific \"category\" using python and any NLP library? For example given a step for a culinary recipe as input text, I'd like to retrive and while given should return . But this should also work with sentences like that do not contain any food entity in them. So far, what I was able to achieve is using the library for training a NER module to parse sentences. The problem with the NER pipeline is that it is a rule-based parsing, it is trained providing a set of sentences and entities/matches/labels to learn, which works fine as expeted on sentences similar to the one used during train, but performs bad on new and different sentences: I am aware that there are several similar questions and posts, but I have found only solutions working for \"semi-structured\" text, i.e. list of ingredients as which can be easily solved using the previous rule-based approach. Also and part-of-speech (POS) are an option, but I'd prefer an alternative solution rather than having to compare each noun with an exhaustive list of foods. What instead I am looking for is a way of to extract specific entities or at least to classify words in generic text with additional categories beyond those of the basic parsing. Which methods should I use/look at to achieve this?",
         "",
         "How to extract specific entities from unstructured text Given a generic text sentence (in a specific context) how can I extract word/entities of interest belonging to a specific \"category\" using python and any NLP library? For example given a step for a culinary recipe as input text, I'd like to retrive and while given should return . But this should also work with sentences like that do not contain any food entity in them. So far, what I was able to achieve is using the library for training a NER module to parse sentences. The problem with the NER pipeline is that it is a rule-based parsing, it is trained providing a set of sentences and entities/matches/labels to learn, which works fine as expeted on sentences similar to the one used during train, but performs bad on new and different sentences: I am aware that there are several similar questions and posts, but I have found only solutions working for \"semi-structured\" text, i.e. list of ingredients as which can be easily solved using the previous rule-based approach. Also and part-of-speech (POS) are an option, but I'd prefer an alternative solution rather than having to compare each noun with an exhaustive list of foods. What instead I am looking for is a way of to extract specific entities or at least to classify words in generic text with additional categories beyond those of the basic parsing. Which methods should I use/look at to achieve this? ",
         "extract specific entities unstructured text given generic text sentence ( specific context ) extract word/entities interest belonging specific `` category '' using python nlp library ? example given step culinary recipe input text , 'd like retrive given return . also work sentences like contain food entity . far , able achieve using library training ner module parse sentences . problem ner pipeline rule-based parsing , trained providing set sentences entities/matches/labels learn , works fine expeted sentences similar one used train , performs bad new different sentences : aware several similar questions posts , found solutions working `` semi-structured '' text , i.e . list ingredients easily solved using previous rule-based approach . also part-of-speech ( pos ) option , 'd prefer alternative solution rather compare noun exhaustive list foods . instead looking way extract specific entities least classify words generic text additional categories beyond basic parsing . methods use/look achieve ?",
         "3"
        ],
        [
         "42",
         "79202614",
         "Understanding byte-pair encoding tokenization for Greek characters",
         "<p>I am trying to train a new tokenizer with Greek text to later add the new tokens into the Llama 3.1 tokenizer using</p>\n<pre class=\"lang-py prettyprint-override\"><code>tokenizer.add_tokens(list(new_tokens)).\n</code></pre>\n<p>However, upon training the byte-pair encoding tokenizer on Greek and Spanish text, the result looks something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>\\['Translate', 'Ġfrom', 'ĠGreek', 'Ġto', 'ĠSpanish', ':', 'ĠÎĿÎ±', 'ĠÎŃÏĥÎ¹', 'ĠÎ¿Î³Î¯', 'ĠÎ³ÎŃÏģÎ¿Ïħ'\\]\n</code></pre>\n<p>When extending the token vocabulary in the tokenizer, it seems that those encoded tokens are being passed literally, not as encodings of Greek characters, and they are not recognized by the tokenizer to encode a sentence. However, when using the same method and new tokens are hardcoded, such as in</p>\n<pre class=\"lang-py prettyprint-override\"><code>extender_tokenizer.add_tokens(['Αυτό', 'είναι'])\n</code></pre>\n<p>it does work.</p>\n<p>I assume this is an encoding issue or it is related to BPE inner workings.\nWhy are Greek characters shown that way? Is it related to encoding, BPE or both? How to obtain a list of Greek character tokens that can be added to the tokenizer?</p>\n<p>Reference code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n\ntokenizer = Tokenizer(models.BPE())\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\ntrainer = trainers.BpeTrainer(vocab_size = 2000, min_frequency = 3, show_progress = True)\ntokenizer.train_from_iterator(training_corpus, trainer = trainer)\n</code></pre>\n",
         "2024-11-19 08:31:26",
         "0",
         "74",
         "1",
         "<encoding><nlp><tokenize><byte-pair-encoding>",
         null,
         null,
         "tokenizer.add_tokens(list(new_tokens)).\n---\n\\['Translate', 'Ġfrom', 'ĠGreek', 'Ġto', 'ĠSpanish', ':', 'ĠÎĿÎ±', 'ĠÎŃÏĥÎ¹', 'ĠÎ¿Î³Î¯', 'ĠÎ³ÎŃÏģÎ¿Ïħ'\\]\n---\nextender_tokenizer.add_tokens(['Αυτό', 'είναι'])\n---\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers\n\ntokenizer = Tokenizer(models.BPE())\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\ntrainer = trainers.BpeTrainer(vocab_size = 2000, min_frequency = 3, show_progress = True)\ntokenizer.train_from_iterator(training_corpus, trainer = trainer)",
         "",
         "Understanding byte-pair encoding tokenization for Greek characters",
         "I am trying to train a new tokenizer with Greek text to later add the new tokens into the Llama 3.1 tokenizer using However, upon training the byte-pair encoding tokenizer on Greek and Spanish text, the result looks something like this: When extending the token vocabulary in the tokenizer, it seems that those encoded tokens are being passed literally, not as encodings of Greek characters, and they are not recognized by the tokenizer to encode a sentence. However, when using the same method and new tokens are hardcoded, such as in it does work. I assume this is an encoding issue or it is related to BPE inner workings. Why are Greek characters shown that way? Is it related to encoding, BPE or both? How to obtain a list of Greek character tokens that can be added to the tokenizer? Reference code:",
         "",
         "Understanding byte-pair encoding tokenization for Greek characters I am trying to train a new tokenizer with Greek text to later add the new tokens into the Llama 3.1 tokenizer using However, upon training the byte-pair encoding tokenizer on Greek and Spanish text, the result looks something like this: When extending the token vocabulary in the tokenizer, it seems that those encoded tokens are being passed literally, not as encodings of Greek characters, and they are not recognized by the tokenizer to encode a sentence. However, when using the same method and new tokens are hardcoded, such as in it does work. I assume this is an encoding issue or it is related to BPE inner workings. Why are Greek characters shown that way? Is it related to encoding, BPE or both? How to obtain a list of Greek character tokens that can be added to the tokenizer? Reference code: ",
         "understanding byte-pair encoding tokenization greek characters trying train new tokenizer greek text later add new tokens llama 3.1 tokenizer using however , upon training byte-pair encoding tokenizer greek spanish text , result looks something like : extending token vocabulary tokenizer , seems encoded tokens passed literally , encodings greek characters , recognized tokenizer encode sentence . however , using method new tokens hardcoded , work . assume encoding issue related bpe inner workings . greek characters shown way ? related encoding , bpe ? obtain list greek character tokens added tokenizer ? reference code :",
         "8"
        ],
        [
         "43",
         "79192130",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>\n<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>\n",
         "2024-11-15 11:13:36",
         "1",
         "52",
         "2",
         "<python><amazon-web-services><aws-lambda><nlp><large-language-model>",
         "79192427.0",
         "<p>In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:</p>\n<ol>\n<li>Package your KeyBERT script in a lambda and easily deploy it with a container.</li>\n<li>Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.</li>\n</ol>\n",
         "",
         "",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords. Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.",
         "In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do: Package your KeyBERT script in a lambda and easily deploy it with a container. Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so its cost-efficient for occasional tasks.",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT? I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords. Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks. In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do: Package your KeyBERT script in a lambda and easily deploy it with a container. Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so its cost-efficient for occasional tasks.",
         "using aws service execute python script extract keywords text using keybert ? simple python script given two blocks text , extracts keywords using keybert , compares lists keywords sort two lists depending lists share keywords . aws service would best fit needs ? want able esentially spin needed , give blocks text , execute return results , n't want integrate projects n't use python . 've attempted use lambda 'm concerned potential cost running . thanks . cases , would normally think two resources aligned best practices aws software engineering . sagemaker lambda . model 'm using resource-intensive requires gpu acceleration 'd go sagemaker otherwise lambda good solution . case , 's 'd : package keybert script lambda easily deploy container . invoke whenever need process text blocks . aws lambda charges execution time , cost-efficient occasional tasks .",
         "6"
        ],
        [
         "44",
         "79190601",
         "Pyspark sentiment analysis invalid output",
         "<p>I am trying to perform sentiment analysis for a use case. Most of the time, it is giving correct results, but in some cases, even positive comments are being marked as negative. How can I fix my code to achieve better accuracy?</p>\n<p>My code</p>\n<pre><code>from pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import StringType\nfrom transformers import pipeline\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Define the filter_stopwords function\ndef filter_stopwords(sentence):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(sentence)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n    return &quot; &quot;.join(filtered_sentence)\n\n# Initialize the sentiment analysis pipeline with a different model\nsentiment_pipeline = pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)\n\n# Define a function to get sentiment using the pipeline\ndef get_sentiment(text):\n    filtered_text = filter_stopwords(text)\n    result = sentiment_pipeline(filtered_text)[0]\n    return result['label'].lower()  # returns 'positive', 'negative', etc.\n\n# Register the function as a UDF\nsentiment_udf = udf(get_sentiment, StringType())\n\n# df = df.withColumn(&quot;sentiment&quot;, sentiment_udf(col(&quot;text_column&quot;)))\n</code></pre>\n<p>Input data</p>\n<ol>\n<li><p>Didn't get it right the first 2 times but when it was fixed it was fixed well.</p>\n</li>\n<li><p>The Response time was grate -- <strong>Note</strong> Looks like spelling mistake but his review is positive</p>\n</li>\n<li><p>The initial agent contact could not resolve my issue but escalated it quickly to someone who could.</p>\n</li>\n</ol>\n<p>for this inputs i am expecting all should be <strong>positive</strong> instead i am getting <strong>negative</strong></p>\n",
         "2024-11-14 22:24:54",
         "3",
         "60",
         "2",
         "<pyspark><nlp><nltk><huggingface-transformers>",
         null,
         null,
         "from pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import StringType\nfrom transformers import pipeline\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Define the filter_stopwords function\ndef filter_stopwords(sentence):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(sentence)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n    return \" \".join(filtered_sentence)\n\n# Initialize the sentiment analysis pipeline with a different model\nsentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n# Define a function to get sentiment using the pipeline\ndef get_sentiment(text):\n    filtered_text = filter_stopwords(text)\n    result = sentiment_pipeline(filtered_text)[0]\n    return result['label'].lower()  # returns 'positive', 'negative', etc.\n\n# Register the function as a UDF\nsentiment_udf = udf(get_sentiment, StringType())\n\n# df = df.withColumn(\"sentiment\", sentiment_udf(col(\"text_column\")))",
         "",
         "Pyspark sentiment analysis invalid output",
         "I am trying to perform sentiment analysis for a use case. Most of the time, it is giving correct results, but in some cases, even positive comments are being marked as negative. How can I fix my code to achieve better accuracy? My code Input data Didn't get it right the first 2 times but when it was fixed it was fixed well. The Response time was grate -- Note Looks like spelling mistake but his review is positive The initial agent contact could not resolve my issue but escalated it quickly to someone who could. for this inputs i am expecting all should be positive instead i am getting negative",
         "",
         "Pyspark sentiment analysis invalid output I am trying to perform sentiment analysis for a use case. Most of the time, it is giving correct results, but in some cases, even positive comments are being marked as negative. How can I fix my code to achieve better accuracy? My code Input data Didn't get it right the first 2 times but when it was fixed it was fixed well. The Response time was grate -- Note Looks like spelling mistake but his review is positive The initial agent contact could not resolve my issue but escalated it quickly to someone who could. for this inputs i am expecting all should be positive instead i am getting negative ",
         "pyspark sentiment analysis invalid output trying perform sentiment analysis use case . time , giving correct results , cases , even positive comments marked negative . fix code achieve better accuracy ? code input data n't get right first 2 times fixed fixed well . response time grate -- note looks like spelling mistake review positive initial agent contact could resolve issue escalated quickly someone could . inputs expecting positive instead getting negative",
         "2"
        ],
        [
         "45",
         "79180131",
         "How to parse a resume with few shot method using the specified models from HuggingFace and Langchain?",
         "<h1>Model selection confusion and some errors while trying to parse a resume with the following codes</h1>\n<ul>\n<li>Trying to do a few shot prompting with a google flan t5 base model</li>\n<li>While Doing so I am getting an error\n<code>ERROR:Service.service:Error parsing resume: &quot;'ContactInformation'&quot;</code></li>\n<li>exception as <code>&quot;detail&quot;: &quot;Failed to parse the file: An error occurred in parsed_resume: \\&quot;'ContactInformation'\\&quot;&quot;</code></li>\n</ul>\n<h2>The code is given below</h2>\n<pre class=\"lang-none prettyprint-override\"><code>from typing import List, Optional, Union, Any, re\nimport json\n\n\n\nclass ContactInformation(BaseModel):\n    Name: Optional[str] = None\n    Email: Optional[str] = None\n    Contact: Optional[str] = None\n    Links: Optional[List[str]] = None\n\n\n\nclass Experience(BaseModel):\n    title: Optional[str] = None\n    company: Optional[str] = None\n    duration: Optional[str] = None\n\n\nclass Education(BaseModel):\n    course: Optional[str] = None\n    branch: Optional[str] = None\n    institute: Optional[str] = None\n\n\nclass Projects(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n    link: Optional[str] = None\n\nclass OutputFormat(BaseModel):\n    ContactInformation: Optional[Any] = None\n    AboutMe: Optional[Any] = None\n    Experiences: Optional[List[Any]] = None\n    Educations: Optional[List[Any]] = None\n    Skills: Optional[List[Any]] = None\n    Certificates: Optional[List[Any]] = None\n    Projects: Optional[List[Any]] = None\n    Achievements: Optional[List[Any]] = None\n    Volunteer: Optional[List[Any]] = None\n\n</code></pre>\n<pre class=\"lang-none prettyprint-override\"><code>    def __init__(self, model_name=model_1, fine_tune_model_path: str = None):\n        # Initialize LLM service with specified model\n\n\n        if fine_tune_model_path:\n            # Load fine-tuned model from local directory\n            self.tokenizer = AutoTokenizer.from_pretrained(fine_tune_model_path)\n            self.model = AutoModel.from_pretrained(fine_tune_model_path)\n        else:\n            # Load base model\n            self.llm_service = HuggingFaceHub(\n                repo_id=&quot;google/flan-t5-base&quot;,\n                huggingfacehub_api_token=huggingface_api_key,\n                model_kwargs={\n                    &quot;temperature&quot;: 0.5,\n                    &quot;max_new_tokens&quot;: 200\n                }  # Model parameters for consistent output\n            )\n    def parsed_resume(self, resume_txt: str):\n        df = pd.read_csv(r&quot;C:\\Users\\Sarthak\\PycharmProjects\\JobAxle\\Service\\data\\for_model_resume_dataset.csv&quot;)\n        print(df['prompt'][0])\n        examples = [\n            {'prompt':df['prompt'][0], &quot;completion&quot;:df['completion'][0]},\n            {'prompt': df['prompt'][1], &quot;completion&quot;: df['completion'][1]}\n        ]\n        print('Examples:',examples[0])\n        example_formatter_template = &quot;&quot;&quot;\n        {prompt}\n        {completion}\\n\n        &quot;&quot;&quot;\n        example_prompt = PromptTemplate(\n            input_variables=[&quot;prompt&quot;, &quot;completion&quot;],\n            template=example_formatter_template,\n        )\n        parser = PydanticOutputParser(pydantic_object=OutputFormat)\n        few_shot_prompt_template = FewShotPromptTemplate(\n            examples=examples,\n            example_prompt=example_prompt,\n            suffix=&quot;&quot;&quot;\n                Parse the given resume text, ensuring the output in JSON format:\n        \n                Resume:\n                {resume}\n        \n                {format_instructions}\n        \n                Output as JSON below:\n                completion:&quot;&quot;&quot;,\n            input_variables=[&quot;resume&quot;],\n            example_separator=&quot;\\n&quot;,\n            partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}\n        )\n        print(&quot;Few-Shot Prompt Template with Examples and JSON Instructions:\\n&quot;, few_shot_prompt_template)\n\n        prompt_template = PromptTemplate(\n            input_variables=['resume'],\n            template=Prompt_2\n        )\n        # print(few_shot_prompt_template)\n        # Initialize the LLM chain\n        chain = LLMChain(\n            llm=self.llm_service,\n            prompt=few_shot_prompt_template,\n            verbose=True\n        )\n        print(&quot;Chain:&quot;, chain)\n        print(resume_txt)\n        try:\n            response = chain.invoke({'resume': resume_txt}, verbose=True)\n            print(response)\n            logger.info('Model Response: %s', response)\n            print(&quot;Type of Response:&quot;,type(response))\n            # Invoke the chain and get a response\n            response_json = self.process_response(response)\n            parsed_json = self.structure_response(response_json)\n            return OutputFormat(**parsed_json)  # Return as OutputFormat object\n\n        except Exception as e:\n            logger.error(&quot;Error parsing resume: %s&quot;, e)\n            raise Exception(f&quot;An error occurred in parsed_resume: {e}&quot;)\n\n    def process_response(self, response_text: str) -&gt; Dict:\n        &quot;&quot;&quot;Process LLM response into JSON format.&quot;&quot;&quot;\n        response_json = json.dumps(response_text).strip()\n\n        # Remove extraneous characters\n        if response_json.startswith(&quot;```json&quot;):\n            response_json = response_json[len(&quot;```json&quot;):].strip()\n        if response_json.endswith(&quot;```&quot;):\n            response_json = response_json[:-len(&quot;```&quot;)].strip()\n\n        response_json = remove_trailing_commas(response_json)\n\n        try:\n\n            return json.loads(response_json)\n        except json.JSONDecodeError as e:\n            logger.error(&quot;JSON decoding error: %s. Response text: %s&quot;, e, response_json)\n            raise ValueError(&quot;Failed to parse response as valid JSON&quot;)\n\n    def structure_response(self, parsed_json: Dict) -&gt; Dict:\n        &quot;&quot;&quot;\n        Structure response JSON to match the OutputFormat schema, ensuring lists for 'Experiences' and 'Educations'.\n        &quot;&quot;&quot;\n        # Ensure ContactInformation, Experiences, and Educations are present in the expected structure\n        parsed_json[&quot;ContactInformation&quot;] = parsed_json.get(&quot;ContactInformation&quot;, {})\n        if isinstance(parsed_json.get(&quot;Experiences&quot;), dict):\n            parsed_json[&quot;Experiences&quot;] = [parsed_json[&quot;Experiences&quot;]]\n        else:\n            parsed_json[&quot;Experiences&quot;] = parsed_json.get(&quot;Experiences&quot;, [])\n\n        if isinstance(parsed_json.get(&quot;Educations&quot;), dict):\n            parsed_json[&quot;Educations&quot;] = [parsed_json[&quot;Educations&quot;]]\n        else:\n            parsed_json[&quot;Educations&quot;] = parsed_json.get(&quot;Educations&quot;, [])\n\n        parsed_json[&quot;Projects&quot;] = parsed_json.get(&quot;Projects&quot;, [])\n\n        return parsed_json\n</code></pre>\n<p>The df contains prompt, completion feature with resume_text as a prompt and json output as completion.</p>\n<p>Or Is there any Alternative to doing the task on a low-end specs laptop? I am in need of help because I can't access big parameter models. anybody??</p>\n",
         "2024-11-12 07:20:00",
         "2",
         "59",
         "1",
         "<nlp><prompt><langchain><large-language-model><huggingface>",
         null,
         null,
         "ERROR:Service.service:Error parsing resume: \"'ContactInformation'\"\n---\n\"detail\": \"Failed to parse the file: An error occurred in parsed_resume: \\\"'ContactInformation'\\\"\"\n---\nfrom typing import List, Optional, Union, Any, re\nimport json\n\n\n\nclass ContactInformation(BaseModel):\n    Name: Optional[str] = None\n    Email: Optional[str] = None\n    Contact: Optional[str] = None\n    Links: Optional[List[str]] = None\n\n\n\nclass Experience(BaseModel):\n    title: Optional[str] = None\n    company: Optional[str] = None\n    duration: Optional[str] = None\n\n\nclass Education(BaseModel):\n    course: Optional[str] = None\n    branch: Optional[str] = None\n    institute: Optional[str] = None\n\n\nclass Projects(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n    link: Optional[str] = None\n\nclass OutputFormat(BaseModel):\n    ContactInformation: Optional[Any] = None\n    AboutMe: Optional[Any] = None\n    Experiences: Optional[List[Any]] = None\n    Educations: Optional[List[Any]] = None\n    Skills: Optional[List[Any]] = None\n    Certificates: Optional[List[Any]] = None\n    Projects: Optional[List[Any]] = None\n    Achievements: Optional[List[Any]] = None\n    Volunteer: Optional[List[Any]] = None\n---\ndef __init__(self, model_name=model_1, fine_tune_model_path: str = None):\n        # Initialize LLM service with specified model\n\n\n        if fine_tune_model_path:\n            # Load fine-tuned model from local directory\n            self.tokenizer = AutoTokenizer.from_pretrained(fine_tune_model_path)\n            self.model = AutoModel.from_pretrained(fine_tune_model_path)\n        else:\n            # Load base model\n            self.llm_service = HuggingFaceHub(\n                repo_id=\"google/flan-t5-base\",\n                huggingfacehub_api_token=huggingface_api_key,\n                model_kwargs={\n                    \"temperature\": 0.5,\n                    \"max_new_tokens\": 200\n                }  # Model parameters for consistent output\n            )\n    def parsed_resume(self, resume_txt: str):\n        df = pd.read_csv(r\"C:\\Users\\Sarthak\\PycharmProjects\\JobAxle\\Service\\data\\for_model_resume_dataset.csv\")\n        print(df['prompt'][0])\n        examples = [\n            {'prompt':df['prompt'][0], \"completion\":df['completion'][0]},\n            {'prompt': df['prompt'][1], \"completion\": df['completion'][1]}\n        ]\n        print('Examples:',examples[0])\n        example_formatter_template = \"\"\"\n        {prompt}\n        {completion}\\n\n        \"\"\"\n        example_prompt = PromptTemplate(\n            input_variables=[\"prompt\", \"completion\"],\n            template=example_formatter_template,\n        )\n        parser = PydanticOutputParser(pydantic_object=OutputFormat)\n        few_shot_prompt_template = FewShotPromptTemplate(\n            examples=examples,\n            example_prompt=example_prompt,\n            suffix=\"\"\"\n                Parse the given resume text, ensuring the output in JSON format:\n        \n                Resume:\n                {resume}\n        \n                {format_instructions}\n        \n                Output as JSON below:\n                completion:\"\"\",\n            input_variables=[\"resume\"],\n            example_separator=\"\\n\",\n            partial_variables={\"format_instructions\": parser.get_format_instructions()}\n        )\n        print(\"Few-Shot Prompt Template with Examples and JSON Instructions:\\n\", few_shot_prompt_template)\n\n        prompt_template = PromptTemplate(\n            input_variables=['resume'],\n            template=Prompt_2\n        )\n        # print(few_shot_prompt_template)\n        # Initialize the LLM chain\n        chain = LLMChain(\n            llm=self.llm_service,\n            prompt=few_shot_prompt_template,\n            verbose=True\n        )\n        print(\"Chain:\", chain)\n        print(resume_txt)\n        try:\n            response = chain.invoke({'resume': resume_txt}, verbose=True)\n            print(response)\n            logger.info('Model Response: %s', response)\n            print(\"Type of Response:\",type(response))\n            # Invoke the chain and get a response\n            response_json = self.process_response(response)\n            parsed_json = self.structure_response(response_json)\n            return OutputFormat(**parsed_json)  # Return as OutputFormat object\n\n        except Exception as e:\n            logger.error(\"Error parsing resume: %s\", e)\n            raise Exception(f\"An error occurred in parsed_resume: {e}\")\n\n    def process_response(self, response_text: str) -> Dict:\n        \"\"\"Process LLM response into JSON format.\"\"\"\n        response_json = json.dumps(response_text).strip()\n\n        # Remove extraneous characters\n        if response_json.startswith(\"```json\"):\n            response_json = response_json[len(\"```json\"):].strip()\n        if response_json.endswith(\"```\"):\n            response_json = response_json[:-len(\"```\")].strip()\n\n        response_json = remove_trailing_commas(response_json)\n\n        try:\n\n            return json.loads(response_json)\n        except json.JSONDecodeError as e:\n            logger.error(\"JSON decoding error: %s. Response text: %s\", e, response_json)\n            raise ValueError(\"Failed to parse response as valid JSON\")\n\n    def structure_response(self, parsed_json: Dict) -> Dict:\n        \"\"\"\n        Structure response JSON to match the OutputFormat schema, ensuring lists for 'Experiences' and 'Educations'.\n        \"\"\"\n        # Ensure ContactInformation, Experiences, and Educations are present in the expected structure\n        parsed_json[\"ContactInformation\"] = parsed_json.get(\"ContactInformation\", {})\n        if isinstance(parsed_json.get(\"Experiences\"), dict):\n            parsed_json[\"Experiences\"] = [parsed_json[\"Experiences\"]]\n        else:\n            parsed_json[\"Experiences\"] = parsed_json.get(\"Experiences\", [])\n\n        if isinstance(parsed_json.get(\"Educations\"), dict):\n            parsed_json[\"Educations\"] = [parsed_json[\"Educations\"]]\n        else:\n            parsed_json[\"Educations\"] = parsed_json.get(\"Educations\", [])\n\n        parsed_json[\"Projects\"] = parsed_json.get(\"Projects\", [])\n\n        return parsed_json",
         "",
         "How to parse a resume with few shot method using the specified models from HuggingFace and Langchain?",
         "Model selection confusion and some errors while trying to parse a resume with the following codes Trying to do a few shot prompting with a google flan t5 base model While Doing so I am getting an error exception as The code is given below The df contains prompt, completion feature with resume_text as a prompt and json output as completion. Or Is there any Alternative to doing the task on a low-end specs laptop? I am in need of help because I can't access big parameter models. anybody??",
         "",
         "How to parse a resume with few shot method using the specified models from HuggingFace and Langchain? Model selection confusion and some errors while trying to parse a resume with the following codes Trying to do a few shot prompting with a google flan t5 base model While Doing so I am getting an error exception as The code is given below The df contains prompt, completion feature with resume_text as a prompt and json output as completion. Or Is there any Alternative to doing the task on a low-end specs laptop? I am in need of help because I can't access big parameter models. anybody?? ",
         "parse resume shot method using specified models huggingface langchain ? model selection confusion errors trying parse resume following codes trying shot prompting google flan t5 base model getting error exception code given df contains prompt , completion feature resume_text prompt json output completion . alternative task low-end specs laptop ? need help ca n't access big parameter models . anybody ? ?",
         "7"
        ],
        [
         "46",
         "79178041",
         "Normalization of token embeddings in BERT encoder blocks",
         "<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>\n",
         "2024-11-11 14:30:31",
         "2",
         "141",
         "2",
         "<nlp><normalization><bert-language-model><attention-model>",
         "79238393.0",
         "<p>I tracked down full details of layer normalization (LN) in BERT <a href=\"https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para\">here</a>.</p>\n<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>\n",
         "",
         "",
         "Normalization of token embeddings in BERT encoder blocks",
         "Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?",
         "I tracked down full details of layer normalization (LN) in BERT here . Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.",
         "Normalization of token embeddings in BERT encoder blocks Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)? I tracked down full details of layer normalization (LN) in BERT here . Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.",
         "normalization token embeddings bert encoder blocks following multi-headed attention layer bert encoder block , layer normalization done separately embedding token ( i.e. , one mean variance per token embedding ) , concatenated vector token embeddings ( mean variance embeddings ) ? tracked full details layer normalization ( ln ) bert . mean variance computed per token . weight bias parameters learned ln per token - 's per embedding dimension .",
         "7"
        ],
        [
         "47",
         "79177228",
         "DASK to_csv() problems due to memory",
         "<p>I'm cleaning my text data and afterwards want to save it to csv. Defined cleaning functions work fine, but when to_csv() part comes, here come the problems as well.\nMaybe someone have faced similar problem and has a trick to share with me how it would be possible to solve it? Maybe saving data to csv in chunks or something?</p>\n<p>`if <strong>name</strong> == '<strong>main</strong>':\n# Initialize the Dask client\nclient = Client(n_workers=3, threads_per_worker=1,\nmemory_limit='1.5GB')<br />\nprint('Dask client created')</p>\n<pre><code>PATH = &quot;C:\\\\Users\\\\el ruchenzo\\\\jobsproject\\\\jobsproject\\\\lt_data.csv&quot;\nreqd = ['description', 'title', 'code']\nblocksize = 25e6  # REDUCED FROM 100 GB TO 25 GB\n\n# Load the CSV with Dask\ndf = dd.read_csv(PATH,\n                 usecols=reqd,\n                 blocksize=blocksize,\n                 dtype={'Code': 'float'},\n                 engine='python',\n                 encoding='utf-8',\n                 on_bad_lines='skip')\n\n# Apply the cleaning function to the 'title' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_title'] = df['title'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('title', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f&quot;1. Processing time: {end_time - start_time:.2f} seconds&quot;)\n\n# Apply the cleaning function to the 'description' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_description'] = df['description'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('description', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f&quot;2. Processing time: {end_time - start_time:.2f} seconds&quot;)\n\ndf.to_csv('cleaned_lemma_*.csv', index=False, encoding='utf-8', single_file = False)\nprint('Saved to csv successfully')\n\nprint('Work ended successfully')`\n</code></pre>\n<p>Tried changing workers number, blocksize, memory limit and etc., but nothing seems to work. Also change map() to apply(), tried using df = df.persist(), writing data to csv with pandas in chunks instead with dask, and so on.</p>\n",
         "2024-11-11 10:13:33",
         "0",
         "38",
         "1",
         "<csv><text><nlp><export-to-csv><dask>",
         null,
         null,
         "PATH = \"C:\\\\Users\\\\el ruchenzo\\\\jobsproject\\\\jobsproject\\\\lt_data.csv\"\nreqd = ['description', 'title', 'code']\nblocksize = 25e6  # REDUCED FROM 100 GB TO 25 GB\n\n# Load the CSV with Dask\ndf = dd.read_csv(PATH,\n                 usecols=reqd,\n                 blocksize=blocksize,\n                 dtype={'Code': 'float'},\n                 engine='python',\n                 encoding='utf-8',\n                 on_bad_lines='skip')\n\n# Apply the cleaning function to the 'title' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_title'] = df['title'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('title', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f\"1. Processing time: {end_time - start_time:.2f} seconds\")\n\n# Apply the cleaning function to the 'description' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_description'] = df['description'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('description', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f\"2. Processing time: {end_time - start_time:.2f} seconds\")\n\ndf.to_csv('cleaned_lemma_*.csv', index=False, encoding='utf-8', single_file = False)\nprint('Saved to csv successfully')\n\nprint('Work ended successfully')`",
         "",
         "DASK to_csv() problems due to memory",
         "I'm cleaning my text data and afterwards want to save it to csv. Defined cleaning functions work fine, but when to_csv() part comes, here come the problems as well. Maybe someone have faced similar problem and has a trick to share with me how it would be possible to solve it? Maybe saving data to csv in chunks or something? `if name == ' main ': # Initialize the Dask client client = Client(n_workers=3, threads_per_worker=1, memory_limit='1.5GB') print('Dask client created') Tried changing workers number, blocksize, memory limit and etc., but nothing seems to work. Also change map() to apply(), tried using df = df.persist(), writing data to csv with pandas in chunks instead with dask, and so on.",
         "",
         "DASK to_csv() problems due to memory I'm cleaning my text data and afterwards want to save it to csv. Defined cleaning functions work fine, but when to_csv() part comes, here come the problems as well. Maybe someone have faced similar problem and has a trick to share with me how it would be possible to solve it? Maybe saving data to csv in chunks or something? `if name == ' main ': # Initialize the Dask client client = Client(n_workers=3, threads_per_worker=1, memory_limit='1.5GB') print('Dask client created') Tried changing workers number, blocksize, memory limit and etc., but nothing seems to work. Also change map() to apply(), tried using df = df.persist(), writing data to csv with pandas in chunks instead with dask, and so on. ",
         "dask to_csv ( ) problems due memory 'm cleaning text data afterwards want save csv . defined cleaning functions work fine , to_csv ( ) part comes , come problems well . maybe someone faced similar problem trick share would possible solve ? maybe saving data csv chunks something ? ` name == ' main ' : # initialize dask client client = client ( n_workers=3 , threads_per_worker=1 , memory_limit= ' 1.5gb ' ) print ( 'dask client created ' ) tried changing workers number , blocksize , memory limit etc. , nothing seems work . also change map ( ) apply ( ) , tried using df = df.persist ( ) , writing data csv pandas chunks instead dask , .",
         "2"
        ],
        [
         "48",
         "79173053",
         "How to convert character indices to BERT token indices",
         "<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>\n<pre><code>from datasets import load_dataset\nds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\n</code></pre>\n<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>\n<pre><code>d0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n</code></pre>\n<p>After tokenization, the answer indices are 56  and 16:</p>\n<pre><code>from transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'\n</code></pre>\n<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>\n<p>This is from a <a href=\"https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false\" rel=\"nofollow noreferrer\">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=\"https://i.sstatic.net/GsZ6mfcQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/GsZ6mfcQ.png\" alt=\"QA dataset with token answer indices\" /></a></p>\n",
         "2024-11-09 15:15:33",
         "2",
         "33",
         "1",
         "<python><nlp><dataset><large-language-model><bert-language-model>",
         "79175157.0",
         "<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>\n<p>The following function does the above for you:</p>\n<pre><code>def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start &lt;= char_start &lt; end: \n            start_token_idx = i\n        if start &lt; char_end &lt;= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n</code></pre>\n<p>Here's how you can use and test this function:</p>\n<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])\nprint(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(&quot;Tokenized answer:&quot;, answer_tokens)\n</code></pre>\n<p>Output:</p>\n<pre><code>Tokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream\n</code></pre>\n",
         "UCLNLP/adversarial_qa\n---\nfrom datasets import load_dataset\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\n---\nd0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n---\nfrom transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'",
         "def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start <= char_start < end: \n            start_token_idx = i\n        if start < char_end <= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n---\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(\"Tokenized start index:\", d0_tokenized['answer_start_token_idx'])\nprint(\"Tokenized end index:\", d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(\"Tokenized answer:\", answer_tokens)\n---\nTokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream",
         "How to convert character indices to BERT token indices",
         "I am working with a question-answer dataset . How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset: After tokenization, the answer indices are 56 and 16: I want to create a new dataset with the answer's token indices, e.g., 56 ad 60. This is from a linkedin learning class . The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:",
         "You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices. The following function does the above for you: Here's how you can use and test this function: Output:",
         "How to convert character indices to BERT token indices I am working with a question-answer dataset . How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset: After tokenization, the answer indices are 56 and 16: I want to create a new dataset with the answer's token indices, e.g., 56 ad 60. This is from a linkedin learning class . The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result: You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices. The following function does the above for you: Here's how you can use and test this function: Output:",
         "convert character indices bert token indices working question-answer dataset . map character-based answer indices token-based indices tokenizing context question together using tokenizer like bert . 's example row dataset : tokenization , answer indices 56 16 : want create new dataset answer 's token indices , e.g. , 56 ad 60. linkedin learning class . instructor conversion created csv file share code . expected result : encode question context , locate token span answer within tokenized context , update dataset token-level indices . following function : 's use test function : output :",
         "7"
        ],
        [
         "49",
         "79165649",
         "Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy",
         "<p>I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity.</p>\n<p>My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold.</p>\n<p>For instance, the text &quot;Standard PRS for Alzheimer's&quot; incorrectly links entities for &quot;Standard&quot; and &quot;PRS,&quot; in addition to &quot;Alzheimer's.&quot; Another example, &quot;Other diseases of respiratory system, NEC,&quot; captures &quot;respiratory&quot; and &quot;diseases&quot; but misses &quot;NEC&quot; (Necrotizing enterocolitis), which should be prioritized.</p>\n<p>I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms.</p>\n<p>I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well).</p>\n<p>I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd).</p>\n<p>I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must.</p>\n",
         "2024-11-07 08:52:14",
         "2",
         "34",
         "1",
         "<nlp><data-science><spacy><named-entity-recognition><entity-linking>",
         null,
         null,
         "",
         "",
         "Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy",
         "I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity. My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold. For instance, the text \"Standard PRS for Alzheimer's\" incorrectly links entities for \"Standard\" and \"PRS,\" in addition to \"Alzheimer's.\" Another example, \"Other diseases of respiratory system, NEC,\" captures \"respiratory\" and \"diseases\" but misses \"NEC\" (Necrotizing enterocolitis), which should be prioritized. I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms. I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well). I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd). I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must.",
         "",
         "Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity. My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold. For instance, the text \"Standard PRS for Alzheimer's\" incorrectly links entities for \"Standard\" and \"PRS,\" in addition to \"Alzheimer's.\" Another example, \"Other diseases of respiratory system, NEC,\" captures \"respiratory\" and \"diseases\" but misses \"NEC\" (Necrotizing enterocolitis), which should be prioritized. I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms. I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well). I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd). I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must. ",
         "handling multiple entity candidates short texts entity linking scispacy working linking short texts entities biomedical knowledge graph ( umls cuis ) using scispacy research project . goal analyze relationship linked entity separate predefined entity . challenge managing multiple possible entities identified texts , introduces noise results . although use heuristics regex , manual stop list , filtering semantic categories ( tuis ) clean data , issue persists due text complexity . typically select top ~3 entities per text based ner score , relatively high threshold . instance , text `` standard prs alzheimer 's '' incorrectly links entities `` standard '' `` prs , '' addition `` alzheimer 's . '' another example , `` diseases respiratory system , nec , '' captures `` respiratory '' `` diseases '' misses `` nec '' ( necrotizing enterocolitis ) , prioritized . 've tried filtering results semantic similarity using biomedical model , approach still imprecise heavily dependent number results . linker often seems prioritize entities appearing earlier text . also use abbreviation expander handle non-standard acronym forms . think smarter linker ( supported scispacy ) might help , better matching sentence/whole text level , n't know much . ( filtering results using sentence transformers , 's cossine sim - could n't find clear cutoff generalized well ) . resources/time learn fine-tune new linker model+data ( sub-component overall phd ) . 'm looking advice effective strategies entity linking sentence whole-text level without resources fine-tune new model . compatability scispacy important , since linkage umls ontology ( kg cui entites ) must .",
         "3"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 16679
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AllTags</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AcceptedAnswerBody</th>\n",
       "      <th>Question_Code</th>\n",
       "      <th>Answer_Code</th>\n",
       "      <th>Title_Clean</th>\n",
       "      <th>Body_Clean</th>\n",
       "      <th>AcceptedAnswerBody_Clean</th>\n",
       "      <th>combination_text</th>\n",
       "      <th>combination_text_no_stopw</th>\n",
       "      <th>cluster_kmean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this [site][...</td>\n",
       "      <td>2025-03-11 14:50:31</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;python&gt;&lt;nlp&gt;&lt;large-language-model&gt;</td>\n",
       "      <td>79501337.0</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "      <td>server\\n---\\nSSH\\n---\\nskip_tokens = [1]  # sk...</td>\n",
       "      <td>matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>I am running the code found on this site1 in m...</td>\n",
       "      <td>I can't test it but ... I checked source code ...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>store images instead showing server running co...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79498915</td>\n",
       "      <td>Comparing the similarity of spoken and written...</td>\n",
       "      <td>&lt;p&gt;I'm converting spoken form text to its writ...</td>\n",
       "      <td>2025-03-10 18:55:59</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;nlp&gt;&lt;large-language-model&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Comparing the similarity of spoken and written...</td>\n",
       "      <td>I'm converting spoken form text to its written...</td>\n",
       "      <td></td>\n",
       "      <td>Comparing the similarity of spoken and written...</td>\n",
       "      <td>comparing similarity spoken written form text ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79488426</td>\n",
       "      <td>Upserting in Pinecone takes too long</td>\n",
       "      <td>&lt;p&gt;I'm trying to upsert reviews that i've scra...</td>\n",
       "      <td>2025-03-06 06:22:35</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;python&gt;&lt;nlp&gt;&lt;rag&gt;&lt;pinecone&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jina-embedding-v3\\n---\\nif index_name not in p...</td>\n",
       "      <td></td>\n",
       "      <td>Upserting in Pinecone takes too long</td>\n",
       "      <td>I'm trying to upsert reviews that i've scraped...</td>\n",
       "      <td></td>\n",
       "      <td>Upserting in Pinecone takes too long I'm tryin...</td>\n",
       "      <td>upserting pinecone takes long 'm trying upsert...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79484448</td>\n",
       "      <td>How does ELMo generate words for training ? Is...</td>\n",
       "      <td>&lt;p&gt;I'm confused about using Bidirectional LM f...</td>\n",
       "      <td>2025-03-04 17:32:14</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;nlp&gt;&lt;language-model&gt;&lt;autoregressive-models&gt;&lt;e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>How does ELMo generate words for training ? Is...</td>\n",
       "      <td>I'm confused about using Bidirectional LM for ...</td>\n",
       "      <td></td>\n",
       "      <td>How does ELMo generate words for training ? Is...</td>\n",
       "      <td>elmo generate words training ? autoregressive ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79482290</td>\n",
       "      <td>How to handle German language specific charact...</td>\n",
       "      <td>&lt;p&gt;I am working with German Texts, where I nee...</td>\n",
       "      <td>2025-03-03 22:32:36</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;python&gt;&lt;nlp&gt;&lt;tokenize&gt;&lt;large-language-model&gt;&lt;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>from transformers import GPT2Tokenizer\\n\\ntext...</td>\n",
       "      <td></td>\n",
       "      <td>How to handle German language specific charact...</td>\n",
       "      <td>I am working with German Texts, where I need t...</td>\n",
       "      <td></td>\n",
       "      <td>How to handle German language specific charact...</td>\n",
       "      <td>handle german language specific characters lik...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>62328</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>&lt;p&gt;input: phrase 1, phrase 2&lt;/p&gt;\\n\\n&lt;p&gt;output:...</td>\n",
       "      <td>2008-09-15 12:26:42</td>\n",
       "      <td>65</td>\n",
       "      <td>49872</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;algorithm&gt;&lt;nlp&gt;&lt;semantics&gt;</td>\n",
       "      <td>63076.0</td>\n",
       "      <td>&lt;hr&gt;\\n\\n&lt;p&gt;You might want to check out this pa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>input: phrase 1, phrase 2 output: semantic sim...</td>\n",
       "      <td>You might want to check out this paper: Senten...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>algorithm tells semantic similarity two phrase...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16675</th>\n",
       "      <td>41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03 10:36:13</td>\n",
       "      <td>118</td>\n",
       "      <td>33149</td>\n",
       "      <td>17</td>\n",
       "      <td>&lt;nlp&gt;</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>&lt;p&gt;Actually what Google does is very much non-...</td>\n",
       "      <td>&lt;spell_checked_word&gt;</td>\n",
       "      <td></td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>Possible Duplicate: How does the Google Did yo...</td>\n",
       "      <td>Actually what Google does is much non-trivial ...</td>\n",
       "      <td>How do you implement a \"Did you mean\"? Possibl...</td>\n",
       "      <td>implement `` mean '' ? possible duplicate : go...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16676</th>\n",
       "      <td>36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31 01:08:48</td>\n",
       "      <td>3</td>\n",
       "      <td>5658</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;windows-vista&gt;&lt;nlp&gt;&lt;speech-recognition&gt;&lt;multi...</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>&lt;p&gt;Citation from Vista &lt;a href=\"http://blogs.m...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>my primary language is spanish, but I use all ...</td>\n",
       "      <td>Citation from Vista speech recognition blog : ...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>vista speech recognition multiple languages pr...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16677</th>\n",
       "      <td>25332</td>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>&lt;p&gt;I'm looking for an existing library to summ...</td>\n",
       "      <td>2008-08-24 20:57:33</td>\n",
       "      <td>14</td>\n",
       "      <td>6483</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;language-agnostic&gt;&lt;nlp&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>I'm looking for an existing library to summari...</td>\n",
       "      <td></td>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>'s good natural language library use paraphras...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16678</th>\n",
       "      <td>23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22 22:45:10</td>\n",
       "      <td>27</td>\n",
       "      <td>6462</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;.net&gt;&lt;datetime&gt;&lt;nlp&gt;</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>&lt;p&gt;We developed exactly what you are looking f...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>Does anyone know of a .NET date/time parser si...</td>\n",
       "      <td>We developed exactly what you are looking for ...</td>\n",
       "      <td>Natural language date/time parser for .NET? Do...</td>\n",
       "      <td>natural language date/time parser .net ? anyon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16679 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       QuestionId                                              Title  \\\n",
       "0        79501178        Store images instead of showing in a server   \n",
       "1        79498915  Comparing the similarity of spoken and written...   \n",
       "2        79488426               Upserting in Pinecone takes too long   \n",
       "3        79484448  How does ELMo generate words for training ? Is...   \n",
       "4        79482290  How to handle German language specific charact...   \n",
       "...           ...                                                ...   \n",
       "16674       62328  Is there an algorithm that tells the semantic ...   \n",
       "16675       41424             How do you implement a \"Did you mean\"?   \n",
       "16676       36533     Vista speech recognition in multiple languages   \n",
       "16677       25332  What's a good natural language library to use ...   \n",
       "16678       23689        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                    Body         CreationDate  \\\n",
       "0      <p>I am running the code found on this [site][...  2025-03-11 14:50:31   \n",
       "1      <p>I'm converting spoken form text to its writ...  2025-03-10 18:55:59   \n",
       "2      <p>I'm trying to upsert reviews that i've scra...  2025-03-06 06:22:35   \n",
       "3      <p>I'm confused about using Bidirectional LM f...  2025-03-04 17:32:14   \n",
       "4      <p>I am working with German Texts, where I nee...  2025-03-03 22:32:36   \n",
       "...                                                  ...                  ...   \n",
       "16674  <p>input: phrase 1, phrase 2</p>\\n\\n<p>output:...  2008-09-15 12:26:42   \n",
       "16675  <blockquote>\\n  <p><strong>Possible Duplicate:...  2008-09-03 10:36:13   \n",
       "16676  <p>my primary language is spanish, but I use a...  2008-08-31 01:08:48   \n",
       "16677  <p>I'm looking for an existing library to summ...  2008-08-24 20:57:33   \n",
       "16678  <p>Does anyone know of a .NET date/time parser...  2008-08-22 22:45:10   \n",
       "\n",
       "       Score  ViewCount  AnswerCount  \\\n",
       "0          0         23            1   \n",
       "1          0         20            1   \n",
       "2          1         37            1   \n",
       "3          0         28            1   \n",
       "4          1         59            1   \n",
       "...      ...        ...          ...   \n",
       "16674     65      49872           11   \n",
       "16675    118      33149           17   \n",
       "16676      3       5658            6   \n",
       "16677     14       6483            4   \n",
       "16678     27       6462            9   \n",
       "\n",
       "                                                 AllTags  AcceptedAnswerId  \\\n",
       "0                    <python><nlp><large-language-model>        79501337.0   \n",
       "1                            <nlp><large-language-model>               NaN   \n",
       "2                           <python><nlp><rag><pinecone>               NaN   \n",
       "3      <nlp><language-model><autoregressive-models><e...               NaN   \n",
       "4      <python><nlp><tokenize><large-language-model><...               NaN   \n",
       "...                                                  ...               ...   \n",
       "16674                        <algorithm><nlp><semantics>           63076.0   \n",
       "16675                                              <nlp>           41448.0   \n",
       "16676  <windows-vista><nlp><speech-recognition><multi...           36684.0   \n",
       "16677                           <language-agnostic><nlp>               NaN   \n",
       "16678                              <.net><datetime><nlp>          631134.0   \n",
       "\n",
       "                                      AcceptedAnswerBody  \\\n",
       "0      <p>I can't test it but ...</p>\\n<p>I checked <...   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "16674  <hr>\\n\\n<p>You might want to check out this pa...   \n",
       "16675  <p>Actually what Google does is very much non-...   \n",
       "16676  <p>Citation from Vista <a href=\"http://blogs.m...   \n",
       "16677                                                NaN   \n",
       "16678  <p>We developed exactly what you are looking f...   \n",
       "\n",
       "                                           Question_Code  \\\n",
       "0      server\\n---\\nSSH\\n---\\nskip_tokens = [1]  # sk...   \n",
       "1                                                          \n",
       "2      jina-embedding-v3\\n---\\nif index_name not in p...   \n",
       "3                                                          \n",
       "4      from transformers import GPT2Tokenizer\\n\\ntext...   \n",
       "...                                                  ...   \n",
       "16674                                                      \n",
       "16675                               <spell_checked_word>   \n",
       "16676                                                      \n",
       "16677                                                      \n",
       "16678                                                      \n",
       "\n",
       "                                             Answer_Code  \\\n",
       "0      matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...   \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          \n",
       "...                                                  ...   \n",
       "16674                                                      \n",
       "16675                                                      \n",
       "16676                                                      \n",
       "16677                                                      \n",
       "16678                                                      \n",
       "\n",
       "                                             Title_Clean  \\\n",
       "0            Store images instead of showing in a server   \n",
       "1      Comparing the similarity of spoken and written...   \n",
       "2                   Upserting in Pinecone takes too long   \n",
       "3      How does ELMo generate words for training ? Is...   \n",
       "4      How to handle German language specific charact...   \n",
       "...                                                  ...   \n",
       "16674  Is there an algorithm that tells the semantic ...   \n",
       "16675             How do you implement a \"Did you mean\"?   \n",
       "16676     Vista speech recognition in multiple languages   \n",
       "16677  What's a good natural language library to use ...   \n",
       "16678        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                              Body_Clean  \\\n",
       "0      I am running the code found on this site1 in m...   \n",
       "1      I'm converting spoken form text to its written...   \n",
       "2      I'm trying to upsert reviews that i've scraped...   \n",
       "3      I'm confused about using Bidirectional LM for ...   \n",
       "4      I am working with German Texts, where I need t...   \n",
       "...                                                  ...   \n",
       "16674  input: phrase 1, phrase 2 output: semantic sim...   \n",
       "16675  Possible Duplicate: How does the Google Did yo...   \n",
       "16676  my primary language is spanish, but I use all ...   \n",
       "16677  I'm looking for an existing library to summari...   \n",
       "16678  Does anyone know of a .NET date/time parser si...   \n",
       "\n",
       "                                AcceptedAnswerBody_Clean  \\\n",
       "0      I can't test it but ... I checked source code ...   \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          \n",
       "...                                                  ...   \n",
       "16674  You might want to check out this paper: Senten...   \n",
       "16675  Actually what Google does is much non-trivial ...   \n",
       "16676  Citation from Vista speech recognition blog : ...   \n",
       "16677                                                      \n",
       "16678  We developed exactly what you are looking for ...   \n",
       "\n",
       "                                        combination_text  \\\n",
       "0      Store images instead of showing in a server I ...   \n",
       "1      Comparing the similarity of spoken and written...   \n",
       "2      Upserting in Pinecone takes too long I'm tryin...   \n",
       "3      How does ELMo generate words for training ? Is...   \n",
       "4      How to handle German language specific charact...   \n",
       "...                                                  ...   \n",
       "16674  Is there an algorithm that tells the semantic ...   \n",
       "16675  How do you implement a \"Did you mean\"? Possibl...   \n",
       "16676  Vista speech recognition in multiple languages...   \n",
       "16677  What's a good natural language library to use ...   \n",
       "16678  Natural language date/time parser for .NET? Do...   \n",
       "\n",
       "                               combination_text_no_stopw  cluster_kmean  \n",
       "0      store images instead showing server running co...              8  \n",
       "1      comparing similarity spoken written form text ...              3  \n",
       "2      upserting pinecone takes long 'm trying upsert...              8  \n",
       "3      elmo generate words training ? autoregressive ...              7  \n",
       "4      handle german language specific characters lik...              9  \n",
       "...                                                  ...            ...  \n",
       "16674  algorithm tells semantic similarity two phrase...              3  \n",
       "16675  implement `` mean '' ? possible duplicate : go...              8  \n",
       "16676  vista speech recognition multiple languages pr...              8  \n",
       "16677  's good natural language library use paraphras...              3  \n",
       "16678  natural language date/time parser .net ? anyon...              1  \n",
       "\n",
       "[16679 rows x 18 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_title = model.encode(df['Title_Clean'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5,min_samples=5,metric='euclidean')\n",
    "clusters = dbscan.fit_predict(embeddings_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AllTags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Question_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Answer_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerBody_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_no_stopw",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "98c39de1-46c0-4bf7-a2c7-82bd6026ba04",
       "rows": [
        [
         "0",
         "79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this [site][1] in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?\n[1]: <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Llama2_LLM_Attribution</a></p>\n",
         "2025-03-11 14:50:31",
         "0",
         "23",
         "1",
         "<python><nlp><large-language-model>",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n",
         "server\n---\nSSH\n---\nskip_tokens = [1]  # skip the special token for the start of the text <s>\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = \"playing guitar, hiking, and spending time with his family.\"\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)",
         "matplotlib\n---\nshow=True\n---\nfig, ax\n---\nimport matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(\"output.png\")\n#plt.show()  # eventually show it after saving\n---\nfig\n---\nfig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(\"output.png\")",
         "Store images instead of showing in a server",
         "I am running the code found on this site1 in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection. The code is for instance this one: How to store the files locally instead of showing them? 1:",
         "I can't test it but ... I checked source code and it uses for this. If you remove then it shouldn't show it but it should only get . I think you could use matplotlib.pyplot.savefig(filename) to save it in file. Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site1 in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection. The code is for instance this one: How to store the files locally instead of showing them? 1: I can't test it but ... I checked source code and it uses for this. If you remove then it shouldn't show it but it should only get . I think you could use matplotlib.pyplot.savefig(filename) to save it in file. Probably you can also use for this",
         "store images instead showing server running code found site1 server would like store images instead showing since connected remotely ssh connection via connection . code instance one : store files locally instead showing ? 1 : ca n't test ... checked source code uses . remove n't show get . think could use matplotlib.pyplot.savefig ( filename ) save file . probably also use"
        ],
        [
         "1",
         "79498915",
         "Comparing the similarity of spoken and written form text",
         "<p>I'm converting spoken form text to its written form. For example, &quot;he owes me two-thousand dollars&quot; should be converted to &quot;he owes me $2,000&quot; . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of &quot;two-thousand dollars&quot; to &quot;$2,000&quot; to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this?</p>\n",
         "2025-03-10 18:55:59",
         "0",
         "20",
         "1",
         "<nlp><large-language-model>",
         null,
         null,
         "",
         "",
         "Comparing the similarity of spoken and written form text",
         "I'm converting spoken form text to its written form. For example, \"he owes me two-thousand dollars\" should be converted to \"he owes me $2,000\" . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of \"two-thousand dollars\" to \"$2,000\" to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this?",
         "",
         "Comparing the similarity of spoken and written form text I'm converting spoken form text to its written form. For example, \"he owes me two-thousand dollars\" should be converted to \"he owes me $2,000\" . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of \"two-thousand dollars\" to \"$2,000\" to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this? ",
         "comparing similarity spoken written form text 'm converting spoken form text written form . example , `` owes two-thousand dollars '' converted `` owes $ 2,000 '' . want automatic check , judge conversion right . use sentence transformers compare embeddings `` two-thousand dollars '' `` $ 2,000 '' check spoken written conversion right ? example , cosine similarity embeddings close 1 , would mean right conversion . better way ?"
        ],
        [
         "2",
         "79488426",
         "Upserting in Pinecone takes too long",
         "<p>I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using <code>jina-embedding-v3</code>. For 204 reviews this takes around <strong>2.5 hours!</strong> in Colab. Tried using GPU but the embeddings arent using GPU.\nAm i doing something wrong? Is there a way that i can speed up the process? The code is below:</p>\n<p>Initialising DB:</p>\n<pre><code>if index_name not in pc.list_indexes().names():\n  pc.create_index(\n    name=index_name,\n    dimension=1024,\n    metric=&quot;cosine&quot;,\n    spec=ServerlessSpec(\n        cloud=&quot;aws&quot;,\n        region=&quot;us-east-1&quot;\n    )\n)\n</code></pre>\n<p>Embedding &amp; Upserting:</p>\n<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the Jina embedding model and tokenizer from Hugging Face\nmodel_name = &quot;jinaai/jina-embeddings-v3&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\nfrom langchain.text_splitter import SpacyTextSplitter\ntext_splitter = SpacyTextSplitter(chunk_size=500)\n\n# Function to generate embeddings\ndef generate_embeddings(text, task='retrieval.passage'):\n    return model.encode(text, convert_to_tensor=True, task=task).numpy()\n\nfor review_id, review in enumerate(all_reviews[:2]):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n\n# Generate and store embeddings\nfor review_id, review in enumerate(all_reviews):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n</code></pre>\n",
         "2025-03-06 06:22:35",
         "1",
         "37",
         "1",
         "<python><nlp><rag><pinecone>",
         null,
         null,
         "jina-embedding-v3\n---\nif index_name not in pc.list_indexes().names():\n  pc.create_index(\n    name=index_name,\n    dimension=1024,\n    metric=\"cosine\",\n    spec=ServerlessSpec(\n        cloud=\"aws\",\n        region=\"us-east-1\"\n    )\n)\n---\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the Jina embedding model and tokenizer from Hugging Face\nmodel_name = \"jinaai/jina-embeddings-v3\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\nfrom langchain.text_splitter import SpacyTextSplitter\ntext_splitter = SpacyTextSplitter(chunk_size=500)\n\n# Function to generate embeddings\ndef generate_embeddings(text, task='retrieval.passage'):\n    return model.encode(text, convert_to_tensor=True, task=task).numpy()\n\nfor review_id, review in enumerate(all_reviews[:2]):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f\"{review_id}_{chunk_index}\"\n\n        metadata = {\"review_id\": review_id, \"chunk_index\": chunk_index, \"text\": chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n\n# Generate and store embeddings\nfor review_id, review in enumerate(all_reviews):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f\"{review_id}_{chunk_index}\"\n\n        metadata = {\"review_id\": review_id, \"chunk_index\": chunk_index, \"text\": chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])",
         "",
         "Upserting in Pinecone takes too long",
         "I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using . For 204 reviews this takes around 2.5 hours! in Colab. Tried using GPU but the embeddings arent using GPU. Am i doing something wrong? Is there a way that i can speed up the process? The code is below: Initialising DB: Embedding & Upserting:",
         "",
         "Upserting in Pinecone takes too long I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using . For 204 reviews this takes around 2.5 hours! in Colab. Tried using GPU but the embeddings arent using GPU. Am i doing something wrong? Is there a way that i can speed up the process? The code is below: Initialising DB: Embedding & Upserting: ",
         "upserting pinecone takes long 'm trying upsert reviews 've scraped pinecone . embedding model im using . 204 reviews takes around 2.5 hours ! colab . tried using gpu embeddings arent using gpu . something wrong ? way speed process ? code : initialising db : embedding & upserting :"
        ],
        [
         "3",
         "79484448",
         "How does ELMo generate words for training ? Is it autoregressive?",
         "<p>I'm confused about using Bidirectional LM for words prediction and loss computing  while training.</p>\n<p>At first we have a sequence of tokens X1, ..., Xn.</p>\n<p>After gathering their context independent embeddings via CNN... we pass them to 2-layer Stacked BiLSTM.</p>\n<p>BiLSTM works as a tagger here, and for every input token X1, ...,Xn we get probabilities of the next token (for X1 it <strong>should</strong> be X2, for X2 - X3 and etc.). For Xn it will be a probability of the next word in the sentence - Y_n.</p>\n<p>Now, we can compute loss for every token.</p>\n<p>So, I don't understand what we do next. Does ELMo works like an autoregressive LM?</p>\n<p>If it does and we put the newly predicted token Y_n right after X1, ...,Xn and feed it to the BiLSTM to predict the second new word - Y_{n+1}, won't predictions for all previous tokens change ?</p>\n<p>I guess they should because of the Bidirectional nature of the model. There will be new context from the right, and the model can change everything.</p>\n<p>Can we simply compute loss for every token again, not just for a new one ?</p>\n<p>But, if we want to use this model like LM in inference, and predict next <strong>several</strong> words, newly predicted words will affect previous. What predictions should we use ?</p>\n<p>We pass X1, ..., Xn to the ElMo. We get new word Y1. We pass X1, ..., Xn, Y1 to the ELMo. We get another new word Y2 and the previous word Y1 changes to Z1. Should we take Z1, Y2 as an answer, or we freeze Y1, ignore Z1 and use Y1, Y2 as an answer?</p>\n<p>In Transformer this problem is solved by Masked Self-Attention in decoder, and newly predicted words won't affect previous.</p>\n<p>I tried looking for the answer in the original paper, but didn't find anything about the training process.</p>\n",
         "2025-03-04 17:32:14",
         "0",
         "28",
         "1",
         "<nlp><language-model><autoregressive-models><elmo>",
         null,
         null,
         "",
         "",
         "How does ELMo generate words for training ? Is it autoregressive?",
         "I'm confused about using Bidirectional LM for words prediction and loss computing while training. At first we have a sequence of tokens X1, ..., Xn. After gathering their context independent embeddings via CNN... we pass them to 2-layer Stacked BiLSTM. BiLSTM works as a tagger here, and for every input token X1, ...,Xn we get probabilities of the next token (for X1 it should be X2, for X2 - X3 and etc.). For Xn it will be a probability of the next word in the sentence - Y_n. Now, we can compute loss for every token. So, I don't understand what we do next. Does ELMo works like an autoregressive LM? If it does and we put the newly predicted token Y_n right after X1, ...,Xn and feed it to the BiLSTM to predict the second new word - Y_{n+1}, won't predictions for all previous tokens change ? I guess they should because of the Bidirectional nature of the model. There will be new context from the right, and the model can change everything. Can we simply compute loss for every token again, not just for a new one ? But, if we want to use this model like LM in inference, and predict next several words, newly predicted words will affect previous. What predictions should we use ? We pass X1, ..., Xn to the ElMo. We get new word Y1. We pass X1, ..., Xn, Y1 to the ELMo. We get another new word Y2 and the previous word Y1 changes to Z1. Should we take Z1, Y2 as an answer, or we freeze Y1, ignore Z1 and use Y1, Y2 as an answer? In Transformer this problem is solved by Masked Self-Attention in decoder, and newly predicted words won't affect previous. I tried looking for the answer in the original paper, but didn't find anything about the training process.",
         "",
         "How does ELMo generate words for training ? Is it autoregressive? I'm confused about using Bidirectional LM for words prediction and loss computing while training. At first we have a sequence of tokens X1, ..., Xn. After gathering their context independent embeddings via CNN... we pass them to 2-layer Stacked BiLSTM. BiLSTM works as a tagger here, and for every input token X1, ...,Xn we get probabilities of the next token (for X1 it should be X2, for X2 - X3 and etc.). For Xn it will be a probability of the next word in the sentence - Y_n. Now, we can compute loss for every token. So, I don't understand what we do next. Does ELMo works like an autoregressive LM? If it does and we put the newly predicted token Y_n right after X1, ...,Xn and feed it to the BiLSTM to predict the second new word - Y_{n+1}, won't predictions for all previous tokens change ? I guess they should because of the Bidirectional nature of the model. There will be new context from the right, and the model can change everything. Can we simply compute loss for every token again, not just for a new one ? But, if we want to use this model like LM in inference, and predict next several words, newly predicted words will affect previous. What predictions should we use ? We pass X1, ..., Xn to the ElMo. We get new word Y1. We pass X1, ..., Xn, Y1 to the ELMo. We get another new word Y2 and the previous word Y1 changes to Z1. Should we take Z1, Y2 as an answer, or we freeze Y1, ignore Z1 and use Y1, Y2 as an answer? In Transformer this problem is solved by Masked Self-Attention in decoder, and newly predicted words won't affect previous. I tried looking for the answer in the original paper, but didn't find anything about the training process. ",
         "elmo generate words training ? autoregressive ? 'm confused using bidirectional lm words prediction loss computing training . first sequence tokens x1 , ... , xn . gathering context independent embeddings via cnn ... pass 2-layer stacked bilstm . bilstm works tagger , every input token x1 , ... , xn get probabilities next token ( x1 x2 , x2 - x3 etc. ) . xn probability next word sentence - y_n . , compute loss every token . , n't understand next . elmo works like autoregressive lm ? put newly predicted token y_n right x1 , ... , xn feed bilstm predict second new word - y_ { n+1 } , wo n't predictions previous tokens change ? guess bidirectional nature model . new context right , model change everything . simply compute loss every token , new one ? , want use model like lm inference , predict next several words , newly predicted words affect previous . predictions use ? pass x1 , ... , xn elmo . get new word y1 . pass x1 , ... , xn , y1 elmo . get another new word y2 previous word y1 changes z1 . take z1 , y2 answer , freeze y1 , ignore z1 use y1 , y2 answer ? transformer problem solved masked self-attention decoder , newly predicted words wo n't affect previous . tried looking answer original paper , n't find anything training process ."
        ],
        [
         "4",
         "79482290",
         "How to handle German language specific characters like (ä, ö, ü, ß) while tokenizing using GPT2Tokenizer?",
         "<p>I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer.</p>\n<p>To tokenize the text, I wrote the implementation as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import GPT2Tokenizer\n\ntext = &quot;zügiger Transport des ABCD stabilen Kindes in die Notaufnahme UKA&quot;\ntext = text.encode(&quot;utf-8&quot;).decode(&quot;utf-8&quot;)  # Re-encode to fix encoding issues\n\n# Load GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\n\nprint(tokens)  # Should properly tokenize &quot;zügiger&quot; instead of splitting &quot;ü&quot;\n</code></pre>\n<p>Now, when I execute this code snippet I get output as follows:</p>\n<pre><code>['z', 'Ã¼', 'g', 'iger', 'ĠTransport', 'Ġdes', 'ĠABC', 'D', 'Ġstabil', 'en', 'ĠKind', 'es', 'Ġin', 'Ġdie', 'ĠNot', 'au', 'fn', 'ah', 'me', 'ĠUK', 'A']\n</code></pre>\n<p>After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below.</p>\n<pre class=\"lang-markdown prettyprint-override\"><code>| Character | UTF-8 Bytes | Misdecoded as Latin-1 | Resulting String |\n|-----------|-------------|-----------------------|------------------|\n| ä         | C3 A4       | Ã + ¤                 | Ã¤               |\n| ö         | C3 B6       | Ã + ¶                 | Ã¶               |\n| ü         | C3 BC       | Ã + ¼                 | Ã¼               |\n| ß         | C3 9F       | Ã + Ÿ                 | ÃŸ               |\n</code></pre>\n<p>Now, how I can keep German language specific characters like (ä, ö, ü, ß) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. &quot;zügiger&quot; becomes something like ['z', 'ü', 'g', 'iger'].</p>\n",
         "2025-03-03 22:32:36",
         "1",
         "59",
         "1",
         "<python><nlp><tokenize><large-language-model><gpt-2>",
         null,
         null,
         "from transformers import GPT2Tokenizer\n\ntext = \"zügiger Transport des ABCD stabilen Kindes in die Notaufnahme UKA\"\ntext = text.encode(\"utf-8\").decode(\"utf-8\")  # Re-encode to fix encoding issues\n\n# Load GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\n\nprint(tokens)  # Should properly tokenize \"zügiger\" instead of splitting \"ü\"\n---\n['z', 'Ã¼', 'g', 'iger', 'ĠTransport', 'Ġdes', 'ĠABC', 'D', 'Ġstabil', 'en', 'ĠKind', 'es', 'Ġin', 'Ġdie', 'ĠNot', 'au', 'fn', 'ah', 'me', 'ĠUK', 'A']\n---\n| Character | UTF-8 Bytes | Misdecoded as Latin-1 | Resulting String |\n|-----------|-------------|-----------------------|------------------|\n| ä         | C3 A4       | Ã + ¤                 | Ã¤               |\n| ö         | C3 B6       | Ã + ¶                 | Ã¶               |\n| ü         | C3 BC       | Ã + ¼                 | Ã¼               |\n| ß         | C3 9F       | Ã + Ÿ                 | ÃŸ               |",
         "",
         "How to handle German language specific characters like (, , , ) while tokenizing using GPT2Tokenizer?",
         "I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer. To tokenize the text, I wrote the implementation as follows: Now, when I execute this code snippet I get output as follows: After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below. Now, how I can keep German language specific characters like (, , , ) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. \"zgiger\" becomes something like 'z', '', 'g', 'iger'.",
         "",
         "How to handle German language specific characters like (, , , ) while tokenizing using GPT2Tokenizer? I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer. To tokenize the text, I wrote the implementation as follows: Now, when I execute this code snippet I get output as follows: After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below. Now, how I can keep German language specific characters like (, , , ) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. \"zgiger\" becomes something like 'z', '', 'g', 'iger'. ",
         "handle german language specific characters like ( , , , ) tokenizing using gpt2tokenizer ? working german texts , need tokenize texts using gpt2tokenizer . tokenize text , wrote implementation follows : , execute code snippet get output follows : bit analysis , found german language specific characters mis-decoded latin-1 see table . , keep german language specific characters like ( , , , ) inside tokens tokenization process , avoiding unintentional misdecodeding , i.e . `` zgiger '' becomes something like ' z ' , `` , ' g ' , 'iger ' ."
        ],
        [
         "5",
         "79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03 22:27:07",
         "4",
         "182",
         "2",
         "<python><nlp><spacy><langchain><presidio>",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n",
         "from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"pl\", \"model_name\": \"pl_core_news_lg\"}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                               languages_config=config)\n\ntext = \"Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\"\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(\"Anonymized text:\", anonymized_result)\nprint(\"Deanonymized text:\", deanonymized_result)\nprint(\"Map:\", anonymizer_tool.deanonymizer_mapping)\nprint(\"Anonymized text:\", anon_result)\n---\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\npython -m spacy download pl_core_news_lg\n---\nanonymizer_tool = PresidioReversibleAnonymizer()\n---\nAnonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n---\nnlp = spacy.load(\"pl_core_news_lg\")\ndoc = nlp(text)\n---\nJan Kowalski persName\nWarszawie placeName",
         "config = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": 'pl', \"model_name\": \"pl_core_news_lg\"}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=\"pl\",\n    supported_entities=[\"persName\"]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"CREDIT_CARD\"], languages_config=config)\n---\nAnonymized text: <persName> mieszka w Warszawie i ma e-mail glenn58@example.org.\n---\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\nMap: {'persName': {'<persName>': 'Jan Kowalski', '<persName_2>': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}\n---\nSpacyRecognizer\n---\nsupported_entities",
         "Presidio with Langchain Experimental does not detect Polish names",
         "I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., \"Jan Kowalski\"). Here is my code: Output: I expected the name \"Jan Kowalski\" and the email address to be anonymized, but the output remains unchanged. I have installed the pl_core_news_lg model using: Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly? Any suggestions on how to make it detect names in Polish? The interesting thing is that when I use only Then the output look like this: As mentioned below if I use only spaCy: Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy: So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected.",
         "After some test I was able to find the solution: The output look like this: You need to directly add with formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., \"Jan Kowalski\"). Here is my code: Output: I expected the name \"Jan Kowalski\" and the email address to be anonymized, but the output remains unchanged. I have installed the pl_core_news_lg model using: Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly? Any suggestions on how to make it detect names in Polish? The interesting thing is that when I use only Then the output look like this: As mentioned below if I use only spaCy: Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy: So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected. After some test I was able to find the solution: The output look like this: You need to directly add with formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.",
         "presidio langchain experimental detect polish names using presidio/langchain_experimental anonymize text polish , detect names ( e.g. , `` jan kowalski '' ) . code : output : expected name `` jan kowalski '' email address anonymized , output remains unchanged . installed pl_core_news_lg model using : missing something configuration , presidio support polish entity recognition properly ? suggestions make detect names polish ? interesting thing use output look like : mentioned use spacy : output correct guess 's problem presidio . output spacy : would like create custom analyzer use spacy presidio works expected . test able find solution : output look like : need directly add formatted according spacy 's requirements . believe 's something missing unclear documentation , causing misunderstanding ."
        ],
        [
         "6",
         "79465047",
         "Where is the HuggingFace model saved in when loading a model on colab?",
         "<p>I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.).</p>\n<pre><code>model_id = &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;\n\n\npipeline = transformers.pipeline(\n            &quot;text-generation&quot;,\n            model=model_id,\n            #model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16, &quot;cache_dir&quot;: cache_dir},\n            device_map=&quot;auto&quot;)\n</code></pre>\n",
         "2025-02-24 23:25:42",
         "1",
         "46",
         "1",
         "<nlp><huggingface-transformers><huggingface>",
         null,
         null,
         "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n\n\npipeline = transformers.pipeline(\n            \"text-generation\",\n            model=model_id,\n            #model_kwargs={\"torch_dtype\": torch.bfloat16, \"cache_dir\": cache_dir},\n            device_map=\"auto\")",
         "",
         "Where is the HuggingFace model saved in when loading a model on colab?",
         "I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.).",
         "",
         "Where is the HuggingFace model saved in when loading a model on colab? I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.). ",
         "huggingface model saved loading model colab ? code loading generative model . 'm sure see model files colab ( i.e. , config.json etc . ) ."
        ],
        [
         "7",
         "79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22 16:06:11",
         "1",
         "31",
         "1",
         "<nlp><opennlp>",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n",
         "",
         "",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using TokenizerME , then I tagged it with POSTaggerME . For both I used the ready-made models provided by the project here . For the sentence Ivo viu a uva, POSTaggerME returns the tags PROPN, VERB, DET, NOUN. The model seems to be using the UD POS Tags . As there is no ready-made model for ChunkerME in portuguese, I followed the instructions and did the training first using the ChunkerConverter tool (to convert from \"arvore deitada\" to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags (B-NP, B-VP, B-NP, I-NP). But, for more complex sentences, it hasn't produced such good results. I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus ( Bosque 8.0 ) seems to be using portuguese tags. For example, instead of PROPN , the corpus uses prop and instead of DET , it uses art . It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag... But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if this does indeed have an impact, there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall.",
         "Q1 Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner: Penn -> UD should work well. UD -> Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the \"classic' Penn tag set. Using a custom, language specific tag-set can work, but it is a matter of \"mapping\" from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy. Q2 No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang. Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists . Alternatively, feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition.",
         "OpenNLP POSTaggerME and ChunkerME synergy I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using TokenizerME , then I tagged it with POSTaggerME . For both I used the ready-made models provided by the project here . For the sentence Ivo viu a uva, POSTaggerME returns the tags PROPN, VERB, DET, NOUN. The model seems to be using the UD POS Tags . As there is no ready-made model for ChunkerME in portuguese, I followed the instructions and did the training first using the ChunkerConverter tool (to convert from \"arvore deitada\" to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags (B-NP, B-VP, B-NP, I-NP). But, for more complex sentences, it hasn't produced such good results. I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus ( Bosque 8.0 ) seems to be using portuguese tags. For example, instead of PROPN , the corpus uses prop and instead of DET , it uses art . It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag... But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if this does indeed have an impact, there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall. Q1 Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner: Penn -> UD should work well. UD -> Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the \"classic' Penn tag set. Using a custom, language specific tag-set can work, but it is a matter of \"mapping\" from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy. Q2 No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang. Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists . Alternatively, feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition.",
         "opennlp postaggerme chunkerme synergy 'm trying use opennlp chunking api chunk portuguese sentence . , first tokenized sentence using tokenizerme , tagged postaggerme . used ready-made models provided project . sentence ivo viu uva , postaggerme returns tags propn , verb , det , noun . model seems using ud pos tags . ready-made model chunkerme portuguese , followed instructions training first using chunkerconverter tool ( convert `` arvore deitada '' conll2000 ) generating model chunkertrainerme tool . everything worked well . sentence , chunker produced correct tags ( b-np , b-vp , b-np , i-np ) . , complex sentences , n't produced good results . trying identify could improve chunker training , one things noticed difference types tags . portuguese corpus ( bosque 8.0 ) seems using portuguese tags . example , instead propn , corpus uses prop instead det , uses art . seems could lead problems , especially since one parameters chunker receives array ud tags , trained another type tag ... writing code creating routine convert portuguese notation ud ( penn ) wanted ask , indeed impact , tool already translation suggestions improving chunker precision/recall . q1 yes , chosen tag set ( ud , penn , custom ) impact . conversion possible bi-directional manner : penn - > ud work well . ud - > penn good idea lossy conversion . ud tag set less detailed compared `` classic ' penn tag set . using custom , language specific tag-set work , matter `` mapping '' from/to ud correctly . might work tag sets languages , others might complicated / lossy . q2 , n't . opennlp project takes code donations upcoming releases , want provide mapping/translation pt lang . q3 needs details/discussion apache opennlp user and/or dev mailing lists . alternatively , feel free open jira issue drill topic clear idea proposed code addition ."
        ],
        [
         "8",
         "79451974",
         "word/ sentence similarities",
         "<p>I am trying to find if a given word/ set of words are similar to a definition.</p>\n<p>Example - Definition - &quot;vegetarian User&quot;</p>\n<p>Now, if I want to check a set of sentences like below</p>\n<pre><code>sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n</code></pre>\n<p>I tried using some sentence transformer like below</p>\n<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n</code></pre>\n<p>But this is not giving me expected results.</p>\n<p>What is the best approach to achieve results like below?</p>\n<pre><code>[False,True,True,False]\n</code></pre>\n<p>Is it doable with nlp/ some other technique?</p>\n",
         "2025-02-19 15:47:45",
         "1",
         "43",
         "1",
         "<python><python-3.x><nlp>",
         null,
         null,
         "sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n---\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n---\n[False,True,True,False]",
         "",
         "word/ sentence similarities",
         "I am trying to find if a given word/ set of words are similar to a definition. Example - Definition - \"vegetarian User\" Now, if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results. What is the best approach to achieve results like below? Is it doable with nlp/ some other technique?",
         "",
         "word/ sentence similarities I am trying to find if a given word/ set of words are similar to a definition. Example - Definition - \"vegetarian User\" Now, if I want to check a set of sentences like below I tried using some sentence transformer like below But this is not giving me expected results. What is the best approach to achieve results like below? Is it doable with nlp/ some other technique? ",
         "word/ sentence similarities trying find given word/ set words similar definition . example - definition - `` vegetarian user '' , want check set sentences like tried using sentence transformer like giving expected results . best approach achieve results like ? doable nlp/ technique ?"
        ],
        [
         "9",
         "79449476",
         "How do I remove escape characters from output of nltk.word_tokenize?",
         "<p>How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output.</p>\n<p>Here's my code:</p>\n<pre><code>from __future__ import division\nimport nltk, re, pprint\nfrom urllib.request import urlopen\n\nurl = &quot;https://www.gutenberg.org/cache/epub/75394/pg75394.txt&quot;\nraw = urlopen(url).read()\nraw = raw.decode('utf-8')\ntokens = nltk.word_tokenize(raw)\nprint(type(tokens))\nprint(len(tokens))\nprint(tokens[:10])\n</code></pre>\n<p>And the output, with the escapes visible in the first list item:\n<a href=\"https://i.sstatic.net/L1QJ1Mdr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/L1QJ1Mdr.png\" alt=\"enter image description here\" /></a></p>\n<p>I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong?</p>\n<p>I'm using Python 3.12.1 on Windows 11.</p>\n<p>Thanks in advance - please do let me know if I can provide any further helpful information.</p>\n",
         "2025-02-18 20:10:13",
         "1",
         "59",
         "1",
         "<python><nlp><nltk><tokenize><text-processing>",
         null,
         null,
         "from __future__ import division\nimport nltk, re, pprint\nfrom urllib.request import urlopen\n\nurl = \"https://www.gutenberg.org/cache/epub/75394/pg75394.txt\"\nraw = urlopen(url).read()\nraw = raw.decode('utf-8')\ntokens = nltk.word_tokenize(raw)\nprint(type(tokens))\nprint(len(tokens))\nprint(tokens[:10])",
         "",
         "How do I remove escape characters from output of nltk.word_tokenize?",
         "How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output. Here's my code: And the output, with the escapes visible in the first list item: I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong? I'm using Python 3.12.1 on Windows 11. Thanks in advance - please do let me know if I can provide any further helpful information.",
         "",
         "How do I remove escape characters from output of nltk.word_tokenize? How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output. Here's my code: And the output, with the escapes visible in the first list item: I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong? I'm using Python 3.12.1 on Windows 11. Thanks in advance - please do let me know if I can provide any further helpful information. ",
         "remove escape characters output nltk.word_tokenize ? get rid non-printing ( escaped ) characters output nltk.word_tokenize method ? working book 'natural language processing python ' following code examples , inform output consist words punctuation , however 'm still getting escapes output . 's code : output , escapes visible first list item : 've poked around online suspicion may fact book 's sample code written python 2 , already caused encoding issues ( needed add line convert output bytes string ) . right track ? , wrong ? 'm using python 3.12.1 windows 11. thanks advance - please let know provide helpful information ."
        ],
        [
         "10",
         "79448878",
         "Python Farm-haystack Dependencies",
         "<p>i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these</p>\n<p>from this implementation:</p>\n<pre><code>import os\nfrom typing import List\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes import PreProcessor, BM25Retriever, FARMReader\n\n# Initialize an in-memory document store (replaceable with Elasticsearch)\ndocument_store = InMemoryDocumentStore()\n\n# Folder where uploaded documents are stored\nUPLOAD_FOLDER = &quot;uploaded_docs&quot;\n\n# Ensure the upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\n\ndef list_documents() -&gt; List[str]:\n    &quot;&quot;&quot;List all uploaded documents.&quot;&quot;&quot;\n    try:\n        return os.listdir(UPLOAD_FOLDER)\n    except FileNotFoundError:\n        raise RuntimeError(f&quot;Upload folder '{UPLOAD_FOLDER}' not found. Please create it.&quot;)\n\n\ndef read_document(file_path: str) -&gt; str:\n    &quot;&quot;&quot;Read the content of a document.&quot;&quot;&quot;\n    try:\n        with open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:\n            return f.read()\n    except Exception as e:\n        raise RuntimeError(f&quot;Error reading file '{file_path}': {str(e)}&quot;)\n\n\ndef preprocess_document(content: str) -&gt; List[dict]:\n    &quot;&quot;&quot;Preprocess the document content into smaller chunks for indexing.&quot;&quot;&quot;\n    preprocessor = PreProcessor(\n        split_by=&quot;word&quot;,  # Split the content into chunks by word count\n        split_length=200,  # Chunk size\n        split_overlap=20,  # Overlap between chunks\n        split_respect_sentence_boundary=True,\n    )\n    return preprocessor.process({&quot;content&quot;: content})\n\n\ndef index_document(file_name: str):\n    &quot;&quot;&quot;Read, preprocess, and index a document.&quot;&quot;&quot;\n    file_path = os.path.join(UPLOAD_FOLDER, file_name)\n    if not os.path.isfile(file_path):\n        raise RuntimeError(f&quot;File '{file_name}' not found in '{UPLOAD_FOLDER}'.&quot;)\n\n    content = read_document(file_path)\n    chunks = preprocess_document(content)\n\n    # Prepare chunks in Haystack-compatible format\n    formatted_chunks = [{&quot;content&quot;: chunk[&quot;content&quot;]} for chunk in chunks]\n    document_store.write_documents(formatted_chunks)\n\n    return {\n        &quot;message&quot;: f&quot;Document '{file_name}' indexed successfully.&quot;,\n        &quot;chunks_count&quot;: len(formatted_chunks),\n    }\n\n\ndef search_documents(query: str):\n    &quot;&quot;&quot;Search indexed documents using a query.&quot;&quot;&quot;\n    retriever = BM25Retriever(document_store=document_store)\n    reader = FARMReader(model_name_or_path=&quot;deepset/roberta-base-squad2&quot;, use_gpu=False)\n    \n    # Retrieve documents\n    retrieved_docs = retriever.retrieve(query)\n    if not retrieved_docs:\n        return {&quot;message&quot;: &quot;No relevant documents found.&quot;}\n\n    # Reader to predict answers from retrieved documents\n    answers = reader.predict(query=query, documents=retrieved_docs, top_k=3)\n\n    # Serialize the results to avoid unsupported types\n    results = [\n        {\n            &quot;answer&quot;: ans.answer,\n            &quot;score&quot;: ans.score,\n            &quot;context&quot;: ans.context,\n            &quot;document_id&quot;: ans.document_id,\n        }\n        for ans in answers[&quot;answers&quot;]\n    ]\n\n    return {&quot;results&quot;: results}\n</code></pre>\n<p>But i keep getting this error:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 122, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 131, in _main\n    prepare(preparation_data)\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 244, in prepare\n    _fixup_main_from_name(data['init_main_from_name'])\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 268, in _fixup_main_from_name\n    main_content = runpy.run_module(mod_name,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;&lt;frozen runpy&gt;&quot;, line 226, in run_module\n  File &quot;&lt;frozen runpy&gt;&quot;, line 98, in _run_module_code\n  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/main.py&quot;, line 7, in &lt;module&gt;\n    from app.views.routes import router\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/views/routes.py&quot;, line 2, in &lt;module&gt;\n    from app.services.document_service import list_documents, index_document, search_documents\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/services/document_service.py&quot;, line 3, in &lt;module&gt;\n    from haystack.document_stores import InMemoryDocumentStore\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/__init__.py&quot;, line 8, in &lt;module&gt;\n    from haystack.schema import Document, Answer, Label, MultiLabel, Span, EvaluationResult, TableCell\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/schema.py&quot;, line 42, in &lt;module&gt;\n    @dataclass\n     ^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 250, in dataclass\n    return create_dataclass(_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 241, in create_dataclass\n    pydantic_complete = _pydantic_dataclasses.complete_dataclass(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py&quot;, line 159, in complete_dataclass\n    schema = gen_schema.generate_schema(cls, from_dunder_get_core_schema=False)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 832, in match_type\n    return self._dataclass_schema(obj, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1561, in _dataclass_schema\n    args = sorted(\n           ^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1562, in &lt;genexpr&gt;\n    (self._generate_dc_field_schema(k, v, decorators) for k, v in fields.items()),\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 933, in _generate_dc_field_schema\n    common_field = self._common_field_schema(name, field_info, decorators)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1081, in _common_field_schema\n    schema = self._apply_annotations(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1825, in _apply_annotations\n    schema = get_inner_schema(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py&quot;, line 82, in __call__\n    schema = self._handler(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1806, in inner_handler\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 840, in match_type\n    return self._match_generic_type(obj, origin)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 864, in _match_generic_type\n    return self._union_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1152, in _union_schema\n    choices.append(self.generate_schema(arg))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 844, in match_type\n    return self._unknown_type_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 405, in _unknown_type_schema\n    raise PydanticSchemaGenerationError(\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for &lt;class 'pandas.core.frame.DataFrame'&gt;. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(&lt;some type&gt;) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(&lt;some type&gt;)` since we do not call `__get_pydantic_core_schema__` on `&lt;some type&gt;` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type\n</code></pre>\n",
         "2025-02-18 16:05:55",
         "-1",
         "29",
         "1",
         "<python-3.x><nlp><artificial-intelligence><fastapi><haystack>",
         null,
         null,
         "import os\nfrom typing import List\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes import PreProcessor, BM25Retriever, FARMReader\n\n# Initialize an in-memory document store (replaceable with Elasticsearch)\ndocument_store = InMemoryDocumentStore()\n\n# Folder where uploaded documents are stored\nUPLOAD_FOLDER = \"uploaded_docs\"\n\n# Ensure the upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\n\ndef list_documents() -> List[str]:\n    \"\"\"List all uploaded documents.\"\"\"\n    try:\n        return os.listdir(UPLOAD_FOLDER)\n    except FileNotFoundError:\n        raise RuntimeError(f\"Upload folder '{UPLOAD_FOLDER}' not found. Please create it.\")\n\n\ndef read_document(file_path: str) -> str:\n    \"\"\"Read the content of a document.\"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n    except Exception as e:\n        raise RuntimeError(f\"Error reading file '{file_path}': {str(e)}\")\n\n\ndef preprocess_document(content: str) -> List[dict]:\n    \"\"\"Preprocess the document content into smaller chunks for indexing.\"\"\"\n    preprocessor = PreProcessor(\n        split_by=\"word\",  # Split the content into chunks by word count\n        split_length=200,  # Chunk size\n        split_overlap=20,  # Overlap between chunks\n        split_respect_sentence_boundary=True,\n    )\n    return preprocessor.process({\"content\": content})\n\n\ndef index_document(file_name: str):\n    \"\"\"Read, preprocess, and index a document.\"\"\"\n    file_path = os.path.join(UPLOAD_FOLDER, file_name)\n    if not os.path.isfile(file_path):\n        raise RuntimeError(f\"File '{file_name}' not found in '{UPLOAD_FOLDER}'.\")\n\n    content = read_document(file_path)\n    chunks = preprocess_document(content)\n\n    # Prepare chunks in Haystack-compatible format\n    formatted_chunks = [{\"content\": chunk[\"content\"]} for chunk in chunks]\n    document_store.write_documents(formatted_chunks)\n\n    return {\n        \"message\": f\"Document '{file_name}' indexed successfully.\",\n        \"chunks_count\": len(formatted_chunks),\n    }\n\n\ndef search_documents(query: str):\n    \"\"\"Search indexed documents using a query.\"\"\"\n    retriever = BM25Retriever(document_store=document_store)\n    reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)\n    \n    # Retrieve documents\n    retrieved_docs = retriever.retrieve(query)\n    if not retrieved_docs:\n        return {\"message\": \"No relevant documents found.\"}\n\n    # Reader to predict answers from retrieved documents\n    answers = reader.predict(query=query, documents=retrieved_docs, top_k=3)\n\n    # Serialize the results to avoid unsupported types\n    results = [\n        {\n            \"answer\": ans.answer,\n            \"score\": ans.score,\n            \"context\": ans.context,\n            \"document_id\": ans.document_id,\n        }\n        for ans in answers[\"answers\"]\n    ]\n\n    return {\"results\": results}\n---\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 131, in _main\n    prepare(preparation_data)\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 244, in prepare\n    _fixup_main_from_name(data['init_main_from_name'])\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 268, in _fixup_main_from_name\n    main_content = runpy.run_module(mod_name,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen runpy>\", line 226, in run_module\n  File \"<frozen runpy>\", line 98, in _run_module_code\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/main.py\", line 7, in <module>\n    from app.views.routes import router\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/views/routes.py\", line 2, in <module>\n    from app.services.document_service import list_documents, index_document, search_documents\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/services/document_service.py\", line 3, in <module>\n    from haystack.document_stores import InMemoryDocumentStore\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/__init__.py\", line 8, in <module>\n    from haystack.schema import Document, Answer, Label, MultiLabel, Span, EvaluationResult, TableCell\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/schema.py\", line 42, in <module>\n    @dataclass\n     ^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py\", line 250, in dataclass\n    return create_dataclass(_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py\", line 241, in create_dataclass\n    pydantic_complete = _pydantic_dataclasses.complete_dataclass(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 159, in complete_dataclass\n    schema = gen_schema.generate_schema(cls, from_dunder_get_core_schema=False)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 832, in match_type\n    return self._dataclass_schema(obj, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1561, in _dataclass_schema\n    args = sorted(\n           ^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1562, in <genexpr>\n    (self._generate_dc_field_schema(k, v, decorators) for k, v in fields.items()),\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 933, in _generate_dc_field_schema\n    common_field = self._common_field_schema(name, field_info, decorators)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1081, in _common_field_schema\n    schema = self._apply_annotations(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1825, in _apply_annotations\n    schema = get_inner_schema(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py\", line 82, in __call__\n    schema = self._handler(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1806, in inner_handler\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 840, in match_type\n    return self._match_generic_type(obj, origin)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 864, in _match_generic_type\n    return self._union_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1152, in _union_schema\n    choices.append(self.generate_schema(arg))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 844, in match_type\n    return self._unknown_type_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 405, in _unknown_type_schema\n    raise PydanticSchemaGenerationError(\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'pandas.core.frame.DataFrame'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type",
         "",
         "Python Farm-haystack Dependencies",
         "i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these from this implementation: But i keep getting this error:",
         "",
         "Python Farm-haystack Dependencies i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these from this implementation: But i keep getting this error: ",
         "python farm-haystack dependencies trying implement model using farm-haystack , however dependency mismatch following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart , currently 2 versions python installed machine ( 3.12 3.11.10 ) , facing challenges . need help proper version dependencies python version works better implementation : keep getting error :"
        ],
        [
         "11",
         "79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07 02:48:25",
         "-1",
         "43",
         "1",
         "<keras><deep-learning><nlp><lstm><sentiment-analysis>",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n",
         "lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()",
         "",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes. However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue. This is several information about the dataset: Embedding size: (41151, 100) Maximum sequence length: 731 Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106} Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106} Total x training set (padded): (140997, 200) Total x validation set (padded): (17625, 200) Total x testing set (padded): (17625, 200) Total y training set (one hot): (140997, 3) Total y validation set (one hot): (17625, 3) Total y testing set (one hot): (17625, 2003 This is my full code: enter link description here This is my highlight code for this issue:",
         "Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort. I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then. Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So \"queen\" might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece. This is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context. (It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling with 20+ layers, whereas LSTMs tend to choke after two layers.)",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes. However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue. This is several information about the dataset: Embedding size: (41151, 100) Maximum sequence length: 731 Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106} Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106} Total x training set (padded): (140997, 200) Total x validation set (padded): (17625, 200) Total x testing set (padded): (17625, 200) Total y training set (one hot): (140997, 3) Total y validation set (one hot): (17625, 3) Total y testing set (one hot): (17625, 2003 This is my full code: enter link description here This is my highlight code for this issue: Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort. I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then. Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So \"queen\" might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece. This is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context. (It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling with 20+ layers, whereas LSTMs tend to choke after two layers.)",
         "underfitting pre-trained glove + lstm model : accurcacy unchanged sentiment classification using pre-trained glove lstm model . use google play review scrap , resulting 50k++ texts . implement random sampling minority classes . however , train lstm model , training accuracy remain unchanged several epoch , need insight fix issue . several information dataset : embedding size : ( 41151 , 100 ) maximum sequence length : 731 label distribution random sampling : { 'positive ' : 58749 , 'negative ' : 26643 , 'neutral ' : 9106 } label distribution random sampling : ( 'positive ' : 58749 , 'negative ' : 26643 , 'neutral ' : 9106 } total x training set ( padded ) : ( 140997 , 200 ) total x validation set ( padded ) : ( 17625 , 200 ) total x testing set ( padded ) : ( 17625 , 200 ) total training set ( one hot ) : ( 140997 , 3 ) total validation set ( one hot ) : ( 17625 , 3 ) total testing set ( one hot ) : ( 17625 , 2003 full code : enter link description highlight code issue : based extra information comments , 'm going say reason lstm model hits wall ( unspecified ) lower accuracy 85 % trying reach best type model problem . case tweaking parameters likely wasted effort . 'm fairly sure encoder transformers ( e.g . bert ) surpassed sentiment analysis benchmarks number years back ( sorry , quick search could n't find killer reference insert ) , transformers got bigger better since . extra thought : building top glove embeddings presents problem n't handle multiple meanings word . `` queen '' might female king ( embedding 's party trick : king - male + female = queen ) might pop group , might gay man , might chess piece . going put limit accuracy models built , whereas transformers n't limitation look whole string see words context . ( possible argue , course , bringing context lstm comes . transformers still scaling 20+ layers , whereas lstms tend choke two layers . )"
        ],
        [
         "12",
         "79406743",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text",
         "<p>I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns &quot;UNK&quot;. Here is my code:</p>\n<pre><code>from quickumls import QuickUMLS\n\nquickumls_fp = &quot;med7_en/lib/python3.10/site-packages/quickumls&quot;\nmatcher = QuickUMLS(quickumls_fp)\n\ndef extract_umls_cuis(text):\n    &quot;&quot;&quot;Extract UMLS CUIs using QuickUMLS.&quot;&quot;&quot;\n    if isinstance(text, str):\n        matches = matcher.match(text)\n        if matches:\n            return [match['cui'] for match in matches[0]]\n        else:\n            return &quot;UNK&quot;\n\nsample_text = &quot;diclofenac.&quot;\nprint(extract_umls_cuis(sample_text))\n</code></pre>\n<p>What I Have Checked:</p>\n<ul>\n<li>QuickUMLS Installation: I have installed QuickUMLS correctly.</li>\n<li>UMLS Data Availability: I have set the correct path to QuickUMLS.</li>\n<li>Different Input Words: I tried various medical terms, but all return &quot;UNK&quot;.</li>\n</ul>\n",
         "2025-02-02 14:12:55",
         "0",
         "26",
         "1",
         "<python><machine-learning><deep-learning><nlp>",
         null,
         null,
         "from quickumls import QuickUMLS\n\nquickumls_fp = \"med7_en/lib/python3.10/site-packages/quickumls\"\nmatcher = QuickUMLS(quickumls_fp)\n\ndef extract_umls_cuis(text):\n    \"\"\"Extract UMLS CUIs using QuickUMLS.\"\"\"\n    if isinstance(text, str):\n        matches = matcher.match(text)\n        if matches:\n            return [match['cui'] for match in matches[0]]\n        else:\n            return \"UNK\"\n\nsample_text = \"diclofenac.\"\nprint(extract_umls_cuis(sample_text))",
         "",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text",
         "I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns \"UNK\". Here is my code: What I Have Checked: QuickUMLS Installation: I have installed QuickUMLS correctly. UMLS Data Availability: I have set the correct path to QuickUMLS. Different Input Words: I tried various medical terms, but all return \"UNK\".",
         "",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns \"UNK\". Here is my code: What I Have Checked: QuickUMLS Installation: I have installed QuickUMLS correctly. UMLS Data Availability: I have set the correct path to QuickUMLS. Different Input Words: I tried various medical terms, but all return \"UNK\". ",
         "quickumls always returns `` unk '' input text using quickumls extract umls concept unique identifiers ( cuis ) text , matter word input , always returns `` unk '' . code : checked : quickumls installation : installed quickumls correctly . umls data availability : set correct path quickumls . different input words : tried various medical terms , return `` unk '' ."
        ],
        [
         "13",
         "79402035",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query",
         "<p>How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation.\nI have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n",
         "2025-01-31 07:26:35",
         "1",
         "50",
         "1",
         "<nlp><large-language-model><rag><vectara><enterprise-rag>",
         null,
         null,
         "mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n---\nmockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)",
         "",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation. I have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working. The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset: I have used these models, but still, the citation is not showing in the response. The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset: I have used these models, but still, the citation is not showing in the response.",
         "",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation. I have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working. The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset: I have used these models, but still, the citation is not showing in the response. The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset: I have used these models, but still, the citation is not showing in the response. ",
         "add citations response vectara ? testing multiple corpora query add citations response vectara ? testing multiple corpora query , updated citation payload . followed approach mentioned vectara documentation . tried models mentioned documentation followed instructions provide citation style , still working . documentation states use citations , one must specify one following summarizers generation_preset : used models , still , citation showing response . documentation states use citations , one must specify one following summarizers generation_preset : used models , still , citation showing response ."
        ],
        [
         "14",
         "79399448",
         "LLM Model Lacking Confidence and Changing Answers Based on User Input",
         "<p>I've trained a Large Language Model (LLM) using the RAG method to answer user queries. However, I'm facing an issue where the model lacks confidence in its answers and changes them based on user input, even when the initial response is correct.</p>\n<p>For example, when asked &quot;What is the capital of France?&quot;, the model correctly responds with &quot;Paris.&quot; However, if the user replies &quot;No, it's Berlin,&quot; the model accepts this incorrect response and later provides &quot;Berlin&quot; as the capital of France when asked again.</p>\n<p>I've tried using different prompt templates to reinforce answer consistency, but the issue persists. How can I improve the model’s robustness and prevent it from altering correct answers based on user responses? Any suggestions or guidance would be greatly appreciated.</p>\n",
         "2025-01-30 09:49:37",
         "3",
         "60",
         "1",
         "<nlp><langchain><large-language-model><aiml>",
         null,
         null,
         "",
         "",
         "LLM Model Lacking Confidence and Changing Answers Based on User Input",
         "I've trained a Large Language Model (LLM) using the RAG method to answer user queries. However, I'm facing an issue where the model lacks confidence in its answers and changes them based on user input, even when the initial response is correct. For example, when asked \"What is the capital of France?\", the model correctly responds with \"Paris.\" However, if the user replies \"No, it's Berlin,\" the model accepts this incorrect response and later provides \"Berlin\" as the capital of France when asked again. I've tried using different prompt templates to reinforce answer consistency, but the issue persists. How can I improve the models robustness and prevent it from altering correct answers based on user responses? Any suggestions or guidance would be greatly appreciated.",
         "",
         "LLM Model Lacking Confidence and Changing Answers Based on User Input I've trained a Large Language Model (LLM) using the RAG method to answer user queries. However, I'm facing an issue where the model lacks confidence in its answers and changes them based on user input, even when the initial response is correct. For example, when asked \"What is the capital of France?\", the model correctly responds with \"Paris.\" However, if the user replies \"No, it's Berlin,\" the model accepts this incorrect response and later provides \"Berlin\" as the capital of France when asked again. I've tried using different prompt templates to reinforce answer consistency, but the issue persists. How can I improve the models robustness and prevent it from altering correct answers based on user responses? Any suggestions or guidance would be greatly appreciated. ",
         "llm model lacking confidence changing answers based user input 've trained large language model ( llm ) using rag method answer user queries . however , 'm facing issue model lacks confidence answers changes based user input , even initial response correct . example , asked `` capital france ? `` , model correctly responds `` paris . '' however , user replies `` , 's berlin , '' model accepts incorrect response later provides `` berlin '' capital france asked . 've tried using different prompt templates reinforce answer consistency , issue persists . improve models robustness prevent altering correct answers based user responses ? suggestions guidance would greatly appreciated ."
        ],
        [
         "15",
         "79393930",
         "Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data?",
         "<p>I’m running into a frustrating issue while training a BERT-based multi-label text classification model on an imbalanced dataset. After a few epochs, the training loss suddenly becomes NaN, and I can’t seem to figure out why. I’ve tried a bunch of different things, but nothing has worked so far. Hoping someone here has dealt with this before.\n<strong>Dataset Setup</strong>\nNumber of samples is around 100K\nNumber of labels is around 50\nImbalance: Some labels are super common (appear in 80% of samples), while others are barely there (less than 0.5%)</p>\n<p><strong>My Setup</strong>\nI’m using Hugging Face’s bert-base-uncased with a custom classification head.</p>\n<pre><code>from transformers import BertModel\nimport torch.nn as nn\n\nclass MultiLabelClassifier(nn.Module):\n    def __init__(self, num_labels):\n        super(MultiLabelClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.classifier(outputs.pooler_output)  # Using the [CLS] token\n        return logits\n\n</code></pre>\n<p>I’m using ***BCEWithLogitsLoss ***with a weighted loss function to deal with the imbalance:</p>\n<pre><code>class_weights = torch.tensor([1.0 / (freq + 1e-5) for freq in label_frequencies]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n\n</code></pre>\n<p>after 2 or 3 epochs;</p>\n<ul>\n<li>The loss starts off fine but becomes NaN</li>\n<li>Some logits are ridiculously large or small (1e20, -1e20) before the NaN happens.</li>\n<li>Gradients also seem to explode right before the NaN loss kicks in.</li>\n</ul>\n<p>to solve this issue,</p>\n<ol>\n<li>Added gradient clipping, which helps a bit but doesn’t fully fix the issue\n<code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code></li>\n<li>Tried reducing the learning rate to 5e-6, which delays the NaN issue but doesn’t stop it completely</li>\n<li>Thought the issue might be with the classifier weights, so I reinitialized them like this,</li>\n</ol>\n<pre><code>for layer in model.classifier.parameters():\n    if isinstance(layer, nn.Linear):\n        nn.init.xavier_normal_(layer.weight)\n\n</code></pre>\n<ol start=\"4\">\n<li>Rewrote the loss calculation using torch.logsigmoid for numerical stability...</li>\n</ol>\n<pre><code>loss = -labels * torch.logsigmoid(logits) - (1 - labels) * torch.logsigmoid(-logits)\nloss = loss.mean()\n</code></pre>\n<p>Nothing seems to solve the problem completely.</p>\n<p><strong>------my questions---</strong></p>\n<ol>\n<li>Why is this happening? Is it because of the extreme imbalance in my dataset or something else?</li>\n<li>How can I fix it? Should I try something like label smoothing, or is there a better way to stabilize the training?</li>\n</ol>\n<p>here is a snippet of my training loop for context.</p>\n<pre><code>optimizer = AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in dataloader:\n        input_ids, attention_mask, labels = batch[&quot;input_ids&quot;], batch[&quot;attention_mask&quot;], batch[&quot;labels&quot;]\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n</code></pre>\n<p>---<strong>I Need</strong>--\nI’m looking for practical suggestions to <strong>prevent the NaN loss issue entirely</strong> and <strong>stabilize training when working with imbalanced multi-label datasets</strong>.</p>\n<p>If anyone has faced this issue or knows a good fix, I’d really appreciate the help. Thanks!</p>\n",
         "2025-01-28 13:03:15",
         "0",
         "64",
         "1",
         "<deep-learning><nlp><bert-language-model><multilabel-classification><imbalanced-data>",
         null,
         null,
         "from transformers import BertModel\nimport torch.nn as nn\n\nclass MultiLabelClassifier(nn.Module):\n    def __init__(self, num_labels):\n        super(MultiLabelClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.classifier(outputs.pooler_output)  # Using the [CLS] token\n        return logits\n---\nclass_weights = torch.tensor([1.0 / (freq + 1e-5) for freq in label_frequencies]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n---\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n---\nfor layer in model.classifier.parameters():\n    if isinstance(layer, nn.Linear):\n        nn.init.xavier_normal_(layer.weight)\n---\nloss = -labels * torch.logsigmoid(logits) - (1 - labels) * torch.logsigmoid(-logits)\nloss = loss.mean()\n---\noptimizer = AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in dataloader:\n        input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()",
         "",
         "Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data?",
         "Im running into a frustrating issue while training a BERT-based multi-label text classification model on an imbalanced dataset. After a few epochs, the training loss suddenly becomes NaN, and I cant seem to figure out why. Ive tried a bunch of different things, but nothing has worked so far. Hoping someone here has dealt with this before. Dataset Setup Number of samples is around 100K Number of labels is around 50 Imbalance: Some labels are common (appear in 80% of samples), while others are barely there (less than 0.5%) My Setup Im using Hugging Faces bert-base-uncased with a custom classification head. Im using ***BCEWithLogitsLoss ***with a weighted loss function to deal with the imbalance: after 2 or 3 epochs; The loss starts off fine but becomes NaN Some logits are ridiculously large or small (1e20, -1e20) before the NaN happens. Gradients also seem to explode right before the NaN loss kicks in. to solve this issue, Added gradient clipping, which helps a bit but doesnt fully fix the issue Tried reducing the learning rate to 5e-6, which delays the NaN issue but doesnt stop it completely Thought the issue might be with the classifier weights, so I reinitialized them like this, Rewrote the loss calculation using torch.logsigmoid for numerical stability... Nothing seems to solve the problem completely. ------my questions--- Why is this happening? Is it because of the extreme imbalance in my dataset or something else? How can I fix it? Should I try something like label smoothing, or is there a better way to stabilize the training? here is a snippet of my training loop for context. --- I Need -- Im looking for practical suggestions to prevent the NaN loss issue entirely and stabilize training when working with imbalanced multi-label datasets . If anyone has faced this issue or knows a good fix, Id appreciate the help. Thanks!",
         "",
         "Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data? Im running into a frustrating issue while training a BERT-based multi-label text classification model on an imbalanced dataset. After a few epochs, the training loss suddenly becomes NaN, and I cant seem to figure out why. Ive tried a bunch of different things, but nothing has worked so far. Hoping someone here has dealt with this before. Dataset Setup Number of samples is around 100K Number of labels is around 50 Imbalance: Some labels are common (appear in 80% of samples), while others are barely there (less than 0.5%) My Setup Im using Hugging Faces bert-base-uncased with a custom classification head. Im using ***BCEWithLogitsLoss ***with a weighted loss function to deal with the imbalance: after 2 or 3 epochs; The loss starts off fine but becomes NaN Some logits are ridiculously large or small (1e20, -1e20) before the NaN happens. Gradients also seem to explode right before the NaN loss kicks in. to solve this issue, Added gradient clipping, which helps a bit but doesnt fully fix the issue Tried reducing the learning rate to 5e-6, which delays the NaN issue but doesnt stop it completely Thought the issue might be with the classifier weights, so I reinitialized them like this, Rewrote the loss calculation using torch.logsigmoid for numerical stability... Nothing seems to solve the problem completely. ------my questions--- Why is this happening? Is it because of the extreme imbalance in my dataset or something else? How can I fix it? Should I try something like label smoothing, or is there a better way to stabilize the training? here is a snippet of my training loop for context. --- I Need -- Im looking for practical suggestions to prevent the NaN loss issue entirely and stabilize training when working with imbalanced multi-label datasets . If anyone has faced this issue or knows a good fix, Id appreciate the help. Thanks! ",
         "bert model producing nan loss training multi-label classification imbalanced data ? im running frustrating issue training bert-based multi-label text classification model imbalanced dataset . epochs , training loss suddenly becomes nan , cant seem figure . ive tried bunch different things , nothing worked far . hoping someone dealt . dataset setup number samples around 100k number labels around 50 imbalance : labels common ( appear 80 % samples ) , others barely ( less 0.5 % ) setup im using hugging faces bert-base-uncased custom classification head . im using * * * bcewithlogitsloss * * * weighted loss function deal imbalance : 2 3 epochs ; loss starts fine becomes nan logits ridiculously large small ( 1e20 , -1e20 ) nan happens . gradients also seem explode right nan loss kicks . solve issue , added gradient clipping , helps bit doesnt fully fix issue tried reducing learning rate 5e-6 , delays nan issue doesnt stop completely thought issue might classifier weights , reinitialized like , rewrote loss calculation using torch.logsigmoid numerical stability ... nothing seems solve problem completely . -- -- -- questions -- - happening ? extreme imbalance dataset something else ? fix ? try something like label smoothing , better way stabilize training ? snippet training loop context . -- - need -- im looking practical suggestions prevent nan loss issue entirely stabilize training working imbalanced multi-label datasets . anyone faced issue knows good fix , id appreciate help . thanks !"
        ],
        [
         "16",
         "79385917",
         "torch.OutOfMemoryError: CUDA out of memory. (Google Colab)",
         "<p>I tried to adapt the mBERT model to an existing code. However, I received the following issue even though I tried different solutions.</p>\n<pre><code>torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 9.06 MiB is free. Process 84806 has 14.74 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 129.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n</code></pre>\n<p>Here's the news model that I'm trying to adapt to DST-MetaASSIST(STAR), which you can find it here:</p>\n<pre><code>### For DST-MetaASSIST\n!git clone https://github.com/smartyfh/DST-MetaASSIST\n</code></pre>\n<p>These are the new models:</p>\n<pre><code># First model mBERT \nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n# Load the model with the correct number of labels\nmodel = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n\n# Second model XLM-R\n\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n\n# Load the tokenizer and model\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nmodel = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=number_of_labels)\n\n\n# Third model mT5\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the mT5 tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained('google/mt5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('google/mt5-small')\n</code></pre>\n<p><strong>I changed the 'train-S1.py' file and then run it</strong></p>\n<pre><code>import torch\nimport os\nimport torch\n\n# Set environment variable for CUDA memory management\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ntorch.cuda.empty_cache()  # Clear CUDA cache\ntorch.backends.cudnn.benchmark = True\n\n# Check for GPU availability\nif torch.cuda.is_available():\n    device = torch.device(&quot;cuda&quot;)\n    print(&quot;Using GPU:&quot;, torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(&quot;cpu&quot;)\n    print(&quot;Using CPU&quot;)\n\n# Reduce batch sizes for memory optimization\ntrain_batch_size = 2  # Reduced from 4 or 16\nmeta_batch_size = 1    # Reduced from 2 or 8\n\n# Run your training script with reduced batch sizes\n!python3 /content/DST-MetaASSIST/STAR/train-S1.py --data_dir data/mwz2.4 --save_dir output-meta24-S1/exp --train_batch_size 2 --meta_batch_size 1 --enc_lr 4e-5 --dec_lr 1e-4 --sw_lr 5e-5 --init_weight 0.5 --n_epochs 1 --do_train\n</code></pre>\n<p>The new script:</p>\n<pre><code># import faulthandler\n# faulthandler.enable()\n# learn slot-wise weight\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport argparse\nimport random\nimport json\nimport time\nimport logging\nfrom tqdm import tqdm, trange\n\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom utils.data_utils import Processor, MultiWozDataset\nfrom utils.eval_utils import model_evaluation\nfrom utils.loss_utils import *\nfrom utils.label_lookup import get_label_lookup_from_first_token\nfrom models.DST import UtteranceEncoding, BeliefTracker\n# Import necessary libraries\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n\n\n#====================================\nimport higher\nimport itertools\nfrom models.WeightNet import SlotWeight\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR\n#====================================\n\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup as get_linear_schedule_with_warmup_T\n\nos.environ['CUDA_VISIBLE_DEVICES']='0'\n# torch.cuda.set_device(0)\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                    datefmt='%m/%d/%Y %H:%M:%S',\n                    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, dec_num_warmup_steps, num_training_steps, last_epoch=-1):\n    &quot;&quot;&quot;\n    see https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/optimization.py#L75\n    &quot;&quot;&quot;\n    def enc_lr_lambda(current_step: int):\n        if current_step &lt; enc_num_warmup_steps:\n            return float(current_step) / float(max(1, enc_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - enc_num_warmup_steps))\n        )\n    \n    def dec_lr_lambda(current_step: int):\n        if current_step &lt; dec_num_warmup_steps:\n            return float(current_step) / float(max(1, dec_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - dec_num_warmup_steps))\n        )\n\n    return LambdaLR(optimizer, [enc_lr_lambda, enc_lr_lambda, dec_lr_lambda], last_epoch)\n\ndef set_seed(args, device):\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n    torch.manual_seed(args.random_seed)\n    if device == &quot;cuda&quot;:\n        torch.cuda.manual_seed(args.random_seed)\n        torch.cuda.manual_seed_all(args.random_seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        \ndef get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device):\n    slot_lookup = get_label_lookup_from_first_token(slot_meta, tokenizer, sv_encoder, device)\n    value_lookup = []\n    for slot in ontology.keys():\n        value_lookup.append(get_label_lookup_from_first_token(ontology[slot], tokenizer, sv_encoder, device))\n    return slot_lookup, value_lookup\n\ndef prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    enc_param_optimizer = list(model.encoder.named_parameters())\n    dec_param_optimizer = list(model.decoder.parameters())\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in enc_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in enc_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n        {'params': dec_param_optimizer, 'lr': dec_learning_rate}\n        ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    scheduler = get_linear_schedule_with_warmup(optimizer, int(num_train_steps * enc_warmup_ratio),\n                                                int(num_train_steps * dec_warmup_ratio), num_train_steps)\n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n\n'''def prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    \n    # Access all parameters of the model\n    param_optimizer = list(model.named_parameters())\n    \n    # Group parameters for the optimizer\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    \n    # Calculate warmup steps\n    enc_num_warmup_steps = int(num_train_steps * enc_warmup_ratio)\n    \n    # Create the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, num_train_steps)\n    \n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n'''\n\ndef get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids):\n    _, pred_all_distance = slot_value_matching(slot_output, value_lookup)\n                \n    loss_slot_gt = unreduced_cross_entropy_loss(pred_all_distance, label_ids)\n    loss_slot_pseudo = unreduced_cross_entropy_loss(pred_all_distance, pseudo_label_ids)\n    \n    return loss_slot_gt, loss_slot_pseudo\n\ndef main(args):\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n        \n    # logger\n    logger_file_name = args.save_dir.split('/')[1]\n    fileHandler = logging.FileHandler(os.path.join(args.save_dir, &quot;%s.txt&quot;%(logger_file_name)))\n    logger.addHandler(fileHandler)\n    logger.info(args)\n    \n    # cuda setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logger.info(&quot;device: {}&quot;.format(device))\n    \n    # set random seed\n    set_seed(args, device)\n\n    #******************************************************\n    # load data\n    #******************************************************\n    processor = Processor(args)\n    slot_meta = processor.slot_meta\n    ontology = processor.ontology\n    logger.info(slot_meta)\n   \n    # Load the mBERT tokenizer\n    #tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n    tokenizer = BertTokenizer.from_pretrained(args.pretrained_model)\n\n    \n    \n    if args.do_train:\n        train_data_raw = processor.get_instances(args.data_dir, args.train_data, tokenizer, True)\n        print(&quot;# train examples %d&quot; % len(train_data_raw))\n\n        # Get unique labels from the training data\n        unique_labels = set(label for instance in train_data_raw for label in instance.label_ids)  # Flatten the list\n        num_labels = len(unique_labels)  # Count unique labels\n        print(f&quot;Number of unique labels: {num_labels}&quot;)\n        \n        meta_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(&quot;# meta examples %d&quot; % len(meta_data_raw))\n        \n        dev_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(&quot;# dev examples %d&quot; % len(dev_data_raw))\n    \n    test_data_raw = processor.get_instances(args.data_dir, args.test_data, tokenizer)\n    print(&quot;# test examples %d&quot; % len(test_data_raw))\n    logger.info(&quot;Data loaded!&quot;)\n    \n    ## Initialize slot and value embeddings\n    sv_encoder = UtteranceEncoding.from_pretrained(args.pretrained_model)\n    for p in sv_encoder.bert.parameters():\n        p.requires_grad = False  \n    slot_lookup, value_lookup = get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device)\n\n    #num_labels = len(value_lookup)\n    # Load the mBERT model with the correct number of labels\n    #model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n    # Clear unused variables and cache\n    torch.cuda.empty_cache()\n    if args.do_train:\n        train_data = MultiWozDataset(train_data_raw,\n                                     tokenizer,\n                                     word_dropout=args.word_dropout,\n                                     max_seq_length=args.max_seq_length,\n                                     use_pseudo_label=True)\n        meta_data = MultiWozDataset(meta_data_raw,\n                                    tokenizer,\n                                    word_dropout=0.0, # do word dropout here???\n                                    max_seq_length=args.max_seq_length)\n\n        num_train_steps = int(len(train_data_raw) / args.train_batch_size * args.n_epochs)\n        logger.info(&quot;***** Run training *****&quot;)\n        logger.info(&quot; Num examples = %d&quot;, len(train_data_raw))\n        logger.info(&quot; Batch size = %d&quot;, args.train_batch_size)\n        logger.info(&quot; Num steps = %d&quot;, num_train_steps)\n\n        train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data,\n                                      sampler=train_sampler,\n                                      batch_size=args.train_batch_size,\n                                      collate_fn=train_data.collate_fn)\n        \n        meta_sampler = RandomSampler(meta_data)\n        meta_dataloader = DataLoader(meta_data,\n                                     sampler=meta_sampler,\n                                     batch_size=args.meta_batch_size,\n                                     collate_fn=meta_data.collate_fn)\n        meta_dataloader = itertools.cycle(meta_dataloader)\n        \n        #******************************************************\n        # build model\n        #******************************************************\n        ## model initialization\n        base_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        # Load the model without unsupported arguments\n        # Load the mBERT model with the correct number of labels\n        #base_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        base_model.to(device)\n\n\n\n\n        '''\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        meta_model.to(device)\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                    num_self_attention_layer=args.num_self_attention_layer)\n        #\n        '''\n        meta_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        meta_model.to(device)\n        \n        # Number of slots\n        SW = SlotWeight(len(slot_meta), init_val=np.log(args.init_weight/(1.0 - args.init_weight)))\n        SW.to(device)\n\n        ## prepare optimizer\n        # Prepare optimizer\n        # Prepare optimizer\n        #base_optimizer, base_scheduler = prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n    \n\n        base_optimizer, base_scheduler = \\\n        prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n        \n        logger.info(base_optimizer)\n        # meta model is a copy of the base model, thus shares the optimizer and scheduler\n        meta_optimizer, meta_scheduler = \\\n        prepare_optimizer(meta_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n\n        sw_param_optimizer = list(SW.parameters())\n        sw_optimizer = optim.AdamW(sw_param_optimizer, lr=args.sw_lr)\n        sw_scheduler = get_linear_schedule_with_warmup_T(sw_optimizer,\n                                                         int(num_train_steps * args.sw_warmup),\n                                                         num_train_steps)\n        \n        #******************************************************\n        # training\n        #******************************************************\n        logger.info(&quot;Training...&quot;)\n\n        best_loss = None\n        best_acc = None\n        last_update = None\n\n        for epoch in trange(int(args.n_epochs), desc=&quot;Epoch&quot;):       \n            batch_loss, meta_batch_loss = [], []\n            for step, batch in enumerate(tqdm(train_dataloader)):\n                base_model.train()\n\n                batch = [b.to(device) for b in batch]\n                input_ids, segment_ids, input_mask, label_ids, pseudo_label_ids = batch\n                \n                # forward (meta model)\n                meta_model.load_state_dict(base_model.state_dict())\n                meta_optimizer.load_state_dict(base_optimizer.state_dict())\n                meta_optimizer.zero_grad()\n                with higher.innerloop_ctx(meta_model, meta_optimizer) as (meta_m, meta_opt):\n                    meta_m.train()\n                    slot_output = meta_m(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    \n                    loss_slot_gt, loss_slot_pseudo = \\\n                    get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                    \n                    s_weight = SW()\n                \n                    meta_loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                    # first backward\n                    meta_opt.step(meta_loss)\n                    \n                    # compute on the meta validation set\n                    batch_v = next(meta_dataloader)\n                    batch_v = [b.to(device) for b in batch_v]\n                    input_ids_v, segment_ids_v, input_mask_v, label_ids_v = batch_v\n                    # second forward\n                    meta_m.eval() # disable dropout\n                    slot_output_v = meta_m(input_ids=input_ids_v,\n                                           attention_mask=input_mask_v,\n                                           token_type_ids=segment_ids_v,\n                                           slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    _, pred_all_distance = slot_value_matching(slot_output_v, value_lookup)\n                    loss_v, _, _ = hard_cross_entropy_loss(pred_all_distance, label_ids_v)\n                    # backward over backward\n                    sw_optimizer.zero_grad()\n                    loss_v.backward()\n                    sw_optimizer.step()\n                    sw_scheduler.step()\n                    meta_batch_loss.append(loss_v.item())\n                \n                # Now we have the updated weight net  \n                # forward (base model)\n                slot_output = base_model(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n\n                loss_slot_gt, loss_slot_pseudo = \\\n                get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                with torch.no_grad():    \n                    s_weight = SW()\n\n                loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                # backward (base model)\n                base_optimizer.zero_grad()\n                loss.backward()\n                base_optimizer.step()\n                base_scheduler.step()\n\n                batch_loss.append(loss.item())\n                if step % 300 == 0:\n                    logger.info(&quot;[%d/%d] [%d/%d] mean_loss: %.6f mean_meta_loss: %.6f&quot; % \\\n                               (epoch+1, args.n_epochs, step, len(train_dataloader),\n                                np.mean(batch_loss), np.mean(meta_batch_loss)))\n                    batch_loss, meta_batch_loss = [], []\n                    logger.info(f'Slot weights: {s_weight.cpu().numpy()}')\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, dev_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n                if last_update is None or best_loss &gt; eval_res['loss']:\n                    best_loss = eval_res['loss']\n#                     save_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#                     torch.save(base_model.state_dict(), save_path)\n                    print(&quot;Best Loss : &quot;, best_loss)\n                    print(&quot;\\n&quot;)\n                if last_update is None or best_acc &lt; eval_res['joint_acc']:\n                    best_acc = eval_res['joint_acc']\n                    save_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n                    save_path_w = os.path.join(args.save_dir, 'sw.bin')\n                    torch.save(base_model.state_dict(), save_path)\n                    torch.save(SW.state_dict(), save_path_w)\n                    last_update = epoch\n                    print(&quot;Best Acc : &quot;, best_acc)\n                    print(&quot;\\n&quot;)\n\n                logger.info(&quot;*** Epoch=%d, Last Update=%d, Dev Loss=%.6f, Dev Acc=%.6f, Dev Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***&quot; % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, test_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n\n                logger.info(&quot;*** Epoch=%d, Last Update=%d, Tes Loss=%.6f, Tes Acc=%.6f, Tes Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***&quot; % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if last_update + args.patience &lt;= epoch:\n                    break\n            torch.cuda.empty_cache()\n\n#         print(&quot;Test using best loss model...&quot;)\n#         best_epoch = 0\n#         ckpt_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#         model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n#                               num_self_attention_layer=args.num_self_attention_layer)\n#         ckpt = torch.load(ckpt_path, map_location='cpu')\n#         model.load_state_dict(ckpt)\n#         model.to(device)\n\n#         test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n#                                     ontology, best_epoch, is_gt_p_state=False)\n#         logger.info(&quot;Results based on best loss: &quot;)\n#         logger.info(test_res)\n    #----------------------------------------------------------------------\n    print(&quot;Test using best acc model...&quot;)\n    best_epoch = 1\n    ckpt_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n    model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                          num_self_attention_layer=args.num_self_attention_layer)\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    model.load_state_dict(ckpt)\n    model.to(device)\n\n    test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n                                ontology, best_epoch, is_gt_p_state=False)\n    logger.info(&quot;Results based on best acc: &quot;)\n    logger.info(test_res)\n    \n\nif __name__ == &quot;__main__&quot;:\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(&quot;--data_dir&quot;, default='data/mwz2.4', type=str)\n    parser.add_argument(&quot;--train_data&quot;, default='train_dials_v2.json', type=str)\n    parser.add_argument(&quot;--dev_data&quot;, default='dev_dials_v2.json', type=str)\n    parser.add_argument(&quot;--test_data&quot;, default='test_dials_v2.json', type=str)\n    #parser.add_argument(&quot;--pretrained_model&quot;, default='bert-base-uncased', type=str)\n    parser.add_argument(&quot;--pretrained_model&quot;, default='bert-base-multilingual-cased', type=str)\n\n    parser.add_argument(&quot;--save_dir&quot;, default='output-meta24-S1/exp', type=str)\n\n    parser.add_argument(&quot;--random_seed&quot;, default=42, type=int)\n\n    #parser.add_argument(&quot;--train_batch_size&quot;, default=16, type=int)\n    #parser.add_argument(&quot;--meta_batch_size&quot;, default=8, type=int)\n    parser.add_argument(&quot;--train_batch_size&quot;, default=2, type=int)  # Reduce from 16 to 8 to 2\n    parser.add_argument(&quot;--meta_batch_size&quot;, default=1, type=int)   # Reduce from 8 to 4 to 1\n    \n    parser.add_argument(&quot;--enc_warmup&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--dec_warmup&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--sw_warmup&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--enc_lr&quot;, default=4e-5, type=float)\n    parser.add_argument(&quot;--dec_lr&quot;, default=1e-4, type=float)\n    parser.add_argument(&quot;--sw_lr&quot;, default=5e-5, type=float)\n    parser.add_argument(&quot;--init_weight&quot;, default=0.5, type=float)\n    parser.add_argument(&quot;--n_epochs&quot;, default=15, type=int)\n    parser.add_argument(&quot;--eval_epoch&quot;, default=1, type=int)\n    parser.add_argument(&quot;--eval_step&quot;, default=100000, type=int)\n\n    parser.add_argument(&quot;--dropout_prob&quot;, default=0.1, type=float)\n    parser.add_argument(&quot;--word_dropout&quot;, default=0.1, type=float)\n    \n    parser.add_argument(&quot;--max_seq_length&quot;, default=512, type=int)\n    parser.add_argument(&quot;--patience&quot;, default=6, type=int)\n    parser.add_argument(&quot;--attn_head&quot;, default=4, type=int)\n    parser.add_argument(&quot;--num_history&quot;, default=20, type=int)\n    parser.add_argument(&quot;--num_self_attention_layer&quot;, default=6, type=int)\n    \n    parser.add_argument(&quot;--do_train&quot;, action='store_true')\n       \n    args = parser.parse_args()\n    \n    print('pytorch version: ', torch.__version__)\n    args.torch_version = torch.__version__\n    args.transformers_version = transformers.__version__\n    args.save_dir = args.save_dir + \\\n    f'-sd{args.random_seed}-bz{args.train_batch_size}-{args.meta_batch_size}-lr{args.enc_lr}-{args.dec_lr}-{args.sw_lr}-ep{args.n_epochs}'\n\n    main(args)\n\n</code></pre>\n<p>Any suggestions?! Thanks in advance.</p>\n",
         "2025-01-24 23:41:19",
         "0",
         "81",
         "1",
         "<python><nlp><google-colaboratory><bert-language-model>",
         null,
         null,
         "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 9.06 MiB is free. Process 84806 has 14.74 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 129.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n---\n### For DST-MetaASSIST\n!git clone https://github.com/smartyfh/DST-MetaASSIST\n---\n# First model mBERT \nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n# Load the model with the correct number of labels\nmodel = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n\n# Second model XLM-R\n\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n\n# Load the tokenizer and model\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nmodel = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=number_of_labels)\n\n\n# Third model mT5\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the mT5 tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained('google/mt5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('google/mt5-small')\n---\nimport torch\nimport os\nimport torch\n\n# Set environment variable for CUDA memory management\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ntorch.cuda.empty_cache()  # Clear CUDA cache\ntorch.backends.cudnn.benchmark = True\n\n# Check for GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using GPU:\", torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\n# Reduce batch sizes for memory optimization\ntrain_batch_size = 2  # Reduced from 4 or 16\nmeta_batch_size = 1    # Reduced from 2 or 8\n\n# Run your training script with reduced batch sizes\n!python3 /content/DST-MetaASSIST/STAR/train-S1.py --data_dir data/mwz2.4 --save_dir output-meta24-S1/exp --train_batch_size 2 --meta_batch_size 1 --enc_lr 4e-5 --dec_lr 1e-4 --sw_lr 5e-5 --init_weight 0.5 --n_epochs 1 --do_train\n---\n# import faulthandler\n# faulthandler.enable()\n# learn slot-wise weight\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport argparse\nimport random\nimport json\nimport time\nimport logging\nfrom tqdm import tqdm, trange\n\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom utils.data_utils import Processor, MultiWozDataset\nfrom utils.eval_utils import model_evaluation\nfrom utils.loss_utils import *\nfrom utils.label_lookup import get_label_lookup_from_first_token\nfrom models.DST import UtteranceEncoding, BeliefTracker\n# Import necessary libraries\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the mBERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n\n\n#====================================\nimport higher\nimport itertools\nfrom models.WeightNet import SlotWeight\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR\n#====================================\n\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup as get_linear_schedule_with_warmup_T\n\nos.environ['CUDA_VISIBLE_DEVICES']='0'\n# torch.cuda.set_device(0)\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                    datefmt='%m/%d/%Y %H:%M:%S',\n                    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, dec_num_warmup_steps, num_training_steps, last_epoch=-1):\n    \"\"\"\n    see https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/optimization.py#L75\n    \"\"\"\n    def enc_lr_lambda(current_step: int):\n        if current_step < enc_num_warmup_steps:\n            return float(current_step) / float(max(1, enc_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - enc_num_warmup_steps))\n        )\n    \n    def dec_lr_lambda(current_step: int):\n        if current_step < dec_num_warmup_steps:\n            return float(current_step) / float(max(1, dec_num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - dec_num_warmup_steps))\n        )\n\n    return LambdaLR(optimizer, [enc_lr_lambda, enc_lr_lambda, dec_lr_lambda], last_epoch)\n\ndef set_seed(args, device):\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n    torch.manual_seed(args.random_seed)\n    if device == \"cuda\":\n        torch.cuda.manual_seed(args.random_seed)\n        torch.cuda.manual_seed_all(args.random_seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        \ndef get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device):\n    slot_lookup = get_label_lookup_from_first_token(slot_meta, tokenizer, sv_encoder, device)\n    value_lookup = []\n    for slot in ontology.keys():\n        value_lookup.append(get_label_lookup_from_first_token(ontology[slot], tokenizer, sv_encoder, device))\n    return slot_lookup, value_lookup\n\ndef prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    enc_param_optimizer = list(model.encoder.named_parameters())\n    dec_param_optimizer = list(model.decoder.parameters())\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in enc_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in enc_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n        {'params': dec_param_optimizer, 'lr': dec_learning_rate}\n        ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    scheduler = get_linear_schedule_with_warmup(optimizer, int(num_train_steps * enc_warmup_ratio),\n                                                int(num_train_steps * dec_warmup_ratio), num_train_steps)\n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n\n'''def prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    \n    # Access all parameters of the model\n    param_optimizer = list(model.named_parameters())\n    \n    # Group parameters for the optimizer\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)\n    \n    # Calculate warmup steps\n    enc_num_warmup_steps = int(num_train_steps * enc_warmup_ratio)\n    \n    # Create the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, num_train_steps)\n    \n    print(f'Number of parameter groups: {len(optimizer.param_groups)}')\n    return optimizer, scheduler\n'''\n\ndef get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids):\n    _, pred_all_distance = slot_value_matching(slot_output, value_lookup)\n                \n    loss_slot_gt = unreduced_cross_entropy_loss(pred_all_distance, label_ids)\n    loss_slot_pseudo = unreduced_cross_entropy_loss(pred_all_distance, pseudo_label_ids)\n    \n    return loss_slot_gt, loss_slot_pseudo\n\ndef main(args):\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n        \n    # logger\n    logger_file_name = args.save_dir.split('/')[1]\n    fileHandler = logging.FileHandler(os.path.join(args.save_dir, \"%s.txt\"%(logger_file_name)))\n    logger.addHandler(fileHandler)\n    logger.info(args)\n    \n    # cuda setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logger.info(\"device: {}\".format(device))\n    \n    # set random seed\n    set_seed(args, device)\n\n    #******************************************************\n    # load data\n    #******************************************************\n    processor = Processor(args)\n    slot_meta = processor.slot_meta\n    ontology = processor.ontology\n    logger.info(slot_meta)\n   \n    # Load the mBERT tokenizer\n    #tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n    tokenizer = BertTokenizer.from_pretrained(args.pretrained_model)\n\n    \n    \n    if args.do_train:\n        train_data_raw = processor.get_instances(args.data_dir, args.train_data, tokenizer, True)\n        print(\"# train examples %d\" % len(train_data_raw))\n\n        # Get unique labels from the training data\n        unique_labels = set(label for instance in train_data_raw for label in instance.label_ids)  # Flatten the list\n        num_labels = len(unique_labels)  # Count unique labels\n        print(f\"Number of unique labels: {num_labels}\")\n        \n        meta_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(\"# meta examples %d\" % len(meta_data_raw))\n        \n        dev_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)\n        print(\"# dev examples %d\" % len(dev_data_raw))\n    \n    test_data_raw = processor.get_instances(args.data_dir, args.test_data, tokenizer)\n    print(\"# test examples %d\" % len(test_data_raw))\n    logger.info(\"Data loaded!\")\n    \n    ## Initialize slot and value embeddings\n    sv_encoder = UtteranceEncoding.from_pretrained(args.pretrained_model)\n    for p in sv_encoder.bert.parameters():\n        p.requires_grad = False  \n    slot_lookup, value_lookup = get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device)\n\n    #num_labels = len(value_lookup)\n    # Load the mBERT model with the correct number of labels\n    #model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n    # Clear unused variables and cache\n    torch.cuda.empty_cache()\n    if args.do_train:\n        train_data = MultiWozDataset(train_data_raw,\n                                     tokenizer,\n                                     word_dropout=args.word_dropout,\n                                     max_seq_length=args.max_seq_length,\n                                     use_pseudo_label=True)\n        meta_data = MultiWozDataset(meta_data_raw,\n                                    tokenizer,\n                                    word_dropout=0.0, # do word dropout here???\n                                    max_seq_length=args.max_seq_length)\n\n        num_train_steps = int(len(train_data_raw) / args.train_batch_size * args.n_epochs)\n        logger.info(\"***** Run training *****\")\n        logger.info(\" Num examples = %d\", len(train_data_raw))\n        logger.info(\" Batch size = %d\", args.train_batch_size)\n        logger.info(\" Num steps = %d\", num_train_steps)\n\n        train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data,\n                                      sampler=train_sampler,\n                                      batch_size=args.train_batch_size,\n                                      collate_fn=train_data.collate_fn)\n        \n        meta_sampler = RandomSampler(meta_data)\n        meta_dataloader = DataLoader(meta_data,\n                                     sampler=meta_sampler,\n                                     batch_size=args.meta_batch_size,\n                                     collate_fn=meta_data.collate_fn)\n        meta_dataloader = itertools.cycle(meta_dataloader)\n        \n        #******************************************************\n        # build model\n        #******************************************************\n        ## model initialization\n        base_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        # Load the model without unsupported arguments\n        # Load the mBERT model with the correct number of labels\n        #base_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        base_model.to(device)\n\n\n\n\n        '''\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)\n        meta_model.to(device)\n        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                    num_self_attention_layer=args.num_self_attention_layer)\n        #\n        '''\n        meta_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                                  num_self_attention_layer=args.num_self_attention_layer)\n        meta_model.to(device)\n        \n        # Number of slots\n        SW = SlotWeight(len(slot_meta), init_val=np.log(args.init_weight/(1.0 - args.init_weight)))\n        SW.to(device)\n\n        ## prepare optimizer\n        # Prepare optimizer\n        # Prepare optimizer\n        #base_optimizer, base_scheduler = prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n    \n\n        base_optimizer, base_scheduler = \\\n        prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n        \n        logger.info(base_optimizer)\n        # meta model is a copy of the base model, thus shares the optimizer and scheduler\n        meta_optimizer, meta_scheduler = \\\n        prepare_optimizer(meta_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)\n\n        sw_param_optimizer = list(SW.parameters())\n        sw_optimizer = optim.AdamW(sw_param_optimizer, lr=args.sw_lr)\n        sw_scheduler = get_linear_schedule_with_warmup_T(sw_optimizer,\n                                                         int(num_train_steps * args.sw_warmup),\n                                                         num_train_steps)\n        \n        #******************************************************\n        # training\n        #******************************************************\n        logger.info(\"Training...\")\n\n        best_loss = None\n        best_acc = None\n        last_update = None\n\n        for epoch in trange(int(args.n_epochs), desc=\"Epoch\"):       \n            batch_loss, meta_batch_loss = [], []\n            for step, batch in enumerate(tqdm(train_dataloader)):\n                base_model.train()\n\n                batch = [b.to(device) for b in batch]\n                input_ids, segment_ids, input_mask, label_ids, pseudo_label_ids = batch\n                \n                # forward (meta model)\n                meta_model.load_state_dict(base_model.state_dict())\n                meta_optimizer.load_state_dict(base_optimizer.state_dict())\n                meta_optimizer.zero_grad()\n                with higher.innerloop_ctx(meta_model, meta_optimizer) as (meta_m, meta_opt):\n                    meta_m.train()\n                    slot_output = meta_m(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    \n                    loss_slot_gt, loss_slot_pseudo = \\\n                    get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                    \n                    s_weight = SW()\n                \n                    meta_loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                    # first backward\n                    meta_opt.step(meta_loss)\n                    \n                    # compute on the meta validation set\n                    batch_v = next(meta_dataloader)\n                    batch_v = [b.to(device) for b in batch_v]\n                    input_ids_v, segment_ids_v, input_mask_v, label_ids_v = batch_v\n                    # second forward\n                    meta_m.eval() # disable dropout\n                    slot_output_v = meta_m(input_ids=input_ids_v,\n                                           attention_mask=input_mask_v,\n                                           token_type_ids=segment_ids_v,\n                                           slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n                    _, pred_all_distance = slot_value_matching(slot_output_v, value_lookup)\n                    loss_v, _, _ = hard_cross_entropy_loss(pred_all_distance, label_ids_v)\n                    # backward over backward\n                    sw_optimizer.zero_grad()\n                    loss_v.backward()\n                    sw_optimizer.step()\n                    sw_scheduler.step()\n                    meta_batch_loss.append(loss_v.item())\n                \n                # Now we have the updated weight net  \n                # forward (base model)\n                slot_output = base_model(input_ids=input_ids,\n                                         attention_mask=input_mask,\n                                         token_type_ids=segment_ids,\n                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]\n\n                loss_slot_gt, loss_slot_pseudo = \\\n                get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)\n                with torch.no_grad():    \n                    s_weight = SW()\n\n                loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)\n                # backward (base model)\n                base_optimizer.zero_grad()\n                loss.backward()\n                base_optimizer.step()\n                base_scheduler.step()\n\n                batch_loss.append(loss.item())\n                if step % 300 == 0:\n                    logger.info(\"[%d/%d] [%d/%d] mean_loss: %.6f mean_meta_loss: %.6f\" % \\\n                               (epoch+1, args.n_epochs, step, len(train_dataloader),\n                                np.mean(batch_loss), np.mean(meta_batch_loss)))\n                    batch_loss, meta_batch_loss = [], []\n                    logger.info(f'Slot weights: {s_weight.cpu().numpy()}')\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, dev_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n                if last_update is None or best_loss > eval_res['loss']:\n                    best_loss = eval_res['loss']\n#                     save_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#                     torch.save(base_model.state_dict(), save_path)\n                    print(\"Best Loss : \", best_loss)\n                    print(\"\\n\")\n                if last_update is None or best_acc < eval_res['joint_acc']:\n                    best_acc = eval_res['joint_acc']\n                    save_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n                    save_path_w = os.path.join(args.save_dir, 'sw.bin')\n                    torch.save(base_model.state_dict(), save_path)\n                    torch.save(SW.state_dict(), save_path_w)\n                    last_update = epoch\n                    print(\"Best Acc : \", best_acc)\n                    print(\"\\n\")\n\n                logger.info(\"*** Epoch=%d, Last Update=%d, Dev Loss=%.6f, Dev Acc=%.6f, Dev Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***\" % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if (epoch+1) % args.eval_epoch == 0:\n                eval_res = model_evaluation(base_model, test_data_raw, tokenizer,\n                                            slot_lookup, value_lookup, ontology, epoch+1)\n\n                logger.info(\"*** Epoch=%d, Last Update=%d, Tes Loss=%.6f, Tes Acc=%.6f, Tes Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***\" % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))\n\n            if last_update + args.patience <= epoch:\n                    break\n            torch.cuda.empty_cache()\n\n#         print(\"Test using best loss model...\")\n#         best_epoch = 0\n#         ckpt_path = os.path.join(args.save_dir, 'model_best_loss.bin')\n#         model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n#                               num_self_attention_layer=args.num_self_attention_layer)\n#         ckpt = torch.load(ckpt_path, map_location='cpu')\n#         model.load_state_dict(ckpt)\n#         model.to(device)\n\n#         test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n#                                     ontology, best_epoch, is_gt_p_state=False)\n#         logger.info(\"Results based on best loss: \")\n#         logger.info(test_res)\n    #----------------------------------------------------------------------\n    print(\"Test using best acc model...\")\n    best_epoch = 1\n    ckpt_path = os.path.join(args.save_dir, 'model_best_acc.bin')\n    model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,\n                          num_self_attention_layer=args.num_self_attention_layer)\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    model.load_state_dict(ckpt)\n    model.to(device)\n\n    test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,\n                                ontology, best_epoch, is_gt_p_state=False)\n    logger.info(\"Results based on best acc: \")\n    logger.info(test_res)\n    \n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(\"--data_dir\", default='data/mwz2.4', type=str)\n    parser.add_argument(\"--train_data\", default='train_dials_v2.json', type=str)\n    parser.add_argument(\"--dev_data\", default='dev_dials_v2.json', type=str)\n    parser.add_argument(\"--test_data\", default='test_dials_v2.json', type=str)\n    #parser.add_argument(\"--pretrained_model\", default='bert-base-uncased', type=str)\n    parser.add_argument(\"--pretrained_model\", default='bert-base-multilingual-cased', type=str)\n\n    parser.add_argument(\"--save_dir\", default='output-meta24-S1/exp', type=str)\n\n    parser.add_argument(\"--random_seed\", default=42, type=int)\n\n    #parser.add_argument(\"--train_batch_size\", default=16, type=int)\n    #parser.add_argument(\"--meta_batch_size\", default=8, type=int)\n    parser.add_argument(\"--train_batch_size\", default=2, type=int)  # Reduce from 16 to 8 to 2\n    parser.add_argument(\"--meta_batch_size\", default=1, type=int)   # Reduce from 8 to 4 to 1\n    \n    parser.add_argument(\"--enc_warmup\", default=0.1, type=float)\n    parser.add_argument(\"--dec_warmup\", default=0.1, type=float)\n    parser.add_argument(\"--sw_warmup\", default=0.1, type=float)\n    parser.add_argument(\"--enc_lr\", default=4e-5, type=float)\n    parser.add_argument(\"--dec_lr\", default=1e-4, type=float)\n    parser.add_argument(\"--sw_lr\", default=5e-5, type=float)\n    parser.add_argument(\"--init_weight\", default=0.5, type=float)\n    parser.add_argument(\"--n_epochs\", default=15, type=int)\n    parser.add_argument(\"--eval_epoch\", default=1, type=int)\n    parser.add_argument(\"--eval_step\", default=100000, type=int)\n\n    parser.add_argument(\"--dropout_prob\", default=0.1, type=float)\n    parser.add_argument(\"--word_dropout\", default=0.1, type=float)\n    \n    parser.add_argument(\"--max_seq_length\", default=512, type=int)\n    parser.add_argument(\"--patience\", default=6, type=int)\n    parser.add_argument(\"--attn_head\", default=4, type=int)\n    parser.add_argument(\"--num_history\", default=20, type=int)\n    parser.add_argument(\"--num_self_attention_layer\", default=6, type=int)\n    \n    parser.add_argument(\"--do_train\", action='store_true')\n       \n    args = parser.parse_args()\n    \n    print('pytorch version: ', torch.__version__)\n    args.torch_version = torch.__version__\n    args.transformers_version = transformers.__version__\n    args.save_dir = args.save_dir + \\\n    f'-sd{args.random_seed}-bz{args.train_batch_size}-{args.meta_batch_size}-lr{args.enc_lr}-{args.dec_lr}-{args.sw_lr}-ep{args.n_epochs}'\n\n    main(args)",
         "",
         "torch.OutOfMemoryError: CUDA out of memory. (Google Colab)",
         "I tried to adapt the mBERT model to an existing code. However, I received the following issue even though I tried different solutions. Here's the news model that I'm trying to adapt to DST-MetaASSIST(STAR), which you can find it here: These are the new models: I changed the 'train-S1.py' file and then run it The new script: Any suggestions?! Thanks in advance.",
         "",
         "torch.OutOfMemoryError: CUDA out of memory. (Google Colab) I tried to adapt the mBERT model to an existing code. However, I received the following issue even though I tried different solutions. Here's the news model that I'm trying to adapt to DST-MetaASSIST(STAR), which you can find it here: These are the new models: I changed the 'train-S1.py' file and then run it The new script: Any suggestions?! Thanks in advance. ",
         "torch.outofmemoryerror : cuda memory . ( google colab ) tried adapt mbert model existing code . however , received following issue even though tried different solutions . 's news model 'm trying adapt dst-metaassist ( star ) , find : new models : changed 'train-s1.py ' file run new script : suggestions ? ! thanks advance ."
        ],
        [
         "17",
         "79377676",
         "how to create a Natural Language Inference pipeline in haystack",
         "<p>Could anyone help me with some advice on how to create a Natural Language Inference pipeline in haystack</p>\n<p>I want to use the Haystack framework to create a pipeline for Natural Language Inference on the response from a Retrieval-Augmented Generation (RAG) application</p>\n<p>Because I'm using haystack-ai , I cannot use farm-haystack. If I could use farm-haystack (v1.0) I believe I could do something like below:</p>\n<pre><code>from haystack import Pipeline\nfrom haystack_ai.nodes import HuggingFaceTextClassifier\n\nclassifier = HuggingFaceTextClassifier(\n    model_name_or_path=entailment_model,\n    task=&quot;text-classification&quot;,  # Task type: text classification\n    labels=[\n        &quot;entailment&quot;,\n        &quot;contradiction&quot;,\n        &quot;neutral&quot;,\n    ],  # Define the labels your model is trained on\n)\n\nclassifier_pipeline = Pipeline()\nclassifier_pipeline.add_cmponent(&quot;classifier_llm&quot;, classifier)\npremise = &quot;The sun rises in the east and sets in the west.&quot;\nhypothesis = &quot;The sun rises in the east.&quot;\n\nclassifier_pipeline.run({&quot;classifier_llm&quot;: {&quot;text&quot;: premise, &quot;text_pair&quot;: hypothesis}})\n</code></pre>\n<p>However I cannot see how to achieve the same in haystack v2.0 (haystack-ai) .</p>\n<p>Any comments or pointers welcome.</p>\n",
         "2025-01-22 12:17:47",
         "0",
         "44",
         "1",
         "<nlp><huggingface><haystack>",
         null,
         null,
         "from haystack import Pipeline\nfrom haystack_ai.nodes import HuggingFaceTextClassifier\n\nclassifier = HuggingFaceTextClassifier(\n    model_name_or_path=entailment_model,\n    task=\"text-classification\",  # Task type: text classification\n    labels=[\n        \"entailment\",\n        \"contradiction\",\n        \"neutral\",\n    ],  # Define the labels your model is trained on\n)\n\nclassifier_pipeline = Pipeline()\nclassifier_pipeline.add_cmponent(\"classifier_llm\", classifier)\npremise = \"The sun rises in the east and sets in the west.\"\nhypothesis = \"The sun rises in the east.\"\n\nclassifier_pipeline.run({\"classifier_llm\": {\"text\": premise, \"text_pair\": hypothesis}})",
         "",
         "how to create a Natural Language Inference pipeline in haystack",
         "Could anyone help me with some advice on how to create a Natural Language Inference pipeline in haystack I want to use the Haystack framework to create a pipeline for Natural Language Inference on the response from a Retrieval-Augmented Generation (RAG) application Because I'm using haystack-ai , I cannot use farm-haystack. If I could use farm-haystack (v1.0) I believe I could do something like below: However I cannot see how to achieve the same in haystack v2.0 (haystack-ai) . Any comments or pointers welcome.",
         "",
         "how to create a Natural Language Inference pipeline in haystack Could anyone help me with some advice on how to create a Natural Language Inference pipeline in haystack I want to use the Haystack framework to create a pipeline for Natural Language Inference on the response from a Retrieval-Augmented Generation (RAG) application Because I'm using haystack-ai , I cannot use farm-haystack. If I could use farm-haystack (v1.0) I believe I could do something like below: However I cannot see how to achieve the same in haystack v2.0 (haystack-ai) . Any comments or pointers welcome. ",
         "create natural language inference pipeline haystack could anyone help advice create natural language inference pipeline haystack want use haystack framework create pipeline natural language inference response retrieval-augmented generation ( rag ) application 'm using haystack-ai , use farm-haystack . could use farm-haystack ( v1.0 ) believe could something like : however see achieve haystack v2.0 ( haystack-ai ) . comments pointers welcome ."
        ],
        [
         "18",
         "79372521",
         "TypeError: isinstance() arg 2 must be a type or tuple of types with collections search in Weaviate",
         "<p>I have the following code:</p>\n<pre><code>from weaviate.classes.query import MetadataQuery\nimport weaviate\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\nclient = weaviate.connect_to_local()\nauto_finance= client.collections.get(&quot;AutomotiveFinance&quot;)\n\nquery_vector = embedding_model.embed_documents(&quot;what is Honda Cash and cash equivalents?&quot;)\nprint(len(query_vector[0]))\n#query_vector=[float(i) for i in query_vector[0]]\nquery_vector = [0.023]*768\nresponse = auto_finance.query.hybrid(\n    query = &quot;what is Honda Cash and cash equivalents?&quot;,\n    vector = query_vector, alpha = 0.25, limit = 4  \n)\n</code></pre>\n<p>It queries from a collections called AutomotiveFinance, which is locally hosted.\nI have replaced the query_vec with a dummy vec just to debug, since that example is used in the official document.</p>\n<p>The Automotive Finance collections seems to be ok: below is the inspection code and output:</p>\n<pre><code>import weaviate\nimport weaviate.classes.config as wc\n\n\nclient = weaviate.connect_to_local()\n\nresult = client.collections.get('AutomotiveFinance')\nfor item in result.iterator():\n    print(item.uuid, item.properties)\n\ndata_object = result.query.fetch_object_by_id(\n    &quot;ffdc789b-b188-4fcf-94d5-2e4ad6be37ec&quot;,\n    include_vector=True\n)\n\nprint(len(data_object.vector[&quot;default&quot;]))\n\nclient.close()\n</code></pre>\n<p>output from the AutomotiveFinance inspection code has all the metadata and the id as expected:</p>\n<pre><code>faab711b-f714-457c-a547-ff755b2de4d3 {'page1': 2, 'company': 'honda', 'doc_type': '10q', 'page2': 3, 'raw_text': 'THIS IS PAGE 3\\nBased on the provided text from the SEC Form 10-Q, here is a structured summary of the key sections and items:\\n\\n### Company Information\\n- **Company Name:** American Honda Finance Corporation\\n- **Report \nType:** Quarterly Report on Form 10-Q\\n- **Reporting Period:** For the quarter ended June 30, 2024'}\n</code></pre>\n<p>And the length of the vector is 768 -&gt; this matches the vector embedding model's expected behavior.</p>\n<p>However, when I run the query, I am keep getting the following error:</p>\n<pre><code>PS C:\\Users\\ikim1&gt; &amp; C:/Users/ikim1/RAG-blog/Scripts/python.exe &quot;c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py&quot;\n768\nTraceback (most recent call last):\n  File &quot;c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py&quot;, line 13, in &lt;module&gt;\n    response = auto_finance.query.hybrid(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\syncify.py&quot;, line 23, in sync_method\n    return _EventLoopSingleton.get_instance().run_until_complete(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\event_loop.py&quot;, line 40, in run_until_complete       \n    return fut.result()\n  File &quot;C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result    \n    return self.__get_result()\n  File &quot;C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\queries\\hybrid\\query.py&quot;, line 107, in hybrid\n    res = await self._query.hybrid(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\grpc\\query.py&quot;, line 189, in hybrid      \n    _validate_input(\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 31, in _validate_input\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 31, in &lt;genexpr&gt;\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 61, in _is_valid\n    return all(isinstance(val, args[0]) for val in value)\n  File &quot;C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py&quot;, line 61, in &lt;genexpr&gt;\n    return all(isinstance(val, args[0]) for val in value)\nTypeError: isinstance() arg 2 must be a type or tuple of types\n</code></pre>\n<p>I am not completely sure as to what is going on. I think I inputted all the parameters as described in this: <a href=\"https://weaviate.io/developers/weaviate/search/hybrid#specify-a-search-vector\" rel=\"nofollow noreferrer\">https://weaviate.io/developers/weaviate/search/hybrid#specify-a-search-vector</a>.</p>\n<p>EDIT: I figured I should also provide on how I am doing the embedding and all:</p>\n<pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nimport weaviate\nimport weaviate.classes.config as wc\nimport json\nimport os\n\nclient = weaviate.connect_to_local()\n# check if the client is alive\nassert client.is_live()\n\n# delete the existing schema + create schema \nclient.collections.delete(&quot;AutomotiveFinance&quot;)\nclient.collections.create(\n    name = 'AutomotiveFinance',\n    properties=[\n        wc.Property(name = 'page1', data_type = wc.DataType.INT),\n        wc.Property(name = 'page2', data_type = wc.DataType.INT),\n        wc.Property(name = 'company', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'doc_type', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'raw_text', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'embedding', data_type = wc.DataType.BLOB) # here BLOB stands for binary large object.\n    ]\n)\nauto_finance = client.collections.get(&quot;AutomotiveFinance&quot;)\n\n# get the path of each json file\njson_top_file_path = r'C:\\Users\\ikim1\\OneDrive\\Desktop\\RAG file'\njson_file_path = []\nfor file in os.listdir(json_top_file_path):\n    if file.endswith('.json'):\n        file_path = os.path.join(json_top_file_path, file)\n        json_file_path.append(file_path)\n\n# Initialize the text splitter +  embedding model\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,  # Maximum size of each chunk\n    chunk_overlap=100  # Overlap between consecutive chunks\n)\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\n# for each file path of json get the chunks. \n\nchunks_with_metadata_list = []\nfor one_json_path in json_file_path: \n    # each json had the following structure\n    # json['pages'], json['file_path'], json['company'] json['doc_type']\n    # in json['pages'], there is list of each page as element \n    \n    # open the json file \n    with open(one_json_path, 'r') as file: \n        json_data = json.load(file)\n        pages = json_data['pages']\n        company = json_data['company']\n        doc_type = json_data['doc_type']\n        \n        # make the entire string from the pages\n        # make sure to insert the page numbers as well. \n        old_page_num = 0; old_md = ''; old_raw_txt = ''\n        json_string = '' \n        for i, page in enumerate(pages): \n            md = page['md']\n            raw_txt = page['text']\n            page_num = page['page']\n            print(i)\n            # if this is the second one, then start the chunking process\n            if i &gt; 0: \n                old_combined_str = &quot;THIS IS PAGE &quot; + str(old_page_num) + '\\n' + old_md + '\\n' + old_raw_txt\n                new_combined_str = &quot;THIS IS PAGE &quot; + str(page_num) + '\\n' + md + '\\n' + raw_txt\n                combined_str = new_combined_str + '\\n' + old_combined_str\n                # chunk the combined_str using recursive splittin,g but inject the metadata. \n                chunks = text_splitter.split_text(combined_str)\n                # inject the metadata into the chunks\n                for chunk in chunks: \n                    # embed the chunk : output is already a list. so no need for conversion for Weaviate\n                    embedded_chunk = embedding_model.embed_documents(chunk)[0]\n                    chunk_metadata = {\n                        &quot;page1&quot; : old_page_num, &quot;page2&quot; : page_num, \n                        &quot;company&quot; : company, &quot;doc_type&quot; : doc_type, \n                        'raw_text' : chunk\n                        }\n                    # put the chunk data into the vector database\n                    auto_finance.data.insert(\n                        properties = chunk_metadata, \n                        vector = [float(i) for i in embedded_chunk]\n                    )\n            # cache the previous one\n            old_md = md\n            old_raw_txt = raw_txt\n            old_page_num = page_num\n            \n\nclient.close()\n\n</code></pre>\n<p>Thanks!</p>\n",
         "2025-01-20 20:00:44",
         "1",
         "35",
         "1",
         "<python-3.x><nlp><vector-database><rag><weaviate>",
         null,
         null,
         "from weaviate.classes.query import MetadataQuery\nimport weaviate\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\nclient = weaviate.connect_to_local()\nauto_finance= client.collections.get(\"AutomotiveFinance\")\n\nquery_vector = embedding_model.embed_documents(\"what is Honda Cash and cash equivalents?\")\nprint(len(query_vector[0]))\n#query_vector=[float(i) for i in query_vector[0]]\nquery_vector = [0.023]*768\nresponse = auto_finance.query.hybrid(\n    query = \"what is Honda Cash and cash equivalents?\",\n    vector = query_vector, alpha = 0.25, limit = 4  \n)\n---\nimport weaviate\nimport weaviate.classes.config as wc\n\n\nclient = weaviate.connect_to_local()\n\nresult = client.collections.get('AutomotiveFinance')\nfor item in result.iterator():\n    print(item.uuid, item.properties)\n\ndata_object = result.query.fetch_object_by_id(\n    \"ffdc789b-b188-4fcf-94d5-2e4ad6be37ec\",\n    include_vector=True\n)\n\nprint(len(data_object.vector[\"default\"]))\n\nclient.close()\n---\nfaab711b-f714-457c-a547-ff755b2de4d3 {'page1': 2, 'company': 'honda', 'doc_type': '10q', 'page2': 3, 'raw_text': 'THIS IS PAGE 3\\nBased on the provided text from the SEC Form 10-Q, here is a structured summary of the key sections and items:\\n\\n### Company Information\\n- **Company Name:** American Honda Finance Corporation\\n- **Report \nType:** Quarterly Report on Form 10-Q\\n- **Reporting Period:** For the quarter ended June 30, 2024'}\n---\nPS C:\\Users\\ikim1> & C:/Users/ikim1/RAG-blog/Scripts/python.exe \"c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py\"\n768\nTraceback (most recent call last):\n  File \"c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py\", line 13, in <module>\n    response = auto_finance.query.hybrid(\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\syncify.py\", line 23, in sync_method\n    return _EventLoopSingleton.get_instance().run_until_complete(\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\event_loop.py\", line 40, in run_until_complete       \n    return fut.result()\n  File \"C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py\", line 439, in result    \n    return self.__get_result()\n  File \"C:\\Users\\ikim1\\AppData\\Local\\Programs\\Python38\\lib\\concurrent\\futures\\_base.py\", line 388, in __get_result\n    raise self._exception\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\queries\\hybrid\\query.py\", line 107, in hybrid\n    res = await self._query.hybrid(\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\collections\\grpc\\query.py\", line 189, in hybrid      \n    _validate_input(\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py\", line 31, in _validate_input\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py\", line 31, in <genexpr>\n    if not any(_is_valid(exp, validate.value) for exp in validate.expected):\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py\", line 61, in _is_valid\n    return all(isinstance(val, args[0]) for val in value)\n  File \"C:\\Users\\ikim1\\RAG-blog\\lib\\site-packages\\weaviate\\validator.py\", line 61, in <genexpr>\n    return all(isinstance(val, args[0]) for val in value)\nTypeError: isinstance() arg 2 must be a type or tuple of types\n---\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nimport weaviate\nimport weaviate.classes.config as wc\nimport json\nimport os\n\nclient = weaviate.connect_to_local()\n# check if the client is alive\nassert client.is_live()\n\n# delete the existing schema + create schema \nclient.collections.delete(\"AutomotiveFinance\")\nclient.collections.create(\n    name = 'AutomotiveFinance',\n    properties=[\n        wc.Property(name = 'page1', data_type = wc.DataType.INT),\n        wc.Property(name = 'page2', data_type = wc.DataType.INT),\n        wc.Property(name = 'company', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'doc_type', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'raw_text', data_type = wc.DataType.TEXT),\n        wc.Property(name = 'embedding', data_type = wc.DataType.BLOB) # here BLOB stands for binary large object.\n    ]\n)\nauto_finance = client.collections.get(\"AutomotiveFinance\")\n\n# get the path of each json file\njson_top_file_path = r'C:\\Users\\ikim1\\OneDrive\\Desktop\\RAG file'\njson_file_path = []\nfor file in os.listdir(json_top_file_path):\n    if file.endswith('.json'):\n        file_path = os.path.join(json_top_file_path, file)\n        json_file_path.append(file_path)\n\n# Initialize the text splitter +  embedding model\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,  # Maximum size of each chunk\n    chunk_overlap=100  # Overlap between consecutive chunks\n)\nembedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\n# for each file path of json get the chunks. \n\nchunks_with_metadata_list = []\nfor one_json_path in json_file_path: \n    # each json had the following structure\n    # json['pages'], json['file_path'], json['company'] json['doc_type']\n    # in json['pages'], there is list of each page as element \n    \n    # open the json file \n    with open(one_json_path, 'r') as file: \n        json_data = json.load(file)\n        pages = json_data['pages']\n        company = json_data['company']\n        doc_type = json_data['doc_type']\n        \n        # make the entire string from the pages\n        # make sure to insert the page numbers as well. \n        old_page_num = 0; old_md = ''; old_raw_txt = ''\n        json_string = '' \n        for i, page in enumerate(pages): \n            md = page['md']\n            raw_txt = page['text']\n            page_num = page['page']\n            print(i)\n            # if this is the second one, then start the chunking process\n            if i > 0: \n                old_combined_str = \"THIS IS PAGE \" + str(old_page_num) + '\\n' + old_md + '\\n' + old_raw_txt\n                new_combined_str = \"THIS IS PAGE \" + str(page_num) + '\\n' + md + '\\n' + raw_txt\n                combined_str = new_combined_str + '\\n' + old_combined_str\n                # chunk the combined_str using recursive splittin,g but inject the metadata. \n                chunks = text_splitter.split_text(combined_str)\n                # inject the metadata into the chunks\n                for chunk in chunks: \n                    # embed the chunk : output is already a list. so no need for conversion for Weaviate\n                    embedded_chunk = embedding_model.embed_documents(chunk)[0]\n                    chunk_metadata = {\n                        \"page1\" : old_page_num, \"page2\" : page_num, \n                        \"company\" : company, \"doc_type\" : doc_type, \n                        'raw_text' : chunk\n                        }\n                    # put the chunk data into the vector database\n                    auto_finance.data.insert(\n                        properties = chunk_metadata, \n                        vector = [float(i) for i in embedded_chunk]\n                    )\n            # cache the previous one\n            old_md = md\n            old_raw_txt = raw_txt\n            old_page_num = page_num\n            \n\nclient.close()",
         "",
         "TypeError: isinstance() arg 2 must be a type or tuple of types with collections search in Weaviate",
         "I have the following code: It queries from a collections called AutomotiveFinance, which is locally hosted. I have replaced the query_vec with a dummy vec just to debug, since that example is used in the official document. The Automotive Finance collections seems to be ok: below is the inspection code and output: output from the AutomotiveFinance inspection code has all the metadata and the id as expected: And the length of the vector is 768 -> this matches the vector embedding model's expected behavior. However, when I run the query, I am keep getting the following error: I am not completely sure as to what is going on. I think I inputted all the parameters as described in this: . EDIT: I figured I should also provide on how I am doing the embedding and all: Thanks!",
         "",
         "TypeError: isinstance() arg 2 must be a type or tuple of types with collections search in Weaviate I have the following code: It queries from a collections called AutomotiveFinance, which is locally hosted. I have replaced the query_vec with a dummy vec just to debug, since that example is used in the official document. The Automotive Finance collections seems to be ok: below is the inspection code and output: output from the AutomotiveFinance inspection code has all the metadata and the id as expected: And the length of the vector is 768 -> this matches the vector embedding model's expected behavior. However, when I run the query, I am keep getting the following error: I am not completely sure as to what is going on. I think I inputted all the parameters as described in this: . EDIT: I figured I should also provide on how I am doing the embedding and all: Thanks! ",
         "typeerror : isinstance ( ) arg 2 must type tuple types collections search weaviate following code : queries collections called automotivefinance , locally hosted . replaced query_vec dummy vec debug , since example used official document . automotive finance collections seems ok : inspection code output : output automotivefinance inspection code metadata id expected : length vector 768 - > matches vector embedding model 's expected behavior . however , run query , keep getting following error : completely sure going . think inputted parameters described : . edit : figured also provide embedding : thanks !"
        ],
        [
         "19",
         "79367089",
         "Is it possible to Fine-Tune TinyBERT on Mac (M1 chip)?",
         "<p>Does fine-tuning require huge resources, or is it possible to do this task on the local machine as well? I have an Apple M1 (8GB RAM). I know bigger models GPU access but what about TinyBERT? My training dataset has 100,000 samples for your reference.</p>\n",
         "2025-01-18 11:58:02",
         "-1",
         "53",
         "1",
         "<deep-learning><nlp><huggingface-transformers><text-classification><fine-tuning>",
         null,
         null,
         "",
         "",
         "Is it possible to Fine-Tune TinyBERT on Mac (M1 chip)?",
         "Does fine-tuning require huge resources, or is it possible to do this task on the local machine as well? I have an Apple M1 (8GB RAM). I know bigger models GPU access but what about TinyBERT? My training dataset has 100,000 samples for your reference.",
         "",
         "Is it possible to Fine-Tune TinyBERT on Mac (M1 chip)? Does fine-tuning require huge resources, or is it possible to do this task on the local machine as well? I have an Apple M1 (8GB RAM). I know bigger models GPU access but what about TinyBERT? My training dataset has 100,000 samples for your reference. ",
         "possible fine-tune tinybert mac ( m1 chip ) ? fine-tuning require huge resources , possible task local machine well ? apple m1 ( 8gb ram ) . know bigger models gpu access tinybert ? training dataset 100,000 samples reference ."
        ],
        [
         "20",
         "79330283",
         "Can't compile Marian NMT",
         "<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=\"https://marian-nmt.github.io/docs/#installation\" rel=\"nofollow noreferrer\">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>\n<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>\n<p>These are the steps that I followed:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n</code></pre>\n<p>This is the result I had:</p>\n<pre><code>➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2\n</code></pre>\n<p>Please help.</p>\n",
         "2025-01-05 06:04:59",
         "4",
         "62",
         "1",
         "<gcc><cmake><nlp><g++>",
         "79332711.0",
         "<p>The diagnostic that your build is tripping, <code>Wtemplate-id-cdtor</code>, was introduced\nwith GCC 14.1. It is a warning, not an error, but your build promotes all warnings to\nerrors, so it breaks your build.</p>\n<p>Although your build specifies <code>-std=c++11</code> in <code>src/3rd_party/spdlog/CMakeLists.txt</code>, which\ngenerates the failure, g++-14 emits <code>Wtemplate-id-cdtor</code> to warn you that the code <em>would be</em>\nillegal under the more recent standard c++20 (and later). Then the warning is made an error.</p>\n<p>The warning is made an error by the compile option <code>-Werror</code>. This option is included in the list\nof compile options <code>ALL_WARNINGS</code>, which is created in the top-level <code>marian/CMakeLists.txt</code>\nat line 227 <em>et seq</em>:</p>\n<pre><code># These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n</code></pre>\n<p>and then applied as compile options for the <code>marian</code> library target in <code>src/CMakeLists.txt</code>\nat line 133:</p>\n<pre><code>target_compile_options(marian PRIVATE ${ALL_WARNINGS})\n</code></pre>\n<p>whence the options are operative for the failing compilation of <code>src/CMakeFiles/marian.dir/common/logging.cpp</code>.</p>\n<p>This failure is a bug in the <code>marian</code> repo which you should <a href=\"https://github.com/marian-nmt/marian/issues\" rel=\"nofollow noreferrer\">report to the maintainers</a>, as\nit does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14.</p>\n<p>Pending a fix, you seem to have three interim options to get your build done. Either:</p>\n<ul>\n<li><p>Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:</p>\n<pre><code>/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\n</code></pre>\n</li>\n</ul>\n<p>e.g. make it <code>registry_t(const registry_t&lt;Mutex&gt;&amp;) = delete;</code> in this occurrence.</p>\n<p>Or:</p>\n<ul>\n<li><p>Locally disable <code>-Wtemplate-id-cdtor</code> at each occurrence, e.g:</p>\n<pre><code>#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored &quot;-Wtemplate-id-cdtor&quot;\nregistry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n#pragma GCC diagnostic pop\n</code></pre>\n</li>\n</ul>\n<p>Or:</p>\n<ul>\n<li>Remove <code>-Werror</code> from the <code>ALL_WARNINGS</code> list in <code>marian/CMakeLists.txt</code> so that <code>Wtemplate-id-cdtor</code> remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status).</li>\n</ul>\n<p>I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.</p>\n",
         "CMakeLists.txt\n---\nset (CMAKE_CXX_STANDARD 11)\n---\ngit clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n---\n➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2",
         "Wtemplate-id-cdtor\n---\n-std=c++11\n---\nsrc/3rd_party/spdlog/CMakeLists.txt\n---\nWtemplate-id-cdtor\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\n# These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n---\nmarian\n---\nsrc/CMakeLists.txt\n---\ntarget_compile_options(marian PRIVATE ${ALL_WARNINGS})\n---\nsrc/CMakeFiles/marian.dir/common/logging.cpp\n---\nmarian\n---\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\n---\nregistry_t(const registry_t<Mutex>&) = delete;\n---\n-Wtemplate-id-cdtor\n---\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wtemplate-id-cdtor\"\nregistry_t<Mutex>(const registry_t<Mutex>&) = delete;\n#pragma GCC diagnostic pop\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\nWtemplate-id-cdtor",
         "Can't compile Marian NMT",
         "I'm using endeavouros. I'm trying to compile Marian with these instructions: . But it fails. The error message seemingly indicates a conflict between the code and c++20. But in all the files of the repo, there is the line . These are the steps that I followed: This is the result I had: Please help.",
         "The diagnostic that your build is tripping, , was introduced with GCC 14.1. It is a warning, not an error, but your build promotes all warnings to errors, so it breaks your build. Although your build specifies in , which generates the failure, g++-14 emits to warn you that the code would be illegal under the more recent standard c++20 (and later). Then the warning is made an error. The warning is made an error by the compile option . This option is included in the list of compile options , which is created in the top-level at line 227 et seq : and then applied as compile options for the library target in at line 133: whence the options are operative for the failing compilation of . This failure is a bug in the repo which you should report to the maintainers , as it does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14. Pending a fix, you seem to have three interim options to get your build done. Either: Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence: e.g. make it in this occurrence. Or: Locally disable at each occurrence, e.g: Or: Remove from the list in so that remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status). I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.",
         "Can't compile Marian NMT I'm using endeavouros. I'm trying to compile Marian with these instructions: . But it fails. The error message seemingly indicates a conflict between the code and c++20. But in all the files of the repo, there is the line . These are the steps that I followed: This is the result I had: Please help. The diagnostic that your build is tripping, , was introduced with GCC 14.1. It is a warning, not an error, but your build promotes all warnings to errors, so it breaks your build. Although your build specifies in , which generates the failure, g++-14 emits to warn you that the code would be illegal under the more recent standard c++20 (and later). Then the warning is made an error. The warning is made an error by the compile option . This option is included in the list of compile options , which is created in the top-level at line 227 et seq : and then applied as compile options for the library target in at line 133: whence the options are operative for the failing compilation of . This failure is a bug in the repo which you should report to the maintainers , as it does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14. Pending a fix, you seem to have three interim options to get your build done. Either: Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence: e.g. make it in this occurrence. Or: Locally disable at each occurrence, e.g: Or: Remove from the list in so that remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status). I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.",
         "ca n't compile marian nmt 'm using endeavouros . 'm trying compile marian instructions : . fails . error message seemingly indicates conflict code c++20 . files repo , line . steps followed : result : please help . diagnostic build tripping , , introduced gcc 14.1. warning , error , build promotes warnings errors , breaks build . although build specifies , generates failure , g++-14 emits warn code would illegal recent standard c++20 ( later ) . warning made error . warning made error compile option . option included list compile options , created top-level line 227 et seq : applied compile options library target line 133 : whence options operative failing compilation . failure bug repo report maintainers , seem reported already . head revision v1.12.0 year older gcc 14. pending fix , seem three interim options get build done . either : make code legal c++11 c++20 diagnostic advice says occurrence : e.g . make occurrence . : locally disable occurrence , e.g : : remove list remains warning . may result diagnostics demoted errors warnings ( default status ) . n't tested options 'd need go trouble installing cuda ."
        ],
        [
         "21",
         "79328514",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )\n</code></pre>\n<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>\n",
         "2025-01-04 08:57:44",
         "2",
         "32",
         "1",
         "<pytorch><nlp><large-language-model><huggingface-trainer>",
         "79328698.0",
         "<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>\n<p>To modify the <strong>data collator</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>class CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])\n        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])\n        return batch\n</code></pre>\n<p>then pass it to <code>Trainer</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n</code></pre>\n<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>\n<p>Hope, it will work!</p>\n",
         "input_ids\n---\nlabels\n---\ninteract_ids\n---\ncandidate_ids\n---\nclass LLMWithCustomLayer(LlamaForCausalLM)\n---\ndef forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )",
         "interact_ids\n---\ncandidate_ids\n---\nclass CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[\"interact_ids\"] = torch.tensor([f[\"interact_ids\"] for f in features])\n        batch[\"candidate_ids\"] = torch.tensor([f[\"candidate_ids\"] for f in features])\n        return batch\n---\nTrainer\n---\ntrainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(\"your-llama-model\"),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n---\nforward()\n---\ninteract_ids\n---\ncandidate_ids",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields ' ', ' ' and so on, and I additionally add 2 custom colunms ' ' and ' '. But i can't get these custom fields in the forward() function of my Model ' '. I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.",
         "You need to modify the data collator to pass and to your model, as Trainer ignores extra columns by default. To modify the data collator then pass it to Now, your method will receive and . Hope, it will work!",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer? I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields ' ', ' ' and so on, and I additionally add 2 custom colunms ' ' and ' '. But i can't get these custom fields in the forward() function of my Model ' '. I an new in LLM fine tuning. Can anyone help me? I would be grateful so much. You need to modify the data collator to pass and to your model, as Trainer ignores extra columns by default. To modify the data collator then pass it to Now, your method will receive and . Hope, it will work!",
         "get custom column model 's forward ( ) function training huggingface trainer ? using huggingface trainer train cumstom model subclassing llama llm . tokenized tokenizer , dataset fields ' ' , ' ' , additionally add 2 custom colunms ' ' ' ' . ca n't get custom fields forward ( ) function model ' ' . new llm fine tuning . anyone help ? would grateful much . need modify data collator pass model , trainer ignores extra columns default . modify data collator pass , method receive . hope , work !"
        ],
        [
         "22",
         "79321551",
         "getting an error: object of type 'float' has no len()",
         "<p>I am a naive in python and started learning python few months ago\nI am working on twitter data in my local, it has 4 columns. ID, brand, sentiment, comment</p>\n<pre><code>def data_clean_pipeline(text):\n    #removing html tags\n    text = str(BeautifulSoup(text).get_text())\n    #removing any nonletter words \n    text = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, text)\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    SW = stopwords.words('english')\n    text = [t for t in text if not t in set(SW)]\n    #Then we can apply stemming and then lemmitize the data\n    SS_stem = SnowballStemmer(language='english')\n    text = [SS_stem.stem(t) for t in text]\n    word_lemmitize = WordNetLemmatizer()\n    text = [word_lemmitize.lemmatize(t) for t in text]\n    return &quot; &quot;.join(text)\n</code></pre>\n<p>When I apply this function to one of the review in twitter data it works, but when i apply to the entire column<br />\nfor example</p>\n<pre><code>data_clean_pipeline(twitter_data['comment'][0] #it works fine and return the output\n</code></pre>\n<p>but the error occurs when i apply this function to a column</p>\n<pre><code>twitter_data['clean'] = twitter_data['comment'].apply(data_clean_pipeline)  \n</code></pre>\n<p>Any feedback would be helpful, thank you:)</p>\n<p>i have attached an image of my error code in the image description for the python error window</p>\n<p><a href=\"https://i.sstatic.net/JpUuncF2.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<p>I was expecting that it will apply the function to the entire comment column which is not happening.<br />\nI have tried to make multiple unsuccessful attempts</p>\n",
         "2025-01-01 12:01:20",
         "0",
         "48",
         "1",
         "<python><python-3.x><twitter><nlp><project>",
         null,
         null,
         "def data_clean_pipeline(text):\n    #removing html tags\n    text = str(BeautifulSoup(text).get_text())\n    #removing any nonletter words \n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    SW = stopwords.words('english')\n    text = [t for t in text if not t in set(SW)]\n    #Then we can apply stemming and then lemmitize the data\n    SS_stem = SnowballStemmer(language='english')\n    text = [SS_stem.stem(t) for t in text]\n    word_lemmitize = WordNetLemmatizer()\n    text = [word_lemmitize.lemmatize(t) for t in text]\n    return \" \".join(text)\n---\ndata_clean_pipeline(twitter_data['comment'][0] #it works fine and return the output\n---\ntwitter_data['clean'] = twitter_data['comment'].apply(data_clean_pipeline)",
         "",
         "getting an error: object of type 'float' has no len()",
         "I am a naive in python and started learning python few months ago I am working on twitter data in my local, it has 4 columns. ID, brand, sentiment, comment When I apply this function to one of the review in twitter data it works, but when i apply to the entire column for example but the error occurs when i apply this function to a column Any feedback would be helpful, thank you:) i have attached an image of my error code in the image description for the python error window enter image description here I was expecting that it will apply the function to the entire comment column which is not happening. I have tried to make multiple unsuccessful attempts",
         "",
         "getting an error: object of type 'float' has no len() I am a naive in python and started learning python few months ago I am working on twitter data in my local, it has 4 columns. ID, brand, sentiment, comment When I apply this function to one of the review in twitter data it works, but when i apply to the entire column for example but the error occurs when i apply this function to a column Any feedback would be helpful, thank you:) i have attached an image of my error code in the image description for the python error window enter image description here I was expecting that it will apply the function to the entire comment column which is not happening. I have tried to make multiple unsuccessful attempts ",
         "getting error : object type 'float ' len ( ) naive python started learning python months ago working twitter data local , 4 columns . id , brand , sentiment , comment apply function one review twitter data works , apply entire column example error occurs apply function column feedback would helpful , thank : ) attached image error code image description python error window enter image description expecting apply function entire comment column happening . tried make multiple unsuccessful attempts"
        ],
        [
         "23",
         "79315936",
         "Is n-gram precision the number of elements in the intersection of one hypothesis and possibly many references?",
         "<p>I was trying to understand how BLEU score works and noticed that if I had to compute the n-gram precisions and have multiple reference sentences, it makes more sense to turn everything into sets to remove duplicates. Order in terms of n-grams does not seem to matter, the order of n-tokens is persisted by n-grams but the order each individial n-gram is irrelevant, so we can indeed just use sets to compute n-gram precision of the hypothesis with respect to many references.</p>\n<p>In other words, if I'm given many references and one hypothesis, I compute the n-gram precision by:</p>\n<ol>\n<li>Forming two sets, one set is the union of all n-gram sets of all references, the other set consists of all n-grams in the hypothesis. Duplicates are eliminated.</li>\n<li>Look for the intersection between both sets. The number of n-grams in that intersection divided by the number of n-grams in the hypothesis set is the n-gram precision.</li>\n</ol>\n<p>I think this idea is also backed up by the <a href=\"https://www.nltk.org/_modules/nltk/translate/bleu_score.html\" rel=\"nofollow noreferrer\">nltk implementation</a> of BLEU scores.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    # Extracts all ngrams in hypothesis\n    # Set an empty Counter if hypothesis is empty.\n    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) &gt;= n else Counter()\n    \n    # Extract a union of references' counts.\n    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n    max_counts = {}\n    for reference in references:\n        reference_counts = (\n            Counter(ngrams(reference, n)) if len(reference) &gt;= n else Counter()\n        )\n        for ngram in counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n    # Assigns the intersection between hypothesis and references' counts.\n    clipped_counts = {\n        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n    }\n</code></pre>\n<p><strong>Am I misunderstanding something or this a correct way of computing n-gram precision?</strong></p>\n",
         "2024-12-29 16:25:12",
         "2",
         "52",
         "2",
         "<nlp><nltk><bleu>",
         null,
         null,
         "# Extracts all ngrams in hypothesis\n    # Set an empty Counter if hypothesis is empty.\n    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n    \n    # Extract a union of references' counts.\n    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n    max_counts = {}\n    for reference in references:\n        reference_counts = (\n            Counter(ngrams(reference, n)) if len(reference) >= n else Counter()\n        )\n        for ngram in counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n    # Assigns the intersection between hypothesis and references' counts.\n    clipped_counts = {\n        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n    }",
         "",
         "Is n-gram precision the number of elements in the intersection of one hypothesis and possibly many references?",
         "I was trying to understand how BLEU score works and noticed that if I had to compute the n-gram precisions and have multiple reference sentences, it makes more sense to turn everything into sets to remove duplicates. Order in terms of n-grams does not seem to matter, the order of n-tokens is persisted by n-grams but the order each individial n-gram is irrelevant, so we can indeed just use sets to compute n-gram precision of the hypothesis with respect to many references. In other words, if I'm given many references and one hypothesis, I compute the n-gram precision by: Forming two sets, one set is the union of all n-gram sets of all references, the other set consists of all n-grams in the hypothesis. Duplicates are eliminated. Look for the intersection between both sets. The number of n-grams in that intersection divided by the number of n-grams in the hypothesis set is the n-gram precision. I think this idea is also backed up by the nltk implementation of BLEU scores. Am I misunderstanding something or this a correct way of computing n-gram precision?",
         "",
         "Is n-gram precision the number of elements in the intersection of one hypothesis and possibly many references? I was trying to understand how BLEU score works and noticed that if I had to compute the n-gram precisions and have multiple reference sentences, it makes more sense to turn everything into sets to remove duplicates. Order in terms of n-grams does not seem to matter, the order of n-tokens is persisted by n-grams but the order each individial n-gram is irrelevant, so we can indeed just use sets to compute n-gram precision of the hypothesis with respect to many references. In other words, if I'm given many references and one hypothesis, I compute the n-gram precision by: Forming two sets, one set is the union of all n-gram sets of all references, the other set consists of all n-grams in the hypothesis. Duplicates are eliminated. Look for the intersection between both sets. The number of n-grams in that intersection divided by the number of n-grams in the hypothesis set is the n-gram precision. I think this idea is also backed up by the nltk implementation of BLEU scores. Am I misunderstanding something or this a correct way of computing n-gram precision? ",
         "n-gram precision number elements intersection one hypothesis possibly many references ? trying understand bleu score works noticed compute n-gram precisions multiple reference sentences , makes sense turn everything sets remove duplicates . order terms n-grams seem matter , order n-tokens persisted n-grams order individial n-gram irrelevant , indeed use sets compute n-gram precision hypothesis respect many references . words , 'm given many references one hypothesis , compute n-gram precision : forming two sets , one set union n-gram sets references , set consists n-grams hypothesis . duplicates eliminated . look intersection sets . number n-grams intersection divided number n-grams hypothesis set n-gram precision . think idea also backed nltk implementation bleu scores . misunderstanding something correct way computing n-gram precision ?"
        ],
        [
         "24",
         "79312133",
         "Getting all leaf words (reverse stemming) into one Python List",
         "<p>On the same lines as the solution provided <a href=\"https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword\">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>\n<p>Imagine I have a shorter sample word list as follows:</p>\n<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n</code></pre>\n<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>\n<pre><code>get_word_forms(&quot;command&quot;)\n</code></pre>\n<p>and get the following output:</p>\n<pre><code>{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n</code></pre>\n<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>\n<p>If I try to reverse-stem the entire list in one go:</p>\n<pre><code>[get_word_forms(word) for word in sample]\n</code></pre>\n<p>I fail at getting any output:</p>\n<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n</code></pre>\n<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>\n<p>something like:</p>\n<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. \n</code></pre>\n",
         "2024-12-27 15:04:05",
         "1",
         "46",
         "1",
         "<python><nlp><nltk>",
         "79312987.0",
         "<p>One solution using nested list comprehensions after stripping forgotten spaces:</p>\n<pre><code>all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']\n</code></pre>\n",
         "get_word_forms\n---\nmy_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n---\nget_word_forms(\"command\")\n---\n{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n---\n[get_word_forms(word) for word in sample]\n---\n[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n---\n['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.",
         "all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']",
         "Getting all leaf words (reverse stemming) into one Python List",
         "On the same lines as the solution provided in this link , I am trying to get all leaf words of one stem word. I am using the community-contributed ( Srivastava) package Imagine I have a shorter sample word list as follows: If I work it manually, I do the following (which is go word-by-word, which is time-consuming if I have a list of 200 words): and get the following output: 'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb. If I try to reverse-stem the entire list in one go: I fail at getting any output: I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb: something like:",
         "One solution using nested list comprehensions after stripping forgotten spaces:",
         "Getting all leaf words (reverse stemming) into one Python List On the same lines as the solution provided in this link , I am trying to get all leaf words of one stem word. I am using the community-contributed ( Srivastava) package Imagine I have a shorter sample word list as follows: If I work it manually, I do the following (which is go word-by-word, which is time-consuming if I have a list of 200 words): and get the following output: 'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb. If I try to reverse-stem the entire list in one go: I fail at getting any output: I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb: something like: One solution using nested list comprehensions after stripping forgotten spaces:",
         "getting leaf words ( reverse stemming ) one python list lines solution provided link , trying get leaf words one stem word . using community-contributed ( srivastava ) package imagine shorter sample word list follows : work manually , following ( go word-by-word , time-consuming list 200 words ) : get following output : 'n ' noun , ' ' adjective , ' v ' verb , ' r ' adverb . try reverse-stem entire list one go : fail getting output : think failing saving output dictionary . eventually , would like output list without breaking noun , adjective , adverb , verb : something like : one solution using nested list comprehensions stripping forgotten spaces :"
        ],
        [
         "25",
         "79302218",
         "torch.nn.functional.softmax giving inaccurate softmax output",
         "<p>I am trying to implement masked self-attention from scratch but when calculating the softmax for the similarity scores I get odd results. I looked at the documentation and other questions posted on here but I still cant figure out what I am doing wrong. Below is a test I set up with the results.</p>\n<p>What I tried:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(sims)\nprint(torch.nn.functional.softmax(sims, dim=1))\n</code></pre>\n<p>Which gives the following output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n        [ 7.,  8.,  9., 10., 11., 12.],\n        [13., 14., 15., 16., 17., 18.],\n        [19., 20., 21., 22., 23., 24.],\n        [25., 26., 27., 28., 29., 30.],\n        [31., 32., 33., 34., 35., 36.],\n        [37., 38., 39., 40., 41., 42.],\n        [43., 44., 45., 46., 47., 48.],\n        [49., 50., 51., 52., 53., 54.],\n        [55., 56., 57., 58., 59., 60.]])\n\ntensor([[0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337]])\n</code></pre>\n<p>As an example, I am expecting the output of the <code>softmax</code> function on the first row &quot;sims&quot;</p>\n<pre><code>[ 1.,  2.,  3.,  4.,  5.,  6.]\n</code></pre>\n<p>to show</p>\n<pre><code>[ .047,  .095,  .142,  .19,  .238,  .285]\n</code></pre>\n<p>which would be the accurate softmax attention percentages needed to apply to my value tensor</p>\n",
         "2024-12-23 05:18:17",
         "0",
         "51",
         "1",
         "<python><pytorch><nlp><softmax>",
         null,
         null,
         "print(sims)\nprint(torch.nn.functional.softmax(sims, dim=1))\n---\ntensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n        [ 7.,  8.,  9., 10., 11., 12.],\n        [13., 14., 15., 16., 17., 18.],\n        [19., 20., 21., 22., 23., 24.],\n        [25., 26., 27., 28., 29., 30.],\n        [31., 32., 33., 34., 35., 36.],\n        [37., 38., 39., 40., 41., 42.],\n        [43., 44., 45., 46., 47., 48.],\n        [49., 50., 51., 52., 53., 54.],\n        [55., 56., 57., 58., 59., 60.]])\n\ntensor([[0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337]])\n---\nsoftmax\n---\n[ 1.,  2.,  3.,  4.,  5.,  6.]\n---\n[ .047,  .095,  .142,  .19,  .238,  .285]",
         "",
         "torch.nn.functional.softmax giving inaccurate softmax output",
         "I am trying to implement masked self-attention from scratch but when calculating the softmax for the similarity scores I get odd results. I looked at the documentation and other questions posted on here but I still cant figure out what I am doing wrong. Below is a test I set up with the results. What I tried: Which gives the following output: As an example, I am expecting the output of the function on the first row \"sims\" to show which would be the accurate softmax attention percentages needed to apply to my value tensor",
         "",
         "torch.nn.functional.softmax giving inaccurate softmax output I am trying to implement masked self-attention from scratch but when calculating the softmax for the similarity scores I get odd results. I looked at the documentation and other questions posted on here but I still cant figure out what I am doing wrong. Below is a test I set up with the results. What I tried: Which gives the following output: As an example, I am expecting the output of the function on the first row \"sims\" to show which would be the accurate softmax attention percentages needed to apply to my value tensor ",
         "torch.nn.functional.softmax giving inaccurate softmax output trying implement masked self-attention scratch calculating softmax similarity scores get odd results . looked documentation questions posted still cant figure wrong . test set results . tried : gives following output : example , expecting output function first row `` sims '' show would accurate softmax attention percentages needed apply value tensor"
        ],
        [
         "26",
         "79300156",
         "Why does my contrastive learning model's loss and gradients explode during training?",
         "<p>I am fine-tuning an embedding model using contrastive learning. For the loss function, I’m using <code>torch.nn.CrossEntropyLoss</code>.</p>\n<p>The training process initially seems to work fine — the loss decreases steadily on average. However, at some point during training (usually around step 16,000 in this case), the loss and gradients explode. After this, the model is unable to stabilize, and training becomes unusable.</p>\n<p>Here is a graph showing the behavior:</p>\n<p><a href=\"https://i.sstatic.net/o0g7mnA4.png\" rel=\"nofollow noreferrer\">Tensorboard loss by step graph</a></p>\n<h4>What I have tried so far:</h4>\n<ol>\n<li><p><strong>Data preprocessing</strong>:</p>\n<ul>\n<li><p>Removed outliers (e.g., very long texts).</p>\n</li>\n<li><p>Cleaned and filtered the dataset for consistency.</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Hyperparameter tuning</strong>:</p>\n<ul>\n<li><p>Adjusted the learning rate and tried different values.</p>\n</li>\n<li><p>Changed the optimizer (e.g., switching from Adam to SGD)</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Gradient clipping</strong>:</p>\n<ul>\n<li>Clipped gradients to a max norm of 1 using <code>torch.nn.utils.clip_grad_norm_</code>.</li>\n</ul>\n</li>\n</ol>\n<h4>My setup:</h4>\n<ul>\n<li><p><strong>Dataset size</strong>: ~14,000 samples</p>\n</li>\n<li><p><strong>Model architecture</strong>: Transformer-based embedding model</p>\n</li>\n<li><p><strong>Batch size</strong>: 1 (given my gpu capacity)</p>\n</li>\n<li><p><strong>Learning rate</strong>: 1e-5</p>\n</li>\n<li><p><strong>Optimizer</strong>: Adam with weight decay</p>\n</li>\n</ul>\n<p><strong>Training loop (relevant part):</strong></p>\n<pre><code>for epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    for step, batch in enumerate(dataset_train):\n        temperature = max(0.1, 0.05 * (1 - step / num_training_steps))\n\n        # Move data to device\n        anchor_input_ids = batch[&quot;anchor_input_ids&quot;].to(device)\n        anchor_attention_mask = batch[&quot;anchor_attention_mask&quot;].to(device)\n        positive_input_ids = batch[&quot;positive_input_ids&quot;].to(device)\n        positive_attention_mask = batch[&quot;positive_attention_mask&quot;].to(device)\n        negative_input_ids = batch[&quot;negative_input_ids&quot;].to(device)\n        negative_attention_mask = batch[&quot;negative_attention_mask&quot;].to(device)\n\n        anchor_input_ids = anchor_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        anchor_attention_mask = anchor_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        positive_input_ids = positive_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        positive_attention_mask = positive_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        negative_input_ids = negative_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        negative_attention_mask = negative_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n\n        optimizer.zero_grad()\n\n        # Generate embeddings\n        anchor_embeddings = model.forward(anchor_input_ids, anchor_attention_mask)\n        positive_embeddings = model.forward(positive_input_ids, positive_attention_mask)\n        negative_embeddings = model.forward(negative_input_ids, negative_attention_mask)\n\n        # Calculate cosine similarities\n        pos_sim = cosine_similarity(anchor_embeddings, positive_embeddings)\n        neg_sim = cosine_similarity(anchor_embeddings, negative_embeddings)\n\n        # Calculate logits\n        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim.unsqueeze(1)], dim=1) / temperature\n        labels = torch.zeros(logits.size(0), dtype=torch.long).to(device)  # The positive class is always the first\n\n        # Calculate InfoNCE loss\n        loss = torch.nn.CrossEntropyLoss()(logits, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n</code></pre>\n<p><strong>Dataset generation:</strong></p>\n<pre><code>import torch\nfrom torch.utils.data import Dataset\nimport random\n\nclass FineTuneContrastiveDataset(Dataset):\n    def __init__(self, pairs_data, df_1, df_2, tokenizer, max_tokens=512):\n        &quot;&quot;&quot;\n        pairs_data: List of tuples (id_1, id_2, label).\n        df_1: DataFrame containing text data associated with id_1.\n        df_2: DataFrame containing text data associated with id_2.\n        tokenizer: Hugging Face tokenizer.\n        max_tokens: Maximum allowed length for the tokenized text.\n        &quot;&quot;&quot;\n        self.pairs_data = pairs_data\n        self.df_1 = df_1.set_index(&quot;id_1&quot;)\n        self.df_2 = df_2.set_index(&quot;id_2&quot;)\n        self.tokenizer = tokenizer\n        self.max_tokens = max_tokens\n        self.id_2_list = list(self.df_2.index)  # For selecting negative samples\n\n    def __len__(self):\n        return len(self.pairs_data)\n\n    def __getitem__(self, idx):\n        # Retrieve data from the pair\n        id_1, id_2_positive, label = self.pairs_data[idx]\n        \n        # Text associated with id_1 (anchor)\n        text_1 = &quot; &quot;.join(self.df_1.loc[id_1][&quot;chunks&quot;])\n\n        # Positive text associated with id_2\n        text_2_positive = &quot; &quot;.join(self.df_2.loc[id_2_positive][&quot;chunks&quot;])\n\n        # Generate a negative sample from id_2\n        id_2_negative = random.choice(\n            [candidate_id for candidate_id in self.id_2_list if candidate_id != id_2_positive]\n        )\n        text_2_negative = &quot; &quot;.join(self.df_2.loc[id_2_negative][&quot;chunks&quot;])\n\n        # Tokenize inputs\n        inputs_anchor = self.tokenizer(\n            text_1, truncation=True, max_length=self.max_tokens, \n            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;\n        )\n        inputs_positive = self.tokenizer(\n            text_2_positive, truncation=True, max_length=self.max_tokens, \n            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;\n        )\n        inputs_negative = self.tokenizer(\n            text_2_negative, truncation=True, max_length=self.max_tokens, \n            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;\n        )\n\n        return {\n            &quot;anchor_input_ids&quot;: inputs_anchor[&quot;input_ids&quot;].squeeze(0),\n            &quot;anchor_attention_mask&quot;: inputs_anchor[&quot;attention_mask&quot;].squeeze(0),\n            &quot;positive_input_ids&quot;: inputs_positive[&quot;input_ids&quot;].squeeze(0),\n            &quot;positive_attention_mask&quot;: inputs_positive[&quot;attention_mask&quot;].squeeze(0),\n            &quot;negative_input_ids&quot;: inputs_negative[&quot;input_ids&quot;].squeeze(0),\n            &quot;negative_attention_mask&quot;: inputs_negative[&quot;attention_mask&quot;].squeeze(0),\n            &quot;label&quot;: torch.tensor(label, dtype=torch.float),\n            &quot;id_1&quot;: id_1,\n        }\n</code></pre>\n",
         "2024-12-21 21:46:25",
         "1",
         "72",
         "1",
         "<deep-learning><pytorch><nlp><word-embedding>",
         null,
         null,
         "torch.nn.CrossEntropyLoss\n---\ntorch.nn.utils.clip_grad_norm_\n---\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    for step, batch in enumerate(dataset_train):\n        temperature = max(0.1, 0.05 * (1 - step / num_training_steps))\n\n        # Move data to device\n        anchor_input_ids = batch[\"anchor_input_ids\"].to(device)\n        anchor_attention_mask = batch[\"anchor_attention_mask\"].to(device)\n        positive_input_ids = batch[\"positive_input_ids\"].to(device)\n        positive_attention_mask = batch[\"positive_attention_mask\"].to(device)\n        negative_input_ids = batch[\"negative_input_ids\"].to(device)\n        negative_attention_mask = batch[\"negative_attention_mask\"].to(device)\n\n        anchor_input_ids = anchor_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        anchor_attention_mask = anchor_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        positive_input_ids = positive_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        positive_attention_mask = positive_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n        negative_input_ids = negative_input_ids.unsqueeze(0)  # Add a dimension for the batch\n        negative_attention_mask = negative_attention_mask.unsqueeze(0)  # Add a dimension for the batch\n\n        optimizer.zero_grad()\n\n        # Generate embeddings\n        anchor_embeddings = model.forward(anchor_input_ids, anchor_attention_mask)\n        positive_embeddings = model.forward(positive_input_ids, positive_attention_mask)\n        negative_embeddings = model.forward(negative_input_ids, negative_attention_mask)\n\n        # Calculate cosine similarities\n        pos_sim = cosine_similarity(anchor_embeddings, positive_embeddings)\n        neg_sim = cosine_similarity(anchor_embeddings, negative_embeddings)\n\n        # Calculate logits\n        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim.unsqueeze(1)], dim=1) / temperature\n        labels = torch.zeros(logits.size(0), dtype=torch.long).to(device)  # The positive class is always the first\n\n        # Calculate InfoNCE loss\n        loss = torch.nn.CrossEntropyLoss()(logits, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n---\nimport torch\nfrom torch.utils.data import Dataset\nimport random\n\nclass FineTuneContrastiveDataset(Dataset):\n    def __init__(self, pairs_data, df_1, df_2, tokenizer, max_tokens=512):\n        \"\"\"\n        pairs_data: List of tuples (id_1, id_2, label).\n        df_1: DataFrame containing text data associated with id_1.\n        df_2: DataFrame containing text data associated with id_2.\n        tokenizer: Hugging Face tokenizer.\n        max_tokens: Maximum allowed length for the tokenized text.\n        \"\"\"\n        self.pairs_data = pairs_data\n        self.df_1 = df_1.set_index(\"id_1\")\n        self.df_2 = df_2.set_index(\"id_2\")\n        self.tokenizer = tokenizer\n        self.max_tokens = max_tokens\n        self.id_2_list = list(self.df_2.index)  # For selecting negative samples\n\n    def __len__(self):\n        return len(self.pairs_data)\n\n    def __getitem__(self, idx):\n        # Retrieve data from the pair\n        id_1, id_2_positive, label = self.pairs_data[idx]\n        \n        # Text associated with id_1 (anchor)\n        text_1 = \" \".join(self.df_1.loc[id_1][\"chunks\"])\n\n        # Positive text associated with id_2\n        text_2_positive = \" \".join(self.df_2.loc[id_2_positive][\"chunks\"])\n\n        # Generate a negative sample from id_2\n        id_2_negative = random.choice(\n            [candidate_id for candidate_id in self.id_2_list if candidate_id != id_2_positive]\n        )\n        text_2_negative = \" \".join(self.df_2.loc[id_2_negative][\"chunks\"])\n\n        # Tokenize inputs\n        inputs_anchor = self.tokenizer(\n            text_1, truncation=True, max_length=self.max_tokens, \n            padding=\"max_length\", return_tensors=\"pt\"\n        )\n        inputs_positive = self.tokenizer(\n            text_2_positive, truncation=True, max_length=self.max_tokens, \n            padding=\"max_length\", return_tensors=\"pt\"\n        )\n        inputs_negative = self.tokenizer(\n            text_2_negative, truncation=True, max_length=self.max_tokens, \n            padding=\"max_length\", return_tensors=\"pt\"\n        )\n\n        return {\n            \"anchor_input_ids\": inputs_anchor[\"input_ids\"].squeeze(0),\n            \"anchor_attention_mask\": inputs_anchor[\"attention_mask\"].squeeze(0),\n            \"positive_input_ids\": inputs_positive[\"input_ids\"].squeeze(0),\n            \"positive_attention_mask\": inputs_positive[\"attention_mask\"].squeeze(0),\n            \"negative_input_ids\": inputs_negative[\"input_ids\"].squeeze(0),\n            \"negative_attention_mask\": inputs_negative[\"attention_mask\"].squeeze(0),\n            \"label\": torch.tensor(label, dtype=torch.float),\n            \"id_1\": id_1,\n        }",
         "",
         "Why does my contrastive learning model's loss and gradients explode during training?",
         "I am fine-tuning an embedding model using contrastive learning. For the loss function, Im using . The training process initially seems to work fine the loss decreases steadily on average. However, at some point during training (usually around step 16,000 in this case), the loss and gradients explode. After this, the model is unable to stabilize, and training becomes unusable. Here is a graph showing the behavior: Tensorboard loss by step graph What I have tried so far: Data preprocessing : Removed outliers (e.g., long texts). Cleaned and filtered the dataset for consistency. Hyperparameter tuning : Adjusted the learning rate and tried different values. Changed the optimizer (e.g., switching from Adam to SGD) Gradient clipping : Clipped gradients to a max norm of 1 using . My setup: Dataset size : ~14,000 samples Model architecture : Transformer-based embedding model Batch size : 1 (given my gpu capacity) Learning rate : 1e-5 Optimizer : Adam with weight decay Training loop (relevant part): Dataset generation:",
         "",
         "Why does my contrastive learning model's loss and gradients explode during training? I am fine-tuning an embedding model using contrastive learning. For the loss function, Im using . The training process initially seems to work fine the loss decreases steadily on average. However, at some point during training (usually around step 16,000 in this case), the loss and gradients explode. After this, the model is unable to stabilize, and training becomes unusable. Here is a graph showing the behavior: Tensorboard loss by step graph What I have tried so far: Data preprocessing : Removed outliers (e.g., long texts). Cleaned and filtered the dataset for consistency. Hyperparameter tuning : Adjusted the learning rate and tried different values. Changed the optimizer (e.g., switching from Adam to SGD) Gradient clipping : Clipped gradients to a max norm of 1 using . My setup: Dataset size : ~14,000 samples Model architecture : Transformer-based embedding model Batch size : 1 (given my gpu capacity) Learning rate : 1e-5 Optimizer : Adam with weight decay Training loop (relevant part): Dataset generation: ",
         "contrastive learning model 's loss gradients explode training ? fine-tuning embedding model using contrastive learning . loss function , im using . training process initially seems work fine loss decreases steadily average . however , point training ( usually around step 16,000 case ) , loss gradients explode . , model unable stabilize , training becomes unusable . graph showing behavior : tensorboard loss step graph tried far : data preprocessing : removed outliers ( e.g. , long texts ) . cleaned filtered dataset consistency . hyperparameter tuning : adjusted learning rate tried different values . changed optimizer ( e.g. , switching adam sgd ) gradient clipping : clipped gradients max norm 1 using . setup : dataset size : ~14,000 samples model architecture : transformer-based embedding model batch size : 1 ( given gpu capacity ) learning rate : 1e-5 optimizer : adam weight decay training loop ( relevant part ) : dataset generation :"
        ],
        [
         "27",
         "79298368",
         "Inspect all probabilities of BERTopic model",
         "<p>Say I build a BERTopic model using</p>\n<pre><code>from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre>\n<p>Inspecting <code>probs</code> gives me just a single value for each item in <code>docs</code>.</p>\n<pre><code>probs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n</code></pre>\n<p>I would like the entire probability vector across all topics (so in this case, where <code>nr_topics=20</code>, I want a vector of 20 probabilities for each item in <code>docs</code>). In other words, if I have N items in <code>docs</code> and K topics, I would like an NxK output.</p>\n",
         "2024-12-20 20:49:34",
         "1",
         "41",
         "1",
         "<python><nlp><topic-modeling>",
         "79299703.0",
         "<p>For individual topic probability across each document you need to add one more argument.</p>\n<pre><code>topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n</code></pre>\n<p>Note: This calculate_probabilities = True will only work if you are using <strong><code>HDBSCAN</code></strong> clustering embedding model. And Bertopic by default uses <code>all-MiniLM-L6-v2</code>.</p>\n<p><strong>Official documentation:</strong> <a href=\"https://maartengr.github.io/BERTopic/api/bertopic.html\" rel=\"nofollow noreferrer\">https://maartengr.github.io/BERTopic/api/bertopic.html</a></p>\n<p>They have mentioned the same in document as well.</p>\n",
         "from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n---\nprobs\n---\ndocs\n---\nprobs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n---\nnr_topics=20\n---\ndocs\n---\ndocs",
         "topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n---\nHDBSCAN\n---\nall-MiniLM-L6-v2",
         "Inspect all probabilities of BERTopic model",
         "Say I build a BERTopic model using Inspecting gives me just a single value for each item in . I would like the entire probability vector across all topics (so in this case, where , I want a vector of 20 probabilities for each item in ). In other words, if I have N items in and K topics, I would like an NxK output.",
         "For individual topic probability across each document you need to add one more argument. Note: This calculate_probabilities = True will only work if you are using clustering embedding model. And Bertopic by default uses . Official documentation: They have mentioned the same in document as well.",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in . I would like the entire probability vector across all topics (so in this case, where , I want a vector of 20 probabilities for each item in ). In other words, if I have N items in and K topics, I would like an NxK output. For individual topic probability across each document you need to add one more argument. Note: This calculate_probabilities = True will only work if you are using clustering embedding model. And Bertopic by default uses . Official documentation: They have mentioned the same in document as well.",
         "inspect probabilities bertopic model say build bertopic model using inspecting gives single value item . would like entire probability vector across topics ( case , , want vector 20 probabilities item ) . words , n items k topics , would like nxk output . individual topic probability across document need add one argument . note : calculate_probabilities = true work using clustering embedding model . bertopic default uses . official documentation : mentioned document well ."
        ],
        [
         "28",
         "79293919",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>\n<p>For example:\nA sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>\n<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>\n<p>Any and all help is welcome and appreciated.</p>\n<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>\n",
         "2024-12-19 10:24:04",
         "0",
         "62",
         "2",
         "<python><nlp><nltk><detection>",
         "79294074.0",
         "<p>You need a external dataset for this task. You can try dataset such as google n gram dataset.</p>\n<p>Here is the breakdown of the problem statement:</p>\n<ol>\n<li>Input: &quot;I enjoy a cold glass of water on a hot day&quot;. <code>Output</code>: &quot;water&quot;.</li>\n<li>Split the sentences into words list.</li>\n</ol>\n<blockquote>\n<p>Example: [&quot;I&quot;, &quot;enjoy&quot;, &quot;a&quot;, &quot;cold&quot;, &quot;glass&quot;, &quot;of&quot;, &quot;water&quot;, &quot;on&quot;,\n&quot;a&quot;, &quot;hot&quot;, &quot;day&quot;]</p>\n</blockquote>\n<ol start=\"3\">\n<li>First loop in through all the word of the sentences. so let say you are at first word &quot;I&quot;.</li>\n<li>Now you will look the same word &quot;I&quot; in external dataset and will look for the frequency of that word.\nLet say the word &quot;I&quot; in external dataset is repeated <code>5000000</code> times</li>\n<li>Repeat this task for all the word.</li>\n<li>Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.\nFrequency in the below example is random value not exact value.</li>\n</ol>\n<blockquote>\n<pre><code>{\n    &quot;I&quot;: 5000000,\n    &quot;enjoy&quot;: 50000,\n    &quot;a&quot;: 10000000,\n    &quot;cold&quot;: 30000,\n    &quot;glass&quot;: 100000,\n    &quot;of&quot;: 8000000,\n    &quot;water&quot;: 1200000,\n    &quot;on&quot;: 6000000,\n    &quot;hot&quot;: 700000,\n    &quot;day&quot;: 400000\n}\n</code></pre>\n</blockquote>\n<ol start=\"7\">\n<li>Pick the word with highest frequency.</li>\n</ol>\n<p>Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself</p>\n",
         "",
         "Output\n---\n5000000\n---\n{\n    \"I\": 5000000,\n    \"enjoy\": 50000,\n    \"a\": 10000000,\n    \"cold\": 30000,\n    \"glass\": 100000,\n    \"of\": 8000000,\n    \"water\": 1200000,\n    \"on\": 6000000,\n    \"hot\": 700000,\n    \"day\": 400000\n}",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do. For example: A sentence \"I enjoy a cold glass of water on a hot day\" would return \"water\" because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations. I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route Any and all help is welcome and appreciated. For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.",
         "You need a external dataset for this task. You can try dataset such as google n gram dataset. Here is the breakdown of the problem statement: Input: \"I enjoy a cold glass of water on a hot day\". : \"water\". Split the sentences into words list. Example: \"I\", \"enjoy\", \"a\", \"cold\", \"glass\", \"of\", \"water\", \"on\", \"a\", \"hot\", \"day\" First loop in through all the word of the sentences. so let say you are at first word \"I\". Now you will look the same word \"I\" in external dataset and will look for the frequency of that word. Let say the word \"I\" in external dataset is repeated times Repeat this task for all the word. Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data. Frequency in the below example is random value not exact value. Pick the word with highest frequency. Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do. For example: A sentence \"I enjoy a cold glass of water on a hot day\" would return \"water\" because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations. I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route Any and all help is welcome and appreciated. For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess. You need a external dataset for this task. You can try dataset such as google n gram dataset. Here is the breakdown of the problem statement: Input: \"I enjoy a cold glass of water on a hot day\". : \"water\". Split the sentences into words list. Example: \"I\", \"enjoy\", \"a\", \"cold\", \"glass\", \"of\", \"water\", \"on\", \"a\", \"hot\", \"day\" First loop in through all the word of the sentences. so let say you are at first word \"I\". Now you will look the same word \"I\" in external dataset and will look for the frequency of that word. Let say the word \"I\" in external dataset is repeated times Repeat this task for all the word. Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data. Frequency in the below example is random value not exact value. Pick the word with highest frequency. Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself",
         "determining popular words english dictionary within dictionary words forgive wording awful , 'm trying figure determine used words english language set words dictionary 've made . 've done research nltk ca n't seem find function within ( library matter ) help need . example : sentence `` enjoy cold glass water hot day '' would return `` water '' 's used word day day conversation sentence . essentially need returned value frequently used word conversations . figure 'll likely involve ai , time 've tried use ai wind copy pasting code n't understand , 'm trying avoid going route help welcome appreciated . context , decided start project would essentially guess predetermined word based characters user says n't computers guess . need external dataset task . try dataset google n gram dataset . breakdown problem statement : input : `` enjoy cold glass water hot day '' . : `` water '' . split sentences words list . example : `` '' , `` enjoy '' , `` '' , `` cold '' , `` glass '' , `` '' , `` water '' , `` '' , `` '' , `` hot '' , `` day '' first loop word sentences . let say first word `` '' . look word `` '' external dataset look frequency word . let say word `` '' external dataset repeated times repeat task word . dictionary word sentence key value frequency word get external data . frequency example random value exact value . pick word highest frequency . note : try big corpus external data . using big corpus english word used conversation . even frequency mentioned create"
        ],
        [
         "29",
         "79293889",
         "catelog sentences into 5 words that represent them",
         "<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>\n<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>\n<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>\n<p>I will glad for recomendations how to do that</p>\n<p><strong>edit</strong></p>\n<p>represnt = the semantic distance between the word to the text.</p>\n<p>for example -\nlets say in row 1 the text is &quot;i want to eat&quot;\nand I have 2 words : food and house.</p>\n<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>\n",
         "2024-12-19 10:16:47",
         "0",
         "53",
         "1",
         "<python><pandas><nlp><text-mining><similarity>",
         "79294099.0",
         "<p>You could use a pre-trained sentence transformer model from <a href=\"https://pypi.org/project/sentence-transformers/\" rel=\"nofollow noreferrer\"><code>sentence_transformers</code></a>:</p>\n<pre><code>import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -&gt; None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -&gt; list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -&gt; pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838\n</code></pre>\n",
         "df['text']\n---\ndf[\"word1\"]\n---\ndf[\"word2\"]\n---\ndf[\"food \"]\n---\ndf[\"house\"]",
         "sentence_transformers\n---\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -> None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -> list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -> pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n---\ntext  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838",
         "catelog sentences into 5 words that represent them",
         "I have dataframe with 1000 text rows. I also have 5 words that I want to know for each one of them how much they represnt the text (between 0 to 1) every score will be in , and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text. for example - lets say in row 1 the text is \"i want to eat\" and I have 2 words : food and house. so in it would be higher score than in",
         "You could use a pre-trained sentence transformer model from : Output:",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows. I also have 5 words that I want to know for each one of them how much they represnt the text (between 0 to 1) every score will be in , and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text. for example - lets say in row 1 the text is \"i want to eat\" and I have 2 words : food and house. so in it would be higher score than in You could use a pre-trained sentence transformer model from : Output:",
         "catelog sentences 5 words represent dataframe 1000 text rows . also 5 words want know one much represnt text ( 0 1 ) every score , etc glad recomendations edit represnt = semantic distance word text . example - lets say row 1 text `` want eat '' 2 words : food house . would higher score could use pre-trained sentence transformer model : output :"
        ],
        [
         "30",
         "79287799",
         "How to correctly identify entity types for tokens using spaCy using python?",
         "<p>I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this.</p>\n<p>Here is the code I am using:</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\ndef getPayeeName(description):\n    description = description.replace(&quot;-&quot;, &quot; &quot;).replace(&quot;/&quot;, &quot; &quot;).strip()\n    doc = nlp(description)\n\n    for token in doc:\n        print(f&quot;Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else 'None'}&quot;)\n\n# Example input\ndescription = &quot;UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 MEDICAL LTD HDFC 50200&quot;\ngetPayeeName(description)\n</code></pre>\n<p>Token: UPI, Entity: ORG</p>\n<p>Token: DR, Entity: ORG</p>\n<p>Token: 400874707203, Entity: None</p>\n<p>Token: BENGALORE, Entity: None</p>\n<p>Token: 08, Entity: DATE</p>\n<p>Token: JAN, Entity: DATE</p>\n<p>Token: 2024, Entity: DATE</p>\n<p>Token: 14:38:56, Entity: None</p>\n<p>Token: MEDICAL, Entity: ORG</p>\n<p>Token: LTD, Entity: ORG</p>\n<p>Token: HDFC, Entity: ORG</p>\n<p>Token: 50200, Entity: ORG</p>\n<ul>\n<li><p>50200 is identified as ORG, but it is just a number.</p>\n</li>\n<li><p>BENGALORE is a city, but it is not recognized as a GPE or location\n(returns None).</p>\n</li>\n<li><p>UPI and DR are acronyms/abbreviations, but they are incorrectly\nidentified as ORG.</p>\n</li>\n</ul>\n<p>I want the entity recognition to be more accurate and reliable.\nHow can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition?</p>\n<p>Note: I tried ChatGPT as well, but still this issue is not solved.</p>\n",
         "2024-12-17 12:09:49",
         "1",
         "44",
         "1",
         "<python><machine-learning><nlp><spacy>",
         null,
         null,
         "import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef getPayeeName(description):\n    description = description.replace(\"-\", \" \").replace(\"/\", \" \").strip()\n    doc = nlp(description)\n\n    for token in doc:\n        print(f\"Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else 'None'}\")\n\n# Example input\ndescription = \"UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 MEDICAL LTD HDFC 50200\"\ngetPayeeName(description)",
         "",
         "How to correctly identify entity types for tokens using spaCy using python?",
         "I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this. Here is the code I am using: Token: UPI, Entity: ORG Token: DR, Entity: ORG Token: 400874707203, Entity: None Token: BENGALORE, Entity: None Token: 08, Entity: DATE Token: JAN, Entity: DATE Token: 2024, Entity: DATE Token: 14:38:56, Entity: None Token: MEDICAL, Entity: ORG Token: LTD, Entity: ORG Token: HDFC, Entity: ORG Token: 50200, Entity: ORG 50200 is identified as ORG, but it is just a number. BENGALORE is a city, but it is not recognized as a GPE or location (returns None). UPI and DR are acronyms/abbreviations, but they are incorrectly identified as ORG. I want the entity recognition to be more accurate and reliable. How can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition? Note: I tried ChatGPT as well, but still this issue is not solved.",
         "",
         "How to correctly identify entity types for tokens using spaCy using python? I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this. Here is the code I am using: Token: UPI, Entity: ORG Token: DR, Entity: ORG Token: 400874707203, Entity: None Token: BENGALORE, Entity: None Token: 08, Entity: DATE Token: JAN, Entity: DATE Token: 2024, Entity: DATE Token: 14:38:56, Entity: None Token: MEDICAL, Entity: ORG Token: LTD, Entity: ORG Token: HDFC, Entity: ORG Token: 50200, Entity: ORG 50200 is identified as ORG, but it is just a number. BENGALORE is a city, but it is not recognized as a GPE or location (returns None). UPI and DR are acronyms/abbreviations, but they are incorrectly identified as ORG. I want the entity recognition to be more accurate and reliable. How can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition? Note: I tried ChatGPT as well, but still this issue is not solved. ",
         "correctly identify entity types tokens using spacy using python ? 'm using spacy extract identify entity types ( like org , gpe , date , etc . ) text description . however , noticing incorrect results , 'm unsure fix . code using : token : upi , entity : org token : dr , entity : org token : 400874707203 , entity : none token : bengalore , entity : none token : 08 , entity : date token : jan , entity : date token : 2024 , entity : date token : 14:38:56 , entity : none token : medical , entity : org token : ltd , entity : org token : hdfc , entity : org token : 50200 , entity : org 50200 identified org , number . bengalore city , recognized gpe location ( returns none ) . upi dr acronyms/abbreviations , incorrectly identified org . want entity recognition accurate reliable . fix issues ? additional spacy configurations , custom rules , pre-trained models use improve entity recognition ? note : tried chatgpt well , still issue solved ."
        ],
        [
         "31",
         "79283846",
         "--user-dir in Fairseq in failing",
         "<p>I’m trying to fine-tune the IndicTrans2 model using fairseq-train, but I keep encountering the following error:</p>\n<p>fairseq-train: error: argument --user-dir: invalid Optional value: 'C:/Users/sasid/Downloads/en-indic-exp/model_configs'</p>\n<p>I’ve provided the --user-dir argument as the path to the model_configs directory (e.g., --user-dir C:/Users/sasid/Downloads/IndicTrans2/model_configs), but the training script fails with the above error.</p>\n<p>I figured out that the problem is it's not taking path in windows convention with backslashes instead it is taking forward slashes so it is failing with that error.</p>\n<p>So how can I make it take the path in windows convention?</p>\n",
         "2024-12-16 07:03:15",
         "1",
         "30",
         "1",
         "<python><nlp><huggingface-transformers><large-language-model><fairseq>",
         null,
         null,
         "",
         "",
         "--user-dir in Fairseq in failing",
         "Im trying to fine-tune the IndicTrans2 model using fairseq-train, but I keep encountering the following error: fairseq-train: error: argument --user-dir: invalid Optional value: 'C:/Users/sasid/Downloads/en-indic-exp/model_configs' Ive provided the --user-dir argument as the path to the model_configs directory (e.g., --user-dir C:/Users/sasid/Downloads/IndicTrans2/model_configs), but the training script fails with the above error. I figured out that the problem is it's not taking path in windows convention with backslashes instead it is taking forward slashes so it is failing with that error. So how can I make it take the path in windows convention?",
         "",
         "--user-dir in Fairseq in failing Im trying to fine-tune the IndicTrans2 model using fairseq-train, but I keep encountering the following error: fairseq-train: error: argument --user-dir: invalid Optional value: 'C:/Users/sasid/Downloads/en-indic-exp/model_configs' Ive provided the --user-dir argument as the path to the model_configs directory (e.g., --user-dir C:/Users/sasid/Downloads/IndicTrans2/model_configs), but the training script fails with the above error. I figured out that the problem is it's not taking path in windows convention with backslashes instead it is taking forward slashes so it is failing with that error. So how can I make it take the path in windows convention? ",
         "-- user-dir fairseq failing im trying fine-tune indictrans2 model using fairseq-train , keep encountering following error : fairseq-train : error : argument -- user-dir : invalid optional value : ' c : /users/sasid/downloads/en-indic-exp/model_configs ' ive provided -- user-dir argument path model_configs directory ( e.g. , -- user-dir c : /users/sasid/downloads/indictrans2/model_configs ) , training script fails error . figured problem 's taking path windows convention backslashes instead taking forward slashes failing error . make take path windows convention ?"
        ],
        [
         "32",
         "79279045",
         "Recommending a pre-train NER model for geospatial entities",
         "<p>I am trying to find the best pre-trained Hugging Face Transformer model exclusively dedicated to geospatial or location entities to extract location entities in English from a text. Does it work way better than roberta-large?</p>\n",
         "2024-12-13 16:53:40",
         "1",
         "34",
         "1",
         "<nlp><geolocation><pipeline><geospatial><huggingface-transformers>",
         null,
         null,
         "",
         "",
         "Recommending a pre-train NER model for geospatial entities",
         "I am trying to find the best pre-trained Hugging Face Transformer model exclusively dedicated to geospatial or location entities to extract location entities in English from a text. Does it work way better than roberta-large?",
         "",
         "Recommending a pre-train NER model for geospatial entities I am trying to find the best pre-trained Hugging Face Transformer model exclusively dedicated to geospatial or location entities to extract location entities in English from a text. Does it work way better than roberta-large? ",
         "recommending pre-train ner model geospatial entities trying find best pre-trained hugging face transformer model exclusively dedicated geospatial location entities extract location entities english text . work way better roberta-large ?"
        ],
        [
         "33",
         "79274184",
         "How to split and spelling correct arabic text without spaces into list of words",
         "<p>I'm looking for a way to split the Arabic text and correct the spelling. Whitespace is the first splitting criterion, then, Maybe based on a dictionary of correct words the splitting should done, considering spelling issues:</p>\n<pre><code>تشرابالقطط الحليب =&gt; [تشرب، القطط، الحليب]\n\nمخمديوسف =&gt; [&quot;محمد&quot;, &quot;يوسف&quot;]\n\nانا ارييدان اشراب =&gt; [&quot;انا&quot;, &quot;أريد&quot;, &quot;أن&quot;, &quot;أشرب&quot;]\n\nجملة صحيحة =&gt; [&quot;جملة&quot;, &quot;صحيحة&quot;]\n</code></pre>\n<p>And if there are multiple correct splitting ways, return them all:</p>\n<pre><code>مخمديوسف =&gt; [ [&quot;محمد&quot;, &quot;يوسف&quot;] , [&quot;احمد&quot;, &quot;يوسف&quot;] ]\n</code></pre>\n<p>If any libraries can do the same. Otherwise, A custom algorithm/code that we can implement?</p>\n",
         "2024-12-12 07:34:49",
         "3",
         "69",
         "2",
         "<algorithm><deep-learning><nlp>",
         null,
         null,
         "تشرابالقطط الحليب => [تشرب، القطط، الحليب]\n\nمخمديوسف => [\"محمد\", \"يوسف\"]\n\nانا ارييدان اشراب => [\"انا\", \"أريد\", \"أن\", \"أشرب\"]\n\nجملة صحيحة => [\"جملة\", \"صحيحة\"]\n---\nمخمديوسف => [ [\"محمد\", \"يوسف\"] , [\"احمد\", \"يوسف\"] ]",
         "",
         "How to split and spelling correct arabic text without spaces into list of words",
         "I'm looking for a way to split the Arabic text and correct the spelling. Whitespace is the first splitting criterion, then, Maybe based on a dictionary of correct words the splitting should done, considering spelling issues: And if there are multiple correct splitting ways, return them all: If any libraries can do the same. Otherwise, A custom algorithm/code that we can implement?",
         "",
         "How to split and spelling correct arabic text without spaces into list of words I'm looking for a way to split the Arabic text and correct the spelling. Whitespace is the first splitting criterion, then, Maybe based on a dictionary of correct words the splitting should done, considering spelling issues: And if there are multiple correct splitting ways, return them all: If any libraries can do the same. Otherwise, A custom algorithm/code that we can implement? ",
         "split spelling correct arabic text without spaces list words 'm looking way split arabic text correct spelling . whitespace first splitting criterion , , maybe based dictionary correct words splitting done , considering spelling issues : multiple correct splitting ways , return : libraries . otherwise , custom algorithm/code implement ?"
        ],
        [
         "34",
         "79264247",
         "similarity from word to sentence after doing words Embedding",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did word2vec .</p>\n<p>Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word &quot;king&quot;.</p>\n<p>I thought about taking in each sentence the 4 closet words to the word king  and make average of them.\nmaybe by using <code>model.wv.similarity</code>.\nthe avg of each sentnce will be in the field df['king']</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n<p>example data:</p>\n<pre><code>    data = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\ndf['text']=df['text'].str.split()    \n\nmodel = Word2Vec(df['text'], vector_size=100, window=2, min_count=1 )\n\nmodel.wv.similarity('Knights','king')\n</code></pre>\n<p><strong>edit</strong>:</p>\n<p>My mission is:</p>\n<p>I have 1000 text rows (people that complain about something)\nI want to catalog them into 4 words.\nLets say that word 1 is king. Word 2 is castle…\nI want to know about each sentence which word from the  4 words most represent the sentence.\nIn order to do that I thought about taking each word from the 4 words and calculate <code>model.wv.similarity</code> to all of the words in  df['text'].\nAfter that, for each sentence, take the 3  words that have  the highest score to word king  (and to the word  castle and ets..)  .\ncalculate mean of the 3 highest score and that would be the value of df['king'] for the sentence</p>\n",
         "2024-12-09 08:14:04",
         "0",
         "64",
         "1",
         "<python><nlp><text-mining><word2vec><similarity>",
         null,
         null,
         "model.wv.similarity\n---\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\ndf['text']=df['text'].str.split()    \n\nmodel = Word2Vec(df['text'], vector_size=100, window=2, min_count=1 )\n\nmodel.wv.similarity('Knights','king')\n---\nmodel.wv.similarity",
         "",
         "similarity from word to sentence after doing words Embedding",
         "I have dataframe with 1000 text rows. I did word2vec . Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word \"king\". I thought about taking in each sentence the 4 closet words to the word king and make average of them. maybe by using . the avg of each sentnce will be in the field df'king' I will glad to know how to do that or to hear about another method. example data: edit : My mission is: I have 1000 text rows (people that complain about something) I want to catalog them into 4 words. Lets say that word 1 is king. Word 2 is castle I want to know about each sentence which word from the 4 words most represent the sentence. In order to do that I thought about taking each word from the 4 words and calculate to all of the words in df'text'. After that, for each sentence, take the 3 words that have the highest score to word king (and to the word castle and ets..) . calculate mean of the 3 highest score and that would be the value of df'king' for the sentence",
         "",
         "similarity from word to sentence after doing words Embedding I have dataframe with 1000 text rows. I did word2vec . Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word \"king\". I thought about taking in each sentence the 4 closet words to the word king and make average of them. maybe by using . the avg of each sentnce will be in the field df'king' I will glad to know how to do that or to hear about another method. example data: edit : My mission is: I have 1000 text rows (people that complain about something) I want to catalog them into 4 words. Lets say that word 1 is king. Word 2 is castle I want to know about each sentence which word from the 4 words most represent the sentence. In order to do that I thought about taking each word from the 4 words and calculate to all of the words in df'text'. After that, for each sentence, take the 3 words that have the highest score to word king (and to the word castle and ets..) . calculate mean of the 3 highest score and that would be the value of df'king' for the sentence ",
         "similarity word sentence words embedding dataframe 1000 text rows . word2vec . want create new field give distance sentence word want , lets say word `` king '' . thought taking sentence 4 closet words word king make average . maybe using . avg sentnce field df'king ' glad know hear another method . example data : edit : mission : 1000 text rows ( people complain something ) want catalog 4 words . lets say word 1 king . word 2 castle want know sentence word 4 words represent sentence . order thought taking word 4 words calculate words df'text ' . , sentence , take 3 words highest score word king ( word castle ets .. ) . calculate mean 3 highest score would value df'king ' sentence"
        ],
        [
         "35",
         "79260748",
         "How to install spacy?",
         "<p>I am using trying to install spacy library using 'pip install -U spacy' in the command prompt (run as admin) in Windows-11 O.S., but it shows some error I don't understand. I am using Python 3.13.0, gcc 13.2.0 and make 4.4.1. What could be the problem? Or is there any other way to install spacy?</p>\n<pre class=\"lang-none prettyprint-override\"><code>C:\\&gt;pip install -U spacy\nCollecting spacy\n  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n\n  × pip subprocess to install build dependencies did not run successfully.\n  │ exit code: 1\n  ╰─&gt; [113 lines of output]\n      Ignoring numpy: markers 'python_version &lt; &quot;3.9&quot;' don't match your environment\n      Collecting setuptools\n        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n      Collecting cython&lt;3.0,&gt;=0.25\n        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n      Collecting cymem&lt;2.1.0,&gt;=2.0.2\n        Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n      Collecting preshed&lt;3.1.0,&gt;=3.0.2\n        Using cached preshed-3.0.9.tar.gz (14 kB)\n        Installing build dependencies: started\n        Installing build dependencies: finished with status 'done'\n        Getting requirements to build wheel: started\n        Getting requirements to build wheel: finished with status 'done'\n        Preparing metadata (pyproject.toml): started\n        Preparing metadata (pyproject.toml): finished with status 'done'\n      Collecting murmurhash&lt;1.1.0,&gt;=0.28.0\n        Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n      Collecting thinc&lt;8.4.0,&gt;=8.3.0\n        Using cached thinc-8.3.2.tar.gz (193 kB)\n        Installing build dependencies: started\n        Installing build dependencies: still running...\n        Installing build dependencies: finished with status 'error'\n        error: subprocess-exited-with-error\n\n        pip subprocess to install build dependencies did not run successfully.\n        exit code: 1\n\n        [74 lines of output]\n        Ignoring numpy: markers 'python_version &lt; &quot;3.9&quot;' don't match your environment\n        Collecting setuptools\n          Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n        Collecting cython&lt;3.0,&gt;=0.25\n          Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n        Collecting murmurhash&lt;1.1.0,&gt;=1.0.2\n          Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n        Collecting cymem&lt;2.1.0,&gt;=2.0.2\n          Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n        Collecting preshed&lt;3.1.0,&gt;=3.0.2\n          Using cached preshed-3.0.9.tar.gz (14 kB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting blis&lt;1.1.0,&gt;=1.0.0\n          Using cached blis-1.0.1.tar.gz (3.6 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting numpy&lt;2.1.0,&gt;=2.0.0\n          Using cached numpy-2.0.2.tar.gz (18.9 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Installing backend dependencies: started\n          Installing backend dependencies: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'error'\n          error: subprocess-exited-with-error\n\n          Preparing metadata (pyproject.toml) did not run successfully.\n          exit code: 1\n\n          [22 lines of output]\n          + C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\vendored-meson\\meson\\meson.py setup C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-python-native-file.ini\n          The Meson build system\n          Version: 1.4.99\n          Source dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\n          Build dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\n          Build type: native build\n          Project name: NumPy\n          Project version: 2.0.2\n          C compiler for the host machine: gcc (gcc 13.2.0 &quot;gcc (GCC) 13.2.0&quot;)\n          C linker for the host machine: gcc ld.bfd 2.41\n          C++ compiler for the host machine: c++ (gcc 6.3.0 &quot;c++ (MinGW.org GCC-6.3.0-1) 6.3.0&quot;)\n          C++ linker for the host machine: c++ ld.bfd 2.28\n          Cython compiler for the host machine: cython (cython 3.0.11)\n          Host machine cpu family: x86\n          Host machine cpu: x86\n          Program python found: YES (C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe)\n          Need python for x86, but found x86_64\n          Run-time dependency python found: NO (tried sysconfig)\n\n          ..\\meson.build:41:12: ERROR: Python dependency not found\n\n          A full log can be found at C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-logs\\meson-log.txt\n          [end of output]\n\n          note: This error originates from a subprocess, and is likely not a problem with pip.\n        error: metadata-generation-failed\n\n        Encountered error while generating package metadata.\n\n        See above for output.\n\n        note: This is an issue with the package mentioned above, not pip.\n        hint: See above for details.\n        [end of output]\n\n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: subprocess-exited-with-error\n\n      pip subprocess to install build dependencies did not run successfully.\n      exit code: 1\n\n      See above for output.\n\n      note: This error originates from a subprocess, and is likely not a problem with pip.\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n× pip subprocess to install build dependencies did not run successfully.\n│ exit code: 1\n╰─&gt; See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n</code></pre>\n",
         "2024-12-07 14:07:54",
         "0",
         "718",
         "4",
         "<python><pip><nlp><spacy>",
         null,
         null,
         "C:\\>pip install -U spacy\nCollecting spacy\n  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n\n  × pip subprocess to install build dependencies did not run successfully.\n  │ exit code: 1\n  ╰─> [113 lines of output]\n      Ignoring numpy: markers 'python_version < \"3.9\"' don't match your environment\n      Collecting setuptools\n        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n      Collecting cython<3.0,>=0.25\n        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n      Collecting cymem<2.1.0,>=2.0.2\n        Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n      Collecting preshed<3.1.0,>=3.0.2\n        Using cached preshed-3.0.9.tar.gz (14 kB)\n        Installing build dependencies: started\n        Installing build dependencies: finished with status 'done'\n        Getting requirements to build wheel: started\n        Getting requirements to build wheel: finished with status 'done'\n        Preparing metadata (pyproject.toml): started\n        Preparing metadata (pyproject.toml): finished with status 'done'\n      Collecting murmurhash<1.1.0,>=0.28.0\n        Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n      Collecting thinc<8.4.0,>=8.3.0\n        Using cached thinc-8.3.2.tar.gz (193 kB)\n        Installing build dependencies: started\n        Installing build dependencies: still running...\n        Installing build dependencies: finished with status 'error'\n        error: subprocess-exited-with-error\n\n        pip subprocess to install build dependencies did not run successfully.\n        exit code: 1\n\n        [74 lines of output]\n        Ignoring numpy: markers 'python_version < \"3.9\"' don't match your environment\n        Collecting setuptools\n          Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n        Collecting cython<3.0,>=0.25\n          Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n        Collecting murmurhash<1.1.0,>=1.0.2\n          Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)\n        Collecting cymem<2.1.0,>=2.0.2\n          Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n        Collecting preshed<3.1.0,>=3.0.2\n          Using cached preshed-3.0.9.tar.gz (14 kB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting blis<1.1.0,>=1.0.0\n          Using cached blis-1.0.1.tar.gz (3.6 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'done'\n        Collecting numpy<2.1.0,>=2.0.0\n          Using cached numpy-2.0.2.tar.gz (18.9 MB)\n          Installing build dependencies: started\n          Installing build dependencies: finished with status 'done'\n          Getting requirements to build wheel: started\n          Getting requirements to build wheel: finished with status 'done'\n          Installing backend dependencies: started\n          Installing backend dependencies: finished with status 'done'\n          Preparing metadata (pyproject.toml): started\n          Preparing metadata (pyproject.toml): finished with status 'error'\n          error: subprocess-exited-with-error\n\n          Preparing metadata (pyproject.toml) did not run successfully.\n          exit code: 1\n\n          [22 lines of output]\n          + C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\vendored-meson\\meson\\meson.py setup C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-python-native-file.ini\n          The Meson build system\n          Version: 1.4.99\n          Source dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\n          Build dir: C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\n          Build type: native build\n          Project name: NumPy\n          Project version: 2.0.2\n          C compiler for the host machine: gcc (gcc 13.2.0 \"gcc (GCC) 13.2.0\")\n          C linker for the host machine: gcc ld.bfd 2.41\n          C++ compiler for the host machine: c++ (gcc 6.3.0 \"c++ (MinGW.org GCC-6.3.0-1) 6.3.0\")\n          C++ linker for the host machine: c++ ld.bfd 2.28\n          Cython compiler for the host machine: cython (cython 3.0.11)\n          Host machine cpu family: x86\n          Host machine cpu: x86\n          Program python found: YES (C:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe)\n          Need python for x86, but found x86_64\n          Run-time dependency python found: NO (tried sysconfig)\n\n          ..\\meson.build:41:12: ERROR: Python dependency not found\n\n          A full log can be found at C:\\Users\\rohan\\AppData\\Local\\Temp\\pip-install-s6zj7q4q\\numpy_fe36df85b8944a7fb67f6135b78a4bde\\.mesonpy-c4lb8p4h\\meson-logs\\meson-log.txt\n          [end of output]\n\n          note: This error originates from a subprocess, and is likely not a problem with pip.\n        error: metadata-generation-failed\n\n        Encountered error while generating package metadata.\n\n        See above for output.\n\n        note: This is an issue with the package mentioned above, not pip.\n        hint: See above for details.\n        [end of output]\n\n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: subprocess-exited-with-error\n\n      pip subprocess to install build dependencies did not run successfully.\n      exit code: 1\n\n      See above for output.\n\n      note: This error originates from a subprocess, and is likely not a problem with pip.\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n× pip subprocess to install build dependencies did not run successfully.\n│ exit code: 1\n╰─> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.",
         "",
         "How to install spacy?",
         "I am using trying to install spacy library using 'pip install -U spacy' in the command prompt (run as admin) in Windows-11 O.S., but it shows some error I don't understand. I am using Python 3.13.0, gcc 13.2.0 and make 4.4.1. What could be the problem? Or is there any other way to install spacy?",
         "",
         "How to install spacy? I am using trying to install spacy library using 'pip install -U spacy' in the command prompt (run as admin) in Windows-11 O.S., but it shows some error I don't understand. I am using Python 3.13.0, gcc 13.2.0 and make 4.4.1. What could be the problem? Or is there any other way to install spacy? ",
         "install spacy ? using trying install spacy library using 'pip install -u spacy ' command prompt ( run admin ) windows-11 o.s. , shows error n't understand . using python 3.13.0 , gcc 13.2.0 make 4.4.1. could problem ? way install spacy ?"
        ],
        [
         "36",
         "79256112",
         "How to log only the current script file to W&B code panel immediately?",
         "<p>How can I ensure that only the current script file (e.g., <code>train.py</code>) is logged to the W&amp;B Code panel when running a script, without logging the entire directory?</p>\n<p>Currently, I'm using:</p>\n<pre class=\"lang-py prettyprint-override\"><code>wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;)\n</code></pre>\n<p>I want to confirm if this approach works reliably across different environments and if there are better practices for this use case.</p>\n<p>Main part of the code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;) # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)\n    print(f'{run.get_url()=}')\n    wandb.finish()\n</code></pre>\n<p>All the code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\nfrom typing import Optional\nimport random\nimport torch\nfrom transformers import PushToHubCallback\nfrom transformers import get_cosine_schedule_with_warmup\nfrom trl import SFTConfig, SFTTrainer\nimport os\nimport fire\nimport wandb\nimport sys\n\nfrom train.callbacks import GenCallbackWithHFGenerate\nfrom train.data import load_math_style_dataset, print_first_example_after_decode\nimport train.models\n\nfrom train.utils import seed_everything\n\ndef main(**config):\n    # -- Seed everything\n    seed_everything(seed=config.get('seed', 0))\n    \n    # -- HF login\n    from huggingface_hub import login\n    token = open(os.path.expanduser(&quot;~/keys/master_hf_token.txt&quot;)).read().strip()\n    login(token=token)\n\n    # -- Get model\n    model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'google/gemma-2-2b')) \n    # model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'meta-llama/Llama-3.1-8B')) \n\n    # -- Load datasets\n    ds_name_or_path = config.get('ds_name_or_path', 'Putnam-AXIOM/putnam-axiom-dataset')\n    train_split, val_split = config.get('train_split', 'func_original_53_10_30_2024'), config.get('val_split', 'func_variations_265_11_23_2024')\n    print(f'\\n---&gt; {ds_name_or_path=} {train_split=} {val_split=}\\n')\n    train_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=1, split=train_split)\n    print_first_example_after_decode(train_dataset, tok)\n    # eval_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=15, split=val_split)\n    eval_dataset = train_dataset\n    print(f'{len(train_dataset)=}\\n{len(eval_dataset)=}')\n    wandb.config.update({'dataset': f'{ds_name_or_path} ({train_split=} {val_split=})'})\n\n    # -- Prepare output directory\n    today: str = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss')\n    output_dir: str = os.path.expanduser(f&quot;~/data/runs_logic_cont/run_{config.get('today', today)}&quot;)\n    print(f'{output_dir=}')\n    \n    # Save the initial model and tokenizer as checkpoint-0\n    initial_checkpoint_dir = os.path.join(output_dir, &quot;checkpoint-0&quot;)\n    os.makedirs(initial_checkpoint_dir, exist_ok=True)\n    print(f&quot;Saving initial checkpoint and tokenizer at {initial_checkpoint_dir}&quot;)\n    model.save_pretrained(initial_checkpoint_dir)\n    tok.save_pretrained(initial_checkpoint_dir)\n\n    # -- Train model\n    # max_steps = 50  # Limit fine-tuning to a few steps\n    # os.environ['CUDA_VISIBLE_DEVICES'] = str(random.randint(0, 7))\n    # config = {'max_steps': 2, 'eval_steps': 1, 'logging_steps': 1, \n    #           'save_strategy': 'steps', 'save_steps': 1, 'eval_strategy': 'steps'}\n    # config = config | {'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'maybe 0')}\n    training_args = SFTConfig(\n        max_steps=config.get('max_steps', 30),\n        # --\n        output_dir=output_dir,\n        bf16=torch.cuda.is_bf16_supported(),\n        fp16=not torch.cuda.is_bf16_supported(),\n        # -- logging opts\n        save_steps=config.get('save_steps', 5), \n        save_strategy=config.get('save_strategy', 'steps'),\n        eval_on_start=config.get('eval_on_start', True),\n        evaluation_strategy=config.get('eval_strategy', 'steps'), \n        eval_steps=config.get('eval_steps', 1), \n        logging_first_step=config.get('logging_first_step', True), # Default to False, unsure 100% what this does but looks like a good idea\n        logging_strategy=config.get('logging_strategy', 'steps'),\n        logging_steps=config.get('logging_steps', 1),\n        # --\n        num_train_epochs=config.get('num_train_epochs', 10),\n        max_seq_length=config.get('max_seq_length', 512),\n        per_device_train_batch_size=config.get('batch_size', 2),\n        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 2),\n    )\n    # Calculate Total Steps\n    steps_per_epoch = (len(train_dataset) // training_args.per_device_train_batch_size) // training_args.gradient_accumulation_steps\n    total_steps = steps_per_epoch * training_args.num_train_epochs\n    print(f'{steps_per_epoch=}')\n\n    # Optimizer and Scheduler\n    # optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 1e-4}]\n    optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.get('learning_rate', 1e-5))\n\n    # Add Cosine Learning Rate Scheduler\n    # warmup_steps = int(0.01 * total_steps)  # Warm-up for 1% of total steps\n    warmup_steps = 0\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps,\n    )\n    scheduler = None\n    print(f'{total_steps=} {warmup_steps=}')\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tok,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        args=training_args,\n        optimizers=(optimizer, scheduler),\n        callbacks=[GenCallbackWithHFGenerate(model, tok)]\n    )\n    print(f&quot;\\nStarting fine-tuning...&quot;)\n    trainer.train()\n    # - end run\n    return os.path.expanduser(output_dir)\n\ndef run_eval_logic_contamination(output_dir: str):\n    &quot;&quot;&quot;\n    Runs the eval_logic_contamination.py script with the specified output directory.\n\n    Args:\n        output_dir (str): The directory where the model is saved, expanded using `os.path.expanduser`.\n    &quot;&quot;&quot;\n    import gc\n    torch.cuda.empty_cache()\n    gc.collect()\n    output_dir = os.path.expanduser(output_dir)  # Ensure `output_dir` is expanded \n    from eval_logic_contamination import main\n    task='putnam_axiom_53'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # task='putnam_axiom_53' # for debugging\n    task='putnam_axiom_variations'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # wandb.run.define_metric(&quot;eval/accuracy&quot;, step_metric=&quot;eval/checkpoint_idx&quot;)\n    # wandb.run.define_metric(&quot;eval/checkpoint_idx&quot;) \n    # for idx, acc in [(10,5), (20,10), (30,15)]:\n    #     wandb.log({'eval/accuracy': acc, 'eval/checkpoint_idx': idx})\n\ndef _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;) # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)\n    print(f'{run.get_url()=}')\n    wandb.finish()\n\nif __name__ == &quot;__main__&quot;:\n    import time\n    start_time = time.time()\n    fire.Fire(_main)\n    print(f&quot;Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\\a&quot;)\n</code></pre>\n<p>Cross: <a href=\"https://community.wandb.ai/t/how-to-log-only-the-current-script-file-to-w-b-code-panel-immediately/8537\" rel=\"nofollow noreferrer\">https://community.wandb.ai/t/how-to-log-only-the-current-script-file-to-w-b-code-panel-immediately/8537</a></p>\n",
         "2024-12-05 20:11:21",
         "0",
         "18",
         "1",
         "<deep-learning><nlp><wandb>",
         null,
         null,
         "train.py\n---\nwandb.run.log_code(f\"./{os.path.basename(__file__)}\")\n---\ndef _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=\"putnam-axiom\", name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=\"putnam-axiom\", name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f\"./{os.path.basename(__file__)}\") # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=\"Run Completed\", text=f\"Run finished, run url: {run.get_url()}\")\n    print(f'{run.get_url()=}')\n    wandb.finish()\n---\nfrom datetime import datetime\nfrom typing import Optional\nimport random\nimport torch\nfrom transformers import PushToHubCallback\nfrom transformers import get_cosine_schedule_with_warmup\nfrom trl import SFTConfig, SFTTrainer\nimport os\nimport fire\nimport wandb\nimport sys\n\nfrom train.callbacks import GenCallbackWithHFGenerate\nfrom train.data import load_math_style_dataset, print_first_example_after_decode\nimport train.models\n\nfrom train.utils import seed_everything\n\ndef main(**config):\n    # -- Seed everything\n    seed_everything(seed=config.get('seed', 0))\n    \n    # -- HF login\n    from huggingface_hub import login\n    token = open(os.path.expanduser(\"~/keys/master_hf_token.txt\")).read().strip()\n    login(token=token)\n\n    # -- Get model\n    model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'google/gemma-2-2b')) \n    # model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'meta-llama/Llama-3.1-8B')) \n\n    # -- Load datasets\n    ds_name_or_path = config.get('ds_name_or_path', 'Putnam-AXIOM/putnam-axiom-dataset')\n    train_split, val_split = config.get('train_split', 'func_original_53_10_30_2024'), config.get('val_split', 'func_variations_265_11_23_2024')\n    print(f'\\n---> {ds_name_or_path=} {train_split=} {val_split=}\\n')\n    train_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=1, split=train_split)\n    print_first_example_after_decode(train_dataset, tok)\n    # eval_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=15, split=val_split)\n    eval_dataset = train_dataset\n    print(f'{len(train_dataset)=}\\n{len(eval_dataset)=}')\n    wandb.config.update({'dataset': f'{ds_name_or_path} ({train_split=} {val_split=})'})\n\n    # -- Prepare output directory\n    today: str = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss')\n    output_dir: str = os.path.expanduser(f\"~/data/runs_logic_cont/run_{config.get('today', today)}\")\n    print(f'{output_dir=}')\n    \n    # Save the initial model and tokenizer as checkpoint-0\n    initial_checkpoint_dir = os.path.join(output_dir, \"checkpoint-0\")\n    os.makedirs(initial_checkpoint_dir, exist_ok=True)\n    print(f\"Saving initial checkpoint and tokenizer at {initial_checkpoint_dir}\")\n    model.save_pretrained(initial_checkpoint_dir)\n    tok.save_pretrained(initial_checkpoint_dir)\n\n    # -- Train model\n    # max_steps = 50  # Limit fine-tuning to a few steps\n    # os.environ['CUDA_VISIBLE_DEVICES'] = str(random.randint(0, 7))\n    # config = {'max_steps': 2, 'eval_steps': 1, 'logging_steps': 1, \n    #           'save_strategy': 'steps', 'save_steps': 1, 'eval_strategy': 'steps'}\n    # config = config | {'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'maybe 0')}\n    training_args = SFTConfig(\n        max_steps=config.get('max_steps', 30),\n        # --\n        output_dir=output_dir,\n        bf16=torch.cuda.is_bf16_supported(),\n        fp16=not torch.cuda.is_bf16_supported(),\n        # -- logging opts\n        save_steps=config.get('save_steps', 5), \n        save_strategy=config.get('save_strategy', 'steps'),\n        eval_on_start=config.get('eval_on_start', True),\n        evaluation_strategy=config.get('eval_strategy', 'steps'), \n        eval_steps=config.get('eval_steps', 1), \n        logging_first_step=config.get('logging_first_step', True), # Default to False, unsure 100% what this does but looks like a good idea\n        logging_strategy=config.get('logging_strategy', 'steps'),\n        logging_steps=config.get('logging_steps', 1),\n        # --\n        num_train_epochs=config.get('num_train_epochs', 10),\n        max_seq_length=config.get('max_seq_length', 512),\n        per_device_train_batch_size=config.get('batch_size', 2),\n        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 2),\n    )\n    # Calculate Total Steps\n    steps_per_epoch = (len(train_dataset) // training_args.per_device_train_batch_size) // training_args.gradient_accumulation_steps\n    total_steps = steps_per_epoch * training_args.num_train_epochs\n    print(f'{steps_per_epoch=}')\n\n    # Optimizer and Scheduler\n    # optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 1e-4}]\n    optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.get('learning_rate', 1e-5))\n\n    # Add Cosine Learning Rate Scheduler\n    # warmup_steps = int(0.01 * total_steps)  # Warm-up for 1% of total steps\n    warmup_steps = 0\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps,\n    )\n    scheduler = None\n    print(f'{total_steps=} {warmup_steps=}')\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tok,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        args=training_args,\n        optimizers=(optimizer, scheduler),\n        callbacks=[GenCallbackWithHFGenerate(model, tok)]\n    )\n    print(f\"\\nStarting fine-tuning...\")\n    trainer.train()\n    # - end run\n    return os.path.expanduser(output_dir)\n\ndef run_eval_logic_contamination(output_dir: str):\n    \"\"\"\n    Runs the eval_logic_contamination.py script with the specified output directory.\n\n    Args:\n        output_dir (str): The directory where the model is saved, expanded using `os.path.expanduser`.\n    \"\"\"\n    import gc\n    torch.cuda.empty_cache()\n    gc.collect()\n    output_dir = os.path.expanduser(output_dir)  # Ensure `output_dir` is expanded \n    from eval_logic_contamination import main\n    task='putnam_axiom_53'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # task='putnam_axiom_53' # for debugging\n    task='putnam_axiom_variations'\n    res: dict = main(model_name_or_path=output_dir, task=task)\n    print(f'Results for {task=}: {res}')\n    print(res)\n    # wandb.run.define_metric(\"eval/accuracy\", step_metric=\"eval/checkpoint_idx\")\n    # wandb.run.define_metric(\"eval/checkpoint_idx\") \n    # for idx, acc in [(10,5), (20,10), (30,15)]:\n    #     wandb.log({'eval/accuracy': acc, 'eval/checkpoint_idx': idx})\n\ndef _main(**kwargs):\n    from datetime import datetime\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    run_name = f'{today}' \n    kwargs = kwargs | {'today': today}\n    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=\"putnam-axiom\", name=run_name, save_code=True, config=kwargs)\n    run = wandb.init(mode=kwargs.get('mode', 'online'), project=\"putnam-axiom\", name=run_name, save_code=True, config=kwargs)\n    wandb.run.log_code(f\"./{os.path.basename(__file__)}\") # maybe logscode immediately\n    # wandb.config.update()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)\n    output_dir = main(**kwargs)\n    run_eval_logic_contamination(output_dir)\n    # from train.utils import copy_to_dfs\n    # copy_to_dfs(output_dir)\n    run.alert(title=\"Run Completed\", text=f\"Run finished, run url: {run.get_url()}\")\n    print(f'{run.get_url()=}')\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    import time\n    start_time = time.time()\n    fire.Fire(_main)\n    print(f\"Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\\a\")",
         "",
         "How to log only the current script file to W&B code panel immediately?",
         "How can I ensure that only the current script file (e.g., ) is logged to the W&B Code panel when running a script, without logging the entire directory? Currently, I'm using: I want to confirm if this approach works reliably across different environments and if there are better practices for this use case. Main part of the code: All the code: Cross:",
         "",
         "How to log only the current script file to W&B code panel immediately? How can I ensure that only the current script file (e.g., ) is logged to the W&B Code panel when running a script, without logging the entire directory? Currently, I'm using: I want to confirm if this approach works reliably across different environments and if there are better practices for this use case. Main part of the code: All the code: Cross: ",
         "log current script file w & b code panel immediately ? ensure current script file ( e.g. , ) logged w & b code panel running script , without logging entire directory ? currently , 'm using : want confirm approach works reliably across different environments better practices use case . main part code : code : cross :"
        ],
        [
         "37",
         "79253283",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>\n<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>\n<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n</code></pre>\n<p>A small Example -</p>\n<pre><code>text_dict = {\n    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]\n}\n\ndf = pd.DataFrame(text_dict)\n</code></pre>\n<p>My expected outcome is -</p>\n<pre><code>                 ITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n</code></pre>\n<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>\n<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>\n",
         "2024-12-05 03:05:06",
         "0",
         "81",
         "1",
         "<python><pandas><nlp>",
         "79263000.0",
         "<p>You need to process each row of text by identifying occurrences of <code>key_words</code> and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique <code>search_word</code> found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique <code>search_words</code> near <code>key_words</code>.</p>\n<pre><code>import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        &quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process &quot;\n        &quot;of adopting Symantec. We believe these technologies will improve our access control. &quot;\n        &quot;Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = &quot; &quot;.join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n</code></pre>\n<p>Which returns</p>\n<pre><code>                                               ITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2\n</code></pre>\n",
         "search words\n---\nkey words\n---\nsearch words\n---\nkey words\n---\nkey words\n---\nsearch words\n---\nsearch word\n---\nkey word\n---\nsearch_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n---\ntext_dict = {\n    'ITEM7':[\"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"]\n}\n\ndf = pd.DataFrame(text_dict)\n---\nITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n---\ndf\n---\nAVG\n---\nAlibaba Security\n---\nsearch_words\n---\nkey_words\n---\nSymantec\n---\nAccess Control\n---\ndata security\n---\ndiagnostic program\n---\nsearch_words\n---\nadopting\n---\nimprove\n---\nintegrated\n---\nkey_words\n---\nFrequency\n---\ndf\n---\nkey_words",
         "key_words\n---\nsearch_word\n---\nsearch_words\n---\nkey_words\n---\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        \"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \"\n        \"of adopting Symantec. We believe these technologies will improve our access control. \"\n        \"Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = \" \".join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n---\nITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "I have two sets of word lists - first one I called and the second one I called . My goal is to calculate the frequency of within 10 words of . For example, assume that the word - acquire - is in list, then I will look for the words in list within 10 words of acquire . Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement. Below is my and lists - A small Example - My expected outcome is - For the first row in , we see the word and are from list and around the word acquired , the base form of which - acquire - is in the list. Similarly, , , , are from list and these words are within 10 words of , , from list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the column of , the value is 6. Please note that the words in are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.",
         "You need to process each row of text by identifying occurrences of and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique near . Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists - first one I called and the second one I called . My goal is to calculate the frequency of within 10 words of . For example, assume that the word - acquire - is in list, then I will look for the words in list within 10 words of acquire . Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement. Below is my and lists - A small Example - My expected outcome is - For the first row in , we see the word and are from list and around the word acquired , the base form of which - acquire - is in the list. Similarly, , , , are from list and these words are within 10 words of , , from list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the column of , the value is 6. Please note that the words in are in basically base form, so their variation (like adopted, adopting) should be counted as key words also. You need to process each row of text by identifying occurrences of and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique near . Which returns",
         "counting frequency words within key words text two sets word lists - first one called second one called . goal calculate frequency within 10 words . example , assume word - acquire - list , look words list within 10 words acquire . within 10 words mean , 10 words forward key words 10 words backward key words , meaning forward backward movement . lists - small example - expected outcome - first row , see word list around word acquired , base form - acquire - list . similarly , , , , list words within 10 words , , list . , total search words 6 ( avg+alibaba security+symantec+access control+data security+diagnostic program ) . therefore , column , value 6. please note words basically base form , variation ( like adopted , adopting ) counted key words also . need process row text identifying occurrences capturing 10-word window around . within window , need check multi-word search_words , ensuring matched phrases . unique found within windows needs counted , avoiding double-counting across row . stored results frequency count row , accurately reflecting number unique near . returns"
        ],
        [
         "38",
         "79247672",
         "Error in getting Captum text explanations for text classification",
         "<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>\n<pre><code>import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f&quot;Test Accuracy: {accuracy:.2f}&quot;)\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n<p>But I am getting the following error in running the above.</p>\n<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n</code></pre>\n",
         "2024-12-03 12:47:45",
         "2",
         "73",
         "1",
         "<machine-learning><pytorch><nlp><huggingface-transformers><text-classification>",
         "79248379.0",
         "<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>\n<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=\"https://captum.ai/tutorials/Bert_SQUAD_Interpret\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>\n<p>Below please find snippet that works:</p>\n<pre><code>from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n",
         "import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)\n---\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-f047b509c98d> in <cell line: 90>()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
         "from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    # print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)",
         "Error in getting Captum text explanations for text classification",
         "I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above.",
         "You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly. I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial Below please find snippet that works:",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above. You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly. I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial Below please find snippet that works:",
         "error getting captum text explanations text classification following code using identify influential words used correctly predict text test dataset getting following error running . need slightly change gradients calculation class . also , n't include forward_func gradients class constructor , attribute method able launch stuff properly . think using layerintegratedgradients better debugging bert - line tutorial please find snippet works :"
        ],
        [
         "39",
         "79247594",
         "euclidian distance from word to sentence after doing Vectorizer",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did TfidfVectorizer.</p>\n<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>\n<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n",
         "2024-12-03 12:25:05",
         "1",
         "43",
         "1",
         "<pandas><dataframe><nlp><text-classification><tf-idf>",
         "79248087.0",
         "<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([&quot;king&quot;]).toarray()\nexcept KeyError:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n</code></pre>\n<p>which would give you</p>\n<pre><code>                                                text  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0  \n</code></pre>\n<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif &quot;king&quot; in feature_names:\n    king_index = np.where(feature_names == &quot;king&quot;)[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n\n</code></pre>\n<p>which gives</p>\n<pre><code>                                                text  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214\n</code></pre>\n",
         "",
         "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([\"king\"]).toarray()\nexcept KeyError:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n---\ntext  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0\n---\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif \"king\" in feature_names:\n    king_index = np.where(feature_names == \"king\")[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n---\ntext  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214",
         "euclidian distance from word to sentence after doing Vectorizer",
         "I have dataframe with 1000 text rows. I did TfidfVectorizer. Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word \"king\". df'king' I thought about taking in each sentence the 5 closet words to the word king and make average of them. I will glad to know how to do that or to hear about another method.",
         "I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores: which would give you That being said, if you want to focus on Euclidean distance, here is a method: which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows. I did TfidfVectorizer. Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word \"king\". df'king' I thought about taking in each sentence the 5 closet words to the word king and make average of them. I will glad to know how to do that or to hear about another method. I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores: which would give you That being said, if you want to focus on Euclidean distance, here is a method: which gives",
         "euclidian distance word sentence vectorizer dataframe 1000 text rows . tfidfvectorizer . want create new field give distance sentence word want , lets say word `` king '' . df'king ' thought taking sentence 5 closet words word king make average . glad know hear another method . convinced euclidean distance would optimal measure . would actually look similarity scores : would give said , want focus euclidean distance , method : gives"
        ],
        [
         "40",
         "79234004",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "<p>I want to use <code>Llama-3.2-1B-Instruct</code> model, and although I have set <code>&quot;temperature&quot;: 0.0, &quot;top_p&quot;:0.0 and &quot;top_k&quot;:0</code>, it still generates inconsistent output. This is how my pipeline looks like:</p>\n<pre><code>pipe = pipeline(\n    &quot;text-generation&quot;,\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=&quot;mps&quot;,\n        model_kwargs={&quot;temperature&quot;: 0.0,\n                  &quot;do_sample&quot;:True,\n                              &quot;top_p&quot;:0.0,\n                              &quot;top_k&quot;:0,},\n)\n</code></pre>\n<p>Any idea how to solve this issue?</p>\n",
         "2024-11-28 13:02:37",
         "1",
         "532",
         "2",
         "<python><nlp><huggingface-transformers><large-language-model>",
         "79246602.0",
         "<p>The model inconsistent output can be due to two main factors:</p>\n<p><strong>1. Temperature:</strong></p>\n<p>setting temperature to zero give more inconsistent result. You can refer <a href=\"https://community.openai.com/t/why-the-api-output-is-inconsistent-even-after-the-temperature-is-set-to-0/329541/2\" rel=\"nofollow noreferrer\">Opeani discussion page</a> for detail.</p>\n<p>So the best option is to set temperature to very low values such as 0.00001 instead of zero.</p>\n<p><strong>2. do_sample</strong></p>\n<p>You already set it false, and it should remain that way only.</p>\n",
         "Llama-3.2-1B-Instruct\n---\n\"temperature\": 0.0, \"top_p\":0.0 and \"top_k\":0\n---\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"mps\",\n        model_kwargs={\"temperature\": 0.0,\n                  \"do_sample\":True,\n                              \"top_p\":0.0,\n                              \"top_k\":0,},\n)",
         "",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "I want to use model, and although I have set , it still generates inconsistent output. This is how my pipeline looks like: Any idea how to solve this issue?",
         "The model inconsistent output can be due to two main factors: 1. Temperature: setting temperature to zero give more inconsistent result. You can refer Opeani discussion page for detail. So the best option is to set temperature to low values such as 0.00001 instead of zero. 2. do_sample You already set it false, and it should remain that way only.",
         "Llama-3.2-1B-Instruct generate inconsistent output I want to use model, and although I have set , it still generates inconsistent output. This is how my pipeline looks like: Any idea how to solve this issue? The model inconsistent output can be due to two main factors: 1. Temperature: setting temperature to zero give more inconsistent result. You can refer Opeani discussion page for detail. So the best option is to set temperature to low values such as 0.00001 instead of zero. 2. do_sample You already set it false, and it should remain that way only.",
         "llama-3.2-1b-instruct generate inconsistent output want use model , although set , still generates inconsistent output . pipeline looks like : idea solve issue ? model inconsistent output due two main factors : 1. temperature : setting temperature zero give inconsistent result . refer opeani discussion page detail . best option set temperature low values 0.00001 instead zero . 2. do_sample already set false , remain way ."
        ],
        [
         "41",
         "79227390",
         "How to extract specific entities from unstructured text",
         "<p>Given a generic text sentence (in a specific context) how can I extract word/entities of interest belonging to a specific &quot;category&quot; using python and any NLP library?</p>\n<p>For example given a step for a culinary recipe <code>Add an onion to a bowl of carrots</code> as input text, I'd like to retrive <code>onion</code> and <code>carrots</code> while given <code>Sprinkle with paprika.</code> should return <code>paprika</code>.\nBut this should also work with sentences like <code>stir well, and cook an additional minute.</code> that do not contain any food entity in them.</p>\n<p>So far, what I was able to achieve is using the <code>spacy</code> library for training a NER module to parse sentences. The problem with the NER pipeline is that it is a rule-based parsing, it is trained providing a set of sentences and entities/matches/labels to learn, which works fine as expeted on sentences similar to the one used during train, but performs bad on new and different sentences:</p>\n<pre class=\"lang-py prettyprint-override\"><code>nlp = spacy.load('trained_model')\n\ndocument = nlp('Add flour, mustard, and salt')\n[(ent.text, ent.label_) for ent in document.ents]\n# &gt;&gt; [('Add flour', 'FOOD'), ('mustard', 'FOOD'), ('salt', 'FOOD')]\n# (quite) correct output\n\ndocument = nlp('I took a building, car and squirrel on the weekend')\n[(ent.text, ent.label_) for ent in document.ents]\n# &gt;&gt; [('building', 'FOOD'), ('car', 'FOOD'), ('squirrel', 'FOOD')]\n# wrong output\n\ndocument = nlp('stir well, and cook an additional minute.')\n[(ent.text, ent.label_) for ent in document.ents]\n# &gt;&gt; [('stir well', 'FOOD'), ('cook', 'FOOD'), ('additional minute.', 'FOOD')]\n# wrong output\n</code></pre>\n<p>I am aware that there are several similar questions and posts, but I have found only solutions working for &quot;semi-structured&quot; text, i.e. list of ingredients as <code>1 tsp. of sugar, 1 cup of milk, ...</code> which can be easily solved using the previous rule-based approach. Also <code>nltk</code> and part-of-speech (POS) are an option, but I'd prefer an alternative solution rather than having to compare each noun with an exhaustive list of foods.</p>\n<p>What instead I am looking for is a way of to extract specific entities or at least to classify words in generic text with additional categories beyond those of the basic parsing.\nWhich methods should I use/look at to achieve this?</p>\n",
         "2024-11-26 15:46:48",
         "1",
         "92",
         "1",
         "<python><machine-learning><nlp><nltk><spacy>",
         null,
         null,
         "Add an onion to a bowl of carrots\n---\nonion\n---\ncarrots\n---\nSprinkle with paprika.\n---\npaprika\n---\nstir well, and cook an additional minute.\n---\nspacy\n---\nnlp = spacy.load('trained_model')\n\ndocument = nlp('Add flour, mustard, and salt')\n[(ent.text, ent.label_) for ent in document.ents]\n# >> [('Add flour', 'FOOD'), ('mustard', 'FOOD'), ('salt', 'FOOD')]\n# (quite) correct output\n\ndocument = nlp('I took a building, car and squirrel on the weekend')\n[(ent.text, ent.label_) for ent in document.ents]\n# >> [('building', 'FOOD'), ('car', 'FOOD'), ('squirrel', 'FOOD')]\n# wrong output\n\ndocument = nlp('stir well, and cook an additional minute.')\n[(ent.text, ent.label_) for ent in document.ents]\n# >> [('stir well', 'FOOD'), ('cook', 'FOOD'), ('additional minute.', 'FOOD')]\n# wrong output\n---\n1 tsp. of sugar, 1 cup of milk, ...\n---\nnltk",
         "",
         "How to extract specific entities from unstructured text",
         "Given a generic text sentence (in a specific context) how can I extract word/entities of interest belonging to a specific \"category\" using python and any NLP library? For example given a step for a culinary recipe as input text, I'd like to retrive and while given should return . But this should also work with sentences like that do not contain any food entity in them. So far, what I was able to achieve is using the library for training a NER module to parse sentences. The problem with the NER pipeline is that it is a rule-based parsing, it is trained providing a set of sentences and entities/matches/labels to learn, which works fine as expeted on sentences similar to the one used during train, but performs bad on new and different sentences: I am aware that there are several similar questions and posts, but I have found only solutions working for \"semi-structured\" text, i.e. list of ingredients as which can be easily solved using the previous rule-based approach. Also and part-of-speech (POS) are an option, but I'd prefer an alternative solution rather than having to compare each noun with an exhaustive list of foods. What instead I am looking for is a way of to extract specific entities or at least to classify words in generic text with additional categories beyond those of the basic parsing. Which methods should I use/look at to achieve this?",
         "",
         "How to extract specific entities from unstructured text Given a generic text sentence (in a specific context) how can I extract word/entities of interest belonging to a specific \"category\" using python and any NLP library? For example given a step for a culinary recipe as input text, I'd like to retrive and while given should return . But this should also work with sentences like that do not contain any food entity in them. So far, what I was able to achieve is using the library for training a NER module to parse sentences. The problem with the NER pipeline is that it is a rule-based parsing, it is trained providing a set of sentences and entities/matches/labels to learn, which works fine as expeted on sentences similar to the one used during train, but performs bad on new and different sentences: I am aware that there are several similar questions and posts, but I have found only solutions working for \"semi-structured\" text, i.e. list of ingredients as which can be easily solved using the previous rule-based approach. Also and part-of-speech (POS) are an option, but I'd prefer an alternative solution rather than having to compare each noun with an exhaustive list of foods. What instead I am looking for is a way of to extract specific entities or at least to classify words in generic text with additional categories beyond those of the basic parsing. Which methods should I use/look at to achieve this? ",
         "extract specific entities unstructured text given generic text sentence ( specific context ) extract word/entities interest belonging specific `` category '' using python nlp library ? example given step culinary recipe input text , 'd like retrive given return . also work sentences like contain food entity . far , able achieve using library training ner module parse sentences . problem ner pipeline rule-based parsing , trained providing set sentences entities/matches/labels learn , works fine expeted sentences similar one used train , performs bad new different sentences : aware several similar questions posts , found solutions working `` semi-structured '' text , i.e . list ingredients easily solved using previous rule-based approach . also part-of-speech ( pos ) option , 'd prefer alternative solution rather compare noun exhaustive list foods . instead looking way extract specific entities least classify words generic text additional categories beyond basic parsing . methods use/look achieve ?"
        ],
        [
         "42",
         "79202614",
         "Understanding byte-pair encoding tokenization for Greek characters",
         "<p>I am trying to train a new tokenizer with Greek text to later add the new tokens into the Llama 3.1 tokenizer using</p>\n<pre class=\"lang-py prettyprint-override\"><code>tokenizer.add_tokens(list(new_tokens)).\n</code></pre>\n<p>However, upon training the byte-pair encoding tokenizer on Greek and Spanish text, the result looks something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>\\['Translate', 'Ġfrom', 'ĠGreek', 'Ġto', 'ĠSpanish', ':', 'ĠÎĿÎ±', 'ĠÎŃÏĥÎ¹', 'ĠÎ¿Î³Î¯', 'ĠÎ³ÎŃÏģÎ¿Ïħ'\\]\n</code></pre>\n<p>When extending the token vocabulary in the tokenizer, it seems that those encoded tokens are being passed literally, not as encodings of Greek characters, and they are not recognized by the tokenizer to encode a sentence. However, when using the same method and new tokens are hardcoded, such as in</p>\n<pre class=\"lang-py prettyprint-override\"><code>extender_tokenizer.add_tokens(['Αυτό', 'είναι'])\n</code></pre>\n<p>it does work.</p>\n<p>I assume this is an encoding issue or it is related to BPE inner workings.\nWhy are Greek characters shown that way? Is it related to encoding, BPE or both? How to obtain a list of Greek character tokens that can be added to the tokenizer?</p>\n<p>Reference code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n\ntokenizer = Tokenizer(models.BPE())\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\ntrainer = trainers.BpeTrainer(vocab_size = 2000, min_frequency = 3, show_progress = True)\ntokenizer.train_from_iterator(training_corpus, trainer = trainer)\n</code></pre>\n",
         "2024-11-19 08:31:26",
         "0",
         "74",
         "1",
         "<encoding><nlp><tokenize><byte-pair-encoding>",
         null,
         null,
         "tokenizer.add_tokens(list(new_tokens)).\n---\n\\['Translate', 'Ġfrom', 'ĠGreek', 'Ġto', 'ĠSpanish', ':', 'ĠÎĿÎ±', 'ĠÎŃÏĥÎ¹', 'ĠÎ¿Î³Î¯', 'ĠÎ³ÎŃÏģÎ¿Ïħ'\\]\n---\nextender_tokenizer.add_tokens(['Αυτό', 'είναι'])\n---\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers\n\ntokenizer = Tokenizer(models.BPE())\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\ntrainer = trainers.BpeTrainer(vocab_size = 2000, min_frequency = 3, show_progress = True)\ntokenizer.train_from_iterator(training_corpus, trainer = trainer)",
         "",
         "Understanding byte-pair encoding tokenization for Greek characters",
         "I am trying to train a new tokenizer with Greek text to later add the new tokens into the Llama 3.1 tokenizer using However, upon training the byte-pair encoding tokenizer on Greek and Spanish text, the result looks something like this: When extending the token vocabulary in the tokenizer, it seems that those encoded tokens are being passed literally, not as encodings of Greek characters, and they are not recognized by the tokenizer to encode a sentence. However, when using the same method and new tokens are hardcoded, such as in it does work. I assume this is an encoding issue or it is related to BPE inner workings. Why are Greek characters shown that way? Is it related to encoding, BPE or both? How to obtain a list of Greek character tokens that can be added to the tokenizer? Reference code:",
         "",
         "Understanding byte-pair encoding tokenization for Greek characters I am trying to train a new tokenizer with Greek text to later add the new tokens into the Llama 3.1 tokenizer using However, upon training the byte-pair encoding tokenizer on Greek and Spanish text, the result looks something like this: When extending the token vocabulary in the tokenizer, it seems that those encoded tokens are being passed literally, not as encodings of Greek characters, and they are not recognized by the tokenizer to encode a sentence. However, when using the same method and new tokens are hardcoded, such as in it does work. I assume this is an encoding issue or it is related to BPE inner workings. Why are Greek characters shown that way? Is it related to encoding, BPE or both? How to obtain a list of Greek character tokens that can be added to the tokenizer? Reference code: ",
         "understanding byte-pair encoding tokenization greek characters trying train new tokenizer greek text later add new tokens llama 3.1 tokenizer using however , upon training byte-pair encoding tokenizer greek spanish text , result looks something like : extending token vocabulary tokenizer , seems encoded tokens passed literally , encodings greek characters , recognized tokenizer encode sentence . however , using method new tokens hardcoded , work . assume encoding issue related bpe inner workings . greek characters shown way ? related encoding , bpe ? obtain list greek character tokens added tokenizer ? reference code :"
        ],
        [
         "43",
         "79192130",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>\n<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>\n",
         "2024-11-15 11:13:36",
         "1",
         "52",
         "2",
         "<python><amazon-web-services><aws-lambda><nlp><large-language-model>",
         "79192427.0",
         "<p>In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:</p>\n<ol>\n<li>Package your KeyBERT script in a lambda and easily deploy it with a container.</li>\n<li>Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.</li>\n</ol>\n",
         "",
         "",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords. Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.",
         "In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do: Package your KeyBERT script in a lambda and easily deploy it with a container. Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so its cost-efficient for occasional tasks.",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT? I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords. Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks. In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do: Package your KeyBERT script in a lambda and easily deploy it with a container. Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so its cost-efficient for occasional tasks.",
         "using aws service execute python script extract keywords text using keybert ? simple python script given two blocks text , extracts keywords using keybert , compares lists keywords sort two lists depending lists share keywords . aws service would best fit needs ? want able esentially spin needed , give blocks text , execute return results , n't want integrate projects n't use python . 've attempted use lambda 'm concerned potential cost running . thanks . cases , would normally think two resources aligned best practices aws software engineering . sagemaker lambda . model 'm using resource-intensive requires gpu acceleration 'd go sagemaker otherwise lambda good solution . case , 's 'd : package keybert script lambda easily deploy container . invoke whenever need process text blocks . aws lambda charges execution time , cost-efficient occasional tasks ."
        ],
        [
         "44",
         "79190601",
         "Pyspark sentiment analysis invalid output",
         "<p>I am trying to perform sentiment analysis for a use case. Most of the time, it is giving correct results, but in some cases, even positive comments are being marked as negative. How can I fix my code to achieve better accuracy?</p>\n<p>My code</p>\n<pre><code>from pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import StringType\nfrom transformers import pipeline\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Define the filter_stopwords function\ndef filter_stopwords(sentence):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(sentence)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n    return &quot; &quot;.join(filtered_sentence)\n\n# Initialize the sentiment analysis pipeline with a different model\nsentiment_pipeline = pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)\n\n# Define a function to get sentiment using the pipeline\ndef get_sentiment(text):\n    filtered_text = filter_stopwords(text)\n    result = sentiment_pipeline(filtered_text)[0]\n    return result['label'].lower()  # returns 'positive', 'negative', etc.\n\n# Register the function as a UDF\nsentiment_udf = udf(get_sentiment, StringType())\n\n# df = df.withColumn(&quot;sentiment&quot;, sentiment_udf(col(&quot;text_column&quot;)))\n</code></pre>\n<p>Input data</p>\n<ol>\n<li><p>Didn't get it right the first 2 times but when it was fixed it was fixed well.</p>\n</li>\n<li><p>The Response time was grate -- <strong>Note</strong> Looks like spelling mistake but his review is positive</p>\n</li>\n<li><p>The initial agent contact could not resolve my issue but escalated it quickly to someone who could.</p>\n</li>\n</ol>\n<p>for this inputs i am expecting all should be <strong>positive</strong> instead i am getting <strong>negative</strong></p>\n",
         "2024-11-14 22:24:54",
         "3",
         "60",
         "2",
         "<pyspark><nlp><nltk><huggingface-transformers>",
         null,
         null,
         "from pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import StringType\nfrom transformers import pipeline\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Define the filter_stopwords function\ndef filter_stopwords(sentence):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(sentence)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n    return \" \".join(filtered_sentence)\n\n# Initialize the sentiment analysis pipeline with a different model\nsentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n# Define a function to get sentiment using the pipeline\ndef get_sentiment(text):\n    filtered_text = filter_stopwords(text)\n    result = sentiment_pipeline(filtered_text)[0]\n    return result['label'].lower()  # returns 'positive', 'negative', etc.\n\n# Register the function as a UDF\nsentiment_udf = udf(get_sentiment, StringType())\n\n# df = df.withColumn(\"sentiment\", sentiment_udf(col(\"text_column\")))",
         "",
         "Pyspark sentiment analysis invalid output",
         "I am trying to perform sentiment analysis for a use case. Most of the time, it is giving correct results, but in some cases, even positive comments are being marked as negative. How can I fix my code to achieve better accuracy? My code Input data Didn't get it right the first 2 times but when it was fixed it was fixed well. The Response time was grate -- Note Looks like spelling mistake but his review is positive The initial agent contact could not resolve my issue but escalated it quickly to someone who could. for this inputs i am expecting all should be positive instead i am getting negative",
         "",
         "Pyspark sentiment analysis invalid output I am trying to perform sentiment analysis for a use case. Most of the time, it is giving correct results, but in some cases, even positive comments are being marked as negative. How can I fix my code to achieve better accuracy? My code Input data Didn't get it right the first 2 times but when it was fixed it was fixed well. The Response time was grate -- Note Looks like spelling mistake but his review is positive The initial agent contact could not resolve my issue but escalated it quickly to someone who could. for this inputs i am expecting all should be positive instead i am getting negative ",
         "pyspark sentiment analysis invalid output trying perform sentiment analysis use case . time , giving correct results , cases , even positive comments marked negative . fix code achieve better accuracy ? code input data n't get right first 2 times fixed fixed well . response time grate -- note looks like spelling mistake review positive initial agent contact could resolve issue escalated quickly someone could . inputs expecting positive instead getting negative"
        ],
        [
         "45",
         "79180131",
         "How to parse a resume with few shot method using the specified models from HuggingFace and Langchain?",
         "<h1>Model selection confusion and some errors while trying to parse a resume with the following codes</h1>\n<ul>\n<li>Trying to do a few shot prompting with a google flan t5 base model</li>\n<li>While Doing so I am getting an error\n<code>ERROR:Service.service:Error parsing resume: &quot;'ContactInformation'&quot;</code></li>\n<li>exception as <code>&quot;detail&quot;: &quot;Failed to parse the file: An error occurred in parsed_resume: \\&quot;'ContactInformation'\\&quot;&quot;</code></li>\n</ul>\n<h2>The code is given below</h2>\n<pre class=\"lang-none prettyprint-override\"><code>from typing import List, Optional, Union, Any, re\nimport json\n\n\n\nclass ContactInformation(BaseModel):\n    Name: Optional[str] = None\n    Email: Optional[str] = None\n    Contact: Optional[str] = None\n    Links: Optional[List[str]] = None\n\n\n\nclass Experience(BaseModel):\n    title: Optional[str] = None\n    company: Optional[str] = None\n    duration: Optional[str] = None\n\n\nclass Education(BaseModel):\n    course: Optional[str] = None\n    branch: Optional[str] = None\n    institute: Optional[str] = None\n\n\nclass Projects(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n    link: Optional[str] = None\n\nclass OutputFormat(BaseModel):\n    ContactInformation: Optional[Any] = None\n    AboutMe: Optional[Any] = None\n    Experiences: Optional[List[Any]] = None\n    Educations: Optional[List[Any]] = None\n    Skills: Optional[List[Any]] = None\n    Certificates: Optional[List[Any]] = None\n    Projects: Optional[List[Any]] = None\n    Achievements: Optional[List[Any]] = None\n    Volunteer: Optional[List[Any]] = None\n\n</code></pre>\n<pre class=\"lang-none prettyprint-override\"><code>    def __init__(self, model_name=model_1, fine_tune_model_path: str = None):\n        # Initialize LLM service with specified model\n\n\n        if fine_tune_model_path:\n            # Load fine-tuned model from local directory\n            self.tokenizer = AutoTokenizer.from_pretrained(fine_tune_model_path)\n            self.model = AutoModel.from_pretrained(fine_tune_model_path)\n        else:\n            # Load base model\n            self.llm_service = HuggingFaceHub(\n                repo_id=&quot;google/flan-t5-base&quot;,\n                huggingfacehub_api_token=huggingface_api_key,\n                model_kwargs={\n                    &quot;temperature&quot;: 0.5,\n                    &quot;max_new_tokens&quot;: 200\n                }  # Model parameters for consistent output\n            )\n    def parsed_resume(self, resume_txt: str):\n        df = pd.read_csv(r&quot;C:\\Users\\Sarthak\\PycharmProjects\\JobAxle\\Service\\data\\for_model_resume_dataset.csv&quot;)\n        print(df['prompt'][0])\n        examples = [\n            {'prompt':df['prompt'][0], &quot;completion&quot;:df['completion'][0]},\n            {'prompt': df['prompt'][1], &quot;completion&quot;: df['completion'][1]}\n        ]\n        print('Examples:',examples[0])\n        example_formatter_template = &quot;&quot;&quot;\n        {prompt}\n        {completion}\\n\n        &quot;&quot;&quot;\n        example_prompt = PromptTemplate(\n            input_variables=[&quot;prompt&quot;, &quot;completion&quot;],\n            template=example_formatter_template,\n        )\n        parser = PydanticOutputParser(pydantic_object=OutputFormat)\n        few_shot_prompt_template = FewShotPromptTemplate(\n            examples=examples,\n            example_prompt=example_prompt,\n            suffix=&quot;&quot;&quot;\n                Parse the given resume text, ensuring the output in JSON format:\n        \n                Resume:\n                {resume}\n        \n                {format_instructions}\n        \n                Output as JSON below:\n                completion:&quot;&quot;&quot;,\n            input_variables=[&quot;resume&quot;],\n            example_separator=&quot;\\n&quot;,\n            partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}\n        )\n        print(&quot;Few-Shot Prompt Template with Examples and JSON Instructions:\\n&quot;, few_shot_prompt_template)\n\n        prompt_template = PromptTemplate(\n            input_variables=['resume'],\n            template=Prompt_2\n        )\n        # print(few_shot_prompt_template)\n        # Initialize the LLM chain\n        chain = LLMChain(\n            llm=self.llm_service,\n            prompt=few_shot_prompt_template,\n            verbose=True\n        )\n        print(&quot;Chain:&quot;, chain)\n        print(resume_txt)\n        try:\n            response = chain.invoke({'resume': resume_txt}, verbose=True)\n            print(response)\n            logger.info('Model Response: %s', response)\n            print(&quot;Type of Response:&quot;,type(response))\n            # Invoke the chain and get a response\n            response_json = self.process_response(response)\n            parsed_json = self.structure_response(response_json)\n            return OutputFormat(**parsed_json)  # Return as OutputFormat object\n\n        except Exception as e:\n            logger.error(&quot;Error parsing resume: %s&quot;, e)\n            raise Exception(f&quot;An error occurred in parsed_resume: {e}&quot;)\n\n    def process_response(self, response_text: str) -&gt; Dict:\n        &quot;&quot;&quot;Process LLM response into JSON format.&quot;&quot;&quot;\n        response_json = json.dumps(response_text).strip()\n\n        # Remove extraneous characters\n        if response_json.startswith(&quot;```json&quot;):\n            response_json = response_json[len(&quot;```json&quot;):].strip()\n        if response_json.endswith(&quot;```&quot;):\n            response_json = response_json[:-len(&quot;```&quot;)].strip()\n\n        response_json = remove_trailing_commas(response_json)\n\n        try:\n\n            return json.loads(response_json)\n        except json.JSONDecodeError as e:\n            logger.error(&quot;JSON decoding error: %s. Response text: %s&quot;, e, response_json)\n            raise ValueError(&quot;Failed to parse response as valid JSON&quot;)\n\n    def structure_response(self, parsed_json: Dict) -&gt; Dict:\n        &quot;&quot;&quot;\n        Structure response JSON to match the OutputFormat schema, ensuring lists for 'Experiences' and 'Educations'.\n        &quot;&quot;&quot;\n        # Ensure ContactInformation, Experiences, and Educations are present in the expected structure\n        parsed_json[&quot;ContactInformation&quot;] = parsed_json.get(&quot;ContactInformation&quot;, {})\n        if isinstance(parsed_json.get(&quot;Experiences&quot;), dict):\n            parsed_json[&quot;Experiences&quot;] = [parsed_json[&quot;Experiences&quot;]]\n        else:\n            parsed_json[&quot;Experiences&quot;] = parsed_json.get(&quot;Experiences&quot;, [])\n\n        if isinstance(parsed_json.get(&quot;Educations&quot;), dict):\n            parsed_json[&quot;Educations&quot;] = [parsed_json[&quot;Educations&quot;]]\n        else:\n            parsed_json[&quot;Educations&quot;] = parsed_json.get(&quot;Educations&quot;, [])\n\n        parsed_json[&quot;Projects&quot;] = parsed_json.get(&quot;Projects&quot;, [])\n\n        return parsed_json\n</code></pre>\n<p>The df contains prompt, completion feature with resume_text as a prompt and json output as completion.</p>\n<p>Or Is there any Alternative to doing the task on a low-end specs laptop? I am in need of help because I can't access big parameter models. anybody??</p>\n",
         "2024-11-12 07:20:00",
         "2",
         "59",
         "1",
         "<nlp><prompt><langchain><large-language-model><huggingface>",
         null,
         null,
         "ERROR:Service.service:Error parsing resume: \"'ContactInformation'\"\n---\n\"detail\": \"Failed to parse the file: An error occurred in parsed_resume: \\\"'ContactInformation'\\\"\"\n---\nfrom typing import List, Optional, Union, Any, re\nimport json\n\n\n\nclass ContactInformation(BaseModel):\n    Name: Optional[str] = None\n    Email: Optional[str] = None\n    Contact: Optional[str] = None\n    Links: Optional[List[str]] = None\n\n\n\nclass Experience(BaseModel):\n    title: Optional[str] = None\n    company: Optional[str] = None\n    duration: Optional[str] = None\n\n\nclass Education(BaseModel):\n    course: Optional[str] = None\n    branch: Optional[str] = None\n    institute: Optional[str] = None\n\n\nclass Projects(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n    link: Optional[str] = None\n\nclass OutputFormat(BaseModel):\n    ContactInformation: Optional[Any] = None\n    AboutMe: Optional[Any] = None\n    Experiences: Optional[List[Any]] = None\n    Educations: Optional[List[Any]] = None\n    Skills: Optional[List[Any]] = None\n    Certificates: Optional[List[Any]] = None\n    Projects: Optional[List[Any]] = None\n    Achievements: Optional[List[Any]] = None\n    Volunteer: Optional[List[Any]] = None\n---\ndef __init__(self, model_name=model_1, fine_tune_model_path: str = None):\n        # Initialize LLM service with specified model\n\n\n        if fine_tune_model_path:\n            # Load fine-tuned model from local directory\n            self.tokenizer = AutoTokenizer.from_pretrained(fine_tune_model_path)\n            self.model = AutoModel.from_pretrained(fine_tune_model_path)\n        else:\n            # Load base model\n            self.llm_service = HuggingFaceHub(\n                repo_id=\"google/flan-t5-base\",\n                huggingfacehub_api_token=huggingface_api_key,\n                model_kwargs={\n                    \"temperature\": 0.5,\n                    \"max_new_tokens\": 200\n                }  # Model parameters for consistent output\n            )\n    def parsed_resume(self, resume_txt: str):\n        df = pd.read_csv(r\"C:\\Users\\Sarthak\\PycharmProjects\\JobAxle\\Service\\data\\for_model_resume_dataset.csv\")\n        print(df['prompt'][0])\n        examples = [\n            {'prompt':df['prompt'][0], \"completion\":df['completion'][0]},\n            {'prompt': df['prompt'][1], \"completion\": df['completion'][1]}\n        ]\n        print('Examples:',examples[0])\n        example_formatter_template = \"\"\"\n        {prompt}\n        {completion}\\n\n        \"\"\"\n        example_prompt = PromptTemplate(\n            input_variables=[\"prompt\", \"completion\"],\n            template=example_formatter_template,\n        )\n        parser = PydanticOutputParser(pydantic_object=OutputFormat)\n        few_shot_prompt_template = FewShotPromptTemplate(\n            examples=examples,\n            example_prompt=example_prompt,\n            suffix=\"\"\"\n                Parse the given resume text, ensuring the output in JSON format:\n        \n                Resume:\n                {resume}\n        \n                {format_instructions}\n        \n                Output as JSON below:\n                completion:\"\"\",\n            input_variables=[\"resume\"],\n            example_separator=\"\\n\",\n            partial_variables={\"format_instructions\": parser.get_format_instructions()}\n        )\n        print(\"Few-Shot Prompt Template with Examples and JSON Instructions:\\n\", few_shot_prompt_template)\n\n        prompt_template = PromptTemplate(\n            input_variables=['resume'],\n            template=Prompt_2\n        )\n        # print(few_shot_prompt_template)\n        # Initialize the LLM chain\n        chain = LLMChain(\n            llm=self.llm_service,\n            prompt=few_shot_prompt_template,\n            verbose=True\n        )\n        print(\"Chain:\", chain)\n        print(resume_txt)\n        try:\n            response = chain.invoke({'resume': resume_txt}, verbose=True)\n            print(response)\n            logger.info('Model Response: %s', response)\n            print(\"Type of Response:\",type(response))\n            # Invoke the chain and get a response\n            response_json = self.process_response(response)\n            parsed_json = self.structure_response(response_json)\n            return OutputFormat(**parsed_json)  # Return as OutputFormat object\n\n        except Exception as e:\n            logger.error(\"Error parsing resume: %s\", e)\n            raise Exception(f\"An error occurred in parsed_resume: {e}\")\n\n    def process_response(self, response_text: str) -> Dict:\n        \"\"\"Process LLM response into JSON format.\"\"\"\n        response_json = json.dumps(response_text).strip()\n\n        # Remove extraneous characters\n        if response_json.startswith(\"```json\"):\n            response_json = response_json[len(\"```json\"):].strip()\n        if response_json.endswith(\"```\"):\n            response_json = response_json[:-len(\"```\")].strip()\n\n        response_json = remove_trailing_commas(response_json)\n\n        try:\n\n            return json.loads(response_json)\n        except json.JSONDecodeError as e:\n            logger.error(\"JSON decoding error: %s. Response text: %s\", e, response_json)\n            raise ValueError(\"Failed to parse response as valid JSON\")\n\n    def structure_response(self, parsed_json: Dict) -> Dict:\n        \"\"\"\n        Structure response JSON to match the OutputFormat schema, ensuring lists for 'Experiences' and 'Educations'.\n        \"\"\"\n        # Ensure ContactInformation, Experiences, and Educations are present in the expected structure\n        parsed_json[\"ContactInformation\"] = parsed_json.get(\"ContactInformation\", {})\n        if isinstance(parsed_json.get(\"Experiences\"), dict):\n            parsed_json[\"Experiences\"] = [parsed_json[\"Experiences\"]]\n        else:\n            parsed_json[\"Experiences\"] = parsed_json.get(\"Experiences\", [])\n\n        if isinstance(parsed_json.get(\"Educations\"), dict):\n            parsed_json[\"Educations\"] = [parsed_json[\"Educations\"]]\n        else:\n            parsed_json[\"Educations\"] = parsed_json.get(\"Educations\", [])\n\n        parsed_json[\"Projects\"] = parsed_json.get(\"Projects\", [])\n\n        return parsed_json",
         "",
         "How to parse a resume with few shot method using the specified models from HuggingFace and Langchain?",
         "Model selection confusion and some errors while trying to parse a resume with the following codes Trying to do a few shot prompting with a google flan t5 base model While Doing so I am getting an error exception as The code is given below The df contains prompt, completion feature with resume_text as a prompt and json output as completion. Or Is there any Alternative to doing the task on a low-end specs laptop? I am in need of help because I can't access big parameter models. anybody??",
         "",
         "How to parse a resume with few shot method using the specified models from HuggingFace and Langchain? Model selection confusion and some errors while trying to parse a resume with the following codes Trying to do a few shot prompting with a google flan t5 base model While Doing so I am getting an error exception as The code is given below The df contains prompt, completion feature with resume_text as a prompt and json output as completion. Or Is there any Alternative to doing the task on a low-end specs laptop? I am in need of help because I can't access big parameter models. anybody?? ",
         "parse resume shot method using specified models huggingface langchain ? model selection confusion errors trying parse resume following codes trying shot prompting google flan t5 base model getting error exception code given df contains prompt , completion feature resume_text prompt json output completion . alternative task low-end specs laptop ? need help ca n't access big parameter models . anybody ? ?"
        ],
        [
         "46",
         "79178041",
         "Normalization of token embeddings in BERT encoder blocks",
         "<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>\n",
         "2024-11-11 14:30:31",
         "2",
         "141",
         "2",
         "<nlp><normalization><bert-language-model><attention-model>",
         "79238393.0",
         "<p>I tracked down full details of layer normalization (LN) in BERT <a href=\"https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para\">here</a>.</p>\n<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>\n",
         "",
         "",
         "Normalization of token embeddings in BERT encoder blocks",
         "Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?",
         "I tracked down full details of layer normalization (LN) in BERT here . Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.",
         "Normalization of token embeddings in BERT encoder blocks Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)? I tracked down full details of layer normalization (LN) in BERT here . Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.",
         "normalization token embeddings bert encoder blocks following multi-headed attention layer bert encoder block , layer normalization done separately embedding token ( i.e. , one mean variance per token embedding ) , concatenated vector token embeddings ( mean variance embeddings ) ? tracked full details layer normalization ( ln ) bert . mean variance computed per token . weight bias parameters learned ln per token - 's per embedding dimension ."
        ],
        [
         "47",
         "79177228",
         "DASK to_csv() problems due to memory",
         "<p>I'm cleaning my text data and afterwards want to save it to csv. Defined cleaning functions work fine, but when to_csv() part comes, here come the problems as well.\nMaybe someone have faced similar problem and has a trick to share with me how it would be possible to solve it? Maybe saving data to csv in chunks or something?</p>\n<p>`if <strong>name</strong> == '<strong>main</strong>':\n# Initialize the Dask client\nclient = Client(n_workers=3, threads_per_worker=1,\nmemory_limit='1.5GB')<br />\nprint('Dask client created')</p>\n<pre><code>PATH = &quot;C:\\\\Users\\\\el ruchenzo\\\\jobsproject\\\\jobsproject\\\\lt_data.csv&quot;\nreqd = ['description', 'title', 'code']\nblocksize = 25e6  # REDUCED FROM 100 GB TO 25 GB\n\n# Load the CSV with Dask\ndf = dd.read_csv(PATH,\n                 usecols=reqd,\n                 blocksize=blocksize,\n                 dtype={'Code': 'float'},\n                 engine='python',\n                 encoding='utf-8',\n                 on_bad_lines='skip')\n\n# Apply the cleaning function to the 'title' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_title'] = df['title'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('title', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f&quot;1. Processing time: {end_time - start_time:.2f} seconds&quot;)\n\n# Apply the cleaning function to the 'description' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_description'] = df['description'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('description', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f&quot;2. Processing time: {end_time - start_time:.2f} seconds&quot;)\n\ndf.to_csv('cleaned_lemma_*.csv', index=False, encoding='utf-8', single_file = False)\nprint('Saved to csv successfully')\n\nprint('Work ended successfully')`\n</code></pre>\n<p>Tried changing workers number, blocksize, memory limit and etc., but nothing seems to work. Also change map() to apply(), tried using df = df.persist(), writing data to csv with pandas in chunks instead with dask, and so on.</p>\n",
         "2024-11-11 10:13:33",
         "0",
         "38",
         "1",
         "<csv><text><nlp><export-to-csv><dask>",
         null,
         null,
         "PATH = \"C:\\\\Users\\\\el ruchenzo\\\\jobsproject\\\\jobsproject\\\\lt_data.csv\"\nreqd = ['description', 'title', 'code']\nblocksize = 25e6  # REDUCED FROM 100 GB TO 25 GB\n\n# Load the CSV with Dask\ndf = dd.read_csv(PATH,\n                 usecols=reqd,\n                 blocksize=blocksize,\n                 dtype={'Code': 'float'},\n                 engine='python',\n                 encoding='utf-8',\n                 on_bad_lines='skip')\n\n# Apply the cleaning function to the 'title' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_title'] = df['title'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('title', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f\"1. Processing time: {end_time - start_time:.2f} seconds\")\n\n# Apply the cleaning function to the 'description' column in the DataFrame\nstart_time = time.time()\ndf['cleaned_description'] = df['description'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('description', 'object'))\ngc.collect()\nend_time = time.time()\nprint(f\"2. Processing time: {end_time - start_time:.2f} seconds\")\n\ndf.to_csv('cleaned_lemma_*.csv', index=False, encoding='utf-8', single_file = False)\nprint('Saved to csv successfully')\n\nprint('Work ended successfully')`",
         "",
         "DASK to_csv() problems due to memory",
         "I'm cleaning my text data and afterwards want to save it to csv. Defined cleaning functions work fine, but when to_csv() part comes, here come the problems as well. Maybe someone have faced similar problem and has a trick to share with me how it would be possible to solve it? Maybe saving data to csv in chunks or something? `if name == ' main ': # Initialize the Dask client client = Client(n_workers=3, threads_per_worker=1, memory_limit='1.5GB') print('Dask client created') Tried changing workers number, blocksize, memory limit and etc., but nothing seems to work. Also change map() to apply(), tried using df = df.persist(), writing data to csv with pandas in chunks instead with dask, and so on.",
         "",
         "DASK to_csv() problems due to memory I'm cleaning my text data and afterwards want to save it to csv. Defined cleaning functions work fine, but when to_csv() part comes, here come the problems as well. Maybe someone have faced similar problem and has a trick to share with me how it would be possible to solve it? Maybe saving data to csv in chunks or something? `if name == ' main ': # Initialize the Dask client client = Client(n_workers=3, threads_per_worker=1, memory_limit='1.5GB') print('Dask client created') Tried changing workers number, blocksize, memory limit and etc., but nothing seems to work. Also change map() to apply(), tried using df = df.persist(), writing data to csv with pandas in chunks instead with dask, and so on. ",
         "dask to_csv ( ) problems due memory 'm cleaning text data afterwards want save csv . defined cleaning functions work fine , to_csv ( ) part comes , come problems well . maybe someone faced similar problem trick share would possible solve ? maybe saving data csv chunks something ? ` name == ' main ' : # initialize dask client client = client ( n_workers=3 , threads_per_worker=1 , memory_limit= ' 1.5gb ' ) print ( 'dask client created ' ) tried changing workers number , blocksize , memory limit etc. , nothing seems work . also change map ( ) apply ( ) , tried using df = df.persist ( ) , writing data csv pandas chunks instead dask , ."
        ],
        [
         "48",
         "79173053",
         "How to convert character indices to BERT token indices",
         "<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>\n<pre><code>from datasets import load_dataset\nds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\n</code></pre>\n<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>\n<pre><code>d0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n</code></pre>\n<p>After tokenization, the answer indices are 56  and 16:</p>\n<pre><code>from transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'\n</code></pre>\n<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>\n<p>This is from a <a href=\"https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false\" rel=\"nofollow noreferrer\">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=\"https://i.sstatic.net/GsZ6mfcQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/GsZ6mfcQ.png\" alt=\"QA dataset with token answer indices\" /></a></p>\n",
         "2024-11-09 15:15:33",
         "2",
         "33",
         "1",
         "<python><nlp><dataset><large-language-model><bert-language-model>",
         "79175157.0",
         "<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>\n<p>The following function does the above for you:</p>\n<pre><code>def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start &lt;= char_start &lt; end: \n            start_token_idx = i\n        if start &lt; char_end &lt;= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n</code></pre>\n<p>Here's how you can use and test this function:</p>\n<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])\nprint(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(&quot;Tokenized answer:&quot;, answer_tokens)\n</code></pre>\n<p>Output:</p>\n<pre><code>Tokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream\n</code></pre>\n",
         "UCLNLP/adversarial_qa\n---\nfrom datasets import load_dataset\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\n---\nd0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n---\nfrom transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'",
         "def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start <= char_start < end: \n            start_token_idx = i\n        if start < char_end <= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n---\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(\"Tokenized start index:\", d0_tokenized['answer_start_token_idx'])\nprint(\"Tokenized end index:\", d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(\"Tokenized answer:\", answer_tokens)\n---\nTokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream",
         "How to convert character indices to BERT token indices",
         "I am working with a question-answer dataset . How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset: After tokenization, the answer indices are 56 and 16: I want to create a new dataset with the answer's token indices, e.g., 56 ad 60. This is from a linkedin learning class . The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:",
         "You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices. The following function does the above for you: Here's how you can use and test this function: Output:",
         "How to convert character indices to BERT token indices I am working with a question-answer dataset . How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset: After tokenization, the answer indices are 56 and 16: I want to create a new dataset with the answer's token indices, e.g., 56 ad 60. This is from a linkedin learning class . The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result: You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices. The following function does the above for you: Here's how you can use and test this function: Output:",
         "convert character indices bert token indices working question-answer dataset . map character-based answer indices token-based indices tokenizing context question together using tokenizer like bert . 's example row dataset : tokenization , answer indices 56 16 : want create new dataset answer 's token indices , e.g. , 56 ad 60. linkedin learning class . instructor conversion created csv file share code . expected result : encode question context , locate token span answer within tokenized context , update dataset token-level indices . following function : 's use test function : output :"
        ],
        [
         "49",
         "79165649",
         "Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy",
         "<p>I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity.</p>\n<p>My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold.</p>\n<p>For instance, the text &quot;Standard PRS for Alzheimer's&quot; incorrectly links entities for &quot;Standard&quot; and &quot;PRS,&quot; in addition to &quot;Alzheimer's.&quot; Another example, &quot;Other diseases of respiratory system, NEC,&quot; captures &quot;respiratory&quot; and &quot;diseases&quot; but misses &quot;NEC&quot; (Necrotizing enterocolitis), which should be prioritized.</p>\n<p>I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms.</p>\n<p>I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well).</p>\n<p>I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd).</p>\n<p>I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must.</p>\n",
         "2024-11-07 08:52:14",
         "2",
         "34",
         "1",
         "<nlp><data-science><spacy><named-entity-recognition><entity-linking>",
         null,
         null,
         "",
         "",
         "Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy",
         "I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity. My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold. For instance, the text \"Standard PRS for Alzheimer's\" incorrectly links entities for \"Standard\" and \"PRS,\" in addition to \"Alzheimer's.\" Another example, \"Other diseases of respiratory system, NEC,\" captures \"respiratory\" and \"diseases\" but misses \"NEC\" (Necrotizing enterocolitis), which should be prioritized. I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms. I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well). I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd). I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must.",
         "",
         "Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity. My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold. For instance, the text \"Standard PRS for Alzheimer's\" incorrectly links entities for \"Standard\" and \"PRS,\" in addition to \"Alzheimer's.\" Another example, \"Other diseases of respiratory system, NEC,\" captures \"respiratory\" and \"diseases\" but misses \"NEC\" (Necrotizing enterocolitis), which should be prioritized. I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms. I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well). I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd). I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must. ",
         "handling multiple entity candidates short texts entity linking scispacy working linking short texts entities biomedical knowledge graph ( umls cuis ) using scispacy research project . goal analyze relationship linked entity separate predefined entity . challenge managing multiple possible entities identified texts , introduces noise results . although use heuristics regex , manual stop list , filtering semantic categories ( tuis ) clean data , issue persists due text complexity . typically select top ~3 entities per text based ner score , relatively high threshold . instance , text `` standard prs alzheimer 's '' incorrectly links entities `` standard '' `` prs , '' addition `` alzheimer 's . '' another example , `` diseases respiratory system , nec , '' captures `` respiratory '' `` diseases '' misses `` nec '' ( necrotizing enterocolitis ) , prioritized . 've tried filtering results semantic similarity using biomedical model , approach still imprecise heavily dependent number results . linker often seems prioritize entities appearing earlier text . also use abbreviation expander handle non-standard acronym forms . think smarter linker ( supported scispacy ) might help , better matching sentence/whole text level , n't know much . ( filtering results using sentence transformers , 's cossine sim - could n't find clear cutoff generalized well ) . resources/time learn fine-tune new linker model+data ( sub-component overall phd ) . 'm looking advice effective strategies entity linking sentence whole-text level without resources fine-tune new model . compatability scispacy important , since linkage umls ontology ( kg cui entites ) must ."
        ]
       ],
       "shape": {
        "columns": 17,
        "rows": 16679
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AllTags</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AcceptedAnswerBody</th>\n",
       "      <th>Question_Code</th>\n",
       "      <th>Answer_Code</th>\n",
       "      <th>Title_Clean</th>\n",
       "      <th>Body_Clean</th>\n",
       "      <th>AcceptedAnswerBody_Clean</th>\n",
       "      <th>combination_text</th>\n",
       "      <th>combination_text_no_stopw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this [site][...</td>\n",
       "      <td>2025-03-11 14:50:31</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;python&gt;&lt;nlp&gt;&lt;large-language-model&gt;</td>\n",
       "      <td>79501337.0</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "      <td>server\\n---\\nSSH\\n---\\nskip_tokens = [1]  # sk...</td>\n",
       "      <td>matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>I am running the code found on this site1 in m...</td>\n",
       "      <td>I can't test it but ... I checked source code ...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>store images instead showing server running co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79498915</td>\n",
       "      <td>Comparing the similarity of spoken and written...</td>\n",
       "      <td>&lt;p&gt;I'm converting spoken form text to its writ...</td>\n",
       "      <td>2025-03-10 18:55:59</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;nlp&gt;&lt;large-language-model&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Comparing the similarity of spoken and written...</td>\n",
       "      <td>I'm converting spoken form text to its written...</td>\n",
       "      <td></td>\n",
       "      <td>Comparing the similarity of spoken and written...</td>\n",
       "      <td>comparing similarity spoken written form text ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79488426</td>\n",
       "      <td>Upserting in Pinecone takes too long</td>\n",
       "      <td>&lt;p&gt;I'm trying to upsert reviews that i've scra...</td>\n",
       "      <td>2025-03-06 06:22:35</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;python&gt;&lt;nlp&gt;&lt;rag&gt;&lt;pinecone&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jina-embedding-v3\\n---\\nif index_name not in p...</td>\n",
       "      <td></td>\n",
       "      <td>Upserting in Pinecone takes too long</td>\n",
       "      <td>I'm trying to upsert reviews that i've scraped...</td>\n",
       "      <td></td>\n",
       "      <td>Upserting in Pinecone takes too long I'm tryin...</td>\n",
       "      <td>upserting pinecone takes long 'm trying upsert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79484448</td>\n",
       "      <td>How does ELMo generate words for training ? Is...</td>\n",
       "      <td>&lt;p&gt;I'm confused about using Bidirectional LM f...</td>\n",
       "      <td>2025-03-04 17:32:14</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;nlp&gt;&lt;language-model&gt;&lt;autoregressive-models&gt;&lt;e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>How does ELMo generate words for training ? Is...</td>\n",
       "      <td>I'm confused about using Bidirectional LM for ...</td>\n",
       "      <td></td>\n",
       "      <td>How does ELMo generate words for training ? Is...</td>\n",
       "      <td>elmo generate words training ? autoregressive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79482290</td>\n",
       "      <td>How to handle German language specific charact...</td>\n",
       "      <td>&lt;p&gt;I am working with German Texts, where I nee...</td>\n",
       "      <td>2025-03-03 22:32:36</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;python&gt;&lt;nlp&gt;&lt;tokenize&gt;&lt;large-language-model&gt;&lt;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>from transformers import GPT2Tokenizer\\n\\ntext...</td>\n",
       "      <td></td>\n",
       "      <td>How to handle German language specific charact...</td>\n",
       "      <td>I am working with German Texts, where I need t...</td>\n",
       "      <td></td>\n",
       "      <td>How to handle German language specific charact...</td>\n",
       "      <td>handle german language specific characters lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>62328</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>&lt;p&gt;input: phrase 1, phrase 2&lt;/p&gt;\\n\\n&lt;p&gt;output:...</td>\n",
       "      <td>2008-09-15 12:26:42</td>\n",
       "      <td>65</td>\n",
       "      <td>49872</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;algorithm&gt;&lt;nlp&gt;&lt;semantics&gt;</td>\n",
       "      <td>63076.0</td>\n",
       "      <td>&lt;hr&gt;\\n\\n&lt;p&gt;You might want to check out this pa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>input: phrase 1, phrase 2 output: semantic sim...</td>\n",
       "      <td>You might want to check out this paper: Senten...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>algorithm tells semantic similarity two phrase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16675</th>\n",
       "      <td>41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03 10:36:13</td>\n",
       "      <td>118</td>\n",
       "      <td>33149</td>\n",
       "      <td>17</td>\n",
       "      <td>&lt;nlp&gt;</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>&lt;p&gt;Actually what Google does is very much non-...</td>\n",
       "      <td>&lt;spell_checked_word&gt;</td>\n",
       "      <td></td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>Possible Duplicate: How does the Google Did yo...</td>\n",
       "      <td>Actually what Google does is much non-trivial ...</td>\n",
       "      <td>How do you implement a \"Did you mean\"? Possibl...</td>\n",
       "      <td>implement `` mean '' ? possible duplicate : go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16676</th>\n",
       "      <td>36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31 01:08:48</td>\n",
       "      <td>3</td>\n",
       "      <td>5658</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;windows-vista&gt;&lt;nlp&gt;&lt;speech-recognition&gt;&lt;multi...</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>&lt;p&gt;Citation from Vista &lt;a href=\"http://blogs.m...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>my primary language is spanish, but I use all ...</td>\n",
       "      <td>Citation from Vista speech recognition blog : ...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>vista speech recognition multiple languages pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16677</th>\n",
       "      <td>25332</td>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>&lt;p&gt;I'm looking for an existing library to summ...</td>\n",
       "      <td>2008-08-24 20:57:33</td>\n",
       "      <td>14</td>\n",
       "      <td>6483</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;language-agnostic&gt;&lt;nlp&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>I'm looking for an existing library to summari...</td>\n",
       "      <td></td>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>'s good natural language library use paraphras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16678</th>\n",
       "      <td>23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22 22:45:10</td>\n",
       "      <td>27</td>\n",
       "      <td>6462</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;.net&gt;&lt;datetime&gt;&lt;nlp&gt;</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>&lt;p&gt;We developed exactly what you are looking f...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>Does anyone know of a .NET date/time parser si...</td>\n",
       "      <td>We developed exactly what you are looking for ...</td>\n",
       "      <td>Natural language date/time parser for .NET? Do...</td>\n",
       "      <td>natural language date/time parser .net ? anyon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16679 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       QuestionId                                              Title  \\\n",
       "0        79501178        Store images instead of showing in a server   \n",
       "1        79498915  Comparing the similarity of spoken and written...   \n",
       "2        79488426               Upserting in Pinecone takes too long   \n",
       "3        79484448  How does ELMo generate words for training ? Is...   \n",
       "4        79482290  How to handle German language specific charact...   \n",
       "...           ...                                                ...   \n",
       "16674       62328  Is there an algorithm that tells the semantic ...   \n",
       "16675       41424             How do you implement a \"Did you mean\"?   \n",
       "16676       36533     Vista speech recognition in multiple languages   \n",
       "16677       25332  What's a good natural language library to use ...   \n",
       "16678       23689        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                    Body         CreationDate  \\\n",
       "0      <p>I am running the code found on this [site][...  2025-03-11 14:50:31   \n",
       "1      <p>I'm converting spoken form text to its writ...  2025-03-10 18:55:59   \n",
       "2      <p>I'm trying to upsert reviews that i've scra...  2025-03-06 06:22:35   \n",
       "3      <p>I'm confused about using Bidirectional LM f...  2025-03-04 17:32:14   \n",
       "4      <p>I am working with German Texts, where I nee...  2025-03-03 22:32:36   \n",
       "...                                                  ...                  ...   \n",
       "16674  <p>input: phrase 1, phrase 2</p>\\n\\n<p>output:...  2008-09-15 12:26:42   \n",
       "16675  <blockquote>\\n  <p><strong>Possible Duplicate:...  2008-09-03 10:36:13   \n",
       "16676  <p>my primary language is spanish, but I use a...  2008-08-31 01:08:48   \n",
       "16677  <p>I'm looking for an existing library to summ...  2008-08-24 20:57:33   \n",
       "16678  <p>Does anyone know of a .NET date/time parser...  2008-08-22 22:45:10   \n",
       "\n",
       "       Score  ViewCount  AnswerCount  \\\n",
       "0          0         23            1   \n",
       "1          0         20            1   \n",
       "2          1         37            1   \n",
       "3          0         28            1   \n",
       "4          1         59            1   \n",
       "...      ...        ...          ...   \n",
       "16674     65      49872           11   \n",
       "16675    118      33149           17   \n",
       "16676      3       5658            6   \n",
       "16677     14       6483            4   \n",
       "16678     27       6462            9   \n",
       "\n",
       "                                                 AllTags  AcceptedAnswerId  \\\n",
       "0                    <python><nlp><large-language-model>        79501337.0   \n",
       "1                            <nlp><large-language-model>               NaN   \n",
       "2                           <python><nlp><rag><pinecone>               NaN   \n",
       "3      <nlp><language-model><autoregressive-models><e...               NaN   \n",
       "4      <python><nlp><tokenize><large-language-model><...               NaN   \n",
       "...                                                  ...               ...   \n",
       "16674                        <algorithm><nlp><semantics>           63076.0   \n",
       "16675                                              <nlp>           41448.0   \n",
       "16676  <windows-vista><nlp><speech-recognition><multi...           36684.0   \n",
       "16677                           <language-agnostic><nlp>               NaN   \n",
       "16678                              <.net><datetime><nlp>          631134.0   \n",
       "\n",
       "                                      AcceptedAnswerBody  \\\n",
       "0      <p>I can't test it but ...</p>\\n<p>I checked <...   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "16674  <hr>\\n\\n<p>You might want to check out this pa...   \n",
       "16675  <p>Actually what Google does is very much non-...   \n",
       "16676  <p>Citation from Vista <a href=\"http://blogs.m...   \n",
       "16677                                                NaN   \n",
       "16678  <p>We developed exactly what you are looking f...   \n",
       "\n",
       "                                           Question_Code  \\\n",
       "0      server\\n---\\nSSH\\n---\\nskip_tokens = [1]  # sk...   \n",
       "1                                                          \n",
       "2      jina-embedding-v3\\n---\\nif index_name not in p...   \n",
       "3                                                          \n",
       "4      from transformers import GPT2Tokenizer\\n\\ntext...   \n",
       "...                                                  ...   \n",
       "16674                                                      \n",
       "16675                               <spell_checked_word>   \n",
       "16676                                                      \n",
       "16677                                                      \n",
       "16678                                                      \n",
       "\n",
       "                                             Answer_Code  \\\n",
       "0      matplotlib\\n---\\nshow=True\\n---\\nfig, ax\\n---\\...   \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          \n",
       "...                                                  ...   \n",
       "16674                                                      \n",
       "16675                                                      \n",
       "16676                                                      \n",
       "16677                                                      \n",
       "16678                                                      \n",
       "\n",
       "                                             Title_Clean  \\\n",
       "0            Store images instead of showing in a server   \n",
       "1      Comparing the similarity of spoken and written...   \n",
       "2                   Upserting in Pinecone takes too long   \n",
       "3      How does ELMo generate words for training ? Is...   \n",
       "4      How to handle German language specific charact...   \n",
       "...                                                  ...   \n",
       "16674  Is there an algorithm that tells the semantic ...   \n",
       "16675             How do you implement a \"Did you mean\"?   \n",
       "16676     Vista speech recognition in multiple languages   \n",
       "16677  What's a good natural language library to use ...   \n",
       "16678        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                              Body_Clean  \\\n",
       "0      I am running the code found on this site1 in m...   \n",
       "1      I'm converting spoken form text to its written...   \n",
       "2      I'm trying to upsert reviews that i've scraped...   \n",
       "3      I'm confused about using Bidirectional LM for ...   \n",
       "4      I am working with German Texts, where I need t...   \n",
       "...                                                  ...   \n",
       "16674  input: phrase 1, phrase 2 output: semantic sim...   \n",
       "16675  Possible Duplicate: How does the Google Did yo...   \n",
       "16676  my primary language is spanish, but I use all ...   \n",
       "16677  I'm looking for an existing library to summari...   \n",
       "16678  Does anyone know of a .NET date/time parser si...   \n",
       "\n",
       "                                AcceptedAnswerBody_Clean  \\\n",
       "0      I can't test it but ... I checked source code ...   \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          \n",
       "...                                                  ...   \n",
       "16674  You might want to check out this paper: Senten...   \n",
       "16675  Actually what Google does is much non-trivial ...   \n",
       "16676  Citation from Vista speech recognition blog : ...   \n",
       "16677                                                      \n",
       "16678  We developed exactly what you are looking for ...   \n",
       "\n",
       "                                        combination_text  \\\n",
       "0      Store images instead of showing in a server I ...   \n",
       "1      Comparing the similarity of spoken and written...   \n",
       "2      Upserting in Pinecone takes too long I'm tryin...   \n",
       "3      How does ELMo generate words for training ? Is...   \n",
       "4      How to handle German language specific charact...   \n",
       "...                                                  ...   \n",
       "16674  Is there an algorithm that tells the semantic ...   \n",
       "16675  How do you implement a \"Did you mean\"? Possibl...   \n",
       "16676  Vista speech recognition in multiple languages...   \n",
       "16677  What's a good natural language library to use ...   \n",
       "16678  Natural language date/time parser for .NET? Do...   \n",
       "\n",
       "                               combination_text_no_stopw  \n",
       "0      store images instead showing server running co...  \n",
       "1      comparing similarity spoken written form text ...  \n",
       "2      upserting pinecone takes long 'm trying upsert...  \n",
       "3      elmo generate words training ? autoregressive ...  \n",
       "4      handle german language specific characters lik...  \n",
       "...                                                  ...  \n",
       "16674  algorithm tells semantic similarity two phrase...  \n",
       "16675  implement `` mean '' ? possible duplicate : go...  \n",
       "16676  vista speech recognition multiple languages pr...  \n",
       "16677  's good natural language library use paraphras...  \n",
       "16678  natural language date/time parser .net ? anyon...  \n",
       "\n",
       "[16679 rows x 17 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# or whatever value you want to test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m cluster_kmean \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit_predict(embeddings_title)\n\u001b[0;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_kmean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cluster_kmean\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "k = 10  # or whatever value you want to test\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "cluster_kmean = kmeans.fit_predict(embeddings_title)\n",
    "\n",
    "df['cluster_kmean'] = cluster_kmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cluster",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cluster_kmean",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "09ab0e32-88af-436f-af43-42c6d3afc8ed",
       "rows": [
        [
         "0",
         "Store images instead of showing in a server",
         "-1",
         "8"
        ],
        [
         "1",
         "Comparing the similarity of spoken and written form text",
         "-1",
         "3"
        ],
        [
         "2",
         "Upserting in Pinecone takes too long",
         "-1",
         "8"
        ],
        [
         "3",
         "How does ELMo generate words for training ? Is it autoregressive?",
         "-1",
         "7"
        ],
        [
         "4",
         "How to handle German language specific characters like (, , , ) while tokenizing using GPT2Tokenizer?",
         "-1",
         "9"
        ],
        [
         "5",
         "Presidio with Langchain Experimental does not detect Polish names",
         "-1",
         "8"
        ],
        [
         "6",
         "Where is the HuggingFace model saved in when loading a model on colab?",
         "-1",
         "8"
        ],
        [
         "7",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "-1",
         "1"
        ],
        [
         "8",
         "word/ sentence similarities",
         "-1",
         "3"
        ],
        [
         "9",
         "How do I remove escape characters from output of nltk.word_tokenize?",
         "-1",
         "1"
        ],
        [
         "10",
         "Python Farm-haystack Dependencies",
         "-1",
         "2"
        ],
        [
         "11",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "-1",
         "7"
        ],
        [
         "12",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text",
         "-1",
         "8"
        ],
        [
         "13",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query",
         "-1",
         "8"
        ],
        [
         "14",
         "LLM Model Lacking Confidence and Changing Answers Based on User Input",
         "-1",
         "8"
        ],
        [
         "15",
         "Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data?",
         "-1",
         "7"
        ],
        [
         "16",
         "torch.OutOfMemoryError: CUDA out of memory. (Google Colab)",
         "-1",
         "2"
        ],
        [
         "17",
         "how to create a Natural Language Inference pipeline in haystack",
         "-1",
         "3"
        ],
        [
         "18",
         "TypeError: isinstance() arg 2 must be a type or tuple of types with collections search in Weaviate",
         "-1",
         "2"
        ],
        [
         "19",
         "Is it possible to Fine-Tune TinyBERT on Mac (M1 chip)?",
         "-1",
         "8"
        ],
        [
         "20",
         "Can't compile Marian NMT",
         "-1",
         "2"
        ],
        [
         "21",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "-1",
         "7"
        ],
        [
         "22",
         "getting an error: object of type 'float' has no len()",
         "-1",
         "2"
        ],
        [
         "23",
         "Is n-gram precision the number of elements in the intersection of one hypothesis and possibly many references?",
         "-1",
         "8"
        ],
        [
         "24",
         "Getting all leaf words (reverse stemming) into one Python List",
         "-1",
         "6"
        ],
        [
         "25",
         "torch.nn.functional.softmax giving inaccurate softmax output",
         "-1",
         "7"
        ],
        [
         "26",
         "Why does my contrastive learning model's loss and gradients explode during training?",
         "-1",
         "7"
        ],
        [
         "27",
         "Inspect all probabilities of BERTopic model",
         "-1",
         "7"
        ],
        [
         "28",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "-1",
         "3"
        ],
        [
         "29",
         "catelog sentences into 5 words that represent them",
         "-1",
         "3"
        ],
        [
         "30",
         "How to correctly identify entity types for tokens using spaCy using python?",
         "-1",
         "0"
        ],
        [
         "31",
         "--user-dir in Fairseq in failing",
         "-1",
         "2"
        ],
        [
         "32",
         "Recommending a pre-train NER model for geospatial entities",
         "-1",
         "8"
        ],
        [
         "33",
         "How to split and spelling correct arabic text without spaces into list of words",
         "-1",
         "9"
        ],
        [
         "34",
         "similarity from word to sentence after doing words Embedding",
         "-1",
         "5"
        ],
        [
         "35",
         "How to install spacy?",
         "0",
         "0"
        ],
        [
         "36",
         "How to log only the current script file to W&B code panel immediately?",
         "-1",
         "8"
        ],
        [
         "37",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "-1",
         "9"
        ],
        [
         "38",
         "Error in getting Captum text explanations for text classification",
         "-1",
         "2"
        ],
        [
         "39",
         "euclidian distance from word to sentence after doing Vectorizer",
         "-1",
         "5"
        ],
        [
         "40",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "-1",
         "8"
        ],
        [
         "41",
         "How to extract specific entities from unstructured text",
         "-1",
         "3"
        ],
        [
         "42",
         "Understanding byte-pair encoding tokenization for Greek characters",
         "-1",
         "8"
        ],
        [
         "43",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "-1",
         "6"
        ],
        [
         "44",
         "Pyspark sentiment analysis invalid output",
         "-1",
         "2"
        ],
        [
         "45",
         "How to parse a resume with few shot method using the specified models from HuggingFace and Langchain?",
         "-1",
         "7"
        ],
        [
         "46",
         "Normalization of token embeddings in BERT encoder blocks",
         "-1",
         "7"
        ],
        [
         "47",
         "DASK to_csv() problems due to memory",
         "-1",
         "2"
        ],
        [
         "48",
         "How to convert character indices to BERT token indices",
         "-1",
         "7"
        ],
        [
         "49",
         "Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy",
         "-1",
         "3"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 16679
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title_Clean</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_kmean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comparing the similarity of spoken and written...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Upserting in Pinecone takes too long</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does ELMo generate words for training ? Is...</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to handle German language specific charact...</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16675</th>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16676</th>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16677</th>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16678</th>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16679 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Title_Clean  cluster  \\\n",
       "0            Store images instead of showing in a server       -1   \n",
       "1      Comparing the similarity of spoken and written...       -1   \n",
       "2                   Upserting in Pinecone takes too long       -1   \n",
       "3      How does ELMo generate words for training ? Is...       -1   \n",
       "4      How to handle German language specific charact...       -1   \n",
       "...                                                  ...      ...   \n",
       "16674  Is there an algorithm that tells the semantic ...       -1   \n",
       "16675             How do you implement a \"Did you mean\"?       -1   \n",
       "16676     Vista speech recognition in multiple languages       -1   \n",
       "16677  What's a good natural language library to use ...       -1   \n",
       "16678        Natural language date/time parser for .NET?       -1   \n",
       "\n",
       "       cluster_kmean  \n",
       "0                  8  \n",
       "1                  3  \n",
       "2                  8  \n",
       "3                  7  \n",
       "4                  9  \n",
       "...              ...  \n",
       "16674              3  \n",
       "16675              8  \n",
       "16676              8  \n",
       "16677              3  \n",
       "16678              1  \n",
       "\n",
       "[16679 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Title_Clean','cluster','cluster_kmean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tags:\n",
    "\n",
    "Plan to get tags?\n",
    "\n",
    "- Keep filtering the Word Cloud and then manually create 10 topics?\n",
    "\n",
    ": first to identify the words that better represent a certain document\n",
    "second to encode them into vectors\n",
    "BERT-based keyword extraction algorithm\n",
    "KeyBERT [20], and a pre-defined dictionary of\n",
    "topic-keywords developed by the team\n",
    "The computation of semantic text similarity is performed after encoding the\n",
    "keywords with Sentence-BERT\n",
    "\n",
    "The first\n",
    "stream of methods include solutions such as a set of logical rules that map words to topics\n",
    "or comparison with a user defined taxonomy or ontology\n",
    "\n",
    "The long-standing Latent Semantic Analysis (LSA) [5] and Latent Dirichlet Allocation (LDA) [3] models are well\n",
    "suited to perform information reduction and exploratory analysis tasks\n",
    "    However, this unsupervised approach has a few drawbacks: topic models might be unstable when not optimized [1] and their outputs might often be difficult to understand [4], as each topic corresponds with a combination of words which need to be interpreted by the user\n",
    "    Another downside is the fact that the researcher needs to make assumptions on the number of topics to be retained from a certain collection\n",
    "\n",
    "    Some scholars tried to mitigate all these issues by developing semi-supervised models that include ”anchor words” [7], or using partial labeling strategies \n",
    "\n",
    "KeyBERT\n",
    "\n",
    "Create a keyword dict, I can start using word cloud for this to do 1 by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define a function to removing some noise word\n",
    "This will return a new column in dataframe the text that remove defined noise word'''\n",
    "\n",
    "def remove_noise_word(text,noise_words):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    # Remove noise words\n",
    "    filtered = [word for word in words if word.lower() not in noise_words]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define function to lematize can work parallel'''\n",
    "# Lemmatization function\n",
    "def lemma_texts_parallel(texts): #code is help by AI - Reference (4)\n",
    "    cleaned_texts_lemma = []\n",
    "    # Lemmatization with spaCy using parallel processing\n",
    "    for doc in nlp.pipe(texts, batch_size=50, n_process=4):  # Process in parallel\n",
    "        cleaned_tokens = [\n",
    "            token.lemma_ for token in doc\n",
    "        ]\n",
    "        cleaned_texts_lemma.append(\" \".join(cleaned_tokens))  # Join cleaned words back\n",
    "    \n",
    "    return pd.Series(cleaned_texts_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_words1 = {'word','text','python','string','sentence','model'}\n",
    "df['Title_Clean_No_Noise1'] = df['Title_Clean'].apply(lambda x: remove_noise_word(x, noise_words1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_generating(df['Title_Clean_No_Noise1'],\"No Noise Word 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_words2 = {'word','words','list','natural language','nlp','text','python','string','sentence','model'}\n",
    "df['Title_Clean_No_Noise2'] = df['Title_Clean'].apply(lambda x: remove_noise_word(x, noise_words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_generating(df['Title_Clean_No_Noise2'],\"No Noise Word 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemma_Title'] = lemma_texts_parallel(df['Title_Clean'].tolist(remove_noise_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_Clean_No_Noise2'].to_csv(\"title_clean_remove_noise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_Clean_No_Noise2'] = df['Title_Clean_No_Noise2'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword(text):\n",
    "    # Extract keywords using KeyBert; returns list of (keyword, score)\n",
    "    keyword_tuples = kw_model.extract_keywords(\n",
    "        text, \n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        use_maxsum=True, \n",
    "        nr_candidates=10, \n",
    "        top_n=5\n",
    "    )\n",
    "    # Extract only the keyword strings from the tuples\n",
    "    keywords = [kw for kw, score in keyword_tuples]\n",
    "    return \",\".join(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keywords_fromBert'] = df['Title_Clean_No_Noise2'].apply(extract_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['keywords_fromBert','cluster_kmean']].to_csv(\"keywords_fromBert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_category = (\n",
    "    df[['keywords_fromBert', 'cluster_kmean']]\n",
    "    .replace('', np.nan)    # Replace empty strings with NaN\n",
    "    .dropna(axis=0)        # Now drop rows that have NaN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_category['keywords_list'] = for_category['keywords_fromBert'].str.split(',') \n",
    "cluster_group = for_category['cluster_kmean'].unique()\n",
    "cluster_group.sort()\n",
    "\n",
    "pattern_dfs = []\n",
    "def get_frequency(data,min_support=0.01):\n",
    "    te = TransactionEncoder()\n",
    "    item_te = te.fit(data).transform(data)\n",
    "    df_items_1hot = pd.DataFrame(item_te, columns=te.columns_)\n",
    "    frequent_itemsets_apriori = apriori(df_items_1hot, min_support=min_support, use_colnames=True)\n",
    "    return frequent_itemsets_apriori\n",
    "\n",
    "for cluster in cluster_group:\n",
    "    cluster_data = for_category.loc[for_category['cluster_kmean']==cluster,'keywords_list']\n",
    "    pattern =  get_frequency(cluster_data)\n",
    "    pattern = pattern.loc[pattern['itemsets'].apply(lambda x:len(x) >= 2)]\n",
    "    # Add a new column for the cluster\n",
    "    pattern['cluster'] = cluster\n",
    "    pattern_dfs.append(pattern)\n",
    "    \n",
    "# Concatenate all clusters into one dataframe\n",
    "frequency_df = pd.concat(pattern_dfs, ignore_index=True)\n",
    "# Rename columns as required: score, pattern, and cluster\n",
    "frequency_df.rename(columns={'support': 'score', 'itemsets': 'pattern'}, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "5742a164-a69b-4040-8ee7-2f6136f84628",
       "rows": [
        [
         "0",
         "0.02566633761105627",
         "frozenset({'spacy', 'custom'})",
         "0"
        ],
        [
         "1",
         "0.013820335636722606",
         "frozenset({'spacy', 'dependency'})",
         "0"
        ],
        [
         "2",
         "0.01085883514313919",
         "frozenset({'doc', 'spacy'})",
         "0"
        ],
        [
         "3",
         "0.01085883514313919",
         "frozenset({'spacy', 'document'})",
         "0"
        ],
        [
         "4",
         "0.01085883514313919",
         "frozenset({'entities', 'ner'})",
         "0"
        ],
        [
         "5",
         "0.02171767028627838",
         "frozenset({'entities', 'spacy'})",
         "0"
        ],
        [
         "6",
         "0.02073050345508391",
         "frozenset({'spacy', 'entity'})",
         "0"
        ],
        [
         "7",
         "0.017769002961500493",
         "frozenset({'error', 'spacy'})",
         "0"
        ],
        [
         "8",
         "0.01678183613030602",
         "frozenset({'extract', 'spacy'})",
         "0"
        ],
        [
         "9",
         "0.013820335636722606",
         "frozenset({'lemmatization', 'spacy'})",
         "0"
        ],
        [
         "10",
         "0.027640671273445213",
         "frozenset({'matcher', 'spacy'})",
         "0"
        ],
        [
         "11",
         "0.014807502467917079",
         "frozenset({'named', 'spacy'})",
         "0"
        ],
        [
         "12",
         "0.055281342546890426",
         "frozenset({'ner', 'spacy'})",
         "0"
        ],
        [
         "13",
         "0.01085883514313919",
         "frozenset({'training', 'ner'})",
         "0"
        ],
        [
         "14",
         "0.012833168805528134",
         "frozenset({'pos', 'spacy'})",
         "0"
        ],
        [
         "15",
         "0.01085883514313919",
         "frozenset({'possible', 'spacy'})",
         "0"
        ],
        [
         "16",
         "0.012833168805528134",
         "frozenset({'tag', 'spacy'})",
         "0"
        ],
        [
         "17",
         "0.014807502467917079",
         "frozenset({'token', 'spacy'})",
         "0"
        ],
        [
         "18",
         "0.012833168805528134",
         "frozenset({'tokenizer', 'spacy'})",
         "0"
        ],
        [
         "19",
         "0.014807502467917079",
         "frozenset({'train', 'spacy'})",
         "0"
        ],
        [
         "20",
         "0.028627838104639685",
         "frozenset({'training', 'spacy'})",
         "0"
        ],
        [
         "21",
         "0.013820335636722606",
         "frozenset({'use', 'spacy'})",
         "0"
        ],
        [
         "22",
         "0.044422507403751234",
         "frozenset({'using', 'spacy'})",
         "0"
        ],
        [
         "23",
         "0.018756169792694965",
         "frozenset({'spacy', 'using spacy'})",
         "0"
        ],
        [
         "24",
         "0.01085883514313919",
         "frozenset({'working', 'spacy'})",
         "0"
        ],
        [
         "25",
         "0.02171767028627838",
         "frozenset({'using', 'using spacy'})",
         "0"
        ],
        [
         "26",
         "0.014807502467917079",
         "frozenset({'using', 'spacy', 'using spacy'})",
         "0"
        ],
        [
         "27",
         "0.032817627754336616",
         "frozenset({'corenlp', 'stanford'})",
         "1"
        ],
        [
         "28",
         "0.010314111579934365",
         "frozenset({'dependency', 'stanford'})",
         "1"
        ],
        [
         "29",
         "0.014064697609001406",
         "frozenset({'using', 'nltk'})",
         "1"
        ],
        [
         "30",
         "0.03375527426160337",
         "frozenset({'parser', 'stanford'})",
         "1"
        ],
        [
         "31",
         "0.010782934833567745",
         "frozenset({'pos', 'tagger'})",
         "1"
        ],
        [
         "32",
         "0.015471167369901548",
         "frozenset({'pos', 'tagging'})",
         "1"
        ],
        [
         "33",
         "0.015471167369901548",
         "frozenset({'using', 'stanford'})",
         "1"
        ],
        [
         "34",
         "0.015939990623534926",
         "frozenset({'using nltk', 'using'})",
         "1"
        ],
        [
         "35",
         "0.013600572655690766",
         "frozenset({'attributeerror', 'attribute'})",
         "2"
        ],
        [
         "36",
         "0.010021474588403722",
         "frozenset({'module', 'attribute'})",
         "2"
        ],
        [
         "37",
         "0.015032211882605583",
         "frozenset({'attributeerror', 'object'})",
         "2"
        ],
        [
         "38",
         "0.01145311381531854",
         "frozenset({'module', 'named'})",
         "2"
        ],
        [
         "39",
         "0.01643535427319211",
         "frozenset({'sentiment', 'analysis'})",
         "3"
        ],
        [
         "40",
         "0.013878743608473338",
         "frozenset({'analysis', 'sentiment analysis'})",
         "3"
        ],
        [
         "41",
         "0.01241782322863404",
         "frozenset({'named', 'entity'})",
         "3"
        ],
        [
         "42",
         "0.01095690284879474",
         "frozenset({'entity', 'recognition'})",
         "3"
        ],
        [
         "43",
         "0.02885317750182615",
         "frozenset({'language', 'natural'})",
         "3"
        ],
        [
         "44",
         "0.014609203798392988",
         "frozenset({'processing', 'language'})",
         "3"
        ],
        [
         "45",
         "0.01643535427319211",
         "frozenset({'language processing', 'natural'})",
         "3"
        ],
        [
         "46",
         "0.01643535427319211",
         "frozenset({'natural', 'natural language'})",
         "3"
        ],
        [
         "47",
         "0.02008765522279036",
         "frozenset({'processing', 'natural'})",
         "3"
        ],
        [
         "48",
         "0.01095690284879474",
         "frozenset({'sentiment', 'sentiment analysis'})",
         "3"
        ],
        [
         "49",
         "0.01168736303871439",
         "frozenset({'processing', 'language', 'natural'})",
         "3"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 80
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025666</td>\n",
       "      <td>(spacy, custom)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013820</td>\n",
       "      <td>(spacy, dependency)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010859</td>\n",
       "      <td>(doc, spacy)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010859</td>\n",
       "      <td>(spacy, document)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010859</td>\n",
       "      <td>(entities, ner)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.010270</td>\n",
       "      <td>(using word2vec, word2vec)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.019255</td>\n",
       "      <td>(vector, word2vec)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.015404</td>\n",
       "      <td>(vectors, word2vec)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.010270</td>\n",
       "      <td>(using word2vec, using, word2vec)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.010855</td>\n",
       "      <td>(embeddings, bert)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score                            pattern  cluster\n",
       "0   0.025666                    (spacy, custom)        0\n",
       "1   0.013820                (spacy, dependency)        0\n",
       "2   0.010859                       (doc, spacy)        0\n",
       "3   0.010859                  (spacy, document)        0\n",
       "4   0.010859                    (entities, ner)        0\n",
       "..       ...                                ...      ...\n",
       "75  0.010270         (using word2vec, word2vec)        5\n",
       "76  0.019255                 (vector, word2vec)        5\n",
       "77  0.015404                (vectors, word2vec)        5\n",
       "78  0.010270  (using word2vec, using, word2vec)        5\n",
       "79  0.010855                 (embeddings, bert)        7\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and compare with BERT topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the BERTopic model\n",
    "docs = df['Title_Clean_No_Noise2'].tolist()\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Representation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Representative_Docs",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4e5d126d-59ac-416d-bfeb-d2c4fab9068c",
       "rows": [
        [
         "0",
         "-1",
         "4872",
         "-1_find_nltk_language_file",
         "['find', 'nltk', 'language', 'file', 'sentences', 'error', 'using', 'corpus', 'similar', 'use']",
         "['identify strings two different lists', 'compare context two different paragraph using machine learning', 'spacy create new language data corpus']"
        ],
        [
         "1",
         "0",
         "411",
         "0_pandas_dataframe_column_frame",
         "['pandas', 'dataframe', 'column', 'frame', 'columns', 'rows', 'row', 'df', 'apply', 'dataframes']",
         "['implement function pandas dataframe column', 'pandas data frame make columns pandas', 'way create tfidf column pandas dataframe']"
        ],
        [
         "2",
         "1",
         "344",
         "1_bert_bertmodel_embeddings_fine",
         "['bert', 'bertmodel', 'embeddings', 'fine', 'finetuning', 'bertformaskedlm', 'berts', 'pretrained', 'finetuned', 'finetune']",
         "['language training bert', 'bert classification', 'training bert using bert embeddings']"
        ],
        [
         "3",
         "2",
         "285",
         "2_sentiment_analysis_reviews_aspect",
         "['sentiment', 'analysis', 'reviews', 'aspect', 'negative', 'emotion', 'polarity', 'sentiments', 'twitter', 'positive']",
         "['sentiment analysis object attribute sentiment', 'sentiment analysis using nltk', 'training data sentiment analysis']"
        ],
        [
         "4",
         "3",
         "262",
         "3_word2vec_text2vec_vectors_pretrained",
         "['word2vec', 'text2vec', 'vectors', 'pretrained', 'vector', 'representation', 'wordembedding', 'embeddings', 'embedding', 'sense2vec']",
         "['using word2vec embedding sentences', 'using word2vec', 'using word2vec extract data']"
        ],
        [
         "5",
         "4",
         "226",
         "4_tfidf_tfidfvectorizer_vectorizer_idf",
         "['tfidf', 'tfidfvectorizer', 'vectorizer', 'idf', 'tf', 'sklearn', 'scikit', 'scores', 'scikitlearn', 'calculate']",
         "['document corpus tfidf', 'tfidf value matching output tfidfvectorizer', 'use tfidf classification']"
        ],
        [
         "6",
         "5",
         "164",
         "5_tensorflow_tensor_tensors_tensorflow2",
         "['tensorflow', 'tensor', 'tensors', 'tensorflow2', 'dtype', 'invalidargumenterror', 'gradients', 'tensorflows', 'shape', 'saved']",
         "['slice tensorflow tensor multiple', 'convert tensor written tensor', 'embedding tensorflow language']"
        ],
        [
         "7",
         "6",
         "163",
         "6_tm_package_characters_column",
         "['tm', 'package', 'characters', 'column', 'occur', 'select', 'commas', 'vector', 'rs', 'udpipe']",
         "['find 2 phrase using tm r', 'searching specific corpus r tm package', 'r tm package german']"
        ],
        [
         "8",
         "7",
         "160",
         "7_huggingface_hugging_face_transformers",
         "['huggingface', 'hugging', 'face', 'transformers', 'trainer', 'huggingfaces', 'models', 'hugginface', 'tokenizer', 'pretrained']",
         "['loss function used trainer transformers library hugging face', 'get size hugging face pretrained', 'using huggingface transformers trainer method hugging face datasets']"
        ],
        [
         "9",
         "8",
         "149",
         "8_tokenize_tokenizer_tokenizing_tokens",
         "['tokenize', 'tokenizer', 'tokenizing', 'tokens', 'tokenizers', 'tokenization', 'token', 'tokenized', 'lexer', 'tokenising']",
         "['tokenize using', 'tokenize', 'tokenizing tokenize punctuation like']"
        ],
        [
         "10",
         "9",
         "145",
         "9_opennlp_apache_simplenlg_finder",
         "['opennlp', 'apache', 'simplenlg', 'finder', 'opennlps', 'sentencedetector', 'conduct', 'nodenlp', 'eclipse', 'class']",
         "['using tokenizer opennlp', 'opennlp parser training', 'use opennlp java']"
        ],
        [
         "11",
         "10",
         "140",
         "10_embedding_embeddings_oov_layer",
         "['embedding', 'embeddings', 'oov', 'layer', 'encoder', 'universal', 'pretrained', 'tsne', 'vector', 'vectors']",
         "['difference embedding embedding', 'embedding layer embeddings', 'embedding']"
        ],
        [
         "12",
         "11",
         "136",
         "11_information_unstructured_extraction_extract",
         "['information', 'unstructured', 'extraction', 'extract', 'extracting', 'medical', 'product', 'structured', 'image', 'area']",
         "['information extraction', 'information extraction', 'extracting information unstructured']"
        ],
        [
         "13",
         "12",
         "130",
         "12_topic_lda_modeling_topics",
         "['topic', 'lda', 'modeling', 'topics', 'modelling', 'dirichlet', 'allocation', 'mallet', 'latent', 'bertopic']",
         "['topic modeling using gensim', 'topic modelling using lda', 'lda topic modeling']"
        ],
        [
         "14",
         "13",
         "122",
         "13_corenlp_stanfordnlp_stanfordcorenlp_stanford",
         "['corenlp', 'stanfordnlp', 'stanfordcorenlp', 'stanford', 'server', 'openie', 'stanfords', 'edustanfordnlptimetimeexpressionextractorimpl', 'throwing', 'overhead']",
         "['difference stanford corenlp stanford ner', 'stanford corenlp output', 'using stanford corenlp']"
        ],
        [
         "15",
         "14",
         "115",
         "14_dictionary_dictionaries_keys_values",
         "['dictionary', 'dictionaries', 'keys', 'values', 'dict', 'nested', 'key', 'dictionarys', 'fuzzily', 'create']",
         "['extract dictionary', 'data dictionary', 'dictionary']"
        ],
        [
         "16",
         "15",
         "112",
         "15_transformer_transformers_peft_automodelforsequenceclassification",
         "['transformer', 'transformers', 'peft', 'automodelforsequenceclassification', 'decoder', 'automodelforcausallm', 'automodelforseq2seqlm', 'automodel', 'tfbertforsequenceclassification', 'adapters']",
         "['error loading transformer', 'using pretrained transformer keras', 'dataset transformer']"
        ],
        [
         "17",
         "16",
         "107",
         "16_ngrams_ngram_viewer_google",
         "['ngrams', 'ngram', 'viewer', 'google', 'increases', 'efficiently', 'generator', 'distinct', 'scala', 'next']",
         "['train language using google ngrams', 'ngrams', 'ngrams']"
        ],
        [
         "18",
         "17",
         "102",
         "17_chatbot_bot_witai_chat",
         "['chatbot', 'bot', 'witai', 'chat', 'rasa', 'messenger', 'chatscript', 'conversation', 'conversational', 'dialogflow']",
         "['handle context chatbot', 'mining chatbot', 'training chatbot']"
        ],
        [
         "19",
         "18",
         "102",
         "18_regex_regular_expression_expressions",
         "['regex', 'regular', 'expression', 'expressions', 'match', 'pattern', 'followed', 'left', 'compiled', 'preceded']",
         "['find using regular expression', 'regular expression', 'extract using regex']"
        ],
        [
         "20",
         "19",
         "101",
         "19_characters_spaces_letters_remove",
         "['characters', 'spaces', 'letters', 'remove', 'special', 'space', 'removing', 'character', 'white', 'alphabet']",
         "['remove special characters except n', 'remove special characters analysis', 'function remove special character space']"
        ],
        [
         "21",
         "20",
         "101",
         "20_keywords_keyword_extraction_search",
         "['keywords', 'keyword', 'extraction', 'search', 'keybert', 'key', 'extract', 'rake', 'recommendation', 'relevant']",
         "['java library keywords extraction input', 'extract keywords tags', 'data extraction keywords']"
        ],
        [
         "22",
         "21",
         "98",
         "21_tokenizer_token_spacy_tokens",
         "['tokenizer', 'token', 'spacy', 'tokens', 'tokenization', 'spacytokenstokentoken', 'incorrect', 'str', 'untokenize', 'spacytokensdocdoc']",
         "['make spacy tokenizer split', 'training tokenizer spacy', 'spacy tokenizer add tokenizer exception']"
        ],
        [
         "23",
         "22",
         "97",
         "22_fasttext_pretrained_getsentencevector_fasttexts",
         "['fasttext', 'pretrained', 'getsentencevector', 'fasttexts', 'gensim', 'load', 'opened', 'textlmdatabunch', 'vec', 'trainsupervised']",
         "['loading pretrained fasttext gensim', 'fasttext using pretrained vector classification', 'convert pretrained fasttext vectors gensim']"
        ],
        [
         "24",
         "23",
         "94",
         "23_count_frequency_frequencies_counting",
         "['count', 'frequency', 'frequencies', 'counting', 'occurrences', 'number', 'times', 'row', 'timer', 'cells']",
         "['counting frequency', 'count frequency within another', 'frequency time count frequency date']"
        ],
        [
         "25",
         "24",
         "93",
         "24_date_dates_time_datetime",
         "['date', 'dates', 'time', 'datetime', 'temporal', 'relative', 'sutime', 'natural', 'year', 'timezone']",
         "['date extraction', 'natural language date time parser java', 'date parsing']"
        ],
        [
         "26",
         "25",
         "92",
         "25_recognition_named_entity_lingpipe",
         "['recognition', 'named', 'entity', 'lingpipe', 'amc', 'namedentity', 'recognizer', 'name', 'entities', 'tools']",
         "['named entity recognition', 'named entity recognition', 'named entity recognition']"
        ],
        [
         "27",
         "26",
         "86",
         "26_matcher_rulebased_rule_matching",
         "['matcher', 'rulebased', 'rule', 'matching', 'pattern', 'spacy', 'phrasematcher', 'matches', 'match', 'patterns']",
         "['spacy matcher find tokens matching custom attribute', 'spacy matcher pattern regex tag', 'spacy custom rule matcher']"
        ],
        [
         "28",
         "27",
         "85",
         "27_pos_tagging_tag_tagger",
         "['pos', 'tagging', 'tag', 'tagger', 'tags', 'frequent', 'creating', 'incorrect', 'partofspeechtagging', 'baumwelch']",
         "['pos tagging', 'pos tagging', 'pos tagging']"
        ],
        [
         "29",
         "28",
         "84",
         "28_doc2vec_vectors_pvdbow_document",
         "['doc2vec', 'vectors', 'pvdbow', 'document', 'doc', 'verses', 'taggeddocument', 'paragraph', 'buildvocab', 'predictions']",
         "['doc2vec tensorflow', 'dataset doc2vec', 'document similarity doc2vec']"
        ],
        [
         "30",
         "29",
         "83",
         "29_pytorch_rnn_theano_loss",
         "['pytorch', 'rnn', 'theano', 'loss', 'bidirectional', 'dataloader', 'batchsize', 'pytorchs', 'gru', 'final']",
         "['getting hidden size error pytorch rnn', 'bidirectional rnn implementation pytorch', 'pytorch rnn html generation']"
        ],
        [
         "31",
         "30",
         "81",
         "30_install_version_installing_spacy",
         "['install', 'version', 'installing', 'spacy', 'anaconda', 'import', 'en', 'conda', 'loading', 'download']",
         "['unable install spacy anaconda', 'install issue spacy package anaconda environment', 'install specific version spacy']"
        ],
        [
         "32",
         "31",
         "81",
         "31_noun_spacy_verb_phrases",
         "['noun', 'spacy', 'verb', 'phrases', 'spacys', 'textcatmultilabel', 'spacypython', 'nounchunks', 'phrase', 'chunks']",
         "['spacy noun phrases locate noun phrase span start end token every nounchunk doc spacy', 'spacy extract specific noun phrase', 'get noun phrases spacy']"
        ],
        [
         "33",
         "32",
         "79",
         "32_tweets_twitter_tweet_tweepy",
         "['tweets', 'twitter', 'tweet', 'tweepy', 'trending', 'def', 'twitters', 'retweets', 'location', 'categorize']",
         "['trying find name specific location tweets', 'processing tool tweets', 'extracting ngrams tweets']"
        ],
        [
         "34",
         "33",
         "78",
         "33_grammar_parsing_parser_free",
         "['grammar', 'parsing', 'parser', 'free', 'earley', 'parse', 'cfg', 'grammars', 'productions', 'formal']",
         "['parsing sentences', 'english grammar parsing php link grammar', 'file parsing grammar']"
        ],
        [
         "35",
         "34",
         "76",
         "34_countvectorizer_sklearns_countvectorizers_sklearn",
         "['countvectorizer', 'sklearns', 'countvectorizers', 'sklearn', 'vocabulary', 'notfittederror', 'wasnt', 'fitted', 'dictvectorizer', 'vectorizer']",
         "['set custom stop sklearn countvectorizer', 'sklearn countvectorizer custom vocabulary', 'classification countvectorizer shape error']"
        ],
        [
         "36",
         "35",
         "76",
         "35_scikitlearn_scikit_svc_sklearn",
         "['scikitlearn', 'scikit', 'svc', 'sklearn', 'learn', 'svm', 'featureunion', 'features', 'expecting', 'pipeline']",
         "['wrong prediction svc classifier scikitlearn', 'use strings training data svm using scikitlearn', 'classification scikitlearn large dataset']"
        ],
        [
         "37",
         "36",
         "76",
         "36_lemmatizer_lemmatization_lemma_lemmatize",
         "['lemmatizer', 'lemmatization', 'lemma', 'lemmatize', 'lemmas', 'wordnet', 'lemmatizing', 'lemmatizers', 'machinelearning', 'lemmatized']",
         "['getting root using wordnet lemmatizer', 'lemmatizer r', 'wordnet lemmatizer r']"
        ],
        [
         "38",
         "37",
         "75",
         "37_spell_correction_spelling_checker",
         "['spell', 'correction', 'spelling', 'checker', 'typos', 'misspelled', 'checking', 'misspellings', 'spellcheck', 'autocorrect']",
         "['spell check andor spell correction java', 'spell checker fused spelling error correction algorithm', 'multiple spelling correction']"
        ],
        [
         "39",
         "38",
         "74",
         "38_names_person_name_company",
         "['names', 'person', 'name', 'company', 'persons', 'human', 'organization', 'identify', 'companies', 'distinguish']",
         "['distinguish persons names organization names structured table column', 'extracting person names named entity recognition using', 'identify names']"
        ],
        [
         "40",
         "39",
         "73",
         "39_processing_natural_language_someone",
         "['processing', 'natural', 'language', 'someone', 'statistical', 'algorithms', 'truecaser', 'dependancy', 'qualitative', 'processingsyntatcticsemanticprogmatic']",
         "['natural language processing', 'natural language processing', 'natural language processing']"
        ],
        [
         "41",
         "40",
         "73",
         "40_html_scrape_scraping_web",
         "['html', 'scrape', 'scraping', 'web', 'page', 'webpage', 'beautifulsoup', 'links', 'http', 'url']",
         "['im trying scrape frequent web page filter stop', 'web scraping amazon website giving http error', 'web scraping getting data']"
        ],
        [
         "42",
         "41",
         "69",
         "41_semantic_similarity_sentences_analogy",
         "['semantic', 'similarity', 'sentences', 'analogy', 'two', 'compare', 'meaning', 'similar', 'phrases', 'appropriate']",
         "['find semantic meaning similarity two', 'find semantic similarity 2 sentences', 'semantic similarity sentences']"
        ],
        [
         "43",
         "42",
         "68",
         "42_question_questions_answering_answer",
         "['question', 'questions', 'answering', 'answer', 'answers', 'yesno', 'choice', 'yes', 'interrogative', 'gpt3']",
         "['generate answer questions using', 'question answering dataset multiple answers', 'questions generation question answering']"
        ],
        [
         "44",
         "43",
         "68",
         "43_typeerror_callable_init_iterable",
         "['typeerror', 'callable', 'init', 'iterable', 'argument', 'str', 'keyerror', 'unpack', 'enough', 'object']",
         "['typeerror init got unexpected keyword argument numsamples', 'typeerror init got multiple values keyword argument encoding', 'typeerror object callable']"
        ],
        [
         "45",
         "44",
         "67",
         "44_gensim_word2vec_epoch_vocabulary",
         "['gensim', 'word2vec', 'epoch', 'vocabulary', 'saveword2vecformat', 'iteratively', 'amount', 'word2vecs', 'mostsimilar', 'gensims']",
         "['different models gensim word2vec', 'gensim word2vec find number vocabulary', 'gensim word2vec training data']"
        ],
        [
         "46",
         "45",
         "66",
         "45_pdf_pdfminer_pdfs_page",
         "['pdf', 'pdfminer', 'pdfs', 'page', 'reading', 'images', 'pdfplumber', 'scanned', 'contents', 'extracting']",
         "['extract specific value pdf using', 'extract two column pdf', 'get data pdf']"
        ],
        [
         "47",
         "46",
         "65",
         "46_classification_learning_classify_machine",
         "['classification', 'learning', 'classify', 'machine', 'reinforcement', 'binary', 'examples', 'perceptron', 'classifier', 'approach']",
         "['classification', 'classification', 'machine learning classification use']"
        ],
        [
         "48",
         "47",
         "65",
         "47_tagger_pos_tagging_nltk",
         "['tagger', 'pos', 'tagging', 'nltk', 'tags', 'tagset', 'nltagger', 'stts', 'otherword', 'postag']",
         "['training tagger custom tags nltk', 'pos tagger without nltk', 'nltk language pos tagger']"
        ],
        [
         "49",
         "48",
         "63",
         "48_cleaning_preprocessing_cloud_clean",
         "['cleaning', 'preprocessing', 'cloud', 'clean', 'amounts', 'data', 'dawg', 'dataset', 'store', 'processed']",
         "['cleaning', 'cleaning', 'cleaning dataset']"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 275
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>4872</td>\n",
       "      <td>-1_find_nltk_language_file</td>\n",
       "      <td>[find, nltk, language, file, sentences, error,...</td>\n",
       "      <td>[identify strings two different lists, compare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>0_pandas_dataframe_column_frame</td>\n",
       "      <td>[pandas, dataframe, column, frame, columns, ro...</td>\n",
       "      <td>[implement function pandas dataframe column, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>344</td>\n",
       "      <td>1_bert_bertmodel_embeddings_fine</td>\n",
       "      <td>[bert, bertmodel, embeddings, fine, finetuning...</td>\n",
       "      <td>[language training bert, bert classification, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>285</td>\n",
       "      <td>2_sentiment_analysis_reviews_aspect</td>\n",
       "      <td>[sentiment, analysis, reviews, aspect, negativ...</td>\n",
       "      <td>[sentiment analysis object attribute sentiment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>262</td>\n",
       "      <td>3_word2vec_text2vec_vectors_pretrained</td>\n",
       "      <td>[word2vec, text2vec, vectors, pretrained, vect...</td>\n",
       "      <td>[using word2vec embedding sentences, using wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>269</td>\n",
       "      <td>10</td>\n",
       "      <td>269_ner_rulers_identity_types</td>\n",
       "      <td>[ner, rulers, identity, types, entities, overa...</td>\n",
       "      <td>[large difference overall f score custom spacy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>270_layoutlm_javamodel_falcon7b40b_simpletrans...</td>\n",
       "      <td>[layoutlm, javamodel, falcon7b40b, simpletrans...</td>\n",
       "      <td>[prepare custom training data layoutlm, input ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>271</td>\n",
       "      <td>10</td>\n",
       "      <td>271_java_classifiers_svmhmm_program</td>\n",
       "      <td>[java, classifiers, svmhmm, program, virtual, ...</td>\n",
       "      <td>[classification java, using multiple classifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>272</td>\n",
       "      <td>10</td>\n",
       "      <td>272_cuda_colab_usecuda_torchoutofmemoryerror</td>\n",
       "      <td>[cuda, colab, usecuda, torchoutofmemoryerror, ...</td>\n",
       "      <td>[running process error saying cuda memory, err...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>273</td>\n",
       "      <td>10</td>\n",
       "      <td>273_glove_gloveword2vec_keyedvectorssave_glove...</td>\n",
       "      <td>[glove, gloveword2vec, keyedvectorssave, glove...</td>\n",
       "      <td>[adding additional word2vec glove maybe using ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1   4872                         -1_find_nltk_language_file   \n",
       "1        0    411                    0_pandas_dataframe_column_frame   \n",
       "2        1    344                   1_bert_bertmodel_embeddings_fine   \n",
       "3        2    285                2_sentiment_analysis_reviews_aspect   \n",
       "4        3    262             3_word2vec_text2vec_vectors_pretrained   \n",
       "..     ...    ...                                                ...   \n",
       "270    269     10                      269_ner_rulers_identity_types   \n",
       "271    270     10  270_layoutlm_javamodel_falcon7b40b_simpletrans...   \n",
       "272    271     10                271_java_classifiers_svmhmm_program   \n",
       "273    272     10       272_cuda_colab_usecuda_torchoutofmemoryerror   \n",
       "274    273     10  273_glove_gloveword2vec_keyedvectorssave_glove...   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [find, nltk, language, file, sentences, error,...   \n",
       "1    [pandas, dataframe, column, frame, columns, ro...   \n",
       "2    [bert, bertmodel, embeddings, fine, finetuning...   \n",
       "3    [sentiment, analysis, reviews, aspect, negativ...   \n",
       "4    [word2vec, text2vec, vectors, pretrained, vect...   \n",
       "..                                                 ...   \n",
       "270  [ner, rulers, identity, types, entities, overa...   \n",
       "271  [layoutlm, javamodel, falcon7b40b, simpletrans...   \n",
       "272  [java, classifiers, svmhmm, program, virtual, ...   \n",
       "273  [cuda, colab, usecuda, torchoutofmemoryerror, ...   \n",
       "274  [glove, gloveword2vec, keyedvectorssave, glove...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [identify strings two different lists, compare...  \n",
       "1    [implement function pandas dataframe column, p...  \n",
       "2    [language training bert, bert classification, ...  \n",
       "3    [sentiment analysis object attribute sentiment...  \n",
       "4    [using word2vec embedding sentences, using wor...  \n",
       "..                                                 ...  \n",
       "270  [large difference overall f score custom spacy...  \n",
       "271  [prepare custom training data layoutlm, input ...  \n",
       "272  [classification java, using multiple classifie...  \n",
       "273  [running process error saying cuda memory, err...  \n",
       "274  [adding additional word2vec glove maybe using ...  \n",
       "\n",
       "[275 rows x 5 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a summary of topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "-https://medium.com/@davidlfliang/intro-getting-started-with-text-embeddings-using-bert-9f8c3b98dee6\n",
    "- https://www.sciencedirect.com/science/article/pii/S1877050922008766"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between the two algorithms:\n",
    "\n",
    "DBSCAN is a density-based clustering algorithm, whereas K-Means is a centroid-based clustering algorithm.\n",
    "DBSCAN can discover clusters of arbitrary shapes, whereas K-Means assumes that the clusters are spherical.\n",
    "DBSCAN does not require the number of clusters to be specified in advance, whereas K-Means requires the number of clusters to be specified.\n",
    "DBSCAN is less sensitive to initialization than K-Means.\n",
    "When to use DBSCAN vs. K-Means?\n",
    "\n",
    "Use DBSCAN when the data has irregular shapes or when there is no prior knowledge about the number of clusters.\n",
    "Use K-Means when the data has spherical shapes and when the number of clusters is known beforehand.\n",
    "If you are unsure which algorithm to use, it is always a good idea to try both algorithms and compare their results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
