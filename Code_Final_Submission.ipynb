{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoang\\anaconda3\\envs\\GPU_TF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:246: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n",
      "c:\\Users\\hoang\\anaconda3\\envs\\GPU_TF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n"
     ]
    }
   ],
   "source": [
    "#Import Lib\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\n",
    "\n",
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "\n",
    "color_pal = sns.color_palette()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_embedding = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use StackExchange Query function to get data. It's an SQL syntax.\n",
    "```sql\n",
    "SELECT TOP 25000\n",
    "  q.Id                     AS QuestionId,\n",
    "  CONCAT('https://stackoverflow.com/questions/', q.Id) AS QuestionUrl,\n",
    "  q.Title,\n",
    "  q.Body,\n",
    "  q.CreationDate,\n",
    "  q.Tags,\n",
    "  q.Score,\n",
    "  q.ViewCount,\n",
    "  q.AnswerCount,\n",
    "  a.Id                     AS AcceptedAnswerId,\n",
    "  a.Body                   AS AcceptedAnswerBody,\n",
    "  a.Score                  AS AcceptedAnswerScore\n",
    "FROM Posts q\n",
    "  JOIN PostTags pt\n",
    "    ON pt.PostId = q.Id\n",
    "  JOIN Tags t\n",
    "    ON t.Id = pt.TagId\n",
    "   AND t.TagName = 'nlp'\n",
    "  LEFT JOIN Posts a \n",
    "    ON q.AcceptedAnswerId = a.Id\n",
    "ORDER BY q.CreationDate DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "AcceptedAnswerScore",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bc77f3a4-371d-40d9-b425-447ec6764bbe",
       "rows": [
        [
         "0",
         "79577043",
         "https://stackoverflow.com/questions/79577043",
         "Unsupervised Topic Modeling for Short Event Descriptions",
         "<p>I have a dataset of approximately 750 lines containing quite short texts (less than 150 words each). These are all event descriptions related to a single broad topic (which I cannot specify for anonymity reasons).</p>\n<p>I want to apply unsupervised topic modeling methods to get more precise and non-exclusive categories/tags for each text. My goal is to implement a multi-label approach where each text can belong to multiple categories simultaneously. Ideally, I'd like to identify around 10-20 relevant topics.</p>\n<p>So far, I've tried:</p>\n<ul>\n<li>BERTopic: Since it relies on clustering, I end up with each text belonging to a single cluster, which is not what I want.</li>\n<li>KeyBERT + BERTopic: I first used KeyBERT to extract keywords from each text and then applied BERTopic to get non-exclusive categories. This gave me the best results so far, but I'm still not completely convinced.</li>\n</ul>\n<p>I haven't tried LDA as I consider it a bit outdated and expected disappointing results for such short texts.</p>\n",
         "2025-04-16",
         "machine-learning,nlp,topic-modeling",
         "-1",
         "34",
         "1",
         null,
         null,
         null
        ],
        [
         "1",
         "79574001",
         "https://stackoverflow.com/questions/79574001",
         "Is there a way to reuse a heavy service across tasks in Airflow?",
         "<p>I'm building an Airflow DAG where some of the steps should do ML/NLP processing.</p>\n<p>I have a service class that loads NLP model in constructor. E.g.:</p>\n<pre><code>class SentenceService:\n    def __init__(self, model: str = &quot;abc&quot;):\n        self.nlp = spacy.load(model)\n</code></pre>\n<p>I'd like to reuse this service instance so that the tasks execute faster. Some of the models are very heavy and take disproportional amount of time to load.</p>\n<p>I'm using <code>processed_texts = process_text.expand(texts=texts)</code> to create task for each text to process so that I can distribute the processing horizontally across multiple workers.</p>\n<p>What I tried is to instantiate the service globally in the DAG - as a module-level/top-level variable)\nAccording to the docs, this seems not advisable anyway (<a href=\"https://airflow.apache.org/docs/apache-airflow/2.6.0/best-practices.html#top-level-python-code\" rel=\"nofollow noreferrer\">https://airflow.apache.org/docs/apache-airflow/2.6.0/best-practices.html#top-level-python-code</a>).</p>\n<p>I also tried to implement typical Python-ish singleton for the service, so that whenever it's used in the tasks, same instance would be returned.</p>\n<p>I also understand that if I <strong>won't</strong> run the processing in parallel (<code>.expand</code> above), I can have one service per batch - but only on one worker.</p>\n<p>None of that led to what I want - quite obviously to me now :), tasks execute separately so DAG &quot;runs fresh again&quot;, so it makes sense. Neither the global variable, nor Singleton is shared across processes.</p>\n<p>Is there any technique to handle this in Airflow?</p>\n",
         "2025-04-14",
         "nlp,airflow,microservices",
         "1",
         "19",
         "0",
         null,
         null,
         null
        ],
        [
         "2",
         "79572620",
         "https://stackoverflow.com/questions/79572620",
         "How can I link tasks using machine learning / ai based on historical task sequences?",
         "<p>I'm working on an AI model to predict dependency links between tasks for industrial plannifications, based on historical project data. I have two tables:</p>\n<p>Tasks Table:\nTaskID, TaskName, EquipmentType\n(I'm considering adding StartDate and EndDate)</p>\n<p>Dependencies Table:\nPredecessorTaskID, SuccessorTaskID, LinkType</p>\n<p><strong>Goal:</strong>\nGiven a new list of tasks (typically filtered by EquipmentType), I want the model to suggest likely dependencies between them (and eventually, the LinkType) — learned from historical patterns in the existing data.</p>\n<p><strong>What I’ve tried:</strong></p>\n<p>Decision Trees</p>\n<p>Basic Neural Networks</p>\n<p><strong>Problems encountered:</strong></p>\n<p>Random or irrelevant links</p>\n<p>Models predicting dependencies between all tasks</p>\n<p>Lack of logical flow learned from historical data</p>\n<p>I'm pretty sure i am not pre-processing the data correctly as i'm not sur how to treat the tasks name for it to recognize the &quot;pattern&quot;</p>\n<p><strong>My Question:</strong>\nWould it make sense to frame this as a graph problem and use Graph Neural Networks (GNNs)? Or is there a better ML or statistical approach for modeling and predicting dependencies between tasks in this kind of scenario?</p>\n<p>I'm open to advice on model architecture or data pre-processing strategies that might improve performance.</p>\n",
         "2025-04-14",
         "python,machine-learning,nlp,artificial-intelligence",
         "-6",
         "95",
         "0",
         null,
         null,
         null
        ],
        [
         "3",
         "79559702",
         "https://stackoverflow.com/questions/79559702",
         "NameError: name 'init_empty_weights' is not defined while using hugging face models",
         "<p>I am trying to set up hugging face locally and im running into this issue.</p>\n<pre><code>NameError: name 'init_empty_weights' is not defined\n</code></pre>\n<p>Here is the code I have tested my installation with</p>\n<pre><code>from transformers import pipeline\nclassifier = pipeline(&quot;sentiment-analysis&quot;)\ntext = &quot;I love using Hugging Face Transformers!&quot;\nresult = classifier(text)\nprint(result)\n\n\n</code></pre>\n<p>transformers: 4.51.0 <br>\ntokenizers: 0.21.1 <br>\naccelerate: 1.6.0 <br>\nsentence-transformers: 4.0.2 <br>\nhuggingface_hub: 0.30.1 <br>\nI am currently using  pytorch-metal mac M3 pro.<br></p>\n<p>What causes this, and how can I fix it?</p>\n",
         "2025-04-07",
         "nlp,huggingface-transformers,huggingface",
         "3",
         "629",
         "2",
         "79577000.0",
         "<p>Try using this version, it should resolve the issue.</p>\n<pre><code>transformers==4.50.3\n</code></pre>\n",
         "1.0"
        ],
        [
         "4",
         "79557354",
         "https://stackoverflow.com/questions/79557354",
         "Sentencepiece not generating models after preprocessing (SOLVED)",
         "<p>So this is the log that I see on the terminal:</p>\n<pre><code>sentencepiece_trainer.cc(78) LOG(INFO) Starts training with :  \ntrainer_spec {  \n  input: C:\\Users\\xxxx\\OneDrive\\Documents\\Projects\\py\\xxxxx\\data\\tokenizer\\final_text_corpus.txt  \n  input_format:  \n  model_prefix: C:\\Users\\xxxx\\OneDrive\\Documents\\Projects\\py\\xxxxxxxx\\tokenizer\\multilingual_unigram  \n  model_type: UNIGRAM  \n  vocab_size: 50000  \n  self_test_sample_size: 0  \n  character_coverage: 1  \n  input_sentence_size: 10000000  \n  shuffle_input_sentence: 1  \n  seed_sentencepiece_size: 1000000  \n  shrinking_factor: 0.75  \n  max_sentence_length: 16384  \n  num_threads: 16  \n  num_sub_iterations: 2  \n  max_sentencepiece_length: 16  \n  split_by_unicode_script: 1  \n  split_by_number: 1  \n  split_by_whitespace: 1  \n  split_digits: 0  \n  pretokenization_delimiter:  \n  treat_whitespace_as_suffix: 0  \n  allow_whitespace_only_pieces: 0  \n  user_defined_symbols: &lt;newline&gt;  \n  required_chars:  \n  byte_fallback: 0  \n  vocabulary_output_piece_score: 1  \n  train_extremely_large_corpus: 1  \n  seed_sentencepieces_file:  \n  hard_vocab_limit: 1  \n  use_all_vocab: 0  \n  unk_id: 1  \n  bos_id: 2  \n  eos_id: 3  \n  pad_id: 0  \n  unk_piece: &lt;unk&gt;  \n  bos_piece: &lt;s&gt;  \n  eos_piece: &lt;/s&gt;  \n  pad_piece: &lt;pad&gt;  \n  unk_surface:  Γüç  \n  enable_differential_privacy: 0  \n  differential_privacy_noise_level: 0  \n  differential_privacy_clipping_threshold: 0  \n}  \nnormalizer_spec {  \n  name: nmt_nfkc  \n  add_dummy_prefix: 1  \n  remove_extra_whitespaces: 1  \n  escape_whitespaces: 1  \n  normalization_rule_tsv:  \n}  \ndenormalizer_spec {}  \ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.  \ntrainer_interface.cc(185) LOG(INFO) Loading corpus: C:\\Users\\xxxxxxx\\OneDrive\\Documents\\Projects\\py\\xxxxxxx\\data\\tokenizer\\final_text_corpus.txt  \ntrainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines  \ntrainer_interface.cc(147) LOG(INFO) Loaded 2000000 lines  \ntrainer_interface.cc(147) LOG(INFO) Loaded 3000000 lines  \ntrainer_interface.cc(147) LOG(INFO) Loaded 4000000 lines  \ntrainer_interface.cc(147) LOG(INFO) Loaded 5000000 lines  \ntrainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (5816781), which may slow down training.  \ntrainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=&lt;size&gt; and --shuffle_input_sentence=true.  \ntrainer_interface.cc(129) LOG(WARNING) They allow to randomly sample &lt;size&gt; sentences from the entire corpus.  \ntrainer_interface.cc(409) LOG(INFO) Loaded all 5816781 sentences  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;pad&gt;  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;unk&gt;  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;s&gt;  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;/s&gt;  \ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;newline&gt;  \ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...  \ntrainer_interface.cc(539) LOG(INFO) all chars count=731130164  \ntrainer_interface.cc(550) LOG(INFO) Done: 100% characters are covered.  \ntrainer_interface.cc(560) LOG(INFO) Alphabet size=1280  \ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1  \ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 5816741 sentences.\n</code></pre>\n<p>After this the terminal closes.</p>\n<p>This is the part of the code that trains:</p>\n<pre><code>import sys\nimport os\nfrom pathlib import Path\nimport sentencepiece as spm\n\n# === Paths ===\nroot_dir = Path(__file__).resolve().parent.parent  \ninput_path = root_dir / &quot;data&quot; / &quot;tokenizer&quot; / &quot;final_text_corpus.txt&quot;\noutput_dir = root_dir / &quot;tokenizer&quot;\noutput_dir.mkdir(parents=True, exist_ok=True)\nmodel_prefix = &quot;spm_tokenizer&quot;\nlog_path = output_dir / &quot;training.log&quot;\n\n# === Logging setup ===\nwith open(log_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as log_file:\n    sys.stdout = log_file\n    sys.stderr = log_file\n\n    print(&quot;Starting tokenizer training...&quot;)\n    print(f&quot;Input corpus: {input_path}&quot;)\n    print(f&quot;Output prefix: {model_prefix}&quot;)\n    print(f&quot;Vocab size: {50000}&quot;)\n\n    # === Train Tokenizer ===\n    spm.SentencePieceTrainer.train(\n        input=str(input_path),\n        model_prefix=model_prefix,\n        vocab_size=50000,\n        model_type=&quot;unigram&quot;,\n        character_coverage=1.0, \n        pad_id=0,\n        unk_id=1,\n        bos_id=2,\n        eos_id=3,\n        user_defined_symbols=[&quot;&lt;newline&gt;&quot;],\n        train_extremely_large_corpus=True,\n        input_sentence_size=10_000_000,\n        shuffle_input_sentence=True,\n        max_sentence_length=16384\n    )\n\n    print(f&quot;Tokenizer trained! Model saved to: {model_prefix}.model / .vocab&quot;)\n\n</code></pre>\n<p>I have added the <code>train_extremely_large_corpus</code> flag to True and the <code>input_sentence_size</code> and <code>max_sentence_length</code> flags in case it's because of memory bottlenecks, but still no help.I tried finding this issue all over the internet, still no luck.\nCan anyone explain what's causing the problem?</p>\n",
         "2025-04-05",
         "python,nlp,sentencepiece",
         "0",
         "54",
         "0",
         null,
         null,
         null
        ],
        [
         "5",
         "79557313",
         "https://stackoverflow.com/questions/79557313",
         "No attention output in jinaai/jina-embeddings-v3 embedding model",
         "<p>When I use this model like so -</p>\n<pre><code>from transformers import AutoModel, AutoTokenizer\n\nmodel_id = &quot;jinaai/jina-embeddings-v3&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n\ninputs = tokenizer([\n    &quot;The weather is lovely today.&quot;,\n    &quot;It's so sunny outside!&quot;,\n    &quot;He drove to the stadium.&quot;\n], return_tensors=&quot;pt&quot;, padding=True, truncation=True)\n\noutputs = model(**inputs, output_attentions=True)\n\nattentions = outputs.attentions\n</code></pre>\n<p>I get this warning which seems contradictory -</p>\n<pre><code>flash_attn is not installed. Using PyTorch native attention implementation.\nFlash attention implementation does not support kwargs: output_attentions\n</code></pre>\n<p>attentions is None</p>\n<p>I tried it with other models and it works as expected.</p>\n",
         "2025-04-05",
         "machine-learning,nlp,artificial-intelligence,sentence-transformers",
         "0",
         "31",
         "0",
         null,
         null,
         null
        ],
        [
         "6",
         "79549787",
         "https://stackoverflow.com/questions/79549787",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?",
         "<p>I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER):</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;pl_core_news_lg&quot;)\ntext = &quot;Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.&quot;\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n</code></pre>\n<p>Output:</p>\n<pre><code>[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n</code></pre>\n<p>However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities.</p>\n<pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=&quot;path_to_my_file/nlp_config.yaml&quot;) \nnlp_engine = provider.create_engine()\n\nprint(f&quot;Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}&quot;)\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[&quot;pl&quot;])\nregistry.load_predefined_recognizers([&quot;pl&quot;])\n\nprint(f&quot;Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}&quot;)\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, &quot;pl&quot;)\n\nfor entity in results:\n    print(f&quot;Found entity: {entity.entity_type} with score {entity.score}&quot;)\n</code></pre>\n<p>Output:</p>\n<pre><code>Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n</code></pre>\n<p>Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text.</p>\n<p>My config file:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n<p>Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?</p>\n",
         "2025-04-02",
         "python,nlp,spacy,presidio",
         "0",
         "97",
         "1",
         "79552218.0",
         "<p>The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine :</p>\n<pre><code>  labels_to_ignore:\n    - O\n</code></pre>\n<p>On your configuration it would look like this:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n<p><strong>Edit:</strong> This was fixed to be the default if 'labels_to_ignore' is not specified</p>\n<p>Will be part of version 2.2.359 release</p>\n",
         "1.0"
        ],
        [
         "7",
         "79548202",
         "https://stackoverflow.com/questions/79548202",
         "GPT-2 and other models from huggingface -100 label index for training, instead of pad token",
         "<p>I understand the -100 label id is used so that the predictions for these are not included when calculating the loss.</p>\n<p>However on <a href=\"https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16#bert2gpt2-summarization-with-%F0%9F%A4%97-encoderdecoder-framework\" rel=\"nofollow noreferrer\">huggingface</a>, they state\n&quot;complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not&quot;, when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument &quot;ignore_index&quot;.</p>\n<p>Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same?</p>\n<p>The way it is written makes me think there is some benefit, but the description of &quot;ignore_index&quot; appears to achieve what is wanted.</p>\n",
         "2025-04-01",
         "nlp,huggingface-transformers,pre-trained-model",
         "0",
         "54",
         "1",
         "79551169.0",
         "<p>The author of the tutorial you mentioned sets it to <code>-100</code> <strong>and</strong> uses <code>ignore_index</code> to save a few lines of code. You don't see the line where the author pass something to <code>ignore_index</code> because it has a default value. The default value of <code>ignore_index</code> for <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\" rel=\"nofollow noreferrer\">nn.CrossEntropyLoss</a> is <code>-100</code>. Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.</p>\n",
         "1.0"
        ],
        [
         "8",
         "79546188",
         "https://stackoverflow.com/questions/79546188",
         "simpler gmail Filter syntax for \"word family\" [verif +(y/ied/ification] + similar loanwords [term +(s/es/a)]?",
         "<p>Is there simpler filter that I can use for below cases?\nGoogle has a very smart AI gemini, I hope there is a shortcut for this as I am receiving bilingual emails and loan words in Malay/Indonesia are quite similar to English.</p>\n<p>a. variations of the same word?</p>\n<pre><code>subject:{+updates +updated +updating +update}\nsubject:{+verify +verification +verified}\n</code></pre>\n<p>b. similar transliteration of words</p>\n<pre><code>subject:{+terms +termes +terma}\nsubject:{+invois +invoice}\nsubject:{+privacy +privasi +privacidad}\n</code></pre>\n<p>c. words with different character(s)/prefix/suffix</p>\n<pre><code>subject:{+e-statement +estatement +statement}\nsubject:{+&quot;log in&quot; +login +&quot;log-ins&quot; &quot;logged in&quot;}\n</code></pre>\n",
         "2025-03-31",
         "filter,nlp,gmail,lemmatization,transliteration",
         "0",
         "28",
         "0",
         null,
         null,
         null
        ],
        [
         "9",
         "79533402",
         "https://stackoverflow.com/questions/79533402",
         "Creating regular expression(s) which finds capitalization errors",
         "<blockquote>\n<p>This is a Sentence which contains<br/>\nSome capitalization errors.</p>\n</blockquote>\n<p>So far I have this: <code>(?&lt;![.!?]\\s)(?&lt;!^)(?&lt;!\\sI\\s)(?!I['’][a-z])(?!\\b(?:Dr|Mr|Mrs)\\.[\\s\\r\\n])\\b(?!I\\b)[A-Z]\\w*</code></p>\n<p>It will find &quot;Sentence&quot; in the above. It avoids hitting on I and I' contractions, and Dr. / Mr. / Mrs.</p>\n<p>What I can't get it to do is find &quot;Some&quot; in the above.</p>\n<p>I feel maybe a second expression might be better for document scanning, as the first expression is quite long and probably not optimized.</p>\n<p>I need the expression to be PCRE compliant that avoids non fixed width errors and such.</p>\n<p>Just can't solve this on my own unfortunately. As expected AI is no help here... the best models struggle with regular expressions unless they are more simple.</p>\n<p>Tried many different RegEx's to match the word &quot;Some&quot; in the above. It should NOT match a preceding line that ends with a period, question mark, or exclamation point. It should also NOT match on I and I' contractions, or on Dr. / Mr. / Mrs.</p>\n",
         "2025-03-25",
         "regex,nlp,capitalization",
         "0",
         "68",
         "0",
         null,
         null,
         null
        ],
        [
         "10",
         "79527785",
         "https://stackoverflow.com/questions/79527785",
         "SFTTrainer Error : prepare_model_for_kbit_training() got an unexpected keyword argument 'gradient_checkpointing_kwargs'",
         "<p>I'm trying to fine-tune a model using SFTTrainer from trl.</p>\n<p>This is how my SFTConfig arguments look like,</p>\n<pre><code>from trl import SFTConfig\ntraining_arguments = SFTConfig(\n       output_dir=output_dir,\n       num_train_epochs=num_train_epochs,\n       per_device_train_batch_size=per_device_train_batch_size,\n       gradient_accumulation_steps=gradient_accumulation_steps,\n       optim=optim,\n       save_steps=save_steps,\n       logging_steps=logging_steps,\n       learning_rate=learning_rate,\n       weight_decay=weight_decay,\n       fp16=fp16,\n       bf16=bf16,\n       max_grad_norm=max_grad_norm,\n       max_steps=max_steps,\n       warmup_ratio=warmup_ratio,\n       group_by_length=group_by_length,\n       lr_scheduler_type=lr_scheduler_type,\n       report_to=&quot;tensorboard&quot;,\n       dataset_text_field=&quot;instruction&quot;,\n       max_seq_length=None,\n       packing=False,\n       gradient_checkpointing=False,\n   )\n\n</code></pre>\n<p>and this is my SFTTrainer block.</p>\n<pre><code>trainer = SFTTrainer(\n   model=model,\n   train_dataset=dataset,\n   peft_config=peft_config,\n   tokenizer=tokenizer,\n   args=training_arguments,\n)\n</code></pre>\n<p>The error comes from internal function <code>SFTTrainer._prepare_model_for_kbit_training</code>.</p>\n<pre><code> &quot;&quot;&quot;Prepares a quantized model for kbit training.&quot;&quot;&quot;\n    330 prepare_model_kwargs = {\n    331     &quot;use_gradient_checkpointing&quot;: args.gradient_checkpointing,\n    332     &quot;gradient_checkpointing_kwargs&quot;: args.gradient_checkpointing_kwargs or {},\n    333 }\n\n</code></pre>\n<p>I tried passing <code>gradient_checkpointing</code> as False and <code>gradient_checkpointing_kwargs</code> as an empty dictionary, but no luck.</p>\n<p>How can I avoid this error?</p>\n",
         "2025-03-22",
         "python,nlp,large-language-model,transformer-model",
         "0",
         "66",
         "1",
         null,
         null,
         null
        ],
        [
         "11",
         "79527581",
         "https://stackoverflow.com/questions/79527581",
         "AllenNLP all models about ccg_supertagger are unavailable. How to fix or download it?",
         "<p>I am trying to use AllenNLP models to parse a file to create a CCG dataset, because as a student I can't afford the CCGBank dataset, However I have to, cuz I need a dataset to help me to train a model to resolve syntactic ambiguities, parsing the sentence to ccg format is an inevitable step. I really need the model like\npredictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/ccg_supertagger-2020.02.10.tar.gz&quot;)\nor if you have better option , I am willing to have a try!\nIt's my code below</p>\n<pre><code>import pandas as pd\nfrom allennlp.predictors.predictor import Predictor\nimport allennlp_models.tagging\n\n# 读取原始 CSV 文件\ninput_path = &quot;validation.csv&quot;  # 替换为你的本地路径\ndf = pd.read_csv(input_path)\nsentences = df[&quot;sentence&quot;].tolist()\n\n# 加载 AllenNLP 的预训练 CCG Supertagger 模型\npredictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/ccg_supertagger-2020.02.10.tar.gz&quot;)\n\n# 定义预测函数：输入句子，输出 “词/范畴” 序列\ndef get_ccg_tags(sentence):\n    output = predictor.predict(sentence=sentence)\n    tokens = output[&quot;words&quot;]\n    tags = output[&quot;ccg_tags&quot;]\n    tagged = [f&quot;{w}/{t}&quot; for w, t in zip(tokens, tags)]\n    return &quot; &quot;.join(tagged)\n\n# 批量处理每个句子，添加 ccg_tags 列\ndf[&quot;ccg_tags&quot;] = df[&quot;sentence&quot;].apply(get_ccg_tags)\n\n# 保存结果到新文件\noutput_path = &quot;validation_with_allennlp_ccg.csv&quot;\ndf.to_csv(output_path, index=False)\n\nprint(f&quot; AllenNLP CCG ：{output_path}&quot;)\n</code></pre>\n",
         "2025-03-22",
         "nlp,allennlp",
         "0",
         "25",
         "0",
         null,
         null,
         null
        ],
        [
         "12",
         "79526774",
         "https://stackoverflow.com/questions/79526774",
         "Unable to get the tokenizer of Gemma-3",
         "<p>I am trying to get the tokenizer using huggingface AutoTokenizer library, but I am unable to fetch, is there any other way to get it?\nWhere I am doing wrong?</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoTokenizer\n# Load Gemma 3‑27B‑IT’s tokenizer\nMODEL_GEMMA = &quot;google/gemma-3-27b-it&quot;\ngemma_tokenizer = AutoTokenizer.from_pretrained(MODEL_GEMMA, trust_remote_code=True)\n</code></pre>\n<p>I am getting the following error on running it</p>\n<pre><code>ValueError: Tokenizer class GemmaTokenizer does not exist or is not \ncurrently imported.\n</code></pre>\n",
         "2025-03-22",
         "deep-learning,nlp,huggingface-transformers,huggingface-tokenizers,gemma",
         "2",
         "273",
         "2",
         null,
         null,
         null
        ],
        [
         "13",
         "79523696",
         "https://stackoverflow.com/questions/79523696",
         "how to modify a step or a prompt of an existing langchain chain (customize SelfQueryRetriever)?",
         "<p>I need to customize a SelfQueryRetriever(the reason is: the generated target queries in OpenSearch are being generated incorrrectly so we need to tune prompts + we need to add some custom behavior such as multi-tenancy) but we don't want to re-write the whole chain, just the parts what we need to customize. How can we customize specific steps of a chain, is there  a way to modify it by position, let's say something like this (pseudo-code):</p>\n<pre><code>retriever = SelfQueryRetriever(**config)\nretriever[2] = create_custom_module1()\nretriever[4] = create_custom_module2()\n</code></pre>\n<p>In this example we preserve the majority of the chain but  customize only the third and fifth elements.</p>\n<p>Is it possible to do?</p>\n",
         "2025-03-20",
         "python,nlp,artificial-intelligence,langchain,large-language-model",
         "0",
         "32",
         "0",
         null,
         null,
         null
        ],
        [
         "14",
         "79523269",
         "https://stackoverflow.com/questions/79523269",
         "Trouble getting importing gensim to work in colab",
         "<p>I am trying to import gensim into colab.</p>\n<pre><code>!pip install gensim\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--&gt; 367             msg = (&quot;The current Numpy installation ({!r}) fails to &quot;\n    368                    &quot;pass simple sanity checks. This can be caused for example &quot;\n    369                    &quot;by incorrect BLAS library being linked in, or by mixing &quot;\n\nModuleNotFoundError: No module named 'numpy.char'\n</code></pre>\n<p>my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks</p>\n",
         "2025-03-20",
         "numpy,nlp,dependencies,google-colaboratory,gensim",
         "0",
         "209",
         "1",
         "79523777.0",
         "<p>You have to restart the session for the underlying runtime to notice the package changes. See: <a href=\"https://stackoverflow.com/a/79518359/130288\">https://stackoverflow.com/a/79518359/130288</a></p>\n<p>I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded <code>numpy</code>/etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade.</p>\n<p>But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two.</p>\n<p>Explicitly restarting after the Gensim-install &amp; <code>numpy</code>/<code>scipy</code> downgrades resolves the errors.</p>\n",
         "1.0"
        ],
        [
         "15",
         "79523261",
         "https://stackoverflow.com/questions/79523261",
         "How to Identify Similar Code Parts Using CodeBERT Embeddings?",
         "<p>I'm using CodeBERT to compare how similar two pieces of code are. For example:</p>\n<pre><code># Code 1\ndef calculate_area(radius):\nreturn 3.14 * radius * radius\n</code></pre>\n<pre><code># Code 2\ndef compute_circle_area(r):\nreturn 3.14159 * r * r\n</code></pre>\n<p>CodeBERT creates &quot;embeddings,&quot; which are like detailed descriptions of the code as numbers. I then compare these numerical descriptions to see how similar the codes are. This works well for telling me how much the codes are alike.</p>\n<p>However, I can't tell which parts of the code CodeBERT thinks are similar. Because the &quot;embeddings&quot; are complex, I can't easily see what CodeBERT is focusing on. Comparing the code word-by-word doesn't work here.</p>\n<p>My question is: How can I figure out which specific parts of two code snippets CodeBERT considers similar, beyond just getting a general similarity score?</p>\n<p>I tried simple diff methods but that defeats the purpose of purely using CodeBERT.\nI want to know if it's possible using CodeBERT alone.</p>\n",
         "2025-03-20",
         "machine-learning,nlp,bert-language-model,plagiarism-detection,multihead-attention",
         "2",
         "78",
         "1",
         null,
         null,
         null
        ],
        [
         "16",
         "79520986",
         "https://stackoverflow.com/questions/79520986",
         "Converting data into spacy format \"convert_to_spacy_format\" in Name entity recognition Model",
         "<p><a href=\"https://i.sstatic.net/TMDd85MJ.png\" rel=\"nofollow noreferrer\">Dataset structure</a>Can somebody help me with the NER model in converting the data into spacy format.\nThe dataset format is shown in the screenshot here (<a href=\"https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus\" rel=\"nofollow noreferrer\">https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus</a>)</p>\n<p>Though i build but the model is not giving any output during test.</p>\n<pre><code>#Convert data to spaCy format\ndef convert_to_spacy_format(data):\n    nlp = spacy.blank(&quot;en&quot;)  # Creating blank English language model\n    db = DocBin()  # document bin object\n    \n    for _, row in tqdm(data.iterrows(), total=len(data)):\n        sentence = row[&quot;CleanSentence&quot;]\n        pos_tags = row[&quot;POS&quot;]\n        ner_tags = row[&quot;Tag&quot;]\n        \n        # Create a doc object\n        doc = nlp.make_doc(sentence)\n        \n        # Split the sentence into words (tokens)\n        words = sentence.split()\n        \n        # Check if lengths match\n        if len(words) != len(ner_tags) or len(words) != len(pos_tags):\n            print(f&quot;Warning: Length mismatch: Words: {len(words)}, NER tags: {len(ner_tags)}, POS tags: {len(pos_tags)}&quot;)\n            continue\n            \n        ents = []\n        current_ent = None\n        current_ent_start = None\n        \n        # Process each token\n        for idx, (token, tag) in enumerate(zip(doc, ner_tags)):\n            # If it's the beginning of an entity\n            if tag.startswith(&quot;B-&quot;):\n                # If we were tracking an entity, add it to our list\n                if current_ent is not None:\n                    ents.append((current_ent_start, token.idx + len(token), current_ent))\n                \n                # Start tracking a new entity\n                current_ent = tag[2:]  # Remove &quot;B-&quot; prefix\n                current_ent_start = token.idx\n            \n            # If it's inside an entity\n            elif tag.startswith(&quot;I-&quot;):\n                # Continue tracking the current entity\n                pass\n            \n            # If it's outside any entity\n            elif tag == &quot;O&quot;:\n                # If we were tracking an entity, add it to the list\n                if current_ent is not None:\n                    ents.append((current_ent_start, token.idx, current_ent))\n                    current_ent = None\n                    current_ent_start = None\n        \n        # Add the last entity if we were tracking one\n        if current_ent is not None:\n            ents.append((current_ent_start, len(sentence), current_ent))\n        \n        # Create spans for each entity\n        spans = []\n        for start, end, label in ents:\n            span = doc.char_span(start, end, label=label)\n            if span is not None:\n                spans.append(span)\n        \n        # Filter overlapping spans\n        filtered_spans = filter_spans(spans)\n        \n        # Add entities to the doc\n        doc.ents = filtered_spans\n        \n        # Add the doc to the DocBin\n        db.add(doc)\n    \n    return db\n\n</code></pre>\n<p>I tried to build an NER model but didn't got the expected output. I need help in the function</p>\n<pre><code>convert_to_spacy_format(data)\n</code></pre>\n",
         "2025-03-19",
         "python,string,nlp,spacy,named-entity-recognition",
         "0",
         "35",
         "0",
         null,
         null,
         null
        ],
        [
         "17",
         "79515458",
         "https://stackoverflow.com/questions/79515458",
         "Gensim on Google Colab : ModuleNotFoundError: No module named 'numpy.strings'",
         "<p>I've had some problems using GENSIM.</p>\n<p>After running:</p>\n<pre><code>pip install --upgrade gensim\n</code></pre>\n<p>i execute:</p>\n<pre><code>import gensim.downloader as api\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-7-bfa495c1e338&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 import gensim.downloader as api\n\n9 frames\n/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    374     _sanity_check()\n    375     del _sanity_check\n--&gt; 376 \n    377     def _mac_os_check():\n    378         &quot;&quot;&quot;\n\nModuleNotFoundError: No module named 'numpy.strings'\n\n---------------------------------------------------------------------------\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n&quot;Open Examples&quot; button below.\n---------------------------------------------------------------------------\n</code></pre>\n<p>I understand that the problem is in the versions of some packages that gensim uses.</p>\n<pre><code>Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\nRequirement already satisfied: scipy&lt;1.14.0,&gt;=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\nRequirement already satisfied: smart-open&gt;=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim) (1.17.2)\n</code></pre>\n<p>How to resolve this issue?</p>\n",
         "2025-03-17",
         "numpy,nlp,google-colaboratory,gensim",
         "2",
         "1337",
         "1",
         null,
         null,
         null
        ],
        [
         "18",
         "79513881",
         "https://stackoverflow.com/questions/79513881",
         "How to Fine-Tune Projection Layer in CLIP Model Using LoRA?",
         "<p>I'm trying to fine-tune the projection layers in the CLIP model using LoRA.</p>\n<p>I need help identifying the exact projection layers to modify for my fine-tuning and how I can apply LoRA to them.</p>\n<p>Model loading:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import clip\n\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\nmodel, preprocess = clip.load(&quot;ViT-B/32&quot;, device=device)\n</code></pre>\n<p>Model structure when printed</p>\n<pre><code>CLIP(\n  (visual): VisionTransformer()\n  (transformer): Transformer()\n  (token_embedding): Embedding(49408, 512)\n  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n)\n</code></pre>\n<p>I need help identifying the exact projection layers to modify for my fine-tuning and how I can apply LoRA to them.</p>\n",
         "2025-03-17",
         "nlp,artificial-intelligence,large-language-model,image-text",
         "2",
         "130",
         "1",
         null,
         null,
         null
        ],
        [
         "19",
         "79510017",
         "https://stackoverflow.com/questions/79510017",
         "How can I deploy and run a Flask web application using heavy NLP libraries (pandas, numpy, sklearn) on a SiteGround shared hosting plan?",
         "<p>I have a Flask-based web application that performs NLP tasks using libraries like pandas, numpy, sklearn, and nltk. I've tried deploying it to my current hosting (SiteGround shared hosting plan), but encountered multiple issues, such as:</p>\n<p>Installation issues (pyahocorasick and other dependency errors).\nResource limitations (KeyboardInterrupt when importing heavy libraries).\nDifficulty running continuously in the background.\nMy current setup:\nHosting provider: SiteGround Shared Hosting\nPython version: 3.13.2\nFlask app with dependencies: pandas, numpy, sklearn, nltk, contractions, etc.\nSSH access available, but no root access.\nTried using virtual environment (venv), encountering build issues.\nMy questions are:</p>\n<ol>\n<li>Is it possible to run resource-intensive NLP applications like this on SiteGround’s shared hosting plan at all?</li>\n<li>If yes, how? What steps or configurations are required to overcome these errors?</li>\n<li>If no, what are the simplest and most cost-effective alternatives to deploy such a Flask NLP application smoothly (PythonAnywhere, Render.com, Heroku, AWS, DigitalOcean, or others)?</li>\n</ol>\n<p>Thanks in advance for any guidance or advice!</p>\n",
         "2025-03-14",
         "python,flask,nlp",
         "1",
         "41",
         "0",
         null,
         null,
         null
        ],
        [
         "20",
         "79507530",
         "https://stackoverflow.com/questions/79507530",
         "How to normalize ingredient names in a recipe dataset and handle NOUN + NOUN cases using spaCy in python?",
         "<p>I'm working on normalizing ingredient names from a recipe dataset using Python and spaCy. My goal is to extract only the relevant ingredients and ignore measurement units, fractions, and other unnecessary details. For instance, if I have a string like: &quot;5 tablespoons butter, divided&quot;. I want to extract &quot;butter&quot; as the normalized ingredient. However, i struggle to parse the string &quot;8 cups broccoli florets&quot; - it loses &quot;broccoli&quot; and outputs only &quot;floret&quot;. Other strings, like &quot;3 cups chicken broth&quot;, work fine and output &quot;chicken broth&quot;.</p>\n<p><strong>My final version of the function is:</strong></p>\n<pre><code>`def normalize_ingredient(ingredient_text):\n    doc = nlp(ingredient_text)\n\n    # Set of measurement units to exclude\n    measurement_units = {\n        &quot;cup&quot;, &quot;teaspoon&quot;, &quot;tablespoon&quot;, &quot;tablespoons&quot;, &quot;gram&quot;, &quot;ounce&quot;, &quot;pound&quot;, &quot;can&quot;,\n        &quot;clove&quot;, &quot;pinch&quot;, &quot;dash&quot;, &quot;quart&quot;, &quot;liter&quot;, &quot;milliliter&quot;, &quot;gallon&quot;,\n        &quot;stick&quot;, &quot;rib&quot;, &quot;head&quot;, &quot;package&quot;, &quot;inch&quot;, &quot;piece&quot;, &quot;fluid&quot;, &quot;container&quot;,\n        &quot;jar&quot;, &quot;loaf&quot;, &quot;bottle&quot;, &quot;pack&quot;, &quot;pint&quot;, &quot;cube&quot;, &quot;stalk&quot;, &quot;slice&quot;, &quot;bulb&quot;,\n        &quot;strip&quot;, &quot;packet&quot;, &quot;envelope&quot;, &quot;box&quot;, &quot;bag&quot;, &quot;carton&quot;, &quot;sprig&quot;, &quot;leaf&quot;,\n        &quot;half&quot;, &quot;purpose&quot;, &quot;pound&quot;, &quot;ounce&quot;, &quot;gram&quot;, &quot;milliliter&quot;, &quot;liter&quot;, &quot;gallon&quot;,\n        &quot;quart&quot;, &quot;pint&quot;, &quot;dash&quot;, &quot;pinch&quot;, &quot;clove&quot;, &quot;can&quot;, &quot;package&quot;, &quot;container&quot;,\n        &quot;jar&quot;, &quot;loaf&quot;, &quot;bottle&quot;, &quot;pack&quot;, &quot;cube&quot;, &quot;stalk&quot;, &quot;bulb&quot;, &quot;strip&quot;, &quot;packet&quot;,\n        &quot;envelope&quot;, &quot;box&quot;, &quot;bag&quot;, &quot;carton&quot;, &quot;sprig&quot;, &quot;leaf&quot;, &quot;fluid&quot;, &quot;inch&quot;, &quot;piece&quot;, &quot;cup&quot;,\n        &quot;bite&quot;, &quot;size&quot;, &quot;bunch&quot;, &quot;cups&quot;,&quot;all&quot;, &quot;sized&quot;, &quot;chunks&quot;\n    }\n\n    # Set of fractions to exclude\n    fractions = {'½', '¼', '¾', '⅓', '⅔', '⅛', '⅜', '⅝', '⅞', '⅙', '⅚', '®'}\n\n    # List to store relevant terms\n    relevant_terms = []\n\n    for token in doc:\n        # Skip numbers, fractions, and measurement units\n        if (token.like_num or\n                token.text in fractions or\n                token.lemma_.lower() in measurement_units):\n            continue\n\n            # Focus on nouns, proper nouns, and adjectives that modify nouns\n        if token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;}:\n            if token.pos_ == &quot;ADJ&quot; and token.head.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                relevant_terms.append(token.lemma_.lower())\n            elif token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                relevant_terms.append(token.lemma_.lower())\n\n    return &quot; &quot;.join(relevant_terms)`\n</code></pre>\n<p><strong>This skips &quot;broccoli&quot; for unknown for me reasons (this is the one and only incorrectly parsed string for now).\nI also tried this approach with compounds:</strong></p>\n<pre><code>`if token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;}:\n            \n            if token.pos_ == &quot;ADJ&quot; and token.head.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                relevant_terms.append(f&quot;{token.lemma_.lower()} {token.head.lemma_.lower()}&quot;)\n            elif token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                compound = [t for t in token.children if t.dep_ == &quot;compound&quot;]\n                if compound:\n                    combined = &quot; &quot;.join([t.lemma_.lower() for t in compound] + [token.lemma_.lower()])\n                    relevant_terms.append(combined)\n                else:\n                    relevant_terms.append(token.lemma_.lower())`\n</code></pre>\n<p><strong>This messed up all the logic and didn't exclude some measurement units.</strong></p>\n<p><strong>Finally, i would like to avoid adding special case with broccoli like this:</strong></p>\n<pre><code>        if token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;} or token.lemma_.lower() == &quot;broccoli&quot;:\n            if token.pos_ == &quot;ADJ&quot; and token.head.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:\n                relevant_terms.append(token.lemma_.lower())\n            elif token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;} or token.lemma_.lower() == &quot;broccoli&quot;:\n                relevant_terms.append(token.lemma_.lower())\n</code></pre>\n",
         "2025-03-13",
         "python,nlp,spacy",
         "2",
         "39",
         "0",
         null,
         null,
         null
        ],
        [
         "21",
         "79502509",
         "https://stackoverflow.com/questions/79502509",
         "Difference between TfIdf vectorizer and kwx",
         "<p>I am looking at querying a text database based on keywords and getting the relevant chunks. I want to know what is the difference between python's <a href=\"https://kwx.readthedocs.io/en/latest/index.html\" rel=\"nofollow noreferrer\">kwx package</a> and sklearn's <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"nofollow noreferrer\">tfidfvectoriser</a> in implementing this. Even though kwx extracts keywords and topics, looks like I can't perform a keyword based search using it, For which I anyway have to use sklearn's tfidf vectorizer. Can you please help me understand the difference and implement an elegant solution? TIA :)</p>\n",
         "2025-03-12",
         "nlp,large-language-model,tf-idf,tfidfvectorizer,keyword-search",
         "1",
         "25",
         "0",
         null,
         null,
         null
        ],
        [
         "22",
         "79501263",
         "https://stackoverflow.com/questions/79501263",
         "Calculate the gradient with respect to attention but also the FFN layers for a pre-trained LLMs",
         "<p>I would like to return the gradient with respect to specific layers and the FFN layer in the Transformer architecture of pre-trained LLMs from the hugging-face model. Is that even possible?</p>\n<p>I am working with the code of this <a href=\"https://github.com/kristosh/xAI/blob/main/attn_vizualizations.py\" rel=\"nofollow noreferrer\">repo</a> which is the following:</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/Phi-3-medium-4k-instruct&quot;)\nmodel = AutoModelForCausalLM.from_pretrained(\n    &quot;microsoft/Phi-3-medium-4k-instruct&quot;,  # note: check spelling if you get error\n    device_map=&quot;auto&quot;,\n    torch_dtype=torch.float16,            # or torch.float32 if preferred\n    trust_remote_code=True\n)\n\n# Create a pipeline\ngenerator = pipeline(\n    &quot;text-generation&quot;,\n    model = model,\n    tokenizer = tokenizer,\n    return_full_text= False,\n    max_new_tokens = 100,\n    do_sample = False\n)\n\n# Prepare a prompt\nprompt = &quot;Whats is the co-capital of Greece according to the country's public opinion?&quot;\ninputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)\ninputs = inputs.to(&quot;cuda:0&quot;)  # send inputs to cuda\n\n# Run the model with attention outputs enabled\n# Make sure to pass output_attentions=True\noutputs = model(input_ids=inputs.input_ids, output_attentions=True)\n\n# outputs.attentions is a tuple with one element per layer\n# Each element is a tensor of shape (batch_size, num_heads, seq_len, seq_len)\nattentions = outputs.attentions\n\n# Generate output\noutput = generator(prompt)\nprint(output[0][&quot;generated_text&quot;])\n</code></pre>\n<p>How to return the gradient concerning the input or a specific attention layer (in a similar fashion with <code>grad-CAM</code> in <code>CNN</code>). Is it possible to do that in <code>transformers</code>?</p>\n",
         "2025-03-11",
         "python,pytorch,nlp,large-language-model",
         "0",
         "46",
         "0",
         null,
         null,
         null
        ],
        [
         "23",
         "79501178",
         "https://stackoverflow.com/questions/79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">site</a> in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?</p>\n",
         "2025-03-11",
         "python,nlp,large-language-model",
         "0",
         "39",
         "1",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n",
         "1.0"
        ],
        [
         "24",
         "79498915",
         "https://stackoverflow.com/questions/79498915",
         "Comparing the similarity of spoken and written form text",
         "<p>I'm converting spoken form text to its written form. For example, &quot;he owes me two-thousand dollars&quot; should be converted to &quot;he owes me $2,000&quot; . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of &quot;two-thousand dollars&quot; to &quot;$2,000&quot; to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this?</p>\n",
         "2025-03-10",
         "nlp,large-language-model",
         "1",
         "26",
         "1",
         null,
         null,
         null
        ],
        [
         "25",
         "79488426",
         "https://stackoverflow.com/questions/79488426",
         "Upserting in Pinecone takes too long",
         "<p>I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using <code>jina-embedding-v3</code>. For 204 reviews this takes around <strong>2.5 hours!</strong> in Colab. Tried using GPU but the embeddings arent using GPU.\nAm i doing something wrong? Is there a way that i can speed up the process? The code is below:</p>\n<p>Initialising DB:</p>\n<pre><code>if index_name not in pc.list_indexes().names():\n  pc.create_index(\n    name=index_name,\n    dimension=1024,\n    metric=&quot;cosine&quot;,\n    spec=ServerlessSpec(\n        cloud=&quot;aws&quot;,\n        region=&quot;us-east-1&quot;\n    )\n)\n</code></pre>\n<p>Embedding &amp; Upserting:</p>\n<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the Jina embedding model and tokenizer from Hugging Face\nmodel_name = &quot;jinaai/jina-embeddings-v3&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\nfrom langchain.text_splitter import SpacyTextSplitter\ntext_splitter = SpacyTextSplitter(chunk_size=500)\n\n# Function to generate embeddings\ndef generate_embeddings(text, task='retrieval.passage'):\n    return model.encode(text, convert_to_tensor=True, task=task).numpy()\n\nfor review_id, review in enumerate(all_reviews[:2]):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n\n# Generate and store embeddings\nfor review_id, review in enumerate(all_reviews):\n    chunks = text_splitter.split_text(review)\n\n    for chunk_index, chunk in enumerate(chunks):\n        embedding = generate_embeddings(chunk)\n\n        unique_id = f&quot;{review_id}_{chunk_index}&quot;\n\n        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}\n\n        index.upsert([(unique_id, embedding, metadata)])\n</code></pre>\n",
         "2025-03-06",
         "python,nlp,rag,pinecone",
         "0",
         "69",
         "1",
         null,
         null,
         null
        ],
        [
         "26",
         "79485287",
         "https://stackoverflow.com/questions/79485287",
         "how to pass additional query filters to a SelfQueryRetriever?",
         "<p>We are implementing a SelfQueryRetriever using OpenSearch as vectorstore, in general it works fine generating the metadata filters from the user query but we need some way to append other filters to the query, and I cannot find how to do that in the documentation, the use case is to make some things such as:</p>\n<ul>\n<li>The UI will have some filters that product owners want exposed as tradditional filters instead of filters to be extracted from user query.</li>\n<li>There are roles and the space of possible search results for a query is restricted by the data available only for users of that role.</li>\n</ul>\n<p>A possible solution is to add those to the metadata fields of the SelfQueryRetriever and append a &quot;system&quot; component to the user query and let the self query retriever create the filters, but to me it does't sound so clean and intuitive.</p>\n<p>How can additional filters be added to the query?</p>\n",
         "2025-03-05",
         "python,nlp,artificial-intelligence,langchain",
         "0",
         "51",
         "0",
         null,
         null,
         null
        ],
        [
         "27",
         "79485259",
         "https://stackoverflow.com/questions/79485259",
         "Spacy rules matching entities before text",
         "<p>I'm trying to write a spacy parser to extract the names and terms of a contract.\nTo do that, I've written a rule to extract the sellers and buyers, except it's extracting multiple times over a simple sentence.</p>\n<p>Here's my rule</p>\n<pre><code>[{&quot;label&quot;: &quot;seller&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;:  &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;seller&quot;}]},\n{&quot;label&quot;: &quot;buyer&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;buyer&quot;}]},]\n</code></pre>\n<p>Which results in spans like this:</p>\n<pre><code>span seller Text: john e. smith and wife judy c. smith, seller\nspan seller Text: e. smith and wife judy c. smith, seller\nspan seller Text: smith and wife judy c. smith, seller\nspan seller Text: judy c. smith, seller\nspan seller Text: c. smith, seller\nspan seller Text: smith, seller\n</code></pre>\n<p>It seems that spacy is chunking the person entities. How can I produce a rule that matches multiple sellers (or buyers), but doesn't cut them up like this example?</p>\n<p>My code is below.</p>\n<pre><code>#!/usr/bin/env python3\n\nimport spacy\nfrom spacy.tokens import SpanGroup, DocBin, Span\nfrom spacy import displacy\nimport bodytext\nimport sys\nrules  = [{&quot;label&quot;: &quot;seller&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;seller&quot;}]},\n        {&quot;label&quot;: &quot;buyer&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;buyer&quot;}]},]\nnlp = spacy.load(&quot;en_core_web_lg&quot;)\nruler = nlp.add_pipe(&quot;span_ruler&quot;)\nruler.add_patterns(rules)\n\ntext = &quot;THIS AGREEMENT made on this 12 day of December, 2008, between John E. Smith and wife Judy C. Smith, Seller (whether one or more), whose address is: 1234 CRD 5000, midland, Texas, 79221-2016, and real estate investors, LLC, Buyer, whose address is: 4321 Harvard Ave, Midland, Texas 79701.  &quot;\ndoc = nlp(text.lower())\n\ndoc.spans[&quot;test&quot;] = SpanGroup(doc)\ndb = DocBin()\n\nfor sentence in doc.sents:\n    for span in doc.spans[&quot;ruler&quot;]:\n        print(&quot;span &quot;+ span.label_+&quot; Text: &quot;+span.text)\n        if span.start &gt;= sentence.start and span.end &lt;= sentence.end:\n            doc.spans[&quot;test&quot;] += [\n                Span(doc, start=sentence.start, end=sentence.end, label=span.label_)\n            ]\n            doc.set_ents(entities=[span], default=&quot;unmodified&quot;)\n</code></pre>\n",
         "2025-03-05",
         "python,nlp,spacy",
         "0",
         "31",
         "0",
         null,
         null,
         null
        ],
        [
         "28",
         "79482290",
         "https://stackoverflow.com/questions/79482290",
         "How to handle German language specific characters like (ä, ö, ü, ß) while tokenizing using GPT2Tokenizer?",
         "<p>I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer.</p>\n<p>To tokenize the text, I wrote the implementation as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import GPT2Tokenizer\n\ntext = &quot;zügiger Transport des ABCD stabilen Kindes in die Notaufnahme UKA&quot;\ntext = text.encode(&quot;utf-8&quot;).decode(&quot;utf-8&quot;)  # Re-encode to fix encoding issues\n\n# Load GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\n\nprint(tokens)  # Should properly tokenize &quot;zügiger&quot; instead of splitting &quot;ü&quot;\n</code></pre>\n<p>Now, when I execute this code snippet I get output as follows:</p>\n<pre><code>['z', 'Ã¼', 'g', 'iger', 'ĠTransport', 'Ġdes', 'ĠABC', 'D', 'Ġstabil', 'en', 'ĠKind', 'es', 'Ġin', 'Ġdie', 'ĠNot', 'au', 'fn', 'ah', 'me', 'ĠUK', 'A']\n</code></pre>\n<p>After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below.</p>\n<pre class=\"lang-markdown prettyprint-override\"><code>| Character | UTF-8 Bytes | Misdecoded as Latin-1 | Resulting String |\n|-----------|-------------|-----------------------|------------------|\n| ä         | C3 A4       | Ã + ¤                 | Ã¤               |\n| ö         | C3 B6       | Ã + ¶                 | Ã¶               |\n| ü         | C3 BC       | Ã + ¼                 | Ã¼               |\n| ß         | C3 9F       | Ã + Ÿ                 | ÃŸ               |\n</code></pre>\n<p>Now, how I can keep German language specific characters like (ä, ö, ü, ß) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. &quot;zügiger&quot; becomes something like ['z', 'ü', 'g', 'iger'].</p>\n",
         "2025-03-03",
         "python,nlp,tokenize,large-language-model,gpt-2",
         "1",
         "69",
         "1",
         null,
         null,
         null
        ],
        [
         "29",
         "79482283",
         "https://stackoverflow.com/questions/79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03",
         "python,nlp,spacy,langchain,presidio",
         "4",
         "240",
         "2",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n",
         "-2.0"
        ],
        [
         "30",
         "79475268",
         "https://stackoverflow.com/questions/79475268",
         "Count of Combination of bigrams",
         "<p>I have create a dataset as follows using bigrams</p>\n<pre><code>index                   |   product_action\n-------------------------------------------------------|\n('customer', 'called')  |   action  \n('customer', 'service') |   action\n('blue', 'dress')       |   product\n('the', 'service')      |   product\n('to', 'complain')      |   action\n('complain', 'about')   |   action\n('service', 'received') |   action\n('the', 'dress')        |   product\n</code></pre>\n<p>I want to know if in each sentence how many times the combination has occured for the entire dataset</p>\n<p>I have tried to use a count on the bigram</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_bigrams(text):\n    tokens = nltk.word_tokenize(text.lower()) \n    return list(ngrams(tokens, 2))  \n\ndef count_bigrams(text, bigram):\n    bigrams = get_bigrams(text)\n    return bigrams.count(bigram)\n</code></pre>\n<p>The dataset I have in mind is as follows:</p>\n<pre><code>product           |      action               |  count\n---------------------------------------------------------|\n('blue', 'dress') |  ('customer', 'called')   |   10\n</code></pre>\n",
         "2025-02-28",
         "python,nlp,nltk",
         "0",
         "44",
         "0",
         null,
         null,
         null
        ],
        [
         "31",
         "79465047",
         "https://stackoverflow.com/questions/79465047",
         "Where is the HuggingFace model saved in when loading a model on colab?",
         "<p>I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.).</p>\n<pre><code>model_id = &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;\n\n\npipeline = transformers.pipeline(\n            &quot;text-generation&quot;,\n            model=model_id,\n            #model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16, &quot;cache_dir&quot;: cache_dir},\n            device_map=&quot;auto&quot;)\n</code></pre>\n",
         "2025-02-24",
         "nlp,huggingface-transformers,huggingface",
         "1",
         "59",
         "1",
         null,
         null,
         null
        ],
        [
         "32",
         "79464254",
         "https://stackoverflow.com/questions/79464254",
         "Understanding the Difference Between Entropy and Cross-Entropy in Language Models: Practical Example with Character-Level Unigram Model",
         "<p>I'm trying to understand the difference between entropy and cross-entropy, as I often hear about the entropy of a language and the cross-entropy of a language model, and I want to understand the link between the two.</p>\n<p>To simplify things, let's consider a language (with a vocabulary) and a language model trained on that language.</p>\n<p>We'll work at the character level (which gives us 26 characters), and a limited number of words (let's take the 20 names below).</p>\n<pre><code>prenoms = [\n    &quot;Alice&quot;, &quot;Alfred&quot;, &quot;Alina&quot;, &quot;Aline&quot;, &quot;Alexandre&quot;, \n    &quot;Alicia&quot;, &quot;Alison&quot;, &quot;Alma&quot;, &quot;Alva&quot;, &quot;Elise&quot;, \n    &quot;Elisa&quot;, &quot;Eliane&quot;, &quot;Alain&quot;, &quot;Amélie&quot;, &quot;Arline&quot;, \n    &quot;Olivier&quot;, &quot;Oline&quot;, &quot;Alva&quot;, &quot;Eliott&quot;, &quot;Julien&quot;\n]\n</code></pre>\n<p>How do we calculate the entropy over these 20 names (i.e., the entropy of our language) and the cross-entropy for our language model (let's take a unigram model or any language model you prefer to help me to understand)?</p>\n<p>If you have a more relevant example, I’m open to it.</p>\n<p>PS: My confusion comes from the fact that, in general definitions, we talk about a (language) distribution P when calculating entropy (without quite knowing how to calculate it), and about two distributions P and Q when calculating cross-entropy (where P is a one-hot encoding vector in this case, when calculating cross-entropy-loss).</p>\n<p>PS2:  A python code could help me to well understand, here is my understanding. I based it on y understanding of Jurasky Book (and <a href=\"https://huggingface.co/docs/transformers/perplexity\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/perplexity</a>)</p>\n<pre class=\"lang-py prettyprint-override\"><code>def distribution_ngrams(text, n=4):\n    &quot;&quot;&quot;\n    &quot;&quot;&quot;\n    import math\n    from collections import Counter \n\n    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n    counts = Counter(ngrams)\n    total = len(ngrams)\n    \n    # Calculate the distribution of n-grams\n    distribution = {ngram: count/total for ngram, count in counts.items()}\n    return distribution\n\ndef language_entropy_ngrams(text, n_approx=4):\n    &quot;&quot;&quot;\n    Calculate an estimate of the entropy of a text using n-grams (normally, we take a very large n and consider an infinite sequence L)\n    &quot;&quot;&quot;\n    import math\n    distribution = distribution_ngrams(text, n_approx)\n    # Calculate entropy\n    entropy = -sum((p * math.log2(p)) for ngram,p in distribution.items())\n    entropy_rate = entropy / n_approx  # normalize by the size of the n-gram\n    return entropy_rate  \n\ndef model_cross_entropy(text,n_approx=4):\n    &quot;&quot;&quot;\n    Calculate the cross-entropy between the true text and the model's predictions\n    &quot;&quot;&quot;\n    import math\n    unigram_model_distribution =  distribution_ngrams(text, 1)\n    language_model_distribution_approximation = distribution_ngrams(text, n_approx)\n\n    q = {}\n    cross_entropy = 0\n    for ngram,p in language_model_distribution_approximation.items():\n        q[ngram] = 1\n        for c in ngram:\n            q[ngram] = q[ngram]*unigram_model_distribution[c]\n        cross_entropy -= p*math.log2(q[ngram])\n        \n    return cross_entropy/n_approx\n\nif __name__ == &quot;__main__&quot;:\n    prenoms = [&quot;Alice&quot;, &quot;Alfred&quot;, &quot;Alina&quot;, &quot;Aline&quot;, &quot;Alexandre&quot;, &quot;Alicia&quot;, \n            &quot;Alison&quot;, &quot;Alma&quot;, &quot;Alva&quot;, &quot;Elise&quot;, &quot;Elisa&quot;, &quot;Eliane&quot;, &quot;Alain&quot;, \n            &quot;Amélie&quot;, &quot;Arline&quot;, &quot;Olivier&quot;, &quot;Oline&quot;, &quot;Alva&quot;, &quot;Eliott&quot;, &quot;Julien&quot;] #each prenonm can be seen as a sequence of characters\n\n\n    L = ''.join(prenoms).lower() #the corpus/language L can be seen as the concatenation of the sequences\n    print(language_entropy_ngrams(L))\n    print(model_cross_entropy(L))\n\n\n\n</code></pre>\n",
         "2025-02-24",
         "nlp",
         "1",
         "56",
         "0",
         null,
         null,
         null
        ],
        [
         "33",
         "79459888",
         "https://stackoverflow.com/questions/79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22",
         "nlp,opennlp",
         "-1",
         "42",
         "1",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n",
         "1.0"
        ],
        [
         "34",
         "79455954",
         "https://stackoverflow.com/questions/79455954",
         "Where is GENIA corpus with annotated uncertainity?",
         "<p>I am looking for the well known GENIA corpus, which annotates events and uncertainity of the events (doubtful, etc.) The old webpage seems not to be available any more. I cannot find a Genia corpus version on hugging face, which would contain the uncertainity annotation. Does anybody know? I am usually quite good in finding data I want :-( MAybe I am blind this time :-(</p>\n",
         "2025-02-20",
         "nlp,uncertainty,relation-extraction",
         "0",
         "21",
         "0",
         null,
         null,
         null
        ],
        [
         "35",
         "79455927",
         "https://stackoverflow.com/questions/79455927",
         "how to get the target generated query on a self-query retriever(langchain)",
         "<p>I'm implementing a <a href=\"https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html\" rel=\"nofollow noreferrer\">self-query retriever</a>  using langchain with OpenSearch as the target vectore store, so far everything is good but we need to capture the generated query in DSL, for debugging and auditing purposes, after some testing I cannot find how to do it, I found how to return thye <a href=\"https://api.python.langchain.com/en/latest/chains/langchain.chains.query_constructor.ir.StructuredQuery.html\" rel=\"nofollow noreferrer\">StructuredQuery</a>, and how to use the StructuredQuery and <a href=\"https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.self_query.opensearch.OpenSearchTranslator.html\" rel=\"nofollow noreferrer\">OpenSearchTranslator</a> to  get a step closer to the final query, however it is not the final query sent to OpenSearch. Question is, how to get the query? This is my current code(that returns something close to it but not the final version):</p>\n<pre><code>opensearch_translator = OpenSearchTranslator()\ndef show_translated_query(query):\n    chain_structured_query = retriever.llm_chain.invoke(query)\n    print(&quot;langchain structured query:&quot;)\n    print(chain_structured_query)\n    os_structured_query = opensearch_translator.visit_structured_query(chain_structured_query)\n    print(&quot;OS query(semantic, filter):&quot;)\n    print(os_structured_query)\n\nshow_translated_query(&quot;a fire ocurring before 2023&quot;)\n&gt;&gt;langchain structured query:\n&gt;&gt;query='fire' filter=Comparison(comparator=&lt;Comparator.LT: 'lt'&gt;, attribute='year', value=2023) limit=None\n&gt;&gt;OS query(semantic, filter):\n&gt;&gt;('fire', {'filter': {'range': {'metadata.year': {'lt': 2023}}}})\n</code></pre>\n",
         "2025-02-20",
         "python,nlp,artificial-intelligence,langchain",
         "0",
         "94",
         "0",
         null,
         null,
         null
        ],
        [
         "36",
         "79451974",
         "https://stackoverflow.com/questions/79451974",
         "word/ sentence similarities",
         "<p>I am trying to find if a given word/ set of words are similar to a definition.</p>\n<p>Example - Definition - &quot;vegetarian User&quot;</p>\n<p>Now, if I want to check a set of sentences like below</p>\n<pre><code>sentences = ['vegetarian User',\n            'user sometimes eats chicken',\n            'user is vegetarian',\n            'user only eats fruits',\n            'user likes fish']\n</code></pre>\n<p>I tried using some sentence transformer like below</p>\n<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings,embeddings)\nprint(similarities)\n</code></pre>\n<p>But this is not giving me expected results.</p>\n<p>What is the best approach to achieve results like below?</p>\n<pre><code>[False,True,True,False]\n</code></pre>\n<p>Is it doable with nlp/ some other technique?</p>\n",
         "2025-02-19",
         "python,python-3.x,nlp",
         "1",
         "57",
         "0",
         null,
         null,
         null
        ],
        [
         "37",
         "79449476",
         "https://stackoverflow.com/questions/79449476",
         "How do I remove escape characters from output of nltk.word_tokenize?",
         "<p>How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output.</p>\n<p>Here's my code:</p>\n<pre><code>from __future__ import division\nimport nltk, re, pprint\nfrom urllib.request import urlopen\n\nurl = &quot;https://www.gutenberg.org/cache/epub/75394/pg75394.txt&quot;\nraw = urlopen(url).read()\nraw = raw.decode('utf-8')\ntokens = nltk.word_tokenize(raw)\nprint(type(tokens))\nprint(len(tokens))\nprint(tokens[:10])\n</code></pre>\n<p>And the output, with the escapes visible in the first list item:\n<a href=\"https://i.sstatic.net/L1QJ1Mdr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/L1QJ1Mdr.png\" alt=\"enter image description here\" /></a></p>\n<p>I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong?</p>\n<p>I'm using Python 3.12.1 on Windows 11.</p>\n<p>Thanks in advance - please do let me know if I can provide any further helpful information.</p>\n",
         "2025-02-18",
         "python,nlp,nltk,tokenize,text-processing",
         "1",
         "68",
         "1",
         null,
         null,
         null
        ],
        [
         "38",
         "79449168",
         "https://stackoverflow.com/questions/79449168",
         "MiniBatchKMeans BERTopic not returning topics for half of data",
         "<p>I am trying to topic a dataset of tweets. I have around 50 million tweets. Unfortunately, such a large dataset will not fit in ram (even 128GB) due to the embeddings. Therefore, I have been working on making an incremental BERTopic as per the <a href=\"https://maartengr.github.io/BERTopic/getting_started/online/online.html\" rel=\"nofollow noreferrer\">docs</a></p>\n<p>As such:</p>\n<pre class=\"lang-none prettyprint-override\"><code>from bertopic.vectorizers import OnlineCountVectorizer\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom sklearn.cluster import MiniBatchKMeans\nimport numpy as np\n\n\nclass SafeIncrementalPCA(IncrementalPCA):\n    def partial_fit(self, X, y=None):\n        # Ensure the input is contiguous and in float64\n        X = np.ascontiguousarray(X, dtype=np.float64)\n        return super().partial_fit(X, y)\n    \n    def transform(self, X):\n        result = super().transform(X)\n        # Force the output to be float64 and contiguous\n        return np.ascontiguousarray(result, dtype=np.float64)\n\n\nvectorizer_model = OnlineCountVectorizer(stop_words=&quot;english&quot;)\nctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\numap_model = SafeIncrementalPCA(n_components=100)\ncluster_model = MiniBatchKMeans(n_clusters=1000, random_state=0)\n\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic(umap_model=umap_model,\n                       hdbscan_model=cluster_model,\n\nfor docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):\n\n    docs_pdf = docs_delayed.compute()\n    emb_pdf = emb_delayed.compute()\n\n    docs = docs_pdf[&quot;text&quot;].tolist()\n    embeddings = np.vstack(emb_pdf['embeddings'].tolist())\n    \n    # Partial fit your model (make sure your model supports partial_fit, like many scikit-learn estimators do)\n    topic_model.partial_fit(docs, embeddings)\n\n</code></pre>\n<p>and then transforming the dataset into a SQL database:</p>\n<pre class=\"lang-none prettyprint-override\"><code>\nfor docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):\n\n    docs_pdf = docs_delayed.compute()\n    emb_pdf = emb_delayed.compute()\n    docs = docs_pdf[&quot;text&quot;].tolist()\n    embeddings = np.vstack(emb_pdf['embeddings'].tolist())\n\n    # 3) Apply BERTopic on this shard\n    topics, probs = topic_model.transform(docs, embeddings)\n\n    # Save topics to DataFrame\n    df_topics = pd.DataFrame({\n        &quot;tweet_id&quot;: docs_pdf[&quot;id&quot;].tolist(),\n        &quot;topic&quot;: topics,\n        &quot;probability&quot;: probs\n    })\n\n    ## Merge &amp; store in DB\n    docs_pdf[&quot;topic&quot;] = df_topics[&quot;topic&quot;]\n    docs_pdf[&quot;probability&quot;] = df_topics[&quot;probability&quot;]\n    docs_pdf.to_sql(&quot;tweets&quot;, engine, if_exists=&quot;append&quot;, index=False)\n</code></pre>\n<p>I've been trying to do this for a quite a while and this is the closest working example I have gotten. The only issue is half of the dataset has null topics in the database at the end. From what I understand of the theory, MiniBatchKMeans should not have any outliers and therefore all tweets should be assigned to at least one topic, right? I've checked out the unclassified tweets in question and there is nothing in their doc that should suggest it would be hard to classify (relative to others that are classified).</p>\n<p>I would be very happy to hear any sort of suggestion on what could be going wrong and how I could fix this!</p>\n<p>Thanks!</p>\n",
         "2025-02-18",
         "python,machine-learning,nlp,data-science,topic-modeling",
         "0",
         "40",
         "0",
         null,
         null,
         null
        ],
        [
         "39",
         "79448878",
         "https://stackoverflow.com/questions/79448878",
         "Python Farm-haystack Dependencies",
         "<p>i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these</p>\n<p>from this implementation:</p>\n<pre><code>import os\nfrom typing import List\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes import PreProcessor, BM25Retriever, FARMReader\n\n# Initialize an in-memory document store (replaceable with Elasticsearch)\ndocument_store = InMemoryDocumentStore()\n\n# Folder where uploaded documents are stored\nUPLOAD_FOLDER = &quot;uploaded_docs&quot;\n\n# Ensure the upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\n\ndef list_documents() -&gt; List[str]:\n    &quot;&quot;&quot;List all uploaded documents.&quot;&quot;&quot;\n    try:\n        return os.listdir(UPLOAD_FOLDER)\n    except FileNotFoundError:\n        raise RuntimeError(f&quot;Upload folder '{UPLOAD_FOLDER}' not found. Please create it.&quot;)\n\n\ndef read_document(file_path: str) -&gt; str:\n    &quot;&quot;&quot;Read the content of a document.&quot;&quot;&quot;\n    try:\n        with open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:\n            return f.read()\n    except Exception as e:\n        raise RuntimeError(f&quot;Error reading file '{file_path}': {str(e)}&quot;)\n\n\ndef preprocess_document(content: str) -&gt; List[dict]:\n    &quot;&quot;&quot;Preprocess the document content into smaller chunks for indexing.&quot;&quot;&quot;\n    preprocessor = PreProcessor(\n        split_by=&quot;word&quot;,  # Split the content into chunks by word count\n        split_length=200,  # Chunk size\n        split_overlap=20,  # Overlap between chunks\n        split_respect_sentence_boundary=True,\n    )\n    return preprocessor.process({&quot;content&quot;: content})\n\n\ndef index_document(file_name: str):\n    &quot;&quot;&quot;Read, preprocess, and index a document.&quot;&quot;&quot;\n    file_path = os.path.join(UPLOAD_FOLDER, file_name)\n    if not os.path.isfile(file_path):\n        raise RuntimeError(f&quot;File '{file_name}' not found in '{UPLOAD_FOLDER}'.&quot;)\n\n    content = read_document(file_path)\n    chunks = preprocess_document(content)\n\n    # Prepare chunks in Haystack-compatible format\n    formatted_chunks = [{&quot;content&quot;: chunk[&quot;content&quot;]} for chunk in chunks]\n    document_store.write_documents(formatted_chunks)\n\n    return {\n        &quot;message&quot;: f&quot;Document '{file_name}' indexed successfully.&quot;,\n        &quot;chunks_count&quot;: len(formatted_chunks),\n    }\n\n\ndef search_documents(query: str):\n    &quot;&quot;&quot;Search indexed documents using a query.&quot;&quot;&quot;\n    retriever = BM25Retriever(document_store=document_store)\n    reader = FARMReader(model_name_or_path=&quot;deepset/roberta-base-squad2&quot;, use_gpu=False)\n    \n    # Retrieve documents\n    retrieved_docs = retriever.retrieve(query)\n    if not retrieved_docs:\n        return {&quot;message&quot;: &quot;No relevant documents found.&quot;}\n\n    # Reader to predict answers from retrieved documents\n    answers = reader.predict(query=query, documents=retrieved_docs, top_k=3)\n\n    # Serialize the results to avoid unsupported types\n    results = [\n        {\n            &quot;answer&quot;: ans.answer,\n            &quot;score&quot;: ans.score,\n            &quot;context&quot;: ans.context,\n            &quot;document_id&quot;: ans.document_id,\n        }\n        for ans in answers[&quot;answers&quot;]\n    ]\n\n    return {&quot;results&quot;: results}\n</code></pre>\n<p>But i keep getting this error:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 122, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 131, in _main\n    prepare(preparation_data)\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 244, in prepare\n    _fixup_main_from_name(data['init_main_from_name'])\n  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 268, in _fixup_main_from_name\n    main_content = runpy.run_module(mod_name,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;&lt;frozen runpy&gt;&quot;, line 226, in run_module\n  File &quot;&lt;frozen runpy&gt;&quot;, line 98, in _run_module_code\n  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/main.py&quot;, line 7, in &lt;module&gt;\n    from app.views.routes import router\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/views/routes.py&quot;, line 2, in &lt;module&gt;\n    from app.services.document_service import list_documents, index_document, search_documents\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/services/document_service.py&quot;, line 3, in &lt;module&gt;\n    from haystack.document_stores import InMemoryDocumentStore\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/__init__.py&quot;, line 8, in &lt;module&gt;\n    from haystack.schema import Document, Answer, Label, MultiLabel, Span, EvaluationResult, TableCell\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/schema.py&quot;, line 42, in &lt;module&gt;\n    @dataclass\n     ^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 250, in dataclass\n    return create_dataclass(_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 241, in create_dataclass\n    pydantic_complete = _pydantic_dataclasses.complete_dataclass(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py&quot;, line 159, in complete_dataclass\n    schema = gen_schema.generate_schema(cls, from_dunder_get_core_schema=False)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 832, in match_type\n    return self._dataclass_schema(obj, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1561, in _dataclass_schema\n    args = sorted(\n           ^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1562, in &lt;genexpr&gt;\n    (self._generate_dc_field_schema(k, v, decorators) for k, v in fields.items()),\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 933, in _generate_dc_field_schema\n    common_field = self._common_field_schema(name, field_info, decorators)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1081, in _common_field_schema\n    schema = self._apply_annotations(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1825, in _apply_annotations\n    schema = get_inner_schema(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py&quot;, line 82, in __call__\n    schema = self._handler(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1806, in inner_handler\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 840, in match_type\n    return self._match_generic_type(obj, origin)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 864, in _match_generic_type\n    return self._union_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1152, in _union_schema\n    choices.append(self.generate_schema(arg))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 844, in match_type\n    return self._unknown_type_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 405, in _unknown_type_schema\n    raise PydanticSchemaGenerationError(\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for &lt;class 'pandas.core.frame.DataFrame'&gt;. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(&lt;some type&gt;) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(&lt;some type&gt;)` since we do not call `__get_pydantic_core_schema__` on `&lt;some type&gt;` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type\n</code></pre>\n",
         "2025-02-18",
         "python-3.x,nlp,artificial-intelligence,fastapi,haystack",
         "-1",
         "46",
         "1",
         null,
         null,
         null
        ],
        [
         "40",
         "79437334",
         "https://stackoverflow.com/questions/79437334",
         "How many obs per class are necessary? - transfer learning w. BERT fine-tuning",
         "<p>I seek advice on a classification problem in industry.</p>\n<p>The rows in a dataset must be classified/labeled--it lacks a target/column (labels have dot-separated levels like 'x.x.x.x.x.x.x')--during every business cycle. For each cycle, a dataset is given as input to this exercise. The dataset includes some variables, most importantly 1. an ID variable and 2. a short text description. When the dataset is correctly classified, the ID will correspond perfectly to a class. At every iteration, most of the ID---class links as identities will carry over, but some won't: There will be new labels and some labels get redefined. Basically, <em>there will be unlabeled rows at every iteration</em>.</p>\n<p>The classes are assigned using variable 2, the text description (besides a few others) compared with the content of a, let's say, scoring manual accompanying the cycle's new dataset. (Well, rather, the dataset and the key come from two independent business processes, but we can ignore that here.) As a de facto scoring manual, the classes are unique and are described only once using a handful of descriptions, which are short text fields about what identifies, =, and what contrasts, !=, a class. So, <em>in the manual (available as a second dataset), each class is described only once</em>.</p>\n<p>The desire is to classify the datasets ad infinitum as automatically as possible. The dataset has already been classified at time T0 but will need more new classes at T1. It is possible to supervise a learning algorithm on the first batch. <strong>The question</strong> is whether subsequent batches require labeling by experts for training the model or, ideally, whether the 'scoring manual' with one row/obs/example per label may suffice for fine-tuning? (more loosely formulated:  <a href=\"https://stats.stackexchange.com/q/310947/207649\">How much data is needed for transfer learning?</a>; and in the case of training from scratch: <a href=\"https://stats.stackexchange.com/questions/446667/how-many-data-points-per-class-is-neccesary-to-train-a-multi-class-deep-learning\">how many data points per class is neccesary to train a multi-class deep learning</a>)</p>\n",
         "2025-02-13",
         "nlp,classification,bert-language-model,transfer-learning,fine-tuning",
         "0",
         "48",
         "0",
         null,
         null,
         null
        ],
        [
         "41",
         "79425052",
         "https://stackoverflow.com/questions/79425052",
         "Generating an n-gram dataset based on an LLM",
         "<p>I want a dataset of common n-grams and their log likelihoods. Normally I would download the <a href=\"https://storage.googleapis.com/books/ngrams/books/datasetsv3.html\" rel=\"nofollow noreferrer\">Google Books Ngram Exports</a>, but I wonder if I can generate a better dataset using a large language model. For example, this script uses <a href=\"https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_completion\" rel=\"nofollow noreferrer\">llama_cpp.Llama.create_completion</a> to find likely 3-grams starting with &quot;welcome to&quot;:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from llama_cpp import Llama # pip install llama-cpp-python\n\nllm = Llama.from_pretrained(\n    repo_id=&quot;unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF&quot;,\n    filename=&quot;DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf&quot;,\n    logits_all=True,\n)\nprint(\n    llm.create_completion(\n        &quot;welcome to&quot;,\n        max_tokens=1,\n        logprobs=10,\n    )[&quot;choices&quot;][0][&quot;logprobs&quot;][&quot;top_logprobs&quot;][0],\n)\n</code></pre>\n<p>Output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>{' the': np.float32(-0.18572943), ' this': np.float32(-3.444591), ' our': np.float32(-4.0559974), ' python': np.float32(-4.3010955), ' a': np.float32(-4.571982), ' bc': np.float32(-5.036485), ' module': np.float32(-5.4879394), ' week': np.float32(-5.7402453), ' all': np.float32(-6.2308974), ' thread': np.float32(-6.272795)}\n</code></pre>\n<p>One issue is that the prompt gets prefixed with a <a href=\"https://www.linkedin.com/pulse/what-special-tokens-tokenization-farhan-naqvi-uxxsf/\" rel=\"nofollow noreferrer\">BOS token</a>, so I only get n-grams that appear at the beginning of a sentence. I can fix this by passing a list of tokens instead of a string. But this leads to a problem when generating 1-grams:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(\n    llm.create_completion(\n        [],\n        max_tokens=1,\n        logprobs=10,\n    )[&quot;choices&quot;][0][&quot;logprobs&quot;][&quot;top_logprobs&quot;][0],\n)\n</code></pre>\n<p><code>AssertionError</code> at <a href=\"https://github.com/abetlen/llama-cpp-python/blob/v0.3.7/llama_cpp/llama.py#L788\" rel=\"nofollow noreferrer\">llama.py line 788</a>: <code>assert self.n_tokens &gt; 0</code></p>\n<p>Apparently <a href=\"https://github.com/ggerganov/llama.cpp\" rel=\"nofollow noreferrer\">llama.cpp</a> is unable to generate text when no context is provided. I confirmed this by using llama.cpp directly without the Python wrapper. My question is, <strong>is this an arbitrary limitation of the library, or a fundamental limitation of the language model?</strong></p>\n<p>I found a possible clue in the model's <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/blob/6393b7559e403fd1d80bfead361586fd6f630a4d/config.json#L10\" rel=\"nofollow noreferrer\">config.json</a> file:</p>\n<pre class=\"lang-json prettyprint-override\"><code>  &quot;initializer_range&quot;: 0.02,\n</code></pre>\n<p>The <a href=\"https://huggingface.co/docs/transformers/en/model_doc/qwen2#transformers.Qwen2Config.initializer_range\" rel=\"nofollow noreferrer\">documentation</a> says that <code>initializer_range</code> is</p>\n<blockquote>\n<p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>\n</blockquote>\n<p>I imagine that the model has a hidden state vector which is initialized with random values sampled from a normal distribution, and the values get updated as context is added. I wonder if it's possible to sample from the model in its initial random state, and get a list of the most common words by sampling multiple times with different random seeds.</p>\n",
         "2025-02-09",
         "nlp,large-language-model,n-gram,llama-cpp-python,llamacpp",
         "0",
         "51",
         "0",
         null,
         null,
         null
        ],
        [
         "42",
         "79419884",
         "https://stackoverflow.com/questions/79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07",
         "keras,deep-learning,nlp,lstm,sentiment-analysis",
         "-1",
         "50",
         "1",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n",
         "0.0"
        ],
        [
         "43",
         "79417001",
         "https://stackoverflow.com/questions/79417001",
         "How to pass AzureOpenAIEmbeddings in CrewAI, I don't have api keys I use azure_ad_token?",
         "<p>How to pass AzureOpenAIEmbeddings, I don't have api keys I use azure_ad_token?</p>\n<pre><code>from langchain_openai.embeddings import AzureOpenAIEmbeddings\n\nazure_embeddings = AzureOpenAIEmbeddings(\nopenai_api_version=conf.pf_api_version,\nazure_endpoint=conf.pf_oa_endpoint_embed,\nazure_ad_token=tokens,\nmodel=conf.pf_embedding_engine,\n)\n\ncrew = Crew(\n  agents=[support_agent, support_quality_assurance_agent],\n  tasks=[inquiry_resolution, quality_assurance_review],\n  verbose=2,\n  memory=True,\n  embedder=embedder_config,\n)\n</code></pre>\n<pre><code>raise SchemaError([message] + x.autos, [e.format(data) if e else None] + x.errors)\nschema.SchemaError: Key 'embedder' error:\nKey 'config' error:\nWrong keys 'azure_ad_token', 'azure_endpoint', 'openai_api_version' in {'azure_endpoint\n</code></pre>\n",
         "2025-02-06",
         "python-3.x,nlp,large-language-model,crewai",
         "0",
         "40",
         "0",
         null,
         null,
         null
        ],
        [
         "44",
         "79412932",
         "https://stackoverflow.com/questions/79412932",
         "Calculating Topic Correlations or Coocurrences for keyATM",
         "<p>I have been playing around with the keyATM package extensively, however unfortunately there is no approach how to calculate topic correlations and cooccurences, once the model is calculated. I already adjusted the the alpha prior to receive more topic correlation, however I am stuck at this point.</p>\n<pre><code># Compute total topics\ntotal_k &lt;- 25 + length(keywords)  \n# Construct Gamma matrix for priors\ngamma_matrix &lt;- matrix(as.numeric(1), nrow = total_k, ncol = 2)\n\n\nfor(al in 1:5){\n\nprint(paste0(&quot;Alpha set to &quot;,al))  \n  \npriors          &lt;-  list(alpha = rep(al, total_k),\n                         beta = 0.01,\n                         beta_s = 0.1,\n                         gamma = gamma_matrix)\n\nout_temp &lt;- keyATM(\n  docs              = keyATM_docs,    # text input\n  no_keyword_topics = 25,              # number of topics without keywords\n  keywords          = keywords,       # keywords\n  model             = &quot;base&quot;,         # select the model\n  options           = list(seed = 250,\n                           iterations = 1500,\n                           verbose = TRUE),\n  priors = priors\n)\n\nassign(paste0(&quot;out_alpha&quot;,al), out_temp)\n\n}\n</code></pre>\n<p>I tried several approaches from other topic model packages, but nothing worked.\nIf anyone has any recommendations how to proceed from here, I would be very thankful.</p>\n",
         "2025-02-04",
         "r,nlp,topic-modeling",
         "0",
         "30",
         "0",
         null,
         null,
         null
        ],
        [
         "45",
         "79411913",
         "https://stackoverflow.com/questions/79411913",
         "How to resolve ValueError while training Seq2Seq using DataCollatorForSeq2Seq",
         "<p>I want to fine tune a VisionEncoderDecoderModel.from_pretrained(model_name)\nI use a CustomOCRDataset from <a href=\"https://learnopencv.com/fine-tuning-trocr-training-trocr-to-recognize-curved-text/\" rel=\"nofollow noreferrer\">Learn Open CV</a>.\nBut the default_data_collator fails to stack the inputs because the samples have a different shape , so I decided to use DataCollatorForSeq2Seq and Resize in augmentation.</p>\n<p>I get an error\nValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['pixel_values']</p>\n<p>So I changed <strong>getitem</strong>  to get input_ids, but the error is still the same.</p>\n<pre><code>def __getitem__(self, idx):\n        file_name = self.df['file_name'][idx]\n        text = self.df['text'][idx]\n\n        assert text.strip() != &quot;&quot;, f&quot;ERROR Empty text in {idx}&quot;\n\n        # Read the image, apply augmentations, and get the transformed pixels.\n        image = Image.open(self.root_dir + file_name).convert('RGB')\n        image = train_transforms(image)\n        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n        # Pass the text through the tokenizer and get the labels,\n        # i.e. tokenized labels.\n        labels = self.processor.tokenizer(\n            text,\n            padding='max_length',\n            max_length=self.max_target_length,\n            return_tensors='pt'\n        ).input_ids.squeeze(0)\n\n        # We are using -100 as the padding token.\n        labels = torch.where(labels == self.processor.tokenizer.pad_token_id, torch.tensor(-100), labels)\n        encoding = {&quot;pixel_values&quot;: pixel_values.squeeze(0),\n                    &quot;input_ids&quot;: labels}\n        return encoding\n</code></pre>\n<pre><code>@dataclass(frozen=True)\nclass TrainingConfig:\n    BATCH_SIZE:    int = 16\n    EPOCHS:        int = 5\n    LEARNING_RATE: float = 0.00005\n\n@dataclass(frozen=True)\nclass DatasetConfig:\n    DATA_ROOT:     str = image_dir\n\n@dataclass(frozen=True)\nclass ModelConfig:\n    MODEL_NAME: str = 'microsoft/trocr-base-handwritten'\n</code></pre>\n<pre><code># Augmentations.\ntrain_transforms = transforms.Compose([\n    transforms.Resize((1024, 880))\n])\n</code></pre>\n<pre><code>processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\ntrain_dataset = CustomOCRDataset(\n    root_dir=os.path.join(DatasetConfig.DATA_ROOT, train_destination),\n    df=train_df,\n    processor=processor\n)\nvalid_dataset = CustomOCRDataset(\n    root_dir=os.path.join(DatasetConfig.DATA_ROOT, test_destination),\n    df=test_df,\n    processor=processor\n)\n</code></pre>\n<pre><code>training_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy='epoch',\n    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n    fp16=True,\n    output_dir='seq2seq_model_printed/',\n    logging_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=5,\n    report_to='tensorboard',\n    num_train_epochs=TrainingConfig.EPOCHS\n)\n</code></pre>\n<pre><code>data_collator = DataCollatorForSeq2Seq(tokenizer=processor.tokenizer, model=model, padding=True)\n# Initialize trainer.\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.feature_extractor,\n    args=training_args,\n    compute_metrics=compute_cer,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    data_collator=data_collator\n)\ntrainer.train()\n</code></pre>\n<p>The full ERROR :</p>\n<pre><code>File \\transformers\\data\\data_collator.py:599, in DataCollatorForSeq2Seq.__call__(self, features, return_tensors)\n    596 non_labels_features = [{k: v for k, v in feature.items() if k != label_name} for feature in features]\n    598 # run through tokenizer without labels to ensure no side effects\n--&gt; 599 batch = pad_without_fast_tokenizer_warning(\n    600     self.tokenizer,\n    601     non_labels_features,\n    602     padding=self.padding,\n    603     max_length=self.max_length,\n    604     pad_to_multiple_of=self.pad_to_multiple_of,\n    605     return_tensors=return_tensors,\n    606 )\n    608 # we have to pad the labels manually as we cannot rely on `tokenizer.pad` and we need them to be of the same length to return tensors\n    609 no_padding = self.padding is False or self.padding == PaddingStrategy.DO_NOT_PAD\n\nFile \\transformers\\data\\data_collator.py:66, in pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs)\n     63 tokenizer.deprecation_warnings[&quot;Asking-to-pad-a-fast-tokenizer&quot;] = True\n     65 try:\n---&gt; 66     padded = tokenizer.pad(*pad_args, **pad_kwargs)\n     67 finally:\n     68     # Restore the state of the warning.\n     69     tokenizer.deprecation_warnings[&quot;Asking-to-pad-a-fast-tokenizer&quot;] = warning_state\n\nFile \\transformers\\tokenization_utils_base.py:3305, in PreTrainedTokenizerBase.pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\n   3303 # The model's main input name, usually `input_ids`, has been passed for padding\n   3304 if self.model_input_names[0] not in encoded_inputs:\n-&gt; 3305     raise ValueError(\n   3306         &quot;You should supply an encoding or a list of encodings to this method &quot;\n   3307         f&quot;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&quot;\n   3308     )\n   3310 required_input = encoded_inputs[self.model_input_names[0]]\n   3312 if required_input is None or (isinstance(required_input, Sized) and len(required_input) == 0):\n\nValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['pixel_values']\n</code></pre>\n<p>The solutions that I've already tried were:\n-&gt; to do transforms.Resize((1024, 880)),\n-&gt; to use a custom data collator\nsuch as:\n`def collate_fn(batch):\nbatch = list(filter(lambda x: x is not None, batch))</p>\n<pre><code># Pack pixel vals und labels separately in lists\npixel_values = [item['pixel_values'] for item in batch]\nlabels = [item['labels'] for item in batch]\n\n# Convert lists to tensors + padding\npixel_values = torch.stack(pixel_values)\nlabels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n\nreturn {\n    &quot;pixel_values&quot;: pixel_values,\n    &quot;labels&quot;: labels\n}`\n</code></pre>\n<p>I got a TypeError: ViTModel.forward() got an unexpected keyword argument 'num_items_in_batch'. That's why I decided to use DataCollatorForSeq2Seq.</p>\n<p>But I'm getting another error over and over: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['pixel_values'].\nThanks in advance!</p>\n",
         "2025-02-04",
         "nlp,huggingface-transformers,huggingface-tokenizers,seq2seq",
         "2",
         "41",
         "0",
         null,
         null,
         null
        ],
        [
         "46",
         "79408298",
         "https://stackoverflow.com/questions/79408298",
         "PunktTokenizer does not work with Russian `я.`",
         "<p>When tokenizing paragraphs to sentences in the Russian language, I am observing the special case when the sequence is not treated as the end of the sentence. The case is with the <code>я.</code> at the end of the sentence. See the working example:</p>\n<pre><code>import nltk\n\ntok = nltk.tokenize.PunktTokenizer('russian')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять я. Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять я. - Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n</code></pre>\n<p><a href=\"https://i.sstatic.net/yrj2ukn0.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/yrj2ukn0.png\" alt=\"the output\" /></a></p>\n<p>The second case works as expected. It differs only in adding the dash (kind of introduction of the speaker's note [sorry for my lack of terms]).</p>\n<p>The <code>я</code> is not present in the Russian abbreviations (<code>c:\\nltk_data\\tokenizers\\punkt_tab\\russian\\abbrev_types.txt</code>). However, even when added to the file with abbreviations, it does not make a difference.</p>\n<p>How the situation should be fixed?</p>\n<p>The nltk is of the version 3.9.1, the nltk_data are shared -- stored in c:\\nltk_data; fresh download (30. 1. 2025). Python 3.12 on Windows 10 was used.</p>\n<p><strong>Update:</strong></p>\n<p>(... observing donwnvotes). Please, I am very new to nltk, and I tried my best to get the answer by myself. When down-voting, write the comment why. I am searching for the answer in various sources without success.</p>\n<p>I am using UTF-8 encoding for both the test script here, and also in the file that is actually being processed (the image take from notepad++).\n<a href=\"https://i.sstatic.net/bmEd4NOU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/bmEd4NOU.png\" alt=\"UTF-8 encoding used\" /></a></p>\n<p><strong>2nd Update:</strong>\nAs Joop Eggen suggested, I have tried with other single letters:</p>\n<pre><code>import nltk\n\ntok = nltk.tokenize.PunktTokenizer('russian')\n\nprint('\\n================= я in the original question (я not in the abbrev_types.txt)\\n')\nline = 'Родилась заново, стал размышлять я. Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять я. - Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('\\n================= г is in the abbrev_types.txt\\n')\nline = 'Родилась заново, стал размышлять г. Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять г. - Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('\\n================= а IS NOT the abbrev_types.txt\\n')\nline = 'Родилась заново, стал размышлять а. Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n\nprint('-----------------')\nline = 'Родилась заново, стал размышлять а. - Она не застрелена, а это дело упрощает.'\nlst = tok.tokenize(line)\nfor n, s in enumerate(lst, 1):\n    print(f'{n}: {s!r}')\n</code></pre>\n<p><a href=\"https://i.sstatic.net/9dezvzKN.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/9dezvzKN.png\" alt=\"other single letters\" /></a></p>\n<p>The <code>а</code> is also <strong>not</strong> in the abbreviations; however, it is treated as if it was (differently from <code>я</code>). The <code>г</code> is in the abbreviations, so here the behavior is expected.</p>\n",
         "2025-02-03",
         "nlp,nltk,tokenize",
         "0",
         "39",
         "0",
         null,
         null,
         null
        ],
        [
         "47",
         "79406743",
         "https://stackoverflow.com/questions/79406743",
         "QuickUMLS Always Returns \"UNK\" for Any Input Text",
         "<p>I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns &quot;UNK&quot;. Here is my code:</p>\n<pre><code>from quickumls import QuickUMLS\n\nquickumls_fp = &quot;med7_en/lib/python3.10/site-packages/quickumls&quot;\nmatcher = QuickUMLS(quickumls_fp)\n\ndef extract_umls_cuis(text):\n    &quot;&quot;&quot;Extract UMLS CUIs using QuickUMLS.&quot;&quot;&quot;\n    if isinstance(text, str):\n        matches = matcher.match(text)\n        if matches:\n            return [match['cui'] for match in matches[0]]\n        else:\n            return &quot;UNK&quot;\n\nsample_text = &quot;diclofenac.&quot;\nprint(extract_umls_cuis(sample_text))\n</code></pre>\n<p>What I Have Checked:</p>\n<ul>\n<li>QuickUMLS Installation: I have installed QuickUMLS correctly.</li>\n<li>UMLS Data Availability: I have set the correct path to QuickUMLS.</li>\n<li>Different Input Words: I tried various medical terms, but all return &quot;UNK&quot;.</li>\n</ul>\n",
         "2025-02-02",
         "python,machine-learning,deep-learning,nlp",
         "0",
         "30",
         "1",
         null,
         null,
         null
        ],
        [
         "48",
         "79402492",
         "https://stackoverflow.com/questions/79402492",
         "Transformers PaliGemma evaluate and compute_loss fail with tensors/device errors",
         "<p>I'm loading a PaliGemma2 model <code>google/paligemma2-3b-pt-224</code> and trying to fine-tune using Trainer/Seq2SeqTrainer. If I add evaluation, this fails. After doing some digging, I found that this only happens if the model is in evaluate mode.</p>\n<pre><code>batch = [valid_dataset[i] for i in range(8)]\ninputs = collate_fn(batch)\n#generate_ids = model.generate(**inputs, max_length=286+30)\ntrainer.model.train()\ntrainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\nprint(&quot;works&quot;)\ntrainer.model.train(False)\ntrainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\nprint(&quot;fails.&quot;)\n</code></pre>\n<p>I've worked around it by mokey-patching compute_loss_context_manager as follows:</p>\n<pre><code>orig_context_manager = trainer.compute_loss_context_manager\nclass TempTrainContext(object):\n    def __init__(self, trainer):\n        self.trainer = trainer\n        self.orig_context_manager = trainer.compute_loss_context_manager\n    def __enter__(self):\n        self.orig_context_inst = self.orig_context_manager()\n        self.orig_context_inst.__enter__()\n        self.training_enter = self.trainer.model.training\n        self.trainer.model.train()\n    def __exit__(self, type, value, traceback):\n        self.trainer.model.train(self.training_enter)\n        self.orig_context_inst.__exit__(type, value, traceback)\n    def __call__(self):\n        return self\n\ntrainer.compute_loss_context_manager = TempTrainContext(trainer)\n</code></pre>\n<p>(Bonus question: Is this safe to do, or will I train on the test set?)</p>\n<p>My versions are:</p>\n<pre><code>Python Version: 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 16:05:46) [GCC 13.3.0]\nTorch Version: 2.5.1+cu124\nCUDA Available: True\nCUDA Device Count: 2\nGPU Name: NVIDIA GeForce RTX 3090\nTransformers Version: 4.48.1\nTokenizers Version: 0.21.0\nAccelerate Version: 1.3.0\n</code></pre>\n<p>Error:</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[13], line 8\n      6 print(&quot;works&quot;)\n      7 trainer.model.train(False)\n----&gt; 8 trainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\n      9 print(&quot;fails.&quot;)\n     12 orig_context_manager = trainer.compute_loss_context_manager\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/trainer.py:3731, in Trainer.compute_loss(self, model, inputs, return_outputs, num_items_in_batch)\n   3729         loss_kwargs[&quot;num_items_in_batch&quot;] = num_items_in_batch\n   3730     inputs = {**inputs, **loss_kwargs}\n-&gt; 3731 outputs = model(**inputs)\n   3732 # Save past state if it exists\n   3733 # TODO: this needs to be fixed and made cleaner later.\n   3734 if self.args.past_index &gt;= 0:\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/accelerate/hooks.py:170, in add_hook_to_module.&lt;locals&gt;.new_forward(module, *args, **kwargs)\n    168         output = module._old_forward(*args, **kwargs)\n    169 else:\n--&gt; 170     output = module._old_forward(*args, **kwargs)\n    171 return module._hf_hook.post_forward(module, output)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/paligemma/modeling_paligemma.py:530, in PaliGemmaForConditionalGeneration.forward(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep)\n    525     labels = torch.where(input_ids == self.pad_token_id, self.config.ignore_index, labels)\n    527 causal_mask = self._update_causal_mask(\n    528     attention_mask, token_type_ids, past_key_values, cache_position, input_ids, inputs_embeds, is_training\n    529 )\n--&gt; 530 outputs = self.language_model(\n    531     attention_mask=causal_mask,\n    532     position_ids=position_ids,\n    533     past_key_values=past_key_values,\n    534     inputs_embeds=inputs_embeds,\n    535     use_cache=use_cache,\n    536     output_attentions=output_attentions,\n    537     output_hidden_states=output_hidden_states,\n    538     return_dict=return_dict,\n    539     cache_position=cache_position,\n    540     num_logits_to_keep=num_logits_to_keep,\n    541 )\n    543 logits = outputs.logits\n    544 loss = None\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:842, in Gemma2ForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\n    840 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n    841 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n--&gt; 842 outputs = self.model(\n    843     input_ids=input_ids,\n    844     attention_mask=attention_mask,\n    845     position_ids=position_ids,\n    846     past_key_values=past_key_values,\n    847     inputs_embeds=inputs_embeds,\n    848     use_cache=use_cache,\n    849     output_attentions=output_attentions,\n    850     output_hidden_states=output_hidden_states,\n    851     return_dict=return_dict,\n    852     cache_position=cache_position,\n    853 )\n    855 hidden_states = outputs[0]\n    856 # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:629, in Gemma2Model.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\n    617     layer_outputs = self._gradient_checkpointing_func(\n    618         decoder_layer.__call__,\n    619         hidden_states,\n   (...)\n    626         cache_position,\n    627     )\n    628 else:\n--&gt; 629     layer_outputs = decoder_layer(\n    630         hidden_states,\n    631         position_embeddings=position_embeddings,\n    632         attention_mask=causal_mask,\n    633         position_ids=position_ids,\n    634         past_key_value=past_key_values,\n    635         output_attentions=output_attentions,\n    636         use_cache=use_cache,\n    637         cache_position=cache_position,\n    638         **flash_attn_kwargs,\n    639     )\n    641 hidden_states = layer_outputs[0]\n    643 if output_attentions:\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/accelerate/hooks.py:170, in add_hook_to_module.&lt;locals&gt;.new_forward(module, *args, **kwargs)\n    168         output = module._old_forward(*args, **kwargs)\n    169 else:\n--&gt; 170     output = module._old_forward(*args, **kwargs)\n    171 return module._hf_hook.post_forward(module, output)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:299, in Gemma2DecoderLayer.forward(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\n    296 hidden_states = self.input_layernorm(hidden_states)\n    298 # Self Attention\n--&gt; 299 hidden_states, self_attn_weights = self.self_attn(\n    300     hidden_states=hidden_states,\n    301     position_embeddings=position_embeddings,\n    302     attention_mask=attention_mask,\n    303     position_ids=position_ids,\n    304     past_key_value=past_key_value,\n    305     output_attentions=output_attentions,\n    306     use_cache=use_cache,\n    307     cache_position=cache_position,\n    308 )\n    309 hidden_states = self.post_attention_layernorm(hidden_states)\n    310 hidden_states = residual + hidden_states\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/accelerate/hooks.py:170, in add_hook_to_module.&lt;locals&gt;.new_forward(module, *args, **kwargs)\n    168         output = module._old_forward(*args, **kwargs)\n    169 else:\n--&gt; 170     output = module._old_forward(*args, **kwargs)\n    171 return module._hf_hook.post_forward(module, output)\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:224, in Gemma2Attention.forward(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\n    221 if past_key_value is not None:\n    222     # sin and cos are specific to RoPE models; cache_position needed for the static cache\n    223     cache_kwargs = {&quot;sin&quot;: sin, &quot;cos&quot;: cos, &quot;cache_position&quot;: cache_position}\n--&gt; 224     key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n    226 attention_interface: Callable = eager_attention_forward\n    227 if self.config._attn_implementation != &quot;eager&quot;:\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/cache_utils.py:1717, in HybridCache.update(self, key_states, value_states, layer_idx, cache_kwargs)\n   1714 else:\n   1715     update_fn = self._static_update\n-&gt; 1717 return update_fn(\n   1718     cache_position,\n   1719     layer_idx,\n   1720     key_states,\n   1721     value_states,\n   1722     k_out,\n   1723     v_out,\n   1724     k_out.shape[2],\n   1725 )\n\nFile ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/cache_utils.py:1694, in HybridCache._static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len)\n   1693 def _static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n-&gt; 1694     k_out[:, :, cache_position] = key_states\n   1695     v_out[:, :, cache_position] = value_states\n   1697     self.key_cache[layer_idx] = k_out\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!&quot;\n</code></pre>\n<p>Error of Evaluator (bottom half of file): <a href=\"https://gist.github.com/BlGene/607c7bee450e03835aa2bf0d2fd2959a\" rel=\"nofollow noreferrer\">https://gist.github.com/BlGene/607c7bee450e03835aa2bf0d2fd2959a</a></p>\n",
         "2025-01-31",
         "python,nlp,huggingface-transformers",
         "0",
         "47",
         "0",
         null,
         null,
         null
        ],
        [
         "49",
         "79402035",
         "https://stackoverflow.com/questions/79402035",
         "How can I add citations in the response on Vectara? While testing the Multiple Corpora Query",
         "<p>How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation.\nI have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>\n<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)\nvectara-summary-ext-24-05-sml (gpt-3.5-turbo)\nvectara-summary-ext-24-05-med-omni (gpt-4o)\nvectara-summary-ext-24-05-med (gpt-4.0)\nvectara-summary-ext-24-05-large (gpt-4.0-turbo)\n</code></pre>\n<p>I have used these models, but still, the citation is not showing in the response.</p>\n",
         "2025-01-31",
         "nlp,large-language-model,rag,vectara,enterprise-rag",
         "1",
         "52",
         "1",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 20414
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>QuestionUrl</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AcceptedAnswerBody</th>\n",
       "      <th>AcceptedAnswerScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79577043</td>\n",
       "      <td>https://stackoverflow.com/questions/79577043</td>\n",
       "      <td>Unsupervised Topic Modeling for Short Event De...</td>\n",
       "      <td>&lt;p&gt;I have a dataset of approximately 750 lines...</td>\n",
       "      <td>2025-04-16</td>\n",
       "      <td>machine-learning,nlp,topic-modeling</td>\n",
       "      <td>-1</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79574001</td>\n",
       "      <td>https://stackoverflow.com/questions/79574001</td>\n",
       "      <td>Is there a way to reuse a heavy service across...</td>\n",
       "      <td>&lt;p&gt;I'm building an Airflow DAG where some of t...</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>nlp,airflow,microservices</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79572620</td>\n",
       "      <td>https://stackoverflow.com/questions/79572620</td>\n",
       "      <td>How can I link tasks using machine learning / ...</td>\n",
       "      <td>&lt;p&gt;I'm working on an AI model to predict depen...</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>python,machine-learning,nlp,artificial-intelli...</td>\n",
       "      <td>-6</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79559702</td>\n",
       "      <td>https://stackoverflow.com/questions/79559702</td>\n",
       "      <td>NameError: name 'init_empty_weights' is not de...</td>\n",
       "      <td>&lt;p&gt;I am trying to set up hugging face locally ...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>nlp,huggingface-transformers,huggingface</td>\n",
       "      <td>3</td>\n",
       "      <td>629</td>\n",
       "      <td>2</td>\n",
       "      <td>79577000.0</td>\n",
       "      <td>&lt;p&gt;Try using this version, it should resolve t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79557354</td>\n",
       "      <td>https://stackoverflow.com/questions/79557354</td>\n",
       "      <td>Sentencepiece not generating models after prep...</td>\n",
       "      <td>&lt;p&gt;So this is the log that I see on the termin...</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>python,nlp,sentencepiece</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20409</th>\n",
       "      <td>42489</td>\n",
       "      <td>https://stackoverflow.com/questions/42489</td>\n",
       "      <td>How to implement a \"related\" degree measure al...</td>\n",
       "      <td>&lt;p&gt;I was going to Ask a Question earlier today...</td>\n",
       "      <td>2008-09-03</td>\n",
       "      <td>algorithm,machine-learning,indexing,nlp,full-t...</td>\n",
       "      <td>8</td>\n",
       "      <td>457</td>\n",
       "      <td>2</td>\n",
       "      <td>42532.0</td>\n",
       "      <td>&lt;p&gt;One such way to implement such an algorithm...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20410</th>\n",
       "      <td>41424</td>\n",
       "      <td>https://stackoverflow.com/questions/41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03</td>\n",
       "      <td>nlp</td>\n",
       "      <td>118</td>\n",
       "      <td>33225</td>\n",
       "      <td>11</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>&lt;p&gt;Actually what Google does is very much non-...</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20411</th>\n",
       "      <td>36533</td>\n",
       "      <td>https://stackoverflow.com/questions/36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>windows-vista,nlp,speech-recognition,multilingual</td>\n",
       "      <td>3</td>\n",
       "      <td>5661</td>\n",
       "      <td>6</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>&lt;p&gt;Citation from Vista &lt;a href=\"http://blogs.m...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20412</th>\n",
       "      <td>25332</td>\n",
       "      <td>https://stackoverflow.com/questions/25332</td>\n",
       "      <td>What's a good natural language library to use ...</td>\n",
       "      <td>&lt;p&gt;I'm looking for an existing library to summ...</td>\n",
       "      <td>2008-08-24</td>\n",
       "      <td>language-agnostic,nlp</td>\n",
       "      <td>14</td>\n",
       "      <td>6494</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20413</th>\n",
       "      <td>23689</td>\n",
       "      <td>https://stackoverflow.com/questions/23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22</td>\n",
       "      <td>.net,datetime,nlp</td>\n",
       "      <td>27</td>\n",
       "      <td>6489</td>\n",
       "      <td>9</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>&lt;p&gt;We developed exactly what you are looking f...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20414 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       QuestionId                                   QuestionUrl  \\\n",
       "0        79577043  https://stackoverflow.com/questions/79577043   \n",
       "1        79574001  https://stackoverflow.com/questions/79574001   \n",
       "2        79572620  https://stackoverflow.com/questions/79572620   \n",
       "3        79559702  https://stackoverflow.com/questions/79559702   \n",
       "4        79557354  https://stackoverflow.com/questions/79557354   \n",
       "...           ...                                           ...   \n",
       "20409       42489     https://stackoverflow.com/questions/42489   \n",
       "20410       41424     https://stackoverflow.com/questions/41424   \n",
       "20411       36533     https://stackoverflow.com/questions/36533   \n",
       "20412       25332     https://stackoverflow.com/questions/25332   \n",
       "20413       23689     https://stackoverflow.com/questions/23689   \n",
       "\n",
       "                                                   Title  \\\n",
       "0      Unsupervised Topic Modeling for Short Event De...   \n",
       "1      Is there a way to reuse a heavy service across...   \n",
       "2      How can I link tasks using machine learning / ...   \n",
       "3      NameError: name 'init_empty_weights' is not de...   \n",
       "4      Sentencepiece not generating models after prep...   \n",
       "...                                                  ...   \n",
       "20409  How to implement a \"related\" degree measure al...   \n",
       "20410             How do you implement a \"Did you mean\"?   \n",
       "20411     Vista speech recognition in multiple languages   \n",
       "20412  What's a good natural language library to use ...   \n",
       "20413        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                    Body CreationDate  \\\n",
       "0      <p>I have a dataset of approximately 750 lines...   2025-04-16   \n",
       "1      <p>I'm building an Airflow DAG where some of t...   2025-04-14   \n",
       "2      <p>I'm working on an AI model to predict depen...   2025-04-14   \n",
       "3      <p>I am trying to set up hugging face locally ...   2025-04-07   \n",
       "4      <p>So this is the log that I see on the termin...   2025-04-05   \n",
       "...                                                  ...          ...   \n",
       "20409  <p>I was going to Ask a Question earlier today...   2008-09-03   \n",
       "20410  <blockquote>\\n  <p><strong>Possible Duplicate:...   2008-09-03   \n",
       "20411  <p>my primary language is spanish, but I use a...   2008-08-31   \n",
       "20412  <p>I'm looking for an existing library to summ...   2008-08-24   \n",
       "20413  <p>Does anyone know of a .NET date/time parser...   2008-08-22   \n",
       "\n",
       "                                                    Tags  Score  ViewCount  \\\n",
       "0                    machine-learning,nlp,topic-modeling     -1         34   \n",
       "1                              nlp,airflow,microservices      1         19   \n",
       "2      python,machine-learning,nlp,artificial-intelli...     -6         95   \n",
       "3               nlp,huggingface-transformers,huggingface      3        629   \n",
       "4                               python,nlp,sentencepiece      0         54   \n",
       "...                                                  ...    ...        ...   \n",
       "20409  algorithm,machine-learning,indexing,nlp,full-t...      8        457   \n",
       "20410                                                nlp    118      33225   \n",
       "20411  windows-vista,nlp,speech-recognition,multilingual      3       5661   \n",
       "20412                              language-agnostic,nlp     14       6494   \n",
       "20413                                  .net,datetime,nlp     27       6489   \n",
       "\n",
       "       AnswerCount  AcceptedAnswerId  \\\n",
       "0                1               NaN   \n",
       "1                0               NaN   \n",
       "2                0               NaN   \n",
       "3                2        79577000.0   \n",
       "4                0               NaN   \n",
       "...            ...               ...   \n",
       "20409            2           42532.0   \n",
       "20410           11           41448.0   \n",
       "20411            6           36684.0   \n",
       "20412            4               NaN   \n",
       "20413            9          631134.0   \n",
       "\n",
       "                                      AcceptedAnswerBody  AcceptedAnswerScore  \n",
       "0                                                    NaN                  NaN  \n",
       "1                                                    NaN                  NaN  \n",
       "2                                                    NaN                  NaN  \n",
       "3      <p>Try using this version, it should resolve t...                  1.0  \n",
       "4                                                    NaN                  NaN  \n",
       "...                                                  ...                  ...  \n",
       "20409  <p>One such way to implement such an algorithm...                  5.0  \n",
       "20410  <p>Actually what Google does is very much non-...                 87.0  \n",
       "20411  <p>Citation from Vista <a href=\"http://blogs.m...                  8.0  \n",
       "20412                                                NaN                  NaN  \n",
       "20413  <p>We developed exactly what you are looking f...                 12.0  \n",
       "\n",
       "[20414 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read file\n",
    "df_post = pd.read_csv(\"Dataset_Full.csv\")\n",
    "\n",
    "# Remove time from Creation Date\n",
    "df_post['CreationDate'] = pd.to_datetime(df_post['CreationDate']).dt.date\n",
    "df_post\n",
    "\n",
    "#Tag in format <Tag1><Tag2>. Remove < > and join it with ','\n",
    "df_post['Tags'] = df_post['Tags'].str.findall(r'<([^>]+)>').apply(lambda lst: ','.join(lst))\n",
    "df_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution posts that no answers, answers, accepted answers\n",
    "\n",
    "#Prepare some count\n",
    "no_answers   = (df_post['AnswerCount'] == 0).sum()\n",
    "has_answers  = ((df_post['AnswerCount'] > 0) & df_post['AcceptedAnswerId'].isna()).sum()\n",
    "accepted     = df_post['AcceptedAnswerId'].notna().sum()\n",
    "\n",
    "#Put into dict\n",
    "dist_answer = {'No Answers':no_answers,'Has Answers':has_answers,'Accepted Answers':accepted}\n",
    "plt.figure()\n",
    "sns.barplot(x=dist_answer.keys(), y=dist_answer.values())\n",
    "plt.title('Distribution of Posts by Answer Status')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create some Range \n",
    "bins = [0, 10, 100, 1000, 10000, df_post['ViewCount'].max()]\n",
    "labels = ['0-10','11-100','101-1k','1k-10k','>10k']\n",
    "\n",
    "df_post['view_range'] = pd.cut(df_post['ViewCount'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "plt.figure()\n",
    "sns.countplot(data=df_post, x='view_range', order=labels)\n",
    "plt.title('Post Counts by ViewCount Range')\n",
    "plt.xlabel('ViewCount Range')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Filter Data only have accepted answer\n",
    "df_post_answer = df_post.dropna(subset=['AcceptedAnswerId']).reset_index().drop('index',axis=1)\n",
    "\n",
    "plt.figure()\n",
    "sns.countplot(data=df_post_answer, x='view_range', order=labels)\n",
    "plt.title('Post Counts by ViewCount Range for Accepted Dataset')\n",
    "plt.xlabel('ViewCount Range')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxUAAAGGCAYAAAANcKzOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/aklEQVR4nO3dB3gUVff48ZMQSDAQkN6LSAepCpGmgiBNeMHXAkoRwQLSlOaLdKSJNAEbLUpTRBREOoJ0CEWaNFFQmtI7JJn/c+7/t/vsJiEkmWw22Xw/zzPJ7szsztk+Z+69Z/wsy7IEAAAAABLJP7E3BAAAAABFUgEAAADAFpIKAAAAALaQVAAAAACwhaQCAAAAgC0kFQAAAABsIakAAAAAYAtJBQAAAABbSCoAAAAA2EJSAQCpzM8//yx+fn6yYMECSQ3Onj0rzz33nGTPnt3EPX78ePFVqe21AYCkQlIBALGYOXOm2TkMCgqSv//+O8byJ554QsqVK+eV2FKbHj16yPLly6Vfv37y5ZdfyjPPPHPPdfU5d0z+/v6SL18+qV+/vtlZ94Q5c+akqiTn7t27Ur58eSlWrJjcvHkzxvI//vhDHnjgAfnvf//rlfgApF0kFQAQh9u3b8vIkSO9HUaqtmbNGmnWrJm8++678vLLL0upUqXiXP/pp582ycesWbPkjTfekF9//VWeeuop+emnnyStJxXp06eXzz77TI4fPy5Dhw6NsbxLly6SIUMGmThxolfiA5B2kVQAQBwqVqwon3/+uZw6dUrSmuvXryfJ/Zw7d06yZs0a7/VLlChhko9XXnlFBgwYICtXrhTLslLVzr8nhYaGmmTrww8/lP379zvnf/vtt/Ljjz/KiBEjJG/evKnm/QHAN5BUAEAc3nvvPYmMjLxva4V2O9EuO9ptKjqdP2jQIOd1vazzDh8+bHaes2TJIjlz5pT333/f7DyfPHnSHNkPCQmRPHnyyNixY2Pdpsal8ek6wcHB8uyzz5rbRrd161bT5Ui3o11j6tSpIxs3bnRbxxHTgQMHpFWrVvLggw9KzZo143zMv//+u+lmky1bNnO/1atXNzu10buQ6WOaPHmys1tTQml3nxw5cpij866tH7Vq1TKPWxMWfb4OHjzodrurV69K9+7dpUiRIhIYGCi5cuUyrSA7d+50dmHTeP/8809nbLquw6RJk6Rs2bLmsenzUbVqVdOyER/3e20GDhxoWh3++eefGLft1KmTeUy3bt265/1r4qDPiSYX+vxeu3bNPFZHwhHf110f+1tvvSUlS5aUjBkzmnEv+prq+9mV47Vct26dWV+fywIFCsTruQCQNgR4OwAASMmKFi0qbdq0Ma0Vffv2NX38k8oLL7wgpUuXNgmL7twOGzbM7KB/+umnprvPqFGjZPbs2abb0KOPPiq1a9d2u/3w4cPNjl6fPn1Ma4Aeya9Xr57s3r3b7CA6dr4bNmwoVapUMTuyOk5hxowZ5v5/+eUXeeyxx9zuU3coixcvLh988IHZWY1r8PXjjz8uN27ckK5du5qdUe2upDvPOkj5P//5j4lXuzFpi4PuzOvzmBgXL14008MPP2yur1q1yjymhx56yCRDOrZAE4AaNWqYhMGRGOjOtcaiXYLKlCkj58+flw0bNpjko3LlyvK///1PLl++LH/99ZeMGzfO3CZTpkzmv77e+rh0gHm3bt3MDr52w9IddU267ud+r40+J0OGDJH58+eb+Bzu3LljYm7ZsqUZz3MvmihoFyd9vb744guTDOprol3EdLvxfd23b98umzZtkhdffNEkCZpMTJ061SRcep+ajLjShEITYG1BoqUCgBsLABDDjBkzdI/a2r59u3Xs2DErICDA6tq1q3N5nTp1rLJlyzqvHz9+3Kyvt4tO5w8cONB5XS/rvE6dOjnnRUREWAUKFLD8/PyskSNHOudfvHjRypgxo9W2bVvnvLVr15rb58+f37py5Ypz/tdff23mT5gwwVyPioqyihcvbjVo0MBcdrhx44ZVtGhR6+mnn44R00svvRSv56d79+5m/V9++cU57+rVq+Z+ixQpYkVGRro9/s6dO8frfnXdDh06WP/884917tw5a+vWrVbdunXN/LFjx5p1KlasaOXKlcs6f/6883Z79uyx/P39rTZt2jjnZcmS5b7bbdy4sVW4cOEY85s1a+b2+sZXfF8bFRoaalWrVs3t9gsXLjTr6f3ER5MmTczjTJcundWvX78Ev+46L7rNmzebGMLCwmJ8HmrWrGneqwAQHd2fAOA+9Ii4HlnWAbKnT59Osvt97bXXnJfTpUtnutfofnWHDh2c87UbjHZN0a5G0emR/8yZMzuv61F17Uu/dOlSc12Pih85csQcWdej9P/++6+Z9Ahz3bp1Zf369RIVFeV2n46uM/ej29Cj3a5dpPQov3bd0aPdepQ7saZNm2aOhmsXm2rVqpkuOz179jTde/T518fVrl0706rj8Mgjj5jWEMdjdzx32rKQmPEwelttwdAj+Ylxv9fGsY7Gd+zYMec8bZkqWLCg6aoUH9qtTFs39DbafS6hr7ujRctRWUrX1xYhffyObmKuOnbsaN6rABAdSQUAxEP//v0lIiIiSStBFSpUKEaXFu3yon3lo8/X7j/RaTclV9rtRXcIHf3hdcdStW3b1uyku07aZUYrW2n3n+jdveJD++JrshOddudyLE8sHR+hg7O1m5PudOsOsY4r0S48jvu917YdO89q9OjRsm/fPrPDrQmQdpWKLTmLjXZb0iRJb6fPc+fOnWOMR4jL/V4bR/c3HeuhiYTS12LJkiXSunXreI890feQJl869sORICTkddeuY9qVSZ8jjUXfe7repUuXYrw3EvL+AJD2MKYCAOLZWqGDqrW1QsdWRHevnUAdsHsvsR3xvddR4LjGN9yL42j0mDFjTBWr2DjGEDi4Hrn2Fu3br+MP7Hr++efNYO7vvvtOVqxYYZ4HHaeycOFCM94gLpqgHDp0yOzkL1u2zFRWmjJlitkBHzx4sCQFHfzdpEkTk1To/epYCt3h1/eZHQl53d9++20z1sIxyFsTWH0v6xiL6K1YKeX9ASBlIqkAgAS0Vnz11VdmxzS2HUSlR3hd2Tlifz+OI9KuicfRo0dNVyClJ0hTWkUqKXbSXRUuXNjsdEf322+/OZd7guN+77VtPdKu1ZYctMuRDi7WSQdM6wBtHUTtSCriahHQ+9HWBJ20i1GLFi3MbfUkfnENoo7Pa+PaBUpbZrSblSYXlSpVMq0OdiTkdddERls0XCuM6aD06O9jALgfuj8BQAJ21vQoslZnOnPmjNsy3YHTHVrtr+5Kj257SlhYmCmb6rqDqGMOHDvMWvlHY9bzGWjJ0ehiK2caX40aNZJt27bJ5s2bnfO025G25Gj1Ja225AmaJOjRd6005brjq92ctDVC43K0EEXvvqPdhLR6l7YGuCYOsXXz0bEFrvSEcvqYNDnQsQd2XxsHva7vG01UtVyr3VaKhL7u2jIWvRVMK2nF1cIGALGhpQIAEkDLkGqZVD1SHv2Isg681jEX+l8HXWuCoeei8BQdqKwDpdu3b2/KiWrZUu23r4NplY5B0D70uuOqsep6+fPnl7///lvWrl1rEqHFixcnatvaBWzu3LnmvrX0qsaiO/p6LgntKqTb9hTt1qPb1e46OqjdUVJWu+44zgeiO/TajUoHSFeoUMF099ExGtoi4HpUXnfAtayrDgTXsr26XtOmTaV+/frmHBNapjZ37tymDO3HH38sjRs3dhuAndjXxkHPVaFdjfS+dQf/pZdesv38JOR11+5X+n7W506TJk0S9XnSEsEAkCAx6kEBANxKykan5V11WfSSo1qeU8uhaonPzJkzW88//7wpi3qvkrJaNjX6/QYHB8fYXvTytY6ypXPnzjVlRLW8qpad1fKof/75Z4zb79q1y2rRooWVPXt2KzAw0JRQ1dhWr15935jioqV2n3vuOStr1qxWUFCQ9dhjj1lLliyJsV5CS8rGZ91Vq1ZZNWrUMI87JCTEatq0qXXgwAHn8tu3b1u9evWyKlSoYF4LfV718pQpU9zu59q1a1arVq3MY9BtO8rLfvrpp1bt2rWdz1mxYsXM/V2+fDnOuBL62qht27aZ29SvX99KDI1Z7z8xr7uWLG7fvr2VI0cOK1OmTKYM7W+//WbWdS1jHNfnAQCUn/5JWBoCAACSyp49e0yXLu0ypaWLASA1YkwFAABepGfv1m5XOhAcAFIrxlQAAOAFOq5BTxKog9u7dOniVrUKAFIbuj8BAOAFWiVLB3E3aNDADJaOzwBwAEipSCoAAAAA2MKYCgAAAAC2kFQAAAAAsIWB2vEQFRUlp06dMv1d/fz8vB0OAAAAkCx0pISeUDRfvnxxntiUpCIeNKEoWLCgt8MAAAAAvOLkyZNSoECBey4nqYgHR0UOfTJDQkK8HQ4AAACQLK5cuWIOrt+vQh1JRTw4ujxpQkFSAQAAgLTG7z5DABioDQAAAMAWkgoAAAAAtpBUAAAAALCFpAIAAACALSQVAAAAAGwhqQAAAABgC0kFAAAAAFtIKgAAAADYQlIBAAAAwBaSCgAAAAC2kFQAAAAAsIWkAgAAAIAtAfZuDgAAkDSq9Arz2rbDx7Tx2rYBX0BLBQAAAABbSCoAAAAA2EJSAQAAAMAWkgoAAAAAtpBUAAAAALCFpAIAAACALSQVAAAAAGwhqQAAAABgC0kFAAAAAFtIKgAAAADYQlIBAAAAwBaSCgAAAACpN6lYv369NG3aVPLlyyd+fn6yaNEit+WWZcmAAQMkb968kjFjRqlXr54cOXLEbZ0LFy5I69atJSQkRLJmzSodOnSQa9euua3z66+/Sq1atSQoKEgKFiwoo0ePTpbHBwAAAKQFXk0qrl+/LhUqVJDJkyfHulx3/idOnCiffPKJbN26VYKDg6VBgwZy69Yt5zqaUOzfv19WrlwpS5YsMYlKp06dnMuvXLki9evXl8KFC0t4eLiMGTNGBg0aJJ999lmyPEYAAADA1wV4c+MNGzY0U2y0lWL8+PHSv39/adasmZkXFhYmuXPnNi0aL774ohw8eFCWLVsm27dvl6pVq5p1Jk2aJI0aNZIPP/zQtIDMnj1b7ty5I9OnT5cMGTJI2bJlZffu3fLRRx+5JR8AAAAAfGxMxfHjx+XMmTOmy5NDlixZpFq1arJ582ZzXf9rlydHQqF0fX9/f9Oy4Vindu3aJqFw0NaOQ4cOycWLF5P1MQEAAAC+yKstFXHRhEJpy4Qrve5Ypv9z5crltjwgIECyZcvmtk7RokVj3Idj2YMPPhhj27dv3zaTaxcqAAAAAKmspcKbRowYYVpFHJMO7gYAAACQypKKPHnymP9nz551m6/XHcv0/7lz59yWR0REmIpQruvEdh+u24iuX79+cvnyZed08uTJJHxkAAAAgG9JsUmFdlnSnf7Vq1e7dUPSsRKhoaHmuv6/dOmSqerksGbNGomKijJjLxzraEWou3fvOtfRSlElS5aMteuTCgwMNCVqXScAAAAAKTCp0PNJaCUmnRyDs/XyiRMnzHkrunfvLsOGDZMffvhB9u7dK23atDEVnZo3b27WL126tDzzzDPSsWNH2bZtm2zcuFG6dOliKkPpeqpVq1ZmkLaev0JLz86fP18mTJggPXv29OZDBwAAAHyGVwdq79ixQ5588knndceOftu2bWXmzJnSu3dvcy4LLf2qLRI1a9Y0JWT1JHYOWjJWE4m6deuaqk8tW7Y057Zw0DERK1askM6dO0uVKlUkR44c5oR6lJMFAAAAkoafpSeEQJy025UmJzq+gq5QAAB4RpVeYV7bdviYNl7bNuAL+8EpdkwFAAAAgNSBpAIAAACALSQVAAAAAGwhqQAAAACQeqs/AQCAtDEQWjEYGvBdtFQAAAAAsIWkAgAAAIAtJBUAAAAAbCGpAAAAAGALSQUAAAAAW0gqAAAAANhCUgEAAADAFpIKAAAAALaQVAAAAACwhaQCAAAAgC0kFQAAAABsIakAAAAAYAtJBQAAAABbSCoAAAAA2EJSAQAAAMAWkgoAAAAAtpBUAAAAALCFpAIAAACALSQVAAAAAGwhqQAAAABgC0kFAAAAAFsC7N0cAADA91XpFea1bYePaeO1bQPxRUsFAAAAAFtIKgAAAADYQlIBAAAAwBaSCgAAAAC2kFQAAAAAsIWkAgAAAIAtJBUAAAAAbCGpAAAAAGALSQUAAAAAW0gqAAAAANhCUgEAAADAFpIKAAAAALaQVAAAAACwhaQCAAAAgC0kFQAAAABsCbB3cwAA4KpKrzCvbj98TBuvbh9A2kRLBQAAAADfTSoiIyPl/fffl6JFi0rGjBmlWLFiMnToULEsy7mOXh4wYIDkzZvXrFOvXj05cuSI2/1cuHBBWrduLSEhIZI1a1bp0KGDXLt2zQuPCAAAAPA9KTqpGDVqlEydOlU+/vhjOXjwoLk+evRomTRpknMdvT5x4kT55JNPZOvWrRIcHCwNGjSQW7duOdfRhGL//v2ycuVKWbJkiaxfv146derkpUcFAAAA+JYUPaZi06ZN0qxZM2ncuLG5XqRIEZk7d65s27bN2Uoxfvx46d+/v1lPhYWFSe7cuWXRokXy4osvmmRk2bJlsn37dqlatapZR5OSRo0ayYcffij58uXz4iMEAAAAUr8U3VLx+OOPy+rVq+Xw4cPm+p49e2TDhg3SsGFDc/348eNy5swZ0+XJIUuWLFKtWjXZvHmzua7/tcuTI6FQur6/v79p2YjN7du35cqVK24TAAAAgFTYUtG3b1+zQ1+qVClJly6dGWMxfPhw051JaUKhtGXClV53LNP/uXLlclseEBAg2bJlc64T3YgRI2Tw4MEeelQAAACAb0nRLRVff/21zJ49W+bMmSM7d+6UWbNmmS5L+t+T+vXrJ5cvX3ZOJ0+e9Oj2AAAAgNQsRbdU9OrVy7RW6NgIVb58efnzzz9NS0Lbtm0lT548Zv7Zs2dN9ScHvV6xYkVzWdc5d+6c2/1GRESYilCO20cXGBhoJgAAAACpvKXixo0bZuyDK+0GFRUVZS5rqVlNDHTchYN2l9KxEqGhoea6/r906ZKEh4c711mzZo25Dx17AQAAAMCHWyqaNm1qxlAUKlRIypYtK7t27ZKPPvpIXn31VbPcz89PunfvLsOGDZPixYubJEPPa6EVnZo3b27WKV26tDzzzDPSsWNHU3b27t270qVLF9P6QeUnAAAAwMeTCi39qknCW2+9ZbowaRLw+uuvm5PdOfTu3VuuX79uzjuhLRI1a9Y0JWSDgoKc6+i4DE0k6tata1o+WrZsac5tAQAAAMDHk4rMmTOb81DodC/aWjFkyBAz3YtWetLB3gAAAADS2JgKAAAAACkfSQUAAAAAW0gqAAAAACRvUnHz5k1T6tVBzxuhYx5WrFhhLxIAAAAAaSOpaNasmYSFhZnLWm1Jz/UwduxYM3/q1KmeiBEAAACALyUVO3fulFq1apnLCxYskNy5c5vWCk00KNMKAAAApD0JLimrXZ+01KvSLk8tWrQw536oXr26SS4AAPC0Kr3+f4u5N4SPaeO1bQOAz7RUPPzww7Jo0SI5efKkLF++XOrXr2/m68npQkJCPBEjAAAAAF9KKvRs1u+++64UKVLEjKcIDQ11tlpUqlTJEzECAAAA8KXuT88995zUrFlTTp8+LRUqVHDOr1u3rukKBQAAACBtSXBLxauvvirBwcGmVULHUjiULVtWRo0aldTxAQAAAPC1pGLWrFnmXBXR6TxHqVkAAAAAaUe8uz9duXJFLMsy09WrVyUoKMi5LDIyUpYuXSq5cuXyVJwAAAAAUntSkTVrVvHz8zNTiRIlYizX+YMHD07q+AAAAAD4SlKxdu1a00rx1FNPybfffivZsmVzLsuQIYMULlxY8uXL56k4AQAAAKT2pKJOnTrm//Hjx6VQoUKmZQIAAAAAEjxQ++DBg7Jx40bn9cmTJ0vFihWlVatWcvHixaSODwAAAICvJRW9evUyg7bV3r17pWfPntKoUSPTgqGXAQAAAKQtCT75nSYPZcqUMZd1bEXTpk3lgw8+kJ07d5rkAgAAAEDakuCWCh2UfePGDXN51apVUr9+fXNZB247WjAAAAAApB0JbqmoWbOm6eZUo0YN2bZtm8yfP9/MP3z4sBQoUMATMQIAAADwpZaKjz/+WAICAmTBggUydepUyZ8/v5n/008/yTPPPOOJGAEAAAD4UkuFlpNdsmRJjPnjxo1LqpgAAAAA+HJSoSIjI2XRokWmvKwqW7asPPvss5IuXbqkjg8AAACAryUVR48eNVWe/v77bylZsqSZN2LECClYsKD8+OOPUqxYMU/ECQAAAMBXxlR07drVJA4nT540ZWR1OnHihBQtWtQsAwAAAJC2JLilYt26dbJlyxZTQtYhe/bsMnLkSFMRCgAAAEDakuCWisDAQLl69WqM+deuXTPnsAAAAACQtiQ4qWjSpIl06tRJtm7dKpZlmUlbLt544w0zWBsAAABA2pLg7k8TJ06Utm3bSmhoqKRPn97Mi4iIMAnFhAkTPBEjAAAA7qFKrzCvbTt8TBuvbRupPKnImjWrfP/996YKlKOkbOnSpeXhhx/2RHwAAAAAfCWpiIqKkjFjxsgPP/wgd+7ckbp168rAgQMlY8aMno0QAAAAgG+MqRg+fLi89957kilTJsmfP7/p6tS5c2fPRgcAAADAd5KKsLAwmTJliixfvtycTXvx4sUye/Zs04IBAAAAIO2Kd1KhJ7jTM2k71KtXT/z8/OTUqVOeig0AAACALyUVWuEpKCjIbZ5Wf7p7964n4gIAAADgawO19XwU7dq1Mye/c7h165Y5P0VwcLBz3sKFC5M+SgAAAACpP6nQc1NE9/LLLyd1PAAAAAB8NamYMWOGZyMBAAAA4NtjKgAAAAAgNiQVAAAAAGwhqQAAAABgC0kFAAAAAM8nFZUrV5aLFy+ay0OGDJEbN25Icvn7779Nlans2bNLxowZpXz58rJjxw63UrcDBgyQvHnzmuV6Ur4jR4643ceFCxekdevWEhISIlmzZpUOHTrItWvXku0xAAAAAJLWk4qDBw/K9evXzeXBgwcn2w65JjI1atQwJ9n76aef5MCBAzJ27Fh58MEHneuMHj1aJk6cKJ988ols3brVnDOjQYMG5hwaDppQ7N+/X1auXClLliyR9evXS6dOnZLlMQAAAAC+Ll4lZStWrCjt27eXmjVrmpaBDz/8UDJlyhTrutpqkFRGjRolBQsWdCtnW7RoUedljWX8+PHSv39/adasmZkXFhYmuXPnlkWLFsmLL75oEqJly5bJ9u3bpWrVqmadSZMmSaNGjczjyJcvX5LFCwAAAKRF8WqpmDlzpul+pEf5/fz8TKvBd999F2PSHfmk9MMPP5hE4L///a/kypVLKlWqJJ9//rlz+fHjx+XMmTOmy5NDlixZpFq1arJ582ZzXf9rlydHQqF0fX9/f9OyAQAAACAZWipKliwp8+bNM5d1Z3z16tVmJ9/Tfv/9d5k6dar07NlT3nvvPdPa0LVrV8mQIYM5w7cmFEpbJlzpdccy/R891oCAAMmWLZtznehu375tJocrV6544NEBAAAAaeyM2g5RUVGeieQe29IWhg8++MBc15aKffv2mfETmlR4yogRI8zYEQAAAAAeKil77Ngxefvtt003Ip209UDnJTWt6FSmTBm3eaVLl5YTJ06Yy3ny5DH/z54967aOXncs0//nzp1zWx4REWEqQjnWia5fv35y+fJl53Ty5MkkfVwAAABAmk4qli9fbnb0t23bJo888oiZdGxC2bJlTXWlpKSVnw4dOuQ27/Dhw1K4cGHnoG1NDLQ7lmtXJY0nNDTUXNf/ly5dkvDwcOc6a9asMa0gOvYiNoGBgab8rOsEAAAAIIm6P/Xt21d69OghI0eOjDG/T58+8vTTT0tS0e08/vjjpvvT888/bxKZzz77zExKB413795dhg0bJsWLFzdJxvvvv28qOjVv3tzZsvHMM89Ix44dTbepu3fvSpcuXUxlKCo/AQAAAF5oqdASrXryuOheffVVcx6JpPToo4+aqlJz586VcuXKydChQ00JWT3vhEPv3r1NVyw974Sur+fQ0BKyQUFBznVmz54tpUqVkrp165pSsloa15GYAAAAAEjmloqcOXPK7t27TcuAK53niYpQTZo0MdO9aGuFnuVbp3vRSk9z5sxJ8tgAAAAAJCKp0G5E2iqg5V61a5LauHGjOVGdln4FAAAAkLYkOKnQMQuZM2eWsWPHmipJSscmDBo0yFSBAgAAAJC2JDip0O5GOoBap6tXr5p5mmQAAAAASJsSnFS4IpkAAAAAkKiT3wEAAACAA0kFAAAAAFtIKgAAAAAkX1KhZ6PWE8gdOXLE3lYBAAAApM2B2unTp5dff/3Vc9EAAFKMKr3CvLbt8DFtvLZtAEAydH96+eWXZdq0aYnYFAAAAABflOCSshERETJ9+nRZtWqVVKlSRYKDg92Wf/TRR0kZHwAAAABfSyr27dsnlStXNpcPHz4c48R4AAAAANKWBCcVa9eu9UwkAAAAANJWSdmjR4/K8uXL5ebNm+a6ZVlJGRcAAAAAX00qzp8/b8rKlihRQho1aiSnT5828zt06CDvvPOOJ2IEAAAA4EtJRY8ePUxp2RMnTsgDDzzgnP/CCy/IsmXLkjo+AAAAAL42pmLFihWm21OBAgXc5hcvXlz+/PPPpIwNAHwe54IAAKTJlorr16+7tVA4XLhwQQIDA5MqLgAAAAC+mlTUqlVLwsLC3MrIRkVFyejRo+XJJ59M6vgAAAAA+Fr3J00edKD2jh075M6dO9K7d2/Zv3+/aanYuHGjZ6IEAAAA4DstFeXKlTMnvatZs6Y0a9bMdIdq0aKF7Nq1S4oVK+aZKAEAAAD4TkuFypIli/zvf/9L+mgAAAAApI2k4uLFizJt2jQ5ePCguV6mTBlp3769ZMuWLanjAwAAAOBr3Z/Wr18vRYoUkYkTJ5rkQie9XLRoUbMMAAAAQNqS4JaKzp07mxPdTZ06VdKlS2fmRUZGyltvvWWW7d271xNxAgAAAPCVloqjR4/KO++840wolF7u2bOnWQYAAAAgbUlwUlG5cmXnWApXOq9ChQpJFRcAAAAAX+r+9Ouvvzovd+3aVbp162ZaJapXr27mbdmyRSZPniwjR470XKQAAAAAUm9SUbFiRXPmbMuynPP0pHfRtWrVyoy3AAAAAJB2xCupOH78uOcjAQAAAOC7SUXhwoU9HwkAAACAtHPyu1OnTsmGDRvk3LlzEhUV5bZMx1wAAAAASDsSnFTMnDlTXn/9dcmQIYNkz57djLVw0MskFQAAAEDakuCk4v3335cBAwZIv379xN8/wRVpAQAAAPiYBGcFN27ckBdffJGEAgAAAICR4MygQ4cO8s033yT0ZgAAAAB8VIK7P40YMUKaNGkiy5Ytk/Lly0v69Ondln/00UdJGR8AAAAAX0wqli9fLiVLljTXow/UBgAAAJC2JDipGDt2rEyfPl3atWvnmYgAAAAA+PaYisDAQKlRo4ZnogEAAADg+0lFt27dZNKkSZ6JBgAAAIDvd3/atm2brFmzRpYsWSJly5aNMVB74cKFSRkfAAAAAF9LKrJmzSotWrTwTDQAAAAAfD+pmDFjhnjLyJEjzZm8tQvW+PHjzbxbt27JO++8I/PmzZPbt29LgwYNZMqUKZI7d27n7U6cOCFvvvmmrF27VjJlyiRt27Y1VawCAhL88AEAAABEk2r2qrdv3y6ffvqpPPLII27ze/ToIT/++KM5IV+WLFmkS5cupiVl48aNZnlkZKQ0btxY8uTJI5s2bZLTp09LmzZtTLetDz74wEuPBkByqdIrzKvbDx/TxqvbBwAgRSYVRYsWjfN8FL///rsktWvXrknr1q3l888/l2HDhjnnX758WaZNmyZz5syRp556ytmSUrp0admyZYtUr15dVqxYIQcOHJBVq1aZ1ouKFSvK0KFDpU+fPjJo0CDJkCFDkscLAAAApCUJTiq6d+/udv3u3buya9cuc4btXr16iSd07tzZtDbUq1fPLakIDw8329f5DqVKlZJChQrJ5s2bTVKh//XM367dobSLlHaH2r9/v1SqVMkjMQMAAABpRYKTCh3PEJvJkyfLjh07JKnpWImdO3ea7k/RnTlzxrQ06OBxV5pA6DLHOq4JhWO5Y1lsdGyGTg5XrlxJkscCAAAA+KIEn6fiXho2bCjffvutJKWTJ0+aJGb27NkSFBQkyUUHcev4DMdUsGDBZNs2AAAAkGaTigULFki2bNkkKWn3pnPnzknlypVNpSad1q1bJxMnTjSXtcXhzp07cunSJbfbnT171gzMVvpfr0df7lgWG60wpeM1HJMmNwAAAACSqPuTjkFwHahtWZbpRvTPP/+YUq5JqW7durJ37163ee3btzfjJnSgtbYgaBWn1atXS8uWLc3yQ4cOmRKyoaGh5rr+Hz58uElOcuXKZeatXLlSQkJCpEyZMrFuNzAw0EwAAAAAPJBUNG/e3O26v7+/5MyZU5544gmzs5+UMmfOLOXKlXObFxwcLNmzZ3fO79Chg/Ts2dO0kmii8Pbbb5tEQgdpq/r165vk4ZVXXpHRo0ebBKh///5m8DeJAwAAAOCFpGLgwIGSkowbN84kNtpS4XryO4d06dLJkiVLTLUnTTY0KdGT3w0ZMsSrcQMAAAC+ItWc/M7h559/druuA7i18pRO91K4cGFZunRpMkQHAAAApD3xTiq0NSCuk94pXR4REZEUcQEAAADwtaTiu+++u+cyPcGcVmSKiopKqrgAAAAA+FpS0axZsxjztNJS3759ZfHixdK6dWvGKQAeVKVXmNe2HT6mjde2DQAAfPQ8FadOnZKOHTtK+fLlTXen3bt3y6xZs8zYBQAAAABpS4KSCj0RnJ4f4uGHH5b9+/eb80NoK0X0sq8AAAAA0o54d3/SczyMGjXKnIV67ty5sXaHAgAAAJD2xDup0LETGTNmNK0U2tVJp9gsXLgwKeMDAABAKsV4wLQj3klFmzZt7ltSFgAAAEDaE++kYubMmZ6NBAAAAEDaqf4EAAAAAA4kFQAAAABsIakAAAAAkDxjKgAgJVb3UFT4AADAu2ipAAAAAGALSQUAAAAAW0gqAAAAANhCUgEAAADAFpIKAAAAALZQ/QlIIVWMqGAEAABSK1oqAAAAANhCUgEAAADAFpIKAAAAALaQVAAAAACwhaQCAAAAgC0kFQAAAABsIakAAAAAYAtJBQAAAABbSCoAAAAA2EJSAQAAAMAWkgoAAAAAtpBUAAAAALCFpAIAAACALSQVAAAAAGwhqQAAAABgC0kFAAAAAFtIKgAAAADYQlIBAAAAwBaSCgAAAAC2kFQAAAAAsIWkAgAAAIAtAfZuDiRclV5hXtt2+Jg2Xts2AACAr6KlAgAAAIAtJBUAAAAAbCGpAAAAAOC7ScWIESPk0UcflcyZM0uuXLmkefPmcujQIbd1bt26JZ07d5bs2bNLpkyZpGXLlnL27Fm3dU6cOCGNGzeWBx54wNxPr169JCIiIpkfDQAAAOCbUnRSsW7dOpMwbNmyRVauXCl3796V+vXry/Xr153r9OjRQxYvXizffPONWf/UqVPSokUL5/LIyEiTUNy5c0c2bdoks2bNkpkzZ8qAAQO89KgAAAAA35Kiqz8tW7bM7bomA9rSEB4eLrVr15bLly/LtGnTZM6cOfLUU0+ZdWbMmCGlS5c2iUj16tVlxYoVcuDAAVm1apXkzp1bKlasKEOHDpU+ffrIoEGDJEOGDF56dAAAAIBvSNEtFdFpEqGyZctm/mtyoa0X9erVc65TqlQpKVSokGzevNlc1//ly5c3CYVDgwYN5MqVK7J///5Yt3P79m2z3HUCAAAAkMqTiqioKOnevbvUqFFDypUrZ+adOXPGtDRkzZrVbV1NIHSZYx3XhMKx3LHsXmM5smTJ4pwKFizooUcFAAAApH6pJqnQsRX79u2TefPmeXxb/fr1M60ijunkyZMe3yYAAACQWqXoMRUOXbp0kSVLlsj69eulQIECzvl58uQxA7AvXbrk1lqh1Z90mWOdbdu2ud2fozqUY53oAgMDzQQAAAAglbdUWJZlEorvvvtO1qxZI0WLFnVbXqVKFUmfPr2sXr3aOU9LzmoJ2dDQUHNd/+/du1fOnTvnXEcrSYWEhEiZMmWS8dEAAAAAvikgpXd50spO33//vTlXhWMMhI5zyJgxo/nfoUMH6dmzpxm8rYnC22+/bRIJrfyktAStJg+vvPKKjB492txH//79zX3TGgEAAAD4eFIxdepU8/+JJ55wm69lY9u1a2cujxs3Tvz9/c1J77Rqk1Z2mjJlinPddOnSma5Tb775pkk2goODpW3btjJkyJBkfjQAAACAbwpI6d2f7icoKEgmT55spnspXLiwLF26NImjAwAAAJDix1QAAAAASPlIKgAAAADYQlIBAAAAwBaSCgAAAAC2kFQAAAAAsIWkAgAAAIAtJBUAAAAAbCGpAAAAAGALSQUAAAAAW0gqAAAAANhCUgEAAADAFpIKAAAAALaQVAAAAACwhaQCAAAAgC0kFQAAAABsCbB3cwAAACD1qdIrzKvbDx/TRnwJLRUAAAAAbKGlwkd5M/v2tcwbAAAAcaOlAgAAAIAtJBUAAAAAbCGpAAAAAGALSQUAAAAAW0gqAAAAANhCUgEAAADAFpIKAAAAALaQVAAAAACwhaQCAAAAgC0kFQAAAABsIakAAAAAYAtJBQAAAABbSCoAAAAA2EJSAQAAAMAWkgoAAAAAtpBUAAAAALCFpAIAAACALSQVAAAAAGwhqQAAAABgC0kFAAAAAFtIKgAAAADYEmDv5mlXlV5hXt1++Jg2Xt0+AAAA4EBLBQAAAABbSCoAAAAA2EJSAQAAAMCWNJVUTJ48WYoUKSJBQUFSrVo12bZtm7dDAgAAAFK9NJNUzJ8/X3r27CkDBw6UnTt3SoUKFaRBgwZy7tw5b4cGAAAApGpppvrTRx99JB07dpT27dub65988on8+OOPMn36dOnbt6+3wwMAAABSbZXRNNFScefOHQkPD5d69eo55/n7+5vrmzdv9mpsAAAAQGqXJloq/v33X4mMjJTcuXO7zdfrv/32W4z1b9++bSaHy5cvm/9Xrlxxzou8fVO8yTWW2HgzPmJLHGLzTHwpOTbF6xo7Yks8Pg+JQ2yJQ2y+/1m98n+XLcuK8zZ+1v3W8AGnTp2S/Pnzy6ZNmyQ0NNQ5v3fv3rJu3TrZunWr2/qDBg2SwYMHeyFSAAAAIOU5efKkFChQIG23VOTIkUPSpUsnZ8+edZuv1/PkyRNj/X79+plB3Q5RUVFy4cIFyZ49u/j5+dmORzO+ggULmhcnJCREUhJi8834iC1xiM33Ykvp8RFb4hCbb8ZHbCkjNm1/uHr1quTLly/O9dJEUpEhQwapUqWKrF69Wpo3b+5MFPR6ly5dYqwfGBhoJldZs2ZN8rj0hU5pb0QHYvPN+IgtcYjN92JL6fERW+IQm2/GR2zejy1Lliz3XSdNJBVKWx7atm0rVatWlccee0zGjx8v169fd1aDAgAAAJA4aSapeOGFF+Sff/6RAQMGyJkzZ6RixYqybNmyGIO3AQAAACRMmkkqlHZ1iq27U3LTrlV6Er7oXaxSAmLzzfiILXGIzfdiS+nxEVviEJtvxkdsqSu2NFH9CQAAAIDnpImT3wEAAADwHJIKAAAAALaQVAAAAACwhaQiEUaMGCGPPvqoZM6cWXLlymXOfXHo0CG3dW7duiWdO3c2J8zLlCmTtGzZMsbJ906cOCGNGzeWBx54wNxPr169JCIiwm2d2bNnS4UKFcw6efPmlVdffVXOnz+fYuKbPHmylC5dWjJmzCglS5aUsLCwZImta9eu5twjOghJK3nF5tdff5VatWpJUFCQOQnM6NGjU0Rseh/t2rWT8uXLS0BAgPPcKSkhtp9//lmaNWtm3mvBwcFmHX0PpoTY9D6ffPJJU7FNX9OHHnpI+vfvL3fv3k0R8bk6evSo2d79zm+TXLH98ccf5sSd0actW7Z4PTalQ/s+/PBDKVGihFkvf/78Mnz4cK/HNmjQoFifN/1seDs2tXz5cqlevbrZVs6cOc396GudEmL7+uuvzTL9/ShcuLCMGTPmnnElZXx79uyRl156yXzn6++S/j5NmDAh1u+6ypUrm8fw8MMPy8yZM1NEbKdPn5ZWrVqZz4K/v7907949xTxvCxculKefftq81/TcB6GhoeY9mBJi27Bhg9SoUcPch65TqlQpGTduXIqIzdXGjRvN7/79fkOSMz79LMT2PaeVUhNMB2ojYRo0aGDNmDHD2rdvn7V7926rUaNGVqFChaxr164513njjTesggULWqtXr7Z27NhhVa9e3Xr88cedyyMiIqxy5cpZ9erVs3bt2mUtXbrUypEjh9WvXz/nOhs2bLD8/f2tCRMmWL///rv1yy+/WGXLlrX+85//pIj4pkyZYmXOnNmaN2+edezYMWvu3LlWpkyZrB9++MGjsam3337b+vjjj61XXnnFqlChQoztXL582cqdO7fVunVrsy2NLWPGjNann37q9dj0/vR+PvvsM7PNZs2a3TOm5I5t+PDhVv/+/a2NGzdaR48etcaPH2/eg4sXL/Z6bPoemz59utnGH3/8YX3//fdWrly53N6T3ozP4c6dO1bVqlWthg0bWlmyZEkRsR0/flwLclirVq2yTp8+7Zw0Vm/H5linZMmS5jXV7zq9rxUrVng9tqtXr7o9XzqVKVPGatu2rddj0+cpMDDQvP/1sxoeHm7Vrl3bqlSpktdj09+LgIAAa+rUqeZzu2TJEitv3rzWpEmT7hlbUsU3bdo0q2vXrtbPP/9stv3ll1+a737Xbetz98ADD1g9e/a0Dhw4YJalS5fOWrZsmddj08+qrjNr1iyrYsWKVrdu3eJ8zpIzNo1l1KhR1rZt26zDhw+b91769OmtnTt3ej02jWHOnDlmO/oc6jr6Gnv6Nz8+sTlcvHjReuihh6z69evH+RuS3PGtXbvW/D4cOnTI7bsuMjLSSiiSiiRw7tw584KsW7fOXL906ZL5oH3zzTfOdQ4ePGjW2bx5s/NLV3fWzpw541xHv4BDQkKs27dvm+tjxowxb0BXEydOtPLnz58i4gsNDbXeffddt23pl3SNGjU8GpurgQMHxvrh1ITnwQcfdMaq+vTpY3ZcvB2bK905iU9S4Y3YHPSLrH379ikyth49elg1a9aMd2zJEV/v3r2tl19+2fwY3C+pSK7YHEmFHiBILE/Fpjt1ugP622+/pbjYotMfdr2P9evXez02vb0+b64//HpAx8/PL85kMTlie+mll6znnnsuxm9XgQIFrKioqHjFlhTxObz11lvWk08+6fYZ1QN0rl544QWzE+ft2FzVqVMnXkmFN2Jz0CR78ODBKTI2PQCr38UpJbYXXnjBHLRLyG9ccsTnSCo06bGL7k9J4PLly+Z/tmzZzP/w8HDTJaNevXrOdbQprlChQrJ582ZzXf9r9xfXk+81aNBArly5Ivv37zfXtWnx5MmTsnTpUtM9QJu0FixYII0aNUoR8d2+fdt0Q3GlzWvbtm27b5cUO7HFh65bu3ZtyZAhg1v82nR48eJFr8aWFJIzNt2WYzspKTbtYqQnsKxTp06CbufJ+NasWSPffPON6RaYGJ5+7p599lnTjF6zZk354YcfUkRsixcvNl3ZlixZIkWLFpUiRYrIa6+9JhcuXPB6bNF98cUXpluKdqv0dmza/Ui7x8yYMUMiIyPNdr788ktzv+nTp/dqbPf6bfjrr7/kzz//jPf9JFV80b/DdF3X+3D8PiTkMXoqtqSQXLFFRUXJ1atXvfL7cL/Ydu3aJZs2bUrQ74MnY5sxY4b8/vvv5twRieXp5067ZGnXZ+3ipt20EoOkwib9UGmfR+3LV65cOTNP+6Hpzmz0/tS6g+7oo6b/o5/N23HdsY7ep/Zn17OB6/3lyZNHsmTJkqAdFk/Gp1/C+iOrb2xNenbs2GGu65v833//9Vhs8RGf+L0Vm13JGZv2i96+fbu0b98+xcT2+OOPmx2W4sWLm527IUOGxPu2noxPxzrpWBntm639jRPKk7FpX9uxY8eahOfHH380SYX2z41vYuHJ2PSHVnc0NTYdk6XPn36nPPfcc16PLXrfZf0+7tChQ7xv48nYNAFbsWKFvPfee2ZcgN6f7rTrZ9bbselvg/a/X716tdnO4cOHzfvPMWYgOePTHcv58+dLp06d7vv7oAfNbt686dXY7ErO2HQc1LVr1+T5559PMbEVKFDAfB6qVq1qxhroAQpvx3bkyBHp27evfPXVV2Y8RWJ4Mj5NJD755BP59ttvzaTjL5544gnZuXNnguNMU2fU9gR90+7bt88MEkpqBw4ckG7dusmAAQPMl7R+Getg6TfeeEOmTZvm9fjef/9988bVgYKaVOgbuW3btmZAtB5B82ZsdhGbyNq1a00y8fnnn0vZsmVTTGz6hahHx3QAmn4e9Ietd+/eXo+vY8eOZoCltpAlhidjy5Ejh/Ts2dN5XQf/nTp1ygye1dYLb8amP5Z6ZFsTCm0FUPr9pkfitWVRC0B4KzZX3333nXnf6XdcfHkyNv3u1fecxqMDMTU2/a3QZGzlypVmoKW3YtO4jh07Jk2aNDEHmTTJ1t8yHfgen9+GpIpPb6+FJ/TocP369RN9P8QW05w5c2Tw4MHy/fffm9bPlBLbL7/8YhIdLUKhO/I6CF8/H96KLTIy0vwu6HPl+H5LDE8+d/od6/o9qwfu9POrA9219TMhaKmwoUuXLqbJXne+NDt20BaFO3fuyKVLl9zW1+5LusyxTvSKGo7rjnV05L9mpbrj9Mgjj5jEYsqUKTJ9+vR4He3xdHzanK2x3Lhxw1Qc0WpR2nXBUYnEU7HFR3zi91ZsdiRXbOvWrZOmTZuaL5U2bdqkqNj0KEqZMmXMD8XIkSPNjop+cXs7Pu36pAmOHonSSY9oazOzXtbPiTdji021atVMF7L78XRsepRMnyPXH1ytUKL0O8WbsbnSVljdSY5+hNtbsWmLtbZc60GcSpUqmWRWj4Rq68DWrVu9GpsmNKNGjTI7d9oKpQnQY489ZpZpV7f7SYr49KBc3bp1zRFZrRIXn98HTX70d82bsdmRXLHNmzfPtABoq1j0bmTejk1b8LTrtia2PXr0ML8P3ozt6tWrpheHbsPx26Ct63pQTC/r74Y347sX/bzG5/chBtujMtIgHWjWuXNnK1++fKYCQnSOwTMLFixwztNBiLENhD579qxzHa1SoAOhb926Za63aNHCev75593ue9OmTeZ+/v77b6/HFxutPqKD9DwZW0IGarsOWNRKFXEN1E6u2BIzUDs5Y9NBW8HBwaa6S3x443lz0AopOlg1roGpyRWfDjjeu3evcxo2bJipjqaXL1y44NXYYvPaa6/FWSkouWJbvny5uY1WMIo+IFqrkXgzNtdqQToAOq4qaMkdmxbFeOyxx9zmnTp1ytyPVm/zZmyx0UpRWtwjLkkVn1bL0cpwvXr1inU7OlBbqxu60t+tuAZqJ1dsiRmonZyxaYWloKAga9GiRfeNK7lji04HkBcuXNirsUVGRrr9Luj05ptvmn0Rvexayckb8d2LVv68X6XR2JBUJIK+IbSqi5boci2/dePGDbcyX1r6a82aNabMl36Zun6hOkq2amkx/QHVUnY5c+Z0K4+p1WN0h0l3kLUUmJaY1VKV0X9IvBWf/uBreTJ9s2/dutVUNsiWLZupNOPJ2NSRI0dMJZvXX3/dKlGihLmsk6Pak37YtKSs/pDpB0rL3t6vvFxyxab2799v5jVt2tR64oknnOt4Oza9rT5P+jq7buf8+fNej+2rr76y5s+fb3be9fOgl/XLVssGxyU5X1dX8an+lFyxzZw50+wMaGUQnbR0sB400BK93o5Nf3QrV65sDkhoWUi9n2rVqllPP/2012Nz0Iot+l7T78X7Sa7YtISkJjq646TfwVpSVneKdSfKdVveiO2ff/4x1QL1vabztaSl7ojq74SnnzvdUdPfKq3643ofWjkneklZ3cnSGCdPnnzfkrLJFZtyPJ9VqlSxWrVqZS7rb4a3Y5s9e7bZJ9Hny3Ud/b31dmx6EEyrn+lnQacvvvjCHNT53//+5/XYEpuMJ1d848aNM0mifq51fU1k9fdBS5AnFElFImgWGNukOxEON2/eNGW79Gi5fnlpxqcvpCutta+17LVmsJ4D4p133rHu3r0bowyflmzTdbTOt+5A/fXXXykiPt250zraulxbMPSI+/3KQiZVbHoEJ7b7cU1o9uzZY8qNai13LcM7cuTIFBOb/vDHto63Y9OWk9iW6+28HZsmhrrzqedC0ZYU/Vx88MEH5r5Tyuua0KQiuWLTpKJ06dLm9vpZ1QMTrmUIvf28acurtszqa6sHA9q1axdnIpucsWnSo6VQ33vvvTifL2/Epuff0dYm/TzojsOzzz5rdpK9HZsmFVovX+PS+6hbt661ZcuWZHnudIcttvuIfsRaW2T19ytDhgymdLvrNrwdW3zW8UZs93rd4zpvS3LFpvtKWibY8R2nnws9IBvXuRaS8zVNTFKRXPHpuUeKFStmEn89MKwHOjVJSQy//wscAAAAABKFgdoAAAAAbCGpAAAAAGALSQUAAAAAW0gqAAAAANhCUgEAAADAFpIKAAAAALaQVAAAAACwhaQCAAAAgC0kFQAAAABsIakAACQLy7KkXr160qBBgxjLpkyZIlmzZpW//vrLK7EBAOwhqQAAJAs/Pz+ZMWOGbN26VT799FPn/OPHj0vv3r1l0qRJUqBAgSTd5t27d5P0/gAAsSOpAAAkm4IFC8qECRPk3XffNcmEtl506NBB6tevL5UqVZKGDRtKpkyZJHfu3PLKK6/Iv//+67ztsmXLpGbNmqZFI3v27NKkSRM5duyYc/kff/xhEpf58+dLnTp1JCgoSGbPnu2lRwoAaYufpd/oAAAko+bNm8vly5elRYsWMnToUNm/f7+ULVtWXnvtNWnTpo3cvHlT+vTpIxEREbJmzRpzm2+//dYkDY888ohcu3ZNBgwYYBKJ3bt3i7+/v7lctGhRKVKkiIwdO9YkKZpY5M2b19sPFwB8HkkFACDZnTt3ziQRFy5cMMnCvn375JdffpHly5c719HxFdqycejQISlRokSM+9BWjJw5c8revXulXLlyzqRi/Pjx0q1bt2R+RACQttH9CQCQ7HLlyiWvv/66lC5d2rRa7NmzR9auXWu6PjmmUqVKmXUdXZyOHDkiL730kjz00EMSEhJiWiTUiRMn3O67atWqXnhEAJC2BXg7AABA2hQQEGAmpd2ZmjZtKqNGjYqxnqP7ki4vXLiwfP7555IvXz6JiooyLRR37txxWz84ODiZHgEAwIGkAgDgdZUrVzbdoLT1wZFouDp//rzpBqUJRa1atcy8DRs2eCFSAEBs6P4EAPC6zp07m/EV2r1p+/btpsuTjq9o3769REZGyoMPPmgqPn322Wdy9OhRM3i7Z8+e3g4bAPB/SCoAAF6n3Zk2btxoEggtL1u+fHnp3r27KR+rlZ10mjdvnoSHh5suTz169JAxY8Z4O2wAwP+h+hMAAAAAW2ipAAAAAGALSQUAAAAAW0gqAAAAANhCUgEAAADAFpIKAAAAALaQVAAAAACwhaQCAAAAgC0kFQAAAABsIakAAAAAYAtJBQAAAABbSCoAAAAA2EJSAQAAAEDs+H8V6HCXExgUswAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_post_answer['CreationDate'] = pd.to_datetime(df_post_answer['CreationDate'])\n",
    "df_post_answer['Year'] = df_post_answer['CreationDate'].dt.year\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df_post_answer,x='Year')\n",
    "plt.title('Number of Posts by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A set of common intensifier words to remove\n",
    "intensifiers = {\n",
    "    \"very\", \"really\", \"extremely\", \"absolutely\", \"totally\", \"highly\", \"deeply\",\n",
    "    \"strongly\", \"incredibly\", \"exceptionally\", \"remarkably\", \"unbelievably\",\n",
    "    \"insanely\", \"awfully\", \"horribly\", \"hugely\", \"immensely\", \"overly\",\n",
    "    \"particularly\", \"significantly\", \"seriously\", \"tremendously\", \"wildly\",\n",
    "    \"super\", \"ultra\", \"crazy\", \"majorly\"\n",
    "}\n",
    "\n",
    "def clean_text(text, remove_code=True):\n",
    "    \"\"\"\n",
    "    Clean text:\n",
    "      1. Removes HTML tags and (optionally) code blocks.\n",
    "      2. Strips out URLs, punctuation, non-ASCII chars, and intensifiers.\n",
    "      3. Collapses whitespace to single spaces.\n",
    "    \n",
    "    Parameters:\n",
    "      text (str or any):  Input text to clean; non-string inputs are cast to str.\n",
    "      remove_code (bool): Whether to strip out <code> and <pre> blocks.\n",
    "    \n",
    "    Returns:\n",
    "      str: The cleaned text.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Handle empty or NaN inputs early\n",
    "    if text is None or text == \"\" or (isinstance(text, float) and np.isnan(text)):\n",
    "        return \"\"\n",
    "    \n",
    "    # 2. Cast non‑string inputs (numbers, etc.) to str\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # 3. Parse HTML so we can strip tags cleanly\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    \n",
    "    #    Optionally remove any <code> or <pre> blocks completely\n",
    "    if remove_code:\n",
    "        for code_block in soup.find_all(['code', 'pre']):\n",
    "            code_block.decompose()\n",
    "    \n",
    "    # 4. Extract only the visible text\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    \n",
    "    # 5. Collapse any run of whitespace into a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # 6. Remove URLs (e.g. http://…)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # 7. Strip out common punctuation characters\n",
    "    text = re.sub(r'[\\\"\\!?\\.;,:\\-\\(\\)]', '', text)\n",
    "    \n",
    "    # 8. Remove Twitter‑style mentions (e.g. @username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 9. Remove any non‑ASCII characters\n",
    "    text = ''.join(char for char in text if ord(char) < 128)\n",
    "    \n",
    "    # 10. Remove stray square brackets if any\n",
    "    text = re.sub(r'\\[|\\]', '', text)\n",
    "    \n",
    "    # 11. Split into words so we can remove intensifiers\n",
    "    words = text.split()\n",
    "    \n",
    "    # 12. Filter out any intensifier words\n",
    "    words = [w for w in words if w.lower() not in intensifiers]\n",
    "    \n",
    "    # 13. Rejoin into a single cleaned string\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code(html_content):\n",
    "    \"\"\"\n",
    "    Extracts all code snippets from an HTML string and returns them concatenated.\n",
    "    Code is pulled from both <code> and <pre> tags, and each snippet is separated\n",
    "    by a delimiter for clarity.\n",
    "\n",
    "    Parameters:\n",
    "      html_content (str or any):  \n",
    "          The HTML string to parse. Non-string inputs are cast to str.  \n",
    "          If None, empty, or NaN, returns an empty string.\n",
    "\n",
    "    Returns:\n",
    "      str:  \n",
    "        - A single string containing all extracted code blocks,  \n",
    "          separated by \"\\n---\\n\" if multiple blocks are found.  \n",
    "        - An empty string if no code blocks are present.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Handle None, empty, or NaN inputs early\n",
    "    if html_content is None or html_content == \"\" or (isinstance(html_content, float) and np.isnan(html_content)):\n",
    "        return \"\"\n",
    "\n",
    "    # 2. Ensure we have a string (this covers numeric or other types)\n",
    "    if not isinstance(html_content, str):\n",
    "        html_content = str(html_content)\n",
    "\n",
    "    # 3. Parse the HTML content so we can locate <code> and <pre> tags\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # 4. Find and extract each code block\n",
    "    code_blocks = []\n",
    "    for code_tag in soup.find_all(['code', 'pre']):\n",
    "        # Get the inner text of the tag, stripping leading/trailing whitespace\n",
    "        snippet = code_tag.get_text(strip=True)\n",
    "        if snippet:\n",
    "            code_blocks.append(snippet)\n",
    "        # Remove this tag from the soup to prevent duplicate extraction\n",
    "        code_tag.decompose()\n",
    "\n",
    "    # 5. Join multiple code snippets with a visible delimiter, or return empty if none\n",
    "    if code_blocks:\n",
    "        # \"\\n---\\n\" separates each snippet for readability\n",
    "        return \"\\n---\\n\".join(code_blocks)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from a piece of text.\n",
    "    \n",
    "    Parameters:\n",
    "      text (str or any):  \n",
    "        The input text to clean. Non-string inputs will be converted or return an empty string.\n",
    "    \n",
    "    Returns:\n",
    "      str:  \n",
    "        The text with all English stopwords filtered out, returned as a single space‑joined string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Ensure the input is a string; otherwise, we can't tokenize meaningfully.\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 2. Convert text to lowercase to make stopword matching case‑insensitive.\n",
    "    lower_text = text.lower()\n",
    "    \n",
    "    # 3. Tokenize the text into individual words (tokens).\n",
    "    tokens = word_tokenize(lower_text)\n",
    "    \n",
    "    # 4. Load the set of English stopwords from NLTK.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 5. Filter out any token that appears in the stopword list.\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # 6. Re‑join the filtered tokens into a single string separated by spaces.\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_texts_parallel(texts, batch_size=50, n_process=4) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Lemmatize a list (or Series) of texts using spaCy's parallel processing.\n",
    "\n",
    "    Parameters:\n",
    "      texts:  \n",
    "        The input documents to lemmatize. Non-strings will be cast to str.\n",
    "      batch_size (int):  \n",
    "        Number of texts to buffer and process in one batch.\n",
    "      n_process (int):  \n",
    "        Number of worker processes to use for parallelism (requires spaCy v3+).\n",
    "\n",
    "    Returns:\n",
    "      pd.Series:  \n",
    "        A pandas Series where each entry is the space-joined lemmas of the corresponding input text.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Ensure all inputs are strings (e.g., convert numbers or None to \"None\")\n",
    "    texts = [str(t) for t in texts]\n",
    "\n",
    "    # 2. Prepare a list to collect the lemmatized output\n",
    "    lemmatized_texts = []\n",
    "\n",
    "    # 3. Use spaCy's nlp.pipe for efficient, batched, parallel processing\n",
    "    #    - batch_size controls how many texts are processed at once\n",
    "    #    - n_process specifies the number of CPU cores to use\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size, n_process=n_process):\n",
    "        # 4. Extract the lemma_ for each token in the processed doc\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        # 5. Join lemmas into a single string and append to results\n",
    "        lemmatized_texts.append(\" \".join(lemmas))\n",
    "\n",
    "    # 6. Return results as a pandas Series for easy integration with DataFrames\n",
    "    return pd.Series(lemmatized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_generating(data, title):\n",
    "    \"\"\"\n",
    "    Generate and save a word cloud image from a series of text entries.\n",
    "\n",
    "    Parameters:\n",
    "      data (pd.Series or list of str):\n",
    "        An iterable of text strings. Each entry will be concatenated into\n",
    "        one large corpus for the word cloud.\n",
    "      title (str):\n",
    "        The base filename (without extension) for the output PNG image.\n",
    "        Choose a meaningful title so the saved file is self-documenting.\n",
    "\n",
    "    Returns:\n",
    "      None  # The function writes a file to disk but does not return a value.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define the stopwords set to remove very common words from the cloud\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # 2. Instantiate the WordCloud object with desired configuration\n",
    "    wc = WordCloud(\n",
    "        background_color='white',  # white background for better contrast\n",
    "        stopwords=stopwords,       # words to exclude\n",
    "        height=600,                # image height in pixels\n",
    "        width=400                  # image width in pixels\n",
    "    )\n",
    "\n",
    "    # 3. Combine all text entries into a single string\n",
    "    #    - fillna('') ensures missing values don’t introduce \"nan\"\n",
    "    #    - astype(str) converts any non‑string entries to strings\n",
    "    all_text = \" \".join(data.fillna(\"\").astype(str))\n",
    "\n",
    "    # 4. Generate the word cloud from the aggregated text\n",
    "    wc.generate(all_text)\n",
    "\n",
    "    # 5. Save the resulting word cloud to a PNG file\n",
    "    output_path = f\"{title}.png\"\n",
    "    wc.to_file(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define a function to removing some noise word\n",
    "This will return a new column in dataframe the text that remove defined noise word'''\n",
    "noise_words1 = {'word','example','well','output','code','text','string','sentence','model','work','result','see'}\n",
    "noise_words2 = {'word','example','well','output','code','text','string','sentence','model','work','result','see','one','give','need','list','find','know','look','follow','file','m','etc','two','try','way','use','want','different','two','seem'}\n",
    "def remove_noise_word(text,noise_words):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    # Remove noise words\n",
    "    filtered = [word for word in words if word.lower() not in noise_words]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and get Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['Question_Code'] = df_post_answer['Body'].apply(get_code)\n",
    "df_post_answer['Answer_Code'] = df_post_answer['AcceptedAnswerBody'].apply(get_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['Title_Clean'] = df_post_answer['Title'].apply(clean_text) \n",
    "df_post_answer['Body_Clean'] = df_post_answer['Body'].apply(clean_text)\n",
    "df_post_answer['AcceptedAnswerBody_Clean'] = df_post_answer['AcceptedAnswerBody'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title + Title Body & Accepted Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['combination_text'] = df_post_answer['Title_Clean'] + \" \" + df_post_answer['Body_Clean'] + \" \" +  df_post_answer['AcceptedAnswerBody_Clean']\n",
    "# Remove stop words\n",
    "df_post_answer['combination_text_clean'] = df_post_answer['combination_text'].apply(remove_stopwords)\n",
    "#Lemma Text\n",
    "text = df_post_answer['combination_text_clean'].tolist()\n",
    "lemma_text = lemma_texts_parallel(text)\n",
    "df_post_answer['combination_text_clean'] = lemma_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title + Title Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only combine Title and Its Body\n",
    "df_post_answer['combination_question'] = df_post_answer['Title_Clean'] + \" \" + df_post_answer['Body_Clean']\n",
    "# Remove Stopword\n",
    "df_post_answer['combination_question_clean'] = df_post_answer['combination_question'].apply(remove_stopwords)\n",
    "#Lemma Text\n",
    "text = df_post_answer['combination_question_clean'].tolist()\n",
    "lemma_text = lemma_texts_parallel(text)\n",
    "df_post_answer['combination_question_clean'] = lemma_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['Title_Clean'] = df_post_answer['Title_Clean'].apply(remove_stopwords)\n",
    "#Lemma Text\n",
    "text = df_post_answer['Title_Clean'].tolist()\n",
    "lemma_text = lemma_texts_parallel(text)\n",
    "df_post_answer['Title_Clean'] = lemma_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Removing Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_generating(df_post_answer['Title_Clean'],\"Title without removing Noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise Word 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['Title_Clean_No_Noise'] = df_post_answer['Title_Clean'].apply(lambda x: remove_noise_word(x, noise_words1))\n",
    "wc_generating(df_post_answer['Title_Clean_No_Noise'],\"Title without Noise 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise Word 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['Title_Clean_No_Noise'] = df_post_answer['Title_Clean'].apply(lambda x: remove_noise_word(x, noise_words2))\n",
    "wc_generating(df_post_answer['Title_Clean_No_Noise'],\"Title without Noise 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Full Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Clustering\n",
    "def cluster_embedding(dframe,column,model_embedding,cluster_col_name,cluster_type=\"kmean\"):\n",
    "    embeddings_title = model_embedding.encode(dframe[column].tolist())\n",
    "    normalizer = Normalizer(norm='l2')               # L2 (unit‑length) normalization\n",
    "    embeddings_norm = normalizer.fit_transform(embeddings_title)\n",
    "    dbscan = DBSCAN(eps=0.5,min_samples=5,metric='euclidean')\n",
    "    k = 10  # or whatever value you want to test\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    if cluster_type == \"kmean\":\n",
    "        cluster_kmean = kmeans.fit_predict(embeddings_norm)\n",
    "        dframe[cluster_col_name] = cluster_kmean\n",
    "    elif cluster_type == \"dbscan\":\n",
    "        clusters_dbscan = dbscan.fit_predict(embeddings_norm)\n",
    "        dframe[cluster_col_name] = clusters_dbscan\n",
    "    \n",
    "def cluster_embedding(dframe, column, model_embedding, cluster_col_name, cluster_type=\"kmean\"):\n",
    "    \"\"\"\n",
    "    Generate text embeddings for a DataFrame column, normalize them, and assign cluster labels.\n",
    "\n",
    "    Parameters:\n",
    "      dframe (pd.DataFrame):\n",
    "        The DataFrame containing the text data to cluster.\n",
    "      column (str):\n",
    "        Name of the column in `dframe` whose values will be embedded and clustered.\n",
    "      model_embedding:\n",
    "        A sentence-embedding model with an `.encode()` method.\n",
    "      cluster_col_name (str):\n",
    "        The name of the new column in `dframe` where cluster labels will be stored.\n",
    "      cluster_type (str, optional):\n",
    "        Type of clustering to apply.  \n",
    "        - \"kmean\": use KMeans clustering (default)  \n",
    "        - \"dbscan\": use DBSCAN clustering\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Compute embeddings for each entry in the specified column\n",
    "    texts = dframe[column].astype(str).tolist()\n",
    "    embeddings = model_embedding.encode(texts)\n",
    "\n",
    "    # 2. Normalize embeddings to unit length (L2 normalization)\n",
    "    normalizer = Normalizer(norm='l2')\n",
    "    embeddings_norm = normalizer.fit_transform(embeddings)\n",
    "\n",
    "    # 3. Prepare clustering algorithms\n",
    "    #    - DBSCAN for density‑based clusters\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n",
    "    #    - KMeans with a fixed number of clusters (tune `k` as needed)\n",
    "    k = 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "    # 4. Fit and assign cluster labels based on the chosen method\n",
    "    if cluster_type.lower() == \"kmean\":\n",
    "        labels = kmeans.fit_predict(embeddings_norm)\n",
    "    elif cluster_type.lower() == \"dbscan\":\n",
    "        labels = dbscan.fit_predict(embeddings_norm)\n",
    "\n",
    "    # 5. Store the cluster labels in the DataFrame\n",
    "    dframe[cluster_col_name] = labels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function \n",
    "def extract_keyword(text):\n",
    "    # Extract keywords using KeyBert; returns list of (keyword, score)\n",
    "    keyword_tuples = kw_model.extract_keywords(\n",
    "        text, \n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        use_maxsum=True, \n",
    "        nr_candidates=10, \n",
    "        top_n=5\n",
    "    )\n",
    "    # Extract only the keyword strings from the tuples\n",
    "    keywords = [kw for kw, score in keyword_tuples]\n",
    "    return \",\".join(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(data, min_support=0.01):\n",
    "    \"\"\"\n",
    "    Get frequent itemsets from a list of transactions using the Apriori algorithm.\n",
    "\n",
    "    Parameters:\n",
    "      data (iterable of lists):  \n",
    "        Each entry is a list of items for one transaction.\n",
    "      min_support (float):  \n",
    "        Minimum support threshold for an itemset to be considered frequent.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame:  \n",
    "        DataFrame of frequent itemsets with columns:\n",
    "          - 'support': proportion of transactions containing the itemset\n",
    "          - 'itemsets': the frozenset of items in that itemset\n",
    "    \"\"\"\n",
    "    # 1. Initialize the TransactionEncoder, which will one-hot encode our item lists\n",
    "    te = TransactionEncoder()\n",
    "    \n",
    "    # 2. Fit the encoder to our data and transform into a boolean array\n",
    "    #    shape = (n_transactions, n_unique_items)\n",
    "    item_te = te.fit(data).transform(data)\n",
    "    \n",
    "    # 3. Convert the boolean array to a DataFrame with item names as columns\n",
    "    df_items_1hot = pd.DataFrame(item_te, columns=te.columns_)\n",
    "    \n",
    "    # 4. Run the Apriori algorithm to find all itemsets with support >= min_support\n",
    "    frequent_itemsets_apriori = apriori(\n",
    "        df_items_1hot,\n",
    "        min_support=min_support,\n",
    "        use_colnames=True\n",
    "    )\n",
    "    \n",
    "    return frequent_itemsets_apriori\n",
    "\n",
    "\n",
    "def category_frequency(data, keyword_col, cluster_col, min_support):\n",
    "    \"\"\"\n",
    "    Compute frequent keyword patterns within each cluster category.\n",
    "\n",
    "    Parameters:\n",
    "      data (pd.DataFrame):  \n",
    "        DataFrame containing at least two columns:\n",
    "          - keyword_col: comma-separated keywords per row\n",
    "          - cluster_col: categorical labels to group by\n",
    "      keyword_col (str):  \n",
    "        Name of the column containing comma-separated keywords.\n",
    "      cluster_col (str):  \n",
    "        Name of the column containing cluster/category labels.\n",
    "      min_support (float):  \n",
    "        Minimum support threshold for frequent pattern mining.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame:  \n",
    "        Aggregated DataFrame of frequent patterns across all clusters with columns:\n",
    "          - 'score': support of the pattern (renamed from 'support')\n",
    "          - 'pattern': frozenset of keywords composing the itemset\n",
    "          - 'cluster': cluster label the pattern belongs to\n",
    "    \"\"\"\n",
    "    # 1. Select only the keyword and cluster columns, replace empty strings with NaN, then drop those rows\n",
    "    for_category = (\n",
    "        data[[keyword_col, cluster_col]]\n",
    "        .replace('', np.nan)\n",
    "        .dropna(axis=0)\n",
    "    )\n",
    "    \n",
    "    # 2. Split the comma-separated keyword strings into lists\n",
    "    for_category['keywords_list'] = for_category[keyword_col].str.split(',')\n",
    "    \n",
    "    # 3. Identify unique cluster labels and sort them\n",
    "    cluster_group = np.sort(for_category[cluster_col].unique())\n",
    "    \n",
    "    # 4. Collect per-cluster frequent pattern DataFrames in a list\n",
    "    pattern_dfs = []\n",
    "    \n",
    "    for cluster in cluster_group:\n",
    "        # 4a. Extract only the keyword lists for the given cluster\n",
    "        cluster_data = for_category.loc[\n",
    "            for_category[cluster_col] == cluster,\n",
    "            'keywords_list'\n",
    "        ]\n",
    "        \n",
    "        # 4b. Mine frequent patterns for this cluster\n",
    "        pattern = get_frequency(cluster_data, min_support)\n",
    "        \n",
    "        # 4c. Keep only patterns of length > 1\n",
    "        pattern = pattern.loc[pattern['itemsets'].apply(lambda x: len(x) > 1)].copy()\n",
    "        \n",
    "        # 4d. Annotate the pattern DataFrame with the cluster label\n",
    "        pattern['cluster'] = cluster\n",
    "        \n",
    "        # 4e. Add to our list\n",
    "        pattern_dfs.append(pattern)\n",
    "    \n",
    "    # 5. Concatenate all cluster-specific results into a single DataFrame\n",
    "    frequency_df = pd.concat(pattern_dfs, ignore_index=True)\n",
    "    \n",
    "    # 6. Rename columns for clarity/export\n",
    "    frequency_df.rename(\n",
    "        columns={\n",
    "            'support': 'score',    # pattern frequency -> 'score'\n",
    "            'itemsets': 'pattern'  # itemset -> 'pattern'\n",
    "        },\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return frequency_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Full Implementation for Title only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_embedding(df_post_answer,'Title_Clean_No_Noise',model_embedding,cluster_col_name=\"title_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df_post_answer,x=\"title_cluster\")\n",
    "plt.title('Clusters Distribution')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_answer['keywords_fromBert'] = df_post_answer['Title_Clean_No_Noise'].apply(extract_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Pattern Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_questions = category_frequency(df_post_answer,keyword_col=\"keywords_fromBert\",cluster_col=\"title_cluster\",min_support=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "cf510cee-3222-4943-8d49-c13d69637d82",
       "rows": [
        [
         "197",
         "0.1",
         "frozenset({'corenlp', 'stanford'})",
         "3"
        ],
        [
         "617",
         "0.09830508474576272",
         "frozenset({'gensim', 'word2vec'})",
         "8"
        ],
        [
         "259",
         "0.08703703703703704",
         "frozenset({'stanford', 'parser'})",
         "3"
        ],
        [
         "423",
         "0.06391752577319587",
         "frozenset({'entity', 'spacy'})",
         "5"
        ],
        [
         "563",
         "0.061016949152542375",
         "frozenset({'gensim', 'doc2vec'})",
         "8"
        ],
        [
         "446",
         "0.05979381443298969",
         "frozenset({'ner', 'spacy'})",
         "5"
        ],
        [
         "120",
         "0.05521472392638037",
         "frozenset({'nltk', 'python'})",
         "1"
        ],
        [
         "729",
         "0.03899926416482708",
         "frozenset({'natural', 'language'})",
         "9"
        ],
        [
         "112",
         "0.03558282208588957",
         "frozenset({'python', 'nlp'})",
         "1"
        ],
        [
         "738",
         "0.03016924208977189",
         "frozenset({'natural language', 'natural'})",
         "9"
        ],
        [
         "33",
         "0.028938906752411574",
         "frozenset({'tokenizer', 'keras'})",
         "0"
        ],
        [
         "347",
         "0.026595744680851064",
         "frozenset({'extract', 'python'})",
         "4"
        ],
        [
         "376",
         "0.02553191489361702",
         "frozenset({'python', 'remove'})",
         "4"
        ],
        [
         "53",
         "0.022508038585209004",
         "frozenset({'tokenize', 'python'})",
         "0"
        ],
        [
         "142",
         "0.017761989342806393",
         "frozenset({'object', 'attributeerror'})",
         "2"
        ],
        [
         "152",
         "0.012433392539964476",
         "frozenset({'gensim', 'error'})",
         "2"
        ],
        [
         "512",
         "0.011522633744855968",
         "frozenset({'transformer', 'huggingface'})",
         "6"
        ],
        [
         "523",
         "0.011210762331838564",
         "frozenset({'regular', 'expression'})",
         "7"
        ],
        [
         "511",
         "0.00905349794238683",
         "frozenset({'hug', 'face'})",
         "6"
        ],
        [
         "524",
         "0.008221225710014948",
         "frozenset({'tag', 'pos'})",
         "7"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>(corenlp, stanford)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>0.098305</td>\n",
       "      <td>(gensim, word2vec)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.087037</td>\n",
       "      <td>(stanford, parser)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.063918</td>\n",
       "      <td>(entity, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.061017</td>\n",
       "      <td>(gensim, doc2vec)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>0.059794</td>\n",
       "      <td>(ner, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.055215</td>\n",
       "      <td>(nltk, python)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>0.038999</td>\n",
       "      <td>(natural, language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.035583</td>\n",
       "      <td>(python, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>0.030169</td>\n",
       "      <td>(natural language, natural)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.028939</td>\n",
       "      <td>(tokenizer, keras)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.026596</td>\n",
       "      <td>(extract, python)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.025532</td>\n",
       "      <td>(python, remove)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.022508</td>\n",
       "      <td>(tokenize, python)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.017762</td>\n",
       "      <td>(object, attributeerror)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.012433</td>\n",
       "      <td>(gensim, error)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.011523</td>\n",
       "      <td>(transformer, huggingface)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>0.011211</td>\n",
       "      <td>(regular, expression)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0.009053</td>\n",
       "      <td>(hug, face)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>0.008221</td>\n",
       "      <td>(tag, pos)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                      pattern  cluster\n",
       "197  0.100000          (corenlp, stanford)        3\n",
       "617  0.098305           (gensim, word2vec)        8\n",
       "259  0.087037           (stanford, parser)        3\n",
       "423  0.063918              (entity, spacy)        5\n",
       "563  0.061017            (gensim, doc2vec)        8\n",
       "446  0.059794                 (ner, spacy)        5\n",
       "120  0.055215               (nltk, python)        1\n",
       "729  0.038999          (natural, language)        9\n",
       "112  0.035583                (python, nlp)        1\n",
       "738  0.030169  (natural language, natural)        9\n",
       "33   0.028939           (tokenizer, keras)        0\n",
       "347  0.026596            (extract, python)        4\n",
       "376  0.025532             (python, remove)        4\n",
       "53   0.022508           (tokenize, python)        0\n",
       "142  0.017762     (object, attributeerror)        2\n",
       "152  0.012433              (gensim, error)        2\n",
       "512  0.011523   (transformer, huggingface)        6\n",
       "523  0.011211        (regular, expression)        7\n",
       "511  0.009053                  (hug, face)        6\n",
       "524  0.008221                   (tag, pos)        7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions\n",
    "top2 = (\n",
    "    frequency_questions\n",
    "    .sort_values(\"score\", ascending=False)\n",
    "    .groupby(\"cluster\", as_index=False)\n",
    "    .head(2)\n",
    ")\n",
    "top2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 0: Tokenising Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "be0725ac-af2a-45ed-af8e-47763a283660",
       "rows": [
        [
         "33",
         "0.028938906752411574",
         "frozenset({'tokenizer', 'keras'})",
         "0"
        ],
        [
         "45",
         "0.022508038585209004",
         "frozenset({'tokenizer', 'nltk'})",
         "0"
        ],
        [
         "62",
         "0.022508038585209004",
         "frozenset({'token', 'spacy'})",
         "0"
        ],
        [
         "53",
         "0.022508038585209004",
         "frozenset({'tokenize', 'python'})",
         "0"
        ],
        [
         "65",
         "0.01929260450160772",
         "frozenset({'tokenizer', 'spacy'})",
         "0"
        ],
        [
         "5",
         "0.01607717041800643",
         "frozenset({'token', 'bert'})",
         "0"
        ],
        [
         "6",
         "0.01607717041800643",
         "frozenset({'tokenizer', 'bert'})",
         "0"
        ],
        [
         "42",
         "0.01607717041800643",
         "frozenset({'nltk', 'python'})",
         "0"
        ],
        [
         "64",
         "0.012861736334405145",
         "frozenset({'tokenize', 'spacy'})",
         "0"
        ],
        [
         "44",
         "0.012861736334405145",
         "frozenset({'nltk', 'tokenize'})",
         "0"
        ],
        [
         "52",
         "0.00964630225080386",
         "frozenset({'tokenization', 'python'})",
         "0"
        ],
        [
         "9",
         "0.00964630225080386",
         "frozenset({'character', 'tokenize'})",
         "0"
        ],
        [
         "24",
         "0.00964630225080386",
         "frozenset({'regular', 'expression'})",
         "0"
        ],
        [
         "68",
         "0.00964630225080386",
         "frozenset({'split', 'tokenizer'})",
         "0"
        ],
        [
         "28",
         "0.00964630225080386",
         "frozenset({'tokenizer', 'huggingface'})",
         "0"
        ],
        [
         "13",
         "0.00964630225080386",
         "frozenset({'token', 'count'})",
         "0"
        ],
        [
         "51",
         "0.006430868167202572",
         "frozenset({'punkt', 'tokenizer'})",
         "0"
        ],
        [
         "50",
         "0.006430868167202572",
         "frozenset({'tokenizer', 'prevent spacy'})",
         "0"
        ],
        [
         "82",
         "0.006430868167202572",
         "frozenset({'nltk tokenize', 'nltk', 'tokenize'})",
         "0"
        ],
        [
         "60",
         "0.006430868167202572",
         "frozenset({'tokenization', 'remove'})",
         "0"
        ],
        [
         "55",
         "0.006430868167202572",
         "frozenset({'token', 'quanteda'})",
         "0"
        ],
        [
         "56",
         "0.006430868167202572",
         "frozenset({'tokenize', 'quote'})",
         "0"
        ],
        [
         "57",
         "0.006430868167202572",
         "frozenset({'token', 'relevant'})",
         "0"
        ],
        [
         "81",
         "0.006430868167202572",
         "frozenset({'tokenizer', 'keras', 'keras tokenizer'})",
         "0"
        ],
        [
         "49",
         "0.006430868167202572",
         "frozenset({'tokenizing', 'phrase'})",
         "0"
        ],
        [
         "58",
         "0.006430868167202572",
         "frozenset({'remove punctuation', 'remove'})",
         "0"
        ],
        [
         "59",
         "0.006430868167202572",
         "frozenset({'remove stopwords', 'remove'})",
         "0"
        ],
        [
         "48",
         "0.006430868167202572",
         "frozenset({'tokenize', 'pattern'})",
         "0"
        ],
        [
         "54",
         "0.006430868167202572",
         "frozenset({'tokenizer', 'python'})",
         "0"
        ],
        [
         "78",
         "0.006430868167202572",
         "frozenset({'whitespace', 'tokenizer'})",
         "0"
        ],
        [
         "61",
         "0.006430868167202572",
         "frozenset({'spacy', 'specific substring'})",
         "0"
        ],
        [
         "63",
         "0.006430868167202572",
         "frozenset({'tokenization', 'spacy'})",
         "0"
        ],
        [
         "79",
         "0.006430868167202572",
         "frozenset({'tokenizer', 'keras', 'add'})",
         "0"
        ],
        [
         "47",
         "0.006430868167202572",
         "frozenset({'number', 'tokens'})",
         "0"
        ],
        [
         "66",
         "0.006430868167202572",
         "frozenset({'whitespace', 'spacy'})",
         "0"
        ],
        [
         "67",
         "0.006430868167202572",
         "frozenset({'split', 'token'})",
         "0"
        ],
        [
         "69",
         "0.006430868167202572",
         "frozenset({'stanford', 'tokenization'})",
         "0"
        ],
        [
         "70",
         "0.006430868167202572",
         "frozenset({'tokenizer', 'tensorflow'})",
         "0"
        ],
        [
         "71",
         "0.006430868167202572",
         "frozenset({'token', 'tfidfvectorizer'})",
         "0"
        ],
        [
         "72",
         "0.006430868167202572",
         "frozenset({'transformer', 'token'})",
         "0"
        ],
        [
         "73",
         "0.006430868167202572",
         "frozenset({'token', 'unique'})",
         "0"
        ],
        [
         "74",
         "0.006430868167202572",
         "frozenset({'token', 'vocabulary'})",
         "0"
        ],
        [
         "75",
         "0.006430868167202572",
         "frozenset({'tokenize entity', 'tokenize'})",
         "0"
        ],
        [
         "76",
         "0.006430868167202572",
         "frozenset({'transformer', 'tokenizer'})",
         "0"
        ],
        [
         "77",
         "0.006430868167202572",
         "frozenset({'tokenizer', 'vocabulary'})",
         "0"
        ],
        [
         "80",
         "0.006430868167202572",
         "frozenset({'tokenize entity', 'tokenize', 'entity'})",
         "0"
        ],
        [
         "0",
         "0.006430868167202572",
         "frozenset({'keras', 'add'})",
         "0"
        ],
        [
         "46",
         "0.006430868167202572",
         "frozenset({'tokenize', 'nltk tokenize'})",
         "0"
        ],
        [
         "22",
         "0.006430868167202572",
         "frozenset({'tokenize', 'entity'})",
         "0"
        ],
        [
         "20",
         "0.006430868167202572",
         "frozenset({'tokenize', 'double'})",
         "0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 84
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.028939</td>\n",
       "      <td>(tokenizer, keras)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.022508</td>\n",
       "      <td>(tokenizer, nltk)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.022508</td>\n",
       "      <td>(token, spacy)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.022508</td>\n",
       "      <td>(tokenize, python)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.019293</td>\n",
       "      <td>(tokenizer, spacy)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.006431</td>\n",
       "      <td>(tokenizer, issue)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.006431</td>\n",
       "      <td>(index, token)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.006431</td>\n",
       "      <td>(token, huggingface)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.006431</td>\n",
       "      <td>(huggingface tokenizer, huggingface)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.006431</td>\n",
       "      <td>(punkt, nltk, tokenizer)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score                               pattern  cluster\n",
       "33  0.028939                    (tokenizer, keras)        0\n",
       "45  0.022508                     (tokenizer, nltk)        0\n",
       "62  0.022508                        (token, spacy)        0\n",
       "53  0.022508                    (tokenize, python)        0\n",
       "65  0.019293                    (tokenizer, spacy)        0\n",
       "..       ...                                   ...      ...\n",
       "30  0.006431                    (tokenizer, issue)        0\n",
       "29  0.006431                        (index, token)        0\n",
       "27  0.006431                  (token, huggingface)        0\n",
       "26  0.006431  (huggingface tokenizer, huggingface)        0\n",
       "83  0.006431              (punkt, nltk, tokenizer)        0\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==0].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 1: Using NLTK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c1758d70-a819-49c4-8950-13c38e42ca3f",
       "rows": [
        [
         "120",
         "0.05521472392638037",
         "frozenset({'nltk', 'python'})",
         "1"
        ],
        [
         "112",
         "0.03558282208588957",
         "frozenset({'python', 'nlp'})",
         "1"
        ],
        [
         "92",
         "0.025766871165644172",
         "frozenset({'corpus', 'nltk'})",
         "1"
        ],
        [
         "130",
         "0.018404907975460124",
         "frozenset({'nltk', 'wordnet'})",
         "1"
        ],
        [
         "135",
         "0.01717791411042945",
         "frozenset({'python', 'python nltk'})",
         "1"
        ],
        [
         "104",
         "0.014723926380368098",
         "frozenset({'nltk', 'extract'})",
         "1"
        ],
        [
         "119",
         "0.014723926380368098",
         "frozenset({'nltk', 'pos'})",
         "1"
        ],
        [
         "87",
         "0.013496932515337423",
         "frozenset({'chunk', 'nltk'})",
         "1"
        ],
        [
         "109",
         "0.013496932515337423",
         "frozenset({'nltk', 'language'})",
         "1"
        ],
        [
         "121",
         "0.012269938650306749",
         "frozenset({'nltk', 'python nltk'})",
         "1"
        ],
        [
         "115",
         "0.012269938650306749",
         "frozenset({'task', 'nlp'})",
         "1"
        ],
        [
         "118",
         "0.012269938650306749",
         "frozenset({'nltk', 'parse'})",
         "1"
        ],
        [
         "129",
         "0.011042944785276074",
         "frozenset({'nltk', 'tree'})",
         "1"
        ],
        [
         "128",
         "0.011042944785276074",
         "frozenset({'nltk', 'tagger'})",
         "1"
        ],
        [
         "103",
         "0.011042944785276074",
         "frozenset({'extract', 'nlp'})",
         "1"
        ],
        [
         "102",
         "0.0098159509202454",
         "frozenset({'error', 'nltk'})",
         "1"
        ],
        [
         "138",
         "0.0098159509202454",
         "frozenset({'nltk', 'python', 'python nltk'})",
         "1"
        ],
        [
         "96",
         "0.0098159509202454",
         "frozenset({'datum', 'nlp'})",
         "1"
        ],
        [
         "131",
         "0.0098159509202454",
         "frozenset({'nltk python', 'python'})",
         "1"
        ],
        [
         "132",
         "0.0098159509202454",
         "frozenset({'tag', 'pos'})",
         "1"
        ],
        [
         "113",
         "0.008588957055214725",
         "frozenset({'stanford', 'nlp'})",
         "1"
        ],
        [
         "127",
         "0.008588957055214725",
         "frozenset({'nltk', 'tag'})",
         "1"
        ],
        [
         "85",
         "0.008588957055214725",
         "frozenset({'api', 'nlp'})",
         "1"
        ],
        [
         "84",
         "0.008588957055214725",
         "frozenset({'algorithm', 'nlp'})",
         "1"
        ],
        [
         "90",
         "0.008588957055214725",
         "frozenset({'classifier', 'nltk'})",
         "1"
        ],
        [
         "106",
         "0.008588957055214725",
         "frozenset({'grammar', 'nltk'})",
         "1"
        ],
        [
         "93",
         "0.008588957055214725",
         "frozenset({'nltk', 'count'})",
         "1"
        ],
        [
         "101",
         "0.008588957055214725",
         "frozenset({'recognition', 'entity'})",
         "1"
        ],
        [
         "126",
         "0.007361963190184049",
         "frozenset({'synset', 'nltk'})",
         "1"
        ],
        [
         "110",
         "0.007361963190184049",
         "frozenset({'library', 'nlp'})",
         "1"
        ],
        [
         "124",
         "0.007361963190184049",
         "frozenset({'speech', 'nltk'})",
         "1"
        ],
        [
         "100",
         "0.007361963190184049",
         "frozenset({'nltk', 'entity'})",
         "1"
        ],
        [
         "86",
         "0.007361963190184049",
         "frozenset({'bigram', 'nltk'})",
         "1"
        ],
        [
         "133",
         "0.007361963190184049",
         "frozenset({'tagger', 'pos'})",
         "1"
        ],
        [
         "88",
         "0.007361963190184049",
         "frozenset({'classification', 'nlp'})",
         "1"
        ],
        [
         "97",
         "0.007361963190184049",
         "frozenset({'nltk', 'difference'})",
         "1"
        ],
        [
         "91",
         "0.006134969325153374",
         "frozenset({'corpora', 'nltk'})",
         "1"
        ],
        [
         "108",
         "0.006134969325153374",
         "frozenset({'natural', 'language'})",
         "1"
        ],
        [
         "89",
         "0.006134969325153374",
         "frozenset({'nltk', 'classification'})",
         "1"
        ],
        [
         "94",
         "0.006134969325153374",
         "frozenset({'nltk', 'create'})",
         "1"
        ],
        [
         "136",
         "0.006134969325153374",
         "frozenset({'nltk', 'recognition', 'entity'})",
         "1"
        ],
        [
         "137",
         "0.006134969325153374",
         "frozenset({'nltk', 'nltk python', 'python'})",
         "1"
        ],
        [
         "134",
         "0.006134969325153374",
         "frozenset({'python nlp', 'python'})",
         "1"
        ],
        [
         "123",
         "0.006134969325153374",
         "frozenset({'nltk', 'remove'})",
         "1"
        ],
        [
         "125",
         "0.006134969325153374",
         "frozenset({'stanford', 'nltk'})",
         "1"
        ],
        [
         "107",
         "0.006134969325153374",
         "frozenset({'input', 'nlp'})",
         "1"
        ],
        [
         "122",
         "0.006134969325153374",
         "frozenset({'nltk', 'recognition'})",
         "1"
        ],
        [
         "95",
         "0.006134969325153374",
         "frozenset({'dataset', 'nlp'})",
         "1"
        ],
        [
         "98",
         "0.006134969325153374",
         "frozenset({'document', 'nlp'})",
         "1"
        ],
        [
         "99",
         "0.006134969325153374",
         "frozenset({'english', 'nlp'})",
         "1"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 55
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.055215</td>\n",
       "      <td>(nltk, python)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.035583</td>\n",
       "      <td>(python, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.025767</td>\n",
       "      <td>(corpus, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.018405</td>\n",
       "      <td>(nltk, wordnet)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.017178</td>\n",
       "      <td>(python, python nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.014724</td>\n",
       "      <td>(nltk, extract)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.014724</td>\n",
       "      <td>(nltk, pos)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.013497</td>\n",
       "      <td>(chunk, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.013497</td>\n",
       "      <td>(nltk, language)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.012270</td>\n",
       "      <td>(nltk, python nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.012270</td>\n",
       "      <td>(task, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.012270</td>\n",
       "      <td>(nltk, parse)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.011043</td>\n",
       "      <td>(nltk, tree)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.011043</td>\n",
       "      <td>(nltk, tagger)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.011043</td>\n",
       "      <td>(extract, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.009816</td>\n",
       "      <td>(error, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.009816</td>\n",
       "      <td>(nltk, python, python nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.009816</td>\n",
       "      <td>(datum, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.009816</td>\n",
       "      <td>(nltk python, python)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.009816</td>\n",
       "      <td>(tag, pos)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.008589</td>\n",
       "      <td>(stanford, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.008589</td>\n",
       "      <td>(nltk, tag)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.008589</td>\n",
       "      <td>(api, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.008589</td>\n",
       "      <td>(algorithm, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.008589</td>\n",
       "      <td>(classifier, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.008589</td>\n",
       "      <td>(grammar, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.008589</td>\n",
       "      <td>(nltk, count)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.008589</td>\n",
       "      <td>(recognition, entity)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.007362</td>\n",
       "      <td>(synset, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.007362</td>\n",
       "      <td>(library, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.007362</td>\n",
       "      <td>(speech, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.007362</td>\n",
       "      <td>(nltk, entity)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.007362</td>\n",
       "      <td>(bigram, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.007362</td>\n",
       "      <td>(tagger, pos)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.007362</td>\n",
       "      <td>(classification, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.007362</td>\n",
       "      <td>(nltk, difference)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(corpora, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(natural, language)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(nltk, classification)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(nltk, create)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(nltk, recognition, entity)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(nltk, nltk python, python)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(python nlp, python)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(nltk, remove)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(stanford, nltk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(input, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(nltk, recognition)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(dataset, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(document, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(english, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(nltk, nltk python)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(nlp python, python)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(tagging, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(google, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>(project, nlp)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                      pattern  cluster\n",
       "120  0.055215               (nltk, python)        1\n",
       "112  0.035583                (python, nlp)        1\n",
       "92   0.025767               (corpus, nltk)        1\n",
       "130  0.018405              (nltk, wordnet)        1\n",
       "135  0.017178        (python, python nltk)        1\n",
       "104  0.014724              (nltk, extract)        1\n",
       "119  0.014724                  (nltk, pos)        1\n",
       "87   0.013497                (chunk, nltk)        1\n",
       "109  0.013497             (nltk, language)        1\n",
       "121  0.012270          (nltk, python nltk)        1\n",
       "115  0.012270                  (task, nlp)        1\n",
       "118  0.012270                (nltk, parse)        1\n",
       "129  0.011043                 (nltk, tree)        1\n",
       "128  0.011043               (nltk, tagger)        1\n",
       "103  0.011043               (extract, nlp)        1\n",
       "102  0.009816                (error, nltk)        1\n",
       "138  0.009816  (nltk, python, python nltk)        1\n",
       "96   0.009816                 (datum, nlp)        1\n",
       "131  0.009816        (nltk python, python)        1\n",
       "132  0.009816                   (tag, pos)        1\n",
       "113  0.008589              (stanford, nlp)        1\n",
       "127  0.008589                  (nltk, tag)        1\n",
       "85   0.008589                   (api, nlp)        1\n",
       "84   0.008589             (algorithm, nlp)        1\n",
       "90   0.008589           (classifier, nltk)        1\n",
       "106  0.008589              (grammar, nltk)        1\n",
       "93   0.008589                (nltk, count)        1\n",
       "101  0.008589        (recognition, entity)        1\n",
       "126  0.007362               (synset, nltk)        1\n",
       "110  0.007362               (library, nlp)        1\n",
       "124  0.007362               (speech, nltk)        1\n",
       "100  0.007362               (nltk, entity)        1\n",
       "86   0.007362               (bigram, nltk)        1\n",
       "133  0.007362                (tagger, pos)        1\n",
       "88   0.007362        (classification, nlp)        1\n",
       "97   0.007362           (nltk, difference)        1\n",
       "91   0.006135              (corpora, nltk)        1\n",
       "108  0.006135          (natural, language)        1\n",
       "89   0.006135       (nltk, classification)        1\n",
       "94   0.006135               (nltk, create)        1\n",
       "136  0.006135  (nltk, recognition, entity)        1\n",
       "137  0.006135  (nltk, nltk python, python)        1\n",
       "134  0.006135         (python nlp, python)        1\n",
       "123  0.006135               (nltk, remove)        1\n",
       "125  0.006135             (stanford, nltk)        1\n",
       "107  0.006135                 (input, nlp)        1\n",
       "122  0.006135          (nltk, recognition)        1\n",
       "95   0.006135               (dataset, nlp)        1\n",
       "98   0.006135              (document, nlp)        1\n",
       "99   0.006135               (english, nlp)        1\n",
       "117  0.006135          (nltk, nltk python)        1\n",
       "116  0.006135         (nlp python, python)        1\n",
       "114  0.006135               (tagging, nlp)        1\n",
       "105  0.006135                (google, nlp)        1\n",
       "111  0.006135               (project, nlp)        1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==1].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 2: Handling Error in NLP Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ec4b3268-e1bb-4267-9d01-0364feaee3a2",
       "rows": [
        [
         "142",
         "0.017761989342806393",
         "frozenset({'object', 'attributeerror'})",
         "2"
        ],
        [
         "152",
         "0.012433392539964476",
         "frozenset({'gensim', 'error'})",
         "2"
        ],
        [
         "150",
         "0.010657193605683837",
         "frozenset({'nameerror', 'define'})",
         "2"
        ],
        [
         "154",
         "0.010657193605683837",
         "frozenset({'error', 'python'})",
         "2"
        ],
        [
         "139",
         "0.008880994671403197",
         "frozenset({'attribute', 'attributeerror'})",
         "2"
        ],
        [
         "170",
         "0.008880994671403197",
         "frozenset({'module', 'modulenotfounderror module'})",
         "2"
        ],
        [
         "173",
         "0.007104795737122558",
         "frozenset({'object', 'str'})",
         "2"
        ],
        [
         "172",
         "0.007104795737122558",
         "frozenset({'number sample', 'variable inconsistent'})",
         "2"
        ],
        [
         "171",
         "0.007104795737122558",
         "frozenset({'modulenotfounderror', 'modulenotfounderror module'})",
         "2"
        ],
        [
         "169",
         "0.007104795737122558",
         "frozenset({'module', 'modulenotfounderror'})",
         "2"
        ],
        [
         "166",
         "0.007104795737122558",
         "frozenset({'indexerror index', 'index range'})",
         "2"
        ],
        [
         "165",
         "0.007104795737122558",
         "frozenset({'importerror', 'importerror import'})",
         "2"
        ],
        [
         "140",
         "0.007104795737122558",
         "frozenset({'attribute', 'module'})",
         "2"
        ],
        [
         "180",
         "0.007104795737122558",
         "frozenset({'module', 'modulenotfounderror', 'modulenotfounderror module'})",
         "2"
        ],
        [
         "145",
         "0.007104795737122558",
         "frozenset({'colab', 'google'})",
         "2"
        ],
        [
         "153",
         "0.0053285968028419185",
         "frozenset({'error', 'kera'})",
         "2"
        ],
        [
         "151",
         "0.0053285968028419185",
         "frozenset({'error', 'fix'})",
         "2"
        ],
        [
         "179",
         "0.0053285968028419185",
         "frozenset({'attribute', 'module', 'attributeerror'})",
         "2"
        ],
        [
         "178",
         "0.0053285968028419185",
         "frozenset({'unhashable type', 'unhashable'})",
         "2"
        ],
        [
         "177",
         "0.0053285968028419185",
         "frozenset({'unable import', 'unable'})",
         "2"
        ],
        [
         "176",
         "0.0053285968028419185",
         "frozenset({'valueerror', 'tensorflow'})",
         "2"
        ],
        [
         "175",
         "0.0053285968028419185",
         "frozenset({'valueerror', 'shape'})",
         "2"
        ],
        [
         "174",
         "0.0053285968028419185",
         "frozenset({'pipeline', 'sklearn'})",
         "2"
        ],
        [
         "141",
         "0.0053285968028419185",
         "frozenset({'module', 'attributeerror'})",
         "2"
        ],
        [
         "143",
         "0.0053285968028419185",
         "frozenset({'object attribute', 'attributeerror'})",
         "2"
        ],
        [
         "144",
         "0.0053285968028419185",
         "frozenset({'error', 'bert'})",
         "2"
        ],
        [
         "146",
         "0.0053285968028419185",
         "frozenset({'error', 'countvectorizer'})",
         "2"
        ],
        [
         "168",
         "0.0053285968028419185",
         "frozenset({'low', 'object'})",
         "2"
        ],
        [
         "167",
         "0.0053285968028419185",
         "frozenset({'python', 'load'})",
         "2"
        ],
        [
         "147",
         "0.0053285968028419185",
         "frozenset({'notfittederror', 'countvectorizer'})",
         "2"
        ],
        [
         "148",
         "0.0053285968028419185",
         "frozenset({'vocabulary', 'countvectorizer'})",
         "2"
        ],
        [
         "164",
         "0.0053285968028419185",
         "frozenset({'import', 'importerror import'})",
         "2"
        ],
        [
         "163",
         "0.0053285968028419185",
         "frozenset({'gensim', 'vocabulary'})",
         "2"
        ],
        [
         "162",
         "0.0053285968028419185",
         "frozenset({'gensim', 'python'})",
         "2"
        ],
        [
         "161",
         "0.0053285968028419185",
         "frozenset({'typeerror', 'expect'})",
         "2"
        ],
        [
         "149",
         "0.0053285968028419185",
         "frozenset({'scikitlearn', 'datum'})",
         "2"
        ],
        [
         "159",
         "0.0053285968028419185",
         "frozenset({'error', 'throw'})",
         "2"
        ],
        [
         "158",
         "0.0053285968028419185",
         "frozenset({'error', 'tensorflow'})",
         "2"
        ],
        [
         "157",
         "0.0053285968028419185",
         "frozenset({'error', 'sklearn'})",
         "2"
        ],
        [
         "156",
         "0.0053285968028419185",
         "frozenset({'shape', 'error'})",
         "2"
        ],
        [
         "155",
         "0.0053285968028419185",
         "frozenset({'error', 'run'})",
         "2"
        ],
        [
         "160",
         "0.0053285968028419185",
         "frozenset({'training', 'error'})",
         "2"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 42
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.017762</td>\n",
       "      <td>(object, attributeerror)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.012433</td>\n",
       "      <td>(gensim, error)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.010657</td>\n",
       "      <td>(nameerror, define)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.010657</td>\n",
       "      <td>(error, python)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.008881</td>\n",
       "      <td>(attribute, attributeerror)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.008881</td>\n",
       "      <td>(module, modulenotfounderror module)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.007105</td>\n",
       "      <td>(object, str)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.007105</td>\n",
       "      <td>(number sample, variable inconsistent)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.007105</td>\n",
       "      <td>(modulenotfounderror, modulenotfounderror module)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.007105</td>\n",
       "      <td>(module, modulenotfounderror)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.007105</td>\n",
       "      <td>(indexerror index, index range)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.007105</td>\n",
       "      <td>(importerror, importerror import)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.007105</td>\n",
       "      <td>(attribute, module)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.007105</td>\n",
       "      <td>(module, modulenotfounderror, modulenotfounder...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.007105</td>\n",
       "      <td>(colab, google)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(error, kera)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(error, fix)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(attribute, module, attributeerror)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(unhashable type, unhashable)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(unable import, unable)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(valueerror, tensorflow)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(valueerror, shape)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(pipeline, sklearn)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(module, attributeerror)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(object attribute, attributeerror)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(error, bert)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(error, countvectorizer)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(low, object)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(python, load)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(notfittederror, countvectorizer)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(vocabulary, countvectorizer)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(import, importerror import)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(gensim, vocabulary)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(gensim, python)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(typeerror, expect)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(scikitlearn, datum)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(error, throw)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(error, tensorflow)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(error, sklearn)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(shape, error)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(error, run)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.005329</td>\n",
       "      <td>(training, error)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                                            pattern  cluster\n",
       "142  0.017762                           (object, attributeerror)        2\n",
       "152  0.012433                                    (gensim, error)        2\n",
       "150  0.010657                                (nameerror, define)        2\n",
       "154  0.010657                                    (error, python)        2\n",
       "139  0.008881                        (attribute, attributeerror)        2\n",
       "170  0.008881               (module, modulenotfounderror module)        2\n",
       "173  0.007105                                      (object, str)        2\n",
       "172  0.007105             (number sample, variable inconsistent)        2\n",
       "171  0.007105  (modulenotfounderror, modulenotfounderror module)        2\n",
       "169  0.007105                      (module, modulenotfounderror)        2\n",
       "166  0.007105                    (indexerror index, index range)        2\n",
       "165  0.007105                  (importerror, importerror import)        2\n",
       "140  0.007105                                (attribute, module)        2\n",
       "180  0.007105  (module, modulenotfounderror, modulenotfounder...        2\n",
       "145  0.007105                                    (colab, google)        2\n",
       "153  0.005329                                      (error, kera)        2\n",
       "151  0.005329                                       (error, fix)        2\n",
       "179  0.005329                (attribute, module, attributeerror)        2\n",
       "178  0.005329                      (unhashable type, unhashable)        2\n",
       "177  0.005329                            (unable import, unable)        2\n",
       "176  0.005329                           (valueerror, tensorflow)        2\n",
       "175  0.005329                                (valueerror, shape)        2\n",
       "174  0.005329                                (pipeline, sklearn)        2\n",
       "141  0.005329                           (module, attributeerror)        2\n",
       "143  0.005329                 (object attribute, attributeerror)        2\n",
       "144  0.005329                                      (error, bert)        2\n",
       "146  0.005329                           (error, countvectorizer)        2\n",
       "168  0.005329                                      (low, object)        2\n",
       "167  0.005329                                     (python, load)        2\n",
       "147  0.005329                  (notfittederror, countvectorizer)        2\n",
       "148  0.005329                      (vocabulary, countvectorizer)        2\n",
       "164  0.005329                       (import, importerror import)        2\n",
       "163  0.005329                               (gensim, vocabulary)        2\n",
       "162  0.005329                                   (gensim, python)        2\n",
       "161  0.005329                                (typeerror, expect)        2\n",
       "149  0.005329                               (scikitlearn, datum)        2\n",
       "159  0.005329                                     (error, throw)        2\n",
       "158  0.005329                                (error, tensorflow)        2\n",
       "157  0.005329                                   (error, sklearn)        2\n",
       "156  0.005329                                     (shape, error)        2\n",
       "155  0.005329                                       (error, run)        2\n",
       "160  0.005329                                  (training, error)        2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==2].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 3: Using Stanford Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0eb594bc-10d0-4b5c-a5be-c0fa2542a05e",
       "rows": [
        [
         "197",
         "0.1",
         "frozenset({'corenlp', 'stanford'})",
         "3"
        ],
        [
         "259",
         "0.08703703703703704",
         "frozenset({'stanford', 'parser'})",
         "3"
        ],
        [
         "242",
         "0.07037037037037037",
         "frozenset({'stanford', 'nlp'})",
         "3"
        ],
        [
         "268",
         "0.03333333333333333",
         "frozenset({'tag', 'pos'})",
         "3"
        ],
        [
         "210",
         "0.02962962962962963",
         "frozenset({'dependency', 'stanford'})",
         "3"
        ],
        [
         "206",
         "0.027777777777777776",
         "frozenset({'dependency', 'parser'})",
         "3"
        ],
        [
         "267",
         "0.025925925925925925",
         "frozenset({'stanford', 'pos'})",
         "3"
        ],
        [
         "255",
         "0.025925925925925925",
         "frozenset({'tree', 'parse'})",
         "3"
        ],
        [
         "283",
         "0.022222222222222223",
         "frozenset({'stanford', 'tagger'})",
         "3"
        ],
        [
         "187",
         "0.018518518518518517",
         "frozenset({'stanford', 'core'})",
         "3"
        ],
        [
         "254",
         "0.018518518518518517",
         "frozenset({'stanford', 'parse'})",
         "3"
        ],
        [
         "269",
         "0.016666666666666666",
         "frozenset({'tagger', 'pos'})",
         "3"
        ],
        [
         "239",
         "0.016666666666666666",
         "frozenset({'parse', 'nlp'})",
         "3"
        ],
        [
         "295",
         "0.016666666666666666",
         "frozenset({'dependency', 'stanford', 'parser'})",
         "3"
        ],
        [
         "205",
         "0.016666666666666666",
         "frozenset({'dependency', 'parse'})",
         "3"
        ],
        [
         "287",
         "0.014814814814814815",
         "frozenset({'stanford', 'core', 'nlp'})",
         "3"
        ],
        [
         "238",
         "0.014814814814814815",
         "frozenset({'stanford', 'ner'})",
         "3"
        ],
        [
         "282",
         "0.014814814814814815",
         "frozenset({'stanford', 'tag'})",
         "3"
        ],
        [
         "186",
         "0.014814814814814815",
         "frozenset({'core', 'nlp'})",
         "3"
        ],
        [
         "270",
         "0.012962962962962963",
         "frozenset({'tagging', 'pos'})",
         "3"
        ],
        [
         "195",
         "0.012962962962962963",
         "frozenset({'corenlp', 'parser'})",
         "3"
        ],
        [
         "277",
         "0.012962962962962963",
         "frozenset({'split', 'stanford'})",
         "3"
        ],
        [
         "218",
         "0.012962962962962963",
         "frozenset({'stanford', 'entity'})",
         "3"
        ],
        [
         "231",
         "0.011111111111111112",
         "frozenset({'java', 'nlp'})",
         "3"
        ],
        [
         "274",
         "0.011111111111111112",
         "frozenset({'stanford', 'python'})",
         "3"
        ],
        [
         "232",
         "0.011111111111111112",
         "frozenset({'stanford', 'java'})",
         "3"
        ],
        [
         "311",
         "0.011111111111111112",
         "frozenset({'stanford', 'tagger', 'pos'})",
         "3"
        ],
        [
         "291",
         "0.011111111111111112",
         "frozenset({'corenlp', 'stanford', 'parser'})",
         "3"
        ],
        [
         "193",
         "0.011111111111111112",
         "frozenset({'dependency', 'corenlp'})",
         "3"
        ],
        [
         "183",
         "0.011111111111111112",
         "frozenset({'stanford', 'chinese'})",
         "3"
        ],
        [
         "289",
         "0.009259259259259259",
         "frozenset({'dependency', 'corenlp', 'stanford'})",
         "3"
        ],
        [
         "286",
         "0.009259259259259259",
         "frozenset({'stanford', 'tree'})",
         "3"
        ],
        [
         "224",
         "0.009259259259259259",
         "frozenset({'stanford', 'extract'})",
         "3"
        ],
        [
         "271",
         "0.009259259259259259",
         "frozenset({'pos tag', 'tag'})",
         "3"
        ],
        [
         "240",
         "0.009259259259259259",
         "frozenset({'pos', 'nlp'})",
         "3"
        ],
        [
         "243",
         "0.009259259259259259",
         "frozenset({'stanford core', 'nlp'})",
         "3"
        ],
        [
         "234",
         "0.009259259259259259",
         "frozenset({'natural', 'language'})",
         "3"
        ],
        [
         "189",
         "0.009259259259259259",
         "frozenset({'resolution', 'coreference'})",
         "3"
        ],
        [
         "308",
         "0.009259259259259259",
         "frozenset({'pos tag', 'tag', 'pos'})",
         "3"
        ],
        [
         "265",
         "0.009259259259259259",
         "frozenset({'pos tag', 'pos'})",
         "3"
        ],
        [
         "298",
         "0.009259259259259259",
         "frozenset({'stanford', 'java', 'nlp'})",
         "3"
        ],
        [
         "273",
         "0.009259259259259259",
         "frozenset({'punctuation', 'stanford'})",
         "3"
        ],
        [
         "213",
         "0.009259259259259259",
         "frozenset({'dependency', 'tree'})",
         "3"
        ],
        [
         "279",
         "0.007407407407407408",
         "frozenset({'stanford', 'stanford dependency'})",
         "3"
        ],
        [
         "272",
         "0.007407407407407408",
         "frozenset({'tagging', 'pos tagging'})",
         "3"
        ],
        [
         "281",
         "0.007407407407407408",
         "frozenset({'stanford', 'stanford parser'})",
         "3"
        ],
        [
         "280",
         "0.007407407407407408",
         "frozenset({'stanford', 'stanford nlp'})",
         "3"
        ],
        [
         "275",
         "0.007407407407407408",
         "frozenset({'resolution', 'stanford'})",
         "3"
        ],
        [
         "244",
         "0.007407407407407408",
         "frozenset({'stanford nlp', 'nlp'})",
         "3"
        ],
        [
         "284",
         "0.007407407407407408",
         "frozenset({'stanford', 'train'})",
         "3"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 132
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>(corenlp, stanford)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.087037</td>\n",
       "      <td>(stanford, parser)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.070370</td>\n",
       "      <td>(stanford, nlp)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.033333</td>\n",
       "      <td>(tag, pos)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.029630</td>\n",
       "      <td>(dependency, stanford)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.005556</td>\n",
       "      <td>(format, tree)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.005556</td>\n",
       "      <td>(stanford, format)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.005556</td>\n",
       "      <td>(relation, extraction)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.005556</td>\n",
       "      <td>(nlp, extraction)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.005556</td>\n",
       "      <td>(dependency, stanford, stanford dependency, pa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                                            pattern  cluster\n",
       "197  0.100000                                (corenlp, stanford)        3\n",
       "259  0.087037                                 (stanford, parser)        3\n",
       "242  0.070370                                    (stanford, nlp)        3\n",
       "268  0.033333                                         (tag, pos)        3\n",
       "210  0.029630                             (dependency, stanford)        3\n",
       "..        ...                                                ...      ...\n",
       "228  0.005556                                     (format, tree)        3\n",
       "227  0.005556                                 (stanford, format)        3\n",
       "226  0.005556                             (relation, extraction)        3\n",
       "225  0.005556                                  (nlp, extraction)        3\n",
       "312  0.005556  (dependency, stanford, stanford dependency, pa...        3\n",
       "\n",
       "[132 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==3].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 4: Preprocessing Text in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2544901d-b97f-4bde-a237-6605a228c28a",
       "rows": [
        [
         "347",
         "0.026595744680851064",
         "frozenset({'extract', 'python'})",
         "4"
        ],
        [
         "376",
         "0.02553191489361702",
         "frozenset({'python', 'remove'})",
         "4"
        ],
        [
         "365",
         "0.02446808510638298",
         "frozenset({'panda dataframe', 'panda'})",
         "4"
        ],
        [
         "339",
         "0.019148936170212766",
         "frozenset({'datum', 'python'})",
         "4"
        ],
        [
         "342",
         "0.013829787234042552",
         "frozenset({'dictionary', 'python'})",
         "4"
        ],
        [
         "362",
         "0.013829787234042552",
         "frozenset({'match', 'python'})",
         "4"
        ],
        [
         "377",
         "0.01276595744680851",
         "frozenset({'python', 'replace'})",
         "4"
        ],
        [
         "386",
         "0.01276595744680851",
         "frozenset({'value', 'python'})",
         "4"
        ],
        [
         "318",
         "0.01276595744680851",
         "frozenset({'character', 'python'})",
         "4"
        ],
        [
         "328",
         "0.01276595744680851",
         "frozenset({'count', 'python'})",
         "4"
        ],
        [
         "327",
         "0.010638297872340425",
         "frozenset({'corpus', 'python'})",
         "4"
        ],
        [
         "338",
         "0.010638297872340425",
         "frozenset({'datum', 'panda'})",
         "4"
        ],
        [
         "332",
         "0.010638297872340425",
         "frozenset({'panda dataframe', 'dataframe'})",
         "4"
        ],
        [
         "323",
         "0.010638297872340425",
         "frozenset({'column', 'panda'})",
         "4"
        ],
        [
         "326",
         "0.009574468085106383",
         "frozenset({'convert', 'python'})",
         "4"
        ],
        [
         "348",
         "0.009574468085106383",
         "frozenset({'fast', 'python'})",
         "4"
        ],
        [
         "372",
         "0.009574468085106383",
         "frozenset({'python', 'python regex'})",
         "4"
        ],
        [
         "316",
         "0.009574468085106383",
         "frozenset({'base', 'python'})",
         "4"
        ],
        [
         "334",
         "0.00851063829787234",
         "frozenset({'row', 'dataframe'})",
         "4"
        ],
        [
         "378",
         "0.00851063829787234",
         "frozenset({'python', 'search'})",
         "4"
        ],
        [
         "319",
         "0.00851063829787234",
         "frozenset({'check', 'python'})",
         "4"
        ],
        [
         "321",
         "0.00851063829787234",
         "frozenset({'column', 'dataframe'})",
         "4"
        ],
        [
         "383",
         "0.00851063829787234",
         "frozenset({'specific', 'python'})",
         "4"
        ],
        [
         "358",
         "0.007446808510638298",
         "frozenset({'keyword', 'python'})",
         "4"
        ],
        [
         "375",
         "0.007446808510638298",
         "frozenset({'python', 'regex python'})",
         "4"
        ],
        [
         "371",
         "0.007446808510638298",
         "frozenset({'phrase', 'python'})",
         "4"
        ],
        [
         "381",
         "0.007446808510638298",
         "frozenset({'python', 'similar'})",
         "4"
        ],
        [
         "355",
         "0.007446808510638298",
         "frozenset({'index', 'python'})",
         "4"
        ],
        [
         "343",
         "0.007446808510638298",
         "frozenset({'dictionary', 'value'})",
         "4"
        ],
        [
         "354",
         "0.007446808510638298",
         "frozenset({'identify', 'python'})",
         "4"
        ],
        [
         "373",
         "0.007446808510638298",
         "frozenset({'python', 'read'})",
         "4"
        ],
        [
         "324",
         "0.007446808510638298",
         "frozenset({'panda column', 'column'})",
         "4"
        ],
        [
         "352",
         "0.007446808510638298",
         "frozenset({'function', 'python'})",
         "4"
        ],
        [
         "340",
         "0.007446808510638298",
         "frozenset({'python', 'detect'})",
         "4"
        ],
        [
         "359",
         "0.006382978723404255",
         "frozenset({'python', 'line'})",
         "4"
        ],
        [
         "356",
         "0.006382978723404255",
         "frozenset({'information', 'python'})",
         "4"
        ],
        [
         "366",
         "0.006382978723404255",
         "frozenset({'python', 'panda'})",
         "4"
        ],
        [
         "353",
         "0.006382978723404255",
         "frozenset({'good', 'python'})",
         "4"
        ],
        [
         "313",
         "0.006382978723404255",
         "frozenset({'analysis', 'python'})",
         "4"
        ],
        [
         "349",
         "0.006382978723404255",
         "frozenset({'form', 'python'})",
         "4"
        ],
        [
         "346",
         "0.006382978723404255",
         "frozenset({'english', 'python'})",
         "4"
        ],
        [
         "345",
         "0.006382978723404255",
         "frozenset({'element', 'python'})",
         "4"
        ],
        [
         "369",
         "0.006382978723404255",
         "frozenset({'python', 'parse'})",
         "4"
        ],
        [
         "336",
         "0.006382978723404255",
         "frozenset({'frame', 'datum'})",
         "4"
        ],
        [
         "335",
         "0.006382978723404255",
         "frozenset({'split', 'dataframe'})",
         "4"
        ],
        [
         "331",
         "0.006382978723404255",
         "frozenset({'panda', 'dataframe'})",
         "4"
        ],
        [
         "330",
         "0.006382978723404255",
         "frozenset({'extract', 'dataframe'})",
         "4"
        ],
        [
         "329",
         "0.006382978723404255",
         "frozenset({'create', 'python'})",
         "4"
        ],
        [
         "382",
         "0.006382978723404255",
         "frozenset({'similarity', 'python'})",
         "4"
        ],
        [
         "384",
         "0.006382978723404255",
         "frozenset({'split', 'python'})",
         "4"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 75
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.026596</td>\n",
       "      <td>(extract, python)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.025532</td>\n",
       "      <td>(python, remove)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.024468</td>\n",
       "      <td>(panda dataframe, panda)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.019149</td>\n",
       "      <td>(datum, python)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.013830</td>\n",
       "      <td>(dictionary, python)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.005319</td>\n",
       "      <td>(python, long)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.005319</td>\n",
       "      <td>(loop, python)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.005319</td>\n",
       "      <td>(python, multiple)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.005319</td>\n",
       "      <td>(number, python)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.005319</td>\n",
       "      <td>(panda dataframe, panda, dataframe)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                              pattern  cluster\n",
       "347  0.026596                    (extract, python)        4\n",
       "376  0.025532                     (python, remove)        4\n",
       "365  0.024468             (panda dataframe, panda)        4\n",
       "339  0.019149                      (datum, python)        4\n",
       "342  0.013830                 (dictionary, python)        4\n",
       "..        ...                                  ...      ...\n",
       "360  0.005319                       (python, long)        4\n",
       "361  0.005319                       (loop, python)        4\n",
       "363  0.005319                   (python, multiple)        4\n",
       "364  0.005319                     (number, python)        4\n",
       "387  0.005319  (panda dataframe, panda, dataframe)        4\n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==4].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 5: Using Spacy Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "29f738f1-4493-44b4-b82f-a968820df85b",
       "rows": [
        [
         "423",
         "0.06391752577319587",
         "frozenset({'entity', 'spacy'})",
         "5"
        ],
        [
         "446",
         "0.05979381443298969",
         "frozenset({'ner', 'spacy'})",
         "5"
        ],
        [
         "444",
         "0.03917525773195876",
         "frozenset({'matcher', 'spacy'})",
         "5"
        ],
        [
         "400",
         "0.03505154639175258",
         "frozenset({'custom', 'spacy'})",
         "5"
        ],
        [
         "485",
         "0.03505154639175258",
         "frozenset({'training', 'spacy'})",
         "5"
        ],
        [
         "449",
         "0.03505154639175258",
         "frozenset({'spacy', 'nlp'})",
         "5"
        ],
        [
         "484",
         "0.0288659793814433",
         "frozenset({'spacy', 'train'})",
         "5"
        ],
        [
         "481",
         "0.0288659793814433",
         "frozenset({'tag', 'spacy'})",
         "5"
        ],
        [
         "465",
         "0.026804123711340205",
         "frozenset({'spacy', 'python'})",
         "5"
        ],
        [
         "462",
         "0.026804123711340205",
         "frozenset({'spacy', 'pos'})",
         "5"
        ],
        [
         "427",
         "0.024742268041237112",
         "frozenset({'extract', 'spacy'})",
         "5"
        ],
        [
         "442",
         "0.02268041237113402",
         "frozenset({'match', 'spacy'})",
         "5"
        ],
        [
         "459",
         "0.02268041237113402",
         "frozenset({'phrase', 'spacy'})",
         "5"
        ],
        [
         "409",
         "0.02268041237113402",
         "frozenset({'dependency', 'spacy'})",
         "5"
        ],
        [
         "469",
         "0.020618556701030927",
         "frozenset({'spacy', 'remove'})",
         "5"
        ],
        [
         "452",
         "0.018556701030927835",
         "frozenset({'noun', 'spacy'})",
         "5"
        ],
        [
         "490",
         "0.018556701030927835",
         "frozenset({'spacy', 'vector'})",
         "5"
        ],
        [
         "411",
         "0.018556701030927835",
         "frozenset({'doc', 'spacy'})",
         "5"
        ],
        [
         "422",
         "0.018556701030927835",
         "frozenset({'entity', 'ner'})",
         "5"
        ],
        [
         "426",
         "0.018556701030927835",
         "frozenset({'error', 'spacy'})",
         "5"
        ],
        [
         "458",
         "0.018556701030927835",
         "frozenset({'spacy', 'pattern'})",
         "5"
        ],
        [
         "438",
         "0.018556701030927835",
         "frozenset({'lemmatization', 'spacy'})",
         "5"
        ],
        [
         "414",
         "0.018556701030927835",
         "frozenset({'spacy', 'download'})",
         "5"
        ],
        [
         "406",
         "0.016494845360824743",
         "frozenset({'datum', 'spacy'})",
         "5"
        ],
        [
         "435",
         "0.016494845360824743",
         "frozenset({'label', 'spacy'})",
         "5"
        ],
        [
         "448",
         "0.01443298969072165",
         "frozenset({'training', 'ner'})",
         "5"
        ],
        [
         "464",
         "0.01443298969072165",
         "frozenset({'possible', 'spacy'})",
         "5"
        ],
        [
         "480",
         "0.01443298969072165",
         "frozenset({'stop', 'spacy'})",
         "5"
        ],
        [
         "412",
         "0.01443298969072165",
         "frozenset({'spacy', 'document'})",
         "5"
        ],
        [
         "505",
         "0.01443298969072165",
         "frozenset({'training', 'ner', 'spacy'})",
         "5"
        ],
        [
         "501",
         "0.01443298969072165",
         "frozenset({'ner', 'entity', 'spacy'})",
         "5"
        ],
        [
         "463",
         "0.012371134020618556",
         "frozenset({'tag', 'pos'})",
         "5"
        ],
        [
         "432",
         "0.012371134020618556",
         "frozenset({'index', 'spacy'})",
         "5"
        ],
        [
         "506",
         "0.012371134020618556",
         "frozenset({'tag', 'spacy', 'pos'})",
         "5"
        ],
        [
         "407",
         "0.012371134020618556",
         "frozenset({'training', 'datum'})",
         "5"
        ],
        [
         "393",
         "0.012371134020618556",
         "frozenset({'chunk', 'spacy'})",
         "5"
        ],
        [
         "497",
         "0.012371134020618556",
         "frozenset({'training', 'datum', 'spacy'})",
         "5"
        ],
        [
         "418",
         "0.010309278350515464",
         "frozenset({'encorewebsm', 'spacy'})",
         "5"
        ],
        [
         "486",
         "0.010309278350515464",
         "frozenset({'type', 'spacy'})",
         "5"
        ],
        [
         "483",
         "0.010309278350515464",
         "frozenset({'token', 'spacy'})",
         "5"
        ],
        [
         "468",
         "0.010309278350515464",
         "frozenset({'spacy', 'regex'})",
         "5"
        ],
        [
         "460",
         "0.010309278350515464",
         "frozenset({'phrasematcher', 'spacy'})",
         "5"
        ],
        [
         "433",
         "0.010309278350515464",
         "frozenset({'spacy', 'install'})",
         "5"
        ],
        [
         "431",
         "0.010309278350515464",
         "frozenset({'identify', 'spacy'})",
         "5"
        ],
        [
         "441",
         "0.010309278350515464",
         "frozenset({'spacy', 'load'})",
         "5"
        ],
        [
         "461",
         "0.010309278350515464",
         "frozenset({'spacy', 'pipeline'})",
         "5"
        ],
        [
         "416",
         "0.010309278350515464",
         "frozenset({'en', 'spacy'})",
         "5"
        ],
        [
         "470",
         "0.010309278350515464",
         "frozenset({'return', 'spacy'})",
         "5"
        ],
        [
         "456",
         "0.010309278350515464",
         "frozenset({'spacy', 'parse'})",
         "5"
        ],
        [
         "450",
         "0.008247422680412371",
         "frozenset({'nlppipe', 'spacy'})",
         "5"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 119
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.063918</td>\n",
       "      <td>(entity, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>0.059794</td>\n",
       "      <td>(ner, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.039175</td>\n",
       "      <td>(matcher, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.035052</td>\n",
       "      <td>(custom, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.035052</td>\n",
       "      <td>(training, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.006186</td>\n",
       "      <td>(set, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.006186</td>\n",
       "      <td>(spacy, similar)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.006186</td>\n",
       "      <td>(dictionary, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.006186</td>\n",
       "      <td>(dependency, parsing)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.006186</td>\n",
       "      <td>(span, spacy)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                pattern  cluster\n",
       "423  0.063918        (entity, spacy)        5\n",
       "446  0.059794           (ner, spacy)        5\n",
       "444  0.039175       (matcher, spacy)        5\n",
       "400  0.035052        (custom, spacy)        5\n",
       "485  0.035052      (training, spacy)        5\n",
       "..        ...                    ...      ...\n",
       "474  0.006186           (set, spacy)        5\n",
       "475  0.006186       (spacy, similar)        5\n",
       "410  0.006186    (dictionary, spacy)        5\n",
       "408  0.006186  (dependency, parsing)        5\n",
       "478  0.006186          (span, spacy)        5\n",
       "\n",
       "[119 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==5].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 6: BERT & Hugging Face Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8363571e-23fc-4ec7-9b24-45a1d1faa722",
       "rows": [
        [
         "512",
         "0.011522633744855968",
         "frozenset({'transformer', 'huggingface'})",
         "6"
        ],
        [
         "511",
         "0.00905349794238683",
         "frozenset({'hug', 'face'})",
         "6"
        ],
        [
         "510",
         "0.007407407407407408",
         "frozenset({'training', 'datum'})",
         "6"
        ],
        [
         "507",
         "0.006584362139917695",
         "frozenset({'baye', 'naive'})",
         "6"
        ],
        [
         "513",
         "0.006584362139917695",
         "frozenset({'learn', 'scikit'})",
         "6"
        ],
        [
         "508",
         "0.005761316872427984",
         "frozenset({'bert', 'embedding'})",
         "6"
        ],
        [
         "509",
         "0.005761316872427984",
         "frozenset({'bert', 'huggingface'})",
         "6"
        ],
        [
         "514",
         "0.005761316872427984",
         "frozenset({'rnn', 'pytorch'})",
         "6"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.011523</td>\n",
       "      <td>(transformer, huggingface)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0.009053</td>\n",
       "      <td>(hug, face)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>0.007407</td>\n",
       "      <td>(training, datum)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0.006584</td>\n",
       "      <td>(baye, naive)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.006584</td>\n",
       "      <td>(learn, scikit)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>0.005761</td>\n",
       "      <td>(bert, embedding)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>0.005761</td>\n",
       "      <td>(bert, huggingface)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.005761</td>\n",
       "      <td>(rnn, pytorch)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                     pattern  cluster\n",
       "512  0.011523  (transformer, huggingface)        6\n",
       "511  0.009053                 (hug, face)        6\n",
       "510  0.007407           (training, datum)        6\n",
       "507  0.006584               (baye, naive)        6\n",
       "513  0.006584             (learn, scikit)        6\n",
       "508  0.005761           (bert, embedding)        6\n",
       "509  0.005761         (bert, huggingface)        6\n",
       "514  0.005761              (rnn, pytorch)        6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==6].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 7: Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3c490a3d-a7e7-491e-bb11-8d1025b0f559",
       "rows": [
        [
         "523",
         "0.011210762331838564",
         "frozenset({'regular', 'expression'})",
         "7"
        ],
        [
         "520",
         "0.008221225710014948",
         "frozenset({'similarity', 'cosine'})",
         "7"
        ],
        [
         "524",
         "0.008221225710014948",
         "frozenset({'tag', 'pos'})",
         "7"
        ],
        [
         "516",
         "0.007473841554559043",
         "frozenset({'similarity', 'calculate'})",
         "7"
        ],
        [
         "519",
         "0.007473841554559043",
         "frozenset({'cosine', 'cosine similarity'})",
         "7"
        ],
        [
         "525",
         "0.007473841554559043",
         "frozenset({'regular expression', 'regular'})",
         "7"
        ],
        [
         "517",
         "0.006726457399103139",
         "frozenset({'calculate cosine', 'similarity'})",
         "7"
        ],
        [
         "518",
         "0.005979073243647235",
         "frozenset({'character', 'remove'})",
         "7"
        ],
        [
         "522",
         "0.005979073243647235",
         "frozenset({'datum', 'extract'})",
         "7"
        ],
        [
         "515",
         "0.00523168908819133",
         "frozenset({'calculate cosine', 'calculate'})",
         "7"
        ],
        [
         "521",
         "0.00523168908819133",
         "frozenset({'similarity', 'cosine similarity'})",
         "7"
        ],
        [
         "526",
         "0.00523168908819133",
         "frozenset({'remove stop', 'stop'})",
         "7"
        ],
        [
         "527",
         "0.00523168908819133",
         "frozenset({'calculate cosine', 'similarity', 'calculate'})",
         "7"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 13
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>0.011211</td>\n",
       "      <td>(regular, expression)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>0.008221</td>\n",
       "      <td>(similarity, cosine)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>0.008221</td>\n",
       "      <td>(tag, pos)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>0.007474</td>\n",
       "      <td>(similarity, calculate)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>0.007474</td>\n",
       "      <td>(cosine, cosine similarity)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0.007474</td>\n",
       "      <td>(regular expression, regular)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>0.006726</td>\n",
       "      <td>(calculate cosine, similarity)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>0.005979</td>\n",
       "      <td>(character, remove)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>0.005979</td>\n",
       "      <td>(datum, extract)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.005232</td>\n",
       "      <td>(calculate cosine, calculate)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0.005232</td>\n",
       "      <td>(similarity, cosine similarity)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>0.005232</td>\n",
       "      <td>(remove stop, stop)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0.005232</td>\n",
       "      <td>(calculate cosine, similarity, calculate)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                                    pattern  cluster\n",
       "523  0.011211                      (regular, expression)        7\n",
       "520  0.008221                       (similarity, cosine)        7\n",
       "524  0.008221                                 (tag, pos)        7\n",
       "516  0.007474                    (similarity, calculate)        7\n",
       "519  0.007474                (cosine, cosine similarity)        7\n",
       "525  0.007474              (regular expression, regular)        7\n",
       "517  0.006726             (calculate cosine, similarity)        7\n",
       "518  0.005979                        (character, remove)        7\n",
       "522  0.005979                           (datum, extract)        7\n",
       "515  0.005232              (calculate cosine, calculate)        7\n",
       "521  0.005232            (similarity, cosine similarity)        7\n",
       "526  0.005232                        (remove stop, stop)        7\n",
       "527  0.005232  (calculate cosine, similarity, calculate)        7"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==7].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 8: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bfba4c70-68ec-4a25-9ac7-257ed7d851ad",
       "rows": [
        [
         "617",
         "0.09830508474576272",
         "frozenset({'gensim', 'word2vec'})",
         "8"
        ],
        [
         "563",
         "0.061016949152542375",
         "frozenset({'gensim', 'doc2vec'})",
         "8"
        ],
        [
         "682",
         "0.05084745762711865",
         "frozenset({'word2vec', 'vector'})",
         "8"
        ],
        [
         "661",
         "0.04067796610169491",
         "frozenset({'similarity', 'word2vec'})",
         "8"
        ],
        [
         "574",
         "0.030508474576271188",
         "frozenset({'similarity', 'doc2vec'})",
         "8"
        ],
        [
         "610",
         "0.02711864406779661",
         "frozenset({'gensim', 'similarity'})",
         "8"
        ],
        [
         "577",
         "0.02711864406779661",
         "frozenset({'doc2vec', 'vector'})",
         "8"
        ],
        [
         "677",
         "0.023728813559322035",
         "frozenset({'training', 'word2vec'})",
         "8"
        ],
        [
         "674",
         "0.023728813559322035",
         "frozenset({'word2vec', 'train'})",
         "8"
        ],
        [
         "582",
         "0.020338983050847456",
         "frozenset({'document', 'vector'})",
         "8"
        ],
        [
         "683",
         "0.020338983050847456",
         "frozenset({'word2vec vector', 'vector'})",
         "8"
        ],
        [
         "718",
         "0.01694915254237288",
         "frozenset({'word2vec vector', 'word2vec', 'vector'})",
         "8"
        ],
        [
         "561",
         "0.01694915254237288",
         "frozenset({'doc2vec', 'document'})",
         "8"
        ],
        [
         "551",
         "0.01694915254237288",
         "frozenset({'word2vec', 'datum'})",
         "8"
        ],
        [
         "646",
         "0.01694915254237288",
         "frozenset({'pretraine', 'word2vec'})",
         "8"
        ],
        [
         "689",
         "0.01694915254237288",
         "frozenset({'word2vec vector', 'word2vec'})",
         "8"
        ],
        [
         "583",
         "0.013559322033898305",
         "frozenset({'word2vec', 'document'})",
         "8"
        ],
        [
         "598",
         "0.013559322033898305",
         "frozenset({'gensim', 'gensim word2vec'})",
         "8"
        ],
        [
         "620",
         "0.013559322033898305",
         "frozenset({'gensim word2vec', 'word2vec'})",
         "8"
        ],
        [
         "587",
         "0.013559322033898305",
         "frozenset({'embed', 'word2vec embed'})",
         "8"
        ],
        [
         "641",
         "0.013559322033898305",
         "frozenset({'word2vec', 'parameter'})",
         "8"
        ],
        [
         "535",
         "0.013559322033898305",
         "frozenset({'word2vec object', 'attributeerror'})",
         "8"
        ],
        [
         "703",
         "0.013559322033898305",
         "frozenset({'gensim', 'gensim word2vec', 'word2vec'})",
         "8"
        ],
        [
         "581",
         "0.013559322033898305",
         "frozenset({'similarity', 'document'})",
         "8"
        ],
        [
         "573",
         "0.013559322033898305",
         "frozenset({'doc2vec', 'similar'})",
         "8"
        ],
        [
         "545",
         "0.013559322033898305",
         "frozenset({'corpus', 'word2vec'})",
         "8"
        ],
        [
         "586",
         "0.010169491525423728",
         "frozenset({'embed', 'word2vec'})",
         "8"
        ],
        [
         "696",
         "0.010169491525423728",
         "frozenset({'gensim', 'similarity', 'doc2vec'})",
         "8"
        ],
        [
         "589",
         "0.010169491525423728",
         "frozenset({'word2vec', 'embedding'})",
         "8"
        ],
        [
         "591",
         "0.010169491525423728",
         "frozenset({'epoch', 'gensim'})",
         "8"
        ],
        [
         "597",
         "0.010169491525423728",
         "frozenset({'gensim', 'gensim doc2vec'})",
         "8"
        ],
        [
         "707",
         "0.010169491525423728",
         "frozenset({'gensim', 'similarity', 'word2vec'})",
         "8"
        ],
        [
         "700",
         "0.010169491525423728",
         "frozenset({'vector doc2vec', 'doc2vec', 'vector'})",
         "8"
        ],
        [
         "678",
         "0.010169491525423728",
         "frozenset({'understand', 'word2vec'})",
         "8"
        ],
        [
         "694",
         "0.010169491525423728",
         "frozenset({'gensim', 'doc2vec', 'gensim doc2vec'})",
         "8"
        ],
        [
         "615",
         "0.010169491525423728",
         "frozenset({'gensim', 'vocabulary'})",
         "8"
        ],
        [
         "664",
         "0.010169491525423728",
         "frozenset({'word2vec', 'size'})",
         "8"
        ],
        [
         "621",
         "0.010169491525423728",
         "frozenset({'word2vec', 'glove'})",
         "8"
        ],
        [
         "624",
         "0.010169491525423728",
         "frozenset({'word2vec', 'implementation'})",
         "8"
        ],
        [
         "633",
         "0.010169491525423728",
         "frozenset({'word2vec', 'mean'})",
         "8"
        ],
        [
         "636",
         "0.010169491525423728",
         "frozenset({'word2vec', 'number'})",
         "8"
        ],
        [
         "686",
         "0.010169491525423728",
         "frozenset({'vocabulary', 'word2vec'})",
         "8"
        ],
        [
         "649",
         "0.010169491525423728",
         "frozenset({'word2vec', 'python'})",
         "8"
        ],
        [
         "650",
         "0.010169491525423728",
         "frozenset({'python', 'word2vec python'})",
         "8"
        ],
        [
         "656",
         "0.010169491525423728",
         "frozenset({'score', 'similarity'})",
         "8"
        ],
        [
         "659",
         "0.010169491525423728",
         "frozenset({'word2vec', 'similar'})",
         "8"
        ],
        [
         "680",
         "0.010169491525423728",
         "frozenset({'vector doc2vec', 'vector'})",
         "8"
        ],
        [
         "578",
         "0.010169491525423728",
         "frozenset({'vector doc2vec', 'doc2vec'})",
         "8"
        ],
        [
         "528",
         "0.010169491525423728",
         "frozenset({'word2vec', 'add'})",
         "8"
        ],
        [
         "576",
         "0.010169491525423728",
         "frozenset({'training', 'doc2vec'})",
         "8"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 191
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>0.098305</td>\n",
       "      <td>(gensim, word2vec)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.061017</td>\n",
       "      <td>(gensim, doc2vec)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>0.050847</td>\n",
       "      <td>(word2vec, vector)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>0.040678</td>\n",
       "      <td>(similarity, word2vec)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>0.030508</td>\n",
       "      <td>(similarity, doc2vec)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>0.006780</td>\n",
       "      <td>(gensim, word2vec python)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.006780</td>\n",
       "      <td>(word2vec object, gensim)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0.006780</td>\n",
       "      <td>(doc2vec vector, doc2vec)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.006780</td>\n",
       "      <td>(gensim, weight)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0.006780</td>\n",
       "      <td>(implement word2vec, implement)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                          pattern  cluster\n",
       "617  0.098305               (gensim, word2vec)        8\n",
       "563  0.061017                (gensim, doc2vec)        8\n",
       "682  0.050847               (word2vec, vector)        8\n",
       "661  0.040678           (similarity, word2vec)        8\n",
       "574  0.030508            (similarity, doc2vec)        8\n",
       "..        ...                              ...      ...\n",
       "619  0.006780        (gensim, word2vec python)        8\n",
       "618  0.006780        (word2vec object, gensim)        8\n",
       "559  0.006780        (doc2vec vector, doc2vec)        8\n",
       "616  0.006780                 (gensim, weight)        8\n",
       "623  0.006780  (implement word2vec, implement)        8\n",
       "\n",
       "[191 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==8].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 9: NLP Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pattern",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "dc75a260-dec2-4cc5-845d-07ea7650df57",
       "rows": [
        [
         "729",
         "0.03899926416482708",
         "frozenset({'natural', 'language'})",
         "9"
        ],
        [
         "738",
         "0.03016924208977189",
         "frozenset({'natural language', 'natural'})",
         "9"
        ],
        [
         "739",
         "0.027961736571008096",
         "frozenset({'processing', 'natural'})",
         "9"
        ],
        [
         "732",
         "0.027961736571008096",
         "frozenset({'natural', 'language processing'})",
         "9"
        ],
        [
         "721",
         "0.020603384841795438",
         "frozenset({'analysis', 'sentiment analysis'})",
         "9"
        ],
        [
         "730",
         "0.017660044150110375",
         "frozenset({'natural language', 'language'})",
         "9"
        ],
        [
         "734",
         "0.017660044150110375",
         "frozenset({'processing', 'language processing'})",
         "9"
        ],
        [
         "750",
         "0.01692420897718911",
         "frozenset({'processing', 'natural', 'language processing'})",
         "9"
        ],
        [
         "725",
         "0.01545253863134658",
         "frozenset({'recognition', 'entity'})",
         "9"
        ],
        [
         "747",
         "0.01545253863134658",
         "frozenset({'natural language', 'natural', 'language'})",
         "9"
        ],
        [
         "731",
         "0.013980868285504048",
         "frozenset({'processing', 'language'})",
         "9"
        ],
        [
         "720",
         "0.013245033112582781",
         "frozenset({'sentiment', 'analysis'})",
         "9"
        ],
        [
         "748",
         "0.012509197939661517",
         "frozenset({'processing', 'natural', 'language'})",
         "9"
        ],
        [
         "740",
         "0.01177336276674025",
         "frozenset({'processing', 'natural language'})",
         "9"
        ],
        [
         "726",
         "0.01177336276674025",
         "frozenset({'recognition', 'entity recognition'})",
         "9"
        ],
        [
         "743",
         "0.01177336276674025",
         "frozenset({'sentiment', 'sentiment analysis'})",
         "9"
        ],
        [
         "744",
         "0.011037527593818985",
         "frozenset({'sentiment', 'sentiment analysis', 'analysis'})",
         "9"
        ],
        [
         "728",
         "0.009565857247976454",
         "frozenset({'language processing', 'language'})",
         "9"
        ],
        [
         "751",
         "0.009565857247976454",
         "frozenset({'processing', 'natural language', 'natural'})",
         "9"
        ],
        [
         "746",
         "0.008830022075055188",
         "frozenset({'language processing', 'natural', 'language'})",
         "9"
        ],
        [
         "745",
         "0.008094186902133923",
         "frozenset({'entity', 'recognition', 'entity recognition'})",
         "9"
        ],
        [
         "723",
         "0.008094186902133923",
         "frozenset({'entity recognition', 'entity'})",
         "9"
        ],
        [
         "733",
         "0.007358351729212656",
         "frozenset({'natural language', 'language processing'})",
         "9"
        ],
        [
         "735",
         "0.006622516556291391",
         "frozenset({'topic', 'lda'})",
         "9"
        ],
        [
         "742",
         "0.006622516556291391",
         "frozenset({'semantic', 'similarity'})",
         "9"
        ],
        [
         "737",
         "0.005886681383370125",
         "frozenset({'modeling', 'topic modeling'})",
         "9"
        ],
        [
         "727",
         "0.005886681383370125",
         "frozenset({'phrase', 'extract'})",
         "9"
        ],
        [
         "749",
         "0.005886681383370125",
         "frozenset({'natural language', 'natural', 'language processing'})",
         "9"
        ],
        [
         "719",
         "0.005886681383370125",
         "frozenset({'semantic', 'analysis'})",
         "9"
        ],
        [
         "741",
         "0.0051508462104488595",
         "frozenset({'noun', 'phrase'})",
         "9"
        ],
        [
         "736",
         "0.0051508462104488595",
         "frozenset({'topic', 'modeling'})",
         "9"
        ],
        [
         "724",
         "0.0051508462104488595",
         "frozenset({'extract', 'entity'})",
         "9"
        ],
        [
         "722",
         "0.0051508462104488595",
         "frozenset({'language', 'detect'})",
         "9"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 33
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pattern</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>0.038999</td>\n",
       "      <td>(natural, language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>0.030169</td>\n",
       "      <td>(natural language, natural)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0.027962</td>\n",
       "      <td>(processing, natural)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>0.027962</td>\n",
       "      <td>(natural, language processing)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>0.020603</td>\n",
       "      <td>(analysis, sentiment analysis)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>0.017660</td>\n",
       "      <td>(natural language, language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>0.017660</td>\n",
       "      <td>(processing, language processing)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.016924</td>\n",
       "      <td>(processing, natural, language processing)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>0.015453</td>\n",
       "      <td>(recognition, entity)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.015453</td>\n",
       "      <td>(natural language, natural, language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>0.013981</td>\n",
       "      <td>(processing, language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.013245</td>\n",
       "      <td>(sentiment, analysis)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>0.012509</td>\n",
       "      <td>(processing, natural, language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>0.011773</td>\n",
       "      <td>(processing, natural language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>0.011773</td>\n",
       "      <td>(recognition, entity recognition)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>0.011773</td>\n",
       "      <td>(sentiment, sentiment analysis)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>0.011038</td>\n",
       "      <td>(sentiment, sentiment analysis, analysis)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0.009566</td>\n",
       "      <td>(language processing, language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>0.009566</td>\n",
       "      <td>(processing, natural language, natural)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>0.008830</td>\n",
       "      <td>(language processing, natural, language)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>0.008094</td>\n",
       "      <td>(entity, recognition, entity recognition)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>0.008094</td>\n",
       "      <td>(entity recognition, entity)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>0.007358</td>\n",
       "      <td>(natural language, language processing)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0.006623</td>\n",
       "      <td>(topic, lda)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>0.006623</td>\n",
       "      <td>(semantic, similarity)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0.005887</td>\n",
       "      <td>(modeling, topic modeling)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>0.005887</td>\n",
       "      <td>(phrase, extract)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>0.005887</td>\n",
       "      <td>(natural language, natural, language processing)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0.005887</td>\n",
       "      <td>(semantic, analysis)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0.005151</td>\n",
       "      <td>(noun, phrase)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>0.005151</td>\n",
       "      <td>(topic, modeling)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>0.005151</td>\n",
       "      <td>(extract, entity)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>0.005151</td>\n",
       "      <td>(language, detect)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                                           pattern  cluster\n",
       "729  0.038999                               (natural, language)        9\n",
       "738  0.030169                       (natural language, natural)        9\n",
       "739  0.027962                             (processing, natural)        9\n",
       "732  0.027962                    (natural, language processing)        9\n",
       "721  0.020603                    (analysis, sentiment analysis)        9\n",
       "730  0.017660                      (natural language, language)        9\n",
       "734  0.017660                 (processing, language processing)        9\n",
       "750  0.016924        (processing, natural, language processing)        9\n",
       "725  0.015453                             (recognition, entity)        9\n",
       "747  0.015453             (natural language, natural, language)        9\n",
       "731  0.013981                            (processing, language)        9\n",
       "720  0.013245                             (sentiment, analysis)        9\n",
       "748  0.012509                   (processing, natural, language)        9\n",
       "740  0.011773                    (processing, natural language)        9\n",
       "726  0.011773                 (recognition, entity recognition)        9\n",
       "743  0.011773                   (sentiment, sentiment analysis)        9\n",
       "744  0.011038         (sentiment, sentiment analysis, analysis)        9\n",
       "728  0.009566                   (language processing, language)        9\n",
       "751  0.009566           (processing, natural language, natural)        9\n",
       "746  0.008830          (language processing, natural, language)        9\n",
       "745  0.008094         (entity, recognition, entity recognition)        9\n",
       "723  0.008094                      (entity recognition, entity)        9\n",
       "733  0.007358           (natural language, language processing)        9\n",
       "735  0.006623                                      (topic, lda)        9\n",
       "742  0.006623                            (semantic, similarity)        9\n",
       "737  0.005887                        (modeling, topic modeling)        9\n",
       "727  0.005887                                 (phrase, extract)        9\n",
       "749  0.005887  (natural language, natural, language processing)        9\n",
       "719  0.005887                              (semantic, analysis)        9\n",
       "741  0.005151                                    (noun, phrase)        9\n",
       "736  0.005151                                 (topic, modeling)        9\n",
       "724  0.005151                                 (extract, entity)        9\n",
       "722  0.005151                                (language, detect)        9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_questions.loc[frequency_questions['cluster']==9].sort_values(\"score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put naming back to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QuestionUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreationDate",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ViewCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AnswerCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AcceptedAnswerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AcceptedAnswerBody",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerScore",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "view_range",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "Year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Question_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Answer_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Body_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AcceptedAnswerBody_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_text_clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combination_question_clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean_No_Noise",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title_cluster",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "keywords_fromBert",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_name",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c32c2747-e4d9-4053-bd52-33770415822e",
       "rows": [
        [
         "0",
         "79559702",
         "https://stackoverflow.com/questions/79559702",
         "NameError: name 'init_empty_weights' is not defined while using hugging face models",
         "<p>I am trying to set up hugging face locally and im running into this issue.</p>\n<pre><code>NameError: name 'init_empty_weights' is not defined\n</code></pre>\n<p>Here is the code I have tested my installation with</p>\n<pre><code>from transformers import pipeline\nclassifier = pipeline(&quot;sentiment-analysis&quot;)\ntext = &quot;I love using Hugging Face Transformers!&quot;\nresult = classifier(text)\nprint(result)\n\n\n</code></pre>\n<p>transformers: 4.51.0 <br>\ntokenizers: 0.21.1 <br>\naccelerate: 1.6.0 <br>\nsentence-transformers: 4.0.2 <br>\nhuggingface_hub: 0.30.1 <br>\nI am currently using  pytorch-metal mac M3 pro.<br></p>\n<p>What causes this, and how can I fix it?</p>\n",
         "2025-04-07 00:00:00",
         "nlp,huggingface-transformers,huggingface",
         "3",
         "629",
         "2",
         "79577000.0",
         "<p>Try using this version, it should resolve the issue.</p>\n<pre><code>transformers==4.50.3\n</code></pre>\n",
         "1.0",
         "101-1k",
         "2025",
         "NameError: name 'init_empty_weights' is not defined\n---\nfrom transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\")\ntext = \"I love using Hugging Face Transformers!\"\nresult = classifier(text)\nprint(result)",
         "transformers==4.50.3",
         "nameerror name ' init_empty_weight ' define use hug face model",
         "I am trying to set up hugging face locally and im running into this issue Here is the code I have tested my installation with transformers 4510 tokenizers 0211 accelerate 160 sentencetransformers 402 huggingface_hub 0301 I am currently using pytorchmetal mac M3 pro What causes this and how can I fix it",
         "Try using this version it should resolve the issue",
         "NameError name 'init_empty_weights' is not defined while using hugging face models I am trying to set up hugging face locally and im running into this issue Here is the code I have tested my installation with transformers 4510 tokenizers 0211 accelerate 160 sentencetransformers 402 huggingface_hub 0301 I am currently using pytorchmetal mac M3 pro What causes this and how can I fix it Try using this version it should resolve the issue",
         "nameerror name ' init_empty_weight ' define use hug face model try set hug face locally I m run issue code test installation transformer 4510 tokenizer 0211 accelerate 160 sentencetransformer 402 huggingface_hub 0301 currently use pytorchmetal mac m3 pro cause fix try use version resolve issue",
         "NameError name 'init_empty_weights' is not defined while using hugging face models I am trying to set up hugging face locally and im running into this issue Here is the code I have tested my installation with transformers 4510 tokenizers 0211 accelerate 160 sentencetransformers 402 huggingface_hub 0301 I am currently using pytorchmetal mac M3 pro What causes this and how can I fix it",
         "nameerror name ' init_empty_weight ' define use hug face model try set hug face locally I m run issue code test installation transformer 4510 tokenizer 0211 accelerate 160 sentencetransformer 402 huggingface_hub 0301 currently use pytorchmetal mac m3 pro cause fix",
         "nameerror name initemptyweight define hug face",
         "2",
         "define,face,initemptyweight define,nameerror,define hug",
         "Handling Error in NLP Task"
        ],
        [
         "1",
         "79549787",
         "https://stackoverflow.com/questions/79549787",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?",
         "<p>I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER):</p>\n<pre><code>import spacy\n\nnlp = spacy.load(&quot;pl_core_news_lg&quot;)\ntext = &quot;Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.&quot;\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n</code></pre>\n<p>Output:</p>\n<pre><code>[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n</code></pre>\n<p>However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities.</p>\n<pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=&quot;path_to_my_file/nlp_config.yaml&quot;) \nnlp_engine = provider.create_engine()\n\nprint(f&quot;Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}&quot;)\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[&quot;pl&quot;])\nregistry.load_predefined_recognizers([&quot;pl&quot;])\n\nprint(f&quot;Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}&quot;)\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, &quot;pl&quot;)\n\nfor entity in results:\n    print(f&quot;Found entity: {entity.entity_type} with score {entity.score}&quot;)\n</code></pre>\n<p>Output:</p>\n<pre><code>Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n</code></pre>\n<p>Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text.</p>\n<p>My config file:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n<p>Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?</p>\n",
         "2025-04-02 00:00:00",
         "python,nlp,spacy,presidio",
         "0",
         "97",
         "1",
         "79552218.0",
         "<p>The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine :</p>\n<pre><code>  labels_to_ignore:\n    - O\n</code></pre>\n<p>On your configuration it would look like this:</p>\n<pre><code>nlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID\n</code></pre>\n<p><strong>Edit:</strong> This was fixed to be the default if 'labels_to_ignore' is not specified</p>\n<p>Will be part of version 2.2.359 release</p>\n",
         "1.0",
         "11-100",
         "2025",
         "import spacy\n\nnlp = spacy.load(\"pl_core_news_lg\")\ntext = \"Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.\"\n\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\n\nprint(entities)\n---\n[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]\n---\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nprovider = NlpEngineProvider(conf_file=\"path_to_my_file/nlp_config.yaml\") \nnlp_engine = provider.create_engine()\n\nprint(f\"Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}\")\n\nsupported_languages = list(nlp_engine.get_supported_languages())\nregistry = RecognizerRegistry(supported_languages=[\"pl\"])\nregistry.load_predefined_recognizers([\"pl\"])\n\nprint(f\"Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}\")\n\nanalyzer = AnalyzerEngine(\n    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine\n)\n\nresults = analyzer.analyze(text, \"pl\")\n\nfor entity in results:\n    print(f\"Found entity: {entity.entity_type} with score {entity.score}\")\n---\nSupported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']\nSupported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "labels_to_ignore:\n    - O\n---\nnlp_engine_name: spacy\nmodels:\n  - lang_code: pl\n    model_name: pl_core_news_lg\n\nner_model_configuration:\n  labels_to_ignore:\n    - O\n  model_to_presidio_entity_mapping:\n    persName: PERSON\n    orgName: ORGANIZATION\n#    orgName: ORG\n    placeName: LOCATION\n    geogName: LOCATION\n    LOC: LOCATION\n    GPE: LOCATION\n    FAC: LOCATION\n    DATE: DATE_TIME\n    TIME: DATE_TIME\n    NORP: NRP\n    ID: ID",
         "presidio spacy nlp engine recognize organization pesel spacy",
         "I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and people's names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though 'ORGANIZATION' and 'PL_PESEL' are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them",
         "The configuration file is missing the 'labels_to_ignore' field stating that no entities should be ignored in the nlp engine On your configuration it would look like this Edit This was fixed to be the default if 'labels_to_ignore' is not specified Will be part of version 22359 release",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and people's names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though 'ORGANIZATION' and 'PL_PESEL' are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them The configuration file is missing the 'labels_to_ignore' field stating that no entities should be ignored in the nlp engine On your configuration it would look like this Edit This was fixed to be the default if 'labels_to_ignore' is not specified Will be part of version 22359 release",
         "presidio spacy nlp engine recognize organization pesel spacy ' m use spacy pl_core_news_lg model extract name entity polish text correctly detect organization org people 's name per output however use presidio pl_core_news_lg model configuration file recognizer correctly detect organization org pesel number even though appear list support entity output even though ' organization ' ' pl_pesel ' list org list nlp engine support recognizer presidio detect correctly text config file presidio fail detect organization org pesel number pl_pesel spacy correctly detect configuration file miss ' labels_to_ignore ' field state entity ignore nlp engine configuration would look like edit fix default ' labels_to_ignore ' specify part version 22359 release",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text It correctly detects both organizations ORG and people's names PER Output However when I use Presidio with the pl_core_news_lg model and a configuration file the recognizers do not correctly detect organizations ORG or PESEL numbers even though they appear in the list of supported entities Output Even though 'ORGANIZATION' and 'PL_PESEL' are listed org should be listed in from NLP engine as supported recognizers Presidio does not detect them correctly in the text My config file Why does Presidio fail to detect organizations ORG and PESEL numbers PL_PESEL while spaCy correctly detects them",
         "presidio spacy nlp engine recognize organization pesel spacy ' m use spacy pl_core_news_lg model extract name entity polish text correctly detect organization org people 's name per output however use presidio pl_core_news_lg model configuration file recognizer correctly detect organization org pesel number even though appear list support entity output even though ' organization ' ' pl_pesel ' list org list nlp engine support recognizer presidio detect correctly text config file presidio fail detect organization org pesel number pl_pesel spacy correctly detect",
         "presidio spacy nlp engine recognize organization pesel spacy",
         "5",
         "pesel,spacy,recognize organization,presidio spacy,nlp engine",
         "Using Spacy Library"
        ],
        [
         "2",
         "79548202",
         "https://stackoverflow.com/questions/79548202",
         "GPT-2 and other models from huggingface -100 label index for training, instead of pad token",
         "<p>I understand the -100 label id is used so that the predictions for these are not included when calculating the loss.</p>\n<p>However on <a href=\"https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16#bert2gpt2-summarization-with-%F0%9F%A4%97-encoderdecoder-framework\" rel=\"nofollow noreferrer\">huggingface</a>, they state\n&quot;complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not&quot;, when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument &quot;ignore_index&quot;.</p>\n<p>Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same?</p>\n<p>The way it is written makes me think there is some benefit, but the description of &quot;ignore_index&quot; appears to achieve what is wanted.</p>\n",
         "2025-04-01 00:00:00",
         "nlp,huggingface-transformers,pre-trained-model",
         "0",
         "54",
         "1",
         "79551169.0",
         "<p>The author of the tutorial you mentioned sets it to <code>-100</code> <strong>and</strong> uses <code>ignore_index</code> to save a few lines of code. You don't see the line where the author pass something to <code>ignore_index</code> because it has a default value. The default value of <code>ignore_index</code> for <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\" rel=\"nofollow noreferrer\">nn.CrossEntropyLoss</a> is <code>-100</code>. Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.</p>\n",
         "1.0",
         "11-100",
         "2025",
         "",
         "-100\n---\nignore_index\n---\nignore_index\n---\nignore_index\n---\n-100",
         "gpt2 model huggingface 100 label index training instead pad token",
         "I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted",
         "The author of the tutorial you mentioned sets it to and uses to save a few lines of code You don't see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted The author of the tutorial you mentioned sets it to and uses to save a few lines of code You don't see the line where the author pass something to because it has a default value The default value of for nnCrossEntropyLoss is Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function",
         "gpt2 model huggingface 100 label index training instead pad token understand 100 label i d use prediction include calculate loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label exclude replace pad token implementation use nncrossentropyloss argument ignore_index benefit change i d 100 oppose add argument ignore_index loss set pad token i d result way write make think benefit description ignore_index appear achieve wanted author tutorial mention set use save line code not see line author pass something default value default value nncrossentropyloss use value instead respective pad token i d allow write model indepent training code not pass pad token i d tokenizer loss function",
         "GPT2 and other models from huggingface 100 label index for training instead of pad token I understand the 100 label id is used so that the predictions for these are not included when calculating the loss However on huggingface they state complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not when replacing pad tokens In their implementation they use nnCrossEntropyLoss which has an argument ignore_index Is there any benefit to changing the id to 100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id Or are the results the same The way it is written makes me think there is some benefit but the description of ignore_index appears to achieve what is wanted",
         "gpt2 model huggingface 100 label index training instead pad token understand 100 label i d use prediction include calculate loss however huggingface state complicated list comprehension pad_token_id alone good enough know whether label exclude replace pad token implementation use nncrossentropyloss argument ignore_index benefit change i d 100 oppose add argument ignore_index loss set pad token i d result way write make think benefit description ignore_index appear achieve wanted",
         "gpt2 huggingface 100 label index training instead pad token",
         "6",
         "label,index training,huggingface 100,pad token,gpt2",
         "BERT & Hugging Face Application"
        ],
        [
         "3",
         "79523269",
         "https://stackoverflow.com/questions/79523269",
         "Trouble getting importing gensim to work in colab",
         "<p>I am trying to import gensim into colab.</p>\n<pre><code>!pip install gensim\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--&gt; 367             msg = (&quot;The current Numpy installation ({!r}) fails to &quot;\n    368                    &quot;pass simple sanity checks. This can be caused for example &quot;\n    369                    &quot;by incorrect BLAS library being linked in, or by mixing &quot;\n\nModuleNotFoundError: No module named 'numpy.char'\n</code></pre>\n<p>my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks</p>\n",
         "2025-03-20 00:00:00",
         "numpy,nlp,dependencies,google-colaboratory,gensim",
         "0",
         "209",
         "1",
         "79523777.0",
         "<p>You have to restart the session for the underlying runtime to notice the package changes. See: <a href=\"https://stackoverflow.com/a/79518359/130288\">https://stackoverflow.com/a/79518359/130288</a></p>\n<p>I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded <code>numpy</code>/etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade.</p>\n<p>But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two.</p>\n<p>Explicitly restarting after the Gensim-install &amp; <code>numpy</code>/<code>scipy</code> downgrades resolves the errors.</p>\n",
         "1.0",
         "101-1k",
         "2025",
         "!pip install gensim\n---\n/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)\n    365                 raise AssertionError()\n    366         except AssertionError:\n--> 367             msg = (\"The current Numpy installation ({!r}) fails to \"\n    368                    \"pass simple sanity checks. This can be caused for example \"\n    369                    \"by incorrect BLAS library being linked in, or by mixing \"\n\nModuleNotFoundError: No module named 'numpy.char'",
         "numpy\n---\nnumpy\n---\nscipy",
         "trouble getting import gensim work colab",
         "I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks",
         "You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadn't yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks You have to restart the session for the underlying runtime to notice the package changes See I recall in the past Colab offering a warning when you had to do this And possibly also in the past Colab hadn't yet loaded /etc in a fresh environment and so it was OK for them to downgrade behind the scenes without a problem the 1st import was only after the downgrade But something changed in Colab recently maybe some faststart optimization with a bunch of reports of problems like this in just the last day or two Explicitly restarting after the Gensiminstall & / downgrades resolves the errors",
         "trouble getting import gensim work colab try import gensim colab get follow error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string relate issue thank restart session underlie runtime notice package change see recall past colab offering warn possibly also past colab not yet load /etc fresh environment ok downgrade behind scene without problem 1st import downgrade something change colab recently maybe faststart optimization bunch report problem like last day two explicitly restart gensiminstall & / downgrade resolve error",
         "Trouble getting importing gensim to work in colab I am trying to import gensim into colab I get the following error my numpy version is 202 If I downgrade numpy to another version like say 1264 I get a different error but always a numpy string related issue Thanks",
         "trouble getting import gensim work colab try import gensim colab get follow error numpy version 202 downgrade numpy another version like say 1264 get different error always numpy string relate issue thank",
         "trouble getting import gensim colab",
         "2",
         "getting,trouble,colab,gensim,getting import",
         "Handling Error in NLP Task"
        ],
        [
         "4",
         "79501178",
         "https://stackoverflow.com/questions/79501178",
         "Store images instead of showing in a server",
         "<p>I am running the code found on this <a href=\"https://captum.ai/tutorials/Llama2_LLM_Attribution\" rel=\"nofollow noreferrer\">site</a> in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>\n<p>The code is for instance this one:</p>\n<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = &quot;playing guitar, hiking, and spending time with his family.&quot;\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)\n</code></pre>\n<p>How to store the files locally instead of showing them?</p>\n",
         "2025-03-11 00:00:00",
         "python,nlp,large-language-model",
         "0",
         "39",
         "1",
         "79501337.0",
         "<p>I can't test it but ...</p>\n<p>I checked <a href=\"https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70\" rel=\"nofollow noreferrer\">source code</a> and it uses <code>matplotlib</code> for this.</p>\n<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>\n<p>I think you could use <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>\n<pre><code>import matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(&quot;output.png&quot;)\n#plt.show()  # eventually show it after saving\n</code></pre>\n<hr />\n<p>Probably you can also use <code>fig</code> for this</p>\n<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(&quot;output.png&quot;)\n</code></pre>\n",
         "1.0",
         "11-100",
         "2025",
         "server\n---\nSSH\n---\nskip_tokens = [1]  # skip the special token for the start of the text <s>\ninp = TextTokenInput(\n  eval_prompt, \n  tokenizer,\n  skip_tokens=skip_tokens,\n)\n\ntarget = \"playing guitar, hiking, and spending time with his family.\"\nattr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\nattr_res.plot_token_attr(show=True)",
         "matplotlib\n---\nshow=True\n---\nfig, ax\n---\nimport matplotlib.pyplot as plt\n\n# ... code  ...\n\nattr_res.plot_token_attr()  # without `show=True\nplt.savefig(\"output.png\")\n#plt.show()  # eventually show it after saving\n---\nfig\n---\nfig, ax = attr_res.plot_token_attr()  # without `show=True\nfig.savefig(\"output.png\")",
         "store image instead show server",
         "I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them",
         "I can't test it but I checked source code and it uses for this If you remove then it shouldn't show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them I can't test it but I checked source code and it uses for this If you remove then it shouldn't show it but it should only get I think you could use matplotlibpyplotsavefigfilename to save it in file Probably you can also use for this",
         "store image instead show server run code find site server would like store image instead show since connect remotely ssh connection via connection code instance one store file locally instead show can not test check source code use remove not show get think could use matplotlibpyplotsavefigfilename save file probably also use",
         "Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my via an connection The code is for instance this one How to store the files locally instead of showing them",
         "store image instead show server run code find site server would like store image instead show since connect remotely ssh connection via connection code instance one store file locally instead show",
         "store image instead show server",
         "6",
         "instead,store,image,server,store image",
         "BERT & Hugging Face Application"
        ],
        [
         "5",
         "79482283",
         "https://stackoverflow.com/questions/79482283",
         "Presidio with Langchain Experimental does not detect Polish names",
         "<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>\n<pre><code>from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],\n                                               languages_config=config)\n\ntext = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(&quot;Anonymized text:&quot;, anonymized_result)\nprint(&quot;Deanonymized text:&quot;, deanonymized_result)\nprint(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)\nprint(&quot;Anonymized text:&quot;, anon_result)\n</code></pre>\n<p>Output:</p>\n<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n</code></pre>\n<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.\nI have installed the pl_core_news_lg model using:</p>\n<pre><code>python -m spacy download pl_core_news_lg\n</code></pre>\n<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?\nAny suggestions on how to make it detect names in Polish?</p>\n<p>The interesting thing is that when I use only</p>\n<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()\n</code></pre>\n<p>Then the output look like this:</p>\n<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n</code></pre>\n<p><strong>As mentioned below if I use only spaCy:</strong></p>\n<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)\ndoc = nlp(text)\n</code></pre>\n<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>\n<pre><code>Jan Kowalski persName\nWarszawie placeName\n</code></pre>\n<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>\n",
         "2025-03-03 00:00:00",
         "python,nlp,spacy,langchain,presidio",
         "4",
         "240",
         "2",
         "79495969.0",
         "<p>After some test I was able to find the solution:</p>\n<pre><code>config = {\n    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,\n    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=&quot;pl&quot;,\n    supported_entities=[&quot;persName&quot;]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)\n</code></pre>\n<p>The output look like this:<br />\n<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>\n<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>\n<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>\n<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>\n",
         "-2.0",
         "101-1k",
         "2025",
         "from presidio_anonymizer import PresidioAnonymizer\nfrom presidio_reversible_anonymizer import PresidioReversibleAnonymizer\n\nconfig = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"pl\", \"model_name\": \"pl_core_news_lg\"}],\n}\n\nanonymizer = PresidioAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                languages_config=config)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n                                               languages_config=config)\n\ntext = \"Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\"\n\nanonymized_result = anonymizer_tool.anonymize(text)\nanon_result = anonymizer.anonymize(text)\ndeanonymized_result = anonymizer_tool.deanonymize(anonymized_result)\n\nprint(\"Anonymized text:\", anonymized_result)\nprint(\"Deanonymized text:\", deanonymized_result)\nprint(\"Map:\", anonymizer_tool.deanonymizer_mapping)\nprint(\"Anonymized text:\", anon_result)\n---\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\nMap: {}\nAnonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\npython -m spacy download pl_core_news_lg\n---\nanonymizer_tool = PresidioReversibleAnonymizer()\n---\nAnonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. \nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. \nMap: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}\n---\nnlp = spacy.load(\"pl_core_news_lg\")\ndoc = nlp(text)\n---\nJan Kowalski persName\nWarszawie placeName",
         "config = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": 'pl', \"model_name\": \"pl_core_news_lg\"}],\n}\nspacy_recognizer = SpacyRecognizer(\n    supported_language=\"pl\",\n    supported_entities=[\"persName\"]\n)\nanonymizer.add_recognizer(spacy_recognizer)\n\nanonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"CREDIT_CARD\"], languages_config=config)\n---\nAnonymized text: <persName> mieszka w Warszawie i ma e-mail glenn58@example.org.\n---\nDeanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.\n---\nMap: {'persName': {'<persName>': 'Jan Kowalski', '<persName_2>': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}\n---\nSpacyRecognizer\n---\nsupported_entities",
         "presidio langchain experimental detect polish name",
         "I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that it's the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected",
         "After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCy's requirements I believe there's something missing or unclear in the documentation which is causing the misunderstanding",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that it's the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected After some test I was able to find the solution The output look like this You need to directly add with formatted according to spaCy's requirements I believe there's something missing or unclear in the documentation which is causing the misunderstanding",
         "presidio langchain experimental detect polish name use presidio / langchain_experimental anonymize text polish detect name eg jan kowalski code output expect name jan kowalski email address anonymize output remain unchanged instal pl_core_news_lg model use miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing use output look like mention use spacy output correct guess 's problem presidio output spacy would like create custom analyzer use spacy presidio work expect test able find solution output look like need directly add format accord spacy 's requirement believe 's something miss unclear documentation cause misunderstanding",
         "Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish but it does not detect names eg Jan Kowalski Here is my code Output I expected the name Jan Kowalski and the email address to be anonymized but the output remains unchanged I have installed the pl_core_news_lg model using Am I missing something in the configuration or does Presidio not support Polish entity recognition properly Any suggestions on how to make it detect names in Polish The interesting thing is that when I use only Then the output look like this As mentioned below if I use only spaCy Then the output is correct so I guess that it's the problem with presidio itself Output from spaCy So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected",
         "presidio langchain experimental detect polish name use presidio / langchain_experimental anonymize text polish detect name eg jan kowalski code output expect name jan kowalski email address anonymize output remain unchanged instal pl_core_news_lg model use miss something configuration presidio support polish entity recognition properly suggestion make detect name polish interesting thing use output look like mention use spacy output correct guess 's problem presidio output spacy would like create custom analyzer use spacy presidio work expect",
         "presidio langchain experimental detect polish name",
         "9",
         "experimental,detect,presidio,polish,langchain experimental",
         "NLP Application"
        ],
        [
         "6",
         "79459888",
         "https://stackoverflow.com/questions/79459888",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api\" rel=\"nofollow noreferrer\">TokenizerME</a>, then I tagged it with <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api\" rel=\"nofollow noreferrer\">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=\"https://opennlp.apache.org/models.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=\"https://universaldependencies.org/u/pos/\" rel=\"nofollow noreferrer\">UD POS Tags</a>.</p>\n<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=\"https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas\" rel=\"nofollow noreferrer\">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>\n<p>But, for more complex sentences, it hasn't produced such good results.</p>\n<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=\"https://www.linguateca.pt/Floresta/corpus.html#download\" rel=\"nofollow noreferrer\">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>\n<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>\n<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>\n<ol>\n<li>this does indeed have an impact,</li>\n<li>there is a tool that already does this translation and</li>\n<li>there are any other suggestions for improving the chunker precision/recall.</li>\n</ol>\n",
         "2025-02-22 00:00:00",
         "nlp,opennlp",
         "-1",
         "42",
         "1",
         "79475445.0",
         "<h2>Q1</h2>\n<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>\n<ul>\n<li>Penn -&gt; UD should work well.</li>\n<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>\n</ul>\n<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>\n<h2>Q2</h2>\n<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>\n<h2>Q3</h2>\n<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=\"https://opennlp.apache.org/mailing-lists.html\" rel=\"nofollow noreferrer\">mailing lists</a>. Alternatively, feel free to open a <a href=\"https://issues.apache.org/jira/projects/OPENNLP\" rel=\"nofollow noreferrer\">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>\n",
         "1.0",
         "11-100",
         "2025",
         "",
         "",
         "opennlp postaggerme chunkerme synergy",
         "I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasn't produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall",
         "Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic' Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isn't The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "OpenNLP POSTaggerME and ChunkerME synergy I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasn't produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall Q1 Yes the chosen tag set UD Penn custom has an impact Conversion is not possible in a bidirectional manner Penn > UD should work well UD > Penn is not a good idea as it a lossy conversion UD tag set are less detailed when compared to the classic' Penn tag set Using a custom language specific tagset can work but it is a matter of mapping from/to UD correctly This might work for some tag sets and languages for others it might be too complicated / lossy Q2 No there isn't The OpenNLP project takes code donations for upcoming releases if you want to provide such a mapping/translation for PT lang Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists Alternatively feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition",
         "opennlp postaggerme chunkerme synergy ' m try use opennlp chunk api chunk portuguese sentence first tokenized sentence use tokenizerme tag postaggerme use readymade model provide project sentence ivo viu uva postaggerme return tag propn verb det noun model seem use ud pos tag readymade model chunkerme portuguese follow instruction training first use chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything work well sentence chunker produce correct tag bnp bvp bnp inp complex sentence not produce good result try identify could improve chunker training one thing notice difference type tag portuguese corpus bosque 80 seem use portuguese tag example instead propn corpus use prop instead det use art seem could lead problem especially since one parameter chunker receive array ud tag train another type tag write code create routine convert portuguese notation ud penn want ask indeed impact tool already translation suggestion improve chunker precision / recall q1 yes choose tag set ud penn custom impact conversion possible bidirectional manner penn > ud work well ud > penn good idea lossy conversion ud tag set less detailed compare classic ' penn tag set use custom language specific tagset work matter mapping from / to ud correctly might work tag set language other might complicated / lossy q2 not opennlp project take code donation upcoming release want provide mapping / translation pt lang q3 need detail / discussion apache opennlp user and/or dev mailing list alternatively feel free open jira issue drill topic clear idea propose code addition",
         "OpenNLP POSTaggerME and ChunkerME synergy I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence So first I tokenized a sentence using TokenizerME then I tagged it with POSTaggerME For both I used the readymade models provided by the project here For the sentence Ivo viu a uva POSTaggerME returns the tags PROPN VERB DET NOUN The model seems to be using the UD POS Tags As there is no readymade model for ChunkerME in portuguese I followed the instructions and did the training first using the ChunkerConverter tool to convert from arvore deitada to CoNLL2000 and then generating the model with ChunkerTrainerME tool Everything worked well For the sentence above the chunker produced correct tags BNP BVP BNP INP But for more complex sentences it hasn't produced such good results I was trying to identify what I could improve in chunker training and one of the things I noticed is that there is a difference between the types of tags The portuguese corpus Bosque 80 seems to be using portuguese tags For example instead of PROPN the corpus uses prop and instead of DET it uses art It seems to me that this could lead to problems especially since one of the parameters the chunker receives is an array with UD tags but it has been trained with another type of tag But before writing code creating a routine to convert from a portuguese notation to UD or Penn I wanted to ask if this does indeed have an impact there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall",
         "opennlp postaggerme chunkerme synergy ' m try use opennlp chunk api chunk portuguese sentence first tokenized sentence use tokenizerme tag postaggerme use readymade model provide project sentence ivo viu uva postaggerme return tag propn verb det noun model seem use ud pos tag readymade model chunkerme portuguese follow instruction training first use chunkerconverter tool convert arvore deitada conll2000 generating model chunkertrainerme tool everything work well sentence chunker produce correct tag bnp bvp bnp inp complex sentence not produce good result try identify could improve chunker training one thing notice difference type tag portuguese corpus bosque 80 seem use portuguese tag example instead propn corpus use prop instead det use art seem could lead problem especially since one parameter chunker receive array ud tag train another type tag write code create routine convert portuguese notation ud penn want ask indeed impact tool already translation suggestion improve chunker precision / recall",
         "opennlp postaggerme chunkerme synergy",
         "3",
         "synergy,postaggerme,chunkerme,opennlp,opennlp postaggerme",
         "Using Stanford Library"
        ],
        [
         "7",
         "79419884",
         "https://stackoverflow.com/questions/79419884",
         "Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged",
         "<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>\n<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>\n<p>This is several information about the dataset:</p>\n<p>Embedding size: (41151, 100)</p>\n<p>Maximum sequence length: 731</p>\n<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>\n<p>Total x training set (padded): (140997, 200)</p>\n<p>Total x validation set (padded): (17625, 200)</p>\n<p>Total x testing set (padded): (17625, 200)</p>\n<p>Total y training set (one hot): (140997, 3)</p>\n<p>Total y validation set (one hot): (17625, 3)</p>\n<p>Total y testing set (one hot): (17625, 2003</p>\n<p>This is my full code:\n<a href=\"https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>This is my highlight code for this issue:</p>\n<pre><code>lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/T6vCZ9Jj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/T6vCZ9Jj.png\" alt=\"enter image description here\" /></a></p>\n",
         "2025-02-07 00:00:00",
         "keras,deep-learning,nlp,lstm,sentiment-analysis",
         "-1",
         "50",
         "1",
         "79425201.0",
         "<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>\n<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>\n<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.\nThis is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.\n(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>\n",
         "0.0",
         "11-100",
         "2025",
         "lstm_model = Sequential()\nlstm_model.add(Input(shape=(max_len,)))\nlstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\nlstm_model.add(LSTM(256, return_sequences=True))\nlstm_model.add(LSTM(128, return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(128, activation='relu'))\nlstm_model.add(Dense(units=3, activation='softmax'))\n\nlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nlstm_model.summary()",
         "",
         "underfitte pretraine glove + lstm model accurcacy unchanged",
         "I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {'positive' 58749 'negative' 26643 'neutral' 9106} Label distribution after random over sampling 'positive' 58749 'negative' 26643 'neutral' 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue",
         "Based on extra information in the comments I'm going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort I'm fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldn't find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word So queen might be a female king as in embedding's party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers don't have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {'positive' 58749 'negative' 26643 'neutral' 9106} Label distribution after random over sampling 'positive' 58749 'negative' 26643 'neutral' 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue Based on extra information in the comments I'm going to say the reason the LSTM model hits a wall at an unspecified lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem In which case tweaking parameters is likely to be wasted effort I'm fairly sure encoder transformers eg BERT surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldn't find a killer reference to insert here and transformers have only got bigger and better since then Extra thought building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word So queen might be a female king as in embedding's party trick king male + female = queen or it might be a pop group or it might be a gay man or it might be a chess piece This is going to put a limit on the accuracy of models built on them whereas transformers don't have that limitation because they look at the whole string to see the words in context It is possible to argue with that of course because bringing in the context is where the LSTM comes in But transformers are still scaling with 20+ layers whereas LSTMs tend to choke after two layers",
         "underfitte pretraine glove + lstm model accurcacy unchanged sentiment classification use pretraine glove lstm model use google play review scrap result 50k++ text implement random sampling minority class however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling { ' positive ' 58749 ' negative ' 26643 ' neutral ' 9106 } label distribution random sampling ' positive ' 58749 ' negative ' 26643 ' neutral ' 9106 } total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue base extra information comment ' m go say reason lstm model hit wall unspecified low accuracy 85 % try reach good type model problem case tweak parameter likely waste effort ' m fairly sure encoder transformer eg bert surpass sentiment analysis benchmark number year back sorry quick search could not find killer reference insert transformer get big well since extra thought build top glove embedding present problem not handle multiple meaning word queen might female king embed 's party trick king male + female = queen might pop group might gay man might chess piece going put limit accuracy model build whereas transformer not limitation look whole string see word context possible argue course bring context lstm come transformer still scale 20 + layer whereas lstms tend choke two layer",
         "Underfitting PreTrained Glove + LSTM Model Accurcacy Unchanged I am doing a sentiment classification using PreTrained Glove and LSTM model I use google play review and scrap it by myself resulting in 50k++ texts I implement random over sampling on the minority classes However when I train my LSTM model the training accuracy is remain unchanged after several epoch need insight how to fix the issue This is several information about the dataset Embedding size 41151 100 Maximum sequence length 731 Label distribution before random over sampling {'positive' 58749 'negative' 26643 'neutral' 9106} Label distribution after random over sampling 'positive' 58749 'negative' 26643 'neutral' 9106} Total x training set padded 140997 200 Total x validation set padded 17625 200 Total x testing set padded 17625 200 Total y training set one hot 140997 3 Total y validation set one hot 17625 3 Total y testing set one hot 17625 2003 This is my full code enter link description here This is my highlight code for this issue",
         "underfitte pretraine glove + lstm model accurcacy unchanged sentiment classification use pretraine glove lstm model use google play review scrap result 50k++ text implement random sampling minority class however train lstm model training accuracy remain unchanged several epoch need insight fix issue several information dataset embed size 41151 100 maximum sequence length 731 label distribution random sampling { ' positive ' 58749 ' negative ' 26643 ' neutral ' 9106 } label distribution random sampling ' positive ' 58749 ' negative ' 26643 ' neutral ' 9106 } total x training set pad 140997 200 total x validation set pad 17625 200 total x testing set pad 17625 200 total training set one hot 140997 3 total validation set one hot 17625 3 total testing set one hot 17625 2003 full code enter link description highlight code issue",
         "underfitte pretraine glove lstm accurcacy unchanged",
         "6",
         "accurcacy unchanged,underfitte,glove,pretraine,lstm",
         "BERT & Hugging Face Application"
        ],
        [
         "8",
         "79330283",
         "https://stackoverflow.com/questions/79330283",
         "Can't compile Marian NMT",
         "<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=\"https://marian-nmt.github.io/docs/#installation\" rel=\"nofollow noreferrer\">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>\n<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>\n<p>These are the steps that I followed:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n</code></pre>\n<p>This is the result I had:</p>\n<pre><code>➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t&lt;Mutex&gt;() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2\n</code></pre>\n<p>Please help.</p>\n",
         "2025-01-05 00:00:00",
         "gcc,cmake,nlp,g++",
         "4",
         "75",
         "1",
         "79332711.0",
         "<p>The diagnostic that your build is tripping, <code>Wtemplate-id-cdtor</code>, was introduced\nwith GCC 14.1. It is a warning, not an error, but your build promotes all warnings to\nerrors, so it breaks your build.</p>\n<p>Although your build specifies <code>-std=c++11</code> in <code>src/3rd_party/spdlog/CMakeLists.txt</code>, which\ngenerates the failure, g++-14 emits <code>Wtemplate-id-cdtor</code> to warn you that the code <em>would be</em>\nillegal under the more recent standard c++20 (and later). Then the warning is made an error.</p>\n<p>The warning is made an error by the compile option <code>-Werror</code>. This option is included in the list\nof compile options <code>ALL_WARNINGS</code>, which is created in the top-level <code>marian/CMakeLists.txt</code>\nat line 227 <em>et seq</em>:</p>\n<pre><code># These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n</code></pre>\n<p>and then applied as compile options for the <code>marian</code> library target in <code>src/CMakeLists.txt</code>\nat line 133:</p>\n<pre><code>target_compile_options(marian PRIVATE ${ALL_WARNINGS})\n</code></pre>\n<p>whence the options are operative for the failing compilation of <code>src/CMakeFiles/marian.dir/common/logging.cpp</code>.</p>\n<p>This failure is a bug in the <code>marian</code> repo which you should <a href=\"https://github.com/marian-nmt/marian/issues\" rel=\"nofollow noreferrer\">report to the maintainers</a>, as\nit does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14.</p>\n<p>Pending a fix, you seem to have three interim options to get your build done. Either:</p>\n<ul>\n<li><p>Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:</p>\n<pre><code>/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’\n</code></pre>\n</li>\n</ul>\n<p>e.g. make it <code>registry_t(const registry_t&lt;Mutex&gt;&amp;) = delete;</code> in this occurrence.</p>\n<p>Or:</p>\n<ul>\n<li><p>Locally disable <code>-Wtemplate-id-cdtor</code> at each occurrence, e.g:</p>\n<pre><code>#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored &quot;-Wtemplate-id-cdtor&quot;\nregistry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;\n#pragma GCC diagnostic pop\n</code></pre>\n</li>\n</ul>\n<p>Or:</p>\n<ul>\n<li>Remove <code>-Werror</code> from the <code>ALL_WARNINGS</code> list in <code>marian/CMakeLists.txt</code> so that <code>Wtemplate-id-cdtor</code> remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status).</li>\n</ul>\n<p>I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.</p>\n",
         "4.0",
         "11-100",
         "2025",
         "CMakeLists.txt\n---\nset (CMAKE_CXX_STANDARD 11)\n---\ngit clone https://github.com/marian-nmt/marian\nmkdir marian/build\ncd marian/build\ncmake ..\nmake -j4\n---\n➜ make -j4\n[  1%] Built target 3rd_party_installs\n[  1%] Built target marian_version\n[  6%] Built target sentencepiece_train-static\n[ 19%] Built target libyaml-cpp\n[ 25%] Built target SQLiteCpp\n[ 25%] Built target pathie-cpp\n[ 32%] Built target zlib\n[ 35%] Built target intgemm\n[ 35%] Built target faiss\n[ 53%] Built target sentencepiece-static\n[ 55%] Built target spm_decode\n[ 55%] Built target spm_normalize\n[ 55%] Built target spm_encode\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o\n[ 56%] Built target spm_train\n[ 57%] Built target spm_export_vocab\n[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/fastopt.h:3,\n                 from /data/tools/marian/src/common/fastopt.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/utils.cpp:2:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/logging.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\nIn file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,\n                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,\n                 from /data/tools/marian/src/common/logging.h:5,\n                 from /data/tools/marian/src/common/definitions.h:3,\n                 from /data/tools/marian/src/common/cli_wrapper.h:6,\n                 from /data/tools/marian/src/common/config_parser.h:4,\n                 from /data/tools/marian/src/common/aliases.cpp:1:\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  138 |     registry_t<Mutex>() {}\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1\ncc1plus: all warnings being treated as errors\nmake[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2\nmake: *** [Makefile:156: all] Error 2",
         "Wtemplate-id-cdtor\n---\n-std=c++11\n---\nsrc/3rd_party/spdlog/CMakeLists.txt\n---\nWtemplate-id-cdtor\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\n# These are used in src/CMakeLists.txt on a per-target basis\nlist(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;\n-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;\n-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;\n-Wno-missing-field-initializers;)\n---\nmarian\n---\nsrc/CMakeLists.txt\n---\ntarget_compile_options(marian PRIVATE ${ALL_WARNINGS})\n---\nsrc/CMakeFiles/marian.dir/common/logging.cpp\n---\nmarian\n---\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]\n  139 |     registry_t<Mutex>(const registry_t<Mutex>&) = delete;\n      |                      ^\n/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’\n---\nregistry_t(const registry_t<Mutex>&) = delete;\n---\n-Wtemplate-id-cdtor\n---\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wtemplate-id-cdtor\"\nregistry_t<Mutex>(const registry_t<Mutex>&) = delete;\n#pragma GCC diagnostic pop\n---\n-Werror\n---\nALL_WARNINGS\n---\nmarian/CMakeLists.txt\n---\nWtemplate-id-cdtor",
         "can not compile marian nmt",
         "I'm using endeavouros I'm trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help",
         "The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I haven't tested any of these options as I'd need to go to the trouble of installing CUDA",
         "Can't compile Marian NMT I'm using endeavouros I'm trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help The diagnostic that your build is tripping was introduced with GCC 141 It is a warning not an error but your build promotes all warnings to errors so it breaks your build Although your build specifies in which generates the failure g++14 emits to warn you that the code would be illegal under the more recent standard c++20 and later Then the warning is made an error The warning is made an error by the compile option This option is included in the list of compile options which is created in the toplevel at line 227 et seq and then applied as compile options for the library target in at line 133 whence the options are operative for the failing compilation of This failure is a bug in the repo which you should report to the maintainers as it does not seem to have been reported already The head revision v1120 is more than a year older than GCC 14 Pending a fix you seem to have three interim options to get your build done Either Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence eg make it in this occurrence Or Locally disable at each occurrence eg Or Remove from the list in so that remains just a warning This may result in other diagnostics being demoted from errors to warnings their default status I haven't tested any of these options as I'd need to go to the trouble of installing CUDA",
         "can not compile marian nmt ' m use endeavouros ' m try compile marian instruction fail error message seemingly indicate conflict code c++20 file repo line step follow result please help diagnostic build tripping introduce gcc 141 warning error build promote warning error break build although build specifie generate failure g++14 emit warn code would illegal recent standard c++20 later warn make error warning make error compile option option include list compile option create toplevel line 227 et seq apply compile option library target line 133 whence option operative failing compilation failure bug repo report maintainer seem report already head revision v1120 year old gcc 14 pende fix seem three interim option get build do either make code legal c++11 c++20 diagnostic advice say occurrence eg make occurrence locally disable occurrence eg remove list remains warn may result diagnostic demote error warning default status not test option would need go trouble instal cuda",
         "Can't compile Marian NMT I'm using endeavouros I'm trying to compile Marian with these instructions But it fails The error message seemingly indicates a conflict between the code and c++20 But in all the files of the repo there is the line These are the steps that I followed This is the result I had Please help",
         "can not compile marian nmt ' m use endeavouros ' m try compile marian instruction fail error message seemingly indicate conflict code c++20 file repo line step follow result please help",
         "can not compile marian nmt",
         "2",
         "nmt,marian,compile,marian nmt,compile marian",
         "Handling Error in NLP Task"
        ],
        [
         "9",
         "79328514",
         "https://stackoverflow.com/questions/79328514",
         "how to get custom column in the model's forward() function when training with Huggingface Trainer?",
         "<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )\n</code></pre>\n<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>\n",
         "2025-01-04 00:00:00",
         "pytorch,nlp,large-language-model,huggingface-trainer",
         "2",
         "44",
         "1",
         "79328698.0",
         "<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>\n<p>To modify the <strong>data collator</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>class CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])\n        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])\n        return batch\n</code></pre>\n<p>then pass it to <code>Trainer</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n</code></pre>\n<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>\n<p>Hope, it will work!</p>\n",
         "1.0",
         "11-100",
         "2025",
         "input_ids\n---\nlabels\n---\ninteract_ids\n---\ncandidate_ids\n---\nclass LLMWithCustomLayer(LlamaForCausalLM)\n---\ndef forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            interact_ids = None,\n            candidate_ids = None,\n        ):\n            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none\n    \n            interact_embs = []\n            candidate_embs = []\n            for i in range(interact_ids.shape(0)):\n                # O_i = F_i (e_i)\n                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))\n                # O_i = F_i (e_i)\n                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))\n                # replace [CandidateEmb] and [HistoryEmb]\n                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)\n    \n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels = labels\n            )",
         "interact_ids\n---\ncandidate_ids\n---\nclass CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[\"interact_ids\"] = torch.tensor([f[\"interact_ids\"] for f in features])\n        batch[\"candidate_ids\"] = torch.tensor([f[\"candidate_ids\"] for f in features])\n        return batch\n---\nTrainer\n---\ntrainer = Trainer(\n    model=LLMWithCustomLayer.from_pretrained(\"your-llama-model\"),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=CustomDataCollator(tokenizer)\n)\n---\nforward()\n---\ninteract_ids\n---\ncandidate_ids",
         "get custom column model 's forward function training huggingface trainer",
         "I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields ' ' ' ' and so on and I additionally add 2 custom colunms ' ' and ' ' But i can't get these custom fields in the forward function of my Model ' ' I an new in LLM fine tuning Can anyone help me I would be grateful so much",
         "You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "how to get custom column in the model's forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields ' ' ' ' and so on and I additionally add 2 custom colunms ' ' and ' ' But i can't get these custom fields in the forward function of my Model ' ' I an new in LLM fine tuning Can anyone help me I would be grateful so much You need to modify the data collator to pass and to your model as Trainer ignores extra columns by default To modify the data collator then pass it to Now your method will receive and Hope it will work",
         "get custom column model 's forward function training huggingface trainer use huggingface trainer train cumstom model subclasse llama llm tokenized tokenizer dataset field ' ' ' ' additionally add 2 custom colunms ' ' ' ' can not get custom field forward function model ' ' new llm fine tuning anyone help would grateful much need modify datum collator pass model trainer ignore extra column default modify datum collator pass method receive hope work",
         "how to get custom column in the model's forward function when training with Huggingface Trainer I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm After tokenized by the tokenizer my dataset has these fields ' ' ' ' and so on and I additionally add 2 custom colunms ' ' and ' ' But i can't get these custom fields in the forward function of my Model ' ' I an new in LLM fine tuning Can anyone help me I would be grateful so much",
         "get custom column model 's forward function training huggingface trainer use huggingface trainer train cumstom model subclasse llama llm tokenized tokenizer dataset field ' ' ' ' additionally add 2 custom colunms ' ' ' ' can not get custom field forward function model ' ' new llm fine tuning anyone help would grateful much",
         "get custom column s forward function training huggingface trainer",
         "6",
         "huggingface,function training,forward function,trainer,custom column",
         "BERT & Hugging Face Application"
        ],
        [
         "10",
         "79312133",
         "https://stackoverflow.com/questions/79312133",
         "Getting all leaf words (reverse stemming) into one Python List",
         "<p>On the same lines as the solution provided <a href=\"https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword\">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>\n<p>Imagine I have a shorter sample word list as follows:</p>\n<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n</code></pre>\n<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>\n<pre><code>get_word_forms(&quot;command&quot;)\n</code></pre>\n<p>and get the following output:</p>\n<pre><code>{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n</code></pre>\n<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>\n<p>If I try to reverse-stem the entire list in one go:</p>\n<pre><code>[get_word_forms(word) for word in sample]\n</code></pre>\n<p>I fail at getting any output:</p>\n<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n</code></pre>\n<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>\n<p>something like:</p>\n<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. \n</code></pre>\n",
         "2024-12-27 00:00:00",
         "python,nlp,nltk",
         "1",
         "51",
         "1",
         "79312987.0",
         "<p>One solution using nested list comprehensions after stripping forgotten spaces:</p>\n<pre><code>all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']\n</code></pre>\n",
         "1.0",
         "11-100",
         "2024",
         "get_word_forms\n---\nmy_list = [' jail', ' belief',' board',' target', ' challenge', ' command']\n---\nget_word_forms(\"command\")\n---\n{'n': {'command',\n  'commandant',\n  'commandants',\n  'commander',\n  'commanders',\n  'commandership',\n  'commanderships',\n  'commandment',\n  'commandments',\n  'commands'},\n 'a': set(),\n 'v': {'command', 'commanded', 'commanding', 'commands'},\n 'r': set()}\n---\n[get_word_forms(word) for word in sample]\n---\n[{'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()},\n {'n': set(), 'a': set(), 'v': set(), 'r': set()}]\n---\n['command','commandant','commandants',  'commander', 'commanders', 'commandership',\n'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.",
         "all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]\n\n# Flatten the list of sets\nall_words = [word for setx in all_words for word in setx]\n\n# Remove the repetitions and sort the set\nall_words = sorted(set(all_words))\nprint(all_words)\n\n['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']",
         "get leaf word reverse stem one python list",
         "On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output 'n' is noun 'a' is adjective 'v' is verb and 'r' is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like",
         "One solution using nested list comprehensions after stripping forgotten spaces",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output 'n' is noun 'a' is adjective 'v' is verb and 'r' is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like One solution using nested list comprehensions after stripping forgotten spaces",
         "get leaf word reverse stem one python list line solution provide link try get leaf word one stem word use communitycontributed srivastava package imagine short sample word list follow work manually follow go wordbyword timeconsuming list 200 word get follow output ' n ' noun ' ' adjective ' v ' verb ' r ' adverb try reversestem entire list one go fail get output think fail save output dictionary eventually would like output list without break noun adjective adverb verb something like one solution use nested list comprehension strip forget space",
         "Getting all leaf words reverse stemming into one Python List On the same lines as the solution provided in this link I am trying to get all leaf words of one stem word I am using the communitycontributed Srivastava package Imagine I have a shorter sample word list as follows If I work it manually I do the following which is go wordbyword which is timeconsuming if I have a list of 200 words and get the following output 'n' is noun 'a' is adjective 'v' is verb and 'r' is adverb If I try to reversestem the entire list in one go I fail at getting any output I think I am failing at saving the output to the dictionary Eventually I would like my output to be a list without breaking it down into noun adjective adverb or verb something like",
         "get leaf word reverse stem one python list line solution provide link try get leaf word one stem word use communitycontributed srivastava package imagine short sample word list follow work manually follow go wordbyword timeconsuming list 200 word get follow output ' n ' noun ' ' adjective ' v ' verb ' r ' adverb try reversestem entire list one go fail get output think fail save output dictionary eventually would like output list without break noun adjective adverb verb something like",
         "get leaf reverse stem python",
         "4",
         "python,reverse,leaf,stem python,reverse stem",
         "Preprocessing Text in Python"
        ],
        [
         "11",
         "79298368",
         "https://stackoverflow.com/questions/79298368",
         "Inspect all probabilities of BERTopic model",
         "<p>Say I build a BERTopic model using</p>\n<pre><code>from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre>\n<p>Inspecting <code>probs</code> gives me just a single value for each item in <code>docs</code>.</p>\n<pre><code>probs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n</code></pre>\n<p>I would like the entire probability vector across all topics (so in this case, where <code>nr_topics=20</code>, I want a vector of 20 probabilities for each item in <code>docs</code>). In other words, if I have N items in <code>docs</code> and K topics, I would like an NxK output.</p>\n",
         "2024-12-20 00:00:00",
         "python,nlp,topic-modeling",
         "1",
         "58",
         "1",
         "79299703.0",
         "<p>For individual topic probability across each document you need to add one more argument.</p>\n<pre><code>topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n</code></pre>\n<p>Note: This calculate_probabilities = True will only work if you are using <strong><code>HDBSCAN</code></strong> clustering embedding model. And Bertopic by default uses <code>all-MiniLM-L6-v2</code>.</p>\n<p><strong>Official documentation:</strong> <a href=\"https://maartengr.github.io/BERTopic/api/bertopic.html\" rel=\"nofollow noreferrer\">https://maartengr.github.io/BERTopic/api/bertopic.html</a></p>\n<p>They have mentioned the same in document as well.</p>\n",
         "1.0",
         "11-100",
         "2024",
         "from bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)\ntopics, probs = topic_model.fit_transform(docs)\n---\nprobs\n---\ndocs\n---\nprobs\narray([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,\n       1.        ])\n---\nnr_topics=20\n---\ndocs\n---\ndocs",
         "topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)\n---\nHDBSCAN\n---\nall-MiniLM-L6-v2",
         "inspect probability bertopic model",
         "Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output",
         "For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output For individual topic probability across each document you need to add one more argument Note This calculate_probabilities = True will only work if you are using clustering embedding model And Bertopic by default uses Official documentation They have mentioned the same in document as well",
         "inspect probability bertopic model say build bertopic model use inspecting give single value item would like entire probability vector across topic case want vector 20 probability item word n item k topic would like nxk output individual topic probability across document need add one argument note calculate_probabilitie = true work use cluster embed model bertopic default use official documentation mention document well",
         "Inspect all probabilities of BERTopic model Say I build a BERTopic model using Inspecting gives me just a single value for each item in I would like the entire probability vector across all topics so in this case where I want a vector of 20 probabilities for each item in In other words if I have N items in and K topics I would like an NxK output",
         "inspect probability bertopic model say build bertopic model use inspecting give single value item would like entire probability vector across topic case want vector 20 probability item word n item k topic would like nxk output",
         "inspect probability bertopic",
         "9",
         "inspect,probability,bertopic,inspect probability,probability bertopic",
         "NLP Application"
        ],
        [
         "12",
         "79293919",
         "https://stackoverflow.com/questions/79293919",
         "Determining most popular words in the English dictionary within a dictionary of words",
         "<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>\n<p>For example:\nA sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>\n<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>\n<p>Any and all help is welcome and appreciated.</p>\n<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>\n",
         "2024-12-19 00:00:00",
         "python,nlp,nltk,detection",
         "0",
         "69",
         "2",
         "79294074.0",
         "<p>You need a external dataset for this task. You can try dataset such as google n gram dataset.</p>\n<p>Here is the breakdown of the problem statement:</p>\n<ol>\n<li>Input: &quot;I enjoy a cold glass of water on a hot day&quot;. <code>Output</code>: &quot;water&quot;.</li>\n<li>Split the sentences into words list.</li>\n</ol>\n<blockquote>\n<p>Example: [&quot;I&quot;, &quot;enjoy&quot;, &quot;a&quot;, &quot;cold&quot;, &quot;glass&quot;, &quot;of&quot;, &quot;water&quot;, &quot;on&quot;,\n&quot;a&quot;, &quot;hot&quot;, &quot;day&quot;]</p>\n</blockquote>\n<ol start=\"3\">\n<li>First loop in through all the word of the sentences. so let say you are at first word &quot;I&quot;.</li>\n<li>Now you will look the same word &quot;I&quot; in external dataset and will look for the frequency of that word.\nLet say the word &quot;I&quot; in external dataset is repeated <code>5000000</code> times</li>\n<li>Repeat this task for all the word.</li>\n<li>Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.\nFrequency in the below example is random value not exact value.</li>\n</ol>\n<blockquote>\n<pre><code>{\n    &quot;I&quot;: 5000000,\n    &quot;enjoy&quot;: 50000,\n    &quot;a&quot;: 10000000,\n    &quot;cold&quot;: 30000,\n    &quot;glass&quot;: 100000,\n    &quot;of&quot;: 8000000,\n    &quot;water&quot;: 1200000,\n    &quot;on&quot;: 6000000,\n    &quot;hot&quot;: 700000,\n    &quot;day&quot;: 400000\n}\n</code></pre>\n</blockquote>\n<ol start=\"7\">\n<li>Pick the word with highest frequency.</li>\n</ol>\n<p>Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself</p>\n",
         "2.0",
         "11-100",
         "2024",
         "",
         "Output\n---\n5000000\n---\n{\n    \"I\": 5000000,\n    \"enjoy\": 50000,\n    \"a\": 10000000,\n    \"cold\": 30000,\n    \"glass\": 100000,\n    \"of\": 8000000,\n    \"water\": 1200000,\n    \"on\": 6000000,\n    \"hot\": 700000,\n    \"day\": 400000\n}",
         "determine popular word english dictionary within dictionary word",
         "Forgive me if my wording is awful but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made I've done some research on NLTK but can't seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because it's the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure I'll likely have to involve AI but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it so I'm trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess",
         "You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made I've done some research on NLTK but can't seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because it's the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure I'll likely have to involve AI but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it so I'm trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess You need a external dataset for this task You can try dataset such as google n gram dataset Here is the breakdown of the problem statement Input I enjoy a cold glass of water on a hot day water Split the sentences into words list Example I enjoy a cold glass of water on a hot day First loop in through all the word of the sentences so let say you are at first word I Now you will look the same word I in external dataset and will look for the frequency of that word Let say the word I in external dataset is repeated times Repeat this task for all the word Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data Frequency in the below example is random value not exact value Pick the word with highest frequency Note You can try any big corpus as external data using big corpus will have most of the English word which is used in conversation And even if the frequency is not mentioned then you can create that yourself",
         "determine popular word english dictionary within dictionary word forgive word awful ' m try figure determine use word english language set word dictionary ' ve make ' ve do research nltk can not seem find function within library matter help need example sentence enjoy cold glass water hot day would return water 's use word day day conversation sentence essentially need return value frequently use word conversation figure will likely involve ai time ' ve try use ai wind copy paste code not understand ' m try avoid go route help welcome appreciated context decide start project would essentially guess predetermine word base character user say not computer guess need external dataset task try dataset google n gram dataset breakdown problem statement input enjoy cold glass water hot day water split sentence word list example enjoy cold glass water hot day first loop word sentence let say first word look word external dataset look frequency word let say word external dataset repeat time repeat task word dictionary word sentence key value frequency word get external data frequency example random value exact value pick word high frequency note try big corpus external datum use big corpus english word use conversation even frequency mention create",
         "Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made I've done some research on NLTK but can't seem to find a function within it or any other library for that matter that will help me do what I need to do For example A sentence I enjoy a cold glass of water on a hot day would return water because it's the most used word in day to day conversation from the sentence Essentially I need a returned value of the most frequently used word in conversations I figure I'll likely have to involve AI but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it so I'm trying to avoid going that route Any and all help is welcome and appreciated For context I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess",
         "determine popular word english dictionary within dictionary word forgive word awful ' m try figure determine use word english language set word dictionary ' ve make ' ve do research nltk can not seem find function within library matter help need example sentence enjoy cold glass water hot day would return water 's use word day day conversation sentence essentially need return value frequently use word conversation figure will likely involve ai time ' ve try use ai wind copy paste code not understand ' m try avoid go route help welcome appreciated context decide start project would essentially guess predetermine word base character user say not computer guess",
         "determine popular english dictionary within dictionary",
         "9",
         "determine,popular,english,determine popular,dictionary dictionary",
         "NLP Application"
        ],
        [
         "13",
         "79293889",
         "https://stackoverflow.com/questions/79293889",
         "catelog sentences into 5 words that represent them",
         "<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>\n<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>\n<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>\n<p>I will glad for recomendations how to do that</p>\n<p><strong>edit</strong></p>\n<p>represnt = the semantic distance between the word to the text.</p>\n<p>for example -\nlets say in row 1 the text is &quot;i want to eat&quot;\nand I have 2 words : food and house.</p>\n<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>\n",
         "2024-12-19 00:00:00",
         "python,pandas,nlp,text-mining,similarity",
         "0",
         "57",
         "1",
         "79294099.0",
         "<p>You could use a pre-trained sentence transformer model from <a href=\"https://pypi.org/project/sentence-transformers/\" rel=\"nofollow noreferrer\"><code>sentence_transformers</code></a>:</p>\n<pre><code>import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -&gt; None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -&gt; list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -&gt; pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838\n</code></pre>\n",
         "0.0",
         "11-100",
         "2024",
         "df['text']\n---\ndf[\"word1\"]\n---\ndf[\"word2\"]\n---\ndf[\"food \"]\n---\ndf[\"house\"]",
         "sentence_transformers\n---\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass SemanticSimilarityCalculator:\n  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None:\n    self.model = SentenceTransformer(model_name)\n    self.word_embeddings = None\n\n  def encode_words(self, words: list[str]) -> None:\n    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)\n    self.words = words\n\n  def calculate_similarity(self, text: str) -> list[float]:\n    if self.word_embeddings is None:\n      raise ValueError('Words must be encoded before calculating similarity.')\n    text_embedding = self.model.encode(text, convert_to_tensor=True)\n    similarities = util.cos_sim(text_embedding, self.word_embeddings)[\n      0\n    ].tolist()\n    return similarities\n\n  def add_similarity_scores_to_df(\n    self, df: pd.DataFrame, text_column: str\n  ) -> pd.DataFrame:\n    if self.words is None:\n      raise ValueError(\n        'Words must be encoded before adding scores to the DataFrame.'\n      )\n    similarity_columns = ['word_' + word for word in self.words]\n    df[similarity_columns] = df[text_column].apply(\n      lambda text: pd.Series(self.calculate_similarity(text))\n    )\n    return df\n\n\ndef main():\n  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}\n  df = pd.DataFrame(data)\n  words = ['food', 'house', 'sleep', 'drink', 'run']\n  calculator = SemanticSimilarityCalculator()\n  calculator.encode_words(words)\n  df_with_scores = calculator.add_similarity_scores_to_df(\n    df, text_column='text'\n  )\n  print(df_with_scores)\n\n\nif __name__ == '__main__':\n  main()\n---\ntext  word_food  word_house  word_sleep  word_drink  word_run\n0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350\n1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716\n2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838",
         "catelog sentence 5 word represent",
         "I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in",
         "You could use a pretrained sentence transformer model from Output",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in You could use a pretrained sentence transformer model from Output",
         "catelog sentence 5 word represent dataframe 1000 text row also 5 word want know one much represnt text 0 1 every score etc glad recomendation edit represnt = semantic distance word text example let say row 1 text want eat 2 word food house would high score could use pretraine sentence transformer model output",
         "catelog sentences into 5 words that represent them I have dataframe with 1000 text rows I also have 5 words that I want to know for each one of them how much they represnt the text between 0 to 1 every score will be in and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text for example lets say in row 1 the text is i want to eat and I have 2 words food and house so in it would be higher score than in",
         "catelog sentence 5 word represent dataframe 1000 text row also 5 word want know one much represnt text 0 1 every score etc glad recomendation edit represnt = semantic distance word text example let say row 1 text want eat 2 word food house would high score",
         "catelog 5 represent",
         "7",
         "",
         "Text Similarity"
        ],
        [
         "14",
         "79253283",
         "https://stackoverflow.com/questions/79253283",
         "Counting the Frequency of Some Words within some other Key Words in Text",
         "<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>\n<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>\n<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n</code></pre>\n<p>A small Example -</p>\n<pre><code>text_dict = {\n    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]\n}\n\ndf = pd.DataFrame(text_dict)\n</code></pre>\n<p>My expected outcome is -</p>\n<pre><code>                 ITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n</code></pre>\n<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>\n<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>\n",
         "2024-12-05 00:00:00",
         "python,pandas,nlp",
         "0",
         "83",
         "1",
         "79263000.0",
         "<p>You need to process each row of text by identifying occurrences of <code>key_words</code> and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique <code>search_word</code> found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique <code>search_words</code> near <code>key_words</code>.</p>\n<pre><code>import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        &quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process &quot;\n        &quot;of adopting Symantec. We believe these technologies will improve our access control. &quot;\n        &quot;Moreover, we also integrated data security diagnostic program.&quot;,\n        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = &quot; &quot;.join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n</code></pre>\n<p>Which returns</p>\n<pre><code>                                               ITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2\n</code></pre>\n",
         "0.0",
         "11-100",
         "2024",
         "search words\n---\nkey words\n---\nsearch words\n---\nkey words\n---\nkey words\n---\nsearch words\n---\nsearch word\n---\nkey word\n---\nsearch_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', \n 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', \n 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', \n 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n 'Securion', 'security event management', 'security information and event management', \n 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', \n 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']\n\nkey_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', \n 'integrate', 'invest', 'lease',\n 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',\n  'upgrade', 'use']\n---\ntext_dict = {\n    'ITEM7':[\"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \\\n    of adopting Symantec. We believe these technologies will improve our access control. \\\n        Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"]\n}\n\ndf = pd.DataFrame(text_dict)\n---\nITEM7                          Frequency\nLast year, from AVG we have acquired Alibaba S...   6\nWe are planning to install end-point security,...   2\n---\ndf\n---\nAVG\n---\nAlibaba Security\n---\nsearch_words\n---\nkey_words\n---\nSymantec\n---\nAccess Control\n---\ndata security\n---\ndiagnostic program\n---\nsearch_words\n---\nadopting\n---\nimprove\n---\nintegrated\n---\nkey_words\n---\nFrequency\n---\ndf\n---\nkey_words",
         "key_words\n---\nsearch_word\n---\nsearch_words\n---\nkey_words\n---\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\ntext_dict = {\n    'ITEM7': [\n        \"Last year, from AVG we have acquired Alibaba Security. This year we are in the process \"\n        \"of adopting Symantec. We believe these technologies will improve our access control. \"\n        \"Moreover, we also integrated data security diagnostic program.\",\n        \"We are planning to install end-point security, which will upgrade intrusion detection system.\"\n    ]\n}\ndf = pd.DataFrame(text_dict)\n\nsearch_words = [\n    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',\n    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',\n    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',\n    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',\n    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',\n    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',\n    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',\n    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',\n    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',\n    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',\n    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',\n    'Securion', 'security event management', 'security information and event management',\n    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',\n    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',\n    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'\n]\n\nkey_words = [\n    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',\n    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',\n    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',\n    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',\n    'purchase', 'replace', 'spend', 'upgrade', 'use'\n]\n\ndef preprocess_text_no_lemmatization(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n    return tokens\n\ndef calculate_final_frequency(row, search_phrases, key_phrases):\n    text = row.lower()\n    tokens = preprocess_text_no_lemmatization(text) \n    search_phrases = [phrase.lower() for phrase in search_phrases]  \n    key_phrases = [phrase.lower() for phrase in key_phrases] \n\n    all_matches = set()\n    token_len = len(tokens)\n    \n    for idx, token in enumerate(tokens):\n        if any(token.startswith(key) for key in key_phrases):  \n            window_start = max(0, idx - 10)\n            window_end = min(token_len, idx + 10 + 1)\n            window_tokens = tokens[window_start:window_end]\n            window_text = \" \".join(window_tokens)  \n\n            for phrase in search_phrases:\n                if phrase in window_text:\n                    all_matches.add(phrase)  \n    return len(all_matches)\n\ndf['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))\n\nprint(df)\n---\nITEM7  Frequency\n0  Last year, from AVG we have acquired Alibaba S...          6\n1  We are planning to install end-point security,...          2",
         "count frequency word within key word text",
         "I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also",
         "You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also You need to process each row of text by identifying occurrences of and capturing a 10word window around them Within this window you need to check for multiword search_words ensuring they are matched as phrases Each unique found within these windows needs to be counted avoiding doublecounting across the row Stored the results as a frequency count for each row accurately reflecting the number of unique near Which returns",
         "count frequency word within key word text two set word list first one call second one call goal calculate frequency within 10 word example assume word acquire list look word list within 10 word acquire within 10 word mean 10 word forward key word 10 word backward key word mean forward backward movement list small example expect outcome first row see word list around word acquire base form acquire list similarly list word within 10 word list total search word 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note word basically base form variation like adopt adopt count key word also need process row text identify occurrence capture 10word window around within window need check multiword search_words ensure match phrase unique find within window need count avoid doublecounte across row store result frequency count row accurately reflect number unique near return",
         "Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists first one I called and the second one I called My goal is to calculate the frequency of within 10 words of For example assume that the word acquire is in list then I will look for the words in list within 10 words of acquire Within 10 words mean 10 words forward from key words and 10 words backward from key words meaning that both forward and backward movement Below is my and lists A small Example My expected outcome is For the first row in we see the word and are from list and around the word acquired the base form of which acquire is in the list Similarly are from list and these words are within 10 words of from list So total search words are 6 AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program Therefore in the column of the value is 6 Please note that the words in are in basically base form so their variation like adopted adopting should be counted as key words also",
         "count frequency word within key word text two set word list first one call second one call goal calculate frequency within 10 word example assume word acquire list look word list within 10 word acquire within 10 word mean 10 word forward key word 10 word backward key word mean forward backward movement list small example expect outcome first row see word list around word acquire base form acquire list similarly list word within 10 word list total search word 6 avg+alibaba security+symantec+access control+data security+diagnostic program therefore column value 6 please note word basically base form variation like adopt adopt count key word also",
         "count frequency within key",
         "7",
         "key,count,frequency,frequency key,count frequency",
         "Text Similarity"
        ],
        [
         "15",
         "79247672",
         "https://stackoverflow.com/questions/79247672",
         "Error in getting Captum text explanations for text classification",
         "<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>\n<pre><code>import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f&quot;Test Accuracy: {accuracy:.2f}&quot;)\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n<p>But I am getting the following error in running the above.</p>\n<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n</code></pre>\n",
         "2024-12-03 00:00:00",
         "machine-learning,pytorch,nlp,huggingface-transformers,text-classification",
         "2",
         "89",
         "1",
         "79248379.0",
         "<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>\n<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=\"https://captum.ai/tutorials/Bert_SQUAD_Interpret\" rel=\"nofollow noreferrer\">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>\n<p>Below please find snippet that works:</p>\n<pre><code>from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)\n    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)\n        print(influential_words)\n</code></pre>\n",
         "1.0",
         "11-100",
         "2024",
         "import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom captum.attr import IntegratedGradients\n\n# Loading data\ntrain_df = pd.read_csv('train_dataset.csv')\ntest_df = pd.read_csv('test_dataset.csv')\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(df, tokenizer, max_len=128):\n    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n    labels = torch.tensor(df['label'].values)\n    return inputs, labels\n\ntrain_inputs, train_labels = preprocess_data(train_df, tokenizer)\ntest_inputs, test_labels = preprocess_data(test_df, tokenizer)\n\n# DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Train for 3 epochs\n    for batch in train_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n\n# Evaluation\nmodel.eval()\ncorrect_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_predictions.extend(\n            (preds == labels).cpu().numpy().tolist()\n        )\naccuracy = accuracy_score(test_labels.numpy(), correct_predictions)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n# Integrated Gradients\nig = IntegratedGradients(model)\n\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor\n\n    print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n    # forward function for IG\n    def forward_func(input_ids):\n        outputs = model(input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n    # Applying Integrated Gradients\n    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\n# Analysing influential words for correctly predicted texts\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)\n---\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 loss: 0.4719192385673523\nEpoch 2 loss: 0.39585667848587036\nEpoch 3 loss: 0.14659778773784637\nTest Accuracy: 0.70\nInput IDs shape: torch.Size([1, 8]) dtype: torch.int64\nAttention mask shape: torch.Size([1, 8]) dtype: torch.int64\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-f047b509c98d> in <cell line: 90>()\n     90 for idx, correct in enumerate(correct_predictions):\n     91     if correct:\n---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n     93         print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n     94         print(influential_words)\n\n18 frames\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2549         # remove once script supports set_grad_enabled\n   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2552 \n   2553 \n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
         "from captum.attr import LayerIntegratedGradients\n\n\ndef custom_forward(inputs):\n    preds = predict(inputs)\n    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)\nlig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\ndef get_influential_words(input_text, model, tokenizer, ig, device):\n    model.eval()\n    # Tokenizing the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    # print(\"Input IDs shape:\", input_ids.shape, \"dtype:\", input_ids.dtype)\n    # print(\"Attention mask shape:\", attention_mask.shape, \"dtype:\", attention_mask.dtype)\n\n    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()\n\n    return list(zip(tokens, token_importances))\n\nresults = []\n\nfor idx, correct in enumerate(correct_predictions):\n    if correct:\n        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)\n        print(f\"Influential words for text: {test_df['text'].iloc[idx]}\")\n        print(influential_words)",
         "error get captum text explanation text classification",
         "I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above",
         "You need to slightly change the gradients calculation class Also you didn't include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above You need to slightly change the gradients calculation class Also you didn't include forward_func into the gradients class constructor so the attribute method was not able to launch the stuff properly I think that using LayerIntegratedGradients is better for debugging BERT in line with this tutorial Below please find snippet that works",
         "error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset get follow error run need slightly change gradient calculation class also not include forward_func gradient class constructor attribute method able launch stuff properly think use layerintegratedgradient well debug bert line tutorial please find snippet work",
         "Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset But I am getting the following error in running the above",
         "error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset get follow error run",
         "error get captum explanation classification",
         "2",
         "explanation,classification,error,explanation classification,error captum",
         "Handling Error in NLP Task"
        ],
        [
         "16",
         "79247594",
         "https://stackoverflow.com/questions/79247594",
         "euclidian distance from word to sentence after doing Vectorizer",
         "<p>I have dataframe with 1000 text rows.</p>\n<p>I did TfidfVectorizer.</p>\n<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>\n<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>\n<p>I will glad to know how to do that or to hear about another method.</p>\n",
         "2024-12-03 00:00:00",
         "pandas,dataframe,nlp,text-classification,tf-idf",
         "1",
         "46",
         "1",
         "79248087.0",
         "<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([&quot;king&quot;]).toarray()\nexcept KeyError:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n</code></pre>\n<p>which would give you</p>\n<pre><code>                                                text  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0  \n</code></pre>\n<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        &quot;The king sat on the throne with wisdom.&quot;,\n        &quot;A queen ruled the kingdom alongside the king.&quot;,\n        &quot;Knights were loyal to their king.&quot;,\n        &quot;The empire prospered under the rule of a wise monarch.&quot;\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif &quot;king&quot; in feature_names:\n    king_index = np.where(feature_names == &quot;king&quot;)[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(&quot;The word 'king' is not in the vocabulary.&quot;)\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n\n</code></pre>\n<p>which gives</p>\n<pre><code>                                                text  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214\n</code></pre>\n",
         "1.0",
         "11-100",
         "2024",
         "",
         "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\ntry:\n    king_vector = tfidf.transform([\"king\"]).toarray()\nexcept KeyError:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros((1, tfidf_matrix.shape[1]))\n\nsimilarities = cosine_similarity(tfidf_matrix, king_vector).flatten()\n\nfeature_names = np.array(tfidf.get_feature_names_out())\n\ndef get_top_n_words(row_vector, top_n=5):\n    indices = row_vector.argsort()[::-1][:top_n]\n    return feature_names[indices]\n\naverages = []\nfor i in range(tfidf_matrix.shape[0]):\n    sentence_vector = tfidf_matrix[i].toarray().flatten()\n    top_words = get_top_n_words(sentence_vector)\n    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]\n    averages.append(np.mean(top_similarities))\n\ndf['king_similarity'] = similarities\ndf['avg_closest_similarity'] = averages\n\nprint(df)\n---\ntext  king_similarity  \\\n0            The king sat on the throne with wisdom.         0.240614   \n1      A queen ruled the kingdom alongside the king.         0.259779   \n2                  Knights were loyal to their king.         0.274487   \n3  The empire prospered under the rule of a wise ...         0.000000   \n\n   avg_closest_similarity  \n0                     0.0  \n1                     0.0  \n2                     0.0  \n3                     0.0\n---\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndata = {\n    'text': [\n        \"The king sat on the throne with wisdom.\",\n        \"A queen ruled the kingdom alongside the king.\",\n        \"Knights were loyal to their king.\",\n        \"The empire prospered under the rule of a wise monarch.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['text']).toarray()\n\nfeature_names = tfidf.get_feature_names_out()\nif \"king\" in feature_names:\n    king_index = np.where(feature_names == \"king\")[0][0]\n    king_vector = np.zeros_like(tfidf_matrix[0])\n    king_vector[king_index] = 1\nelse:\n    print(\"The word 'king' is not in the vocabulary.\")\n    king_vector = np.zeros_like(tfidf_matrix[0])\n\ndf['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]\n\nprint(df)\n---\ntext  king_distance\n0            The king sat on the throne with wisdom.       1.232385\n1      A queen ruled the kingdom alongside the king.       1.216734\n2                  Knights were loyal to their king.       1.204586\n3  The empire prospered under the rule of a wise ...       1.414214",
         "euclidian distance word sentence vectorizer",
         "I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king df'king' I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method",
         "I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king df'king' I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method I am not convinced that the Euclidean distance would be the optimal measure I would actually look at similarity scores which would give you That being said if you want to focus on Euclidean distance here is a method which gives",
         "euclidian distance word sentence vectorizer dataframe 1000 text row tfidfvectorizer want create new field give distance sentence word want let say word king df'king ' thought take sentence 5 closet word word king make average glad know hear another method convince euclidean distance would optimal measure would actually look similarity score would give say want focus euclidean distance method give",
         "euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows I did TfidfVectorizer Now I want to create a new field which give me the distance from each sentence to the word that i want lets say the word king df'king' I thought about taking in each sentence the 5 closet words to the word king and make average of them I will glad to know how to do that or to hear about another method",
         "euclidian distance word sentence vectorizer dataframe 1000 text row tfidfvectorizer want create new field give distance sentence word want let say word king df'king ' thought take sentence 5 closet word word king make average glad know hear another method",
         "euclidian distance vectorizer",
         "6",
         "distance,euclidian,vectorizer,euclidian distance,distance vectorizer",
         "BERT & Hugging Face Application"
        ],
        [
         "17",
         "79234004",
         "https://stackoverflow.com/questions/79234004",
         "Llama-3.2-1B-Instruct generate inconsistent output",
         "<p>I want to use <code>Llama-3.2-1B-Instruct</code> model, and although I have set <code>&quot;temperature&quot;: 0.0, &quot;top_p&quot;:0.0 and &quot;top_k&quot;:0</code>, it still generates inconsistent output. This is how my pipeline looks like:</p>\n<pre><code>pipe = pipeline(\n    &quot;text-generation&quot;,\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=&quot;mps&quot;,\n        model_kwargs={&quot;temperature&quot;: 0.0,\n                  &quot;do_sample&quot;:True,\n                              &quot;top_p&quot;:0.0,\n                              &quot;top_k&quot;:0,},\n)\n</code></pre>\n<p>Any idea how to solve this issue?</p>\n",
         "2024-11-28 00:00:00",
         "python,nlp,huggingface-transformers,large-language-model",
         "1",
         "686",
         "2",
         "79246602.0",
         "<p>The model inconsistent output can be due to two main factors:</p>\n<p><strong>1. Temperature:</strong></p>\n<p>setting temperature to zero give more inconsistent result. You can refer <a href=\"https://community.openai.com/t/why-the-api-output-is-inconsistent-even-after-the-temperature-is-set-to-0/329541/2\" rel=\"nofollow noreferrer\">Opeani discussion page</a> for detail.</p>\n<p>So the best option is to set temperature to very low values such as 0.00001 instead of zero.</p>\n<p><strong>2. do_sample</strong></p>\n<p>You already set it false, and it should remain that way only.</p>\n",
         "1.0",
         "101-1k",
         "2024",
         "Llama-3.2-1B-Instruct\n---\n\"temperature\": 0.0, \"top_p\":0.0 and \"top_k\":0\n---\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"mps\",\n        model_kwargs={\"temperature\": 0.0,\n                  \"do_sample\":True,\n                              \"top_p\":0.0,\n                              \"top_k\":0,},\n)",
         "",
         "llama321binstruct generate inconsistent output",
         "I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue",
         "The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue The model inconsistent output can be due to two main factors 1 Temperature setting temperature to zero give more inconsistent result You can refer Opeani discussion page for detail So the best option is to set temperature to low values such as 000001 instead of zero 2 do_sample You already set it false and it should remain that way only",
         "llama321binstruct generate inconsistent output want use model although set still generate inconsistent output pipeline look like idea solve issue model inconsistent output due two main factor 1 temperature set temperature zero give inconsistent result refer opeani discussion page detail good option set temperature low value 000001 instead zero 2 do_sample already set false remain way",
         "Llama321BInstruct generate inconsistent output I want to use model and although I have set it still generates inconsistent output This is how my pipeline looks like Any idea how to solve this issue",
         "llama321binstruct generate inconsistent output want use model although set still generate inconsistent output pipeline look like idea solve issue",
         "llama321binstruct generate inconsistent",
         "2",
         "generate,inconsistent,generate inconsistent,llama321binstruct,llama321binstruct generate",
         "Handling Error in NLP Task"
        ],
        [
         "18",
         "79192130",
         "https://stackoverflow.com/questions/79192130",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>\n<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>\n",
         "2024-11-15 00:00:00",
         "python,amazon-web-services,aws-lambda,nlp,large-language-model",
         "1",
         "60",
         "2",
         "79192427.0",
         "<p>In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:</p>\n<ol>\n<li>Package your KeyBERT script in a lambda and easily deploy it with a container.</li>\n<li>Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.</li>\n</ol>\n",
         "1.0",
         "11-100",
         "2024",
         "",
         "",
         "use aws service execute python script extract keyword text use keybert",
         "I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I don't want to integrate it into my other projects as they don't use python I've attempted to use lambda but I'm concerned about the potential cost of running this Thanks",
         "In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model I'm using is resourceintensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution So for your case here's what I'd do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I don't want to integrate it into my other projects as they don't use python I've attempted to use lambda but I'm concerned about the potential cost of running this Thanks In such cases I would normally think of two resources aligned with the best practices of AWS and software engineering SageMaker or Lambda If the model I'm using is resourceintensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution So for your case here's what I'd do Package your KeyBERT script in a lambda and easily deploy it with a container Invoke it whenever you need to process text blocks AWS Lambda charges you only for the execution time so its costefficient for occasional tasks",
         "use aws service execute python script extract keyword text use keybert simple python script give two block text extract keyword use keybert compare list keyword sort two list depend list share keyword aws service would well fit need want able esentially spin need give block text execute return result not want integrate project not use python ' ve attempt use lambda ' m concerned potential cost run thank case would normally think two resource align good practice aws software engineering sagemaker lambda model ' m use resourceintensive require gpu acceleration would go sagemaker otherwise lambda good solution case 's would package keybert script lambda easily deploy container invoke whenever need process text block aws lambda charge execution time costefficient occasional task",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT I have a simple python script that is given two blocks of text it then extracts the keywords from them using keyBERT and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords Which AWS service would best fit my needs I want to be able to esentially spin this up when needed give it the blocks of text and then execute it and return the results but I don't want to integrate it into my other projects as they don't use python I've attempted to use lambda but I'm concerned about the potential cost of running this Thanks",
         "use aws service execute python script extract keyword text use keybert simple python script give two block text extract keyword use keybert compare list keyword sort two list depend list share keyword aws service would well fit need want able esentially spin need give block text execute return result not want integrate project not use python ' ve attempt use lambda ' m concerned potential cost run thank",
         "aws service execute python script extract keyword keybert",
         "4",
         "aws,python script,keybert,extract keyword,service execute",
         "Preprocessing Text in Python"
        ],
        [
         "19",
         "79178041",
         "https://stackoverflow.com/questions/79178041",
         "Normalization of token embeddings in BERT encoder blocks",
         "<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>\n",
         "2024-11-11 00:00:00",
         "nlp,normalization,bert-language-model,attention-model",
         "2",
         "177",
         "2",
         "79238393.0",
         "<p>I tracked down full details of layer normalization (LN) in BERT <a href=\"https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para\">here</a>.</p>\n<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>\n",
         "0.0",
         "101-1k",
         "2024",
         "",
         "",
         "normalization token embedding bert encoder block",
         "Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings",
         "I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token it's per embedding dimension",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings I tracked down full details of layer normalization LN in BERT here Mean and variance are computed per token But the weight and bias parameters learned in LN are not per token it's per embedding dimension",
         "normalization token embedding bert encoder block follow multiheade attention layer bert encoder block layer normalization done separately embed token ie one mean variance per token embed concatenate vector token embedding mean variance embedding track full detail layer normalization ln bert mean variance compute per token weight bias parameter learn ln per token 's per embed dimension",
         "Normalization of token embeddings in BERT encoder blocks Following the multiheaded attention layer in a BERT encoder block is layer normalization done separately on the embedding of each token ie one mean and variance per token embedding or on the concatenated vector of all token embeddings the same mean and variance for all embeddings",
         "normalization token embedding bert encoder block follow multiheade attention layer bert encoder block layer normalization done separately embed token ie one mean variance per token embed concatenate vector token embedding mean variance embedding",
         "normalization token embedding bert encoder block",
         "0",
         "token,embedding,normalization,bert,encoder block",
         "Tokenising Text"
        ],
        [
         "20",
         "79173053",
         "https://stackoverflow.com/questions/79173053",
         "How to convert character indices to BERT token indices",
         "<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>\n<pre><code>from datasets import load_dataset\nds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\n</code></pre>\n<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>\n<pre><code>d0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n</code></pre>\n<p>After tokenization, the answer indices are 56  and 16:</p>\n<pre><code>from transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'\n</code></pre>\n<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>\n<p>This is from a <a href=\"https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false\" rel=\"nofollow noreferrer\">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=\"https://i.sstatic.net/GsZ6mfcQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/GsZ6mfcQ.png\" alt=\"QA dataset with token answer indices\" /></a></p>\n",
         "2024-11-09 00:00:00",
         "python,nlp,dataset,large-language-model,bert-language-model",
         "2",
         "37",
         "1",
         "79175157.0",
         "<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>\n<p>The following function does the above for you:</p>\n<pre><code>def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start &lt;= char_start &lt; end: \n            start_token_idx = i\n        if start &lt; char_end &lt;= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n</code></pre>\n<p>Here's how you can use and test this function:</p>\n<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])\nprint(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(&quot;Tokenized answer:&quot;, answer_tokens)\n</code></pre>\n<p>Output:</p>\n<pre><code>Tokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream\n</code></pre>\n",
         "2.0",
         "11-100",
         "2024",
         "UCLNLP/adversarial_qa\n---\nfrom datasets import load_dataset\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\n---\nd0 = ds['train'][0]\nd0\n\n{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',\n 'title': 'Brain',\n 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',\n 'question': 'What sare the benifts of the blood brain barrir?',\n 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},\n 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}\n---\nfrom transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\nbert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])\n'isolated from the bloodstream'",
         "def get_token_indices(example):\n    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token\n    encoded = tokenizer(\n        example['question'], \n        example['context'], \n        return_offsets_mapping=True\n    )\n\n    # Find character start and end from the original answer\n    char_start = example['answers']['answer_start'][0]\n    char_end = char_start + len(example['answers']['text'][0])\n\n    # Identify token indices for the answer\n    start_token_idx = None\n    end_token_idx = None\n    \n    for i, (start, end) in enumerate(encoded['offset_mapping']):\n        if start <= char_start < end: \n            start_token_idx = i\n        if start < char_end <= end:\n            end_token_idx = i\n            break\n\n    example['answer_start_token_idx'] = start_token_idx\n    example['answer_end_token_idx'] = end_token_idx\n    return example\n---\nds = load_dataset(\"UCLNLP/adversarial_qa\", \"adversarialQA\")\ntokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n\ntokenized_ds = ds['train'].map(get_token_indices)\n\n\n# Example\nd0_tokenized = tokenized_ds[0]\nprint(\"Tokenized start index:\", d0_tokenized['answer_start_token_idx'])\nprint(\"Tokenized end index:\", d0_tokenized['answer_end_token_idx'])\n\nanswer_tokens = tokenizer.decode(\n    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]\n)\nprint(\"Tokenized answer:\", answer_tokens)\n---\nTokenized start index: 56\nTokenized end index: 60\nTokenized answer: isolated from the bloodstream",
         "convert character indices bert token index",
         "I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Here's an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answer's token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result",
         "You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Here's how you can use and test this function Output",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Here's an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answer's token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result You should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices The following function does the above for you Here's how you can use and test this function Output",
         "convert character index bert token indices work questionanswer dataset map characterbase answer index tokenbase index tokenize context question together use tokenizer like bert 's example row dataset tokenization answer indice 56 16 want create new dataset answer 's token index eg 56 ad 60 linkedin learn class instructor conversion create csv file share code expect result encode question context locate token span answer within tokenized context update dataset tokenlevel index follow function 's use test function output",
         "How to convert character indices to BERT token indices I am working with a questionanswer dataset How do I map characterbased answer indices to tokenbased indices after tokenizing the context and question together using a tokenizer like BERT Here's an example row from my dataset After tokenization the answer indices are 56 and 16 I want to create a new dataset with the answer's token indices eg 56 ad 60 This is from a linkedin learning class The instructor did the conversion and created the csv file but he did not share it or the code to do that This is the expected result",
         "convert character index bert token indices work questionanswer dataset map characterbase answer index tokenbase index tokenize context question together use tokenizer like bert 's example row dataset tokenization answer indice 56 16 want create new dataset answer 's token index eg 56 ad 60 linkedin learn class instructor conversion create csv file share code expect result",
         "convert character indices bert token index",
         "0",
         "token,convert,bert,index,character indices",
         "Tokenising Text"
        ],
        [
         "21",
         "79159805",
         "https://stackoverflow.com/questions/79159805",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage?",
         "<p>I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage.</p>\n<p>I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing.</p>\n<ul>\n<li><p>Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process?</p>\n</li>\n<li><p>Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes?</p>\n</li>\n<li><p>If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects?</p>\n</li>\n</ul>\n<p>Any suggestions or examples would be greatly appreciated! Thank you!</p>\n",
         "2024-11-05 00:00:00",
         "nlp,multiprocessing,python-multiprocessing,spacy",
         "3",
         "96",
         "2",
         "79162232.0",
         "<p>I would strongly advise you not to treat NLP models like any other Python object. I would always prefer to load an NLP model using a microservice approach, which is more aligned with ML/software engineering best practices by separating the model logic from the main application.</p>\n<p>Instead of loading the model in each process (which can be memory-intensive), the model is loaded just once in a dedicated service. This setup allows the model to be used by multiple parts of the application without duplicating memory usage, making it efficient, modular, and scalable. Not only is your concern about memory efficiency addressed, but scalability and modularity are also improved.</p>\n<p>An example of implementing such a microservice using FastAPI + Docker could look like this:</p>\n<pre><code># main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(&quot;en_core_web_lg&quot;)  # Load model once\n\n@app.post(&quot;/process/&quot;)\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {&quot;tokens&quot;: [(token.text, token.pos_) for token in doc]}\n</code></pre>\n<p>To containerize above FastAPI service:</p>\n<pre><code># Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt &amp;&amp; python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [&quot;gunicorn&quot;, &quot;-w&quot;, &quot;4&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;main:app&quot;]\n</code></pre>\n",
         "3.0",
         "11-100",
         "2024",
         "",
         "# main.py: FastAPI service with spaCy model\nfrom fastapi import FastAPI\nimport spacy\n\napp = FastAPI()\nnlp = spacy.load(\"en_core_web_lg\")  # Load model once\n\n@app.post(\"/process/\")\nasync def process_text(text: str):\n    doc = nlp(text)\n    return {\"tokens\": [(token.text, token.pos_) for token in doc]}\n---\n# Dockerfile for the NLP model microservice\nFROM python:3.9-slim\nCOPY requirements.txt .\nRUN pip install -r requirements.txt && python -m spacy download en_core_web_lg\nCOPY . /app\nWORKDIR /app\nCMD [\"gunicorn\", \"-w\", \"4\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"main:app\"]",
         "share complex spacy nlp model across multiple python process minimize memory usage",
         "I'm working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPI's MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you",
         "I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage I'm working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPI's MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you I would advise you not to treat NLP models like any other Python object I would always prefer to load an NLP model using a microservice approach which is more aligned with ML/software engineering best practices by separating the model logic from the main application Instead of loading the model in each process which can be memoryintensive the model is loaded just once in a dedicated service This setup allows the model to be used by multiple parts of the application without duplicating memory usage making it efficient modular and scalable Not only is your concern about memory efficiency addressed but scalability and modularity are also improved An example of implementing such a microservice using FastAPI + Docker could look like this To containerize above FastAPI service",
         "share complex spacy nlp model across multiple python process minimize memory usage ' m work multiprocesse python application multiple process need access large preloade spacy nlp model eg en_core_web_lg since model memoryintensive want avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage look multiprocessingmanager multiprocessingshared_memory approach seem well suited numpy array raw datum buffer simple object complex object internal reference like nlp model also look mpi 's mpiwinallocate_shared ran issue use redis server make rank 0 processing work mpi since process do single rank defeat propose use multiprocesse efficient way share spacy model instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp model memory across process multiprocessingmanager shared_memory viable way improve performance reduce memory overhead work complex object suggestion example would greatly appreciate thank would advise treat nlp model like python object would always prefer load nlp model use microservice approach align ml / software engineering good practice separate model logic main application instead load model process memoryintensive model load dedicated service setup allow model use multiple part application without duplicate memory usage make efficient modular scalable concern memory efficiency address scalability modularity also improve example implement microservice use fastapi + docker could look like containerize fastapi service",
         "How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage I'm working on a multiprocessing python application where multiple processes need access to a large preloaded spaCy NLP model eg en_core_web_lg Since the model is memoryintensive I want to avoid loading it separately in each process since I quickly run out of main memory and the object is readonly Instead Id like to load it once in a shared location so that all processes can read from it without duplicating memory usage I have looked into multiprocessingManager and multiprocessingshared_memory but these approaches seem better suited to NumPy arrays raw data buffers or simple objects not complex objects with internal references like an NLP model I have also looked into MPI's MPIWinAllocate_shared but I ran into the same issues Using a redis server and make rank 0 do all the processing works with MPI but since all the processing is done by a single rank it defeats the propose I had for using multiprocessing Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process Are there libraries or techniques specifically suited for sharing complex readonly objects like NLP models in memory across processes If multiprocessingManager or shared_memory is viable here are there ways to improve performance or reduce memory overhead when working with complex objects Any suggestions or examples would be greatly appreciated Thank you",
         "share complex spacy nlp model across multiple python process minimize memory usage ' m work multiprocesse python application multiple process need access large preloade spacy nlp model eg en_core_web_lg since model memoryintensive want avoid load separately process since quickly run main memory object readonly instead I d like load share location process read without duplicate memory usage look multiprocessingmanager multiprocessingshared_memory approach seem well suited numpy array raw datum buffer simple object complex object internal reference like nlp model also look mpi 's mpiwinallocate_shared ran issue use redis server make rank 0 processing work mpi since process do single rank defeat propose use multiprocesse efficient way share spacy model instance across multiple process python avoid reloading process library technique specifically suited sharing complex readonly object like nlp model memory across process multiprocessingmanager shared_memory viable way improve performance reduce memory overhead work complex object suggestion example would greatly appreciate thank",
         "share complex spacy nlp across multiple python process minimize memory usage",
         "5",
         "minimize memory,nlp,complex spacy,python process,multiple python",
         "Using Spacy Library"
        ],
        [
         "22",
         "79155290",
         "https://stackoverflow.com/questions/79155290",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing",
         "<p>When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data.</p>\n<p><a href=\"https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment\" rel=\"nofollow noreferrer\">https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment</a></p>\n<p>There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative.</p>\n<p><strong>Is there a way to get neutral labels for such examples in RobBERTje?</strong></p>\n<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = &quot;DTAI-KULeuven/robbert-v2-dutch-sentiment&quot;\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n</code></pre>\n<p>Output:</p>\n<pre><code>[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]\n</code></pre>\n",
         "2024-11-04 00:00:00",
         "python,nlp,bert-language-model,roberta-language-model",
         "2",
         "57",
         "1",
         "79155380.0",
         "<p>This model was trained only on <code>negative</code> and <code>positive</code> labels. Therefore, it will try to categorize every input as positive or negative, even if it is nonsensical or neutral.</p>\n<p>what you can do is to:\n1- Find other models that was trained to include <code>neutral</code> label.\n2- Fine-tune this model on a dataset that includes <code>neutral</code> label.\n3- Empirically define a threshold based on the confidence outputs and interpret it as <code>neutral</code>.</p>\n<p>The first 2 choices are extensive in effort. I would suggest you go with the third option for a quick workaround. Try feeding the model with a few neutral input and observe the range of confidence score in the output. then use that threshold to classify as <code>neutral</code>.</p>\n<p>Here's a sample:</p>\n<pre><code>def classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] &lt; threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result\n</code></pre>\n",
         "3.0",
         "11-100",
         "2024",
         "from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\nimport torch\n\nmodel_name = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)\n\nresult1 = classifier('Fhdf')\nresult2 = classifier('Als gisteren inclusief blauw')\nprint(result1)\nprint(result2)\n---\n[{'label': 'Positive', 'score': 0.7520257234573364}]\n[{'label': 'Negative', 'score': 0.7538396120071411}]",
         "negative\n---\npositive\n---\nneutral\n---\nneutral\n---\nneutral\n---\nneutral\n---\ndef classify_with_neutral(text, threshold=0.5):\n    result = classifier(text)[0]  # Get the classification result\n    if result['score'] < threshold:\n        result['label'] = 'Neutral'  # Override label to 'Neutral'\n    return result",
         "dutch sentiment analysis robbertje outputs positive / negative label netural label miss",
         "When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg 'Fhdf' nonsense and 'Als gisteren inclusief blauw' neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output",
         "This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Here's a sample",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg 'Fhdf' nonsense and 'Als gisteren inclusief blauw' neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output This model was trained only on and labels Therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to 1 Find other models that was trained to include label 2 Finetune this model on a dataset that includes label 3 Empirically define a threshold based on the confidence outputs and interpret it as The first 2 choices are extensive in effort I would suggest you go with the third option for a quick workaround Try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as Here's a sample",
         "dutch sentiment analysis robbertje outputs positive / negative label netural label miss run dutch sentiment analysis robbertje outputs positive / negative label netural label miss datum obvious neutral sentence / word eg ' fhdf ' nonsense ' al gisteren inclusief blauw ' neutral evaluate positive negative way get neutral label example robbertje output model train label therefore try categorize every input positive negative even nonsensical neutral 1 find model train include label 2 finetune model dataset include label 3 empirically define threshold base confidence output interpret first 2 choice extensive effort would suggest go third option quick workaround try feed model neutral input observe range confidence score output use threshold classify 's sample",
         "Dutch sentiment analysis RobBERTje outputs just positive/negative labels netural label is missing When I run Dutch sentiment analysis RobBERTje it outputs just positive/negative labels netural label is missing in the data There are obvious neutral sentences/words eg 'Fhdf' nonsense and 'Als gisteren inclusief blauw' neutral but they both evaluate to positive or negative Is there a way to get neutral labels for such examples in RobBERTje Output",
         "dutch sentiment analysis robbertje outputs positive / negative label netural label miss run dutch sentiment analysis robbertje outputs positive / negative label netural label miss datum obvious neutral sentence / word eg ' fhdf ' nonsense ' al gisteren inclusief blauw ' neutral evaluate positive negative way get neutral label example robbertje output",
         "dutch sentiment analysis robbertje outputs positive negative label netural label miss",
         "9",
         "dutch,netural label,sentiment analysis,robbertje outputs,negative label",
         "NLP Application"
        ],
        [
         "23",
         "79148979",
         "https://stackoverflow.com/questions/79148979",
         "Finding Root Form of Verbs using Curiosity-AI/Catalyst",
         "<p>I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match <code>PartOfSpeech.VERB</code> but I don't know how to continue from there.</p>\n<p>This is what I have so far:</p>\n<pre><code>const string text = &quot;The disastrous cat runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}\n</code></pre>\n<p>Any help is greatly appreciated.</p>\n",
         "2024-11-01 00:00:00",
         "c#,nlp",
         "2",
         "142",
         "1",
         "79160163.0",
         "<p>The following code (targeting .NET 8.0) illustrates one method to obtain the root form of a verb from an inflected form.</p>\n<p>(I have annonoted, as code comments, the three NuGet packages (with versions) required. Most of the code is identical to your original sample above.)</p>\n<pre><code>//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = &quot;The disastrous cat quickly runs after the fat field mouse.&quot;;\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($&quot;Root of the verb '{token.Value}' is '{token.Lemma}'.&quot;);\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(&quot;Complete; press any key.&quot;);\nConsole.ReadKey();\n</code></pre>\n<p><strong>Note:</strong> For this specific sentence, I have added an adverb (&quot;quickly&quot;) before the verb (&quot;runs&quot;). Without this, the library incorrectly interprets &quot;runs&quot; as a noun. Depending on your source text, this might be an issue for you, but I believe it is separate from the question being asked.</p>\n",
         "1.0",
         "101-1k",
         "2024",
         "PartOfSpeech.VERB\n---\nconst string text = \"The disastrous cat runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\n\nforeach (var sentence in doc.TokensData)\n{\n    foreach (var token in sentence)\n    {\n        if(token.Tag == PartOfSpeech.VERB)\n        {\n            //  so here I'd like to the root form of the verb\n        }\n    }\n}",
         "//// Installed Curiosity.Library v24.10.52882\n//// Installed Catalyst v1.0.51118\n//// Installed Catalyst.Models.English v1.0.30952\n\nusing Catalyst;\n\nusing Mosaik.Core;\n\nconst string text = \"The disastrous cat quickly runs after the fat field mouse.\";\nCatalyst.Models.English.Register();\n\nStorage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);\nvar nlp = await Pipeline.ForAsync(Language.English);\nvar doc = new Document(text, Language.English);\nnlp.ProcessSingle(doc);\n\nforeach (var span in doc.Spans)\n{\n    foreach (var token in span.Tokens)\n    {\n        if (token.POS == PartOfSpeech.VERB)\n        {\n            Console.WriteLine($\"Root of the verb '{token.Value}' is '{token.Lemma}'.\");\n        }\n    }\n}\n\nConsole.WriteLine();\nConsole.WriteLine(\"Complete; press any key.\");\nConsole.ReadKey();",
         "find root form verb use curiosityai / catalyst",
         "I'm trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I don't know how to continue from there This is what I have so far Any help is greatly appreciated",
         "The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst I'm trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I don't know how to continue from there This is what I have so far Any help is greatly appreciated The following code targeting NET 80 illustrates one method to obtain the root form of a verb from an inflected form I have annonoted as code comments the three NuGet packages with versions required Most of the code is identical to your original sample above Note For this specific sentence I have added an adverb quickly before the verb runs Without this the library incorrectly interprets runs as a noun Depending on your source text this might be an issue for you but I believe it is separate from the question being asked",
         "find root form verb use curiosityai / catalyst ' m try find root form verb run text pipeline identify tokens match not know continue far help greatly appreciate follow code target net 80 illustrate one method obtain root form verb inflect form annonote code comment three nuget package version require code identical original sample note specific sentence add adverb quickly verb run without library incorrectly interpret run noun depend source text might issue believe separate question ask",
         "Finding Root Form of Verbs using CuriosityAI/Catalyst I'm trying to find the root form of a verb I run text through the pipeline and can identify all tokens which match but I don't know how to continue from there This is what I have so far Any help is greatly appreciated",
         "find root form verb use curiosityai / catalyst ' m try find root form verb run text pipeline identify tokens match not know continue far help greatly appreciate",
         "root form verb curiosityai catalyst",
         "7",
         "form,verb,curiosityai,catalyst,root form",
         "Text Similarity"
        ],
        [
         "24",
         "79145419",
         "https://stackoverflow.com/questions/79145419",
         "Is it possible to get embeddings from NV-Embed using Candle?",
         "<p>What I want to do is a CLI program that outputs embeddings of an arbitrary input.\nTo do that, I want to do an inference with an embeddings model, and I chose <code>NV-Embed-v2</code>. My framework of choice is <a href=\"https://github.com/huggingface/candle\" rel=\"nofollow noreferrer\">Candle</a>, but I also looked at <a href=\"https://github.com/EricLBuehler/mistral.rs\" rel=\"nofollow noreferrer\">Mistral-RS</a>.</p>\n<p>Basically, what I'm trying to do is this code fragment:\n<a href=\"https://huggingface.co/nvidia/NV-Embed-v2\" rel=\"nofollow noreferrer\">https://huggingface.co/nvidia/NV-Embed-v2</a>\nbut with Rust and Candle.</p>\n<p>What I tried is to start off with <a href=\"https://github.com/huggingface/candle/blob/main/candle-examples/examples/mistral/main.rs\" rel=\"nofollow noreferrer\">Mistral Candle's example</a> because the NV-Embed's HF page says: <code>Model Details / Base Decoder-only LLM: Mistral-7B-v0.1</code>.</p>\n<p>I replaced the model id in the original code with <code>nvidia/NV-Embed-v2</code>, and was able to download the weights from Hugging Face, but upon loading the config, I got this:</p>\n<pre><code>Error: missing field `vocab_size` at line 101 column 1\n</code></pre>\n<p>Then I hardcoded the values from the JSON config loaded from HF to a newly created <code>candle_transformers::models::mistral::Config</code> instance. And after that, <code>Mistral::new(&amp;config, vb)</code> fails with:</p>\n<pre><code>Error: cannot find tensor model.embed_tokens.weight\n</code></pre>\n<p>Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?</p>\n",
         "2024-10-31 00:00:00",
         "machine-learning,rust,nlp",
         "0",
         "335",
         "1",
         "79156470.0",
         "<p>candle looking for <code>model.embed_tokens.weight</code> whereas the original tensor name is <code>embedding_model.embed_tokens.weight</code>. You just have to change this line of <code>mistral.rs</code> in candle_transformers.</p>\n<pre class=\"lang-rust prettyprint-override\"><code>// from\nlet vb_m = vb.pp(&quot;model&quot;);\n//to\nlet vb_m = vb.pp(&quot;embedding_model&quot;);\n</code></pre>\n",
         "2.0",
         "101-1k",
         "2024",
         "NV-Embed-v2\n---\nModel Details / Base Decoder-only LLM: Mistral-7B-v0.1\n---\nnvidia/NV-Embed-v2\n---\nError: missing field `vocab_size` at line 101 column 1\n---\ncandle_transformers::models::mistral::Config\n---\nMistral::new(&config, vb)\n---\nError: cannot find tensor model.embed_tokens.weight",
         "model.embed_tokens.weight\n---\nembedding_model.embed_tokens.weight\n---\nmistral.rs\n---\n// from\nlet vb_m = vb.pp(\"model\");\n//to\nlet vb_m = vb.pp(\"embedding_model\");",
         "possible get embedding nvembe use candle",
         "What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what I'm trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candle's example because the NVEmbed's HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe that's a common mistake that could easily be diagnosed",
         "candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what I'm trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candle's example because the NVEmbed's HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe that's a common mistake that could easily be diagnosed candle looking for whereas the original tensor name is You just have to change this line of in candle_transformers",
         "possible get embedding nvembe use candle want cli program output embedding arbitrary input want inference embedding model choose framework choice candle also look mistralrs basically ' m try code fragment rust candle try start mistral candle 's example nvembed 's hf page say replace model i d original code able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail way around maybe candlebase open source work could use inspiration maybe 's common mistake could easily diagnose candle look whereas original tensor name change line candle_transformer",
         "Is it possible to get embeddings from NVEmbed using Candle What I want to do is a CLI program that outputs embeddings of an arbitrary input To do that I want to do an inference with an embeddings model and I chose My framework of choice is Candle but I also looked at MistralRS Basically what I'm trying to do is this code fragment but with Rust and Candle What I tried is to start off with Mistral Candle's example because the NVEmbed's HF page says I replaced the model id in the original code with and was able to download the weights from Hugging Face but upon loading the config I got this Then I hardcoded the values from the JSON config loaded from HF to a newly created instance And after that fails with Is there a way around that maybe there are some other Candlebased open source works that I could use as an inspiration Or maybe that's a common mistake that could easily be diagnosed",
         "possible get embedding nvembe use candle want cli program output embedding arbitrary input want inference embedding model choose framework choice candle also look mistralrs basically ' m try code fragment rust candle try start mistral candle 's example nvembed 's hf page say replace model i d original code able download weight hug face upon loading config get hardcode value json config load hf newly create instance fail way around maybe candlebase open source work could use inspiration maybe 's common mistake could easily diagnose",
         "possible get embedding nvembe candle",
         "6",
         "possible,embedding,possible embedding,nvembe,candle",
         "BERT & Hugging Face Application"
        ],
        [
         "25",
         "79111733",
         "https://stackoverflow.com/questions/79111733",
         "How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)",
         "<p>How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)</p>\n<p>I have short product descriptions that I’d like to transform into structured attributes.</p>\n<p>Example:</p>\n<p>Input:</p>\n<pre><code>“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n</code></pre>\n<p>Output:</p>\n<pre><code>Year = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml\n</code></pre>\n<p>If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:</p>\n<ol>\n<li><p>There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.</p>\n</li>\n<li><p>There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.</p>\n</li>\n<li><p>Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.</p>\n</li>\n<li><p>I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.</p>\n</li>\n</ol>\n<p>I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.</p>\n<p>I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.</p>\n<p>Appreciate any insight into approaches or suggested learning resources.</p>\n<p></p>\n<p>I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.</p>\n",
         "2024-10-21 00:00:00",
         "nlp,artificial-intelligence,large-language-model,named-entity-recognition",
         "0",
         "182",
         "1",
         "79113907.0",
         "<p>LLM would work nicely for this.  I'v done similar tasks before and it worked nicely with minimal training.  Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate,  but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions.</p>\n<p>For you task I would use a framework like Langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  When run with a model it will create an XML output which would be trivial to parse.  You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me.</p>\n<pre><code>You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like &quot;Red&quot; or &quot;White&quot; (e.g., &quot;Red Head Vineyard&quot;) which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., &quot;1500 ml&quot;) rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n&lt;Wine&gt;\n&lt;Year&gt;{{Year}}&lt;/Year&gt;\n&lt;Color&gt;{{Color}}&lt;/Color&gt;\n&lt;Weight&gt;{{Weight}}&lt;/Weight&gt;\n&lt;WeightUnit&gt;{{WeightUnit}}&lt;/WeightUnit&gt;\n&lt;Brand&gt;{{Brand}}&lt;/Brand&gt;\n&lt;GrapeVariety&gt;{{GrapeVariety}}&lt;/GrapeVariety&gt;\n&lt;/Wine&gt;\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   &lt;Wine&gt;\n     &lt;Year&gt;2017&lt;/Year&gt;\n     &lt;Color&gt;Red&lt;/Color&gt;\n     &lt;Weight&gt;750&lt;/Weight&gt;\n     &lt;WeightUnit&gt;ml&lt;/WeightUnit&gt;\n     &lt;Brand&gt;La Lecciaia&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Cabernet Sauvignon&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   &lt;Wine&gt;\n     &lt;Year&gt;2020&lt;/Year&gt;\n     &lt;Color&gt;&lt;/Color&gt;\n     &lt;Weight&gt;1.5&lt;/Weight&gt;\n     &lt;WeightUnit&gt;L&lt;/WeightUnit&gt;\n     &lt;Brand&gt;Red Head Vineyard&lt;/Brand&gt;\n     &lt;GrapeVariety&gt;Chardonnay&lt;/GrapeVariety&gt;\n   &lt;/Wine&gt;\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}\n</code></pre>\n<p>Keep in mind that LLMs are not cheap to run.  But for this tasks given ambiguousness of the domain it is most likely the best choice.  For this particular task it would be 1/1000 of a penny per label using OpenAI service.  You might find a cheaper model / provider.  However when working with LLM it is very important to ensure accuracy first,  then optimize for costs.</p>\n<p>The whole thing will probably take 1-2 hours to build for the intermediate LLM developer.  If you are learning it may vary.  But this is a perfect project to learn about LLMs</p>\n",
         "1.0",
         "101-1k",
         "2024",
         "“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”\n---\nYear = 2017\n\nColor = Red\n\nWeight = 750\n\nWeight Unit = ml",
         "You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:\n\n- **Year**: The vintage year of the wine.\n- **Color**: The color of the wine (e.g., Red, White, Rosé).\n- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).\n- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).\n- **Brand**: The brand or producer of the wine.\n- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).\n\n**Instructions:**\n\n- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.\n- Be cautious of potential ambiguities. For example:\n  - A brand name may include words like \"Red\" or \"White\" (e.g., \"Red Head Vineyard\") which should not be confused with the wine color.\n  - Large numbers may represent weight (e.g., \"1500 ml\") rather than a year.\n- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.\n\n**Output Format:**\n\nProvide the extracted information in XML format, using the following structure:\n\n<Wine>\n<Year>{{Year}}</Year>\n<Color>{{Color}}</Color>\n<Weight>{{Weight}}</Weight>\n<WeightUnit>{{WeightUnit}}</WeightUnit>\n<Brand>{{Brand}}</Brand>\n<GrapeVariety>{{GrapeVariety}}</GrapeVariety>\n</Wine>\n\n**Examples:**\n\n  1. **Input:**\n\n `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`\n\n **Output:**\n\n\n\n```xml\n   <Wine>\n     <Year>2017</Year>\n     <Color>Red</Color>\n     <Weight>750</Weight>\n     <WeightUnit>ml</WeightUnit>\n     <Brand>La Lecciaia</Brand>\n     <GrapeVariety>Cabernet Sauvignon</GrapeVariety>\n   </Wine>\n   ```\n\n   \n   `Red Head Vineyard Chardonnay 2020 1.5L`\n\n   **Output:**\n\n   <Wine>\n     <Year>2020</Year>\n     <Color></Color>\n     <Weight>1.5</Weight>\n     <WeightUnit>L</WeightUnit>\n     <Brand>Red Head Vineyard</Brand>\n     <GrapeVariety>Chardonnay</GrapeVariety>\n   </Wine>\n\n \n\n    **Task:**\n    \n    Given the following wine description, extract the components and provide the output in XML format as specified.\n    \n    {win_description}",
         "derive attribute / label short plain text description ner llm",
         "How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense",
         "LLM would work nicely for this I'v done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense LLM would work nicely for this I'v done similar tasks before and it worked nicely with minimal training Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions For you task I would use a framework like Langchain and the following prompt note you might need to work on your prompt a bit this just an example When run with a model it will create an XML output which would be trivial to parse You can modify the prompt to create different type of outputs But personally I find XML working well for me Keep in mind that LLMs are not cheap to run But for this tasks given ambiguousness of the domain it is most likely the best choice For this particular task it would be 1/1000 of a penny per label using OpenAI service You might find a cheaper model / provider However when working with LLM it is important to ensure accuracy first then optimize for costs The whole thing will probably take 12 hours to build for the intermediate LLM developer If you are learning it may vary But this is a perfect project to learn about LLMs",
         "derive attribute / label short plain text description ner llm derive attribute / label short plain text description ner llm short product description i d like transform structure attribute example input output everything format would trivial write regular expression do many different format nuance increasingly cumbersome hardcode logic format try create generic solution immediately run issue basic approach several different datum provider format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even give provider may multiple format may change time format always strictly follow many way express particular component example weight might express one 15l 1 1/2 liter 1500ml etc part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year etc I m use wine example sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work hard get right there s straightforward method would good know I d like develop generalpurpose function accept description format little experience nlp / artificial intelligence suspect useful tool / algo leverage 1000 + example record could potentially use train model something run locally would prefer necessary I m look specific implementation guidance anyone who s work similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource ' ve look online information many approach involve significant amount front work unclear will work practical sense llm would work nicely ' v do similar task work nicely minimal training keep mind statistical method nlp / llm / ner never 100 % accurate practical purpose find llm accurate custom soup regular expression task would use framework like langchain follow prompt note might need work prompt bit example run model create xml output would trivial parse modify prompt create different type output personally find xml work well keep mind llm cheap run task give ambiguousness domain likely good choice particular task would 1/1000 penny per label use openai service might find cheap model / provider however work llm important ensure accuracy first optimize cost whole thing probably take 12 hour build intermediate llm developer learning may vary perfect project learn llm",
         "How to derive attributes/labels from short plain text descriptions NER LLM How to derive attributes/labels from short plain text descriptions NER LLM I have short product descriptions that Id like to transform into structured attributes Example Input Output If everything was in this format it would be trivial to write a regular expression and be done with it but there are many different formats and nuances It is increasingly cumbersome to hardcode logic for each format Trying to create a generic solution I immediately run into issues with a basic approach There are several different data providers and each has its own format For the example above another provider might use Red 2017 La Lecciaia Cabernet Sauvignon 750 ML Even for a given provider there may be multiple formats and they may change over time Formats are not always strictly followed There are many ways of expressing particular components As an example Weight might be expressed as any one of these 15L 1 1/2 Liters 1500ml etc Parts of the description may be confused for target components There may be a white wine from a brand called Red Head Vineyard A weight of 2000 ml may be confused for a year etc Im only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues Id consider this more of a nice to have but would be useful to be able to parse out even more detail like the algo would be smart enough to know that La Lecciaia is the brand and Cabernet Sauvignon is the grape variety Assuming this would take more up front work and harder to get right but if theres a straightforward method of doing this would be good to know about Id like to develop a generalpurpose function that can accept a description from any format I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage I have 1000+ example records that I could potentially use to train a model Something that can run locally would be preferred but not necessary Im not looking for a specific implementation but for guidance from anyone whos worked on a similar problem Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies Appreciate any insight into approaches or suggested learning resources I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense",
         "derive attribute / label short plain text description ner llm derive attribute / label short plain text description ner llm short product description i d like transform structure attribute example input output everything format would trivial write regular expression do many different format nuance increasingly cumbersome hardcode logic format try create generic solution immediately run issue basic approach several different datum provider format example another provider might use red 2017 la lecciaia cabernet sauvignon 750 ml even give provider may multiple format may change time format always strictly follow many way express particular component example weight might express one 15l 1 1/2 liter 1500ml etc part description may confuse target component may white wine brand call red head vineyard weight 2000 ml may confused year etc I m use wine example sake simplicity general audience product domain conceptual issue i d consider nice would useful able parse even detail like algo would smart enough know la lecciaia brand cabernet sauvignon grape variety assuming would take front work hard get right there s straightforward method would good know I d like develop generalpurpose function accept description format little experience nlp / artificial intelligence suspect useful tool / algo leverage 1000 + example record could potentially use train model something run locally would prefer necessary I m look specific implementation guidance anyone who s work similar problem open hybrid approach additional logic manual oversight could account initial inaccuracy appreciate insight approach suggest learn resource ' ve look online information many approach involve significant amount front work unclear will work practical sense",
         "derive attribute label short plain description ner llm",
         "9",
         "description,ner llm,short plain,derive attribute,attribute label",
         "NLP Application"
        ],
        [
         "26",
         "79102797",
         "https://stackoverflow.com/questions/79102797",
         "Varying embedding dim due to changing padding in batch size",
         "<p>I want to train a simple neural network, which has <strong>embedding_dim</strong> as a parameter:</p>\n<pre><code>class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n</code></pre>\n<p>To load the data I used torchs DataLoader with a custom collate_fn.</p>\n<pre><code>train_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n</code></pre>\n<p>The collate_fn_padd function looks the following:</p>\n<pre><code>def collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels\n\n</code></pre>\n<p><strong>The problem:</strong> For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch).</p>\n<p>That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch.</p>\n<p>Due to that, I receive errors like that: <strong>mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64)</strong></p>\n<p>Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?</p>\n",
         "2024-10-18 00:00:00",
         "python,text,nlp,padding,data-preprocessing",
         "0",
         "48",
         "1",
         "79105117.0",
         "<p>You can add a maximum length argument set to <code>embedding_dim</code> to pad and truncate all the data to a fixed length:</p>\n<pre><code>padded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]\n</code></pre>\n",
         "1.0",
         "11-100",
         "2024",
         "class BoolQNN(nn.Module):\n    def __init__(self, embedding_dim):\n        super(BoolQNN, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, question_emb, passage_emb):\n        combined = torch.cat((question_emb, passage_emb), dim=1)\n        x = self.fc1(combined)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n---\ntrain_dataset = BoolQDataset(train_data, pretrained_embeddings)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)\n\nmodel = BoolQNN(301)\n---\ndef collate_fn_padd(batch):\n\n  questions, passages, labels = zip(*batch)\n\n  questions = [torch.tensor(q) for q in questions]\n  passages = [torch.tensor(p) for p in passages]\n\n  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)\n\n  labels = torch.tensor(labels, dtype=torch.float32)\n  \n  return padded_questions, padded_passages, labels",
         "embedding_dim\n---\npadded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]\npadded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]",
         "vary embed dim due change padding batch size",
         "I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size",
         "You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size You can add a maximum length argument set to to pad and truncate all the data to a fixed length",
         "vary embed dim due change padding batch size want train simple neural network embedding_dim parameter load datum use torchs dataloader custom collate_fn collate_fn_padd function look follow problem every batch want train model embed text get pad differently long take long sequence current batch mean embed dim / input size linear layer neural network change batch batch althoug want size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collate_fn_pad function padd sequence size independet batch size add maximum length argument set pad truncate datum fix length",
         "Varying embedding dim due to changing padding in batch size I want to train a simple neural network which has embedding_dim as a parameter To load the data I used torchs DataLoader with a custom collate_fn The collate_fn_padd function looks the following The problem For every batch I want to train my model with the embedded text gets padded differently long it takes the longest sequence of the current batch That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch althoug I want the size to be the same for every batch Due to that I receive errors like that mat1 and mat2 shapes cannot be multiplied 16x182 and 301x64 Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size independet of the batch size",
         "vary embed dim due change padding batch size want train simple neural network embedding_dim parameter load datum use torchs dataloader custom collate_fn collate_fn_padd function look follow problem every batch want train model embed text get pad differently long take long sequence current batch mean embed dim / input size linear layer neural network change batch batch althoug want size every batch due receive error like mat1 mat2 shape multiply 16x182 301x64 possible adjust collate_fn_pad function padd sequence size independet batch size",
         "vary embed dim due change padding batch size",
         "6",
         "vary,batch,change padding,batch size,embed dim",
         "BERT & Hugging Face Application"
        ],
        [
         "27",
         "79100835",
         "https://stackoverflow.com/questions/79100835",
         "How can I adjust the performance of tokenizer?",
         "<p>Working with the tokenizer from the <code>transformers</code> library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not.</p>\n<p>I'm wondering if I can <strong>&quot;adjust&quot;</strong> (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to.</p>\n<p>To be more specific, the type of tokenizer is <code>transformers.XLMRobertaTokenizerFast</code>, which is a unigram tokenizer, and the model is <code>paraphrase-multilingual-mpnet-base-v2</code>.</p>\n",
         "2024-10-18 00:00:00",
         "nlp,huggingface-transformers,huggingface,huggingface-tokenizers",
         "0",
         "49",
         "1",
         "79107575.0",
         "<p>You can change the tokenizer's vocabulary:</p>\n<pre><code>tokenizer.add_tokens([&quot;asadaf&quot;, &quot;sdfsaf&quot;])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = &quot;This is asadaf and sdfsaf&quot;\nprint(tokenizer(input_text))\n</code></pre>\n<p>As a result, <em>asadaf</em> and <em>sdfsaf</em> would be tokenized as unique words.</p>\n",
         "1.0",
         "11-100",
         "2024",
         "transformers\n---\ntransformers.XLMRobertaTokenizerFast\n---\nparaphrase-multilingual-mpnet-base-v2",
         "tokenizer.add_tokens([\"asadaf\", \"sdfsaf\"])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = \"This is asadaf and sdfsaf\"\nprint(tokenizer(input_text))",
         "adjust performance tokenizer",
         "Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not I'm wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is",
         "You can change the tokenizer's vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not I'm wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is You can change the tokenizer's vocabulary As a result asadaf and sdfsaf would be tokenized as unique words",
         "adjust performance tokenizer working tokenizer library hug face tokenizer work fine case case ' m wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case use specific type tokenizer unigram tokenizer model change tokenizer 's vocabulary result asadaf sdfsaf would tokenize unique word",
         "How can I adjust the performance of tokenizer Working with the tokenizer from the library of Hugging Face The tokenizer works fine in most cases but in some cases it does not I'm wondering if I can adjust not train a new tokenizer from scratch the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to To be more specific the type of tokenizer is which is a unigram tokenizer and the model is",
         "adjust performance tokenizer working tokenizer library hug face tokenizer work fine case case ' m wonder adjust train new tokenizer scratch performance tokenizer handle bad case still maintain good performance case use specific type tokenizer unigram tokenizer model",
         "adjust performance tokenizer",
         "0",
         "adjust,performance,adjust performance,tokenizer,performance tokenizer",
         "Tokenising Text"
        ],
        [
         "28",
         "79081924",
         "https://stackoverflow.com/questions/79081924",
         "With spaCy, how can I get all lemmas from a string?",
         "<p>I have a pandas data frame with a column of text values (documents).  I want to apply lemmatization on these values with the spaCy library using the pandas <code>apply</code> function.  I've defined my <code>to_lemma</code> function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow.  Is there a way to extract the lemmatized form of a document in spaCy?</p>\n<pre><code>def to_lemma(text):\n    tp = nlp(text)\n    line = &quot;&quot;\n    for word in tp:\n        line = line + word.lemma_ + &quot; &quot;\n    return line\n</code></pre>\n",
         "2024-10-12 00:00:00",
         "python,pandas,nlp,spacy,lemmatization",
         "-1",
         "104",
         "2",
         "79086290.0",
         "<p>There are many ways to speed up SpaCy processing. The question which of them make sense for you depends mostly on the size of your input.</p>\n<ol>\n<li>The most obvious one is not individually apply the model to every single row, but rather use batch processing. Use <code>nlp.pipe()</code> with an Iterable of strings. This means it is easier to not use apply.</li>\n<li>Disable components that you do not use. For token level processing where you need the lemmas this would be <code>'parser'</code> (the dependency parser) and <code>'ner'</code> (the Named Entity Recognition component).</li>\n<li>Increase the <code>batch_size</code> (objects to buffer) in pipe(). The default is 1000. Obviously this only makes sense to touch if you have the memory to increase it a lot.</li>\n<li>Increase the number of processors used using <code>n_process</code>. This will increase the time it takes to initially load the model but decrease the processing time. In my experience this starts making sense at about 500k+ texts. Note that this also requires the code to be run in an <code>if __name__ == '__main__':</code> wrapper.</li>\n</ol>\n<p>Basic example with 1. and 2.:</p>\n<pre><code>texts = df[&quot;column_name&quot;]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\ndf[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n<p>Advanced example for all four:</p>\n<pre><code>if __name__ == '__main__':\n    texts = df[&quot;column_name&quot;]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))\n    df[&quot;column_name_lemmas&quot;] = lemmas\n</code></pre>\n",
         "2.0",
         "101-1k",
         "2024",
         "apply\n---\nto_lemma\n---\ndef to_lemma(text):\n    tp = nlp(text)\n    line = \"\"\n    for word in tp:\n        line = line + word.lemma_ + \" \"\n    return line",
         "nlp.pipe()\n---\n'parser'\n---\n'ner'\n---\nbatch_size\n---\nn_process\n---\nif __name__ == '__main__':\n---\ntexts = df[\"column_name\"]\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nlemmas = []\nfor processed_doc in nlp.pipe(texts):\n    lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\ndf[\"column_name_lemmas\"] = lemmas\n---\nif __name__ == '__main__':\n    texts = df[\"column_name\"]\n    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n    lemmas = []\n    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):\n        lemmas.append(\" \".join([token.lemma_ for token in processed_doc]))\n    df[\"column_name_lemmas\"] = lemmas",
         "spacy get lemmas string",
         "I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function I've defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy",
         "There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function I've defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy There are many ways to speed up SpaCy processing The question which of them make sense for you depends mostly on the size of your input The most obvious one is not individually apply the model to every single row but rather use batch processing Use with an Iterable of strings This means it is easier to not use apply Disable components that you do not use For token level processing where you need the lemmas this would be the dependency parser and the Named Entity Recognition component Increase the objects to buffer in pipe The default is 1000 Obviously this only makes sense to touch if you have the memory to increase it a lot Increase the number of processors used using This will increase the time it takes to initially load the model but decrease the processing time In my experience this starts making sense at about 500k+ texts Note that this also requires the code to be run in an wrapper Basic example with 1 and 2 Advanced example for all four",
         "spacy get lemmas string panda datum frame column text value document want apply lemmatization value spacy library use panda function ' ve define function iterate word document concatenate correspond lemmas output string however slow way extract lemmatize form document spacy many way speed spacy processing question make sense depends mostly size input obvious one individually apply model every single row rather use batch processing use iterable string mean easy use apply disable component use token level processing need lemmas would dependency parser name entity recognition component increase object buffer pipe default 1000 obviously make sense touch memory increase lot increase number processor use use increase time take initially load model decrease processing time experience start make sense 500k+ text note also require code run wrapper basic example 1 2 advanced example four",
         "With spaCy how can I get all lemmas from a string I have a pandas data frame with a column of text values documents I want to apply lemmatization on these values with the spaCy library using the pandas function I've defined my function to iterate through the words in the document and concatenate the corresponding lemmas in the output string however this is slow Is there a way to extract the lemmatized form of a document in spaCy",
         "spacy get lemmas string panda datum frame column text value document want apply lemmatization value spacy library use panda function ' ve define function iterate word document concatenate correspond lemmas output string however slow way extract lemmatize form document spacy",
         "spacy get lemmas",
         "5",
         "",
         "Using Spacy Library"
        ],
        [
         "29",
         "79057082",
         "https://stackoverflow.com/questions/79057082",
         "Avoiding overlap in frequency and document frequency count in Quanteda",
         "<p>Below is a dummy corpus of 4 documents.</p>\n<p>The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in.</p>\n<p>The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive.</p>\n<p>Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each,\nbut are intended to be counted once, according to the dictionary.</p>\n<p>The expected overall frequency count is (extracted from the 'pattern&quot; column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key.</p>\n<p>Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key.</p>\n<p>I have three questions:</p>\n<ol>\n<li>are there any problems/issues that could affect output accuracy using this approach?</li>\n<li>is there a better/more parsimonius approach to achieve what I am trying to do?</li>\n<li>what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table?</li>\n</ol>\n<pre><code>        library (quanteda)\n        library(quanteda.textstats)\n\n        txt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\n\n\n         x &lt;- x %&gt;%\n         tokens_compound(phrase(&quot;dairy farmers&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farms&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farm&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farming&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy industry&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;indigenous australians&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;australia post&quot;), concatenator = &quot; &quot;) %&gt;%\n         tokens_compound(phrase(&quot;dairy farmer&quot;), concatenator = &quot; &quot;)\n              x\n\n         dict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign- \n         owned&quot;, &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, &quot;multinational \n         oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n         dairy = c(&quot;dairy farmers&quot;, &quot;dairy farms&quot;,&quot;dairy farm&quot;,&quot;dairy farming&quot;,&quot;dairy \n         industry&quot;, &quot;dairy farmer&quot;,&quot;dairy&quot;, &quot;milk&quot;),\n         auspost = &quot;australia post&quot;,\n         oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n         farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n         foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n         business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n         indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres \n         strait&quot;),\n         peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, &quot;people of \n         this nation&quot;, &quot;people of this country&quot;),\n         industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\n        kwicdict &lt;- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, &quot;D:/Output/TEST.csv&quot;)\n\n       DF &lt;- read.csv(&quot;D://Output/TEST.csv&quot;,header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       &gt; x2 &lt;- DF[,8]\n       &gt; \n       &gt; table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2 \n</code></pre>\n",
         "2024-10-05 00:00:00",
         "r,count,nlp,overlap,quanteda",
         "1",
         "61",
         "1",
         "79058791.0",
         "<p>I don't think that <code>kwic()</code> is what you want here. <code>tokens_lookup()</code> lets you specify that the nested scope should be mutually exclusive across keys, not just within keys. Observe the difference below. (And note the use of wildcarding for dairy key.)</p>\n<pre class=\"lang-r prettyprint-override\"><code>library(quanteda)\n#&gt; Package version: 4.1.0\n#&gt; Unicode version: 14.0\n#&gt; ICU version: 71.1\n#&gt; Parallel computing: 10 of 10 threads used.\n#&gt; See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. &quot;,\n         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in \n         our country&quot;,\n         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned&quot;,\n         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.&quot;)\n\ndict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign-owned&quot;, \n                                     &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, \n                                     &quot;multinational oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),\n                        dairy = c(&quot;dairy farm*&quot;, &quot;dairy industry&quot;, &quot;dairy&quot;, &quot;milk&quot;),\n                        auspost = &quot;australia post&quot;,\n                        oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),\n                        farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),\n                        foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), \n                        business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),\n                        indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres strait&quot;),\n                        peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, \n                                 &quot;people of this nation&quot;, &quot;people of this country&quot;),\n                        industry = c(&quot;industry&quot;, &quot;industries&quot;)))\n\nx &lt;- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       5       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        1\n#&gt;   doc3        1     1       0  0       1       0        1     1    1        0\n#&gt;   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = &quot;dictionary&quot;) |&gt;\n    dfm()\n#&gt; Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#&gt;       features\n#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#&gt;   doc1        0     3       0  2       3       0        0     0    0        1\n#&gt;   doc2        0     2       0  1       0       0        0     0    1        0\n#&gt;   doc3        1     1       0  0       1       0        1     1    0        0\n#&gt;   doc4        0     0       1  1       1       1        0     0    0        0\n</code></pre>\n<p><sup>Created on 2024-10-06 with <a href=\"https://reprex.tidyverse.org\" rel=\"nofollow noreferrer\">reprex v2.1.1</a></sup></p>\n",
         "0.0",
         "11-100",
         "2024",
         "library (quanteda)\n        library(quanteda.textstats)\n\n        txt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\n\n\n         x <- x %>%\n         tokens_compound(phrase(\"dairy farmers\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farms\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farm\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farming\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy industry\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"indigenous australians\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"australia post\"), concatenator = \" \") %>%\n         tokens_compound(phrase(\"dairy farmer\"), concatenator = \" \")\n              x\n\n         dict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign- \n         owned\", \"foreign owned\", \"foreign companies\", \"multinational\", \"multinational \n         oil companies\", \"multinationals\", \"transnational\"),\n         dairy = c(\"dairy farmers\", \"dairy farms\",\"dairy farm\",\"dairy farming\",\"dairy \n         industry\", \"dairy farmer\",\"dairy\", \"milk\"),\n         auspost = \"australia post\",\n         oz = c(\"australia\", \"this country\", \"our country\"),\n         farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n         foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n         business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n         indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres \n         strait\"),\n         peep = c(\"australians\", \"people of australia\", \"australian people\", \"people of \n         this nation\", \"people of this country\"),\n         industry = c(\"industry\", \"industries\")))\n\n        kwicdict <- kwic(x, pattern = dict, window = 4)\n        write.csv (kwicdict, \"D:/Output/TEST.csv\")\n\n       DF <- read.csv(\"D://Output/TEST.csv\",header=T)\n\n       ## obtaining frequency count of KWIC table 'pattern ' values\n       > x2 <- DF[,8]\n       > \n       > table (x2)\n       x2\n       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    \n          1        1        6        5        1        1        1        1     5    2",
         "kwic()\n---\ntokens_lookup()\n---\nlibrary(quanteda)\n#> Package version: 4.1.0\n#> Unicode version: 14.0\n#> ICU version: 71.1\n#> Parallel computing: 10 of 10 threads used.\n#> See https://quanteda.io for tutorials and examples.\nlibrary(quanteda.textstats)\n\ntxt <- c(doc1 = \"A significant percent of all farms in Australia, are dairy. \n         Although there are a lot of dairy farms in this country, \n         it is not the biggest farm industry. The life of a farmer is not easy, a dairy \n        farmer has to be an early riser. \",\n         doc2 = \"Australian people like milk so a healthy dairy industry is important in \n         our country\",\n         doc3 = \"Dairy and sheep farms developed at the expense of Indigenous \n         Australians. Further many companies  are now foreign-owned\",\n         doc4 = \"Some farmers are lucky to receive a service from Australia Post. Mail is \n         sent to many foreign countries and received more quickly than \n         delivered in some locations in Australia.\")\n\ndict <- dictionary(list(multinat = c(\"offshore petroleum companies\", \"foreign-owned\", \n                                     \"foreign owned\", \"foreign companies\", \"multinational\", \n                                     \"multinational oil companies\", \"multinationals\", \"transnational\"),\n                        dairy = c(\"dairy farm*\", \"dairy industry\", \"dairy\", \"milk\"),\n                        auspost = \"australia post\",\n                        oz = c(\"australia\", \"this country\", \"our country\"),\n                        farmers = c(\"farmers\", \"farmer\", \"farm\", \"farms\"),\n                        foreign = c(\"foreign\", \"foreigner\", \"foreigners\"), \n                        business =c(\"small business\", \"business\", \"businesses\", \"company\", \"companies\"),\n                        indig = c(\"aboriginal\", \"aboriginals\", \"indigenous australians\", \"torres strait\"),\n                        peep = c(\"australians\", \"people of australia\", \"australian people\", \n                                 \"people of this nation\", \"people of this country\"),\n                        industry = c(\"industry\", \"industries\")))\n\nx <- tokens(txt)\n\n# with overlap\ntokens_lookup(x, dict) |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       5       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        1\n#>   doc3        1     1       0  0       1       0        1     1    1        0\n#>   doc4        0     0       1  2       1       1        0     0    0        0\n\n# without overlap\ntokens_lookup(x, dict, nested_scope = \"dictionary\") |>\n    dfm()\n#> Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.\n#>       features\n#> docs   multinat dairy auspost oz farmers foreign business indig peep industry\n#>   doc1        0     3       0  2       3       0        0     0    0        1\n#>   doc2        0     2       0  1       0       0        0     0    1        0\n#>   doc3        1     1       0  0       1       0        1     1    0        0\n#>   doc4        0     0       1  1       1       1        0     0    0        0",
         "avoid overlap frequency document frequency count quanteda",
         "Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world 'Australians' occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly 'Australia' oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the 'pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table 'doc names' column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table",
         "I don't think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world 'Australians' occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly 'Australia' oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the 'pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table 'doc names' column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table I don't think that is what you want here lets you specify that the nested scope should be mutually exclusive across keys not just within keys Observe the difference below And note the use of wildcarding for dairy key Created on 20241006 with reprex v211",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency word phrase corpus well number document word phrase occur world ' australian ' occur two dictionary key peep indig key content intend mutually exclusive similarly ' australia ' oz australia post foreign foreign multinat farm / farmer dairy farmer occur two dictionary key intend count accord dictionary expect overall frequency count extract ' pattern column kwic table report x2 note word industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table ' doc name ' column key three question problem / issue could affect output accuracy use approach well / more parsimonius approach achieve trying would well way extract equivalent tetxstat frequency count datum kwic table not think want let specify nest scope mutually exclusive across key within key observe difference note use wildcarde dairy key create 20241006 reprex v211",
         "Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents The dictionary was developed to identify the frequency of words or phrases in the corpus as well as the number of documents a word or phrases occurs in The world 'Australians' occurs in two dictionary keys peep indig Key content is intended to be mutually exclusive Similarly 'Australia' oz and Australia Post foreign foreign and multinat and farm/farmers dairy and farmers occur in two dictionary keys each but are intended to be counted once according to the dictionary The expected overall frequency count is extracted from the 'pattern column of the kwic table and reported as x2 below Note the word industry appears but is not allocated to industry because it is define din the indig key Dairy is the most frequency occuring key occuring in three documents This can calculated from unique rows in the kwic table 'doc names' column for each key I have three questions are there any problems/issues that could affect output accuracy using this approach is there a better/more parsimonius approach to achieve what I am trying to do what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table",
         "avoid overlap frequency document frequency count quanteda dummy corpus 4 document dictionary develop identify frequency word phrase corpus well number document word phrase occur world ' australian ' occur two dictionary key peep indig key content intend mutually exclusive similarly ' australia ' oz australia post foreign foreign multinat farm / farmer dairy farmer occur two dictionary key intend count accord dictionary expect overall frequency count extract ' pattern column kwic table report x2 note word industry appears allocate industry define din indig key dairy frequency occur key occuring three document calculate unique row kwic table ' doc name ' column key three question problem / issue could affect output accuracy use approach well / more parsimonius approach achieve trying would well way extract equivalent tetxstat frequency count datum kwic table",
         "avoid overlap frequency document frequency count quanteda",
         "7",
         "count,document,count quanteda,avoid overlap,frequency",
         "Text Similarity"
        ],
        [
         "30",
         "79005985",
         "https://stackoverflow.com/questions/79005985",
         "Seq2Seq trainer.train() keeps giving indexing error",
         "<p>I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error:</p>\n<blockquote>\n<p>IndexError: Invalid key: 39463 is out of bounds for size 0.</p>\n</blockquote>\n<ul>\n<li>The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B</li>\n<li>The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also.</li>\n</ul>\n<p>Detailed error message:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;nllbtrain.py&quot;, line 273, in &lt;module&gt;\n    print(trainer.train())\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1645, in train\n    return inner_training_loop(\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 631, in __next__\n    data = self._next_data()\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2810, in __getitem__\n    return self._getitem(key)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 583, in query_table\n    _check_valid_index_key(key, size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 526, in _check_valid_index_key\n    raise IndexError(f&quot;Invalid key: {key} is out of bounds for size {size}&quot;)\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n</code></pre>\n<p>The code of the preprocessing done for the data:</p>\n<pre><code>def preprocess_function(examples):\n        inputs = [example + ' &lt;/s&gt;' + f' &lt;2{s_lang}&gt;' for example in examples[source_lang]]\n        targets = [f'&lt;2{t_lang}&gt; ' + example + ' &lt;/s&gt;' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n</code></pre>\n<p>Data after preprocessing:</p>\n<pre><code>DatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n</code></pre>\n<p>The code of model params and training:</p>\n<pre><code>model_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=&quot;hin_Deva&quot;, tgt_lang=&quot;san_Deva&quot;, max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=&quot;epoch&quot;,\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=&quot;./output_dir&quot;,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())\n\n</code></pre>\n<p>Any idea why this error is persisting?</p>\n",
         "2024-09-20 00:00:00",
         "python,nlp,huggingface-transformers,huggingface-trainer",
         "0",
         "57",
         "1",
         "79007590.0",
         "<p><code>size 0</code> indicates that the dataset your trainer gets when the fine-tuning starts is empty. Looking at this (<a href=\"https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25</a>) and this (<a href=\"https://github.com/huggingface/datasets/issues/6535\" rel=\"nofollow noreferrer\">https://github.com/huggingface/datasets/issues/6535</a>) thread suggests adding <code>remove_unused_columns = False</code> to your <code>training_args</code> might resolve the issue, so you could give that a try.</p>\n",
         "0.0",
         "11-100",
         "2024",
         "Traceback (most recent call last):\n  File \"nllbtrain.py\", line 273, in <module>\n    print(trainer.train())\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py\", line 1907, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2814, in __getitems__\n    batch = self.__getitem__(keys)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2810, in __getitem__\n    return self._getitem(key)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2794, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\n    _check_valid_index_key(key, size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File \"/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\nIndexError: Invalid key: 39463 is out of bounds for size 0\n  0%|\n---\ndef preprocess_function(examples):\n        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]\n        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]\n\n        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')\n\n        model_inputs['labels'] = labels['input_ids']\n\n        return model_inputs\n---\nDatasetDict({\n    train: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 39729\n    })\n    val: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2210\n    })\n    test: Dataset({\n        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2214\n    })\n})\n---\nmodel_path = 'facebook/nllb-200-1.3B'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)\ntokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=\"hin_Deva\", tgt_lang=\"san_Deva\", max_length = 500)\n\ntraining_args = Seq2SeqTrainingArguments(\n    evaluation_strategy=\"epoch\",\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=\"./output_dir\",\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=False,\n    push_to_hub=False,\n)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=dataset['train'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(trainer.train())",
         "size 0\n---\nremove_unused_columns = False\n---\ntraining_args",
         "seq2seq trainertrain keep give indexing error",
         "I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting",
         "indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting indicates that the dataset your trainer gets when the finetuning starts is empty Looking at this and this thread suggests adding to your might resolve the issue so you could give that a try",
         "seq2seq trainertrain keep give indexing error try machine translation hindi sanskrit use nllb model keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb model ` facebook / nllb20013b input datum ~40k hindi sentence error arises try train sample datum also detail error message code preprocessing do data datum preprocesse code model param train idea error persisting indicate dataset trainer get finetuning start empty look thread suggest add might resolve issue could give try",
         "Seq2Seq trainertrain keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model But I keep getting the error IndexError Invalid key 39463 is out of bounds for size 0 The error is coming when training the pretrained NLLB model `facebook/nllb20013B The input data is ~40k Hindi sentences The same error arises when I tried training with a sample data also Detailed error message The code of the preprocessing done for the data Data after preprocessing The code of model params and training Any idea why this error is persisting",
         "seq2seq trainertrain keep give indexing error try machine translation hindi sanskrit use nllb model keep get error indexerror invalid key 39463 bound size 0 error come training pretraine nllb model ` facebook / nllb20013b input datum ~40k hindi sentence error arises try train sample datum also detail error message code preprocessing do data datum preprocesse code model param train idea error persist",
         "seq2seq trainertrain keep indexing error",
         "2",
         "error,indexing,trainertrain,seq2seq,seq2seq trainertrain",
         "Handling Error in NLP Task"
        ],
        [
         "31",
         "78985137",
         "https://stackoverflow.com/questions/78985137",
         "Alternative to device_map = \"auto\" in Huggingface Pretrained",
         "<p>I have a model that I was reading from huggingface using the following code:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n</code></pre>\n<p>Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model.</p>\n<p>Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like <code>device_map=&quot;auto&quot;</code> but after reading the model.</p>\n<p>So simply something like</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\nmodel.device_map = &quot;auto&quot;\n</code></pre>\n",
         "2024-09-14 00:00:00",
         "machine-learning,deep-learning,nlp,huggingface-transformers",
         "2",
         "1080",
         "1",
         "79007343.0",
         "<p>I found out that there are actually several methods in <code>accelerate</code> for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map</a></p>\n<p>The second one is used to match your model with the devices:</p>\n<p><a href=\"https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model</a></p>\n<p>So basically, in your case, you can use the following code:</p>\n<pre><code>from accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)\n</code></pre>\n<p>P.S. This code still needs to be tested on fine-tuning.</p>\n",
         "2.0",
         "1k-10k",
         "2024",
         "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n---\ndevice_map=\"auto\"\n---\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\nmodel.device_map = \"auto\"",
         "accelerate\n---\nfrom accelerate import dispatch_model, infer_auto_device_map\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\n***\n...\nnew_model = CustomModel(model)\n...\n***\n\ndevice_map_dict = infer_auto_device_map(new_model)\ndispatch_model(new_model, device_map_dict)",
         "alternative device_map = auto huggingface pretraine",
         "I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers aren't distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like",
         "I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers aren't distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like I found out that there are actually several methods in for this The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model The second one is used to match your model with the devices So basically in your case you can use the following code PS This code still needs to be tested on finetuning",
         "alternative device_map = auto huggingface pretraine model reading huggingface use follow code read model modification internal layer add layer start training / finetune get everything model investigation find custom layer not distribute multi gpus original model need something like read model simply something like find actually several method first one use analyze model calculate total amount available memory occupy model second one use match model device basically case use follow code ps code still need tested finetune",
         "Alternative to device_map = auto in Huggingface Pretrained I have a model that I was reading from huggingface using the following code Now I read the model and I did some modifications to the internal layers and added more layers When I started the training/finetuning I get that not everything is on the same model Now after more investigations I found that my custom layers aren't distributed on multi GPUs as the original model So I need something like but after reading the model So simply something like",
         "alternative device_map = auto huggingface pretraine model reading huggingface use follow code read model modification internal layer add layer start training / finetune get everything model investigation find custom layer not distribute multi gpus original model need something like read model simply something like",
         "alternative devicemap auto huggingface pretraine",
         "6",
         "alternative,auto,pretraine,huggingface,alternative devicemap",
         "BERT & Hugging Face Application"
        ],
        [
         "32",
         "78966943",
         "https://stackoverflow.com/questions/78966943",
         "How are the weights of the Mistral models reinitialized in Huggingface?",
         "<p>From <a href=\"https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic\">How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model?</a> and <a href=\"https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4</a> there's different suggestions to reinitialize the model.</p>\n<p>When I tried this, it seems to work.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;, token=&quot;hf_*****&quot;)\n\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n</code></pre>\n<p>[out]:</p>\n<pre><code>OrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n</code></pre>\n<p>How are the layers re-initialized through the <code>from_config</code> function? Is it using <a href=\"https://cs230.stanford.edu/section/4/\" rel=\"nofollow noreferrer\">Xaiver/He initialization</a> or just random initialization?</p>\n",
         "2024-09-09 00:00:00",
         "nlp,huggingface-transformers,large-language-model,mistral-7b",
         "3",
         "185",
         "2",
         "78969695.0",
         "<p><a href=\"https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralConfig\" rel=\"nofollow noreferrer\">MistralConfig</a> has a default parameter <code>initializer_range</code> which is set to 0.02 and described as <code>The standard deviation of the truncated_normal_initializer for initializing all weight matrices</code>, so one can assume they use a truncated normal distribution with a standard deviation of 0.02.</p>\n<p>If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 0.02 looks like, it seems like a fit to me:</p>\n<pre><code>import numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/M67S0rKp.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/M67S0rKp.png\" alt=\"model_weights\" /></a></p>\n",
         "2.0",
         "101-1k",
         "2024",
         "from transformers import AutoModelForCausalLM, AutoConfig\n\nm = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", token=\"hf_*****\")\n\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\n\nprint(m2.model.layers[0].mlp.down_proj.state_dict())\n\nprint(m.model.layers[0].mlp.down_proj.state_dict())\n---\nOrderedDict([('weight',\n              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],\n                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],\n                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],\n                      ...,\n                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],\n                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],\n                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])\n\n\nOrderedDict([('weight',\n              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n                      ...,\n                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])\n---\nfrom_config",
         "initializer_range\n---\nThe standard deviation of the truncated_normal_initializer for initializing all weight matrices\n---\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import truncnorm\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\n# histogram of actual weights distribution\nc = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\nm2 = AutoModelForCausalLM.from_config(c)\nweights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()\nplt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')\n\n# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like\nlower = -0.1\nupper = 0.1\nmean = 0\nstd = 0.02\na, b = (lower - mean) / std, (upper - mean) / std\nx = np.linspace(lower, upper, 1000)\nplt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')\n\nplt.legend()\nplt.show()",
         "weight mistral model reinitialize huggingface",
         "From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and there's different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization",
         "MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and there's different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization MistralConfig has a default parameter which is set to 002 and described as so one can assume they use a truncated normal distribution with a standard deviation of 002 If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 002 looks like it seems like a fit to me",
         "weight mistral model reinitialize huggingface one reinitialize weight hug face llama v2 model official way original model 's different suggestion reinitialize model try seem work layer reinitialize function use xaiver / he initialization random initialization mistralconfig default parameter set 002 describe one assume use truncate normal distribution standard deviation 002 plot actual model weight distribution truncate normal distribution standard deviation 002 look like seem like fit",
         "How are the weights of the Mistral models reinitialized in Huggingface From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model and there's different suggestions to reinitialize the model When I tried this it seems to work out How are the layers reinitialized through the function Is it using Xaiver/He initialization or just random initialization",
         "weight mistral model reinitialize huggingface one reinitialize weight hug face llama v2 model official way original model 's different suggestion reinitialize model try seem work layer reinitialize function use xaiver / he initialization random initialization",
         "weight mistral reinitialize huggingface",
         "6",
         "reinitialize,weight,huggingface,mistral reinitialize,weight mistral",
         "BERT & Hugging Face Application"
        ],
        [
         "33",
         "78957322",
         "https://stackoverflow.com/questions/78957322",
         "Break after first PER sequence found with Spacy",
         "<p>I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all &quot;PER&quot; tags, but I want to reduce the overhead and get only the first contiguous sequence of &quot;PER&quot; entities. Here’s the example output I get:</p>\n<pre><code>Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n</code></pre>\n<p>But I want the result to be:</p>\n<pre><code>Detected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n</code></pre>\n<p>Here is the code I am currently using:</p>\n<pre><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}]\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == &quot;PER&quot;:\n            per_entities.append(ent.text)\n        elif ent.label_ == &quot;MISC&quot; and misc_count &lt; 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n<p>What I'm looking for:</p>\n<p>I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of &quot;PER&quot; entities in the text, ignoring any subsequent &quot;PER&quot; entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?</p>\n",
         "2024-09-06 00:00:00",
         "python,nlp,spacy",
         "0",
         "41",
         "1",
         "78957722.0",
         "<p>The issues is that <code>doc[start_idx:].ents</code> is <a href=\"https://spacy.io/api/doc#ents\" rel=\"nofollow noreferrer\">only the named entities</a> in that slice of the doc. Thus, you will never process &quot;habló&quot; for the first entry, you will just go straight from &quot;García&quot; to &quot;López&quot;. To actually iterate over the tokens so that you see when the PER sequence ends, you have to leave out the <code>.ents</code> part. Then you just wait until you see the first token with <code>ent_type_</code> PER and start appending, then break after one of your conditions is met. I ended up refactoring your code a little as I debugged this, but here's an edited version of your program that produces the desired outputs:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;es_core_news_lg&quot;)\n\ntexts = [\n    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,\n    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,\n    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,\n    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;,\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],\n    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}],\n]\n\nmatcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == &quot;PER&quot;:\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == &quot;MISC&quot; and misc_count &lt; 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f&quot;Detected Names in Text: {names}&quot;)\n</code></pre>\n",
         "1.0",
         "11-100",
         "2024",
         "Detected Names in Text: ['garcía', 'lópez']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']\nDetected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']\n---\nDetected Names in Text: ['garcía']\nDetected Names in Text: ['j. jesus orozco alfaro']\nDetected Names in Text: ['josé guadarrama márquez']\nDetected Names in Text: ['pedro sánchez']\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\"\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}]\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc, start_idx=0):\n    per_entities = []\n    misc_count = 0\n    \n    for ent in doc[start_idx:].ents:\n        if ent.label_ == \"PER\":\n            per_entities.append(ent.text)\n        elif ent.label_ == \"MISC\" and misc_count < 1:\n            misc_count += 1\n            per_entities.append(ent.text)\n        else:\n            break  # Should stop if any other entity or second MISC is encountered\n    \n    return per_entities\n\nfor text in texts:\n    doc = nlp(text)\n    \n    # Find matches\n    matches = matcher(doc)\n    \n    # Extract the first match and its position\n    title_start = None\n    title_end = None\n    for match_id, start, end in matches:\n        title_start = start\n        title_end = end\n        break\n\n    # If a title was found, start searching for PER entities from that position\n    if title_start is not None:\n        names = find_per_sequence(doc, start_idx=title_end)\n    else:\n        names = find_per_sequence(doc)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "doc[start_idx:].ents\n---\n.ents\n---\nent_type_\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"es_core_news_lg\")\n\ntexts = [\n    \"El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.\",\n    \"PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO\",\n    \"            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez\",\n    \"El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.\",\n]\ntexts = [text.lower() for text in texts]\n\nmatcher = Matcher(nlp.vocab)\n\npatterns = [\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"c\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sr\"}],\n    [{\"LOWER\": \"el\"}, {\"LOWER\": \"sra\"}],\n]\n\nmatcher.add(\"LEGISLATIVE_TITLES\", patterns)\n\n\n# Function to find a sequence of PER entities allowing one MISC\ndef find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):\n    per_entities = []\n    misc_count = 0\n    per_started = False\n\n    for token in doc[start_idx:]:\n        if token.ent_type_ == \"PER\":\n            per_entities.append(token.text)\n            per_started = True\n        elif token.ent_type_ == \"MISC\" and misc_count < 1 and per_started:\n            misc_count += 1\n            per_entities.append(token.text)\n        elif per_started:\n            break  # Should stop if any other entity or second MISC is encountered\n\n    return per_entities\n\n\nfor text in texts:\n    doc = nlp(text)\n\n    # Find matches\n    matches = matcher(doc)\n\n    # Extract the first match and its position\n    _, _, title_end = matches[0] if matches else (None, None, None)\n\n    names = find_per_sequence(doc, title_end if title_end else 0)\n\n    # Output the detected names for each text\n    print(f\"Detected Names in Text: {names}\")",
         "break first per sequence find spacy",
         "I am trying to extract only the first speaker's name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What I'm looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this",
         "The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but here's an edited version of your program that produces the desired outputs",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speaker's name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What I'm looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this The issues is that is only the named entities in that slice of the doc Thus you will never process habl for the first entry you will just go straight from Garca to Lpez To actually iterate over the tokens so that you see when the PER sequence ends you have to leave out the part Then you just wait until you see the first token with PER and start appending then break after one of your conditions is met I ended up refactoring your code a little as I debugged this but here's an edited version of your program that produces the desired outputs",
         "break first per sequence find spacy try extract first speaker 's name list text use spacy currently function return per tag want reduce overhead get first contiguous sequence per entity here example output get want result code currently use ' m look want modify find_per_sequence function return first contiguous sequence per entity text ignore subsequent per entity encounter different type entity provide function return multiple name partial name need way ensure first name sequence include achieve issue name entity slice doc thus never process habl first entry go straight garca lpez actually iterate token see per sequence end leave part wait see first token per start append break one condition meet end refactore code little debug 's edit version program produce desire output",
         "Break after first PER sequence found with Spacy I am trying to extract only the first speaker's name from a list of texts using spaCy Currently my function returns all PER tags but I want to reduce the overhead and get only the first contiguous sequence of PER entities Heres the example output I get But I want the result to be Here is the code I am currently using What I'm looking for I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of PER entities in the text ignoring any subsequent PER entities after encountering a different type of entity The provided function returns multiple names or partial names and I need a way to ensure only the first name or sequence is included How can I achieve this",
         "break first per sequence find spacy try extract first speaker 's name list text use spacy currently function return per tag want reduce overhead get first contiguous sequence per entity here example output get want result code currently use ' m look want modify find_per_sequence function return first contiguous sequence per entity text ignore subsequent per entity encounter different type entity provide function return multiple name partial name need way ensure first name sequence include achieve",
         "break first per sequence spacy",
         "5",
         "spacy,break,sequence,sequence spacy,break sequence",
         "Using Spacy Library"
        ],
        [
         "34",
         "78949607",
         "https://stackoverflow.com/questions/78949607",
         "Trainer huggingface - RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned",
         "<p>I recently got the following error:\n<code>RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned</code>\nwhen doing LoRA on a small LLM.</p>\n<p>I saw on a discord someone saying:</p>\n<blockquote>\n<p>The issue likely stems from the fact that you are manually placing\nyour inputs on the GPU (with to(model.device)), but the Trainer\nexpects data to be on the CPU and will handle the transfer to the GPU\ninternally.</p>\n</blockquote>\n<p>I can't find anything of the sort written in the Trainer documentation of huggingface <a href=\"https://huggingface.co/docs/transformers/en/main_classes/trainer\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/en/main_classes/trainer</a>.</p>\n<p>Is it true? If not, how can I get rid of that error?</p>\n<p>MRE:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = &quot;croissantllm/CroissantLLMBase&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=&quot;auto&quot;)\n\ntexts = [\n    &quot;The first sentence for fine-tuning. &lt;/s&gt;&quot;,\n    &quot;The second sentence for fine-tuning. &lt;/s&gt;&quot;\n]\n\ninputs = [tokenizer(text, return_tensors=&quot;pt&quot;).to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {&quot;input_ids&quot;: input_ids, &quot;labels&quot;: labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n</code></pre>\n<p>The issue is fairly easy to reproduce directly on colab (run <code>%pip install --upgrade torch transformers peft</code> in the first cell).</p>\n",
         "2024-09-04 00:00:00",
         "nlp,huggingface-transformers",
         "2",
         "1512",
         "1",
         "79112186.0",
         "<p>Since pinning memory is only available on CPU and not GPU, when running on GPU on Colab, you can just disable it by setting <code>dataloader_pin_memory</code> to <code>False</code> for <code>TrainingArguments</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(\n    output_dir=&quot;./lora_croissantllm&quot;,\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=&quot;./logs&quot;,\n    logging_steps=10,\n)\n</code></pre>\n",
         "3.0",
         "1k-10k",
         "2024",
         "RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned\n---\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = \"croissantllm/CroissantLLMBase\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\ntexts = [\n    \"The first sentence for fine-tuning. </s>\",\n    \"The second sentence for fine-tuning. </s>\"\n]\n\ninputs = [tokenizer(text, return_tensors=\"pt\").to(model.device) for text in texts]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, lora_config)\n\nclass CustomDataset(Dataset):\n    def __init__(self, input_list):\n        self.input_list = input_list\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_list[idx]['input_ids'].squeeze()\n        labels = input_ids.clone()\n        return {\"input_ids\": input_ids, \"labels\": labels}\n\ntrain_dataset = CustomDataset(inputs)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n---\n%pip install --upgrade torch transformers peft",
         "dataloader_pin_memory\n---\nFalse\n---\nTrainingArguments\n---\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_croissantllm\",\n    dataloader_pin_memory=False,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    save_steps=10,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)",
         "trainer huggingface runtimeerror pin ' torchcudafloattensor ' dense cpu tensor pin",
         "I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I can't find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell",
         "Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "Trainer huggingface RuntimeError cannot pin 'torchcudaFloatTensor' only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I can't find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell Since pinning memory is only available on CPU and not GPU when running on GPU on Colab you can just disable it by setting to for",
         "trainer huggingface runtimeerror pin ' torchcudafloattensor ' dense cpu tensor pin recently get follow error lora small llm see discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not find anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell since pin memory available cpu gpu run gpu colab disable setting",
         "Trainer huggingface RuntimeError cannot pin 'torchcudaFloatTensor' only dense CPU tensors can be pinned I recently got the following error when doing LoRA on a small LLM I saw on a discord someone saying The issue likely stems from the fact that you are manually placing your inputs on the GPU with tomodeldevice but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally I can't find anything of the sort written in the Trainer documentation of huggingface Is it true If not how can I get rid of that error MRE The issue is fairly easy to reproduce directly on colab run in the first cell",
         "trainer huggingface runtimeerror pin ' torchcudafloattensor ' dense cpu tensor pin recently get follow error lora small llm see discord someone say issue likely stem fact manually place input gpu tomodeldevice trainer expect datum cpu handle transfer gpu internally can not find anything sort write trainer documentation huggingface true get rid error mre issue fairly easy reproduce directly colab run first cell",
         "trainer huggingface runtimeerror pin torchcudafloattensor dense cpu tensor pin",
         "2",
         "trainer,huggingface runtimeerror,cpu tensor,runtimeerror pin,torchcudafloattensor dense",
         "Handling Error in NLP Task"
        ],
        [
         "35",
         "78943401",
         "https://stackoverflow.com/questions/78943401",
         "Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error \"Attempting to Unscale FP16 Gradients\"",
         "<p>I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?</p>\n<p>Here is a minimal example:</p>\n<pre class=\"lang-none prettyprint-override\"><code>import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = &quot;facebook/opt-1.3b&quot;\ncache_dir = './models'\nos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;7&quot;\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=&quot;nf4&quot;,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n</code></pre>\n<p>At the line <code>scaler.step(optimizer)</code>, an error occurs:</p>\n<pre><code>Exception has occurred: ValueError: Attempting to unscale FP16 gradients.\n\n</code></pre>\n",
         "2024-09-03 00:00:00",
         "python,pytorch,nlp,huggingface-transformers,fine-tuning",
         "1",
         "516",
         "1",
         "78945455.0",
         "<p>You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32.</p>\n<p>You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training.</p>\n<p>What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details <a href=\"https://huggingface.co/docs/peft/en/developer_guides/quantization\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "1.0",
         "101-1k",
         "2024",
         "import os\nfrom transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\nmodel_name = \"facebook/opt-1.3b\"\ncache_dir = './models'\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\npretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, \n                                                    cache_dir=cache_dir,                                                     \n                                                    quantization_config=quantization_config)\ntokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,\n                                                    cache_dir=cache_dir)\noptimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)\nscaler = GradScaler()\ninput_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)\nlabels = torch.LongTensor([[1, 2, 3, 4]]).to(0)\nwith torch.autocast(device_type='cuda'):\n    out = pretrained_model(input_ids=input_ids, labels=labels)\n    loss = out.loss\nscaler.scale(out.loss).backward()\nscaler.step(optimizer) \nscaler.update()\noptimizer.zero_grad()\n\nprint(f'End')\n---\nscaler.step(optimizer)\n---\nException has occurred: ValueError: Attempting to unscale FP16 gradients.",
         "",
         "finetune pretraine model quantization amp scaler error attempt unscale fp16 gradient",
         "I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I can't seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs",
         "You can't finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldn't finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I can't seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs You can't finetune a fp16/uint8 model with AMP AMP uses fp32 parameters The params are autocast to fp16 for the forward pass but AMP expects the master set of parameters to be FP32 You also shouldn't finetune a quantized model in the first place The quantization causes all sorts of numerical issues and instability during training What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model You can find more details here",
         "finetune pretraine model quantization amp scaler error attempt unscale fp16 gradient try finetune pretraine model limited vram achieve use quantization automatic mixed precision amp however encounter issue can not seem resolve could please help identify problem minimal example line error occur can not finetune fp16 / uint8 model amp amp use fp32 parameter param autocast fp16 forward pass amp expect master set parameter fp32 also not finetune quantize model first place quantization cause sort numerical issue instability training suppose keep quantize model static train adapter top quantize model find detail",
         "Finetuning a Pretrained Model with Quantization and AMP Scaler Error Attempting to Unscale FP16 Gradients I am trying to finetune a pretrained model with limited VRAM To achieve this I am using quantization and automatic mixed precision AMP However I am encountering an issue that I can't seem to resolve Could you please help me identify the problem Here is a minimal example At the line an error occurs",
         "finetune pretraine model quantization amp scaler error attempt unscale fp16 gradient try finetune pretraine model limited vram achieve use quantization automatic mixed precision amp however encounter issue can not seem resolve could please help identify problem minimal example line error occur",
         "finetune pretraine quantization amp scaler error attempt unscale fp16 gradient",
         "2",
         "fp16,pretraine,scaler error,fp16 gradient,quantization amp",
         "Handling Error in NLP Task"
        ],
        [
         "36",
         "78933232",
         "https://stackoverflow.com/questions/78933232",
         "Keep training pytorch model on new data",
         "<p>I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps:</p>\n<ol>\n<li>Load and process the text.</li>\n<li>Use a TF-IDF Vectorizer.</li>\n<li>Build the neural network and save the TF-IDF Vectorizer and model to predict new data.</li>\n</ol>\n<p>However, every day I need to classify new comments and correct any wrong classifications.</p>\n<p>Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct).</p>\n<p>Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen.</p>\n<p>So the mains questions are:</p>\n<ol>\n<li>How could i check if the propossed way to solve this problem work as i expect?</li>\n<li>What can i do with the vectorizer when it face new tokens, can i just do a <code>.fit_transform()</code> or i would loose the original vectorizer?</li>\n</ol>\n<p>Here its the full training process:</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        &quot;set1.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n&quot;&quot;&quot;\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n&quot;&quot;&quot;\n\nlabel_mapping = {\n    &quot;anger&quot;: 0,\n    &quot;fear&quot;: 1,\n    &quot;joy&quot;: 2,\n    &quot;sadness&quot;: 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[&quot;label&quot;])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[&quot;label&quot;]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = &quot;modelbest.pth&quot;\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\ntorch.save(model.state_dict(), model_path)\n\n</code></pre>\n<p>Proposed code:</p>\n<pre><code>import torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = &quot;tfidf_vectorizer.pkl&quot;\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = &quot;text_classification_model.pth&quot;\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\nsentiments = [&quot;fear&quot;,&quot;joy&quot;,&quot;sadness&quot;,&quot;anger&quot;]\n\nnew_data = (\n    pl\n    .read_csv(\n        &quot;set2.txt&quot;,\n        separator=&quot;;&quot;,\n        has_header=False,\n        new_columns=[&quot;text&quot;,&quot;label&quot;]\n    )\n    .filter(pl.col(&quot;label&quot;).is_in(sentiments))\n    .with_columns(\n        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = &quot;modelbest.pth&quot;\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = &quot;new_moedl.pth&quot;\ntorch.save(model.state_dict(), new_best_model_path)\n</code></pre>\n<p>The dataset can be found <a href=\"https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\" rel=\"nofollow noreferrer\">here</a></p>\n",
         "2024-08-30 00:00:00",
         "python,scikit-learn,pytorch,nlp,python-polars",
         "2",
         "282",
         "2",
         "78934212.0",
         "<p>use  pre-trained word embeddings like BertForSequenceClassification.  These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words.</p>\n<p><strong>Model Training with BERT</strong></p>\n<pre><code>import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(&quot;set1.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)\njoy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)\nsadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)\nanger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}\ntrain_df = train_df.with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[&quot;label&quot;])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[&quot;label&quot;])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy: {results['eval_accuracy']:.4f}&quot;)\n\n# Save the model and tokenizer\nmodel.save_pretrained(&quot;saved_model&quot;)\ntokenizer.save_pretrained(&quot;saved_tokenizer&quot;)\n</code></pre>\n<p><strong>Incremental training with least effort</strong></p>\n<pre><code># Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(&quot;saved_model&quot;)\ntokenizer = BertTokenizer.from_pretrained(&quot;saved_tokenizer&quot;)\n\n# Load new data\nnew_data = (\n    pl.read_csv(&quot;set2.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])\n    .filter(pl.col(&quot;label&quot;).is_in([&quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;anger&quot;]))\n    .with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f&quot;Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}&quot;)\n\n# Save the updated model\nmodel.save_pretrained(&quot;saved_model_incremental&quot;)\n</code></pre>\n",
         "3.0",
         "101-1k",
         "2024",
         ".fit_transform()\n---\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib\n\nset1 = (\n    pl\n    .read_csv(\n        \"set1.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n)\n\n# since the dateset its unbalanced, im going to force to have more balance\n\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\n\ntrain_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])\n\n\"\"\"\nThe text its already clean, so im going to change the labels to numeric\nand then split it on train, test ,val\n\"\"\"\n\nlabel_mapping = {\n    \"anger\": 0,\n    \"fear\": 1,\n    \"joy\": 2,\n    \"sadness\": 3\n}\n\ntrain_mapped = (\n    train_df\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n   \n)\n\ntrain_set, pre_Test = train_test_split(train_mapped,\n                                    test_size=0.4,\n                                    random_state=42,\n                                    stratify=train_mapped[\"label\"])\n\ntest_set, val_set = train_test_split(pre_Test,\n                                    test_size=0.5,\n                                    random_state=42,\n                                    stratify=pre_Test[\"label\"]) \n\n# Vectorize text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n\nX_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()\nX_val_tfidf = vectorizer.transform(val_set['text']).toarray()\nX_test_tfidf = vectorizer.transform(test_set['text']).toarray()\n\ny_train = train_set['label']\ny_val = val_set['label']\ny_test = test_set['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n    \ntrain_dataset = TextDataset(X_train_tfidf, y_train)\nval_dataset = TextDataset(X_val_tfidf, y_val)\ntest_dataset = TextDataset(X_test_tfidf, y_test)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \ninput_dim = X_train_tfidf.shape[1]\nmodel = TextClassificationModel(input_dim, 4)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\n# Training loop\nnum_epochs = 17\nbest_val_acc = 0.0\nbest_model_path = \"modelbest.pth\"\n\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, labels in val_loader:\n            texts, labels = texts.float(), labels.long()\n            outputs = model(texts)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = correct / total\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Test the model\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = correct / total\nprint(f'Test Acc: {test_acc:.3f}')\n\n\n# Save the TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\njoblib.dump(vectorizer, vectorizer_path)\n\n# Save the PyTorch model\nmodel_path = \"text_classification_model.pth\"\ntorch.save(model.state_dict(), model_path)\n---\nimport torch\nimport joblib\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load the saved TF-IDF vectorizer\nvectorizer_path = \"tfidf_vectorizer.pkl\"\nvectorizer = joblib.load(vectorizer_path)\n\ninput_dim = len(vectorizer.get_feature_names_out())\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.softmax(self.fc3(x), dim=1)\n        return x\n    \n# Load the saved PyTorch model\nmodel_path = \"text_classification_model.pth\"\nmodel = TextClassificationModel(input_dim, 4)\nmodel.load_state_dict(torch.load(model_path))\n\n# Map labels to numeric values\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\nsentiments = [\"fear\",\"joy\",\"sadness\",\"anger\"]\n\nnew_data = (\n    pl\n    .read_csv(\n        \"set2.txt\",\n        separator=\";\",\n        has_header=False,\n        new_columns=[\"text\",\"label\"]\n    )\n    .filter(pl.col(\"label\").is_in(sentiments))\n    .with_columns(\n        pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16)\n    )\n    \n)\n# Vectorize the new text data using the loaded TF-IDF vectorizer\nX_new = vectorizer.transform(new_data['text']).toarray()\ny_new = new_data['label']\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        return text, label\n\nbatch_size = 10\n   \n# Create DataLoader for the new training data\nnew_train_dataset = TextDataset(X_new, y_new)\nnew_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters())\n\nnum_epochs = 5\nnew_best_model_path = \"modelbest.pth\"\nfor epoch in range(num_epochs):\n    model.train()\n    for texts, labels in new_train_loader:\n        texts, labels = texts.float(), labels.long()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.save(model.state_dict(), new_best_model_path)\n        \nprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Save the PyTorch model\nnew_best_model_path = \"new_moedl.pth\"\ntorch.save(model.state_dict(), new_best_model_path)",
         "import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\n\n# Load and prepare data\nset1 = pl.read_csv(\"set1.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n\n# Balance dataset\nfear_df = set1.filter(pl.col(\"label\") == \"fear\")\njoy_df = set1.filter(pl.col(\"label\") == \"joy\").sample(n=2500)\nsadness_df = set1.filter(pl.col(\"label\") == \"sadness\").sample(n=2500)\nanger_df = set1.filter(pl.col(\"label\") == \"anger\")\ntrain_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])\n\nlabel_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\ntrain_df = train_df.with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n\n# Split dataset\ntrain_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[\"label\"])\ntest_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[\"label\"])\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)\nval_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)\ntest_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)\n\n# Initialize BERT model for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()\n\n# Evaluate model\nresults = trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")\n\n# Save the model and tokenizer\nmodel.save_pretrained(\"saved_model\")\ntokenizer.save_pretrained(\"saved_tokenizer\")\n---\n# Load the saved model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"saved_model\")\ntokenizer = BertTokenizer.from_pretrained(\"saved_tokenizer\")\n\n# Load new data\nnew_data = (\n    pl.read_csv(\"set2.txt\", separator=\";\", has_header=False, new_columns=[\"text\", \"label\"])\n    .filter(pl.col(\"label\").is_in([\"fear\", \"joy\", \"sadness\", \"anger\"]))\n    .with_columns(pl.col(\"label\").replace_strict(label_mapping, default=\"other\").cast(pl.Int16))\n)\n\n# Create new dataset\nnew_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)\n\n# Update training arguments for incremental training\nnew_training_args = TrainingArguments(\n    output_dir='./results_incremental',\n    num_train_epochs=2,  # Fewer epochs since it's incremental\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch',\n    logging_dir='./logs_incremental',\n    learning_rate=2e-5,\n    load_best_model_at_end=True\n)\n\n# Define new trainer\nnew_trainer = Trainer(\n    model=model,\n    args=new_training_args,\n    train_dataset=new_dataset,\n    eval_dataset=val_dataset  # Validate on previous validation set\n)\n\n# Train on new data\nnew_trainer.train()\n\n# Evaluate after retraining\nnew_results = new_trainer.evaluate(test_dataset)\nprint(f\"Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}\")\n\n# Save the updated model\nmodel.save_pretrained(\"saved_model_incremental\")",
         "keep train pytorch model new datum",
         "I'm working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here",
         "use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "Keep training pytorch model on new data I'm working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here use pretrained word embeddings like BertForSequenceClassification These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning reducing the impact of unseen words Model Training with BERT Incremental training with least effort",
         "keep train pytorch model new datum ' m work text classification task decide use pytorch model purpose process mainly involve follow step load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new datum however every day need classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire model process timeconsume new comment lose validation would like create new dataset newly classify text continue train new datum new comment classify manually label correct use gpt online code write desire process however I m sure working expect I m make silly mistake happen main question could check proposse way solve problem work expect vectorizer face new token would loose original vectorizer full training process propose code dataset find use pretraine word embedding like bertforsequenceclassification embedding handle unseen token gracefully since map word continuous vector base semantic meaning reduce impact unseen word model train bert incremental training least effort",
         "Keep training pytorch model on new data I'm working on a text classification task and have decided to use a PyTorch model for this purpose The process mainly involves the following steps Load and process the text Use a TFIDF Vectorizer Build the neural network and save the TFIDF Vectorizer and model to predict new data However every day I need to classify new comments and correct any wrong classifications Currently my approach is to add the new comments with the correct classification to the dataset and retrain the entire model This process is timeconsuming and the new comments can be lost during validation I would like to create a new dataset with the newly classified texts and continue training over this new data the new comments are classified manually so each label is correct Using GPT and some online code i write the desired process however im not sure if its working as expected or im making some silly mistakes that should not happen So the mains questions are How could i check if the propossed way to solve this problem work as i expect What can i do with the vectorizer when it face new tokens can i just do a or i would loose the original vectorizer Here its the full training process Proposed code The dataset can be found here",
         "keep train pytorch model new datum ' m work text classification task decide use pytorch model purpose process mainly involve follow step load process text use tfidf vectorizer build neural network save tfidf vectorizer model predict new datum however every day need classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire model process timeconsume new comment lose validation would like create new dataset newly classify text continue train new datum new comment classify manually label correct use gpt online code write desire process however I m sure working expect I m make silly mistake happen main question could check proposse way solve problem work expect vectorizer face new token would loose original vectorizer full training process propose code dataset find",
         "keep train pytorch new datum",
         "6",
         "new,train,datum,pytorch,pytorch new",
         "BERT & Hugging Face Application"
        ],
        [
         "37",
         "78932356",
         "https://stackoverflow.com/questions/78932356",
         "Capitalized words in sentiment analysis",
         "<p>I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive .\nA common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .</p>\n<p>this is my current code :</p>\n<pre><code>from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)\n</code></pre>\n",
         "2024-08-30 00:00:00",
         "nlp,sentiment-analysis,bert-language-model,data-preprocessing",
         "-1",
         "134",
         "1",
         "78933236.0",
         "<p>If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data.</p>\n<p>If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis.</p>\n<p>If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out <a href=\"https://github.com/EliasK93/transformer-model-comparison-for-review-sentiment-regression\" rel=\"nofollow noreferrer\">this repository</a> as an example.</p>\n",
         "0.0",
         "101-1k",
         "2024",
         "from itertools import chain\n\ndef is_upper_case(text):\n  return [word for word in text.split() if word.isupper() and word != 'I']\n\nunique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\nprint(unique_upper_words)",
         "",
         "capitalize word sentiment analysis",
         "I'm currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code",
         "If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "Capitalized words in sentiment analysis I'm currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code If you are using a BERTbased model or any other LLM to do the actual classification I would recommend to not use any preprocessing at all at least when it comes to capitalization as these models were pretrained on nonpreprocessed data If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group ngrams and to simplify the analysis If you are thinking about having multiple classes to have a better distinction between the prediction I think it would make most sense if you switch to a sentiment regression instead of a classification where you predict a value in a continuous range This comes somewhat natural to the finetuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax so for your needs you can just skip that last step and directly use the model output Many python ML frameworks for finetuning or using language models have their own classes for regression tasks check out this repository as an example",
         "capitalize word sentiment analysis ' m currently work datum customer review product sephora task classify sentiment negative neutral positive common technique text preprocesse low case word situation upper case word like ' amazing ' hide significant emotion behind turn word low case cause information loss would happy opinion subject still low case word personally think create class distinction sentiment good good positive include importance upper case word current code use bertbased model llm actual classification would recommend use preprocessing least come capitalization model pretraine nonpreprocesse datum want kind analysis result label sentence could lowercase everything group ngram simplify analysis think multiple class well distinction prediction think would make sense switch sentiment regression instead classification predict value continuous range come somewhat natural finetune language model normal classification would take continuous output model map categorical class use something like softmax need skip last step directly use model output many python ml framework finetune use language model class regression task check repository example",
         "Capitalized words in sentiment analysis I'm currently working with data of customers reviews on products from Sephora my task to classify them to sentiments negative neutral positive A common technique of text preprocessing is to lower case all the words but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss would be happy for your opinion in the subject should i still lower case all the words i personally think about creating more classes and distinction between sentiments as good good than just positive to include the importance of this upper case words this is my current code",
         "capitalize word sentiment analysis ' m currently work datum customer review product sephora task classify sentiment negative neutral positive common technique text preprocesse low case word situation upper case word like ' amazing ' hide significant emotion behind turn word low case cause information loss would happy opinion subject still low case word personally think create class distinction sentiment good good positive include importance upper case word current code",
         "capitalize sentiment analysis",
         "9",
         "analysis,sentiment,sentiment analysis,capitalize,capitalize sentiment",
         "NLP Application"
        ],
        [
         "38",
         "78920095",
         "https://stackoverflow.com/questions/78920095",
         "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub'",
         "<p>I've been using LLAMA 2 for research for a few months now and I import as follows:</p>\n<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(&quot;cuda&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;,torch_dtype=&quot;auto&quot;)\nmodel = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;, torch_dtype=&quot;auto&quot;, load_in_4bit=True)\n</code></pre>\n<p>It has always worked. However, today it is showing the following error:\n<strong>RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/<strong>init</strong>.py)</strong></p>\n<p>Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.</p>\n",
         "2024-08-27 00:00:00",
         "python,nlp,huggingface-transformers,transformer-model,llama",
         "1",
         "6127",
         "1",
         "78920098.0",
         "<p>The error you're encountering is due to the <code>split_torch_state_dict_into_shards</code> function not being available in <code>huggingface-hub version &lt; 0.23.0</code>.</p>\n<p>This function is included starting from version <code>0.23.0</code>.</p>\n<p><strong>To resolve this issue, update the <code>huggingface-hub</code> library to version 0.23.0 or later</strong></p>\n<p>Also please install accelerate:</p>\n<pre><code>pip install accelerate==0.31.0\n</code></pre>\n<p>here is a git link: <strong><a href=\"https://github.com/run-llama/llama_index/discussions/14605\" rel=\"nofollow noreferrer\">https://github.com/run-llama/llama_index/discussions/14605</a></strong></p>\n",
         "4.0",
         "1k-10k",
         "2024",
         "from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = torch.device(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\",torch_dtype=\"auto\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token = \"token_key\", torch_dtype=\"auto\", load_in_4bit=True)",
         "split_torch_state_dict_into_shards\n---\nhuggingface-hub version < 0.23.0\n---\n0.23.0\n---\nhuggingface-hub\n---\npip install accelerate==0.31.0",
         "import name ' split_torch_state_dict_into_shards ' ' huggingface_hub '",
         "I've been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didn't work I am using Google Colab and Kaggle Notebook",
         "The error you're encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' I've been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didn't work I am using Google Colab and Kaggle Notebook The error you're encountering is due to the function not being available in This function is included starting from version To resolve this issue update the library to version 0230 or later Also please install accelerate here is a git link",
         "import name ' split_torch_state_dict_into_shards ' ' huggingface_hub ' ' ve use llama 2 research month import follows always work however today show follow error runtimeerror fail import transformersmodelsllamamodeling_llama follow error look see traceback fail import transformersgenerationutil follow error look see traceback import name ' split_torch_state_dict_into_shards ' ' huggingface_hub ' /opt / conda / lib / python310 / sitepackage / huggingface_hub/ init py recreate hug face token not work use google colab kaggle notebook error be encounter due function available function include start version resolve issue update library version 0230 later also please install accelerate git link",
         "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' I've been using LLAMA 2 for research for a few months now and I import as follows It has always worked However today it is showing the following error RuntimeError Failed to import transformersmodelsllamamodeling_llama because of the following error look up to see its traceback Failed to import transformersgenerationutils because of the following error look up to see its traceback cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' /opt/conda/lib/python310/sitepackages/huggingface_hub/ init py Recreated the Hugging Face token but it didn't work I am using Google Colab and Kaggle Notebook",
         "import name ' split_torch_state_dict_into_shards ' ' huggingface_hub ' ' ve use llama 2 research month import follows always work however today show follow error runtimeerror fail import transformersmodelsllamamodeling_llama follow error look see traceback fail import transformersgenerationutil follow error look see traceback import name ' split_torch_state_dict_into_shards ' ' huggingface_hub ' /opt / conda / lib / python310 / sitepackage / huggingface_hub/ init py recreate hug face token not work use google colab kaggle notebook",
         "import name splittorchstatedictintoshards huggingfacehub",
         "2",
         "import,huggingfacehub,splittorchstatedictintoshards,splittorchstatedictintoshards huggingfacehub,import splittorchstatedictintoshards",
         "Handling Error in NLP Task"
        ],
        [
         "39",
         "78917743",
         "https://stackoverflow.com/questions/78917743",
         "How to Process Data on GPU Instead of RAM for This Python Code?",
         "<p>I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance.\nmy code :</p>\n<pre><code>def prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n    batch[&quot;input_features&quot;] = feature_extractor(\n        audio[&quot;array&quot;], \n        sampling_rate=audio[&quot;sampling_rate&quot;]\n    ).input_features[0]\n    batch[&quot;labels&quot;] = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[&quot;train&quot;], \n    num_proc=1\n)\n</code></pre>\n<p>How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!</p>\n",
         "2024-08-27 00:00:00",
         "nlp,gpu,torch,openai-whisper",
         "1",
         "64",
         "1",
         "78918851.0",
         "<p>you can using the following code to process audio data on GPU</p>\n<pre><code>import torch\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[&quot;audio&quot;]\n\n    input_features = feature_extractor(audio[&quot;array&quot;], sampling_rate=audio[&quot;sampling_rate&quot;]).input_features[0]\n    batch[&quot;input_features&quot;] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[&quot;sentence&quot;]).input_ids\n    batch[&quot;labels&quot;] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[&quot;train&quot;])\n</code></pre>\n",
         "2.0",
         "11-100",
         "2024",
         "def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n    batch[\"input_features\"] = feature_extractor(\n        audio[\"array\"], \n        sampling_rate=audio[\"sampling_rate\"]\n    ).input_features[0]\n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\ncommon_voice = common_voice.map(\n    prepare_dataset, \n    remove_columns=common_voice.column_names[\"train\"], \n    num_proc=1\n)",
         "import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    input_features = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n    batch[\"input_features\"] = torch.tensor(input_features).to(device)\n\n    labels = tokenizer(batch[\"sentence\"]).input_ids\n    batch[\"labels\"] = torch.tensor(labels).to(device)\n    return batch\n\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"])",
         "process datum gpu instead ram python code",
         "I'm currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated",
         "you can using the following code to process audio data on GPU",
         "How to Process Data on GPU Instead of RAM for This Python Code I'm currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated you can using the following code to process audio data on GPU",
         "process datum gpu instead ram python code ' m currently use follow code process audio data run ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific change much appreciate use follow code process audio data gpu",
         "How to Process Data on GPU Instead of RAM for This Python Code I'm currently using the following code to process audio data but it runs on the RAM I want to offload the processing to the GPU to improve performance my code How can I modify this code to utilize the GPU for processing instead of the RAM Any guidance or specific changes are much appreciated",
         "process datum gpu instead ram python code ' m currently use follow code process audio data run ram want offload processing gpu improve performance code modify code utilize gpu processing instead ram guidance specific change much appreciate",
         "process datum gpu instead ram python",
         "4",
         "process,python,datum,instead ram,gpu instead",
         "Preprocessing Text in Python"
        ],
        [
         "40",
         "78912171",
         "https://stackoverflow.com/questions/78912171",
         "How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation",
         "<p>I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this.</p>\n<p><strong>What I’ve Tried:</strong></p>\n<ul>\n<li><p><strong>Initial Attempt:</strong> I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me.</p>\n</li>\n<li><p><strong>Using Forward Hooks:</strong> To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation:</p>\n</li>\n</ul>\n<pre class=\"lang-py prettyprint-override\"><code># VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = &quot;Helsinki-NLP/opus-mt-en-de&quot;\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = &quot;Please translate this to German.&quot;\ninputs = tokenizer(input_text, return_tensors=&quot;pt&quot;)\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(&quot;layers, heads, output tokens, input tokens&quot;)\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(&quot;Input Tokens&quot;)\nplt.ylabel(&quot;Output Tokens&quot;)\nplt.title(&quot;Cross-Attention Matrix&quot;)\nplt.show()\n</code></pre>\n<p>This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation.</p>\n<p><strong>My Question</strong></p>\n<p>Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8?</p>\n<p>I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.</p>\n",
         "2024-08-25 00:00:00",
         "python,pytorch,nlp,huggingface-transformers",
         "1",
         "402",
         "1",
         "78915504.0",
         "<p>Huggingface has built in methods to return attention weights</p>\n<pre class=\"lang-py prettyprint-override\"><code>translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n&gt; odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n</code></pre>\n<p>With <code>return_dict_in_generate=True</code>, <code>model.generate</code> returns a dict-like object. With <code>output_attentions=True</code>, the output dict will contain all attention weights.</p>\n<p>For this model, it will include encoder attentions, decoder attentions and cross attentions.</p>\n",
         "3.0",
         "101-1k",
         "2024",
         "# VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nmodel_name = \"Helsinki-NLP/opus-mt-en-de\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.eval()\n\nkeys = {}\nqueries = {}\n\ndef get_key(layer):\n    def hook(module, input, output):\n        key, = input\n        keys[layer] = key\n    return hook\n\ndef get_query(layer):\n    def hook(module, input, output):\n        query, = input\n        queries[layer] = query\n    return hook\n\ndef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\nhooks = []\nfor i, layer in enumerate(model.model.decoder.layers):\n    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))\n    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))\n\ninput_text = \"Please translate this to German.\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\ntranslated_tokens = model.generate(**inputs, use_cache=False)\n\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n\ninput_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\noutput_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])\n\nattentions = []\nfor layer in range(len(keys)):\n    K, Q = keys[layer], queries[layer]\n    M = Q @ K.transpose(-2, -1)\n    attentions.append(F.softmax(M, dim=-1))\n\nattentions = torch.stack(attentions, dim=0)\n\nprint(\"layers, heads, output tokens, input tokens\")\nprint(attentions.shape)\nplt.figure(figsize=(10, 8))\nplt.imshow(attentions[0, 0], cmap='viridis')\nplt.colorbar()\n\nplt.xticks(range(len(input_tokens)), input_tokens, rotation=90)\nplt.yticks(range(len(output_tokens)), output_tokens)\n\nplt.xlabel(\"Input Tokens\")\nplt.ylabel(\"Output Tokens\")\nplt.title(\"Cross-Attention Matrix\")\nplt.show()",
         "translated_tokens = model.generate(**inputs, \n                                   output_attentions=True,\n                                   return_dict_in_generate=True\n                                  )\n\nprint(translated_tokens.keys())\n> odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\n---\nreturn_dict_in_generate=True\n---\nmodel.generate\n---\noutput_attentions=True",
         "visualize crossattention matrix marianmtmodel output generation",
         "I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the model's translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that I'm currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this",
         "Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the model's translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that I'm currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this Huggingface has built in methods to return attention weights With returns a dictlike object With the output dict will contain all attention weights For this model it will include encoder attentions decoder attentions and cross attentions",
         "visualize crossattention matrix marianmtmodel output generation work machine translation task use marianmtmodel hug face transformer library specifically want visualize crossattention matrix model 's translation process however encounter difficulty achieve I ve try initial attempt notice crossattention matrix directly return model generate translation example find involved feeding source text translation model however goal access crossattention matrix model generate output translation give use forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache use_cache = false capture full matrix last step here implementation approach seem work capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question give issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate ' m currently reshape key k query q tensor head dimension multiplication want ask advice case there s easy effective way huggingface build method return attention weight return dictlike object output dict contain attention weight model include encoder attention decoder attention cross attention",
         "How to Visualize CrossAttention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library Specifically I want to visualize the crossattention matrices during the model's translation process However I encountered some difficulties in achieving this What Ive Tried Initial Attempt I noticed that the crossattention matrices are not directly returned when the model generates a translation The only example I found involved feeding both the source text and the translation to the model However my goal is to access the crossattention matrices while the model generates the output not for a translation given by me Using Forward Hooks To achieve this I implemented forward hooks on both the key and query projections of the attention mechanism while disabling the keyvalue caching use_cache=False to capture the full matrices at the last step Heres my implementation This approach seemed to work in capturing the crossattention matrices However I observed that the matrices only have 4 attention heads instead of the expected 8 This makes me question the correctness of my implementation My Question Given the issues Ive encountered is there a more reliable method to extract and visualize the crossattention matrices during the translation process Additionally if my current approach is fundamentally okay how can I resolve the issue of capturing only 4 attention heads instead of 8 I suspect that the issue might be related to that I'm currently not reshaping the key K and query Q tensors to the head dimension before multiplication but I wanted to ask for advice in case theres an easier or more effective way to do this",
         "visualize crossattention matrix marianmtmodel output generation work machine translation task use marianmtmodel hug face transformer library specifically want visualize crossattention matrix model 's translation process however encounter difficulty achieve I ve try initial attempt notice crossattention matrix directly return model generate translation example find involved feeding source text translation model however goal access crossattention matrix model generate output translation give use forward hook achieve implement forward hook key query projection attention mechanism disable keyvalue cache use_cache = false capture full matrix last step here implementation approach seem work capture crossattention matrix however observe matrix 4 attention head instead expect 8 make question correctness implementation question give issue I ve encounter reliable method extract visualize crossattention matrix translation process additionally current approach fundamentally okay resolve issue capture 4 attention head instead 8 suspect issue might relate ' m currently reshape key k query q tensor head dimension multiplication want ask advice case there s easy effective way",
         "visualize crossattention matrix marianmtmodel generation",
         "6",
         "generation,matrix,marianmtmodel,crossattention matrix,visualize crossattention",
         "BERT & Hugging Face Application"
        ],
        [
         "41",
         "78905614",
         "https://stackoverflow.com/questions/78905614",
         "Why doesn't permuting positional encodings in BERT affect the output as expected?",
         "<p>I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another <a href=\"https://stackoverflow.com/questions/78902301/why-doesnt-permuting-positional-encodings-in-gpt-2-affect-the-output-as-expecte/78903454#78903454\">question</a> I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results.</p>\n<p><strong>What I expected to happen:</strong></p>\n<ul>\n<li>No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary.</li>\n<li>Permuting only the input IDs should return distribution B.</li>\n<li>Permuting only the positional embeddings should return distribution B.</li>\n<li>Permuting both the input IDs and positional embeddings should return distribution A.</li>\n</ul>\n<p><strong>What actually happens:</strong>\nSometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.</p>\n<p><strong>My question is:</strong> Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding?</p>\n<p>For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in <code>masked_prediction</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(&quot;transformers.modeling_utils&quot;).setLevel(logging.ERROR)\nwarnings.simplefilter(&quot;ignore&quot;, FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) &gt; 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(&quot;You can't use an empty sequence for prediction&quot;)\n            return\n        \n        model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(&quot;You need to include a [MASK] token for prediction&quot;)\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=&quot;Run Prediction&quot;)\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)\n</code></pre>\n",
         "2024-08-23 00:00:00",
         "python,pytorch,nlp,huggingface-transformers",
         "1",
         "80",
         "1",
         "78906902.0",
         "<p>The model inputs have token ids and position ids. There are four scenarios to consider:</p>\n<ol>\n<li>Baseline. Correct order for tokens and positions</li>\n<li>Permute position ids only</li>\n<li>Permute token ids only</li>\n<li>Permute position ids and token ids</li>\n</ol>\n<p>You are correct that scenario 1 and 4 should produce the same results. However you are incorrect in assuming that permuting tokens or positions separately should give the same result. Consider:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n</code></pre>\n<p>In <code>Ex1</code>, the model is told that token <code>2</code> occurs at position <code>0</code>. In <code>Ex2</code>, the model is told that token <code>2</code> occurs at position <code>1</code>. Even though we used the same permutation, the mapping of tokens to positions is different. This results in different model outputs.</p>\n<p>The reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. This is luck - the average case produces different results.</p>\n<p>It is simple to test this. Huggingface models take a <code>position_ids</code> input parameter. We can use this to test permutations of the input ids without messing with the weight matrices.</p>\n<p>To test this, we'll create input data, permute as needed, compute logits and compare logits.</p>\n<p>When comparing logits, we will permute or depermute as needed to compare on a token to token basis. For example if token <code>i</code> in scenario 1 is permuted to token <code>j</code> in scenario 3, we want to compare logits <code>i</code> from scenario 1 to logits <code>j</code> in scenario 3.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f&quot;Comparison {sa}, {sb}: {val.item():.6f}&quot;)\n</code></pre>\n<p>The code should produce an output like:</p>\n<pre><code>Comparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n</code></pre>\n<p>Run the code a bunch of times. You will find that the <code>S1, S4</code> comparison <em>always</em> has a small deviation. This is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues.</p>\n<p>You will find the <code>S2, S3</code> comparison generally has a large deviation, but <em>sometimes</em> has a small deviation. As discussed, this is due to getting a lucky permutation where positions and ids mostly line up.</p>\n",
         "2.0",
         "11-100",
         "2024",
         "masked_prediction\n---\nimport torch\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom transformers import BertForMaskedLM, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# surpress renaming warnings\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ninput_ids = torch.Tensor([[]])\ntokens = []\npermutation = []\n\noutput = widgets.Output()\n\ndef permute_columns(matrix, permutation=None):\n    n = len(permutation)\n    first_n_columns = matrix[:, :n]\n    permuted_columns = first_n_columns[:, permutation]\n    remaining_columns = matrix[:, n:]\n    new_matrix = torch.hstack((permuted_columns, remaining_columns))\n    return new_matrix\n\ndef update_permutation(ordered_tags):\n    global permutation\n    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]\n    \n    permutation = [tokens.index(tag) for tag in fixed_tokens]\n    \n\ndef tokenize(text):\n    global input_ids, tokens\n    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]\n    \n    if len(tokens) > 2:\n        reorderable_tokens = tokens[1:-1]\n    else:\n        reorderable_tokens = []\n    \n    with output:\n        output.clear_output(wait=True)\n        tags_input.allowed_tags = reorderable_tokens\n        tags_input.value = reorderable_tokens\n        update_permutation(tags_input.value)\n\ndef on_tags_change(change):\n    if len(change['new']) != len(tags_input.allowed_tags):\n        tags_input.value = tags_input.allowed_tags  # Restore original value\n\n\ndef masked_prediction(input_ids, permutation, permute_input, permute_encoding):\n    \n    with output:\n        output.clear_output(wait=True)  # Clear previous outputs\n        \n        if input_ids.numel() == 0:\n            print(\"You can't use an empty sequence for prediction\")\n            return\n        \n        model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n        \n        if permute_encoding:\n            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T\n        if permute_input:\n            input_ids = permute_columns(input_ids, permutation)\n            \n        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n            \n        with torch.no_grad():\n            outputs = model(input_ids)\n            \n        logits = outputs.logits\n\n        top_k = 5\n\n        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n        print(decoded_text, mask_token_indices, permutation)\n        num_masks = len(mask_token_indices)\n        if num_masks == 0:\n            print(\"You need to include a [MASK] token for prediction\")\n            return\n\n        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))\n        \n        if num_masks == 1:\n            axs = [axs]\n\n        for i, idx in enumerate(mask_token_indices):\n            mask_token_logits = logits[0, idx, :]\n\n            softmax_probs = F.softmax(mask_token_logits, dim=0)\n\n            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)\n\n            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]\n            predicted_confidences = top_token_probs.tolist()\n\n            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')\n            axs[i].set_xlabel('Predicted Tokens')\n            axs[i].set_ylabel('Confidence')\n            axs[i].set_title(f'Masked Token at Position {idx.item()}')\n            axs[i].set_ylim(0, 1)\n\n        plt.show()\n\ndef on_predict_button_click(b):\n    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)\n\ntext_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')\ntext_input.observe(lambda change: tokenize(change['new']), names='value')\ntags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)\n\n# Observe changes in tags order to update the permutation and prevent deletion\ntags_input.observe(on_tags_change, names='value')\ntags_input.observe(lambda change: update_permutation(change['new']), names='value')\n\n# Create checkboxes for permute_input and permute_encoding\npermute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')\npermute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')\n\n# Create a button to trigger the prediction\npredict_button = widgets.Button(description=\"Run Prediction\")\npredict_button.on_click(on_predict_button_click)\n\n# Display the widgets\ndisplay(text_input)\ndisplay(tags_input)\ndisplay(permute_input_checkbox)\ndisplay(permute_encoding_checkbox)\ndisplay(predict_button)\ndisplay(output)",
         "# Given:\ntokens = [0, 1, 2]\npositions = [0, 1, 2]\npermutation = [2, 0, 1]\n\n# Ex1: Permute tokens but not positions\n[2, 0, 1] # permuted tokens\n[0, 1, 2] # standard positions\n\n# Ex2: Permute positions but not tokens\n[0, 1, 2] # standard tokens\n[2, 0, 1] # permuted positions\n---\nEx1\n---\n2\n---\n0\n---\nEx2\n---\n2\n---\n1\n---\nposition_ids\n---\ni\n---\nj\n---\ni\n---\nj\n---\nimport torch\nfrom transformers import BertForMaskedLM, AutoTokenizer\n\ndef get_logits(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)  \n        logits = outputs.logits\n    return logits\n\ndef permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):\n    outputs = {}\n    for k,v in inputs.items():\n        if k=='position_ids' and permute_positions:\n            outputs[k] = v[permutation]\n        elif k!='position_ids' and permute_ids:\n            outputs[k] = v[:,permutation]\n        else:\n            outputs[k] = v\n            \n    return outputs\n\n# load tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\nmodel.eval() # remember to set model to eval\n\n# create input ids and position ids\ninputs = tokenizer('input text test sequence', return_tensors='pt')\n\ninputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))\n\n# create permutation tensor\npermutation = torch.randperm(inputs['input_ids'].shape[1])\n\n# compute scenario data\ndata = {\n    's1' : { # scenario 1 - baseline\n        'inputs' : inputs,\n        'permuted_ids' : False\n    },\n    's2' : { # scenario 2 - permute positions only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),\n        'permuted_ids' : False\n    },\n    's3' : { # scenario 3 - permute token ids only\n        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),\n        'permuted_ids' : True\n    },\n    's4' : { # scenario 4 - permute tokens and positions\n        'inputs' : permute_inputs(inputs, permutation),\n        'permuted_ids' : True\n    }\n}\n\n# compute logits\nfor k,v in data.items():\n    v['logits'] = get_logits(v['inputs'])\n\ncomparisons = [\n    ['s1', 's2'],\n    ['s1', 's3'],\n    ['s1', 's4'],\n    ['s2', 's3'],\n    ['s2', 's4'],\n    ['s3', 's4'],\n]\n\n# compare scenarios \nfor sa, sb in comparisons:\n    data_a = data[sa]\n    data_b = data[sb]\n    \n    logits_a = data_a['logits']\n    logits_b = data_b['logits']\n    \n    if data_a['permuted_ids'] == data_b['permuted_ids']:\n        # either both logits are permuted or both logits are unpermuted\n        # so we can compare directly\n        val = (logits_a - logits_b).abs().mean()\n    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):\n        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up\n        val = (logits_a - logits_b[:,permutation]).abs().mean()\n    else:\n        # otherwise we permute `b` to make tokens line up\n        val = (logits_a[:,permutation] - logits_b).abs().mean()\n        \n    print(f\"Comparison {sa}, {sb}: {val.item():.6f}\")\n---\nComparison s1, s2: 1.407895\nComparison s1, s3: 1.583560\nComparison s1, s4: 0.000003\nComparison s2, s3: 1.750883\nComparison s2, s4: 1.407894\nComparison s3, s4: 1.583560\n---\nS1, S4\n---\nS2, S3",
         "not permute positional encoding bert affect output expect",
         "I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that don't use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Face's BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in",
         "The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this we'll create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "Why doesn't permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that don't use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Face's BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in The model inputs have token ids and position ids There are four scenarios to consider Baseline Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results However you are incorrect in assuming that permuting tokens or positions separately should give the same result Consider In the model is told that token occurs at position In the model is told that token occurs at position Even though we used the same permutation the mapping of tokens to positions is different This results in different model outputs The reason you sometimes see these results line up is because you can through random chance sample a permutation that results in token/position embeddings lining up the same way or mostly the same way when permuting just one of them This is luck the average case produces different results It is simple to test this Huggingface models take a input parameter We can use this to test permutations of the input ids without messing with the weight matrices To test this we'll create input data permute as needed compute logits and compare logits When comparing logits we will permute or depermute as needed to compare on a token to token basis For example if token in scenario 1 is permuted to token in scenario 3 we want to compare logits from scenario 1 to logits in scenario 3 The code should produce an output like Run the code a bunch of times You will find that the comparison always has a small deviation This is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues You will find the comparison generally has a large deviation but sometimes has a small deviation As discussed this is due to getting a lucky permutation where positions and ids mostly line up",
         "not permute positional encoding bert affect output expect work jupyter notebook transformer section positional encoding want demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply model not use mask attention like gpt2 however attempt approach bert model use crossattention predict mask token encounter unexpected result expect happen permutation cause model predict different token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes result align expectation time permute one aspect either input ids positional embedding lead different outcome even though occasionally produce result question something else hug face 's bert model might influence position beyond positional encoding completeness include full code part notebook try directly important part happen model input token id position id four scenario consider baseline correct order token position permute position ids permute token ids permute position ids token ids correct scenario 1 4 produce result however incorrect assuming permute tokens position separately give result consider model tell token occur position model tell token occur position even though use permutation mapping token position different result different model output reason sometimes see result line random chance sample permutation result token / position embedding line way mostly way permute one luck average case produce different result simple test huggingface model take input parameter use test permutation input id without mess weight matrix test will create input datum permute need compute logit compare logit compare logit permute depermute need compare token token basis example token scenario 1 permute token scenario 3 want compare logits scenario 1 logits scenario 3 code produce output like run code bunch time find comparison always small deviation permute token position together always produce result ignore small deviation cause numeric issue find comparison generally large deviation sometimes small deviation discuss due get lucky permutation position id mostly line",
         "Why doesn't permuting positional encodings in BERT affect the output as expected I am working on a Jupyter notebook about Transformers In the section on positional encodings I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence I previously learned from another question I posted that this concept only applies to models that don't use masked attention like GPT2 However when I attempted the same approach with a BERT model which uses crossattention to predict a MASK token I encountered unexpected results What I expected to happen No permutation should cause the model to predict a different token ie distribution A should be consistent over the vocabulary Permuting only the input IDs should return distribution B Permuting only the positional embeddings should return distribution B Permuting both the input IDs and positional embeddings should return distribution A What actually happens Sometimes the results align with my expectations but other times permuting one aspect either the input IDs or positional embeddings leads to different outcomes even though occasionally they produce the same result My question is Is there something else in Hugging Face's BERT model that might be influenced by position beyond just the positional encoding For completeness I have included the full code from this part of the notebook below so it can be tried out directly The Important part happens in",
         "not permute positional encoding bert affect output expect work jupyter notebook transformer section positional encoding want demonstrate transformer rely entirely positional encoding understand order sequence previously learn another question post concept apply model not use mask attention like gpt2 however attempt approach bert model use crossattention predict mask token encounter unexpected result expect happen permutation cause model predict different token ie distribution consistent vocabulary permuting input id return distribution b permute positional embedding return distribution b permuting input ids positional embedding return distribution actually happen sometimes result align expectation time permute one aspect either input ids positional embedding lead different outcome even though occasionally produce result question something else hug face 's bert model might influence position beyond positional encoding completeness include full code part notebook try directly important part happen",
         "not permute positional encoding bert affect expect",
         "6",
         "encoding,positional,permute,affect expect,bert",
         "BERT & Hugging Face Application"
        ],
        [
         "42",
         "78901998",
         "https://stackoverflow.com/questions/78901998",
         "How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus?",
         "<p>I'm working with the <code>OpenAIEmbeddings()</code> class from <code>OpenAI</code>, which uses the <code>text-embedding-3-small</code> model. According to the <a href=\"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\" rel=\"nofollow noreferrer\">documentation</a>, it generates a 1536-dimensional vector for any input text.</p>\n<p>However, I'm a bit confused about how this works:</p>\n<ul>\n<li>Is the 1536-dimensional vector generated for the entire input text?</li>\n<li>If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</li>\n</ul>\n<p><strong>I was expecting this:</strong></p>\n<p>If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536.</p>\n<p>But the output is a single vector of size 1536 for the whole input text.</p>\n<p>Why I expected this?</p>\n<p>Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings?</p>\n<p>I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input.</p>\n<p>Any insights or examples would be greatly appreciated!</p>\n",
         "2024-08-22 00:00:00",
         "deep-learning,nlp,openai-api,openaiembeddings",
         "2",
         "616",
         "1",
         "78902136.0",
         "<p>Everything you described is 100% expected.</p>\n<h3>Q: Is the 1536-dimensional vector generated for the entire input text?</h3>\n<p>A: Yes.</p>\n<h3>Q: If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</h3>\n<p>A: First, the OpenAI Embeddings model doesn't handle a single word any different than a long text. For the model, it's an input. The input can be even a single character (e.g., &quot;a&quot;), but it doesn't make sense to calculate an embedding vector out of it since &quot;a&quot; doesn't semantically mean anything to us humans.</p>\n<p>Second, what you probably meant with this question is what happens when you do a similarity search with these embeddings. In other words, what happens when you <em>use</em> them? What happens if you use embeddings of words, sentences, paragraphs, or the whole text? Does it matter? Yes!</p>\n<p>This is called chunking. The decision about how to chunk your text depends on the use case. The best thing is probably to simply try and see. If you get meaningful results after doing a similarity search, then this means that chunking is appropriate (even if this means chunking the whole text). If you don't get meaningful results after doing a similarity search, then this means that chunking isn't appropriate (e.g., instead of chunking by paragraph, try chunking by sentences).</p>\n<p>There's an excellent Stack Overflow <a href=\"https://stackoverflow.blog/2024/06/06/breaking-up-is-hard-to-do-chunking-in-rag-applications/\">blog post</a> about this topic you should read (pay attention to the bolded text because this is the best explanation):</p>\n<blockquote>\n<p>With RAG, you create text embeddings of the pieces of data that you\nwant to draw from and retrieve. That allows you to place a piece of\nthe source text within the semantic space that LLMs use to create\nresponses.</p>\n<p>/.../</p>\n<p>When it comes to RAG systems, you’ll need to pay special attention to\nhow big the individual pieces of data are. How you divide your data up\nis called chunking, and it’s more complex than embedding whole\ndocuments.</p>\n<p>/.../</p>\n<p>The size of the chunked data is going to make a huge difference in\nwhat information comes up in a search. When you embed a piece of data,\nthe whole thing is converted into a vector. <strong>Include too much in a\nchunk and the vector loses the ability to be specific to anything it\ndiscusses. Include too little and you lose the context of the data.</strong></p>\n</blockquote>\n",
         "3.0",
         "101-1k",
         "2024",
         "OpenAIEmbeddings()\n---\nOpenAI\n---\ntext-embedding-3-small",
         "",
         "openaiembedding work create single vector size 1536 whole text corpus",
         "I'm working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However I'm a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated",
         "Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesn't handle a single word any different than a long text For the model it's an input The input can be even a single character eg a but it doesn't make sense to calculate an embedding vector out of it since a doesn't semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you don't get meaningful results after doing a similarity search then this means that chunking isn't appropriate eg instead of chunking by paragraph try chunking by sentences There's an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus I'm working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However I'm a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated Everything you described is 100% expected Q Is the 1536dimensional vector generated for the entire input text A Yes Q If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs A First the OpenAI Embeddings model doesn't handle a single word any different than a long text For the model it's an input The input can be even a single character eg a but it doesn't make sense to calculate an embedding vector out of it since a doesn't semantically mean anything to us humans Second what you probably meant with this question is what happens when you do a similarity search with these embeddings In other words what happens when you use them What happens if you use embeddings of words sentences paragraphs or the whole text Does it matter Yes This is called chunking The decision about how to chunk your text depends on the use case The best thing is probably to simply try and see If you get meaningful results after doing a similarity search then this means that chunking is appropriate even if this means chunking the whole text If you don't get meaningful results after doing a similarity search then this means that chunking isn't appropriate eg instead of chunking by paragraph try chunking by sentences There's an excellent Stack Overflow blog post about this topic you should read pay attention to the bolded text because this is the best explanation With RAG you create text embeddings of the pieces of data that you want to draw from and retrieve That allows you to place a piece of the source text within the semantic space that LLMs use to create responses // When it comes to RAG systems youll need to pay special attention to how big the individual pieces of data are How you divide your data up is called chunking and its more complex than embedding whole documents // The size of the chunked data is going to make a huge difference in what information comes up in a search When you embed a piece of data the whole thing is converted into a vector Include too much in a chunk and the vector loses the ability to be specific to anything it discusses Include too little and you lose the context of the data",
         "openaiembedding work create single vector size 1536 whole text corpus ' m work class use model accord documentation generate 1536dimensional vector input text however ' m bit confuse work 1536dimensional vector generate entire input text 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph expect 100 word input text expect openaiembedding would output 100 vector size 1536 output single vector size 1536 whole input text expect learning ' ve understand embedding like word2vec glove provide vector word corpus differ approach take openaiembedding ' m try understand whether 's way extract embedding individual word use model output always single vector represent whole input insight example would greatly appreciate everything describe 100 % expect q 1536dimensional vector generate entire input text yes q 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph first openai embedding model not handle single word different long text model 's input input even single character eg not make sense calculate embed vector since not semantically mean anything we human second probably mean question happen similarity search embedding word happen use happen use embedding word sentence paragraph whole text matter yes call chunk decision chunk text depends use case good thing probably simply try see get meaningful result similarity search mean chunk appropriate even mean chunk whole text not get meaningful result similarity search mean chunk not appropriate eg instead chunk paragraph try chunk sentence 's excellent stack overflow blog post topic read pay attention bolde text good explanation rag create text embedding piece datum want draw retrieve allow place piece source text within semantic space llm use create response // come rag system you ll need pay special attention big individual piece datum divide datum call chunk complex embed whole document // size chunk datum go make huge difference information come search embed piece datum whole thing convert vector include much chunk vector lose ability specific anything discuss include little lose context datum",
         "How does OpenAIEmbeddings work Is it creating a single vector of size 1536 for whole text corpus I'm working with the class from which uses the model According to the documentation it generates a 1536dimensional vector for any input text However I'm a bit confused about how this works Is the 1536dimensional vector generated for the entire input text If the 1536dimensional vector represents the entire input text how does the model handle individual words versus longer texts like sentences or paragraphs I was expecting this If there are 100 words in my input text i expected that OpenAIEmbeddings would output 100 vectors each having size 1536 But the output is a single vector of size 1536 for the whole input text Why I expected this Because in my learning i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus How does this differ from the approach taken by OpenAIEmbeddings I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input Any insights or examples would be greatly appreciated",
         "openaiembedding work create single vector size 1536 whole text corpus ' m work class use model accord documentation generate 1536dimensional vector input text however ' m bit confuse work 1536dimensional vector generate entire input text 1536dimensional vector represent entire input text model handle individual word versus long text like sentence paragraph expect 100 word input text expect openaiembedding would output 100 vector size 1536 output single vector size 1536 whole input text expect learning ' ve understand embedding like word2vec glove provide vector word corpus differ approach take openaiembedding ' m try understand whether 's way extract embedding individual word use model output always single vector represent whole input insight example would greatly appreciate",
         "openaiembedding create single vector size 1536 whole corpus",
         "6",
         "size,create single,vector,1536 corpus,openaiembedding create",
         "BERT & Hugging Face Application"
        ],
        [
         "43",
         "78895710",
         "https://stackoverflow.com/questions/78895710",
         "NER versus LLM to extract name, gender, role and company from text",
         "<p>I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons.</p>\n<p>I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them.</p>\n<p>Is there another, smaller LLM that might be good at this while using fewer processing resources?</p>\n<p>Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.)</p>\n<p>Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass?</p>\n<p>Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction.</p>\n<p>Thanks in advance!</p>\n",
         "2024-08-21 00:00:00",
         "nlp,large-language-model,named-entity-recognition",
         "1",
         "1628",
         "2",
         "78896098.0",
         "<p>Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. <code>NER</code> is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use <code>NLTK</code> or <code>SpaCy</code> for this matter. My personal choice is <code>SpaCy</code>, however a <code>gender</code> as you defined is not a known named entity. you can see a list of named entities in <a href=\"https://github.com/explosion/spaCy/discussions/9147\" rel=\"nofollow noreferrer\">this doc</a>.</p>\n<p>I guess what you mean by <code>gender</code> is the possible <code>gender</code> associated with the names of a <code>PERSON</code> mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use <a href=\"https://pypi.org/project/gender-guesser/\" rel=\"nofollow noreferrer\"><code>gender-guesser</code> package</a>.</p>\n<p>A possible solution would be like this:</p>\n<pre><code>import spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == &quot;PERSON&quot;:\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n</code></pre>\n<p>Note that <code>en_core_web_sm</code> is the small model available via spaCy, you can use the large model by specifying <code>en_core_web_lg</code>, just make sure that the model is downloaded before running your code. here's how you can download the model:</p>\n<pre><code>python -m spacy download en_core_web_sm\n</code></pre>\n",
         "1.0",
         "1k-10k",
         "2024",
         "",
         "NER\n---\nNLTK\n---\nSpaCy\n---\nSpaCy\n---\ngender\n---\ngender\n---\ngender\n---\nPERSON\n---\ngender-guesser\n---\nimport spacy\nimport gender_guesser.detector as gender\n\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_info(text):\n    doc = nlp(text)\n    gender_detector = gender.Detector()\n\n    for ent in doc.ents:\n        if ent.label_ == \"PERSON\":\n            name = ent.text\n            name_gender = gender_detector.get_gender(name)\n    \n    return doc.ents, name_gender\n---\nen_core_web_sm\n---\nen_core_web_lg\n---\npython -m spacy download en_core_web_sm",
         "ner versus llm extract name gender role company text",
         "I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons I've been playing around with Llama 31 but I'm finding I don't get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs I've looked into extract name but not gender I don't know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point I'm much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance",
         "Apart from your limitations I wouldn't recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code here's how you can download the model",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons I've been playing around with Llama 31 but I'm finding I don't get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs I've looked into extract name but not gender I don't know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point I'm much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance Apart from your limitations I wouldn't recommend using LLMs like Llamma 31 for such a task is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal You can use or for this matter My personal choice is however a as you defined is not a known named entity you can see a list of named entities in this doc I guess what you mean by is the possible associated with the names of a mentioned in your articles There are a few python packages that you can use to lookup genders however you should note that this can be ambiguous and there should be a substantial tolerance for error You can use package A possible solution would be like this Note that is the small model available via spaCy you can use the large model by specifying just make sure that the model is downloaded before running your code here's how you can download the model",
         "ner versus llm extract name gender role company text need extract name gender job title employer / company name newspaper article running process local hardware cloud allow due copyright reason ' ve play around llama 31 ' m finding not get useable result model small 70b parameter size model run much slowly good hardware throw another small llm might good use few processing resource ner use extract datum ner ' ve look extract name gender not know extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get well result fast single llm pass answer training model good model use start point ' m much begin machine learn journey would love point right direction thank advance apart limitation would not recommend use llm like llamma 31 task one classic task nlp small language model tool incorporate achieve goal use matter personal choice however define known name entity see list name entity doc guess mean possible associate name mention article python package use lookup gender however note ambiguous substantial tolerance error use package possible solution would like note small model available via spacy use large model specify make sure model download run code 's download model",
         "NER versus LLM to extract name gender role and company from text I need to extract the name gender job title and employer/company name from newspaper articles running the process on local hardware no Cloud allowed due to copyright reasons I've been playing around with Llama 31 but I'm finding I don't get useable results with the models smaller than 70B parameters and at that size the models run much too slowly on the best hardware I have to throw at them Is there another smaller LLM that might be good at this while using fewer processing resources Is there is NER I can use to extract all that data The NERs I've looked into extract name but not gender I don't know if they extract the other data because gender is a showstopper for me Alternatively is there an approach I can take where I do a first pass with a NER and then pass the names through an LLM together with the original newspaper article to extract the other data and get better results faster than a single LLM pass Or if the answer is I should be training some model what is a good model for me to use as my starting point I'm much at the beginning of my machine learning journey and would love to be pointed in the right direction Thanks in advance",
         "ner versus llm extract name gender role company text need extract name gender job title employer / company name newspaper article running process local hardware cloud allow due copyright reason ' ve play around llama 31 ' m finding not get useable result model small 70b parameter size model run much slowly good hardware throw another small llm might good use few processing resource ner use extract datum ner ' ve look extract name gender not know extract datum gender showstopper alternatively approach take first pass ner pass name llm together original newspaper article extract datum get well result fast single llm pass answer training model good model use start point ' m much begin machine learn journey would love point right direction thank advance",
         "ner versus llm extract name gender role company",
         "7",
         "extract,versus llm,gender,role company,ner versus",
         "Text Similarity"
        ],
        [
         "44",
         "78887743",
         "https://stackoverflow.com/questions/78887743",
         "Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask?",
         "<p>In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%).</p>\n<p>I am curious about the following:</p>\n<p>When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively?\nDoes the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact?</p>\n<p>Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%?</p>\n<p>Thank you for your insights!</p>\n",
         "2024-08-19 00:00:00",
         "pytorch,nlp,huggingface-transformers,transformer-model",
         "1",
         "544",
         "1",
         "78890409.0",
         "<p>Attention is computed on a tensor of shape <code>(batch_size, sequence_length, embedding_dimension)</code>. The compute and memory requirements scale with the size of those dimensions.</p>\n<p>For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size.</p>\n<p>With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length <code>8</code> and 10 sequences of length <code>128</code>. Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of <code>128</code>. If you sort by length before batching, you get one batch with sequence length of <code>8</code> and another with length <code>128</code>. The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128).</p>\n<p>That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to &quot;skip over&quot; padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.</p>\n",
         "2.0",
         "101-1k",
         "2024",
         "",
         "(batch_size, sequence_length, embedding_dimension)\n---\n8\n---\n128\n---\n128\n---\n8\n---\n128",
         "padding batch sequence affect performance effective attention mask",
         "In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights",
         "Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding you're not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you aren't going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights Attention is computed on a tensor of shape The compute and memory requirements scale with the size of those dimensions For an input of fixed size the percent padding does not impact performance There is some minor overhead from applying a padding mask at all ie not having a padding mask saves you one mask fill operation but between x% padding and y% padding you're not going to see a difference The overall compute requirements are set by the tensor size With respect to batching sequences there can be added inefficiencies for batching together sequences of different length Say you have 10 sequences of length and 10 sequences of length Now pad and batch those sequences into two batches If you mix lengths evenly you get two batches with a sequence length of If you sort by length before batching you get one batch with sequence length of and another with length The first case two batches of sequence length 128 requires overall more compute compared to the second case one batch of 8 one of 128 That said for a fixed input size you aren't going to see a performance change from the percent padding There is no way for the attention operation to skip over padding tokens The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel The only effect of the padding mask is it assigns 0 attention weight to padding tokens",
         "padding batch sequence affect performance effective attention mask transformer model sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 % curious follow pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow model effectively skip padding token result minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero value computation effectively reduce approximately 10 % thank insight attention compute tensor shape compute memory requirement scale size dimension input fix size percent padding impact performance minor overhead apply padding mask ie padding mask save one mask fill operation x % padding % padding be go see difference overall compute requirement set tensor size respect batching sequence add inefficiency batch together sequence different length say 10 sequence length 10 sequence length pad batch sequence two batch mix length evenly get two batch sequence length sort length batching get one batch sequence length another length first case two batch sequence length 128 require overall compute compare second case one batch 8 one 128 say fix input size not go see performance change percent padding way attention operation skip padding token conditional control flow require sort approach not work well way gpu execute operation parallel effect padding mask assign 0 attention weight padding token",
         "Does Padding in a Batch of Sequences Affect Performance How Effective is the Attention Mask In Transformer models sequences of variable lengths are typically padded to the maximum length in a batch However if my sequence lengths vary the batch may contain a substantial amount of padding potentially over 50% I am curious about the following When PyTorch computes the Transformer do padding tokens impact calculation speed negatively Does the presence of the attention mask allow the model to effectively skip over padding tokens resulting in only a minimal performance impact Overall how effective is the attention mask If I have a sparse attention mask with only 10% nonzero values does the computation effectively reduce to approximately 10% Thank you for your insights",
         "padding batch sequence affect performance effective attention mask transformer model sequence variable length typically pad maximum length batch however sequence length vary batch may contain substantial amount pad potentially 50 % curious follow pytorch compute transformer pad token impact calculation speed negatively presence attention mask allow model effectively skip padding token result minimal performance impact overall effective attention mask sparse attention mask 10 % nonzero value computation effectively reduce approximately 10 % thank insight",
         "padding batch sequence affect performance effective attention mask",
         "6",
         "affect performance,mask,padding,batch sequence,effective attention",
         "BERT & Hugging Face Application"
        ],
        [
         "45",
         "78865486",
         "https://stackoverflow.com/questions/78865486",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text",
         "<p>Using the following Matcher rule:</p>\n<pre><code>{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }\n</code></pre>\n<p>on the text: 'MyLabel: Some Value'</p>\n<p>I get <strong>two</strong> matches: 'MyLabel' and 'MyLabel:'</p>\n<p>For me, that was quite surprising - I was expecting a single match on 'MyLabel:'.\nAdding the new greedy flag didn't make any difference.</p>\n<ul>\n<li>Is this the intended behavior or is it a bug?</li>\n<li>How should I determine that the second match really is just a subset of the first match?</li>\n<li>Will the shorter match always be reported before the longer match?</li>\n</ul>\n<p>SpaCy version 3.7.5</p>\n",
         "2024-08-13 00:00:00",
         "nlp,spacy,matcher",
         "1",
         "37",
         "1",
         "78870921.0",
         "<p>i will say that the behavior you're observing with the SpaCy <code>Matcher</code> is expected, and it is not a bug. When you use the <code>{'TEXT': ':', 'OP': '?'}</code> pattern, the <code>OP: '?'</code> operator means that the colon is optional, so the matcher will generate both the shorter and the longer match, as you've seen.</p>\n<h3>Explanation:</h3>\n<ul>\n<li><strong>Pattern</strong>: <code>{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}</code>.</li>\n<li><strong>Text</strong>: <code>'MyLabel: Some Value'</code>.</li>\n</ul>\n<p>So for this pattern, SpaCy  will try to match:</p>\n<ol>\n<li><code>'MyLabel'</code> alone (because the colon is optional).</li>\n<li><code>'MyLabel:'</code> (because the colon can be included).</li>\n</ol>\n<p>Therefore, you will get two matches: <code>'MyLabel'</code> and <code>'MyLabel:'</code>.</p>\n<h3>Now to  Answer Your Questions:</h3>\n<ol>\n<li><p><strong>Is this the intended behavior or is it a bug?</strong></p>\n<ul>\n<li>This is intended behavior. The <code>OP: '?'</code> operator allows the colon to be optionally matched, leading to multiple matches.</li>\n</ul>\n</li>\n<li><p><strong>How should I determine that the second match really is just a subset of the first match?</strong></p>\n<ul>\n<li>To determine if one match is a subset of another, you can compare the start and end indices of the matches. The longer match will have the same start index but a different end index. Now i wrote a code below even using spacy version 3.7.5, see details below</li>\n</ul>\n</li>\n</ol>\n<pre><code>pip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n</code></pre>\n<p>Now Example in code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\ndoc = nlp(&quot;MyLabel: Some Value&quot;)\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(&quot;R-1&quot;, [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f&quot;Match: {span.text}, Start: {start}, End: {end}&quot;)\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start &gt;= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f&quot;Filtered Match: {span.text}&quot;)\n</code></pre>\n<p>Now, This code will filter out the shorter match and your output will be</p>\n<pre><code>Match: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n\n</code></pre>\n<ol start=\"3\">\n<li><strong>Now Will the shorter match always be reported before the longer match?</strong>\n<ul>\n<li>I don't think the matches are not guaranteed to be reported in a specific order. so to handle this, you can sort the matches by their start and end indices as shown in the code example above.Now, After sorting, you can now filter out matches that are subsets of longer matches.</li>\n</ul>\n</li>\n</ol>\n<h3>Another Alternative Solution:</h3>\n<p>If you want to ensure that only the longest match is returned, you can change the way you define the pattern:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n</code></pre>\n<p>note that the <code>greedy</code> flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings.</p>\n<h3>Now back to the Summary of what i explained:</h3>\n<ul>\n<li>The behavior you're seeing is by design, due to the optional <code>OP: '?'</code> operator.</li>\n<li>in addition, you can filter out the shorter match by comparing start and end indices of the matches.</li>\n<li>furthermore, Sorting the matches by start and end indices allows you to keep only the longest, non-overlapping matches.</li>\n</ul>\n",
         "1.0",
         "11-100",
         "2024",
         "{'label': 'R-1',\n 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],\n 'greedy': 'LONGEST', }",
         "Matcher\n---\n{'TEXT': ':', 'OP': '?'}\n---\nOP: '?'\n---\n{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}\n---\n'MyLabel: Some Value'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\n'MyLabel'\n---\n'MyLabel:'\n---\nOP: '?'\n---\npip show spacy\nName: spacy\nVersion: 3.7.5\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nHome-page: https://spacy.io\nAuthor: Explosion\nAuthor-email: contact@explosion.ai\nLicense: MIT\nLocation: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages\nRequires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\nRequired-by: en-core-web-sm\n---\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"MyLabel: Some Value\")\n\nmatcher = Matcher(nlp.vocab)\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]\nmatcher.add(\"R-1\", [pattern])\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(f\"Match: {span.text}, Start: {start}, End: {end}\")\n\n# Now, we Determine if one match is a subset of another\nmatches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending\nfiltered_matches = []\nlast_end = -1\nfor match_id, start, end in matches:\n    if start >= last_end:  # This is for Avoiding adding subsets\n        filtered_matches.append((match_id, start, end))\n        last_end = end\n\nfor match_id, start, end in filtered_matches:\n    span = doc[start:end]\n    print(f\"Filtered Match: {span.text}\")\n---\nMatch: MyLabel, Start: 0, End: 1\nMatch: MyLabel:, Start: 0, End: 2\nFiltered Match: MyLabel:   , you can see MYLabel: with the colon symbol there\n---\npattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]\n---\ngreedy\n---\nOP: '?'",
         "spacy matcher optional suffix pattern report multiple match text",
         "Using the following Matcher rule on the text 'MyLabel Some Value' I get two matches 'MyLabel' and 'MyLabel' For me that was quite surprising I was expecting a single match on 'MyLabel' Adding the new greedy flag didn't make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375",
         "i will say that the behavior you're observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as you've seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I don't think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior you're seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text 'MyLabel Some Value' I get two matches 'MyLabel' and 'MyLabel' For me that was quite surprising I was expecting a single match on 'MyLabel' Adding the new greedy flag didn't make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375 i will say that the behavior you're observing with the SpaCy is expected and it is not a bug When you use the pattern the operator means that the colon is optional so the matcher will generate both the shorter and the longer match as you've seen Explanation Pattern Text So for this pattern SpaCy will try to match alone because the colon is optional because the colon can be included Therefore you will get two matches and Now to Answer Your Questions Is this the intended behavior or is it a bug This is intended behavior The operator allows the colon to be optionally matched leading to multiple matches How should I determine that the second match is just a subset of the first match To determine if one match is a subset of another you can compare the start and end indices of the matches The longer match will have the same start index but a different end index Now i wrote a code below even using spacy version 375 see details below Now Example in code Now This code will filter out the shorter match and your output will be Now Will the shorter match always be reported before the longer match I don't think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example aboveNow After sorting you can now filter out matches that are subsets of longer matches Another Alternative Solution If you want to ensure that only the longest match is returned you can change the way you define the pattern note that the flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings Now back to the Summary of what i explained The behavior you're seeing is by design due to the optional operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore Sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches",
         "spacy matcher optional suffix pattern report multiple match text use follow matcher rule text ' mylabel value ' get two match ' mylabel ' ' mylabel ' quite surprising expect single match ' mylabel ' add new greedy flag not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375 say behavior be observe spacy expect bug use pattern operator mean colon optional matcher generate shorter long match ' ve see explanation pattern text pattern spacy try match alone colon optional colon include therefore get two match answer question intend behavior bug intend behavior operator allow colon optionally match lead multiple match determine second match subset first match determine one match subset another compare start end index match long match start index different end index write code even use spacy version 375 see detail example code code filter short match output short match always report long match not think match guarantee report specific order handle sort match start end index show code example abovenow sort filter match subset long match another alternative solution want ensure long match return change way define pattern note flag not change behavior match rather influence overlaps handle certain custom setting back summary explain behavior be see design due optional operator addition filter short match compare start end index match furthermore sort match start end index allow keep long nonoverlapping match",
         "SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule on the text 'MyLabel Some Value' I get two matches 'MyLabel' and 'MyLabel' For me that was quite surprising I was expecting a single match on 'MyLabel' Adding the new greedy flag didn't make any difference Is this the intended behavior or is it a bug How should I determine that the second match is just a subset of the first match Will the shorter match always be reported before the longer match SpaCy version 375",
         "spacy matcher optional suffix pattern report multiple match text use follow matcher rule text ' mylabel value ' get two match ' mylabel ' ' mylabel ' quite surprising expect single match ' mylabel ' add new greedy flag not make difference intend behavior bug determine second match subset first match short match always report long match spacy version 375",
         "spacy matcher optional suffix pattern report multiple match",
         "5",
         "spacy,suffix,matcher,optional suffix,pattern report",
         "Using Spacy Library"
        ],
        [
         "46",
         "78862691",
         "https://stackoverflow.com/questions/78862691",
         "`mlflow.transformers.log_model()` does not finish",
         "<h3>Problem</h3>\n<p>I want to use <code>mlflow.transformers.log_model()</code> to log a finetuned huggingface model.</p>\n<p><strong>However, when the <code>mlflow.transformers.log_model</code> method is running, it simply does not finish - runs forever - throws no errors.</strong></p>\n<p>I suspect my configuration is not right, the model is too big?\nThe output says <code>Skipping saving pretrained model weights to disk</code> so that should not be the problem.</p>\n<p>Any ideas how to do this properly?</p>\n<h3>Example</h3>\n<p>This is more or less how my setup looks like, you cannot run this, it includes some pseudocode...</p>\n<p>I am on python 3.11.9 with <code>transformers = &quot;^4.41.2&quot;</code> &amp; <code>mlflow = &quot;^2.15.1&quot;</code>.</p>\n<pre><code>import mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = &quot;LeoLM/leo-hessianai-7b-chat-bilingual&quot;\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=&quot;auto&quot;,\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n<p>The last output I get in the console is:</p>\n<pre><code>INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(&quot;Setuptools is replacing distutils.&quot;)\n</code></pre>\n",
         "2024-08-12 00:00:00",
         "python,nlp,huggingface-transformers,mlflow,mlops",
         "0",
         "353",
         "1",
         "78877979.0",
         "<p>Before defining the trainer, the model has be turned into a Peft model object via <code>get_peft_model</code>, then the <code>mlflow.transformers.log_model</code> works:</p>\n<pre><code>from peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(&quot;my_experiment&quot;)\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         &quot;model&quot;: trainer.model,\n         &quot;tokenizer&quot;: tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=&quot;model&quot;,\n    )\n</code></pre>\n",
         "0.0",
         "101-1k",
         "2024",
         "mlflow.transformers.log_model()\n---\nmlflow.transformers.log_model\n---\nSkipping saving pretrained model weights to disk\n---\ntransformers = \"^4.41.2\"\n---\nmlflow = \"^2.15.1\"\n---\nimport mlflow\nimport torch\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\ntrain_dataset = ...\neval_dataset = ...\n\nmodel_id = \"LeoLM/leo-hessianai-7b-chat-bilingual\"\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\n# Define Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n)\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! This function all does not finish... !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )\n---\nINFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.\n/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")",
         "get_peft_model\n---\nmlflow.transformers.log_model\n---\nfrom peft import LoraConfig, get_peft_model\n\nmodel = ...\npeft_config = LoraConfig(...)\nargs = TrainingArguments(...)\n\npeft_model = get_peft_model(model, peft_config)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=args,\n    ...\n)\n\n\n# mlflow\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run() as run:\n    mlflow.transformers.autolog()\n    trainer.train()\n    \n     components = {\n         \"model\": trainer.model,\n         \"tokenizer\": tokenizer_no_pad,\n     }\n     # !!! Now the logginig of the model works, we can find it in the artifacts !!!\n     mlflow.transformers.log_model(\n         transformers_model=components,\n         artifact_path=\"model\",\n    )",
         "` mlflowtransformerslog_model ` finish",
         "Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is",
         "Before defining the trainer the model has be turned into a Peft model object via then the works",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is Before defining the trainer the model has be turned into a Peft model object via then the works",
         "` mlflowtransformerslog_model ` finish problem want use log finetune huggingface model however method run simply finish run forever throw error suspect configuration right model big output say problem idea properly example less setup look like run include pseudocode python 3119 & last output get console define trainer model turn peft model object via work",
         "`mlflowtransformerslog_model` does not finish Problem I want to use to log a finetuned huggingface model However when the method is running it simply does not finish runs forever throws no errors I suspect my configuration is not right the model is too big The output says so that should not be the problem Any ideas how to do this properly Example This is more or less how my setup looks like you cannot run this it includes some pseudocode I am on python 3119 with & The last output I get in the console is",
         "` mlflowtransformerslog_model ` finish problem want use log finetune huggingface model however method run simply finish run forever throw error suspect configuration right model big output say problem idea properly example less setup look like run include pseudocode python 3119 & last output get console",
         "mlflowtransformerslogmodel finish",
         "6",
         "",
         "BERT & Hugging Face Application"
        ],
        [
         "47",
         "78853409",
         "https://stackoverflow.com/questions/78853409",
         "NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation)",
         "<p>I'm attempting to fine-tune the NLLB model <code>&quot;facebook/nllb-200-distilled-600M&quot;</code> for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb.</p>\n<p>Documentation: <a href=\"https://github.com/facebookresearch/fairseq/tree/nllb?tab=readme-ov-file\" rel=\"nofollow noreferrer\">link</a></p>\n<p>This is the code block which is giving error:</p>\n<pre><code>DATA_CONFIG = &quot;/content/sample_data/data_config.json&quot;\nOUTPUT_DIR = &quot;/content/outputs&quot;\nMODEL_FOLDER = &quot;/content/drive/MyDrive/Thesis/nllb-checkpoints&quot;\nDROP = 0.1\nSRC = &quot;eng_Latn&quot;\nTGT = &quot;deu_Latn&quot;\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=&quot;$SRC-$TGT&quot; \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n</code></pre>\n<p>This is the error:</p>\n<pre><code>/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=&quot;conf&quot;, config_name=&quot;base_config&quot;)\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 289, in main\n    train_module = TrainModule(config)\n  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n</code></pre>\n<p>So far, I understand there is a <code>Missing data_prefix configuration</code>. I created a demo custom data_config.json. Which looks like this:</p>\n<pre><code>{\n    &quot;data_prefix&quot;: &quot;/content/sample_data&quot;,\n    &quot;train_data&quot;: &quot;train_demo.json&quot;,\n    &quot;test_data&quot;: &quot;test_demo.json&quot;,\n    &quot;lang_pairs&quot;: &quot;eng_Latn-deu_Latn&quot;\n}\n</code></pre>\n<p>While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?</p>\n",
         "2024-08-09 00:00:00",
         "python,nlp,machine-translation,fine-tuning,fairseq",
         "1",
         "159",
         "1",
         "78854613.0",
         "<p>While I can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided JSON files), my personal recommendation would be to fine-tune NLLB in the <code>transformers</code> library, specifically using the <code>Seq2SeqTrainer</code>.</p>\n<p>I did this before for multiple models, including NLLB, check out this repository: <a href=\"https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/\" rel=\"nofollow noreferrer\">https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/</a></p>\n<p>This way the fine-tuning and inference process for the NLLB model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so:</p>\n<pre><code>tokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=&quot;eng_Latn&quot;, tgt_lang=&quot;deu_Latn&quot;)\n</code></pre>\n<p>and generate translations like this:</p>\n<pre><code>model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(&quot;deu_Latn&quot;)[1], max_length=512)\n</code></pre>\n",
         "0.0",
         "101-1k",
         "2024",
         "\"facebook/nllb-200-distilled-600M\"\n---\nDATA_CONFIG = \"/content/sample_data/data_config.json\"\nOUTPUT_DIR = \"/content/outputs\"\nMODEL_FOLDER = \"/content/drive/MyDrive/Thesis/nllb-checkpoints\"\nDROP = 0.1\nSRC = \"eng_Latn\"\nTGT = \"deu_Latn\"\n!python /content/fairseq/examples/nllb/modeling/train/train_script.py \\\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\n    cfg/dataset=default \\\n    cfg.dataset.lang_pairs=\"$SRC-$TGT\" \\\n    cfg.fairseq_root=$(pwd) \\\n    cfg.output_dir=$OUTPUT_DIR \\\n    cfg.dropout=$DROP \\\n    cfg.warmup=10 \\\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\n---\n/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=\"conf\", config_name=\"base_config\")\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nTRAINING DIR:  /content/outputs\nError executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']\nTraceback (most recent call last):\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 289, in main\n    train_module = TrainModule(config)\n  File \"/content/fairseq/examples/nllb/modeling/train/train_script.py\", line 122, in __init__\n    assert cluster_name in cfg.dataset.data_prefix\nomegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct\n    full_key: cfg.dataset.data_prefix\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n---\nMissing data_prefix configuration\n---\n{\n    \"data_prefix\": \"/content/sample_data\",\n    \"train_data\": \"train_demo.json\",\n    \"test_data\": \"test_demo.json\",\n    \"lang_pairs\": \"eng_Latn-deu_Latn\"\n}",
         "transformers\n---\nSeq2SeqTrainer\n---\ntokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=\"eng_Latn\", tgt_lang=\"deu_Latn\")\n---\nmodel.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(\"deu_Latn\")[1], max_length=512)",
         "nllb finetune error miss data_prefix configuration englishgerman translation",
         "I'm attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information I'm encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB",
         "While I can't help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation I'm attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information I'm encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB While I can't help you with the concrete error message you are getting my guess would be issues with structure of the provided JSON files my personal recommendation would be to finetune NLLB in the library specifically using the I did this before for multiple models including NLLB check out this repository This way the finetuning and inference process for the NLLB model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this",
         "nllb finetune error miss data_prefix configuration englishgerman translation ' m attempt finetune nllb model scientific translation task english eng_latn german deu_latn follow official guideline finetune author nllb documentation link code block give error error far understand create demo custom data_configjson look like official documentation provide information ' m encounter difficulty apply specific use case someone share detailed guide point helpful resource finetune nllb can not help concrete error message get guess would issue structure provide json file personal recommendation would finetune nllb library specifically use multiple model include nllb check repository way finetune inference process nllb model bilingual model find guide easiely exception load tokenizer like generate translation like",
         "NLLB FineTuning Error Missing data_prefix Configuration EnglishGerman Translation I'm attempting to finetune the NLLB model for a scientific translation task from English eng_Latn to German deu_Latn I followed the official guidelines for finetuning by authors of nllb Documentation link This is the code block which is giving error This is the error So far I understand there is a I created a demo custom data_configjson Which looks like this While the official documentation provides some information I'm encountering difficulties in applying it to my specific use case Can someone share a detailed guide or point me to helpful resources on finetuning NLLB",
         "nllb finetune error miss data_prefix configuration englishgerman translation ' m attempt finetune nllb model scientific translation task english eng_latn german deu_latn follow official guideline finetune author nllb documentation link code block give error error far understand create demo custom data_configjson look like official documentation provide information ' m encounter difficulty apply specific use case someone share detailed guide point helpful resource finetune nllb",
         "nllb finetune error miss dataprefix configuration englishgerman translation",
         "2",
         "englishgerman translation,nllb,finetune error,dataprefix configuration,miss dataprefix",
         "Handling Error in NLP Task"
        ],
        [
         "48",
         "78846004",
         "https://stackoverflow.com/questions/78846004",
         "How can I use structured_output with Azure OpenAI with the openai Python library?",
         "<p>I want to use structured output with Azure OpenAI.</p>\n<p>I tried the following code, based on the code given in <a href=\"https://openai.com/index/introducing-structured-outputs-in-the-api/\" rel=\"nofollow noreferrer\">https://openai.com/index/introducing-structured-outputs-in-the-api/</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=&quot;gpt-4omini-2024-07-18-name&quot;,\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful math tutor.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;solve 8x + 31 = 2&quot;},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n</code></pre>\n<p>I get the error:</p>\n<pre><code>openai.BadRequestError: Error code: 400:\n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Invalid parameter: response_format must be one of json_object, text.&quot;,\n        &quot;type&quot;: &quot;invalid_request_error&quot;,\n        &quot;param&quot;: &quot;response_format&quot;,\n        &quot;code&quot;: &quot;None&quot;\n    }\n}\n</code></pre>\n<p>How to fix it?</p>\n<p>I ran <code>pip install -U openai</code>: I use <code>openai==1.40.1</code> and Python 3.11.</p>\n<hr />\n<p>I also tried <a href=\"https://cookbook.openai.com/examples/structured_outputs_intro\" rel=\"nofollow noreferrer\">https://cookbook.openai.com/examples/structured_outputs_intro</a> using  using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message:</p>\n<pre><code>from openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = &quot;https://&lt;your-resource-name&gt;.openai.azure.com&quot;\napi_key = &quot;&lt;your-api-key&gt;&quot;\ndeployment_name = &quot;&lt;your-deployment-name&gt;&quot; # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f&quot;{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01&quot;\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            &quot;role&quot;: &quot;system&quot;,\n            &quot;content&quot;: math_tutor_prompt\n        },\n        {\n            &quot;role&quot;: &quot;user&quot;,\n            &quot;content&quot;: question\n        }\n    ],\n    response_format={\n        &quot;type&quot;: &quot;json_schema&quot;,\n        &quot;json_schema&quot;: {\n            &quot;name&quot;: &quot;math_reasoning&quot;,\n            &quot;schema&quot;: {\n                &quot;type&quot;: &quot;object&quot;,\n                &quot;properties&quot;: {\n                    &quot;steps&quot;: {\n                        &quot;type&quot;: &quot;array&quot;,\n                        &quot;items&quot;: {\n                            &quot;type&quot;: &quot;object&quot;,\n                            &quot;properties&quot;: {\n                                &quot;explanation&quot;: {&quot;type&quot;: &quot;string&quot;},\n                                &quot;output&quot;: {&quot;type&quot;: &quot;string&quot;}\n                            },\n                            &quot;required&quot;: [&quot;explanation&quot;, &quot;output&quot;],\n                            &quot;additionalProperties&quot;: False\n                        }\n                    },\n                    &quot;final_answer&quot;: {&quot;type&quot;: &quot;string&quot;}\n                },\n                &quot;required&quot;: [&quot;steps&quot;, &quot;final_answer&quot;],\n                &quot;additionalProperties&quot;: False\n            },\n            &quot;strict&quot;: True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = &quot;how can I solve 8x + 7 = -23&quot;\n\nresult = get_math_solution(question)\n\nprint(result.content)\n</code></pre>\n",
         "2024-08-07 00:00:00",
         "python,nlp,azure-openai,gpt-4",
         "0",
         "1225",
         "2",
         "78946352.0",
         "<p>Using <code>gpt-4o-2024-08-06</code>, which finally got deployed today (2024-09-03) on Azure, made it work. Code example from <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python-secure\" rel=\"nofollow noreferrer\">learn.microsoft.com</a>:</p>\n<pre><code>from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = &quot;https://your-azure-openai-endpoint.com&quot;\napi_key = &quot;your-azure-openai-key&quot;\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Extract the event information.&quot;},\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Alice and Bob are going to a science fair on Friday.&quot;},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n</code></pre>\n<p>output:</p>\n<pre><code>name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  &quot;id&quot;: &quot;chatcmpl-A3XDRVolXpjeAAQIGddswI990weid&quot;,\n  &quot;choices&quot;: [\n    {\n      &quot;finish_reason&quot;: &quot;stop&quot;,\n      &quot;index&quot;: 0,\n      &quot;logprobs&quot;: null,\n      &quot;message&quot;: {\n        &quot;content&quot;: &quot;{\\&quot;name\\&quot;:\\&quot;Science Fair\\&quot;,\\&quot;date\\&quot;:\\&quot;Friday\\&quot;,\\&quot;participants\\&quot;:[\\&quot;Alice\\&quot;,\\&quot;Bob\\&quot;]}&quot;,\n        &quot;refusal&quot;: null,\n        &quot;role&quot;: &quot;assistant&quot;,\n        &quot;function_call&quot;: null,\n        &quot;tool_calls&quot;: [],\n        &quot;parsed&quot;: {\n          &quot;name&quot;: &quot;Science Fair&quot;,\n          &quot;date&quot;: &quot;Friday&quot;,\n          &quot;participants&quot;: [\n            &quot;Alice&quot;,\n            &quot;Bob&quot;\n          ]\n        }\n      },\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ],\n  &quot;created&quot;: 1725406029,\n  &quot;model&quot;: &quot;gpt-4o-2024-08-06&quot;,\n  &quot;object&quot;: &quot;chat.completion&quot;,\n  &quot;service_tier&quot;: null,\n  &quot;system_fingerprint&quot;: &quot;fp_b2ffeb31ff&quot;,\n  &quot;usage&quot;: {\n    &quot;completion_tokens&quot;: 17,\n    &quot;prompt_tokens&quot;: 32,\n    &quot;total_tokens&quot;: 49\n  },\n  &quot;prompt_filter_results&quot;: [\n    {\n      &quot;prompt_index&quot;: 0,\n      &quot;content_filter_results&quot;: {\n        &quot;hate&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;self_harm&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;sexual&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        },\n        &quot;violence&quot;: {\n          &quot;filtered&quot;: false,\n          &quot;severity&quot;: &quot;safe&quot;\n        }\n      }\n    }\n  ]\n}\n</code></pre>\n<p>Tested with Python 3.11.7 and openai==1.43.0.</p>\n",
         "0.0",
         "1k-10k",
         "2024",
         "from pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-05-01-preview',\n                     azure_endpoint='[redacted]')\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4omini-2024-07-18-name\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n        {\"role\": \"user\", \"content\": \"solve 8x + 31 = 2\"},\n    ],\n    response_format=MathResponse,\n)\n\nmessage = completion.choices[0].message\nif message.parsed:\n    print(message.parsed.steps)\n    print(message.parsed.final_answer)\nelse:\n    print(message.refusal)\n---\nopenai.BadRequestError: Error code: 400:\n{\n    \"error\": {\n        \"message\": \"Invalid parameter: response_format must be one of json_object, text.\",\n        \"type\": \"invalid_request_error\",\n        \"param\": \"response_format\",\n        \"code\": \"None\"\n    }\n}\n---\npip install -U openai\n---\nopenai==1.40.1\n---\nfrom openai import AzureOpenAI\n\n# Replace these variables with your Azure OpenAI endpoint and API key\nendpoint = \"https://<your-resource-name>.openai.azure.com\"\napi_key = \"<your-api-key>\"\ndeployment_name = \"<your-deployment-name>\" # Replace with your deployment name\nMODEL = deployment_name\n\n# API endpoint for the completion request\napi_url = f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01\"\n\n\nclient = AzureOpenAI(api_key='[redacted]',\n                     api_version='2024-07-01-preview',\n                     azure_endpoint='https://[redacted].openai.azure.com/')\n\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": math_tutor_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": question\n        }\n    ],\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"math_reasoning\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"steps\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"explanation\": {\"type\": \"string\"},\n                                \"output\": {\"type\": \"string\"}\n                            },\n                            \"required\": [\"explanation\", \"output\"],\n                            \"additionalProperties\": False\n                        }\n                    },\n                    \"final_answer\": {\"type\": \"string\"}\n                },\n                \"required\": [\"steps\", \"final_answer\"],\n                \"additionalProperties\": False\n            },\n            \"strict\": True\n        }\n    }\n    )\n\n    return response.choices[0].message\n\n\n# Testing with an example question\nquestion = \"how can I solve 8x + 7 = -23\"\n\nresult = get_math_solution(question)\n\nprint(result.content)",
         "gpt-4o-2024-08-06\n---\nfrom pydantic import BaseModel\nfrom openai import AzureOpenAI\n\nendpoint = \"https://your-azure-openai-endpoint.com\"\napi_key = \"your-azure-openai-key\"\ndeployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name\n\nclient = AzureOpenAI(api_key=api_key,\n                     api_version='2024-08-01-preview',\n                     azure_endpoint=endpoint)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\nprint(event)\nprint(completion.model_dump_json(indent=2))\n---\nname='Science Fair' date='Friday' participants=['Alice', 'Bob']\n{\n  \"id\": \"chatcmpl-A3XDRVolXpjeAAQIGddswI990weid\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"{\\\"name\\\":\\\"Science Fair\\\",\\\"date\\\":\\\"Friday\\\",\\\"participants\\\":[\\\"Alice\\\",\\\"Bob\\\"]}\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": [],\n        \"parsed\": {\n          \"name\": \"Science Fair\",\n          \"date\": \"Friday\",\n          \"participants\": [\n            \"Alice\",\n            \"Bob\"\n          ]\n        }\n      },\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"created\": 1725406029,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": \"fp_b2ffeb31ff\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 32,\n    \"total_tokens\": 49\n  },\n  \"prompt_filter_results\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ]\n}",
         "use structured_output azure openai openai python library",
         "I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didn't work either same error message",
         "Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didn't work either same error message Using which finally got deployed today 20240903 on Azure made it work Code example from learnmicrosoftcom output Tested with Python 3117 and openai==1430",
         "use structured_output azure openai openai python library want use structured output azure openai try follow code base code given get error fix run use python 311 also try use use azure+ gpt4o mini 20240718 not work either error message using finally get deploy today 20240903 azure make work code example learnmicrosoftcom output test python 3117 openai==1430",
         "How can I use structured_output with Azure OpenAI with the openai Python library I want to use structured output with Azure OpenAI I tried the following code based on the code given in I get the error How to fix it I ran I use and Python 311 I also tried using using Azure+ GPT4o mini 20240718 it didn't work either same error message",
         "use structured_output azure openai openai python library want use structured output azure openai try follow code base code given get error fix run use python 311 also try use use azure+ gpt4o mini 20240718 not work either error message",
         "structuredoutput azure openai openai python library",
         "3",
         "library,azure,python library,openai openai,structuredoutput",
         "Using Stanford Library"
        ],
        [
         "49",
         "78836208",
         "https://stackoverflow.com/questions/78836208",
         "Removing bi-grams after tokenization for TfidfVectorizer",
         "<p>I'm attempting to remove bi-grams that are created by <code>TfidfVectorizer</code>.  I'm using <code>text.TfidfVectorizer</code> so that I can use my own preprocessor function.</p>\n<p>Test strings and preprocessor function:</p>\n<pre><code>doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, &quot;&quot;)\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n</code></pre>\n<p>My <code>TfidfVectorizer</code> instantiation and <code>fit_transform</code>:</p>\n<pre><code>from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n</code></pre>\n<p>Here is my error:</p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in &lt;cell line: 5&gt;()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----&gt; 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-&gt; 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 &quot;Upper case characters found in&quot;\n   1332                 &quot; vocabulary while 'lowercase'&quot;\n   1333                 &quot; is True. These entries will not&quot;\n   1334                 &quot; be matched with any documents&quot;\n   1335             )\n   1336             break\n-&gt; 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-&gt; 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--&gt; 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object\n</code></pre>\n<p>How to resolve it?</p>\n",
         "2024-08-05 00:00:00",
         "python,scikit-learn,nlp,preprocessor,tfidfvectorizer",
         "1",
         "41",
         "1",
         "78837616.0",
         "<p>The preprocessor should handle documents, not the whole corpus. (The clues are the &quot;expected string&quot; in the error, and the fact that <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"nofollow noreferrer\">the <code>TfidfVectorizer</code> docs</a> refer to &quot;the preprocessing (string transformation) stage&quot;. The docs could definitely be clearer.)</p>\n<p>This should fix it:</p>\n<pre><code>def remove_bigrams(doc: str) -&gt; str:\n    &quot;&quot;&quot;Remove certain bi-grams from a document.&quot;&quot;&quot;\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, &quot;&quot;)\n    return doc\n</code></pre>\n",
         "2.0",
         "11-100",
         "2024",
         "TfidfVectorizer\n---\ntext.TfidfVectorizer\n---\ndoc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', \n        'another that has aa aa and start date and hurricane hitting south carolina']\n\ndef remove_bigrams(doc):\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    res = []\n    for record in doc:\n        the_string = record\n        for phrase in gram_2:\n            the_string = the_string.replace(phrase, \"\")\n        res.append(the_string)\n    return res\n\nremove_bigrams(doc2)\n---\nTfidfVectorizer\n---\nfit_transform\n---\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ncustom_stop_words = [i for i in stop_words]\n\nvec = text.TfidfVectorizer(stop_words=custom_stop_words,\n                           analyzer='word',\n                           ngram_range=(2, 2),\n                           preprocessor=remove_bigrams,\n                          )\n\nfeatures = vec.fit_transform(doc2)\n---\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [49], in <cell line: 5>()\n      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)\n      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)\n----> 5 features = vec.fit_transform(doc2)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)\n   2072 self._check_params()\n   2073 self._tfidf = TfidfTransformer(\n   2074     norm=self.norm,\n   2075     use_idf=self.use_idf,\n   2076     smooth_idf=self.smooth_idf,\n   2077     sublinear_tf=self.sublinear_tf,\n   2078 )\n-> 2079 X = super().fit_transform(raw_documents)\n   2080 self._tfidf.fit(X)\n   2081 # X is already a transformed view of raw_documents so\n   2082 # we set copy to False\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)\n   1330             warnings.warn(\n   1331                 \"Upper case characters found in\"\n   1332                 \" vocabulary while 'lowercase'\"\n   1333                 \" is True. These entries will not\"\n   1334                 \" be matched with any documents\"\n   1335             )\n   1336             break\n-> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n   1340 if self.binary:\n   1341     X.data.fill(1)\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n   1207 for doc in raw_documents:\n   1208     feature_counter = {}\n-> 1209     for feature in analyze(doc):\n   1210         try:\n   1211             feature_idx = vocabulary[feature]\n\nFile c:\\Development_Solutions\\Sandbox\\SBVE\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\n    111     doc = preprocessor(doc)\n    112 if tokenizer is not None:\n--> 113     doc = tokenizer(doc)\n    114 if ngrams is not None:\n    115     if stop_words is not None:\n\nTypeError: expected string or bytes-like object",
         "TfidfVectorizer\n---\ndef remove_bigrams(doc: str) -> str:\n    \"\"\"Remove certain bi-grams from a document.\"\"\"\n    gram_2 = ['past performance', 'start date', 'aa aa']\n    for phrase in gram_2:\n        doc = doc.replace(phrase, \"\")\n    return doc",
         "remove bigrams tokenization tfidfvectorizer",
         "I'm attempting to remove bigrams that are created by I'm using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it",
         "The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "Removing bigrams after tokenization for TfidfVectorizer I'm attempting to remove bigrams that are created by I'm using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it The preprocessor should handle documents not the whole corpus The clues are the expected string in the error and the fact that the docs refer to the preprocessing string transformation stage The docs could definitely be clearer This should fix it",
         "remove bigrams tokenization tfidfvectorizer ' m attempt remove bigram create ' m use use preprocessor function test string preprocessor function instantiation error resolve preprocessor handle document whole corpus clue expect string error fact docs refer preprocesse string transformation stage doc could definitely clear fix",
         "Removing bigrams after tokenization for TfidfVectorizer I'm attempting to remove bigrams that are created by I'm using so that I can use my own preprocessor function Test strings and preprocessor function My instantiation and Here is my error How to resolve it",
         "remove bigrams tokenization tfidfvectorizer ' m attempt remove bigram create ' m use use preprocessor function test string preprocessor function instantiation error resolve",
         "remove bigrams tokenization tfidfvectorizer",
         "0",
         "remove,tokenization,bigrams,tfidfvectorizer,remove bigrams",
         "Tokenising Text"
        ]
       ],
       "shape": {
        "columns": 27,
        "rows": 8510
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>QuestionUrl</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>...</th>\n",
       "      <th>Body_Clean</th>\n",
       "      <th>AcceptedAnswerBody_Clean</th>\n",
       "      <th>combination_text</th>\n",
       "      <th>combination_text_clean</th>\n",
       "      <th>combination_question</th>\n",
       "      <th>combination_question_clean</th>\n",
       "      <th>Title_Clean_No_Noise</th>\n",
       "      <th>title_cluster</th>\n",
       "      <th>keywords_fromBert</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79559702</td>\n",
       "      <td>https://stackoverflow.com/questions/79559702</td>\n",
       "      <td>NameError: name 'init_empty_weights' is not de...</td>\n",
       "      <td>&lt;p&gt;I am trying to set up hugging face locally ...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>nlp,huggingface-transformers,huggingface</td>\n",
       "      <td>3</td>\n",
       "      <td>629</td>\n",
       "      <td>2</td>\n",
       "      <td>79577000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>I am trying to set up hugging face locally and...</td>\n",
       "      <td>Try using this version it should resolve the i...</td>\n",
       "      <td>NameError name 'init_empty_weights' is not def...</td>\n",
       "      <td>nameerror name ' init_empty_weight ' define us...</td>\n",
       "      <td>NameError name 'init_empty_weights' is not def...</td>\n",
       "      <td>nameerror name ' init_empty_weight ' define us...</td>\n",
       "      <td>nameerror name initemptyweight define hug face</td>\n",
       "      <td>2</td>\n",
       "      <td>define,face,initemptyweight define,nameerror,d...</td>\n",
       "      <td>Handling Error in NLP Task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79549787</td>\n",
       "      <td>https://stackoverflow.com/questions/79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>python,nlp,spacy,presidio</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>79552218.0</td>\n",
       "      <td>...</td>\n",
       "      <td>I'm using spaCy with the pl_core_news_lg model...</td>\n",
       "      <td>The configuration file is missing the 'labels_...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>5</td>\n",
       "      <td>pesel,spacy,recognize organization,presidio sp...</td>\n",
       "      <td>Using Spacy Library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79548202</td>\n",
       "      <td>https://stackoverflow.com/questions/79548202</td>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>&lt;p&gt;I understand the -100 label id is used so t...</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>nlp,huggingface-transformers,pre-trained-model</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>79551169.0</td>\n",
       "      <td>...</td>\n",
       "      <td>I understand the 100 label id is used so that ...</td>\n",
       "      <td>The author of the tutorial you mentioned sets ...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>gpt2 model huggingface 100 label index trainin...</td>\n",
       "      <td>GPT2 and other models from huggingface 100 lab...</td>\n",
       "      <td>gpt2 model huggingface 100 label index trainin...</td>\n",
       "      <td>gpt2 huggingface 100 label index training inst...</td>\n",
       "      <td>6</td>\n",
       "      <td>label,index training,huggingface 100,pad token...</td>\n",
       "      <td>BERT &amp; Hugging Face Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79523269</td>\n",
       "      <td>https://stackoverflow.com/questions/79523269</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>&lt;p&gt;I am trying to import gensim into colab.&lt;/p...</td>\n",
       "      <td>2025-03-20</td>\n",
       "      <td>numpy,nlp,dependencies,google-colaboratory,gensim</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>1</td>\n",
       "      <td>79523777.0</td>\n",
       "      <td>...</td>\n",
       "      <td>I am trying to import gensim into colab I get ...</td>\n",
       "      <td>You have to restart the session for the underl...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "      <td>trouble getting import gensim work colab try i...</td>\n",
       "      <td>Trouble getting importing gensim to work in co...</td>\n",
       "      <td>trouble getting import gensim work colab try i...</td>\n",
       "      <td>trouble getting import gensim colab</td>\n",
       "      <td>2</td>\n",
       "      <td>getting,trouble,colab,gensim,getting import</td>\n",
       "      <td>Handling Error in NLP Task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79501178</td>\n",
       "      <td>https://stackoverflow.com/questions/79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this &lt;a href...</td>\n",
       "      <td>2025-03-11</td>\n",
       "      <td>python,nlp,large-language-model</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>79501337.0</td>\n",
       "      <td>...</td>\n",
       "      <td>I am running the code found on this site in my...</td>\n",
       "      <td>I can't test it but I checked source code and ...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>store image instead show server run code find ...</td>\n",
       "      <td>Store images instead of showing in a server I ...</td>\n",
       "      <td>store image instead show server run code find ...</td>\n",
       "      <td>store image instead show server</td>\n",
       "      <td>6</td>\n",
       "      <td>instead,store,image,server,store image</td>\n",
       "      <td>BERT &amp; Hugging Face Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>62328</td>\n",
       "      <td>https://stackoverflow.com/questions/62328</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>&lt;p&gt;input: phrase 1, phrase 2&lt;/p&gt;\\n\\n&lt;p&gt;output:...</td>\n",
       "      <td>2008-09-15</td>\n",
       "      <td>algorithm,nlp,semantics</td>\n",
       "      <td>65</td>\n",
       "      <td>49906</td>\n",
       "      <td>11</td>\n",
       "      <td>63076.0</td>\n",
       "      <td>...</td>\n",
       "      <td>input phrase 1 phrase 2 output semantic simila...</td>\n",
       "      <td>You might want to check out this paper Sentenc...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>algorithm tell semantic similarity two phrase ...</td>\n",
       "      <td>Is there an algorithm that tells the semantic ...</td>\n",
       "      <td>algorithm tell semantic similarity two phrase ...</td>\n",
       "      <td>algorithm tell semantic similarity phrase</td>\n",
       "      <td>9</td>\n",
       "      <td>tell,phrase,algorithm,semantic,similarity</td>\n",
       "      <td>NLP Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8506</th>\n",
       "      <td>42489</td>\n",
       "      <td>https://stackoverflow.com/questions/42489</td>\n",
       "      <td>How to implement a \"related\" degree measure al...</td>\n",
       "      <td>&lt;p&gt;I was going to Ask a Question earlier today...</td>\n",
       "      <td>2008-09-03</td>\n",
       "      <td>algorithm,machine-learning,indexing,nlp,full-t...</td>\n",
       "      <td>8</td>\n",
       "      <td>457</td>\n",
       "      <td>2</td>\n",
       "      <td>42532.0</td>\n",
       "      <td>...</td>\n",
       "      <td>I was going to Ask a Question earlier today wh...</td>\n",
       "      <td>One such way to implement such an algorithm wo...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "      <td>How to implement a related degree measure algo...</td>\n",
       "      <td>implement relate degree measure algorithm go a...</td>\n",
       "      <td>implement relate degree measure algorithm</td>\n",
       "      <td>7</td>\n",
       "      <td>relate,measure,algorithm,degree,implement relate</td>\n",
       "      <td>Text Similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>41424</td>\n",
       "      <td>https://stackoverflow.com/questions/41424</td>\n",
       "      <td>How do you implement a \"Did you mean\"?</td>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>2008-09-03</td>\n",
       "      <td>nlp</td>\n",
       "      <td>118</td>\n",
       "      <td>33225</td>\n",
       "      <td>11</td>\n",
       "      <td>41448.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Possible Duplicate How does the Google Did you...</td>\n",
       "      <td>Actually what Google does is much nontrivial a...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>How do you implement a Did you mean Possible D...</td>\n",
       "      <td>implement mean possible duplicate google mean ...</td>\n",
       "      <td>implement mean</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>Text Similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8508</th>\n",
       "      <td>36533</td>\n",
       "      <td>https://stackoverflow.com/questions/36533</td>\n",
       "      <td>Vista speech recognition in multiple languages</td>\n",
       "      <td>&lt;p&gt;my primary language is spanish, but I use a...</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>windows-vista,nlp,speech-recognition,multilingual</td>\n",
       "      <td>3</td>\n",
       "      <td>5661</td>\n",
       "      <td>6</td>\n",
       "      <td>36684.0</td>\n",
       "      <td>...</td>\n",
       "      <td>my primary language is spanish but I use all m...</td>\n",
       "      <td>Citation from Vista speech recognition blog In...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "      <td>Vista speech recognition in multiple languages...</td>\n",
       "      <td>vista speech recognition multiple language pri...</td>\n",
       "      <td>vista speech recognition multiple language</td>\n",
       "      <td>9</td>\n",
       "      <td>multiple,recognition,speech,vista,multiple lan...</td>\n",
       "      <td>NLP Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>23689</td>\n",
       "      <td>https://stackoverflow.com/questions/23689</td>\n",
       "      <td>Natural language date/time parser for .NET?</td>\n",
       "      <td>&lt;p&gt;Does anyone know of a .NET date/time parser...</td>\n",
       "      <td>2008-08-22</td>\n",
       "      <td>.net,datetime,nlp</td>\n",
       "      <td>27</td>\n",
       "      <td>6489</td>\n",
       "      <td>9</td>\n",
       "      <td>631134.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Does anyone know of a NET date/time parser sim...</td>\n",
       "      <td>We developed exactly what you are looking for ...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "      <td>natural language date / time parser net anyone...</td>\n",
       "      <td>Natural language date/time parser for NET Does...</td>\n",
       "      <td>natural language date / time parser net anyone...</td>\n",
       "      <td>natural language date time parser net</td>\n",
       "      <td>3</td>\n",
       "      <td>time,net,date,language date,parser net</td>\n",
       "      <td>Using Stanford Library</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8510 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      QuestionId                                   QuestionUrl  \\\n",
       "0       79559702  https://stackoverflow.com/questions/79559702   \n",
       "1       79549787  https://stackoverflow.com/questions/79549787   \n",
       "2       79548202  https://stackoverflow.com/questions/79548202   \n",
       "3       79523269  https://stackoverflow.com/questions/79523269   \n",
       "4       79501178  https://stackoverflow.com/questions/79501178   \n",
       "...          ...                                           ...   \n",
       "8505       62328     https://stackoverflow.com/questions/62328   \n",
       "8506       42489     https://stackoverflow.com/questions/42489   \n",
       "8507       41424     https://stackoverflow.com/questions/41424   \n",
       "8508       36533     https://stackoverflow.com/questions/36533   \n",
       "8509       23689     https://stackoverflow.com/questions/23689   \n",
       "\n",
       "                                                  Title  \\\n",
       "0     NameError: name 'init_empty_weights' is not de...   \n",
       "1     Why does Presidio with spacy nlp engine not re...   \n",
       "2     GPT-2 and other models from huggingface -100 l...   \n",
       "3     Trouble getting importing gensim to work in colab   \n",
       "4           Store images instead of showing in a server   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a \"related\" degree measure al...   \n",
       "8507             How do you implement a \"Did you mean\"?   \n",
       "8508     Vista speech recognition in multiple languages   \n",
       "8509        Natural language date/time parser for .NET?   \n",
       "\n",
       "                                                   Body CreationDate  \\\n",
       "0     <p>I am trying to set up hugging face locally ...   2025-04-07   \n",
       "1     <p>I'm using spaCy with the pl_core_news_lg mo...   2025-04-02   \n",
       "2     <p>I understand the -100 label id is used so t...   2025-04-01   \n",
       "3     <p>I am trying to import gensim into colab.</p...   2025-03-20   \n",
       "4     <p>I am running the code found on this <a href...   2025-03-11   \n",
       "...                                                 ...          ...   \n",
       "8505  <p>input: phrase 1, phrase 2</p>\\n\\n<p>output:...   2008-09-15   \n",
       "8506  <p>I was going to Ask a Question earlier today...   2008-09-03   \n",
       "8507  <blockquote>\\n  <p><strong>Possible Duplicate:...   2008-09-03   \n",
       "8508  <p>my primary language is spanish, but I use a...   2008-08-31   \n",
       "8509  <p>Does anyone know of a .NET date/time parser...   2008-08-22   \n",
       "\n",
       "                                                   Tags  Score  ViewCount  \\\n",
       "0              nlp,huggingface-transformers,huggingface      3        629   \n",
       "1                             python,nlp,spacy,presidio      0         97   \n",
       "2        nlp,huggingface-transformers,pre-trained-model      0         54   \n",
       "3     numpy,nlp,dependencies,google-colaboratory,gensim      0        209   \n",
       "4                       python,nlp,large-language-model      0         39   \n",
       "...                                                 ...    ...        ...   \n",
       "8505                            algorithm,nlp,semantics     65      49906   \n",
       "8506  algorithm,machine-learning,indexing,nlp,full-t...      8        457   \n",
       "8507                                                nlp    118      33225   \n",
       "8508  windows-vista,nlp,speech-recognition,multilingual      3       5661   \n",
       "8509                                  .net,datetime,nlp     27       6489   \n",
       "\n",
       "      AnswerCount  AcceptedAnswerId  ...  \\\n",
       "0               2        79577000.0  ...   \n",
       "1               1        79552218.0  ...   \n",
       "2               1        79551169.0  ...   \n",
       "3               1        79523777.0  ...   \n",
       "4               1        79501337.0  ...   \n",
       "...           ...               ...  ...   \n",
       "8505           11           63076.0  ...   \n",
       "8506            2           42532.0  ...   \n",
       "8507           11           41448.0  ...   \n",
       "8508            6           36684.0  ...   \n",
       "8509            9          631134.0  ...   \n",
       "\n",
       "                                             Body_Clean  \\\n",
       "0     I am trying to set up hugging face locally and...   \n",
       "1     I'm using spaCy with the pl_core_news_lg model...   \n",
       "2     I understand the 100 label id is used so that ...   \n",
       "3     I am trying to import gensim into colab I get ...   \n",
       "4     I am running the code found on this site in my...   \n",
       "...                                                 ...   \n",
       "8505  input phrase 1 phrase 2 output semantic simila...   \n",
       "8506  I was going to Ask a Question earlier today wh...   \n",
       "8507  Possible Duplicate How does the Google Did you...   \n",
       "8508  my primary language is spanish but I use all m...   \n",
       "8509  Does anyone know of a NET date/time parser sim...   \n",
       "\n",
       "                               AcceptedAnswerBody_Clean  \\\n",
       "0     Try using this version it should resolve the i...   \n",
       "1     The configuration file is missing the 'labels_...   \n",
       "2     The author of the tutorial you mentioned sets ...   \n",
       "3     You have to restart the session for the underl...   \n",
       "4     I can't test it but I checked source code and ...   \n",
       "...                                                 ...   \n",
       "8505  You might want to check out this paper Sentenc...   \n",
       "8506  One such way to implement such an algorithm wo...   \n",
       "8507  Actually what Google does is much nontrivial a...   \n",
       "8508  Citation from Vista speech recognition blog In...   \n",
       "8509  We developed exactly what you are looking for ...   \n",
       "\n",
       "                                       combination_text  \\\n",
       "0     NameError name 'init_empty_weights' is not def...   \n",
       "1     Why does Presidio with spacy nlp engine not re...   \n",
       "2     GPT2 and other models from huggingface 100 lab...   \n",
       "3     Trouble getting importing gensim to work in co...   \n",
       "4     Store images instead of showing in a server I ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507  How do you implement a Did you mean Possible D...   \n",
       "8508  Vista speech recognition in multiple languages...   \n",
       "8509  Natural language date/time parser for NET Does...   \n",
       "\n",
       "                                 combination_text_clean  \\\n",
       "0     nameerror name ' init_empty_weight ' define us...   \n",
       "1     presidio spacy nlp engine recognize organizati...   \n",
       "2     gpt2 model huggingface 100 label index trainin...   \n",
       "3     trouble getting import gensim work colab try i...   \n",
       "4     store image instead show server run code find ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tell semantic similarity two phrase ...   \n",
       "8506  implement relate degree measure algorithm go a...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple language pri...   \n",
       "8509  natural language date / time parser net anyone...   \n",
       "\n",
       "                                   combination_question  \\\n",
       "0     NameError name 'init_empty_weights' is not def...   \n",
       "1     Why does Presidio with spacy nlp engine not re...   \n",
       "2     GPT2 and other models from huggingface 100 lab...   \n",
       "3     Trouble getting importing gensim to work in co...   \n",
       "4     Store images instead of showing in a server I ...   \n",
       "...                                                 ...   \n",
       "8505  Is there an algorithm that tells the semantic ...   \n",
       "8506  How to implement a related degree measure algo...   \n",
       "8507  How do you implement a Did you mean Possible D...   \n",
       "8508  Vista speech recognition in multiple languages...   \n",
       "8509  Natural language date/time parser for NET Does...   \n",
       "\n",
       "                             combination_question_clean  \\\n",
       "0     nameerror name ' init_empty_weight ' define us...   \n",
       "1     presidio spacy nlp engine recognize organizati...   \n",
       "2     gpt2 model huggingface 100 label index trainin...   \n",
       "3     trouble getting import gensim work colab try i...   \n",
       "4     store image instead show server run code find ...   \n",
       "...                                                 ...   \n",
       "8505  algorithm tell semantic similarity two phrase ...   \n",
       "8506  implement relate degree measure algorithm go a...   \n",
       "8507  implement mean possible duplicate google mean ...   \n",
       "8508  vista speech recognition multiple language pri...   \n",
       "8509  natural language date / time parser net anyone...   \n",
       "\n",
       "                                   Title_Clean_No_Noise title_cluster  \\\n",
       "0        nameerror name initemptyweight define hug face             2   \n",
       "1     presidio spacy nlp engine recognize organizati...             5   \n",
       "2     gpt2 huggingface 100 label index training inst...             6   \n",
       "3                   trouble getting import gensim colab             2   \n",
       "4                       store image instead show server             6   \n",
       "...                                                 ...           ...   \n",
       "8505          algorithm tell semantic similarity phrase             9   \n",
       "8506          implement relate degree measure algorithm             7   \n",
       "8507                                     implement mean             7   \n",
       "8508         vista speech recognition multiple language             9   \n",
       "8509              natural language date time parser net             3   \n",
       "\n",
       "                                      keywords_fromBert  \\\n",
       "0     define,face,initemptyweight define,nameerror,d...   \n",
       "1     pesel,spacy,recognize organization,presidio sp...   \n",
       "2     label,index training,huggingface 100,pad token...   \n",
       "3           getting,trouble,colab,gensim,getting import   \n",
       "4                instead,store,image,server,store image   \n",
       "...                                                 ...   \n",
       "8505          tell,phrase,algorithm,semantic,similarity   \n",
       "8506   relate,measure,algorithm,degree,implement relate   \n",
       "8507                                                      \n",
       "8508  multiple,recognition,speech,vista,multiple lan...   \n",
       "8509             time,net,date,language date,parser net   \n",
       "\n",
       "                        category_name  \n",
       "0          Handling Error in NLP Task  \n",
       "1                 Using Spacy Library  \n",
       "2     BERT & Hugging Face Application  \n",
       "3          Handling Error in NLP Task  \n",
       "4     BERT & Hugging Face Application  \n",
       "...                               ...  \n",
       "8505                  NLP Application  \n",
       "8506                  Text Similarity  \n",
       "8507                  Text Similarity  \n",
       "8508                  NLP Application  \n",
       "8509           Using Stanford Library  \n",
       "\n",
       "[8510 rows x 27 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_name = {\n",
    "    0:\"Tokenising Text\",\n",
    "    1:\"Using NLTK Library\",\n",
    "    2:\"Handling Error in NLP Task\",\n",
    "    3:\"Using Stanford Library\",\n",
    "    4:\"Preprocessing Text in Python\",\n",
    "    5:\"Using Spacy Library\",\n",
    "    6:\"BERT & Hugging Face Application\",\n",
    "    7:\"Text Similarity\",\n",
    "    8:\"Word Embedding\",\n",
    "    9:\"NLP Application\",\n",
    "}\n",
    "\n",
    "\n",
    "df_post_answer['category_name'] = df_post_answer['title_cluster'].map(category_name)\n",
    "df_post_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Full Process for: Title + Title Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_post_answer['combination_question_clean_no_noise'] = df_post_answer['combination_question_clean'].apply(lambda x: remove_noise_word(x, noise_words2))\n",
    "# cluster_embedding(df_post_answer,'combination_question_clean_no_noise',model_embedding,cluster_col_name=\"title_body_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_post_answer['title_body_keywords_fromBert'] = df_post_answer['combination_question_clean_no_noise'].apply(extract_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency_questions_body = category_frequency(df_post_answer,keyword_col=\"title_body_keywords_fromBert\",cluster_col=\"title_body_cluster\",min_support=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title_cluster",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "category_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "QuestionUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Year",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9900334e-83fd-4888-b38d-559d85d18e5b",
       "rows": [
        [
         "19",
         "Normalization of token embeddings in BERT encoder blocks",
         "normalization token embedding bert encoder block",
         "nlp,normalization,bert-language-model,attention-model",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/79178041",
         "2024"
        ],
        [
         "126",
         "How to Find Positional embeddings from BARTTokenizer?",
         "find positional embedding barttokenizer",
         "pytorch,nlp,huggingface-transformers,summarization,bart",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/77800331",
         "2024"
        ],
        [
         "114",
         "Why token embedding different from the embedding by the BartForConditionalGeneration model",
         "token embed different embed bartforconditionalgeneration model",
         "machine-learning,pytorch,nlp,huggingface-transformers,bart",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/77906649",
         "2024"
        ],
        [
         "27",
         "How can I adjust the performance of tokenizer?",
         "adjust performance tokenizer",
         "nlp,huggingface-transformers,huggingface,huggingface-tokenizers",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/79100835",
         "2024"
        ],
        [
         "20",
         "How to convert character indices to BERT token indices",
         "convert character indices bert token index",
         "python,nlp,dataset,large-language-model,bert-language-model",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/79173053",
         "2024"
        ],
        [
         "89",
         "What is the best function/stage to use tokenizer in Pytorch's data processing?",
         "good function / stage use tokenizer pytorch 's datum processing",
         "pytorch,nlp",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/78284866",
         "2024"
        ],
        [
         "60",
         "How do we add/modify the normalizer in a pretrained Huggingface tokenizer?",
         "add / modify normalizer pretraine huggingface tokenizer",
         "python,nlp,large-language-model,huggingface-tokenizers",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/78612251",
         "2024"
        ],
        [
         "52",
         "Do I need to use Named Entity Recognition (NER) in tokenization?",
         "need use name entity recognition ner tokenization",
         "python,python-3.x,nlp,spacy,named-entity-recognition",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/78778988",
         "2024"
        ],
        [
         "84",
         "R Tidymodels textrecipes - tokenizing with spacyR - how to remove punctuations from produced list of tokens",
         "r tidymodel textrecipe tokenize spacyr remove punctuation produce list token",
         "r,nlp,spacy,tidymodels",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/78314842",
         "2024"
        ],
        [
         "49",
         "Removing bi-grams after tokenization for TfidfVectorizer",
         "remove bigrams tokenization tfidfvectorizer",
         "python,scikit-learn,nlp,preprocessor,tfidfvectorizer",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/78836208",
         "2024"
        ],
        [
         "393",
         "Python NLP processing if statement not in stop words list",
         "python nlp processing statement stop word list",
         "python,python-3.x,nlp,spacy",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/75682401",
         "2023"
        ],
        [
         "446",
         "nltk.download('punkt') giving output as false",
         "nltkdownload'punkt ' give output false",
         "python,machine-learning,nlp,data-science,nltk",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/75272415",
         "2023"
        ],
        [
         "459",
         "Is splitting a long document of a dataset for BERT considered bad practice?",
         "split long document dataset bert consider bad practice",
         "machine-learning,nlp,classification,bert-language-model,text-classification",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/75179250",
         "2023"
        ],
        [
         "107",
         "Shorten product title to a specific length using python nlp libraries",
         "shorten product title specific length use python nlp library",
         "python,string,nlp,nltk,e-commerce",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/78006552",
         "2024"
        ],
        [
         "463",
         "How to train a NLP text model where text files are stored in category named folders?",
         "train nlp text model text file store category name folder",
         "tensorflow,machine-learning,keras,nlp",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/75147383",
         "2023"
        ],
        [
         "110",
         "Extracting and Identifying locations with NLP + Spacy",
         "extract identify location nlp + spacy",
         "nlp,spacy,named-entity-recognition",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/77951208",
         "2024"
        ],
        [
         "328",
         "How to do NLP fill-mask with restricted possible inputs",
         "nlp fillmask restrict possible input",
         "nlp,mask,huggingface",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/76157465",
         "2023"
        ],
        [
         "381",
         "use fine-tuned BERT to train a new sentence-transformer",
         "use finetune bert train new sentencetransformer",
         "nlp,bert-language-model",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/75788612",
         "2023"
        ],
        [
         "55",
         "How to make dynamic API calls based on user input in a Gemini application python nlp?",
         "make dynamic api call base user input gemini application python nlp",
         "python,nlp,request,google-cloud-vertex-ai,google-gemini",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/78707861",
         "2024"
        ],
        [
         "337",
         "Finding words that indicate time order using NLP in python",
         "find word indicate time order use nlp python",
         "python,nlp,spacy",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/76099661",
         "2023"
        ],
        [
         "77",
         "ValueError: Cannot use a compiled regex as replacement pattern with regex=False",
         "valueerror use compile regex replacement pattern regex = false",
         "python,text,nlp",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/78375631",
         "2024"
        ],
        [
         "111",
         "Google semantic retriever example error 'Credentials' object has no attribute 'universe_domain'",
         "google semantic retriever example error ' credential ' object attribute ' universe_domain '",
         "google-cloud-platform,nlp,artificial-intelligence,large-language-model,google-ai-platform",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/77940890",
         "2024"
        ],
        [
         "119",
         "Loading en_core_web_sm results in AttributeError: module 'transformers' has no attribute 'BertTokenizerFast'",
         "load en_core_web_sm result attributeerror module ' transformer ' attribute ' berttokenizerfast '",
         "python,pip,nlp,anaconda,spacy",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/77839628",
         "2024"
        ],
        [
         "101",
         "TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings)",
         "typeerror exception encounter call layer ' embedding ' type tfbertembedding",
         "tensorflow,deep-learning,nlp,bert-language-model,transformer-model",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/78129126",
         "2024"
        ],
        [
         "105",
         "How to resolve ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']",
         "resolve valueerror supply encoding list encoding method include input_id provide ' label '",
         "nlp,huggingface-transformers,huggingface-tokenizers,peft",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/78031519",
         "2024"
        ],
        [
         "0",
         "NameError: name 'init_empty_weights' is not defined while using hugging face models",
         "nameerror name ' init_empty_weight ' define use hug face model",
         "nlp,huggingface-transformers,huggingface",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/79559702",
         "2025"
        ],
        [
         "116",
         "I am unable to produce rearch results from google/youtube in my speech recognition code",
         "unable produce rearch result google / youtube speech recognition code",
         "python,nlp,artificial-intelligence,speech-recognition,google-text-to-speech",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/77886445",
         "2024"
        ],
        [
         "3",
         "Trouble getting importing gensim to work in colab",
         "trouble getting import gensim work colab",
         "numpy,nlp,dependencies,google-colaboratory,gensim",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/79523269",
         "2025"
        ],
        [
         "8",
         "Can't compile Marian NMT",
         "can not compile marian nmt",
         "gcc,cmake,nlp,g++",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/79330283",
         "2025"
        ],
        [
         "86",
         "Error when calling Hugging Face load_dataset(\"glue\", \"mrpc\")",
         "error call hug face load_datasetglue mrpc",
         "python,nlp,huggingface,huggingface-datasets",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/78294720",
         "2024"
        ],
        [
         "940",
         "Is possible to get dependency/pos information for entities in Spacy?",
         "possible get dependency / pos information entity spacy",
         "nlp,spacy,named-entity-recognition,dependency-parsing",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/71726244",
         "2022"
        ],
        [
         "302",
         "Getting entities from pre-saved DocBin",
         "get entity presave docbin",
         "python,nlp,spacy",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/76313499",
         "2023"
        ],
        [
         "198",
         "How to get Enhanced++ dependency labels with a java command line in the terminal?",
         "get enhanced++ dependency label java command line terminal",
         "java,nlp,stanford-nlp",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/77093345",
         "2023"
        ],
        [
         "191",
         "Python API usage for coreference, semantic graph and NERC",
         "python api usage coreference semantic graph nerc",
         "python,nlp,named-entity-recognition,freeling",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/77148560",
         "2023"
        ],
        [
         "80",
         "Stanford NLP Annotation pipeline.annotate resulting into OutOfMemoryError in Java",
         "stanford nlp annotation pipelineannotate result outofmemoryerror java",
         "java,nlp,out-of-memory",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/78344999",
         "2024"
        ],
        [
         "6",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "opennlp postaggerme chunkerme synergy",
         "nlp,opennlp",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/79459888",
         "2025"
        ],
        [
         "358",
         "What is Stanford CoreNLP's recipe for tokenization?",
         "stanford corenlp 's recipe tokenization",
         "python,nlp,stanford-nlp,tokenize",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/75989822",
         "2023"
        ],
        [
         "227",
         "Langchain: Custom Output Parser not working with ConversationChain",
         "langchain custom output parser work conversationchain",
         "python,nlp,langchain,large-language-model",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/76877589",
         "2023"
        ],
        [
         "48",
         "How can I use structured_output with Azure OpenAI with the openai Python library?",
         "use structured_output azure openai openai python library",
         "python,nlp,azure-openai,gpt-4",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/78846004",
         "2024"
        ],
        [
         "85",
         "LangChain agent parsing error with structured_chat_agent and Wikipedia tool, handle_parsing_errors hits limit",
         "langchain agent parse error structured_chat_agent wikipedia tool handle_parsing_errors hit limit",
         "python,nlp,openai-api,langchain,large-language-model",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/78307073",
         "2024"
        ],
        [
         "18",
         "Using an AWS service to execute a python script that will extract keywords from text using keyBERT?",
         "use aws service execute python script extract keyword text use keybert",
         "python,amazon-web-services,aws-lambda,nlp,large-language-model",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/79192130",
         "2024"
        ],
        [
         "10",
         "Getting all leaf words (reverse stemming) into one Python List",
         "get leaf word reverse stem one python list",
         "python,nlp,nltk",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/79312133",
         "2024"
        ],
        [
         "362",
         "What is the most efficient way to identify text similarity between items in large lists of strings in Python?",
         "efficient way identify text similarity item large list string python",
         "python,text,nlp,cosine-similarity,hamming-distance",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/75943880",
         "2023"
        ],
        [
         "67",
         "How to lemmatize text column in pandas dataframes using stanza?",
         "lemmatize text column panda dataframe use stanza",
         "pandas,nlp,tokenize,lemmatization,stanza",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/78489915",
         "2024"
        ],
        [
         "363",
         "Fuzzy string matching in Python for structured strings?",
         "fuzzy string matching python structured string",
         "nlp,string-matching,fuzzy-comparison",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/75940556",
         "2023"
        ],
        [
         "39",
         "How to Process Data on GPU Instead of RAM for This Python Code?",
         "process datum gpu instead ram python code",
         "nlp,gpu,torch,openai-whisper",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/78917743",
         "2024"
        ],
        [
         "96",
         "How can i get the first content of a python synsets list?",
         "get first content python synset list",
         "python,nlp,nltk,sentiment-analysis,synset",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/78211318",
         "2024"
        ],
        [
         "82",
         "Sklearm FeatureHasher not working on a single column in a dataframe",
         "sklearm featurehasher work single column dataframe",
         "pandas,machine-learning,scikit-learn,nlp",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/78324647",
         "2024"
        ],
        [
         "98",
         "How to optimize the function which uses looping on lists on pandas dataframe?",
         "optimize function use loop list panda dataframe",
         "python,python-3.x,pandas,list,nlp",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/78157864",
         "2024"
        ],
        [
         "73",
         "Extracting only technical keywords from a text using RAKE library in Python",
         "extract technical keyword text use rake library python",
         "python,python-3.x,nlp,nltk,rake",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/78397201",
         "2024"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 100
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Title_Clean</th>\n",
       "      <th>Tags</th>\n",
       "      <th>title_cluster</th>\n",
       "      <th>category_name</th>\n",
       "      <th>QuestionUrl</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Normalization of token embeddings in BERT enco...</td>\n",
       "      <td>normalization token embedding bert encoder block</td>\n",
       "      <td>nlp,normalization,bert-language-model,attentio...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tokenising Text</td>\n",
       "      <td>https://stackoverflow.com/questions/79178041</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>How to Find Positional embeddings from BARTTok...</td>\n",
       "      <td>find positional embedding barttokenizer</td>\n",
       "      <td>pytorch,nlp,huggingface-transformers,summariza...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tokenising Text</td>\n",
       "      <td>https://stackoverflow.com/questions/77800331</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Why token embedding different from the embeddi...</td>\n",
       "      <td>token embed different embed bartforconditional...</td>\n",
       "      <td>machine-learning,pytorch,nlp,huggingface-trans...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tokenising Text</td>\n",
       "      <td>https://stackoverflow.com/questions/77906649</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How can I adjust the performance of tokenizer?</td>\n",
       "      <td>adjust performance tokenizer</td>\n",
       "      <td>nlp,huggingface-transformers,huggingface,huggi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tokenising Text</td>\n",
       "      <td>https://stackoverflow.com/questions/79100835</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How to convert character indices to BERT token...</td>\n",
       "      <td>convert character indices bert token index</td>\n",
       "      <td>python,nlp,dataset,large-language-model,bert-l...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tokenising Text</td>\n",
       "      <td>https://stackoverflow.com/questions/79173053</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How to derive attributes/labels from short pla...</td>\n",
       "      <td>derive attribute / label short plain text desc...</td>\n",
       "      <td>nlp,artificial-intelligence,large-language-mod...</td>\n",
       "      <td>9</td>\n",
       "      <td>NLP Application</td>\n",
       "      <td>https://stackoverflow.com/questions/79111733</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>FastText language_identification in R returns ...</td>\n",
       "      <td>fasttext language_identification r return many...</td>\n",
       "      <td>r,nlp,fasttext,language-detection</td>\n",
       "      <td>9</td>\n",
       "      <td>NLP Application</td>\n",
       "      <td>https://stackoverflow.com/questions/78443980</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>How to detect if two sentences are simmilar, n...</td>\n",
       "      <td>detect two sentence simmilar mean syllable / word</td>\n",
       "      <td>search,nlp,full-text-search,similarity,sentenc...</td>\n",
       "      <td>9</td>\n",
       "      <td>NLP Application</td>\n",
       "      <td>https://stackoverflow.com/questions/78241665</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Performance of textSimilarity() from R's text ...</td>\n",
       "      <td>performance textsimilarity r 's text library</td>\n",
       "      <td>r,performance,nlp,huggingface-transformers</td>\n",
       "      <td>9</td>\n",
       "      <td>NLP Application</td>\n",
       "      <td>https://stackoverflow.com/questions/78393709</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>BERTopic: \"Make sure that the iterable only co...</td>\n",
       "      <td>bertopic make sure iterable contain string</td>\n",
       "      <td>python,python-3.x,nlp,topic-modeling</td>\n",
       "      <td>9</td>\n",
       "      <td>NLP Application</td>\n",
       "      <td>https://stackoverflow.com/questions/77846486</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "19   Normalization of token embeddings in BERT enco...   \n",
       "126  How to Find Positional embeddings from BARTTok...   \n",
       "114  Why token embedding different from the embeddi...   \n",
       "27      How can I adjust the performance of tokenizer?   \n",
       "20   How to convert character indices to BERT token...   \n",
       "..                                                 ...   \n",
       "25   How to derive attributes/labels from short pla...   \n",
       "71   FastText language_identification in R returns ...   \n",
       "93   How to detect if two sentences are simmilar, n...   \n",
       "75   Performance of textSimilarity() from R's text ...   \n",
       "118  BERTopic: \"Make sure that the iterable only co...   \n",
       "\n",
       "                                           Title_Clean  \\\n",
       "19    normalization token embedding bert encoder block   \n",
       "126            find positional embedding barttokenizer   \n",
       "114  token embed different embed bartforconditional...   \n",
       "27                        adjust performance tokenizer   \n",
       "20          convert character indices bert token index   \n",
       "..                                                 ...   \n",
       "25   derive attribute / label short plain text desc...   \n",
       "71   fasttext language_identification r return many...   \n",
       "93   detect two sentence simmilar mean syllable / word   \n",
       "75        performance textsimilarity r 's text library   \n",
       "118         bertopic make sure iterable contain string   \n",
       "\n",
       "                                                  Tags  title_cluster  \\\n",
       "19   nlp,normalization,bert-language-model,attentio...              0   \n",
       "126  pytorch,nlp,huggingface-transformers,summariza...              0   \n",
       "114  machine-learning,pytorch,nlp,huggingface-trans...              0   \n",
       "27   nlp,huggingface-transformers,huggingface,huggi...              0   \n",
       "20   python,nlp,dataset,large-language-model,bert-l...              0   \n",
       "..                                                 ...            ...   \n",
       "25   nlp,artificial-intelligence,large-language-mod...              9   \n",
       "71                   r,nlp,fasttext,language-detection              9   \n",
       "93   search,nlp,full-text-search,similarity,sentenc...              9   \n",
       "75          r,performance,nlp,huggingface-transformers              9   \n",
       "118               python,python-3.x,nlp,topic-modeling              9   \n",
       "\n",
       "       category_name                                   QuestionUrl  Year  \n",
       "19   Tokenising Text  https://stackoverflow.com/questions/79178041  2024  \n",
       "126  Tokenising Text  https://stackoverflow.com/questions/77800331  2024  \n",
       "114  Tokenising Text  https://stackoverflow.com/questions/77906649  2024  \n",
       "27   Tokenising Text  https://stackoverflow.com/questions/79100835  2024  \n",
       "20   Tokenising Text  https://stackoverflow.com/questions/79173053  2024  \n",
       "..               ...                                           ...   ...  \n",
       "25   NLP Application  https://stackoverflow.com/questions/79111733  2024  \n",
       "71   NLP Application  https://stackoverflow.com/questions/78443980  2024  \n",
       "93   NLP Application  https://stackoverflow.com/questions/78241665  2024  \n",
       "75   NLP Application  https://stackoverflow.com/questions/78393709  2024  \n",
       "118  NLP Application  https://stackoverflow.com/questions/77846486  2024  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = (\n",
    "    df_post_answer[['Title','Title_Clean','Tags','title_cluster','category_name','QuestionUrl','Year']]\n",
    "    .sort_values(\"Year\", ascending=False)\n",
    "    .groupby(\"title_cluster\", as_index=False)\n",
    "    .head(10)\n",
    ").sort_values(\"title_cluster\")\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title_cluster",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "category_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "QuestionUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Year",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7388bf82-b2fd-48ba-b949-a102d5fc6f66",
       "rows": [
        [
         "0",
         "NameError: name 'init_empty_weights' is not defined while using hugging face models",
         "nameerror name ' init_empty_weight ' define use hug face model",
         "nlp,huggingface-transformers,huggingface",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/79559702",
         "2025"
        ],
        [
         "2",
         "GPT-2 and other models from huggingface -100 label index for training, instead of pad token",
         "gpt2 model huggingface 100 label index training instead pad token",
         "nlp,huggingface-transformers,pre-trained-model",
         "6",
         "BERT & Hugging Face Application",
         "https://stackoverflow.com/questions/79548202",
         "2025"
        ],
        [
         "3",
         "Trouble getting importing gensim to work in colab",
         "trouble getting import gensim work colab",
         "numpy,nlp,dependencies,google-colaboratory,gensim",
         "2",
         "Handling Error in NLP Task",
         "https://stackoverflow.com/questions/79523269",
         "2025"
        ],
        [
         "4",
         "Store images instead of showing in a server",
         "store image instead show server",
         "python,nlp,large-language-model",
         "6",
         "BERT & Hugging Face Application",
         "https://stackoverflow.com/questions/79501178",
         "2025"
        ],
        [
         "5",
         "Presidio with Langchain Experimental does not detect Polish names",
         "presidio langchain experimental detect polish name",
         "python,nlp,spacy,langchain,presidio",
         "9",
         "NLP Application",
         "https://stackoverflow.com/questions/79482283",
         "2025"
        ],
        [
         "6",
         "OpenNLP POSTaggerME and ChunkerME synergy",
         "opennlp postaggerme chunkerme synergy",
         "nlp,opennlp",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/79459888",
         "2025"
        ],
        [
         "1",
         "Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?",
         "presidio spacy nlp engine recognize organization pesel spacy",
         "python,nlp,spacy,presidio",
         "5",
         "Using Spacy Library",
         "https://stackoverflow.com/questions/79549787",
         "2025"
        ],
        [
         "92",
         "Get previous sentence while using SpaCy matcher",
         "get previous sentence use spacy matcher",
         "python,nlp,spacy",
         "5",
         "Using Spacy Library",
         "https://stackoverflow.com/questions/78258373",
         "2024"
        ],
        [
         "87",
         "What's inside inner vertices in Word2Vec Hierarchical Softmax?",
         "'s inside inner vertex word2vec hierarchical softmax",
         "machine-learning,nlp,word2vec,hierarchical,softmax",
         "8",
         "Word Embedding",
         "https://stackoverflow.com/questions/78285447",
         "2024"
        ],
        [
         "89",
         "What is the best function/stage to use tokenizer in Pytorch's data processing?",
         "good function / stage use tokenizer pytorch 's datum processing",
         "pytorch,nlp",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/78284866",
         "2024"
        ],
        [
         "91",
         "Gensim's Doc2Vec with documents in multiple languages",
         "gensim 's doc2vec document multiple language",
         "python,nlp,gensim,recommendation-engine,doc2vec",
         "8",
         "Word Embedding",
         "https://stackoverflow.com/questions/78262529",
         "2024"
        ],
        [
         "93",
         "How to detect if two sentences are simmilar, not in meaning, but in syllables/words?",
         "detect two sentence simmilar mean syllable / word",
         "search,nlp,full-text-search,similarity,sentence-similarity",
         "9",
         "NLP Application",
         "https://stackoverflow.com/questions/78241665",
         "2024"
        ],
        [
         "96",
         "How can i get the first content of a python synsets list?",
         "get first content python synset list",
         "python,nlp,nltk,sentiment-analysis,synset",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/78211318",
         "2024"
        ],
        [
         "85",
         "LangChain agent parsing error with structured_chat_agent and Wikipedia tool, handle_parsing_errors hits limit",
         "langchain agent parse error structured_chat_agent wikipedia tool handle_parsing_errors hit limit",
         "python,nlp,openai-api,langchain,large-language-model",
         "3",
         "Using Stanford Library",
         "https://stackoverflow.com/questions/78307073",
         "2024"
        ],
        [
         "98",
         "How to optimize the function which uses looping on lists on pandas dataframe?",
         "optimize function use loop list panda dataframe",
         "python,python-3.x,pandas,list,nlp",
         "4",
         "Preprocessing Text in Python",
         "https://stackoverflow.com/questions/78157864",
         "2024"
        ],
        [
         "84",
         "R Tidymodels textrecipes - tokenizing with spacyR - how to remove punctuations from produced list of tokens",
         "r tidymodel textrecipe tokenize spacyr remove punctuation produce list token",
         "r,nlp,spacy,tidymodels",
         "0",
         "Tokenising Text",
         "https://stackoverflow.com/questions/78314842",
         "2024"
        ],
        [
         "72",
         "Determining contents of decoder_hidden_states from T5ForConditionalGeneration",
         "determine content decoder_hidden_states t5forconditionalgeneration",
         "pytorch,nlp,huggingface-transformers",
         "7",
         "Text Similarity",
         "https://stackoverflow.com/questions/78430524",
         "2024"
        ],
        [
         "127",
         "How to find similar sounding words?",
         "find similar sound word",
         "python,nlp,nltk",
         "7",
         "Text Similarity",
         "https://stackoverflow.com/questions/77784846",
         "2024"
        ],
        [
         "110",
         "Extracting and Identifying locations with NLP + Spacy",
         "extract identify location nlp + spacy",
         "nlp,spacy,named-entity-recognition",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/77951208",
         "2024"
        ],
        [
         "107",
         "Shorten product title to a specific length using python nlp libraries",
         "shorten product title specific length use python nlp library",
         "python,string,nlp,nltk,e-commerce",
         "1",
         "Using NLTK Library",
         "https://stackoverflow.com/questions/78006552",
         "2024"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Title_Clean</th>\n",
       "      <th>Tags</th>\n",
       "      <th>title_cluster</th>\n",
       "      <th>category_name</th>\n",
       "      <th>QuestionUrl</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NameError: name 'init_empty_weights' is not de...</td>\n",
       "      <td>nameerror name ' init_empty_weight ' define us...</td>\n",
       "      <td>nlp,huggingface-transformers,huggingface</td>\n",
       "      <td>2</td>\n",
       "      <td>Handling Error in NLP Task</td>\n",
       "      <td>https://stackoverflow.com/questions/79559702</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>gpt2 model huggingface 100 label index trainin...</td>\n",
       "      <td>nlp,huggingface-transformers,pre-trained-model</td>\n",
       "      <td>6</td>\n",
       "      <td>BERT &amp; Hugging Face Application</td>\n",
       "      <td>https://stackoverflow.com/questions/79548202</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>trouble getting import gensim work colab</td>\n",
       "      <td>numpy,nlp,dependencies,google-colaboratory,gensim</td>\n",
       "      <td>2</td>\n",
       "      <td>Handling Error in NLP Task</td>\n",
       "      <td>https://stackoverflow.com/questions/79523269</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>store image instead show server</td>\n",
       "      <td>python,nlp,large-language-model</td>\n",
       "      <td>6</td>\n",
       "      <td>BERT &amp; Hugging Face Application</td>\n",
       "      <td>https://stackoverflow.com/questions/79501178</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>presidio langchain experimental detect polish ...</td>\n",
       "      <td>python,nlp,spacy,langchain,presidio</td>\n",
       "      <td>9</td>\n",
       "      <td>NLP Application</td>\n",
       "      <td>https://stackoverflow.com/questions/79482283</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OpenNLP POSTaggerME and ChunkerME synergy</td>\n",
       "      <td>opennlp postaggerme chunkerme synergy</td>\n",
       "      <td>nlp,opennlp</td>\n",
       "      <td>3</td>\n",
       "      <td>Using Stanford Library</td>\n",
       "      <td>https://stackoverflow.com/questions/79459888</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>presidio spacy nlp engine recognize organizati...</td>\n",
       "      <td>python,nlp,spacy,presidio</td>\n",
       "      <td>5</td>\n",
       "      <td>Using Spacy Library</td>\n",
       "      <td>https://stackoverflow.com/questions/79549787</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Get previous sentence while using SpaCy matcher</td>\n",
       "      <td>get previous sentence use spacy matcher</td>\n",
       "      <td>python,nlp,spacy</td>\n",
       "      <td>5</td>\n",
       "      <td>Using Spacy Library</td>\n",
       "      <td>https://stackoverflow.com/questions/78258373</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>What's inside inner vertices in Word2Vec Hiera...</td>\n",
       "      <td>'s inside inner vertex word2vec hierarchical s...</td>\n",
       "      <td>machine-learning,nlp,word2vec,hierarchical,sof...</td>\n",
       "      <td>8</td>\n",
       "      <td>Word Embedding</td>\n",
       "      <td>https://stackoverflow.com/questions/78285447</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>What is the best function/stage to use tokeniz...</td>\n",
       "      <td>good function / stage use tokenizer pytorch 's...</td>\n",
       "      <td>pytorch,nlp</td>\n",
       "      <td>0</td>\n",
       "      <td>Tokenising Text</td>\n",
       "      <td>https://stackoverflow.com/questions/78284866</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Gensim's Doc2Vec with documents in multiple la...</td>\n",
       "      <td>gensim 's doc2vec document multiple language</td>\n",
       "      <td>python,nlp,gensim,recommendation-engine,doc2vec</td>\n",
       "      <td>8</td>\n",
       "      <td>Word Embedding</td>\n",
       "      <td>https://stackoverflow.com/questions/78262529</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>How to detect if two sentences are simmilar, n...</td>\n",
       "      <td>detect two sentence simmilar mean syllable / word</td>\n",
       "      <td>search,nlp,full-text-search,similarity,sentenc...</td>\n",
       "      <td>9</td>\n",
       "      <td>NLP Application</td>\n",
       "      <td>https://stackoverflow.com/questions/78241665</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>How can i get the first content of a python sy...</td>\n",
       "      <td>get first content python synset list</td>\n",
       "      <td>python,nlp,nltk,sentiment-analysis,synset</td>\n",
       "      <td>4</td>\n",
       "      <td>Preprocessing Text in Python</td>\n",
       "      <td>https://stackoverflow.com/questions/78211318</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>LangChain agent parsing error with structured_...</td>\n",
       "      <td>langchain agent parse error structured_chat_ag...</td>\n",
       "      <td>python,nlp,openai-api,langchain,large-language...</td>\n",
       "      <td>3</td>\n",
       "      <td>Using Stanford Library</td>\n",
       "      <td>https://stackoverflow.com/questions/78307073</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>How to optimize the function which uses loopin...</td>\n",
       "      <td>optimize function use loop list panda dataframe</td>\n",
       "      <td>python,python-3.x,pandas,list,nlp</td>\n",
       "      <td>4</td>\n",
       "      <td>Preprocessing Text in Python</td>\n",
       "      <td>https://stackoverflow.com/questions/78157864</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>R Tidymodels textrecipes - tokenizing with spa...</td>\n",
       "      <td>r tidymodel textrecipe tokenize spacyr remove ...</td>\n",
       "      <td>r,nlp,spacy,tidymodels</td>\n",
       "      <td>0</td>\n",
       "      <td>Tokenising Text</td>\n",
       "      <td>https://stackoverflow.com/questions/78314842</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Determining contents of decoder_hidden_states ...</td>\n",
       "      <td>determine content decoder_hidden_states t5forc...</td>\n",
       "      <td>pytorch,nlp,huggingface-transformers</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Similarity</td>\n",
       "      <td>https://stackoverflow.com/questions/78430524</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>How to find similar sounding words?</td>\n",
       "      <td>find similar sound word</td>\n",
       "      <td>python,nlp,nltk</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Similarity</td>\n",
       "      <td>https://stackoverflow.com/questions/77784846</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Extracting and Identifying locations with NLP ...</td>\n",
       "      <td>extract identify location nlp + spacy</td>\n",
       "      <td>nlp,spacy,named-entity-recognition</td>\n",
       "      <td>1</td>\n",
       "      <td>Using NLTK Library</td>\n",
       "      <td>https://stackoverflow.com/questions/77951208</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Shorten product title to a specific length usi...</td>\n",
       "      <td>shorten product title specific length use pyth...</td>\n",
       "      <td>python,string,nlp,nltk,e-commerce</td>\n",
       "      <td>1</td>\n",
       "      <td>Using NLTK Library</td>\n",
       "      <td>https://stackoverflow.com/questions/78006552</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    NameError: name 'init_empty_weights' is not de...   \n",
       "2    GPT-2 and other models from huggingface -100 l...   \n",
       "3    Trouble getting importing gensim to work in colab   \n",
       "4          Store images instead of showing in a server   \n",
       "5    Presidio with Langchain Experimental does not ...   \n",
       "6            OpenNLP POSTaggerME and ChunkerME synergy   \n",
       "1    Why does Presidio with spacy nlp engine not re...   \n",
       "92     Get previous sentence while using SpaCy matcher   \n",
       "87   What's inside inner vertices in Word2Vec Hiera...   \n",
       "89   What is the best function/stage to use tokeniz...   \n",
       "91   Gensim's Doc2Vec with documents in multiple la...   \n",
       "93   How to detect if two sentences are simmilar, n...   \n",
       "96   How can i get the first content of a python sy...   \n",
       "85   LangChain agent parsing error with structured_...   \n",
       "98   How to optimize the function which uses loopin...   \n",
       "84   R Tidymodels textrecipes - tokenizing with spa...   \n",
       "72   Determining contents of decoder_hidden_states ...   \n",
       "127                How to find similar sounding words?   \n",
       "110  Extracting and Identifying locations with NLP ...   \n",
       "107  Shorten product title to a specific length usi...   \n",
       "\n",
       "                                           Title_Clean  \\\n",
       "0    nameerror name ' init_empty_weight ' define us...   \n",
       "2    gpt2 model huggingface 100 label index trainin...   \n",
       "3             trouble getting import gensim work colab   \n",
       "4                      store image instead show server   \n",
       "5    presidio langchain experimental detect polish ...   \n",
       "6                opennlp postaggerme chunkerme synergy   \n",
       "1    presidio spacy nlp engine recognize organizati...   \n",
       "92             get previous sentence use spacy matcher   \n",
       "87   's inside inner vertex word2vec hierarchical s...   \n",
       "89   good function / stage use tokenizer pytorch 's...   \n",
       "91        gensim 's doc2vec document multiple language   \n",
       "93   detect two sentence simmilar mean syllable / word   \n",
       "96                get first content python synset list   \n",
       "85   langchain agent parse error structured_chat_ag...   \n",
       "98     optimize function use loop list panda dataframe   \n",
       "84   r tidymodel textrecipe tokenize spacyr remove ...   \n",
       "72   determine content decoder_hidden_states t5forc...   \n",
       "127                            find similar sound word   \n",
       "110              extract identify location nlp + spacy   \n",
       "107  shorten product title specific length use pyth...   \n",
       "\n",
       "                                                  Tags  title_cluster  \\\n",
       "0             nlp,huggingface-transformers,huggingface              2   \n",
       "2       nlp,huggingface-transformers,pre-trained-model              6   \n",
       "3    numpy,nlp,dependencies,google-colaboratory,gensim              2   \n",
       "4                      python,nlp,large-language-model              6   \n",
       "5                  python,nlp,spacy,langchain,presidio              9   \n",
       "6                                          nlp,opennlp              3   \n",
       "1                            python,nlp,spacy,presidio              5   \n",
       "92                                    python,nlp,spacy              5   \n",
       "87   machine-learning,nlp,word2vec,hierarchical,sof...              8   \n",
       "89                                         pytorch,nlp              0   \n",
       "91     python,nlp,gensim,recommendation-engine,doc2vec              8   \n",
       "93   search,nlp,full-text-search,similarity,sentenc...              9   \n",
       "96           python,nlp,nltk,sentiment-analysis,synset              4   \n",
       "85   python,nlp,openai-api,langchain,large-language...              3   \n",
       "98                   python,python-3.x,pandas,list,nlp              4   \n",
       "84                              r,nlp,spacy,tidymodels              0   \n",
       "72                pytorch,nlp,huggingface-transformers              7   \n",
       "127                                    python,nlp,nltk              7   \n",
       "110                 nlp,spacy,named-entity-recognition              1   \n",
       "107                  python,string,nlp,nltk,e-commerce              1   \n",
       "\n",
       "                       category_name  \\\n",
       "0         Handling Error in NLP Task   \n",
       "2    BERT & Hugging Face Application   \n",
       "3         Handling Error in NLP Task   \n",
       "4    BERT & Hugging Face Application   \n",
       "5                    NLP Application   \n",
       "6             Using Stanford Library   \n",
       "1                Using Spacy Library   \n",
       "92               Using Spacy Library   \n",
       "87                    Word Embedding   \n",
       "89                   Tokenising Text   \n",
       "91                    Word Embedding   \n",
       "93                   NLP Application   \n",
       "96      Preprocessing Text in Python   \n",
       "85            Using Stanford Library   \n",
       "98      Preprocessing Text in Python   \n",
       "84                   Tokenising Text   \n",
       "72                   Text Similarity   \n",
       "127                  Text Similarity   \n",
       "110               Using NLTK Library   \n",
       "107               Using NLTK Library   \n",
       "\n",
       "                                      QuestionUrl  Year  \n",
       "0    https://stackoverflow.com/questions/79559702  2025  \n",
       "2    https://stackoverflow.com/questions/79548202  2025  \n",
       "3    https://stackoverflow.com/questions/79523269  2025  \n",
       "4    https://stackoverflow.com/questions/79501178  2025  \n",
       "5    https://stackoverflow.com/questions/79482283  2025  \n",
       "6    https://stackoverflow.com/questions/79459888  2025  \n",
       "1    https://stackoverflow.com/questions/79549787  2025  \n",
       "92   https://stackoverflow.com/questions/78258373  2024  \n",
       "87   https://stackoverflow.com/questions/78285447  2024  \n",
       "89   https://stackoverflow.com/questions/78284866  2024  \n",
       "91   https://stackoverflow.com/questions/78262529  2024  \n",
       "93   https://stackoverflow.com/questions/78241665  2024  \n",
       "96   https://stackoverflow.com/questions/78211318  2024  \n",
       "85   https://stackoverflow.com/questions/78307073  2024  \n",
       "98   https://stackoverflow.com/questions/78157864  2024  \n",
       "84   https://stackoverflow.com/questions/78314842  2024  \n",
       "72   https://stackoverflow.com/questions/78430524  2024  \n",
       "127  https://stackoverflow.com/questions/77784846  2024  \n",
       "110  https://stackoverflow.com/questions/77951208  2024  \n",
       "107  https://stackoverflow.com/questions/78006552  2024  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top2 = (\n",
    "    df_post_answer[['Title','Title_Clean','Tags','title_cluster','category_name','QuestionUrl','Year']]\n",
    "    .sort_values(\"Year\", ascending=False)\n",
    "    .groupby(\"title_cluster\", as_index=False)\n",
    "    .head(2)\n",
    ")\n",
    "top2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
